{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is a highly applicable field of natural language processing that will likely be very useful for the leadership prize challenge. At a high level, the purpose of NER is to extract the meaningful words from a given sentence. This is done by extracting the name of the entities in question, along with the class. For example, in the sentence\n",
    "\n",
    "`\n",
    "Mark works for Facebook.\n",
    "`\n",
    "\n",
    "We have two named entities that can be extracted. These can be seen in the table below\n",
    "\n",
    "|  Name    |    Class     |\n",
    "|:--------:|:------------:|\n",
    "|`Mark`  |`Person`   |\n",
    "|`Facebook`|`Organization`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling\n",
    "\n",
    "The Mathematics behind NER models was difficult to grasp, even for an Apple-Mathematician such as myself, however in short most models involve Markov Chains, Conditional Probability and Graph Theory. What is more interesting is how the models work. The afforementioned mathematics is used to construct dependency graphs, where the relationship between words in the sentence is conveyed. An example sentence is shown below:\n",
    "\n",
    "<p align=\"center\">\n",
    "    <img src=\"./img/dependency_graph.png\" />\n",
    "</p>\n",
    "\n",
    "This sentence includes annotations with both dependency and named entity information. Inside-outside-beginning (IOB) tagging is used to show how specific words relate to entities. We see that `The` is the sole beginning word, `House`, `of`, and `Representatives` are classified as inside words, while `votes`, `on`, `the`, and `measure` are classified as outside words. Arrows are also included to illustrate the dependency graph between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Information Extraction in Python\n",
    "> This is kind of interesting but don't think it'll be that relevant, skip to the next section if you want ot see more relevant stuff\n",
    "\n",
    "We can implement NER in Python using [NLTK](https://www.nltk.org/) and [SpaCy](https://spacy.io/). For the purpose of this example I will take some words that were said recently by a complete moron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('President', 'NNP'), ('Donald', 'NNP'), ('Trump', 'NNP'), ('told', 'VBD'), ('four', 'CD'), ('congresswomen', 'NNS'), ('to', 'TO'), ('go', 'VB'), ('back', 'RB'), ('to', 'TO'), ('the', 'DT'), ('countries', 'NNS'), ('where', 'WRB'), ('they', 'PRP'), ('came', 'VBD'), ('from', 'IN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "\n",
    "# >:-(\n",
    "ex = \"President Donald Trump told four congresswomen to go back to the countries where they came from.\"\n",
    "\n",
    "\n",
    "def preprocess(sent):\n",
    "    \"\"\"\n",
    "    Apply word tokenization and part-of-speech tagging.\n",
    "    \"\"\"\n",
    "    sent = word_tokenize(sent)\n",
    "    sent = pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "\n",
    "sent = preprocess(ex)\n",
    "print(sent)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we are left with is a list of tuples containing the individual words and their associated part-of-speech. To make this more useful, we'll implement noun phrase chunking. This will make use of *Regular Expressions* to find any named entities. Our regular expression states that a noun phrase (NP) should be formed when the chunker finds an optional determiner (DT) followed by any number of adjectives (JJ) and finally a noun (NN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  President/NNP\n",
      "  Donald/NNP\n",
      "  Trump/NNP\n",
      "  told/VBD\n",
      "  four/CD\n",
      "  congresswomen/NNS\n",
      "  to/TO\n",
      "  go/VB\n",
      "  back/RB\n",
      "  to/TO\n",
      "  the/DT\n",
      "  countries/NNS\n",
      "  where/WRB\n",
      "  they/PRP\n",
      "  came/VBD\n",
      "  from/IN\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "print(cs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with before, nltk allows us to see IOB tags as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('President', 'NNP', 'O'),\n",
      " ('Donald', 'NNP', 'O'),\n",
      " ('Trump', 'NNP', 'O'),\n",
      " ('told', 'VBD', 'O'),\n",
      " ('four', 'CD', 'O'),\n",
      " ('congresswomen', 'NNS', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('go', 'VB', 'O'),\n",
      " ('back', 'RB', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('countries', 'NNS', 'O'),\n",
      " ('where', 'WRB', 'O'),\n",
      " ('they', 'PRP', 'O'),\n",
      " ('came', 'VBD', 'O'),\n",
      " ('from', 'IN', 'O'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.chunk import conlltags2tree, tree2conlltags\n",
    "from pprint import pprint\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NER in Python\n",
    "\n",
    "For extracting named entities from text, we turn to our good old friend `SpaCy`. We will now use a new example to see how we can extract named entities with `SpaCy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('1924', 'DATE'), ('25', 'CARDINAL'), ('six', 'CARDINAL'), ('Canada', 'GPE')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = nlp(\"Since reporting began in 1924, the federal government reports a total of 25 people in six provinces have died of rabies in Canada.\")\n",
    "\n",
    "pprint([(X.text, X.label_) for X in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see here the named entities with their specific types, given to us by `SpaCy`. As per the docs, the types from the output are described as followed:\n",
    "\n",
    "|Type  |Description                           |\n",
    "|:-----|--------------------------------------|\n",
    "|`DATE`|Absolute or relative dates or periods.|\n",
    "|`CARDINAL`|Numerals that do not fall under another type.|\n",
    "|`GPE`| Countries, cities, states. |\n",
    "\n",
    "> See the full list [here](https://spacy.io/api/annotation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now choose to focus on *tokens* instead of entities, we can do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Since, 'O', ''),\n",
      " (reporting, 'O', ''),\n",
      " (began, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (1924, 'B', 'DATE'),\n",
      " (,, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (federal, 'O', ''),\n",
      " (government, 'O', ''),\n",
      " (reports, 'O', ''),\n",
      " (a, 'O', ''),\n",
      " (total, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (25, 'B', 'CARDINAL'),\n",
      " (people, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (six, 'B', 'CARDINAL'),\n",
      " (provinces, 'O', ''),\n",
      " (have, 'O', ''),\n",
      " (died, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (rabies, 'O', ''),\n",
      " (in, 'O', ''),\n",
      " (Canada, 'B', 'GPE'),\n",
      " (., 'O', '')]\n"
     ]
    }
   ],
   "source": [
    "pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see here that `IOB` tagging is provided, and types are provided only to named entities. As a better example if we pick a sentence where certain entities contain multiple words we can see the distinction between tokens and entities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENTITIES:\n",
      "\n",
      "[('Donald Trump', 'PERSON'), ('the United States of America', 'GPE')]\n",
      "\n",
      "TOKENS:\n",
      "\n",
      "[(Donald, 'B', 'PERSON'),\n",
      " (Trump, 'I', 'PERSON'),\n",
      " (is, 'O', ''),\n",
      " (the, 'O', ''),\n",
      " (president, 'O', ''),\n",
      " (of, 'O', ''),\n",
      " (the, 'B', 'GPE'),\n",
      " (United, 'I', 'GPE'),\n",
      " (States, 'I', 'GPE'),\n",
      " (of, 'I', 'GPE'),\n",
      " (America, 'I', 'GPE'),\n",
      " (., 'O', '')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Donald Trump is the president of the United States of America.\")\n",
    "print(\"ENTITIES:\\n\")\n",
    "\n",
    "pprint([(X.text, X.label_) for X in doc.ents])\n",
    "\n",
    "print(\"\\nTOKENS:\\n\")\n",
    "\n",
    "pprint([(X, X.ent_iob_, X.ent_type_) for X in doc])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In conclusion, named entity recognition is really cool and we should definitely use it (with `SpaCy` over `nltk`) in some form in our final product. I hope you enjoyed reading this notebook!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
