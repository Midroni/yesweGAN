{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maskGAN_imdb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZyejo0JiKGZ",
        "colab_type": "code",
        "outputId": "82c1d279-41d7-4b42-851b-1117ee61b287",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        }
      },
      "source": [
        "!git clone -b preprocessing https://github.com/Midroni/yesweGAN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yesweGAN'...\n",
            "remote: Enumerating objects: 253, done.\u001b[K\n",
            "remote: Counting objects: 100% (253/253), done.\u001b[K\n",
            "remote: Compressing objects: 100% (190/190), done.\u001b[K\n",
            "remote: Total 443 (delta 125), reused 183 (delta 60), pack-reused 190\u001b[K\n",
            "Receiving objects: 100% (443/443), 388.35 MiB | 33.09 MiB/s, done.\n",
            "Resolving deltas: 100% (167/167), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOguCX-SiNFt",
        "colab_type": "code",
        "outputId": "d9348325-2290-4112-95c1-a3138a3b9a32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 660
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.6.0\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/a2/38929ec9677cb0009837b77674388ab4a35ad81573f3289b21963eda0f9a/tensorflow_gpu-1.6.0-cp27-cp27mu-manylinux1_x86_64.whl (209.2MB)\n",
            "\u001b[K     |████████████████████████████████| 209.2MB 60kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (2.0.0)\n",
            "Collecting tensorboard<1.7.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/b8/7f64efd6aea9e21b836dc9acac60634ce9c41fe153ffd4df2acedc9a86e6/tensorboard-1.6.0-py2-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.0.post1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.1.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==1.6.0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.6.0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.6.0) (5.4.0)\n",
            "Requirement already satisfied: bleach==1.5.0 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (1.5.0)\n",
            "Requirement already satisfied: html5lib==0.9999999 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (0.9999999)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.4.0->tensorflow-gpu==1.6.0) (44.0.0)\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.15.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorboard, tensorflow-gpu\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "  Found existing installation: tensorflow-gpu 1.5.0\n",
            "    Uninstalling tensorflow-gpu-1.5.0:\n",
            "      Successfully uninstalled tensorflow-gpu-1.5.0\n",
            "Successfully installed tensorboard-1.6.0 tensorflow-gpu-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7f7G5dghg-r",
        "colab_type": "code",
        "outputId": "50b382c2-f2f7-4718-91af-e649a725c112",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda=9.0.176-1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-11 02:19:38--  https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.199.0.24\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.199.0.24|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?qog6p9QAnkJghf8sbN0srD64DoGwMOve5NnvJHhZsQ6hdZIhGXFNXPndN8ATtDaU1X8pj0pUwObiTzHamzUy2GRz4ds2bffIR_TNm6l7kXSOdukWe2QarHumMDqmlhLHPHG-yxtdhh7Xx0rNnP_4fkXH3l-HeelQsIrAlo0RqLBi1IO6IGpO3xLJUABrNFh6T6SzTClE7OvpNKqeUnPT [following]\n",
            "--2020-03-11 02:19:38--  https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?qog6p9QAnkJghf8sbN0srD64DoGwMOve5NnvJHhZsQ6hdZIhGXFNXPndN8ATtDaU1X8pj0pUwObiTzHamzUy2GRz4ds2bffIR_TNm6l7kXSOdukWe2QarHumMDqmlhLHPHG-yxtdhh7Xx0rNnP_4fkXH3l-HeelQsIrAlo0RqLBi1IO6IGpO3xLJUABrNFh6T6SzTClE7OvpNKqeUnPT\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.195.19.142\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.195.19.142|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1212738714 (1.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.13G  37.5MB/s    in 25s     \n",
            "\n",
            "2020-03-11 02:20:03 (47.2 MB/s) - ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’ saved [1212738714/1212738714]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-9-0-local.\n",
            "(Reading database ... 145118 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Ign:4 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:6 file:/var/cuda-repo-9-0-local  Packages [15.4 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [819 B]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:18 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [141 kB]\n",
            "Get:19 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [37.1 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [836 kB]\n",
            "Get:21 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,781 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,128 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [826 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,355 kB]\n",
            "Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [860 kB]\n",
            "Fetched 7,260 kB in 3s (2,190 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "0 upgraded, 34 newly installed, 0 to remove and 84 not upgraded.\n",
            "Need to get 0 B/1,097 MB of archives.\n",
            "After this operation, 2,315 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-9-0-local  cuda-license-9-0 9.0.176-1 [22.0 kB]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  cuda-misc-headers-9-0 9.0.176-1 [684 kB]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  cuda-core-9-0 9.0.176-1 [16.9 MB]\n",
            "Get:4 file:/var/cuda-repo-9-0-local  cuda-cudart-9-0 9.0.176-1 [106 kB]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  cuda-driver-dev-9-0 9.0.176-1 [10.9 kB]\n",
            "Get:6 file:/var/cuda-repo-9-0-local  cuda-cudart-dev-9-0 9.0.176-1 [767 kB]\n",
            "Get:7 file:/var/cuda-repo-9-0-local  cuda-command-line-tools-9-0 9.0.176-1 [25.4 MB]\n",
            "Get:8 file:/var/cuda-repo-9-0-local  cuda-nvrtc-9-0 9.0.176-1 [6,348 kB]\n",
            "Get:9 file:/var/cuda-repo-9-0-local  cuda-nvrtc-dev-9-0 9.0.176-1 [9,334 B]\n",
            "Get:10 file:/var/cuda-repo-9-0-local  cuda-cusolver-9-0 9.0.176-1 [26.2 MB]\n",
            "Get:11 file:/var/cuda-repo-9-0-local  cuda-cusolver-dev-9-0 9.0.176-1 [5,317 kB]\n",
            "Get:12 file:/var/cuda-repo-9-0-local  cuda-cublas-9-0 9.0.176-1 [25.0 MB]\n",
            "Get:13 file:/var/cuda-repo-9-0-local  cuda-cublas-dev-9-0 9.0.176-1 [49.4 MB]\n",
            "Get:14 file:/var/cuda-repo-9-0-local  cuda-cufft-9-0 9.0.176-1 [84.1 MB]\n",
            "Get:15 file:/var/cuda-repo-9-0-local  cuda-cufft-dev-9-0 9.0.176-1 [73.7 MB]\n",
            "Get:16 file:/var/cuda-repo-9-0-local  cuda-curand-9-0 9.0.176-1 [38.8 MB]\n",
            "Get:17 file:/var/cuda-repo-9-0-local  cuda-curand-dev-9-0 9.0.176-1 [57.9 MB]\n",
            "Get:18 file:/var/cuda-repo-9-0-local  cuda-cusparse-9-0 9.0.176-1 [25.2 MB]\n",
            "Get:19 file:/var/cuda-repo-9-0-local  cuda-cusparse-dev-9-0 9.0.176-1 [25.3 MB]\n",
            "Get:20 file:/var/cuda-repo-9-0-local  cuda-npp-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:21 file:/var/cuda-repo-9-0-local  cuda-npp-dev-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:22 file:/var/cuda-repo-9-0-local  cuda-nvgraph-9-0 9.0.176-1 [6,081 kB]\n",
            "Get:23 file:/var/cuda-repo-9-0-local  cuda-nvgraph-dev-9-0 9.0.176-1 [5,658 kB]\n",
            "Get:24 file:/var/cuda-repo-9-0-local  cuda-samples-9-0 9.0.176-1 [75.9 MB]\n",
            "Get:25 file:/var/cuda-repo-9-0-local  cuda-documentation-9-0 9.0.176-1 [53.1 MB]\n",
            "Get:26 file:/var/cuda-repo-9-0-local  cuda-libraries-dev-9-0 9.0.176-1 [2,596 B]\n",
            "Get:27 file:/var/cuda-repo-9-0-local  cuda-nvml-dev-9-0 9.0.176-1 [47.6 kB]\n",
            "Get:28 file:/var/cuda-repo-9-0-local  cuda-visual-tools-9-0 9.0.176-1 [398 MB]\n",
            "Get:29 file:/var/cuda-repo-9-0-local  cuda-toolkit-9-0 9.0.176-1 [2,836 B]\n",
            "Get:30 file:/var/cuda-repo-9-0-local  cuda-libraries-9-0 9.0.176-1 [2,566 B]\n",
            "Get:31 file:/var/cuda-repo-9-0-local  cuda-runtime-9-0 9.0.176-1 [2,526 B]\n",
            "Get:32 file:/var/cuda-repo-9-0-local  cuda-demo-suite-9-0 9.0.176-1 [3,880 kB]\n",
            "Get:33 file:/var/cuda-repo-9-0-local  cuda-9-0 9.0.176-1 [2,552 B]\n",
            "Get:34 file:/var/cuda-repo-9-0-local  cuda 9.0.176-1 [2,504 B]\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-license-9-0.\n",
            "(Reading database ... 145177 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-license-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-9-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-core-9-0.\n",
            "Preparing to unpack .../02-cuda-core-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-core-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-9-0.\n",
            "Preparing to unpack .../03-cuda-cudart-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-9-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-9-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-9-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-9-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-9-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-9-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-9-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-9-0.\n",
            "Preparing to unpack .../11-cuda-cublas-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-9-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-9-0.\n",
            "Preparing to unpack .../13-cuda-cufft-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-9-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-9-0.\n",
            "Preparing to unpack .../15-cuda-curand-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-9-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-9-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-9-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-9-0.\n",
            "Preparing to unpack .../19-cuda-npp-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-9-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-9-0.\n",
            "Preparing to unpack .../21-cuda-nvgraph-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-9-0.\n",
            "Preparing to unpack .../22-cuda-nvgraph-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-samples-9-0.\n",
            "Preparing to unpack .../23-cuda-samples-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-samples-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-documentation-9-0.\n",
            "Preparing to unpack .../24-cuda-documentation-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-9-0.\n",
            "Preparing to unpack .../25-cuda-libraries-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-9-0.\n",
            "Preparing to unpack .../26-cuda-nvml-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-9-0.\n",
            "Preparing to unpack .../27-cuda-visual-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-9-0.\n",
            "Preparing to unpack .../28-cuda-toolkit-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-9-0.\n",
            "Preparing to unpack .../29-cuda-libraries-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-runtime-9-0.\n",
            "Preparing to unpack .../30-cuda-runtime-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-9-0.\n",
            "Preparing to unpack .../31-cuda-demo-suite-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-9-0.\n",
            "Preparing to unpack .../32-cuda-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda.\n",
            "Preparing to unpack .../33-cuda_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda (9.0.176-1) ...\n",
            "Setting up cuda-license-9-0 (9.0.176-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-9.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-core-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-samples-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-9-0 (9.0.176-1) ...\n",
            "Setting up cuda (9.0.176-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejWdzNrl5IPe",
        "colab_type": "code",
        "outputId": "9fd1b7c8-90a3-415e-da48-3836b2f804a8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myXuKPN25x_t",
        "colab_type": "code",
        "outputId": "1532e275-0ed5-47cf-8e65-f15eeb3d0cc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# copy dataset into working directory\n",
        "%cd /content\n",
        "!cp -r '/content/drive/My Drive/imdb' '/content/yesweGAN/maskgan_colab/dataset'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFfWXj1wuYdY",
        "colab_type": "code",
        "outputId": "c37a4c83-acbe-4ad5-89c4-c740599edede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# copy most recent checkpoint into working directory to continue training\n",
        "%cd /content\n",
        "!cp -r '/content/drive/My Drive/imdb mle checkpoint/train' '/content/yesweGAN/maskgan_colab/maskGAN'"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6_G3THLoUoi",
        "colab_type": "text"
      },
      "source": [
        "# Must be using python 2 runtime w GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMz5ZNbql-EW",
        "colab_type": "text"
      },
      "source": [
        "## Run MaskGAN in MLE pretraining mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "03b02743-e7e0-4d17-bab4-d64f2d91de9a",
        "id": "3mV31eblvixR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/yesweGAN/maskgan_colab\n",
        "! python train_mask_gan.py \\\n",
        " --data_dir='dataset/imdb' \\\n",
        " --data_set='imdb' \\\n",
        " --batch_size=128 \\\n",
        " --sequence_length=20 \\\n",
        " --base_directory='maskGAN' \\\n",
        " --hparams=\"gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,dis_num_layers=2,gen_learning_rate=0.00074876,dis_learning_rate=5e-4,baseline_decay=0.99,dis_train_iterations=1,gen_learning_rate_decay=0.95\" \\\n",
        " --mode='TRAIN' \\\n",
        " --max_steps=10000 \\\n",
        " --perplexity_threshold=1000000 \\\n",
        " --generator_model='seq2seq_vd' \\\n",
        " --discriminator_model='seq2seq_vd' \\\n",
        " --is_present_rate=0.5 \\\n",
        " --summaries_every=50 \\\n",
        " --print_every=250 \\\n",
        " --max_num_to_print=3 \\\n",
        " --gen_training_strategy=cross_entropy \\\n",
        " --seq2seq_share_embedding=true \\\n",
        " --baseline_method=critic \\\n",
        " --attention_option=luong"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yesweGAN/maskgan_colab\n",
            "Unique 2-grams: 1300789\n",
            "Unique 3-grams: 3512054\n",
            "Unique 4-grams: 4986879\n",
            "Vocab size: 69849\n",
            "Training model.\n",
            "Optimizing Generator vars.\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69849, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69849,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "\n",
            "Optimizing Discriminator vars:\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(69849, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Optimizing Critic vars:\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Trainable Variables in Graph:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69849, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69849,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(69849, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "WARNING:tensorflow:From train_mask_gan.py:547: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2020-03-11 03:53:24.986013: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-03-11 03:53:25.055410: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-11 03:53:25.056104: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
            "2020-03-11 03:53:25.056146: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n",
            "2020-03-11 03:53:25.337844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10769 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3]...][[0 1 1 0 1 1 1 1 0 1]...][[8 69849 1922 8689 69849 120 26 5755 1110 69849]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3]...][[0 1 1 0 1 1 1 1 0 1]...][[8 69849 1922 8689 69849 120 26 5755 1110 69849]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685]...]\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4]...][[0 1 0 1 0 1 0 0 0 1]...][[10 69849 13 69849 379 69849 9 69849 69849 69849]...][[17 13 37 379 11 9 307 55 4 2911]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4]...][[0 1 0 1 0 1 0 0 0 1]...][[10 69849 13 69849 379 69849 9 69849 69849 69849]...][[17 13 37 379 11 9 307 55 4 2911]...]\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            "global_step: 3\n",
            " perplexity: 69824.945\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            " percent of 3-grams captured: 0.503.\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            " percent of 2-grams captured: 0.713.\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            " percent of 4-grams captured: 0.244.\n",
            " geometric_avg: 0.444.\n",
            " arithmetic_avg: 0.487.\n",
            "global_step: 3\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69284\n",
            " G train loss: 5.72065\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4]...][[0 1 0 1 0 1 0 0 0 1]...][[10 69849 13 69849 379 69849 9 69849 69849 69849]...][[17 13 37 379 11 9 307 55 4 2911]...]\n",
            " Sample 0.\n",
            "   [0]  movie                 0.501   11.150 \n",
            "   [1]  was                   0.501   0.000  \n",
            "   [0]  so                    0.501   11.153 \n",
            "   [1]  awful                 0.501   0.000  \n",
            "   [0]  that                  0.501   11.158 \n",
            "   [1]  i                     0.502   0.000  \n",
            "   [0]  watched               0.502   11.154 \n",
            "   [0]  up                    0.502   11.147 \n",
            "   [0]  to                    0.502   11.145 \n",
            "   [1]  halfway               0.503   0.000  \n",
            "   [0]  through               0.503   11.157 \n",
            "   [1]  waiting               0.503   0.000  \n",
            "   [1]  for                   0.503   0.000  \n",
            "   [0]  it                    0.503   11.145 \n",
            "   [1]  to                    0.503   0.000  \n",
            "   [0]  start                 0.503   11.149 \n",
            "   [0]  for                   0.503   11.152 \n",
            "   [0]  the                   0.503   11.146 \n",
            "   [0]  plot                  0.503   11.158 \n",
            "   [1]  characters            0.503   0.000  \n",
            " Sample 1.\n",
            "   [1]  s                     0.506   0.000  \n",
            "   [1]  easy                  0.506   0.000  \n",
            "   [0]  to                    0.506   11.148 \n",
            "   [0]  see                   0.506   11.159 \n",
            "   [0]  that                  0.506   11.157 \n",
            "   [0]  risa                  0.506   11.153 \n",
            "   [1]  garcia                0.506   0.000  \n",
            "   [1]  is                    0.506   0.000  \n",
            "   [0]  a                     0.505   11.147 \n",
            "   [0]  successful            0.505   11.155 \n",
            "   [1]  casting               0.505   0.000  \n",
            "   [0]  director              0.505   11.155 \n",
            "   [0]  and                   0.505   11.154 \n",
            "   [0]  not                   0.505   11.153 \n",
            "   [0]  a                     0.505   11.147 \n",
            "   [1]  screenwriter          0.505   0.000  \n",
            "   [1]  for                   0.505   0.000  \n",
            "   [1]  dozens                0.505   0.000  \n",
            "   [1]  of                    0.505   0.000  \n",
            "   [0]  hollywood             0.505   11.153 \n",
            " Sample 2.\n",
            "   [0]  movie                 0.500   11.148 \n",
            "   [0]  is                    0.500   11.150 \n",
            "   [1]  ok                    0.501   0.000  \n",
            "   [1]  bogdanovich           0.501   0.000  \n",
            "   [0]  knows                 0.502   11.157 \n",
            "   [0]  how                   0.502   11.158 \n",
            "   [1]  to                    0.502   0.000  \n",
            "   [1]  direct                0.502   0.000  \n",
            "   [1]  a                     0.502   0.000  \n",
            "   [1]  thriller              0.502   0.000  \n",
            "   [0]  although              0.502   11.146 \n",
            "   [0]  his                   0.502   11.158 \n",
            "   [0]  best                  0.502   11.156 \n",
            "   [1]  work                  0.502   0.000  \n",
            "   [1]  was                   0.502   0.000  \n",
            "   [0]  noises                0.502   11.150 \n",
            "   [1]  off                   0.501   0.000  \n",
            "   [1]  scott                 0.501   0.000  \n",
            "   [0]  glenn                 0.502   11.150 \n",
            "   [0]  is                    0.502   11.150 \n",
            "Samples\n",
            "Sample 0 .  movie was so awful that i watched up to halfway through waiting for it to start for the plot characters\n",
            "Sample 1 .  s easy to see that risa garcia is a successful casting director and not a screenwriter for dozens of hollywood\n",
            "Sample 2 .  movie is ok bogdanovich knows how to direct a thriller although his best work was noises off scott glenn is\n",
            "\n",
            "\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12]...][[0 1 0 0 0 1 0 0 0 0]...][[577 69849 12 69849 69849 69849 81 69849 69849 69849]...][[5337 12 361 5 2 81 354 39 12 5]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12]...][[0 1 0 0 0 1 0 0 0 0]...][[577 69849 12 69849 69849 69849 81 69849 69849 69849]...][[5337 12 361 5 2 81 354 39 12 5]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3]...][[1 1 0 1 1 0 1 0 1 0]...][[8 23354 1922 69849 58348 120 69849 5755 69849 3]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3]...][[1 1 0 1 1 0 1 0 1 0]...][[8 23354 1922 69849 58348 120 69849 5755 69849 3]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            "global_step: 6\n",
            " perplexity: 69766.304\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            " percent of 3-grams captured: 0.498.\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            " percent of 2-grams captured: 0.703.\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            " percent of 4-grams captured: 0.231.\n",
            " geometric_avg: 0.432.\n",
            " arithmetic_avg: 0.477.\n",
            "global_step: 6\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69287\n",
            " G train loss: 5.62657\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3]...][[1 1 0 1 1 0 1 0 1 0]...][[8 23354 1922 69849 58348 120 69849 5755 69849 3]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685]...]\n",
            " Sample 0.\n",
            "   [1]  pushover              0.494   0.000  \n",
            "   [1]  fred                  0.495   0.000  \n",
            "   [0]  macmurray             0.496   11.145 \n",
            "   [1]  dusts                 0.497   0.000  \n",
            "   [1]  off                   0.497   0.000  \n",
            "   [0]  his                   0.498   11.154 \n",
            "   [1]  acclaimed             0.498   0.000  \n",
            "   [0]  portrayal             0.498   11.151 \n",
            "   [1]  of                    0.498   0.000  \n",
            "   [0]  walter                0.498   11.158 \n",
            "   [1]  neff                  0.498   0.000  \n",
            "   [0]  the                   0.499   11.139 \n",
            "   [0]  luckless              0.498   11.148 \n",
            "   [1]  insurance             0.498   0.000  \n",
            "   [0]  agent                 0.498   11.153 \n",
            "   [0]  from                  0.498   11.146 \n",
            "   [0]  double                0.498   11.155 \n",
            "   [1]  indemnity             0.498   0.000  \n",
            "   [0]  and                   0.498   11.150 \n",
            "   [1]  gives                 0.499   0.000  \n",
            " Sample 1.\n",
            "   [1]  a                     0.491   0.000  \n",
            "   [0]  nutshell              0.492   11.152 \n",
            "   [0]  so                    0.493   11.144 \n",
            "   [1]  that                  0.493   0.000  \n",
            "   [0]  you                   0.493   11.149 \n",
            "   [0]  don                   0.493   11.151 \n",
            "   [0]  t                     0.493   11.138 \n",
            "   [1]  have                  0.493   0.000  \n",
            "   [1]  to                    0.493   0.000  \n",
            "   [0]  read                  0.493   11.141 \n",
            "   [0]  the                   0.493   11.132 \n",
            "   [0]  rest                  0.493   11.151 \n",
            "   [0]  of                    0.493   11.145 \n",
            "   [1]  my                    0.493   0.000  \n",
            "   [0]  crap                  0.493   11.146 \n",
            "   [1]  if                    0.493   0.000  \n",
            "   [1]  you                   0.493   0.000  \n",
            "   [1]  don                   0.493   0.000  \n",
            "   [0]  t                     0.493   11.139 \n",
            "   [0]  want                  0.493   11.155 \n",
            " Sample 2.\n",
            "   [1]  povich                0.498   0.000  \n",
            "   [1]  or                    0.498   0.000  \n",
            "   [0]  however               0.498   11.153 \n",
            "   [1]  you                   0.498   0.000  \n",
            "   [1]  spell                 0.498   0.000  \n",
            "   [0]  his                   0.498   11.155 \n",
            "   [0]  name                  0.499   11.143 \n",
            "   [1]  is                    0.499   0.000  \n",
            "   [1]  a                     0.498   0.000  \n",
            "   [1]  hypocrite             0.498   0.000  \n",
            "   [0]  because               0.498   11.154 \n",
            "   [1]  his                   0.498   0.000  \n",
            "   [1]  show                  0.498   0.000  \n",
            "   [0]  sends                 0.498   11.154 \n",
            "   [1]  opposing              0.498   0.000  \n",
            "   [1]  messages              0.498   0.000  \n",
            "   [1]  to                    0.498   0.000  \n",
            "   [0]  women                 0.498   11.152 \n",
            "   [1]  on                    0.498   0.000  \n",
            "   [1]  one                   0.498   0.000  \n",
            "Samples\n",
            "Sample 0 .  pushover fred macmurray dusts off his acclaimed portrayal of walter neff the luckless insurance agent from double indemnity and gives\n",
            "Sample 1 .  a nutshell so that you don t have to read the rest of my crap if you don t want\n",
            "Sample 2 .  povich or however you spell his name is a hypocrite because his show sends opposing messages to women on one\n",
            "\n",
            "\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359]...][[1 1 0 0 1 0 1 1 0 1]...][[16 1803 34 69849 69849 0 69849 206 15 69849]...][[1803 34 45 110 0 703 206 15 5359 751]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359]...][[1 1 0 0 1 0 1 1 0 1]...][[16 1803 34 69849 69849 0 69849 206 15 69849]...][[1803 34 45 110 0 703 206 15 5359 751]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12]...][[0 0 1 1 0 1 1 1 1 1]...][[577 69849 69849 361 5 69849 81 354 39 12]...][[5337 12 361 5 2 81 354 39 12 5]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12]...][[0 0 1 1 0 1 1 1 1 1]...][[577 69849 69849 361 5 69849 81 354 39 12]...][[5337 12 361 5 2 81 354 39 12 5]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            "global_step: 9\n",
            " perplexity: 69672.443\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            " percent of 3-grams captured: 0.531.\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            " percent of 2-grams captured: 0.711.\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            " percent of 4-grams captured: 0.251.\n",
            " geometric_avg: 0.456.\n",
            " arithmetic_avg: 0.498.\n",
            "global_step: 9\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69307\n",
            " G train loss: 5.60193\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12]...][[0 0 1 1 0 1 1 1 1 1]...][[577 69849 69849 361 5 69849 81 354 39 12]...][[5337 12 361 5 2 81 354 39 12 5]...]\n",
            " Sample 0.\n",
            "   [0]  cronenberg            0.499   11.154 \n",
            "   [0]  s                     0.499   11.126 \n",
            "   [1]  camera                0.498   0.000  \n",
            "   [1]  is                    0.497   0.000  \n",
            "   [0]  a                     0.497   11.125 \n",
            "   [1]  great                 0.496   0.000  \n",
            "   [1]  short                 0.496   0.000  \n",
            "   [1]  there                 0.496   0.000  \n",
            "   [1]  s                     0.496   0.000  \n",
            "   [1]  is                    0.496   0.000  \n",
            "   [0]  only                  0.496   11.150 \n",
            "   [1]  2                     0.496   0.000  \n",
            "   [1]  people                0.496   0.000  \n",
            "   [0]  in                    0.496   11.122 \n",
            "   [0]  the                   0.496   11.112 \n",
            "   [0]  film                  0.496   11.136 \n",
            "   [1]  witch                 0.496   0.000  \n",
            "   [0]  s                     0.496   11.131 \n",
            "   [0]  makes                 0.496   11.140 \n",
            "   [1]  it                    0.496   0.000  \n",
            " Sample 1.\n",
            "   [1]  have                  0.497   0.000  \n",
            "   [0]  watched               0.498   11.139 \n",
            "   [1]  this                  0.498   0.000  \n",
            "   [1]  film                  0.498   0.000  \n",
            "   [1]  several               0.499   0.000  \n",
            "   [1]  times                 0.499   0.000  \n",
            "   [0]  and                   0.499   11.132 \n",
            "   [0]  aided                 0.499   11.145 \n",
            "   [0]  by                    0.499   11.138 \n",
            "   [0]  a                     0.499   11.112 \n",
            "   [1]  gap                   0.499   0.000  \n",
            "   [1]  of                    0.499   0.000  \n",
            "   [1]  a                     0.499   0.000  \n",
            "   [0]  few                   0.499   11.150 \n",
            "   [0]  years                 0.499   11.141 \n",
            "   [0]  between               0.499   11.139 \n",
            "   [0]  viewings              0.499   11.159 \n",
            "   [0]  and                   0.499   11.132 \n",
            "   [0]  a                     0.499   11.112 \n",
            "   [0]  bad                   0.499   11.135 \n",
            " Sample 2.\n",
            "   [1]  movie                 0.505   0.000  \n",
            "   [1]  really                0.505   0.000  \n",
            "   [0]  sucked                0.505   11.154 \n",
            "   [1]  from                  0.505   0.000  \n",
            "   [0]  the                   0.505   11.120 \n",
            "   [1]  police                0.505   0.000  \n",
            "   [1]  response              0.505   0.000  \n",
            "   [1]  to                    0.505   0.000  \n",
            "   [0]  the                   0.505   11.120 \n",
            "   [0]  bitter                0.505   11.154 \n",
            "   [1]  end                   0.505   0.000  \n",
            "   [0]  it                    0.505   11.129 \n",
            "   [1]  was                   0.505   0.000  \n",
            "   [1]  really                0.505   0.000  \n",
            "   [1]  bad                   0.504   0.000  \n",
            "   [1]  i                     0.504   0.000  \n",
            "   [1]  can                   0.504   0.000  \n",
            "   [1]  t                     0.504   0.000  \n",
            "   [1]  believe               0.504   0.000  \n",
            "   [0]  i                     0.504   11.139 \n",
            "Samples\n",
            "Sample 0 .  cronenberg s camera is a great short there s is only 2 people in the film witch s makes it\n",
            "Sample 1 .  have watched this film several times and aided by a gap of a few years between viewings and a bad\n",
            "Sample 2 .  movie really sucked from the police response to the bitter end it was really bad i can t believe i\n",
            "\n",
            "\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20]...][[0 1 1 1 0 1 0 0 0 1]...][[7 69849 23 174 19 69849 301 69849 69849 69849]...][[5 23 174 19 12 301 4 22800 20 15795]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20]...][[0 1 1 1 0 1 0 0 0 1]...][[7 69849 23 174 19 69849 301 69849 69849 69849]...][[5 23 174 19 12 301 4 22800 20 15795]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359]...][[1 1 0 0 1 0 0 0 0 1]...][[16 1803 34 69849 69849 0 69849 69849 69849 69849]...][[1803 34 45 110 0 703 206 15 5359 751]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359]...][[1 1 0 0 1 0 0 0 0 1]...][[16 1803 34 69849 69849 0 69849 69849 69849 69849]...][[1803 34 45 110 0 703 206 15 5359 751]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            "global_step: 12\n",
            " perplexity: 69462.837\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            " percent of 3-grams captured: 0.518.\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            " percent of 2-grams captured: 0.711.\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            " percent of 4-grams captured: 0.245.\n",
            " geometric_avg: 0.449.\n",
            " arithmetic_avg: 0.492.\n",
            "global_step: 12\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69315\n",
            " G train loss: 5.59281\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359]...][[1 1 0 0 1 0 0 0 0 1]...][[16 1803 34 69849 69849 0 69849 69849 69849 69849]...][[1803 34 45 110 0 703 206 15 5359 751]...]\n",
            " Sample 0.\n",
            "   [1]  somebody              0.500   0.000  \n",
            "   [1]  who                   0.499   0.000  \n",
            "   [0]  has                   0.498   11.126 \n",
            "   [0]  seen                  0.498   11.093 \n",
            "   [1]  the                   0.498   0.000  \n",
            "   [0]  french                0.498   11.115 \n",
            "   [0]  original              0.498   11.094 \n",
            "   [0]  with                  0.498   11.091 \n",
            "   [0]  pierre                0.498   11.116 \n",
            "   [1]  richard               0.499   0.000  \n",
            "   [0]  and                   0.499   11.059 \n",
            "   [1]  gerard                0.499   0.000  \n",
            "   [0]  depardieu             0.499   11.155 \n",
            "   [1]  this                  0.499   0.000  \n",
            "   [0]  is                    0.499   11.047 \n",
            "   [1]  the                   0.499   0.000  \n",
            "   [0]  worst                 0.499   11.107 \n",
            "   [0]  remake                0.499   11.149 \n",
            "   [0]  of                    0.499   11.057 \n",
            "   [0]  a                     0.499   11.051 \n",
            " Sample 1.\n",
            "   [0]  has                   0.488   11.110 \n",
            "   [0]  to                    0.490   11.032 \n",
            "   [1]  be                    0.491   0.000  \n",
            "   [0]  one                   0.491   11.067 \n",
            "   [0]  of                    0.491   11.015 \n",
            "   [0]  my                    0.491   11.084 \n",
            "   [0]  all                   0.492   11.103 \n",
            "   [0]  time                  0.492   11.093 \n",
            "   [0]  greats                0.492   11.141 \n",
            "   [1]  despite               0.492   0.000  \n",
            "   [0]  its                   0.492   11.098 \n",
            "   [0]  cheekiness            0.492   11.153 \n",
            "   [0]  and                   0.492   11.044 \n",
            "   [1]  poor                  0.492   0.000  \n",
            "   [0]  model                 0.492   11.130 \n",
            "   [1]  reproductions         0.492   0.000  \n",
            "   [1]  of                    0.492   0.000  \n",
            "   [0]  oil                   0.492   11.144 \n",
            "   [0]  rigs                  0.492   11.159 \n",
            "   [0]  its                   0.492   11.099 \n",
            " Sample 2.\n",
            "   [0]  there                 0.503   11.123 \n",
            "   [1]  anyone                0.502   0.000  \n",
            "   [0]  else                  0.501   11.131 \n",
            "   [0]  out                   0.501   11.092 \n",
            "   [1]  there                 0.500   0.000  \n",
            "   [0]  who                   0.500   11.102 \n",
            "   [1]  thinks                0.500   0.000  \n",
            "   [1]  that                  0.501   0.000  \n",
            "   [0]  all                   0.501   11.118 \n",
            "   [1]  the                   0.501   0.000  \n",
            "   [1]  wrong                 0.501   0.000  \n",
            "   [0]  characters            0.501   11.121 \n",
            "   [0]  ended                 0.501   11.168 \n",
            "   [0]  up                    0.501   11.114 \n",
            "   [0]  in                    0.501   11.069 \n",
            "   [1]  the                   0.501   0.000  \n",
            "   [0]  wrong                 0.501   11.106 \n",
            "   [1]  spots                 0.501   0.000  \n",
            "   [0]  br                    0.501   11.068 \n",
            "   [1]  br                    0.501   0.000  \n",
            "Samples\n",
            "Sample 0 .  somebody who has seen the french original with pierre richard and gerard depardieu this is the worst remake of a\n",
            "Sample 1 .  has to be one of my all time greats despite its cheekiness and poor model reproductions of oil rigs its\n",
            "Sample 2 .  there anyone else out there who thinks that all the wrong characters ended up in the wrong spots br br\n",
            "\n",
            "\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431]...][[1 1 0 0 0 1 0 0 1 0]...][[10 141 27 69849 69849 69849 126 69849 69849 2431]...][[141 27 77 126 73 126 2 1023 2431 2347]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431]...][[1 1 0 0 0 1 0 0 1 0]...][[10 141 27 69849 69849 69849 126 69849 69849 2431]...][[141 27 77 126 73 126 2 1023 2431 2347]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20]...][[0 1 0 0 1 1 0 0 0 0]...][[7 69849 23 69849 69849 12 301 69849 69849 69849]...][[5 23 174 19 12 301 4 22800 20 15795]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20]...][[0 1 0 0 1 1 0 0 0 0]...][[7 69849 23 69849 69849 12 301 69849 69849 69849]...][[5 23 174 19 12 301 4 22800 20 15795]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            "global_step: 15\n",
            " perplexity: 68906.079\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            " percent of 3-grams captured: 0.516.\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            " percent of 2-grams captured: 0.722.\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            " percent of 4-grams captured: 0.245.\n",
            " geometric_avg: 0.451.\n",
            " arithmetic_avg: 0.495.\n",
            "global_step: 15\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69326\n",
            " G train loss: 5.56941\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20]...][[0 1 0 0 1 1 0 0 0 0]...][[7 69849 23 69849 69849 12 301 69849 69849 69849]...][[5 23 174 19 12 301 4 22800 20 15795]...]\n",
            " Sample 0.\n",
            "   [0]  is                    0.490   10.638 \n",
            "   [1]  not                   0.490   0.000  \n",
            "   [0]  every                 0.489   11.112 \n",
            "   [0]  film                  0.488   10.880 \n",
            "   [1]  s                     0.488   0.000  \n",
            "   [1]  job                   0.487   0.000  \n",
            "   [0]  to                    0.487   10.814 \n",
            "   [0]  stimulate             0.487   11.061 \n",
            "   [0]  you                   0.487   10.952 \n",
            "   [0]  superficially         0.487   11.102 \n",
            "   [0]  i                     0.487   10.815 \n",
            "   [0]  will                  0.487   10.891 \n",
            "   [0]  take                  0.486   10.969 \n",
            "   [1]  an                    0.486   0.000  \n",
            "   [1]  ambitious             0.487   0.000  \n",
            "   [0]  failure               0.487   11.159 \n",
            "   [0]  over                  0.487   11.004 \n",
            "   [0]  a                     0.486   10.815 \n",
            "   [0]  mass                  0.486   11.146 \n",
            "   [1]  market                0.486   0.000  \n",
            " Sample 1.\n",
            "   [0]  this                  0.497   10.770 \n",
            "   [1]  wasn                  0.498   0.000  \n",
            "   [0]  t                     0.498   10.830 \n",
            "   [1]  the                   0.498   0.000  \n",
            "   [0]  worst                 0.498   10.980 \n",
            "   [0]  movie                 0.497   10.838 \n",
            "   [1]  i                     0.497   0.000  \n",
            "   [1]  ve                    0.497   0.000  \n",
            "   [0]  ever                  0.497   11.009 \n",
            "   [0]  seen                  0.497   10.963 \n",
            "   [0]  but                   0.497   10.902 \n",
            "   [0]  i                     0.497   10.887 \n",
            "   [1]  had                   0.497   0.000  \n",
            "   [0]  heard                 0.497   11.063 \n",
            "   [1]  lots                  0.497   0.000  \n",
            "   [0]  of                    0.497   10.831 \n",
            "   [0]  good                  0.497   11.011 \n",
            "   [1]  things                0.497   0.000  \n",
            "   [1]  about                 0.497   0.000  \n",
            "   [0]  it                    0.497   10.835 \n",
            " Sample 2.\n",
            "   [1]  movie                 0.495   0.000  \n",
            "   [1]  was                   0.496   0.000  \n",
            "   [0]  lacking               0.496   11.121 \n",
            "   [0]  a                     0.496   10.957 \n",
            "   [1]  great                 0.496   0.000  \n",
            "   [1]  script                0.496   0.000  \n",
            "   [1]  and                   0.496   0.000  \n",
            "   [0]  the                   0.496   10.942 \n",
            "   [1]  touch                 0.496   0.000  \n",
            "   [0]  of                    0.496   10.993 \n",
            "   [0]  realism               0.495   11.119 \n",
            "   [1]  the                   0.495   0.000  \n",
            "   [1]  dialogue              0.496   0.000  \n",
            "   [1]  was                   0.496   0.000  \n",
            "   [0]  muddled               0.496   11.147 \n",
            "   [1]  confusing             0.496   0.000  \n",
            "   [1]  and                   0.496   0.000  \n",
            "   [1]  trying                0.496   0.000  \n",
            "   [1]  really                0.496   0.000  \n",
            "   [1]  hard                  0.496   0.000  \n",
            "Samples\n",
            "Sample 0 .  is not every film s job to stimulate you superficially i will take an ambitious failure over a mass market\n",
            "Sample 1 .  this wasn t the worst movie i ve ever seen but i had heard lots of good things about it\n",
            "Sample 2 .  movie was lacking a great script and the touch of realism the dialogue was muddled confusing and trying really hard\n",
            "\n",
            "\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075]...][[1 0 1 1 1 0 1 1 0 1]...][[10 17 69849 634 2 181 69849 31 91 69849]...][[17 5 634 2 181 17 31 91 2075 7]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075]...][[1 0 1 1 1 0 1 1 0 1]...][[10 17 69849 634 2 181 69849 31 91 69849]...][[17 5 634 2 181 17 31 91 2075 7]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431]...][[0 1 1 0 1 1 0 0 1 0]...][[10 69849 27 77 69849 73 126 69849 69849 2431]...][[141 27 77 126 73 126 2 1023 2431 2347]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431]...][[0 1 1 0 1 1 0 0 1 0]...][[10 69849 27 77 69849 73 126 69849 69849 2431]...][[141 27 77 126 73 126 2 1023 2431 2347]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            "global_step: 18\n",
            " perplexity: 67362.973\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            " percent of 3-grams captured: 0.498.\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            " percent of 2-grams captured: 0.702.\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            " percent of 4-grams captured: 0.239.\n",
            " geometric_avg: 0.437.\n",
            " arithmetic_avg: 0.480.\n",
            "global_step: 18\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69331\n",
            " G train loss: 5.54169\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431]...][[0 1 1 0 1 1 0 0 1 0]...][[10 69849 27 77 69849 73 126 69849 69849 2431]...][[141 27 77 126 73 126 2 1023 2431 2347]...]\n",
            " Sample 0.\n",
            "   [0]  should                0.508   10.775 \n",
            "   [1]  have                  0.508   0.000  \n",
            "   [1]  been                  0.508   0.000  \n",
            "   [0]  better                0.508   10.581 \n",
            "   [1]  much                  0.508   0.000  \n",
            "   [1]  better                0.507   0.000  \n",
            "   [0]  a                     0.507   9.843  \n",
            "   [0]  former                0.508   10.924 \n",
            "   [1]  reporter              0.508   0.000  \n",
            "   [0]  susan                 0.508   11.081 \n",
            "   [1]  sarandon              0.508   0.000  \n",
            "   [1]  investigates          0.507   0.000  \n",
            "   [0]  the                   0.507   10.020 \n",
            "   [1]  murder                0.508   0.000  \n",
            "   [1]  of                    0.507   0.000  \n",
            "   [0]  a                     0.507   10.179 \n",
            "   [0]  philandering          0.507   11.079 \n",
            "   [0]  dentist               0.507   11.084 \n",
            "   [1]  and                   0.507   0.000  \n",
            "   [0]  the                   0.507   10.106 \n",
            " Sample 1.\n",
            "   [1]  still                 0.510   0.000  \n",
            "   [0]  waters                0.510   11.053 \n",
            "   [0]  is                    0.509   9.814  \n",
            "   [0]  nt                    0.508   11.189 \n",
            "   [1]  comparable            0.508   0.000  \n",
            "   [1]  to                    0.507   0.000  \n",
            "   [1]  some                  0.507   0.000  \n",
            "   [1]  of                    0.506   0.000  \n",
            "   [0]  the                   0.506   10.111 \n",
            "   [1]  masterpieces          0.506   0.000  \n",
            "   [1]  in                    0.506   0.000  \n",
            "   [1]  which                 0.506   0.000  \n",
            "   [1]  yuzna                 0.506   0.000  \n",
            "   [0]  has                   0.506   10.682 \n",
            "   [0]  been                  0.506   10.761 \n",
            "   [0]  involved              0.506   10.961 \n",
            "   [0]  as                    0.506   10.653 \n",
            "   [1]  a                     0.506   0.000  \n",
            "   [0]  director              0.506   10.792 \n",
            "   [0]  society               0.506   11.099 \n",
            " Sample 2.\n",
            "   [1]  someone               0.498   0.000  \n",
            "   [1]  who                   0.499   0.000  \n",
            "   [0]  has                   0.499   10.693 \n",
            "   [0]  been                  0.499   10.697 \n",
            "   [0]  dancing               0.499   10.994 \n",
            "   [1]  all                   0.499   0.000  \n",
            "   [0]  her                   0.499   10.839 \n",
            "   [1]  life                  0.499   0.000  \n",
            "   [1]  and                   0.499   0.000  \n",
            "   [1]  is                    0.499   0.000  \n",
            "   [1]  now                   0.499   0.000  \n",
            "   [1]  a                     0.499   0.000  \n",
            "   [1]  dance                 0.499   0.000  \n",
            "   [0]  teacher               0.499   11.007 \n",
            "   [0]  and                   0.499   10.552 \n",
            "   [1]  a                     0.499   0.000  \n",
            "   [0]  writer                0.499   10.907 \n",
            "   [1]  for                   0.499   0.000  \n",
            "   [1]  a                     0.499   0.000  \n",
            "   [1]  dance                 0.499   0.000  \n",
            "Samples\n",
            "Sample 0 .  should have been better much better a former reporter susan sarandon investigates the murder of a philandering dentist and the\n",
            "Sample 1 .  still waters is nt comparable to some of the masterpieces in which yuzna has been involved as a director society\n",
            "Sample 2 .  someone who has been dancing all her life and is now a dance teacher and a writer for a dance\n",
            "\n",
            "\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98]...][[1 0 1 1 0 0 0 1 0 0]...][[10 5 69849 1563 120 69849 69849 69849 338 69849]...][[5 2 1563 120 3 478 2124 338 98 38]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98]...][[1 0 1 1 0 0 0 1 0 0]...][[10 5 69849 1563 120 69849 69849 69849 338 69849]...][[5 2 1563 120 3 478 2124 338 98 38]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075]...][[1 0 0 0 0 0 0 0 1 0]...][[10 17 69849 69849 69849 69849 69849 69849 69849 2075]...][[17 5 634 2 181 17 31 91 2075 7]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075]...][[1 0 0 0 0 0 0 0 1 0]...][[10 17 69849 69849 69849 69849 69849 69849 69849 2075]...][[17 5 634 2 181 17 31 91 2075 7]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            "global_step: 21\n",
            " perplexity: 61445.977\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            " percent of 3-grams captured: 0.490.\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            " percent of 2-grams captured: 0.694.\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            " percent of 4-grams captured: 0.222.\n",
            " geometric_avg: 0.423.\n",
            " arithmetic_avg: 0.469.\n",
            "global_step: 21\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69334\n",
            " G train loss: 5.49592\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075]...][[1 0 0 0 0 0 0 0 1 0]...][[10 17 69849 69849 69849 69849 69849 69849 69849 2075]...][[17 5 634 2 181 17 31 91 2075 7]...]\n",
            " Sample 0.\n",
            "   [1]  movie                 0.523   0.000  \n",
            "   [0]  is                    0.522   6.008  \n",
            "   [0]  basically             0.521   9.307  \n",
            "   [0]  a                     0.520   5.924  \n",
            "   [0]  action                0.519   9.034  \n",
            "   [0]  movie                 0.519   6.762  \n",
            "   [0]  at                    0.519   8.028  \n",
            "   [0]  its                   0.518   8.914  \n",
            "   [1]  finest                0.518   0.000  \n",
            "   [0]  it                    0.518   7.289  \n",
            "   [0]  severally             0.518   10.705 \n",
            "   [0]  lacks                 0.518   11.003 \n",
            "   [0]  depth                 0.518   10.869 \n",
            "   [0]  and                   0.518   6.603  \n",
            "   [0]  story                 0.518   8.245  \n",
            "   [0]  but                   0.518   8.050  \n",
            "   [1]  compensates           0.518   0.000  \n",
            "   [0]  that                  0.518   7.682  \n",
            "   [1]  with                  0.518   0.000  \n",
            "   [1]  its                   0.518   0.000  \n",
            " Sample 1.\n",
            "   [0]  guess                 0.493   10.487 \n",
            "   [0]  i                     0.495   7.065  \n",
            "   [0]  can                   0.495   8.293  \n",
            "   [1]  see                   0.494   0.000  \n",
            "   [1]  the                   0.494   0.000  \n",
            "   [0]  point                 0.494   9.960  \n",
            "   [0]  this                  0.493   7.379  \n",
            "   [0]  guy                   0.493   9.894  \n",
            "   [0]  was                   0.493   7.917  \n",
            "   [0]  trying                0.493   10.188 \n",
            "   [0]  to                    0.492   7.997  \n",
            "   [1]  make                  0.492   0.000  \n",
            "   [0]  but                   0.492   8.266  \n",
            "   [0]  first                 0.492   8.350  \n",
            "   [0]  why                   0.492   9.417  \n",
            "   [1]  did                   0.492   0.000  \n",
            "   [0]  it                    0.492   7.933  \n",
            "   [0]  have                  0.492   8.761  \n",
            "   [1]  to                    0.492   0.000  \n",
            "   [0]  be                    0.492   8.971  \n",
            " Sample 2.\n",
            "   [0]  am                    0.498   9.419  \n",
            "   [0]  curious               0.498   9.461  \n",
            "   [1]  of                    0.498   0.000  \n",
            "   [1]  what                  0.498   0.000  \n",
            "   [0]  rifle                 0.498   10.601 \n",
            "   [1]  beckett               0.498   0.000  \n",
            "   [1]  was                   0.498   0.000  \n",
            "   [1]  using                 0.498   0.000  \n",
            "   [1]  in                    0.499   0.000  \n",
            "   [0]  the                   0.499   8.035  \n",
            "   [1]  movie                 0.499   0.000  \n",
            "   [1]  and                   0.498   0.000  \n",
            "   [1]  also                  0.498   0.000  \n",
            "   [1]  the                   0.498   0.000  \n",
            "   [1]  caliber               0.498   0.000  \n",
            "   [0]  of                    0.498   8.863  \n",
            "   [0]  the                   0.498   8.606  \n",
            "   [0]  bullet                0.498   11.308 \n",
            "   [1]  that                  0.498   0.000  \n",
            "   [1]  he                    0.499   0.000  \n",
            "Samples\n",
            "Sample 0 .  movie is basically a action movie at its finest it severally lacks depth and story but compensates that with its\n",
            "Sample 1 .  guess i can see the point this guy was trying to make but first why did it have to be\n",
            "Sample 2 .  am curious of what rifle beckett was using in the movie and also the caliber of the bullet that he\n",
            "\n",
            "\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816]...][[0 1 0 0 0 1 1 0 1 1]...][[84 69849 30 69849 69849 69849 136 11 69849 816]...][[3 30 287 71 42 136 11 99 816 10]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816]...][[0 1 0 0 0 1 1 0 1 1]...][[84 69849 30 69849 69849 69849 136 11 69849 816]...][[3 30 287 71 42 136 11 99 816 10]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98]...][[1 1 0 0 1 1 0 1 0 0]...][[10 5 2 69849 69849 3 478 69849 338 69849]...][[5 2 1563 120 3 478 2124 338 98 38]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98]...][[1 1 0 0 1 1 0 1 0 0]...][[10 5 2 69849 69849 3 478 69849 338 69849]...][[5 2 1563 120 3 478 2124 338 98 38]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            "global_step: 24\n",
            " perplexity: 46150.260\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            " percent of 3-grams captured: 0.496.\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            " percent of 2-grams captured: 0.689.\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            " percent of 4-grams captured: 0.260.\n",
            " geometric_avg: 0.446.\n",
            " arithmetic_avg: 0.482.\n",
            "global_step: 24\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69333\n",
            " G train loss: 5.37005\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98]...][[1 1 0 0 1 1 0 1 0 0]...][[10 5 2 69849 69849 3 478 69849 338 69849]...][[5 2 1563 120 3 478 2124 338 98 38]...]\n",
            " Sample 0.\n",
            "   [1]  is                    0.509   0.000  \n",
            "   [1]  a                     0.508   0.000  \n",
            "   [0]  rip                   0.507   13.840 \n",
            "   [0]  off                   0.506   8.828  \n",
            "   [1]  of                    0.505   0.000  \n",
            "   [1]  already               0.504   0.000  \n",
            "   [0]  crappy                0.503   11.413 \n",
            "   [1]  hollywood             0.502   0.000  \n",
            "   [0]  movies                0.501   5.621  \n",
            "   [0]  like                  0.501   6.808  \n",
            "   [1]  scream                0.500   0.000  \n",
            "   [1]  and                   0.500   0.000  \n",
            "   [0]  i                     0.500   4.354  \n",
            "   [0]  know                  0.500   6.573  \n",
            "   [0]  what                  0.500   4.940  \n",
            "   [0]  you                   0.500   6.779  \n",
            "   [1]  did                   0.501   0.000  \n",
            "   [0]  last                  0.501   7.684  \n",
            "   [1]  summer                0.501   0.000  \n",
            "   [0]  the                   0.501   3.173  \n",
            " Sample 1.\n",
            "   [1]  crawford              0.503   0.000  \n",
            "   [0]  is                    0.504   3.519  \n",
            "   [1]  a                     0.504   0.000  \n",
            "   [1]  jewel                 0.504   0.000  \n",
            "   [1]  thief                 0.504   0.000  \n",
            "   [0]  trying                0.504   9.944  \n",
            "   [1]  to                    0.504   0.000  \n",
            "   [1]  break                 0.504   0.000  \n",
            "   [1]  into                  0.504   0.000  \n",
            "   [0]  society               0.505   10.394 \n",
            "   [0]  in                    0.505   3.805  \n",
            "   [1]  the                   0.505   0.000  \n",
            "   [1]  last                  0.505   0.000  \n",
            "   [0]  of                    0.505   3.637  \n",
            "   [1]  mrs                   0.504   0.000  \n",
            "   [1]  cheyney               0.504   0.000  \n",
            "   [1]  a                     0.504   0.000  \n",
            "   [1]  1937                  0.504   0.000  \n",
            "   [1]  film                  0.505   0.000  \n",
            "   [0]  from                  0.504   6.321  \n",
            " Sample 2.\n",
            "   [1]  br                    0.508   0.000  \n",
            "   [0]  br                    0.509   5.523  \n",
            "   [1]  were                  0.509   0.000  \n",
            "   [0]  they                  0.508   5.869  \n",
            "   [0]  trying                0.508   11.195 \n",
            "   [1]  to                    0.508   0.000  \n",
            "   [0]  save                  0.508   11.910 \n",
            "   [1]  time                  0.508   0.000  \n",
            "   [0]  and                   0.508   2.172  \n",
            "   [0]  film                  0.508   4.870  \n",
            "   [0]  br                    0.508   5.483  \n",
            "   [0]  br                    0.508   5.485  \n",
            "   [0]  there                 0.508   7.438  \n",
            "   [1]  needs                 0.508   0.000  \n",
            "   [1]  to                    0.508   0.000  \n",
            "   [1]  be                    0.508   0.000  \n",
            "   [1]  many                  0.509   0.000  \n",
            "   [0]  more                  0.509   7.019  \n",
            "   [1]  steps                 0.509   0.000  \n",
            "   [1]  between               0.509   0.000  \n",
            "Samples\n",
            "Sample 0 .  is a rip off of already crappy hollywood movies like scream and i know what you did last summer the\n",
            "Sample 1 .  crawford is a jewel thief trying to break into society in the last of mrs cheyney a 1937 film from\n",
            "Sample 2 .  br br were they trying to save time and film br br there needs to be many more steps between\n",
            "\n",
            "\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4]...][[1 0 1 0 0 0 0 0 0 1]...][[99 42 69849 228 69849 69849 69849 69849 69849 69849]...][[42 1125 228 78 10 19 9 1739 4 663]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4]...][[1 0 1 0 0 0 0 0 0 1]...][[99 42 69849 228 69849 69849 69849 69849 69849 69849]...][[42 1125 228 78 10 19 9 1739 4 663]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816]...][[1 0 0 1 1 1 0 1 0 1]...][[84 3 69849 69849 71 42 136 69849 99 69849]...][[3 30 287 71 42 136 11 99 816 10]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816]...][[1 0 0 1 1 1 0 1 0 1]...][[84 3 69849 69849 71 42 136 69849 99 69849]...][[3 30 287 71 42 136 11 99 816 10]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            "global_step: 27\n",
            " perplexity: 32388.162\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            " percent of 3-grams captured: 0.500.\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            " percent of 2-grams captured: 0.724.\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            " percent of 4-grams captured: 0.224.\n",
            " geometric_avg: 0.433.\n",
            " arithmetic_avg: 0.482.\n",
            "global_step: 27\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69318\n",
            " G train loss: 5.21093\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816]...][[1 0 0 1 1 1 0 1 0 1]...][[84 3 69849 69849 71 42 136 69849 99 69849]...][[3 30 287 71 42 136 11 99 816 10]...]\n",
            " Sample 0.\n",
            "   [1]  of                    0.512   0.000  \n",
            "   [0]  all                   0.510   5.291  \n",
            "   [0]  let                   0.508   11.530 \n",
            "   [1]  me                    0.508   0.000  \n",
            "   [1]  just                  0.508   0.000  \n",
            "   [1]  say                   0.508   0.000  \n",
            "   [0]  that                  0.508   4.664  \n",
            "   [1]  after                 0.508   0.000  \n",
            "   [0]  viewing               0.508   12.057 \n",
            "   [1]  this                  0.508   0.000  \n",
            "   [0]  film                  0.508   4.491  \n",
            "   [0]  i                     0.507   3.079  \n",
            "   [0]  seriously             0.507   12.963 \n",
            "   [0]  thought               0.507   7.691  \n",
            "   [0]  about                 0.508   5.216  \n",
            "   [0]  writing               0.507   11.376 \n",
            "   [0]  my                    0.507   5.193  \n",
            "   [1]  own                   0.507   0.000  \n",
            "   [0]  screenplay            0.507   12.252 \n",
            "   [1]  because               0.507   0.000  \n",
            " Sample 1.\n",
            "   [1]  remember              0.494   0.000  \n",
            "   [1]  seeing                0.494   0.000  \n",
            "   [0]  this                  0.494   4.171  \n",
            "   [0]  movie                 0.494   3.975  \n",
            "   [1]  in                    0.495   0.000  \n",
            "   [1]  1986                  0.495   0.000  \n",
            "   [1]  when                  0.495   0.000  \n",
            "   [0]  tv                    0.494   7.571  \n",
            "   [0]  32                    0.494   14.657 \n",
            "   [1]  in                    0.494   0.000  \n",
            "   [0]  chicago               0.494   13.509 \n",
            "   [1]  was                   0.494   0.000  \n",
            "   [0]  broadcasting          0.494   11.707 \n",
            "   [1]  the                   0.494   0.000  \n",
            "   [1]  final                 0.494   0.000  \n",
            "   [0]  episode               0.494   8.788  \n",
            "   [1]  of                    0.494   0.000  \n",
            "   [1]  its                   0.494   0.000  \n",
            "   [0]  old                   0.494   7.047  \n",
            "   [1]  school                0.494   0.000  \n",
            " Sample 2.\n",
            "   [1]  is                    0.513   0.000  \n",
            "   [1]  a                     0.512   0.000  \n",
            "   [1]  horrible              0.511   0.000  \n",
            "   [0]  horrible              0.511   11.051 \n",
            "   [1]  effort                0.511   0.000  \n",
            "   [0]  but                   0.511   4.365  \n",
            "   [0]  somehow               0.511   9.710  \n",
            "   [0]  it                    0.511   3.206  \n",
            "   [0]  is                    0.511   3.329  \n",
            "   [0]  also                  0.511   9.159  \n",
            "   [0]  quite                 0.511   8.082  \n",
            "   [0]  entertaining          0.511   13.723 \n",
            "   [0]  there                 0.511   7.016  \n",
            "   [0]  are                   0.511   5.099  \n",
            "   [1]  two                   0.511   0.000  \n",
            "   [1]  kinds                 0.511   0.000  \n",
            "   [1]  of                    0.511   0.000  \n",
            "   [1]  sci                   0.511   0.000  \n",
            "   [0]  fi                    0.512   9.650  \n",
            "   [1]  good                  0.512   0.000  \n",
            "Samples\n",
            "Sample 0 .  of all let me just say that after viewing this film i seriously thought about writing my own screenplay because\n",
            "Sample 1 .  remember seeing this movie in 1986 when tv 32 in chicago was broadcasting the final episode of its old school\n",
            "Sample 2 .  is a horrible horrible effort but somehow it is also quite entertaining there are two kinds of sci fi good\n",
            "\n",
            "\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55]...][[1 1 1 0 1 0 1 0 1 0]...][[25473 185 45 329 69849 894 69849 3 69849 55]...][[185 45 329 40 894 564 3 1902 55 15]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55]...][[1 1 1 0 1 0 1 0 1 0]...][[25473 185 45 329 69849 894 69849 3 69849 55]...][[185 45 329 40 894 564 3 1902 55 15]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4]...][[0 0 1 0 1 0 1 1 1 1]...][[99 69849 69849 228 69849 10 69849 9 1739 4]...][[42 1125 228 78 10 19 9 1739 4 663]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4]...][[0 0 1 0 1 0 1 1 1 1]...][[99 69849 69849 228 69849 10 69849 9 1739 4]...][[42 1125 228 78 10 19 9 1739 4 663]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            "global_step: 30\n",
            " perplexity: 25904.027\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            " percent of 3-grams captured: 0.487.\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            " percent of 2-grams captured: 0.721.\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            " percent of 4-grams captured: 0.220.\n",
            " geometric_avg: 0.426.\n",
            " arithmetic_avg: 0.476.\n",
            "global_step: 30\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69320\n",
            " G train loss: 5.08442\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4]...][[0 0 1 0 1 0 1 1 1 1]...][[99 69849 69849 228 69849 10 69849 9 1739 4]...][[42 1125 228 78 10 19 9 1739 4 663]...]\n",
            " Sample 0.\n",
            "   [0]  just                  0.504   4.939  \n",
            "   [0]  15                    0.503   14.490 \n",
            "   [1]  minutes               0.501   0.000  \n",
            "   [0]  into                  0.499   6.989  \n",
            "   [1]  this                  0.497   0.000  \n",
            "   [0]  film                  0.496   3.728  \n",
            "   [1]  i                     0.495   0.000  \n",
            "   [1]  began                 0.495   0.000  \n",
            "   [1]  to                    0.494   0.000  \n",
            "   [1]  miss                  0.494   0.000  \n",
            "   [0]  zhang                 0.494   13.020 \n",
            "   [0]  yimou                 0.495   14.077 \n",
            "   [0]  s                     0.494   3.843  \n",
            "   [1]  earlier               0.494   0.000  \n",
            "   [0]  more                  0.494   6.127  \n",
            "   [0]  weighty               0.494   13.070 \n",
            "   [1]  films                 0.495   0.000  \n",
            "   [0]  that                  0.495   3.957  \n",
            "   [1]  looked                0.494   0.000  \n",
            "   [0]  at                    0.494   4.513  \n",
            " Sample 1.\n",
            "   [0]  taking                0.519   12.498 \n",
            "   [1]  off                   0.519   0.000  \n",
            "   [0]  for                   0.518   4.996  \n",
            "   [1]  the                   0.517   0.000  \n",
            "   [0]  santa                 0.516   10.754 \n",
            "   [0]  cruz                  0.515   13.466 \n",
            "   [0]  film                  0.515   3.776  \n",
            "   [1]  festival              0.514   0.000  \n",
            "   [1]  i                     0.514   0.000  \n",
            "   [0]  had                   0.514   4.257  \n",
            "   [1]  expectations          0.514   0.000  \n",
            "   [1]  of                    0.514   0.000  \n",
            "   [0]  this                  0.515   3.513  \n",
            "   [0]  movie                 0.515   4.064  \n",
            "   [0]  already               0.515   12.316 \n",
            "   [1]  from                  0.515   0.000  \n",
            "   [1]  everything            0.515   0.000  \n",
            "   [1]  i                     0.514   0.000  \n",
            "   [0]  read                  0.514   6.691  \n",
            "   [1]  and                   0.514   0.000  \n",
            " Sample 2.\n",
            "   [0]  of                    0.504   3.593  \n",
            "   [1]  the                   0.504   0.000  \n",
            "   [1]  more                  0.503   0.000  \n",
            "   [1]  clever                0.501   0.000  \n",
            "   [1]  disney                0.501   0.000  \n",
            "   [1]  television            0.501   0.000  \n",
            "   [0]  movies                0.501   4.840  \n",
            "   [0]  is                    0.501   3.669  \n",
            "   [0]  definitely            0.501   11.885 \n",
            "   [1]  smart                 0.501   0.000  \n",
            "   [0]  house                 0.501   10.158 \n",
            "   [1]  this                  0.501   0.000  \n",
            "   [1]  involves              0.501   0.000  \n",
            "   [1]  a                     0.501   0.000  \n",
            "   [1]  family                0.501   0.000  \n",
            "   [1]  winning               0.501   0.000  \n",
            "   [1]  a                     0.501   0.000  \n",
            "   [0]  contest               0.501   13.657 \n",
            "   [0]  to                    0.502   3.744  \n",
            "   [0]  live                  0.502   8.517  \n",
            "Samples\n",
            "Sample 0 .  just 15 minutes into this film i began to miss zhang yimou s earlier more weighty films that looked at\n",
            "Sample 1 .  taking off for the santa cruz film festival i had expectations of this movie already from everything i read and\n",
            "Sample 2 .  of the more clever disney television movies is definitely smart house this involves a family winning a contest to live\n",
            "\n",
            "\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518]...][[1 1 1 0 0 0 1 1 0 0]...][[761 9365 21650 451 69849 69849 69849 26 183 69849]...][[9365 21650 451 8 1844 15 26 183 518 5809]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518]...][[1 1 1 0 0 0 1 1 0 0]...][[761 9365 21650 451 69849 69849 69849 26 183 69849]...][[9365 21650 451 8 1844 15 26 183 518 5809]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55]...][[1 0 1 0 1 1 0 1 0 0]...][[25473 185 69849 329 69849 894 564 69849 1902 69849]...][[185 45 329 40 894 564 3 1902 55 15]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55]...][[1 0 1 0 1 1 0 1 0 0]...][[25473 185 69849 329 69849 894 564 69849 1902 69849]...][[185 45 329 40 894 564 3 1902 55 15]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            "global_step: 33\n",
            " perplexity: 20775.155\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            " percent of 3-grams captured: 0.498.\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            " percent of 2-grams captured: 0.701.\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            " percent of 4-grams captured: 0.237.\n",
            " geometric_avg: 0.436.\n",
            " arithmetic_avg: 0.479.\n",
            "global_step: 33\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69306\n",
            " G train loss: 4.97637\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55]...][[1 0 1 0 1 1 0 1 0 0]...][[25473 185 69849 329 69849 894 564 69849 1902 69849]...][[185 45 329 40 894 564 3 1902 55 15]...]\n",
            " Sample 0.\n",
            "   [1]  down                  0.508   0.000  \n",
            "   [0]  has                   0.507   4.952  \n",
            "   [1]  used                  0.505   0.000  \n",
            "   [0]  her                   0.504   6.548  \n",
            "   [1]  personal              0.503   0.000  \n",
            "   [1]  experience            0.503   0.000  \n",
            "   [0]  of                    0.502   3.570  \n",
            "   [1]  growing               0.502   0.000  \n",
            "   [0]  up                    0.502   6.327  \n",
            "   [0]  with                  0.502   4.315  \n",
            "   [1]  two                   0.502   0.000  \n",
            "   [1]  autistic              0.502   0.000  \n",
            "   [0]  brothers              0.502   9.991  \n",
            "   [0]  one                   0.502   4.315  \n",
            "   [1]  of                    0.502   0.000  \n",
            "   [1]  whom                  0.502   0.000  \n",
            "   [1]  she                   0.502   0.000  \n",
            "   [0]  describes             0.502   12.387 \n",
            "   [0]  as                    0.502   4.824  \n",
            "   [1]  a                     0.502   0.000  \n",
            " Sample 1.\n",
            "   [1]  in                    0.504   0.000  \n",
            "   [0]  mind                  0.503   9.266  \n",
            "   [0]  that                  0.503   4.558  \n",
            "   [1]  my                    0.503   0.000  \n",
            "   [1]  comments              0.502   0.000  \n",
            "   [0]  are                   0.502   5.708  \n",
            "   [0]  always                0.502   6.595  \n",
            "   [0]  serious               0.502   9.986  \n",
            "   [1]  ha                    0.502   0.000  \n",
            "   [0]  ha                    0.503   11.202 \n",
            "   [0]  ha                    0.503   11.145 \n",
            "   [0]  ha                    0.503   11.093 \n",
            "   [0]  ha                    0.503   11.046 \n",
            "   [1]  ha                    0.503   0.000  \n",
            "   [1]  ha                    0.503   0.000  \n",
            "   [0]  ha                    0.503   10.929 \n",
            "   [1]  ha                    0.503   0.000  \n",
            "   [1]  ha                    0.503   0.000  \n",
            "   [1]  this                  0.503   0.000  \n",
            "   [0]  was                   0.503   4.187  \n",
            " Sample 2.\n",
            "   [1]  s                     0.504   0.000  \n",
            "   [0]  not                   0.505   5.338  \n",
            "   [0]  much                  0.503   5.493  \n",
            "   [0]  to                    0.502   3.376  \n",
            "   [1]  be                    0.500   0.000  \n",
            "   [1]  said                  0.499   0.000  \n",
            "   [0]  about                 0.498   5.087  \n",
            "   [1]  high                  0.498   0.000  \n",
            "   [1]  school                0.498   0.000  \n",
            "   [0]  confidential          0.498   13.018 \n",
            "   [1]  a                     0.498   0.000  \n",
            "   [0]  teen                  0.498   10.972 \n",
            "   [0]  exploitation          0.498   12.879 \n",
            "   [0]  movie                 0.499   4.265  \n",
            "   [1]  from                  0.498   0.000  \n",
            "   [0]  the                   0.498   3.602  \n",
            "   [1]  end                   0.498   0.000  \n",
            "   [1]  of                    0.498   0.000  \n",
            "   [0]  the                   0.498   3.650  \n",
            "   [0]  fabulous              0.498   12.845 \n",
            "Samples\n",
            "Sample 0 .  down has used her personal experience of growing up with two autistic brothers one of whom she describes as a\n",
            "Sample 1 .  in mind that my comments are always serious ha ha ha ha ha ha ha ha ha ha this was\n",
            "Sample 2 .  s not much to be said about high school confidential a teen exploitation movie from the end of the fabulous\n",
            "\n",
            "\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75]...][[1 1 1 0 0 1 0 0 1 0]...][[31 84 9 67 69849 69849 2 69849 69849 75]...][[84 9 67 0 2277 2 171 3 75 319]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75]...][[1 1 1 0 0 1 0 0 1 0]...][[31 84 9 67 69849 69849 2 69849 69849 75]...][[84 9 67 0 2277 2 171 3 75 319]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518]...][[0 0 1 1 1 0 0 1 1 0]...][[761 69849 69849 451 8 1844 69849 69849 183 518]...][[9365 21650 451 8 1844 15 26 183 518 5809]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518]...][[0 0 1 1 1 0 0 1 1 0]...][[761 69849 69849 451 8 1844 69849 69849 183 518]...][[9365 21650 451 8 1844 15 26 183 518 5809]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            "global_step: 36\n",
            " perplexity: 16812.566\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            " percent of 3-grams captured: 0.511.\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            " percent of 2-grams captured: 0.701.\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            " percent of 4-grams captured: 0.252.\n",
            " geometric_avg: 0.449.\n",
            " arithmetic_avg: 0.488.\n",
            "global_step: 36\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69313\n",
            " G train loss: 4.86460\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518]...][[0 0 1 1 1 0 0 1 1 0]...][[761 69849 69849 451 8 1844 69849 69849 183 518]...][[9365 21650 451 8 1844 15 26 183 518 5809]...]\n",
            " Sample 0.\n",
            "   [0]  maxwell               0.508   12.242 \n",
            "   [0]  caulfield             0.506   12.063 \n",
            "   [1]  lives                 0.506   0.000  \n",
            "   [1]  in                    0.506   0.000  \n",
            "   [1]  england               0.506   0.000  \n",
            "   [0]  with                  0.506   4.975  \n",
            "   [0]  his                   0.506   5.265  \n",
            "   [1]  young                 0.506   0.000  \n",
            "   [1]  daughter              0.506   0.000  \n",
            "   [0]  melissa               0.506   12.143 \n",
            "   [1]  charlotte             0.506   0.000  \n",
            "   [0]  savage                0.506   11.859 \n",
            "   [0]  a                     0.506   3.928  \n",
            "   [0]  recent                0.506   11.162 \n",
            "   [0]  widower               0.505   12.127 \n",
            "   [1]  of                    0.505   0.000  \n",
            "   [1]  16                    0.505   0.000  \n",
            "   [0]  months                0.505   9.185  \n",
            "   [0]  tom                   0.505   11.986 \n",
            "   [1]  is                    0.505   0.000  \n",
            " Sample 1.\n",
            "   [0]  i                     0.502   3.510  \n",
            "   [0]  am                    0.503   5.783  \n",
            "   [0]  a                     0.503   4.277  \n",
            "   [1]  fan                   0.504   0.000  \n",
            "   [1]  of                    0.503   0.000  \n",
            "   [1]  the                   0.503   0.000  \n",
            "   [1]  vince                 0.504   0.000  \n",
            "   [0]  vaughn                0.504   13.176 \n",
            "   [1]  ben                   0.504   0.000  \n",
            "   [1]  stiller               0.504   0.000  \n",
            "   [0]  will                  0.504   6.535  \n",
            "   [1]  ferrel                0.504   0.000  \n",
            "   [0]  etc                   0.504   12.594 \n",
            "   [0]  movies                0.504   5.350  \n",
            "   [1]  but                   0.504   0.000  \n",
            "   [1]  the                   0.504   0.000  \n",
            "   [1]  breakup               0.504   0.000  \n",
            "   [1]  was                   0.504   0.000  \n",
            "   [1]  awful                 0.504   0.000  \n",
            "   [1]  br                    0.504   0.000  \n",
            " Sample 2.\n",
            "   [0]  burke                 0.499   12.706 \n",
            "   [0]  is                    0.501   4.164  \n",
            "   [1]  an                    0.502   0.000  \n",
            "   [0]  undercover            0.503   11.013 \n",
            "   [0]  cop                   0.503   11.463 \n",
            "   [1]  sent                  0.503   0.000  \n",
            "   [0]  to                    0.503   4.962  \n",
            "   [0]  investigate           0.503   12.922 \n",
            "   [1]  a                     0.504   0.000  \n",
            "   [0]  prison                0.504   9.849  \n",
            "   [1]  where                 0.504   0.000  \n",
            "   [1]  inmates               0.504   0.000  \n",
            "   [0]  are                   0.504   5.758  \n",
            "   [0]  inexplicably          0.504   12.416 \n",
            "   [1]  dying                 0.504   0.000  \n",
            "   [0]  doesn                 0.504   8.404  \n",
            "   [1]  t                     0.504   0.000  \n",
            "   [1]  sound                 0.503   0.000  \n",
            "   [0]  too                   0.503   6.423  \n",
            "   [1]  shabby                0.503   0.000  \n",
            "Samples\n",
            "Sample 0 .  maxwell caulfield lives in england with his young daughter melissa charlotte savage a recent widower of 16 months tom is\n",
            "Sample 1 .  i am a fan of the vince vaughn ben stiller will ferrel etc movies but the breakup was awful br\n",
            "Sample 2 .  burke is an undercover cop sent to investigate a prison where inmates are inexplicably dying doesn t sound too shabby\n",
            "\n",
            "\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290]...][[0 1 0 0 0 0 1 1 0 1]...][[47 69849 1349 69849 69849 69849 69849 92 8 69849]...][[2 1349 115 19 10 5 92 8 5290 22]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290]...][[0 1 0 0 0 0 1 1 0 1]...][[47 69849 1349 69849 69849 69849 69849 92 8 69849]...][[2 1349 115 19 10 5 92 8 5290 22]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75]...][[1 0 0 0 1 1 1 1 1 1]...][[31 84 69849 69849 69849 2277 2 171 3 75]...][[84 9 67 0 2277 2 171 3 75 319]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75]...][[1 0 0 0 1 1 1 1 1 1]...][[31 84 69849 69849 69849 2277 2 171 3 75]...][[84 9 67 0 2277 2 171 3 75 319]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            "global_step: 39\n",
            " perplexity: 13775.252\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            " percent of 3-grams captured: 0.509.\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            " percent of 2-grams captured: 0.721.\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            " percent of 4-grams captured: 0.235.\n",
            " geometric_avg: 0.442.\n",
            " arithmetic_avg: 0.489.\n",
            "global_step: 39\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69314\n",
            " G train loss: 4.76520\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75]...][[1 0 0 0 1 1 1 1 1 1]...][[31 84 69849 69849 69849 2277 2 171 3 75]...][[84 9 67 0 2277 2 171 3 75 319]...]\n",
            " Sample 0.\n",
            "   [1]  first                 0.499   0.000  \n",
            "   [0]  i                     0.498   5.925  \n",
            "   [0]  had                   0.498   6.348  \n",
            "   [0]  the                   0.497   5.172  \n",
            "   [1]  reaction              0.496   0.000  \n",
            "   [1]  a                     0.496   0.000  \n",
            "   [1]  lot                   0.495   0.000  \n",
            "   [1]  of                    0.494   0.000  \n",
            "   [1]  people                0.495   0.000  \n",
            "   [1]  left                  0.495   0.000  \n",
            "   [1]  with                  0.495   0.000  \n",
            "   [0]  after                 0.496   6.894  \n",
            "   [0]  seeing                0.496   7.587  \n",
            "   [1]  this                  0.496   0.000  \n",
            "   [0]  that                  0.496   5.798  \n",
            "   [0]  shots                 0.496   11.062 \n",
            "   [1]  of                    0.496   0.000  \n",
            "   [1]  fat                   0.496   0.000  \n",
            "   [1]  people                0.496   0.000  \n",
            "   [0]  sunbathing            0.496   11.702 \n",
            " Sample 1.\n",
            "   [0]  movie                 0.521   5.600  \n",
            "   [0]  was                   0.519   5.235  \n",
            "   [0]  so                    0.516   6.647  \n",
            "   [1]  awful                 0.514   0.000  \n",
            "   [0]  it                    0.512   4.977  \n",
            "   [1]  makes                 0.512   0.000  \n",
            "   [0]  me                    0.511   5.119  \n",
            "   [0]  wish                  0.511   10.496 \n",
            "   [1]  i                     0.511   0.000  \n",
            "   [0]  had                   0.511   5.836  \n",
            "   [0]  been                  0.511   6.982  \n",
            "   [0]  struck                0.511   12.080 \n",
            "   [1]  blind                 0.511   0.000  \n",
            "   [1]  and                   0.511   0.000  \n",
            "   [0]  deaf                  0.510   11.742 \n",
            "   [0]  as                    0.510   4.994  \n",
            "   [1]  a                     0.510   0.000  \n",
            "   [0]  child                 0.510   9.633  \n",
            "   [0]  i                     0.510   4.907  \n",
            "   [1]  watched               0.511   0.000  \n",
            " Sample 2.\n",
            "   [1]  two                   0.512   0.000  \n",
            "   [1]  writers               0.510   0.000  \n",
            "   [0]  make                  0.509   6.974  \n",
            "   [1]  a                     0.509   0.000  \n",
            "   [1]  screenplay            0.508   0.000  \n",
            "   [0]  of                    0.508   5.086  \n",
            "   [0]  a                     0.508   4.840  \n",
            "   [1]  horror                0.508   0.000  \n",
            "   [1]  version               0.507   0.000  \n",
            "   [0]  of                    0.507   5.190  \n",
            "   [1]  breakfast             0.507   0.000  \n",
            "   [1]  at                    0.507   0.000  \n",
            "   [1]  tiffany               0.507   0.000  \n",
            "   [0]  s                     0.507   5.934  \n",
            "   [1]  you                   0.507   0.000  \n",
            "   [0]  know                  0.507   5.648  \n",
            "   [0]  something             0.507   7.449  \n",
            "   [0]  is                    0.507   5.373  \n",
            "   [0]  going                 0.507   6.665  \n",
            "   [1]  to                    0.507   0.000  \n",
            "Samples\n",
            "Sample 0 .  first i had the reaction a lot of people left with after seeing this that shots of fat people sunbathing\n",
            "Sample 1 .  movie was so awful it makes me wish i had been struck blind and deaf as a child i watched\n",
            "Sample 2 .  two writers make a screenplay of a horror version of breakfast at tiffany s you know something is going to\n",
            "\n",
            "\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33]...][[1 1 1 0 0 1 1 1 0 1]...][[157 4140 4 2666 69849 69849 78 0 797 69849]...][[4140 4 2666 6756 439 78 0 797 33 575]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33]...][[1 1 1 0 0 1 1 1 0 1]...][[157 4140 4 2666 69849 69849 78 0 797 69849]...][[4140 4 2666 6756 439 78 0 797 33 575]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290]...][[0 0 0 0 1 0 0 0 1 0]...][[47 69849 69849 69849 69849 10 69849 69849 69849 5290]...][[2 1349 115 19 10 5 92 8 5290 22]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290]...][[0 0 0 0 1 0 0 0 1 0]...][[47 69849 69849 69849 69849 10 69849 69849 69849 5290]...][[2 1349 115 19 10 5 92 8 5290 22]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            "global_step: 42\n",
            " perplexity: 11816.275\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            " percent of 3-grams captured: 0.516.\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            " percent of 2-grams captured: 0.706.\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            " percent of 4-grams captured: 0.264.\n",
            " geometric_avg: 0.458.\n",
            " arithmetic_avg: 0.496.\n",
            "global_step: 42\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69318\n",
            " G train loss: 4.69710\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290]...][[0 0 0 0 1 0 0 0 1 0]...][[47 69849 69849 69849 69849 10 69849 69849 69849 5290]...][[2 1349 115 19 10 5 92 8 5290 22]...]\n",
            " Sample 0.\n",
            "   [0]  a                     0.496   4.657  \n",
            "   [0]  terrific              0.497   10.516 \n",
            "   [0]  little                0.497   5.361  \n",
            "   [0]  film                  0.496   4.946  \n",
            "   [1]  this                  0.496   0.000  \n",
            "   [0]  is                    0.495   4.776  \n",
            "   [0]  made                  0.495   5.830  \n",
            "   [0]  in                    0.495   5.736  \n",
            "   [1]  1973                  0.495   0.000  \n",
            "   [0]  on                    0.495   5.323  \n",
            "   [0]  what                  0.495   6.585  \n",
            "   [0]  was                   0.495   5.402  \n",
            "   [1]  presumably            0.495   0.000  \n",
            "   [0]  a                     0.495   4.586  \n",
            "   [1]  very                  0.495   0.000  \n",
            "   [1]  low                   0.495   0.000  \n",
            "   [1]  budget                0.495   0.000  \n",
            "   [0]  this                  0.495   5.211  \n",
            "   [1]  two                   0.495   0.000  \n",
            "   [0]  hander                0.495   12.376 \n",
            " Sample 1.\n",
            "   [1]  you                   0.508   0.000  \n",
            "   [0]  understand            0.509   7.040  \n",
            "   [0]  wretched              0.509   10.968 \n",
            "   [1]  excess                0.509   0.000  \n",
            "   [1]  this                  0.509   0.000  \n",
            "   [1]  movie                 0.509   0.000  \n",
            "   [0]  won                   0.508   8.895  \n",
            "   [1]  t                     0.508   0.000  \n",
            "   [1]  really                0.508   0.000  \n",
            "   [1]  mean                  0.507   0.000  \n",
            "   [1]  much                  0.507   0.000  \n",
            "   [0]  to                    0.507   5.848  \n",
            "   [1]  you                   0.507   0.000  \n",
            "   [0]  an                    0.507   6.378  \n",
            "   [0]  attempt               0.507   10.241 \n",
            "   [1]  was                   0.507   0.000  \n",
            "   [0]  made                  0.508   6.067  \n",
            "   [1]  to                    0.508   0.000  \n",
            "   [1]  interject             0.507   0.000  \n",
            "   [1]  a                     0.507   0.000  \n",
            " Sample 2.\n",
            "   [1]  previous              0.531   0.000  \n",
            "   [1]  poster                0.531   0.000  \n",
            "   [0]  obviously             0.530   11.882 \n",
            "   [0]  worked                0.528   11.507 \n",
            "   [0]  on                    0.527   5.315  \n",
            "   [0]  the                   0.526   4.863  \n",
            "   [1]  movie                 0.525   0.000  \n",
            "   [1]  it                    0.524   0.000  \n",
            "   [1]  s                     0.524   0.000  \n",
            "   [0]  a                     0.523   4.552  \n",
            "   [1]  joke                  0.523   0.000  \n",
            "   [0]  how                   0.523   5.037  \n",
            "   [0]  bad                   0.523   6.825  \n",
            "   [0]  it                    0.523   4.724  \n",
            "   [0]  is                    0.522   4.729  \n",
            "   [0]  and                   0.522   4.941  \n",
            "   [1]  no                    0.522   0.000  \n",
            "   [0]  one                   0.522   5.233  \n",
            "   [1]  would                 0.522   0.000  \n",
            "   [0]  review                0.522   9.448  \n",
            "Samples\n",
            "Sample 0 .  a terrific little film this is made in 1973 on what was presumably a very low budget this two hander\n",
            "Sample 1 .  you understand wretched excess this movie won t really mean much to you an attempt was made to interject a\n",
            "Sample 2 .  previous poster obviously worked on the movie it s a joke how bad it is and no one would review\n",
            "\n",
            "\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2]...][[0 1 1 1 1 0 1 1 0 0]...][[264 69849 1281 10 17 2 69849 16 804 69849]...][[9 1281 10 17 2 223 16 804 2 605]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2]...][[0 1 1 1 1 0 1 1 0 0]...][[264 69849 1281 10 17 2 69849 16 804 69849]...][[9 1281 10 17 2 223 16 804 2 605]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33]...][[0 0 1 0 1 1 1 1 0 0]...][[157 69849 69849 2666 69849 439 78 0 797 69849]...][[4140 4 2666 6756 439 78 0 797 33 575]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33]...][[0 0 1 0 1 1 1 1 0 0]...][[157 69849 69849 2666 69849 439 78 0 797 69849]...][[4140 4 2666 6756 439 78 0 797 33 575]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            "global_step: 45\n",
            " perplexity: 10285.618\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            " percent of 3-grams captured: 0.524.\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            " percent of 2-grams captured: 0.717.\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            " percent of 4-grams captured: 0.257.\n",
            " geometric_avg: 0.459.\n",
            " arithmetic_avg: 0.499.\n",
            "global_step: 45\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69312\n",
            " G train loss: 4.63108\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33]...][[0 0 1 0 1 1 1 1 0 0]...][[157 69849 69849 2666 69849 439 78 0 797 69849]...][[4140 4 2666 6756 439 78 0 797 33 575]...]\n",
            " Sample 0.\n",
            "   [0]  scheme                0.487   12.197 \n",
            "   [0]  to                    0.487   5.461  \n",
            "   [1]  trick                 0.485   0.000  \n",
            "   [0]  casablanca            0.484   12.134 \n",
            "   [1]  fans                  0.484   0.000  \n",
            "   [1]  into                  0.483   0.000  \n",
            "   [1]  the                   0.483   0.000  \n",
            "   [1]  theater               0.483   0.000  \n",
            "   [0]  by                    0.483   5.059  \n",
            "   [0]  including             0.483   11.896 \n",
            "   [0]  favorite              0.483   7.108  \n",
            "   [1]  actors                0.483   0.000  \n",
            "   [1]  from                  0.483   0.000  \n",
            "   [1]  that                  0.483   0.000  \n",
            "   [1]  film                  0.483   0.000  \n",
            "   [1]  in                    0.483   0.000  \n",
            "   [0]  a                     0.483   4.768  \n",
            "   [0]  war                   0.483   9.604  \n",
            "   [1]  story                 0.483   0.000  \n",
            "   [0]  with                  0.483   5.628  \n",
            " Sample 1.\n",
            "   [1]  to                    0.518   0.000  \n",
            "   [0]  begin                 0.516   10.634 \n",
            "   [1]  the                   0.513   0.000  \n",
            "   [0]  special               0.511   8.464  \n",
            "   [1]  effects               0.510   0.000  \n",
            "   [1]  should                0.510   0.000  \n",
            "   [1]  be                    0.511   0.000  \n",
            "   [0]  named                 0.511   9.520  \n",
            "   [1]  special               0.511   0.000  \n",
            "   [0]  defects               0.512   12.388 \n",
            "   [0]  when                  0.512   5.435  \n",
            "   [1]  the                   0.512   0.000  \n",
            "   [0]  director              0.513   6.108  \n",
            "   [1]  shouted               0.513   0.000  \n",
            "   [0]  action                0.513   6.319  \n",
            "   [1]  i                     0.513   0.000  \n",
            "   [0]  guess                 0.513   7.689  \n",
            "   [0]  he                    0.513   5.463  \n",
            "   [0]  also                  0.512   6.875  \n",
            "   [0]  indicated             0.513   12.858 \n",
            " Sample 2.\n",
            "   [0]  so                    0.515   5.831  \n",
            "   [1]  many                  0.516   0.000  \n",
            "   [1]  words                 0.514   0.000  \n",
            "   [1]  describe              0.513   0.000  \n",
            "   [0]  this                  0.511   4.448  \n",
            "   [0]  film                  0.510   5.368  \n",
            "   [1]  but                   0.510   0.000  \n",
            "   [0]  only                  0.509   5.762  \n",
            "   [1]  one                   0.509   0.000  \n",
            "   [0]  will                  0.509   6.798  \n",
            "   [1]  sum                   0.509   0.000  \n",
            "   [0]  it                    0.509   5.157  \n",
            "   [1]  up                    0.509   0.000  \n",
            "   [0]  br                    0.509   4.906  \n",
            "   [0]  br                    0.509   4.911  \n",
            "   [0]  crap                  0.509   10.342 \n",
            "   [1]  br                    0.509   0.000  \n",
            "   [0]  br                    0.509   4.923  \n",
            "   [0]  first                 0.509   5.519  \n",
            "   [1]  off                   0.509   0.000  \n",
            "Samples\n",
            "Sample 0 .  scheme to trick casablanca fans into the theater by including favorite actors from that film in a war story with\n",
            "Sample 1 .  to begin the special effects should be named special defects when the director shouted action i guess he also indicated\n",
            "Sample 2 .  so many words describe this film but only one will sum it up br br crap br br first off\n",
            "\n",
            "\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10]...][[0 1 1 0 1 1 0 1 1 1]...][[9 69849 110 30 69849 3 0 69849 8 10]...][[139 110 30 668 3 0 98 8 10 202]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10]...][[0 1 1 0 1 1 0 1 1 1]...][[9 69849 110 30 69849 3 0 69849 8 10]...][[139 110 30 668 3 0 98 8 10 202]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2]...][[0 1 1 0 1 0 1 0 1 0]...][[264 69849 1281 10 69849 2 69849 16 69849 2]...][[9 1281 10 17 2 223 16 804 2 605]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2]...][[0 1 1 0 1 0 1 0 1 0]...][[264 69849 1281 10 69849 2 69849 16 69849 2]...][[9 1281 10 17 2 223 16 804 2 605]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            "global_step: 48\n",
            " perplexity: 9114.805\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            " percent of 3-grams captured: 0.513.\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            " percent of 2-grams captured: 0.713.\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            " percent of 4-grams captured: 0.245.\n",
            " geometric_avg: 0.447.\n",
            " arithmetic_avg: 0.490.\n",
            "global_step: 48\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69311\n",
            " G train loss: 4.57214\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2]...][[0 1 1 0 1 0 1 0 1 0]...][[264 69849 1281 10 69849 2 69849 16 69849 2]...][[9 1281 10 17 2 223 16 804 2 605]...]\n",
            " Sample 0.\n",
            "   [0]  i                     0.499   3.499  \n",
            "   [1]  rated                 0.501   0.000  \n",
            "   [1]  this                  0.500   0.000  \n",
            "   [0]  movie                 0.499   5.029  \n",
            "   [1]  a                     0.498   0.000  \n",
            "   [0]  2                     0.498   6.840  \n",
            "   [1]  for                   0.498   0.000  \n",
            "   [0]  showing               0.498   12.852 \n",
            "   [1]  a                     0.498   0.000  \n",
            "   [0]  complete              0.498   8.331  \n",
            "   [1]  lack                  0.498   0.000  \n",
            "   [1]  of                    0.498   0.000  \n",
            "   [0]  effort                0.497   10.747 \n",
            "   [0]  in                    0.497   5.807  \n",
            "   [0]  trying                0.497   7.235  \n",
            "   [0]  to                    0.497   4.597  \n",
            "   [0]  create                0.497   12.652 \n",
            "   [1]  a                     0.497   0.000  \n",
            "   [1]  quality               0.497   0.000  \n",
            "   [0]  horror                0.497   5.992  \n",
            " Sample 1.\n",
            "   [1]  looks                 0.514   0.000  \n",
            "   [0]  like                  0.516   5.506  \n",
            "   [0]  the                   0.515   4.066  \n",
            "   [1]  movie                 0.513   0.000  \n",
            "   [0]  didn                  0.511   6.755  \n",
            "   [1]  t                     0.509   0.000  \n",
            "   [1]  cause                 0.508   0.000  \n",
            "   [1]  much                  0.507   0.000  \n",
            "   [1]  money                 0.507   0.000  \n",
            "   [0]  but                   0.507   5.752  \n",
            "   [1]  it                    0.507   0.000  \n",
            "   [0]  did                   0.507   5.716  \n",
            "   [0]  cause                 0.506   9.329  \n",
            "   [0]  the                   0.506   4.025  \n",
            "   [1]  director              0.507   0.000  \n",
            "   [1]  a                     0.506   0.000  \n",
            "   [0]  lot                   0.506   6.803  \n",
            "   [0]  of                    0.506   4.609  \n",
            "   [0]  thinking              0.506   7.250  \n",
            "   [0]  to                    0.506   4.476  \n",
            " Sample 2.\n",
            "   [1]  combination           0.505   0.000  \n",
            "   [1]  of                    0.503   0.000  \n",
            "   [1]  reading               0.502   0.000  \n",
            "   [0]  the                   0.502   4.354  \n",
            "   [0]  novella               0.501   13.946 \n",
            "   [0]  and                   0.500   4.054  \n",
            "   [1]  viewing               0.499   0.000  \n",
            "   [0]  this                  0.499   4.092  \n",
            "   [0]  film                  0.499   4.689  \n",
            "   [0]  has                   0.499   5.004  \n",
            "   [0]  inspired              0.499   11.128 \n",
            "   [0]  my                    0.499   6.300  \n",
            "   [0]  wife                  0.499   8.105  \n",
            "   [0]  and                   0.499   4.059  \n",
            "   [0]  i                     0.498   3.309  \n",
            "   [0]  to                    0.498   4.762  \n",
            "   [0]  new                   0.498   5.731  \n",
            "   [1]  levels                0.499   0.000  \n",
            "   [1]  recently              0.499   0.000  \n",
            "   [1]  i                     0.499   0.000  \n",
            "Samples\n",
            "Sample 0 .  i rated this movie a 2 for showing a complete lack of effort in trying to create a quality horror\n",
            "Sample 1 .  looks like the movie didn t cause much money but it did cause the director a lot of thinking to\n",
            "Sample 2 .  combination of reading the novella and viewing this film has inspired my wife and i to new levels recently i\n",
            "\n",
            "\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8]...][[1 1 0 1 0 0 1 1 0 1]...][[29 3 0 69849 98 69849 69849 23 64 69849]...][[3 0 250 98 123 92 23 64 8 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8]...][[1 1 0 1 0 0 1 1 0 1]...][[29 3 0 69849 98 69849 69849 23 64 69849]...][[3 0 250 98 123 92 23 64 8 0]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10]...][[1 1 1 1 0 0 1 1 0 0]...][[9 139 110 30 668 69849 69849 98 8 69849]...][[139 110 30 668 3 0 98 8 10 202]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10]...][[1 1 1 1 0 0 1 1 0 0]...][[9 139 110 30 668 69849 69849 98 8 69849]...][[139 110 30 668 3 0 98 8 10 202]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            "global_step: 51\n",
            " perplexity: 8158.094\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            " percent of 3-grams captured: 0.516.\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            " percent of 2-grams captured: 0.731.\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            " percent of 4-grams captured: 0.243.\n",
            " geometric_avg: 0.451.\n",
            " arithmetic_avg: 0.496.\n",
            "global_step: 51\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69314\n",
            " G train loss: 4.51706\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10]...][[1 1 1 1 0 0 1 1 0 0]...][[9 139 110 30 668 69849 69849 98 8 69849]...][[139 110 30 668 3 0 98 8 10 202]...]\n",
            " Sample 0.\n",
            "   [1]  ve                    0.506   0.000  \n",
            "   [1]  seen                  0.508   0.000  \n",
            "   [1]  all                   0.508   0.000  \n",
            "   [1]  four                  0.507   0.000  \n",
            "   [0]  of                    0.505   5.023  \n",
            "   [0]  the                   0.504   4.437  \n",
            "   [1]  movies                0.503   0.000  \n",
            "   [1]  in                    0.503   0.000  \n",
            "   [0]  this                  0.503   5.127  \n",
            "   [0]  series                0.502   6.351  \n",
            "   [0]  each                  0.502   10.215 \n",
            "   [1]  one                   0.502   0.000  \n",
            "   [1]  strays                0.502   0.000  \n",
            "   [0]  further               0.502   10.241 \n",
            "   [1]  and                   0.501   0.000  \n",
            "   [1]  further               0.501   0.000  \n",
            "   [0]  from                  0.500   5.752  \n",
            "   [1]  the                   0.500   0.000  \n",
            "   [0]  books                 0.500   11.369 \n",
            "   [0]  this                  0.500   5.240  \n",
            " Sample 1.\n",
            "   [0]  is                    0.508   3.931  \n",
            "   [0]  a                     0.507   3.864  \n",
            "   [1]  really                0.505   0.000  \n",
            "   [1]  cool                  0.502   0.000  \n",
            "   [1]  movie                 0.500   0.000  \n",
            "   [0]  i                     0.499   3.246  \n",
            "   [1]  enjoyed               0.498   0.000  \n",
            "   [0]  it                    0.498   4.582  \n",
            "   [0]  immensely             0.499   11.364 \n",
            "   [0]  and                   0.499   4.535  \n",
            "   [0]  watch                 0.499   5.884  \n",
            "   [1]  it                    0.499   0.000  \n",
            "   [1]  every                 0.500   0.000  \n",
            "   [0]  time                  0.499   5.566  \n",
            "   [0]  it                    0.499   4.632  \n",
            "   [0]  is                    0.499   4.419  \n",
            "   [0]  on                    0.499   4.957  \n",
            "   [0]  tv                    0.499   6.311  \n",
            "   [1]  br                    0.499   0.000  \n",
            "   [0]  br                    0.499   5.146  \n",
            " Sample 2.\n",
            "   [1]  intriguing            0.503   0.000  \n",
            "   [0]  premise               0.504   7.795  \n",
            "   [1]  of                    0.503   0.000  \n",
            "   [1]  hand                  0.501   0.000  \n",
            "   [1]  drawn                 0.499   0.000  \n",
            "   [0]  fantasy               0.498   9.728  \n",
            "   [1]  come                  0.497   0.000  \n",
            "   [0]  to                    0.497   4.261  \n",
            "   [1]  life                  0.497   0.000  \n",
            "   [0]  in                    0.497   5.492  \n",
            "   [1]  a                     0.497   0.000  \n",
            "   [1]  child                 0.497   0.000  \n",
            "   [1]  s                     0.497   0.000  \n",
            "   [1]  fever                 0.498   0.000  \n",
            "   [0]  dreams                0.498   11.040 \n",
            "   [1]  however               0.497   0.000  \n",
            "   [0]  i                     0.497   3.602  \n",
            "   [1]  imagine               0.497   0.000  \n",
            "   [0]  the                   0.497   3.988  \n",
            "   [0]  average               0.497   8.785  \n",
            "Samples\n",
            "Sample 0 .  ve seen all four of the movies in this series each one strays further and further from the books this\n",
            "Sample 1 .  is a really cool movie i enjoyed it immensely and watch it every time it is on tv br br\n",
            "Sample 2 .  intriguing premise of hand drawn fantasy come to life in a child s fever dreams however i imagine the average\n",
            "\n",
            "\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745]...][[0 1 0 1 0 0 0 0 0 1]...][[10 69849 343 69849 451 69849 69849 69849 69849 69849]...][[2789 343 201 451 55 4 91 6380 7745 1]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745]...][[0 1 0 1 0 0 0 0 0 1]...][[10 69849 343 69849 451 69849 69849 69849 69849 69849]...][[2789 343 201 451 55 4 91 6380 7745 1]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8]...][[1 0 1 0 1 0 1 1 0 1]...][[29 3 69849 250 69849 123 69849 23 64 69849]...][[3 0 250 98 123 92 23 64 8 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8]...][[1 0 1 0 1 0 1 1 0 1]...][[29 3 69849 250 69849 123 69849 23 64 69849]...][[3 0 250 98 123 92 23 64 8 0]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            "global_step: 54\n",
            " perplexity: 7432.493\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            " percent of 3-grams captured: 0.500.\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            " percent of 2-grams captured: 0.705.\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            " percent of 4-grams captured: 0.258.\n",
            " geometric_avg: 0.450.\n",
            " arithmetic_avg: 0.488.\n",
            "global_step: 54\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69327\n",
            " G train loss: 4.47440\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8]...][[1 0 1 0 1 0 1 1 0 1]...][[29 3 69849 250 69849 123 69849 23 64 69849]...][[3 0 250 98 123 92 23 64 8 0]...]\n",
            " Sample 0.\n",
            "   [1]  of                    0.520   0.000  \n",
            "   [0]  the                   0.520   3.608  \n",
            "   [1]  worst                 0.518   0.000  \n",
            "   [0]  movies                0.516   5.808  \n",
            "   [1]  ever                  0.514   0.000  \n",
            "   [0]  made                  0.514   6.356  \n",
            "   [1]  not                   0.513   0.000  \n",
            "   [1]  only                  0.512   0.000  \n",
            "   [0]  in                    0.512   5.124  \n",
            "   [1]  the                   0.512   0.000  \n",
            "   [0]  sf                    0.512   14.041 \n",
            "   [0]  genre                 0.512   6.785  \n",
            "   [0]  which                 0.512   6.110  \n",
            "   [1]  includes              0.512   0.000  \n",
            "   [0]  an                    0.511   5.794  \n",
            "   [0]  amazing               0.511   7.293  \n",
            "   [0]  array                 0.511   13.897 \n",
            "   [1]  of                    0.511   0.000  \n",
            "   [1]  cinematic             0.511   0.000  \n",
            "   [1]  dung                  0.512   0.000  \n",
            " Sample 1.\n",
            "   [1]  a                     0.508   0.000  \n",
            "   [0]  name                  0.504   8.042  \n",
            "   [0]  like                  0.501   5.923  \n",
            "   [1]  karate                0.499   0.000  \n",
            "   [1]  cop                   0.498   0.000  \n",
            "   [0]  what                  0.497   6.484  \n",
            "   [0]  are                   0.497   5.128  \n",
            "   [0]  you                   0.497   5.785  \n",
            "   [1]  really                0.497   0.000  \n",
            "   [1]  expecting             0.497   0.000  \n",
            "   [0]  this                  0.498   3.895  \n",
            "   [1]  movie                 0.499   0.000  \n",
            "   [1]  is                    0.499   0.000  \n",
            "   [0]  a                     0.499   3.971  \n",
            "   [1]  perfect               0.499   0.000  \n",
            "   [1]  example               0.499   0.000  \n",
            "   [1]  of                    0.499   0.000  \n",
            "   [0]  a                     0.498   3.992  \n",
            "   [0]  brainless             0.498   13.626 \n",
            "   [0]  bizarre               0.498   12.207 \n",
            " Sample 2.\n",
            "   [1]  chaney                0.516   0.000  \n",
            "   [0]  as                    0.514   4.805  \n",
            "   [1]  alonzo                0.513   0.000  \n",
            "   [0]  the                   0.512   3.479  \n",
            "   [1]  armless               0.511   0.000  \n",
            "   [1]  is                    0.511   0.000  \n",
            "   [1]  a                     0.511   0.000  \n",
            "   [0]  criminal              0.511   14.785 \n",
            "   [0]  posing                0.511   14.596 \n",
            "   [1]  as                    0.511   0.000  \n",
            "   [0]  an                    0.511   6.006  \n",
            "   [1]  armless               0.511   0.000  \n",
            "   [1]  circus                0.511   0.000  \n",
            "   [1]  knife                 0.511   0.000  \n",
            "   [1]  wielder               0.510   0.000  \n",
            "   [1]  he                    0.510   0.000  \n",
            "   [0]  amazes                0.510   14.050 \n",
            "   [0]  by                    0.510   4.603  \n",
            "   [1]  throwing              0.511   0.000  \n",
            "   [1]  knives                0.511   0.000  \n",
            "Samples\n",
            "Sample 0 .  of the worst movies ever made not only in the sf genre which includes an amazing array of cinematic dung\n",
            "Sample 1 .  a name like karate cop what are you really expecting this movie is a perfect example of a brainless bizarre\n",
            "Sample 2 .  chaney as alonzo the armless is a criminal posing as an armless circus knife wielder he amazes by throwing knives\n",
            "\n",
            "\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868]...][[0 1 1 1 0 1 0 1 1 0]...][[10 69849 32756 199 3895 69849 436 69849 4242 868]...][[17 32756 199 3895 5998 436 1 4242 868 6]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868]...][[0 1 1 1 0 1 0 1 1 0]...][[10 69849 32756 199 3895 69849 436 69849 4242 868]...][[17 32756 199 3895 5998 436 1 4242 868 6]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745]...][[0 1 0 1 0 0 1 0 0 1]...][[10 69849 343 69849 451 69849 69849 91 69849 69849]...][[2789 343 201 451 55 4 91 6380 7745 1]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745]...][[0 1 0 1 0 0 1 0 0 1]...][[10 69849 343 69849 451 69849 69849 91 69849 69849]...][[2789 343 201 451 55 4 91 6380 7745 1]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            "global_step: 57\n",
            " perplexity: 6836.775\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            " percent of 3-grams captured: 0.518.\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            " percent of 2-grams captured: 0.716.\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            " percent of 4-grams captured: 0.249.\n",
            " geometric_avg: 0.452.\n",
            " arithmetic_avg: 0.494.\n",
            "global_step: 57\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69335\n",
            " G train loss: 4.43235\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745]...][[0 1 0 1 0 0 1 0 0 1]...][[10 69849 343 69849 451 69849 69849 91 69849 69849]...][[2789 343 201 451 55 4 91 6380 7745 1]...]\n",
            " Sample 0.\n",
            "   [0]  indie                 0.523   12.266 \n",
            "   [1]  sex                   0.515   0.000  \n",
            "   [0]  comedy                0.510   6.737  \n",
            "   [1]  lives                 0.507   0.000  \n",
            "   [0]  up                    0.507   6.091  \n",
            "   [0]  to                    0.507   4.671  \n",
            "   [1]  its                   0.507   0.000  \n",
            "   [0]  billing               0.508   12.944 \n",
            "   [0]  kisses                0.510   12.680 \n",
            "   [1]  and                   0.511   0.000  \n",
            "   [1]  caroms                0.511   0.000  \n",
            "   [1]  builds                0.512   0.000  \n",
            "   [0]  up                    0.513   6.188  \n",
            "   [1]  to                    0.513   0.000  \n",
            "   [1]  a                     0.513   0.000  \n",
            "   [0]  humorous              0.514   12.406 \n",
            "   [0]  and                   0.514   4.530  \n",
            "   [0]  satisfactory          0.514   12.201 \n",
            "   [0]  climax                0.513   12.381 \n",
            "   [0]  there                 0.513   6.087  \n",
            " Sample 1.\n",
            "   [1]  s                     0.501   0.000  \n",
            "   [0]  probably              0.500   6.466  \n",
            "   [1]  no                    0.499   0.000  \n",
            "   [0]  point                 0.498   6.776  \n",
            "   [0]  in                    0.498   5.127  \n",
            "   [1]  trying                0.498   0.000  \n",
            "   [1]  to                    0.498   0.000  \n",
            "   [0]  describe              0.498   11.049 \n",
            "   [1]  aqua                  0.497   0.000  \n",
            "   [1]  teen                  0.497   0.000  \n",
            "   [1]  hunger                0.497   0.000  \n",
            "   [0]  force                 0.497   9.988  \n",
            "   [0]  colon                 0.496   13.437 \n",
            "   [0]  movie                 0.496   4.121  \n",
            "   [0]  to                    0.496   4.505  \n",
            "   [1]  anyone                0.496   0.000  \n",
            "   [1]  who                   0.497   0.000  \n",
            "   [1]  hasn                  0.496   0.000  \n",
            "   [0]  t                     0.496   4.486  \n",
            "   [0]  already               0.496   8.306  \n",
            " Sample 2.\n",
            "   [1]  wont                  0.531   0.000  \n",
            "   [1]  believe               0.526   0.000  \n",
            "   [1]  this                  0.520   0.000  \n",
            "   [1]  i                     0.515   0.000  \n",
            "   [0]  was                   0.513   4.749  \n",
            "   [0]  16                    0.512   13.309 \n",
            "   [0]  when                  0.513   5.791  \n",
            "   [0]  i                     0.513   3.463  \n",
            "   [1]  saw                   0.513   0.000  \n",
            "   [0]  so                    0.514   5.226  \n",
            "   [1]  proudly               0.514   0.000  \n",
            "   [0]  we                    0.514   7.610  \n",
            "   [1]  hail                  0.514   0.000  \n",
            "   [0]  thirty                0.514   12.216 \n",
            "   [0]  years                 0.515   6.619  \n",
            "   [1]  later                 0.515   0.000  \n",
            "   [0]  i                     0.514   3.608  \n",
            "   [0]  it                    0.514   4.817  \n",
            "   [0]  and                   0.514   4.214  \n",
            "   [0]  remembered            0.514   12.842 \n",
            "Samples\n",
            "Sample 0 .  indie sex comedy lives up to its billing kisses and caroms builds up to a humorous and satisfactory climax there\n",
            "Sample 1 .  s probably no point in trying to describe aqua teen hunger force colon movie to anyone who hasn t already\n",
            "Sample 2 .  wont believe this i was 16 when i saw so proudly we hail thirty years later i it and remembered\n",
            "\n",
            "\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340]...][[0 1 1 0 1 0 1 1 1 1]...][[9 69849 2 340 69849 193 69849 5920 257 14340]...][[140 2 340 3 193 156 5920 257 14340 1]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340]...][[0 1 1 0 1 0 1 1 1 1]...][[9 69849 2 340 69849 193 69849 5920 257 14340]...][[140 2 340 3 193 156 5920 257 14340 1]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868]...][[0 1 0 1 1 0 1 1 1 1]...][[10 69849 32756 69849 3895 5998 69849 1 4242 868]...][[17 32756 199 3895 5998 436 1 4242 868 6]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868]...][[0 1 0 1 1 0 1 1 1 1]...][[10 69849 32756 69849 3895 5998 69849 1 4242 868]...][[17 32756 199 3895 5998 436 1 4242 868 6]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            "global_step: 60\n",
            " perplexity: 6329.000\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            " percent of 3-grams captured: 0.490.\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            " percent of 2-grams captured: 0.716.\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            " percent of 4-grams captured: 0.227.\n",
            " geometric_avg: 0.430.\n",
            " arithmetic_avg: 0.478.\n",
            "global_step: 60\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69339\n",
            " G train loss: 4.39441\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868]...][[0 1 0 1 1 0 1 1 1 1]...][[10 69849 32756 69849 3895 5998 69849 1 4242 868]...][[17 32756 199 3895 5998 436 1 4242 868 6]...]\n",
            " Sample 0.\n",
            "   [0]  movie                 0.496   3.851  \n",
            "   [1]  alternated            0.491   0.000  \n",
            "   [0]  between               0.489   7.467  \n",
            "   [1]  captivating           0.489   0.000  \n",
            "   [1]  wine                  0.489   0.000  \n",
            "   [0]  drama                 0.490   7.107  \n",
            "   [1]  and                   0.491   0.000  \n",
            "   [1]  formulaic             0.492   0.000  \n",
            "   [1]  romance               0.492   0.000  \n",
            "   [1]  br                    0.493   0.000  \n",
            "   [1]  br                    0.494   0.000  \n",
            "   [1]  the                   0.495   0.000  \n",
            "   [0]  wine                  0.495   13.807 \n",
            "   [1]  drama                 0.495   0.000  \n",
            "   [1]  part                  0.495   0.000  \n",
            "   [0]  was                   0.495   4.631  \n",
            "   [1]  held                  0.495   0.000  \n",
            "   [0]  together              0.495   8.098  \n",
            "   [0]  by                    0.495   5.874  \n",
            "   [1]  the                   0.495   0.000  \n",
            " Sample 1.\n",
            "   [0]  with                  0.501   6.250  \n",
            "   [0]  the                   0.501   3.351  \n",
            "   [1]  gun                   0.501   0.000  \n",
            "   [0]  is                    0.500   3.626  \n",
            "   [0]  pretty                0.498   5.987  \n",
            "   [0]  much                  0.497   6.655  \n",
            "   [0]  forgotten             0.496   12.818 \n",
            "   [1]  now                   0.496   0.000  \n",
            "   [0]  but                   0.496   5.345  \n",
            "   [0]  caused                0.496   13.662 \n",
            "   [1]  a                     0.497   0.000  \n",
            "   [1]  minor                 0.497   0.000  \n",
            "   [1]  storm                 0.497   0.000  \n",
            "   [0]  of                    0.497   4.230  \n",
            "   [1]  media                 0.497   0.000  \n",
            "   [0]  interest              0.497   8.178  \n",
            "   [1]  back                  0.498   0.000  \n",
            "   [1]  in                    0.498   0.000  \n",
            "   [0]  1955                  0.498   12.980 \n",
            "   [1]  when                  0.498   0.000  \n",
            " Sample 2.\n",
            "   [1]  bodyguard             0.533   0.000  \n",
            "   [0]  is                    0.532   3.336  \n",
            "   [0]  fundamentally         0.529   13.018 \n",
            "   [1]  a                     0.526   0.000  \n",
            "   [0]  love                  0.523   6.670  \n",
            "   [1]  story                 0.522   0.000  \n",
            "   [0]  between               0.521   7.572  \n",
            "   [0]  two                   0.520   6.654  \n",
            "   [0]  contrasting           0.519   13.365 \n",
            "   [1]  characters            0.519   0.000  \n",
            "   [0]  with                  0.519   6.081  \n",
            "   [0]  their                 0.519   6.665  \n",
            "   [0]  own                   0.519   6.853  \n",
            "   [0]  idiosyncrasies        0.519   12.899 \n",
            "   [0]  the                   0.520   3.637  \n",
            "   [1]  plot                  0.521   0.000  \n",
            "   [0]  is                    0.521   3.908  \n",
            "   [0]  intriguing            0.521   10.984 \n",
            "   [1]  enough                0.521   0.000  \n",
            "   [0]  to                    0.521   4.193  \n",
            "Samples\n",
            "Sample 0 .  movie alternated between captivating wine drama and formulaic romance br br the wine drama part was held together by the\n",
            "Sample 1 .  with the gun is pretty much forgotten now but caused a minor storm of media interest back in 1955 when\n",
            "Sample 2 .  bodyguard is fundamentally a love story between two contrasting characters with their own idiosyncrasies the plot is intriguing enough to\n",
            "\n",
            "\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17]...][[1 1 0 0 1 1 1 1 0 1]...][[16 2 366 69849 69849 23 395 149 10 69849]...][[2 366 9 80 23 395 149 10 17 8386]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17]...][[1 1 0 0 1 1 1 1 0 1]...][[16 2 366 69849 69849 23 395 149 10 69849]...][[2 366 9 80 23 395 149 10 17 8386]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340]...][[0 0 1 1 0 0 1 0 1 1]...][[9 69849 69849 340 3 69849 69849 5920 69849 14340]...][[140 2 340 3 193 156 5920 257 14340 1]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340]...][[0 0 1 1 0 0 1 0 1 1]...][[9 69849 69849 340 3 69849 69849 5920 69849 14340]...][[140 2 340 3 193 156 5920 257 14340 1]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            "global_step: 63\n",
            " perplexity: 5846.130\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            " percent of 3-grams captured: 0.512.\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            " percent of 2-grams captured: 0.696.\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            " percent of 4-grams captured: 0.250.\n",
            " geometric_avg: 0.447.\n",
            " arithmetic_avg: 0.486.\n",
            "global_step: 63\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69341\n",
            " G train loss: 4.35349\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340]...][[0 0 1 1 0 0 1 0 1 1]...][[9 69849 69849 340 3 69849 69849 5920 69849 14340]...][[140 2 340 3 193 156 5920 257 14340 1]...]\n",
            " Sample 0.\n",
            "   [0]  m                     0.501   6.800  \n",
            "   [0]  a                     0.500   3.490  \n",
            "   [1]  fan                   0.499   0.000  \n",
            "   [1]  of                    0.499   0.000  \n",
            "   [0]  both                  0.498   7.125  \n",
            "   [0]  actors                0.498   6.380  \n",
            "   [1]  singers               0.498   0.000  \n",
            "   [0]  especially            0.497   8.050  \n",
            "   [1]  gackt                 0.497   0.000  \n",
            "   [1]  and                   0.497   0.000  \n",
            "   [1]  when                  0.497   0.000  \n",
            "   [0]  i                     0.497   3.511  \n",
            "   [1]  first                 0.497   0.000  \n",
            "   [1]  discover              0.497   0.000  \n",
            "   [1]  this                  0.497   0.000  \n",
            "   [1]  movie                 0.498   0.000  \n",
            "   [1]  and                   0.498   0.000  \n",
            "   [1]  watch                 0.498   0.000  \n",
            "   [1]  the                   0.498   0.000  \n",
            "   [0]  trailer               0.498   8.882  \n",
            " Sample 1.\n",
            "   [1]  off                   0.497   0.000  \n",
            "   [1]  i                     0.497   0.000  \n",
            "   [1]  never                 0.496   0.000  \n",
            "   [1]  take                  0.496   0.000  \n",
            "   [1]  the                   0.496   0.000  \n",
            "   [0]  time                  0.495   6.555  \n",
            "   [1]  to                    0.495   0.000  \n",
            "   [1]  actually              0.495   0.000  \n",
            "   [1]  write                 0.495   0.000  \n",
            "   [1]  a                     0.496   0.000  \n",
            "   [1]  review                0.496   0.000  \n",
            "   [1]  of                    0.495   0.000  \n",
            "   [1]  any                   0.495   0.000  \n",
            "   [0]  of                    0.494   4.563  \n",
            "   [1]  the                   0.494   0.000  \n",
            "   [1]  movies                0.495   0.000  \n",
            "   [1]  i                     0.495   0.000  \n",
            "   [1]  have                  0.495   0.000  \n",
            "   [1]  seen                  0.495   0.000  \n",
            "   [1]  but                   0.495   0.000  \n",
            " Sample 2.\n",
            "   [0]  bodyguard             0.510   13.375 \n",
            "   [0]  in                    0.509   5.076  \n",
            "   [1]  my                    0.506   0.000  \n",
            "   [1]  opinion               0.503   0.000  \n",
            "   [1]  is                    0.501   0.000  \n",
            "   [0]  a                     0.500   3.785  \n",
            "   [1]  great                 0.499   0.000  \n",
            "   [1]  whitney               0.499   0.000  \n",
            "   [0]  houston               0.499   12.735 \n",
            "   [1]  movie                 0.499   0.000  \n",
            "   [0]  one                   0.499   5.540  \n",
            "   [1]  of                    0.499   0.000  \n",
            "   [0]  the                   0.499   3.805  \n",
            "   [0]  greatest              0.499   7.729  \n",
            "   [0]  things                0.498   7.796  \n",
            "   [0]  about                 0.498   5.906  \n",
            "   [1]  this                  0.499   0.000  \n",
            "   [0]  movie                 0.499   4.213  \n",
            "   [0]  in                    0.499   5.144  \n",
            "   [1]  my                    0.500   0.000  \n",
            "Samples\n",
            "Sample 0 .  m a fan of both actors singers especially gackt and when i first discover this movie and watch the trailer\n",
            "Sample 1 .  off i never take the time to actually write a review of any of the movies i have seen but\n",
            "Sample 2 .  bodyguard in my opinion is a great whitney houston movie one of the greatest things about this movie in my\n",
            "\n",
            "\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0]...][[1 0 1 0 0 0 0 0 1 0]...][[8 47 69849 1640 69849 69849 69849 69849 69849 0]...][[47 5 1640 36433 117 166 4 1309 0 4853]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0]...][[1 0 1 0 0 0 0 0 1 0]...][[8 47 69849 1640 69849 69849 69849 69849 69849 0]...][[47 5 1640 36433 117 166 4 1309 0 4853]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17]...][[1 1 0 0 0 0 1 0 0 1]...][[16 2 366 69849 69849 69849 69849 149 69849 69849]...][[2 366 9 80 23 395 149 10 17 8386]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17]...][[1 1 0 0 0 0 1 0 0 1]...][[16 2 366 69849 69849 69849 69849 149 69849 69849]...][[2 366 9 80 23 395 149 10 17 8386]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            "global_step: 66\n",
            " perplexity: 5432.035\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            " percent of 3-grams captured: 0.500.\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            " percent of 2-grams captured: 0.713.\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            " percent of 4-grams captured: 0.238.\n",
            " geometric_avg: 0.439.\n",
            " arithmetic_avg: 0.483.\n",
            "global_step: 66\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69341\n",
            " G train loss: 4.31807\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17]...][[1 1 0 0 0 0 1 0 0 1]...][[16 2 366 69849 69849 69849 69849 149 69849 69849]...][[2 366 9 80 23 395 149 10 17 8386]...]\n",
            " Sample 0.\n",
            "   [1]  a                     0.525   0.000  \n",
            "   [1]  start                 0.529   0.000  \n",
            "   [0]  i                     0.530   3.983  \n",
            "   [0]  do                    0.532   7.253  \n",
            "   [0]  not                   0.532   5.189  \n",
            "   [0]  recommend             0.531   8.056  \n",
            "   [1]  watching              0.531   0.000  \n",
            "   [0]  this                  0.531   4.209  \n",
            "   [0]  movie                 0.531   4.399  \n",
            "   [1]  sober                 0.530   0.000  \n",
            "   [0]  the                   0.530   3.413  \n",
            "   [1]  movie                 0.530   0.000  \n",
            "   [1]  follows               0.530   0.000  \n",
            "   [0]  the                   0.530   3.431  \n",
            "   [1]  battle                0.530   0.000  \n",
            "   [0]  for                   0.531   5.093  \n",
            "   [0]  the                   0.531   3.441  \n",
            "   [0]  golden                0.531   11.698 \n",
            "   [1]  ninja                 0.531   0.000  \n",
            "   [0]  warrior               0.531   12.909 \n",
            " Sample 1.\n",
            "   [1]  poehler               0.531   0.000  \n",
            "   [1]  and                   0.527   0.000  \n",
            "   [0]  rachel                0.522   9.391  \n",
            "   [1]  dratch                0.518   0.000  \n",
            "   [1]  are                   0.517   0.000  \n",
            "   [1]  among                 0.516   0.000  \n",
            "   [0]  the                   0.515   3.631  \n",
            "   [0]  funnier               0.515   11.625 \n",
            "   [0]  women                 0.515   7.696  \n",
            "   [1]  to                    0.515   0.000  \n",
            "   [0]  have                  0.516   5.846  \n",
            "   [0]  been                  0.516   6.459  \n",
            "   [0]  on                    0.516   5.607  \n",
            "   [1]  saturday              0.516   0.000  \n",
            "   [0]  night                 0.516   6.725  \n",
            "   [0]  live                  0.516   7.600  \n",
            "   [0]  it                    0.517   4.011  \n",
            "   [0]  s                     0.517   4.736  \n",
            "   [0]  unfortunate           0.517   12.296 \n",
            "   [0]  that                  0.517   4.684  \n",
            " Sample 2.\n",
            "   [1]  movie                 0.528   0.000  \n",
            "   [0]  was                   0.527   4.725  \n",
            "   [0]  such                  0.525   7.415  \n",
            "   [0]  a                     0.524   3.778  \n",
            "   [0]  blast                 0.523   12.433 \n",
            "   [0]  it                    0.522   3.750  \n",
            "   [0]  has                   0.522   6.295  \n",
            "   [0]  that                  0.522   4.628  \n",
            "   [1]  feel                  0.522   0.000  \n",
            "   [1]  good                  0.522   0.000  \n",
            "   [0]  yet                   0.522   6.776  \n",
            "   [1]  totally               0.521   0.000  \n",
            "   [0]  in                    0.521   4.988  \n",
            "   [1]  your                  0.521   0.000  \n",
            "   [0]  face                  0.521   8.419  \n",
            "   [1]  attitude              0.521   0.000  \n",
            "   [1]  that                  0.521   0.000  \n",
            "   [0]  draws                 0.521   8.592  \n",
            "   [1]  me                    0.521   0.000  \n",
            "   [1]  to                    0.521   0.000  \n",
            "Samples\n",
            "Sample 0 .  a start i do not recommend watching this movie sober the movie follows the battle for the golden ninja warrior\n",
            "Sample 1 .  poehler and rachel dratch are among the funnier women to have been on saturday night live it s unfortunate that\n",
            "Sample 2 .  movie was such a blast it has that feel good yet totally in your face attitude that draws me to\n",
            "\n",
            "\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753]...][[0 1 1 1 1 0 1 1 0 1]...][[10 69849 5 677 6798 1753 69849 4924 15 69849]...][[17 5 677 6798 1753 4924 4924 15 1753 1761]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753]...][[0 1 1 1 1 0 1 1 0 1]...][[10 69849 5 677 6798 1753 69849 4924 15 69849]...][[17 5 677 6798 1753 4924 4924 15 1753 1761]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0]...][[0 1 1 0 1 0 0 1 1 1]...][[8 69849 5 1640 69849 117 69849 69849 1309 0]...][[47 5 1640 36433 117 166 4 1309 0 4853]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0]...][[0 1 1 0 1 0 0 1 1 1]...][[8 69849 5 1640 69849 117 69849 69849 1309 0]...][[47 5 1640 36433 117 166 4 1309 0 4853]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            "global_step: 69\n",
            " perplexity: 5084.863\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            " percent of 3-grams captured: 0.518.\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            " percent of 2-grams captured: 0.718.\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            " percent of 4-grams captured: 0.254.\n",
            " geometric_avg: 0.456.\n",
            " arithmetic_avg: 0.497.\n",
            "global_step: 69\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69343\n",
            " G train loss: 4.28496\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0]...][[0 1 1 0 1 0 0 1 1 1]...][[8 69849 5 1640 69849 117 69849 69849 1309 0]...][[47 5 1640 36433 117 166 4 1309 0 4853]...]\n",
            " Sample 0.\n",
            "   [0]  what                  0.518   6.349  \n",
            "   [1]  is                    0.512   0.000  \n",
            "   [1]  adam                  0.506   0.000  \n",
            "   [0]  sandlers              0.501   12.436 \n",
            "   [1]  best                  0.499   0.000  \n",
            "   [0]  work                  0.497   7.755  \n",
            "   [0]  to                    0.497   4.634  \n",
            "   [1]  date                  0.498   0.000  \n",
            "   [1]  the                   0.498   0.000  \n",
            "   [1]  hopeless              0.498   0.000  \n",
            "   [0]  hockey                0.499   12.192 \n",
            "   [0]  failure               0.499   9.956  \n",
            "   [1]  happy                 0.500   0.000  \n",
            "   [1]  gilmore               0.499   0.000  \n",
            "   [0]  must                  0.499   7.550  \n",
            "   [0]  join                  0.499   12.373 \n",
            "   [0]  the                   0.498   3.898  \n",
            "   [0]  professional          0.498   12.033 \n",
            "   [0]  golfers               0.498   12.091 \n",
            "   [1]  tour                  0.498   0.000  \n",
            " Sample 1.\n",
            "   [0]  is                    0.487   3.112  \n",
            "   [0]  by                    0.484   6.380  \n",
            "   [1]  far                   0.482   0.000  \n",
            "   [1]  the                   0.481   0.000  \n",
            "   [0]  greatest              0.480   7.620  \n",
            "   [0]  surprise              0.481   7.901  \n",
            "   [0]  in                    0.482   4.498  \n",
            "   [0]  german                0.483   8.201  \n",
            "   [1]  postwar               0.484   0.000  \n",
            "   [0]  film                  0.484   4.582  \n",
            "   [1]  production            0.485   0.000  \n",
            "   [0]  directed              0.485   7.312  \n",
            "   [0]  by                    0.486   6.317  \n",
            "   [1]  the                   0.486   0.000  \n",
            "   [1]  emerging              0.486   0.000  \n",
            "   [0]  mr                    0.486   9.802  \n",
            "   [1]  till                  0.485   0.000  \n",
            "   [1]  schweiger             0.485   0.000  \n",
            "   [0]  who                   0.485   6.081  \n",
            "   [0]  usually               0.485   7.171  \n",
            " Sample 2.\n",
            "   [0]  is                    0.498   3.050  \n",
            "   [1]  not                   0.499   0.000  \n",
            "   [0]  jess                  0.499   12.820 \n",
            "   [0]  franco                0.499   10.067 \n",
            "   [0]  s                     0.498   4.801  \n",
            "   [1]  best                  0.499   0.000  \n",
            "   [1]  film                  0.499   0.000  \n",
            "   [1]  but                   0.499   0.000  \n",
            "   [1]  it                    0.499   0.000  \n",
            "   [0]  s                     0.500   4.886  \n",
            "   [1]  certainly             0.500   0.000  \n",
            "   [0]  not                   0.501   5.382  \n",
            "   [1]  the                   0.501   0.000  \n",
            "   [0]  worst                 0.502   6.811  \n",
            "   [0]  either                0.502   8.113  \n",
            "   [0]  which                 0.502   6.604  \n",
            "   [0]  doesn                 0.502   6.944  \n",
            "   [0]  t                     0.502   5.968  \n",
            "   [0]  say                   0.502   6.191  \n",
            "   [1]  much                  0.501   0.000  \n",
            "Samples\n",
            "Sample 0 .  what is adam sandlers best work to date the hopeless hockey failure happy gilmore must join the professional golfers tour\n",
            "Sample 1 .  is by far the greatest surprise in german postwar film production directed by the emerging mr till schweiger who usually\n",
            "Sample 2 .  is not jess franco s best film but it s certainly not the worst either which doesn t say much\n",
            "\n",
            "\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25]...][[0 0 0 0 1 1 1 1 0 0]...][[9 69849 69849 69849 69849 884 127 738 34 69849]...][[509 127 738 255 884 127 738 34 25 23]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25]...][[0 0 0 0 1 1 1 1 0 0]...][[9 69849 69849 69849 69849 884 127 738 34 69849]...][[509 127 738 255 884 127 738 34 25 23]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753]...][[0 0 0 1 0 0 1 1 0 0]...][[10 69849 69849 69849 6798 69849 69849 4924 15 69849]...][[17 5 677 6798 1753 4924 4924 15 1753 1761]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753]...][[0 0 0 1 0 0 1 1 0 0]...][[10 69849 69849 69849 6798 69849 69849 4924 15 69849]...][[17 5 677 6798 1753 4924 4924 15 1753 1761]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            "global_step: 72\n",
            " perplexity: 4785.656\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            " percent of 3-grams captured: 0.499.\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            " percent of 2-grams captured: 0.710.\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            " percent of 4-grams captured: 0.239.\n",
            " geometric_avg: 0.439.\n",
            " arithmetic_avg: 0.483.\n",
            "global_step: 72\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69335\n",
            " G train loss: 4.25252\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753]...][[0 0 0 1 0 0 1 1 0 0]...][[10 69849 69849 69849 6798 69849 69849 4924 15 69849]...][[17 5 677 6798 1753 4924 4924 15 1753 1761]...]\n",
            " Sample 0.\n",
            "   [0]  movie                 0.560   3.687  \n",
            "   [0]  is                    0.556   3.563  \n",
            "   [0]  straight              0.553   9.166  \n",
            "   [1]  pimp                  0.550   0.000  \n",
            "   [0]  guns                  0.549   12.327 \n",
            "   [0]  chicks                0.547   10.821 \n",
            "   [1]  chicks                0.547   0.000  \n",
            "   [1]  with                  0.547   0.000  \n",
            "   [0]  guns                  0.547   12.320 \n",
            "   [0]  players               0.546   12.312 \n",
            "   [1]  white                 0.546   0.000  \n",
            "   [1]  cops                  0.546   0.000  \n",
            "   [1]  getting               0.546   0.000  \n",
            "   [0]  spanked               0.545   12.754 \n",
            "   [1]  by                    0.546   0.000  \n",
            "   [0]  black                 0.546   8.133  \n",
            "   [1]  chicks                0.546   0.000  \n",
            "   [0]  who                   0.546   6.130  \n",
            "   [1]  carry                 0.546   0.000  \n",
            "   [0]  guns                  0.547   12.313 \n",
            " Sample 1.\n",
            "   [1]  movie                 0.484   0.000  \n",
            "   [1]  was                   0.481   0.000  \n",
            "   [1]  awful                 0.478   0.000  \n",
            "   [1]  although              0.478   0.000  \n",
            "   [1]  i                     0.478   0.000  \n",
            "   [0]  have                  0.479   5.847  \n",
            "   [0]  seen                  0.480   6.661  \n",
            "   [0]  some                  0.481   6.657  \n",
            "   [1]  worse                 0.481   0.000  \n",
            "   [0]  movies                0.481   5.707  \n",
            "   [0]  before                0.481   7.685  \n",
            "   [1]  the                   0.481   0.000  \n",
            "   [1]  imp                   0.481   0.000  \n",
            "   [1]  is                    0.481   0.000  \n",
            "   [0]  the                   0.482   2.986  \n",
            "   [1]  most                  0.482   0.000  \n",
            "   [1]  annoying              0.483   0.000  \n",
            "   [1]  character             0.483   0.000  \n",
            "   [0]  even                  0.483   6.487  \n",
            "   [0]  though                0.483   6.911  \n",
            " Sample 2.\n",
            "   [0]  cheesy                0.471   7.535  \n",
            "   [0]  direct                0.471   9.895  \n",
            "   [0]  to                    0.471   4.407  \n",
            "   [0]  video                 0.471   6.519  \n",
            "   [1]  b                     0.471   0.000  \n",
            "   [0]  movie                 0.470   3.639  \n",
            "   [1]  although              0.470   0.000  \n",
            "   [0]  the                   0.470   2.982  \n",
            "   [1]  acting                0.470   0.000  \n",
            "   [0]  was                   0.470   4.483  \n",
            "   [0]  pretty                0.470   7.148  \n",
            "   [0]  good                  0.471   5.454  \n",
            "   [0]  there                 0.470   6.805  \n",
            "   [1]  were                  0.470   0.000  \n",
            "   [0]  a                     0.470   3.393  \n",
            "   [1]  lot                   0.470   0.000  \n",
            "   [1]  of                    0.470   0.000  \n",
            "   [0]  first                 0.469   5.615  \n",
            "   [1]  rate                  0.469   0.000  \n",
            "   [0]  actors                0.469   7.066  \n",
            "Samples\n",
            "Sample 0 .  movie is straight pimp guns chicks chicks with guns players white cops getting spanked by black chicks who carry guns\n",
            "Sample 1 .  movie was awful although i have seen some worse movies before the imp is the most annoying character even though\n",
            "Sample 2 .  cheesy direct to video b movie although the acting was pretty good there were a lot of first rate actors\n",
            "\n",
            "\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115]...][[0 0 1 1 0 0 1 1 1 1]...][[1840 69849 69849 368 755 69849 69849 2 368 1115]...][[1943 12 368 755 654 78 2 368 1115 19]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115]...][[0 0 1 1 0 0 1 1 1 1]...][[1840 69849 69849 368 755 69849 69849 2 368 1115]...][[1943 12 368 755 654 78 2 368 1115 19]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25]...][[1 0 0 0 0 1 1 0 1 1]...][[9 509 69849 69849 69849 69849 127 738 69849 25]...][[509 127 738 255 884 127 738 34 25 23]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25]...][[1 0 0 0 0 1 1 0 1 1]...][[9 509 69849 69849 69849 69849 127 738 69849 25]...][[509 127 738 255 884 127 738 34 25 23]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            "global_step: 75\n",
            " perplexity: 4517.186\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            " percent of 3-grams captured: 0.509.\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            " percent of 2-grams captured: 0.718.\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            " percent of 4-grams captured: 0.253.\n",
            " geometric_avg: 0.452.\n",
            " arithmetic_avg: 0.494.\n",
            "global_step: 75\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69338\n",
            " G train loss: 4.22326\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25]...][[1 0 0 0 0 1 1 0 1 1]...][[9 509 69849 69849 69849 69849 127 738 69849 25]...][[509 127 738 255 884 127 738 34 25 23]...]\n",
            " Sample 0.\n",
            "   [1]  enjoyed               0.501   0.000  \n",
            "   [0]  your                  0.500   7.840  \n",
            "   [0]  review                0.499   7.311  \n",
            "   [0]  anyone                0.498   7.749  \n",
            "   [0]  reading               0.498   9.246  \n",
            "   [1]  your                  0.497   0.000  \n",
            "   [1]  review                0.497   0.000  \n",
            "   [0]  who                   0.497   6.657  \n",
            "   [1]  are                   0.497   0.000  \n",
            "   [1]  not                   0.496   0.000  \n",
            "   [0]  consumers             0.496   12.626 \n",
            "   [1]  of                    0.495   0.000  \n",
            "   [1]  metal                 0.495   0.000  \n",
            "   [1]  will                  0.495   0.000  \n",
            "   [1]  learn                 0.495   0.000  \n",
            "   [1]  that                  0.496   0.000  \n",
            "   [1]  this                  0.496   0.000  \n",
            "   [1]  topic                 0.496   0.000  \n",
            "   [1]  could                 0.496   0.000  \n",
            "   [0]  easily                0.496   7.766  \n",
            " Sample 1.\n",
            "   [1]  saw                   0.473   0.000  \n",
            "   [1]  a                     0.470   0.000  \n",
            "   [1]  bit                   0.470   0.000  \n",
            "   [0]  of                    0.470   3.860  \n",
            "   [0]  this                  0.471   3.069  \n",
            "   [1]  last                  0.472   0.000  \n",
            "   [1]  year                  0.473   0.000  \n",
            "   [0]  and                   0.474   3.524  \n",
            "   [0]  all                   0.476   6.191  \n",
            "   [0]  i                     0.476   3.510  \n",
            "   [1]  can                   0.477   0.000  \n",
            "   [0]  say                   0.477   6.403  \n",
            "   [1]  is                    0.477   0.000  \n",
            "   [1]  this                  0.477   0.000  \n",
            "   [1]  br                    0.477   0.000  \n",
            "   [1]  br                    0.477   0.000  \n",
            "   [1]  i                     0.477   0.000  \n",
            "   [0]  hope                  0.477   9.384  \n",
            "   [1]  they                  0.477   0.000  \n",
            "   [0]  can                   0.478   5.929  \n",
            " Sample 2.\n",
            "   [1]  give                  0.491   0.000  \n",
            "   [1]  this                  0.485   0.000  \n",
            "   [1]  movie                 0.482   0.000  \n",
            "   [1]  a                     0.480   0.000  \n",
            "   [1]  4                     0.481   0.000  \n",
            "   [0]  for                   0.482   5.196  \n",
            "   [1]  its                   0.483   0.000  \n",
            "   [0]  sheer                 0.484   11.423 \n",
            "   [1]  awfulness             0.486   0.000  \n",
            "   [1]  i                     0.486   0.000  \n",
            "   [0]  must                  0.487   7.649  \n",
            "   [1]  preface               0.487   0.000  \n",
            "   [0]  my                    0.488   5.711  \n",
            "   [1]  review                0.488   0.000  \n",
            "   [0]  by                    0.488   6.241  \n",
            "   [1]  way                   0.488   0.000  \n",
            "   [1]  of                    0.488   0.000  \n",
            "   [1]  noting                0.488   0.000  \n",
            "   [0]  that                  0.488   4.743  \n",
            "   [0]  i                     0.488   3.864  \n",
            "Samples\n",
            "Sample 0 .  enjoyed your review anyone reading your review who are not consumers of metal will learn that this topic could easily\n",
            "Sample 1 .  saw a bit of this last year and all i can say is this br br i hope they can\n",
            "Sample 2 .  give this movie a 4 for its sheer awfulness i must preface my review by way of noting that i\n",
            "\n",
            "\n",
            "targets[[7 12 2069 0 3572 1 33 0 984 5 37 3056 1306 5 143 18 24 12 3056 9][2907 12 84 81 19 3687 6405 13 2 19 11 13 5536 4 28 3047 120 17790 1 12295][353 0 298 8 60 646 713 1 99 70]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[69 7 12 2069 0 3572 1 33 0 984]...][[0 0 0 1 0 1 1 0 1 1]...][[69 69849 69849 69849 0 69849 1 33 69849 984]...][[7 12 2069 0 3572 1 33 0 984 5]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[69 7 12 2069 0 3572 1 33 0 984]...][[0 0 0 1 0 1 1 0 1 1]...][[69 69849 69849 69849 0 69849 1 33 69849 984]...][[7 12 2069 0 3572 1 33 0 984 5]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115]...][[0 0 0 1 1 0 0 0 1 1]...][[1840 69849 69849 69849 755 654 69849 69849 69849 1115]...][[1943 12 368 755 654 78 2 368 1115 19]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115]...][[0 0 0 1 1 0 0 0 1 1]...][[1840 69849 69849 69849 755 654 69849 69849 69849 1115]...][[1943 12 368 755 654 78 2 368 1115 19]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            "global_step: 78\n",
            " perplexity: 4304.940\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            " percent of 3-grams captured: 0.497.\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            " percent of 2-grams captured: 0.706.\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            " percent of 4-grams captured: 0.225.\n",
            " geometric_avg: 0.429.\n",
            " arithmetic_avg: 0.476.\n",
            "global_step: 78\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69321\n",
            " G train loss: 4.20110\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115]...][[0 0 0 1 1 0 0 0 1 1]...][[1840 69849 69849 69849 755 654 69849 69849 69849 1115]...][[1943 12 368 755 654 78 2 368 1115 19]...]\n",
            " Sample 0.\n",
            "   [0]  hopes                 0.451   8.349  \n",
            "   [0]  s                     0.446   4.247  \n",
            "   [0]  classic               0.446   8.339  \n",
            "   [1]  tale                  0.448   0.000  \n",
            "   [1]  turned                0.450   0.000  \n",
            "   [0]  into                  0.453   7.244  \n",
            "   [0]  a                     0.456   3.148  \n",
            "   [0]  classic               0.458   8.312  \n",
            "   [1]  adventure             0.460   0.000  \n",
            "   [1]  film                  0.461   0.000  \n",
            "   [0]  with                  0.463   5.230  \n",
            "   [0]  ronald                0.464   13.160 \n",
            "   [0]  coleman               0.465   13.184 \n",
            "   [1]  as                    0.466   0.000  \n",
            "   [0]  the                   0.466   2.760  \n",
            "   [0]  king                  0.466   8.621  \n",
            "   [0]  and                   0.465   3.619  \n",
            "   [1]  his                   0.466   0.000  \n",
            "   [0]  commoner              0.466   12.968 \n",
            "   [1]  cousin                0.466   0.000  \n",
            " Sample 1.\n",
            "   [1]  is                    0.509   0.000  \n",
            "   [0]  an                    0.509   5.437  \n",
            "   [0]  update                0.508   13.424 \n",
            "   [1]  of                    0.507   0.000  \n",
            "   [1]  an                    0.506   0.000  \n",
            "   [1]  earlier               0.505   0.000  \n",
            "   [1]  comment               0.505   0.000  \n",
            "   [0]  one                   0.505   5.521  \n",
            "   [1]  way                   0.505   0.000  \n",
            "   [1]  passage               0.505   0.000  \n",
            "   [1]  is                    0.505   0.000  \n",
            "   [0]  likely                0.506   11.755 \n",
            "   [0]  the                   0.506   3.426  \n",
            "   [1]  most                  0.506   0.000  \n",
            "   [0]  underrated            0.506   8.304  \n",
            "   [1]  romance               0.506   0.000  \n",
            "   [1]  picture               0.506   0.000  \n",
            "   [1]  of                    0.506   0.000  \n",
            "   [1]  all                   0.505   0.000  \n",
            "   [1]  time                  0.505   0.000  \n",
            " Sample 2.\n",
            "   [1]  s                     0.525   0.000  \n",
            "   [0]  too                   0.522   7.839  \n",
            "   [1]  bad                   0.517   0.000  \n",
            "   [0]  this                  0.513   3.297  \n",
            "   [1]  film                  0.511   0.000  \n",
            "   [1]  wasn                  0.509   0.000  \n",
            "   [0]  t                     0.507   5.883  \n",
            "   [0]  scraped               0.507   13.714 \n",
            "   [0]  a                     0.507   3.253  \n",
            "   [1]  forth                 0.507   0.000  \n",
            "   [0]  time                  0.506   6.858  \n",
            "   [1]  the                   0.506   0.000  \n",
            "   [0]  idea                  0.507   7.812  \n",
            "   [0]  might                 0.507   7.253  \n",
            "   [1]  have                  0.507   0.000  \n",
            "   [1]  been                  0.508   0.000  \n",
            "   [0]  good                  0.509   5.449  \n",
            "   [1]  but                   0.509   0.000  \n",
            "   [1]  it                    0.508   0.000  \n",
            "   [0]  wasn                  0.507   7.616  \n",
            "Samples\n",
            "Sample 0 .  hopes s classic tale turned into a classic adventure film with ronald coleman as the king and his commoner cousin\n",
            "Sample 1 .  is an update of an earlier comment one way passage is likely the most underrated romance picture of all time\n",
            "Sample 2 .  s too bad this film wasn t scraped a forth time the idea might have been good but it wasn\n",
            "\n",
            "\n",
            "targets[[50418 12 330 8991 8 0 328 45 4 28 29 3 4895 12 117 39 5 983 3 600][17 13 1136 9 59 395 7 4 255 73 126 72 47 9 67 478 7231 7 13 401][9546 288 21 4095 1669 12 872 17 18 7]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[601 50418 12 330 8991 8 0 328 45 4]...][[0 1 0 1 0 1 1 1 0 0]...][[601 69849 12 69849 8991 69849 0 328 45 69849]...][[50418 12 330 8991 8 0 328 45 4 28]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[601 50418 12 330 8991 8 0 328 45 4]...][[0 1 0 1 0 1 1 1 0 0]...][[601 69849 12 69849 8991 69849 0 328 45 69849]...][[50418 12 330 8991 8 0 328 45 4 28]...]\n",
            "Tensor(\"Placeholder:0\", shape=(128, 20), dtype=int32) Tensor(\"Placeholder_1:0\", shape=(128, 20), dtype=int32)\n",
            "Training raising FloatingPoinError.\n",
            "Traceback (most recent call last):\n",
            "  File \"train_mask_gan.py\", line 1163, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"train_mask_gan.py\", line 1145, in main\n",
            "    data_ngram_counts)\n",
            "  File \"train_mask_gan.py\", line 745, in train_model\n",
            "    'Training infinite perplexity: %.3f' % perplexity)\n",
            "FloatingPointError: Training infinite perplexity: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFD9rWo42ezv",
        "colab_type": "code",
        "outputId": "e37e9037-d28d-43da-d2da-ea21c8020616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# make copy of mle training checkpoints\n",
        "%cd /content\n",
        "!cp -r '/content/yesweGAN/maskgan_colab/maskGAN/train' '/content/drive/My Drive/imdb mle checkpoint'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3pcQ4Gpl2wp",
        "colab_type": "text"
      },
      "source": [
        "## Run MaskGAN in GAN mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a06ae42e-4dd5-41a3-f86d-99babab135f7",
        "id": "dLyPlm0cGEZu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/yesweGAN/maskgan_colab\n",
        "!python train_mask_gan.py \\\n",
        " --data_dir='dataset/imdb' \\\n",
        " --data_set='imdb' \\\n",
        " --batch_size=128 \\\n",
        " --sequence_length=20 \\\n",
        " --base_directory='/content/yesweGAN/maskgan_colab/maskGAN' \\\n",
        " --hparams=\"gen_rnn_size=550,dis_rnn_size=550,gen_num_layers=2,dis_num_layers=2,gen_learning_rate=0.000038877,gen_learning_rate_decay=1.0,gen_full_learning_rate_steps=2000000,gen_vd_keep_prob=0.33971,rl_discount_rate=0.89072,dis_learning_rate=5e-4,baseline_decay=0.99,dis_train_iterations=2,dis_pretrain_learning_rate=0.005,critic_learning_rate=5.1761e-7,dis_vd_keep_prob=0.71940\" \\\n",
        " --mode='TRAIN' \\\n",
        " --max_steps=10000 \\\n",
        " --perplexity_threshold=1000000 \\\n",
        " --generator_model='seq2seq_vd' \\\n",
        " --discriminator_model='seq2seq_vd' \\\n",
        " --is_present_rate=0.5 \\\n",
        " --summaries_every=250 \\\n",
        " --print_every=250 \\\n",
        " --max_num_to_print=3 \\\n",
        " --gen_training_strategy='reinforce' \\\n",
        " --seq2seq_share_embedding=true \\\n",
        " --baseline_method=critic \\\n",
        " --attention_option=luong"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yesweGAN/maskgan_colab\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0310 00:27:21.489969 139858729412480 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0310 00:27:23.502741 139858729412480 module_wrapper.py:139] From train_mask_gan.py:1161: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0310 00:27:23.504081 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/data/imdb_loader.py:40: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "W0310 00:29:08.413126 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/data/imdb_loader.py:59: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Unique 2-grams: 1300789\n",
            "Unique 3-grams: 3512054\n",
            "Unique 4-grams: 4986879\n",
            "Vocab size: 69849\n",
            "W0310 00:29:20.685357 139858729412480 module_wrapper.py:139] From train_mask_gan.py:1126: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "Training model.\n",
            "W0310 00:29:20.685695 139858729412480 module_wrapper.py:139] From train_mask_gan.py:502: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0310 00:29:20.685774 139858729412480 train_mask_gan.py:502] Training model.\n",
            "W0310 00:29:20.686714 139858729412480 module_wrapper.py:139] From train_mask_gan.py:515: The name tf.train.replica_device_setter is deprecated. Please use tf.compat.v1.train.replica_device_setter instead.\n",
            "\n",
            "W0310 00:29:20.686961 139858729412480 module_wrapper.py:139] From train_mask_gan.py:517: The name tf.container is deprecated. Please use tf.compat.v1.container instead.\n",
            "\n",
            "W0310 00:29:20.690521 139858729412480 module_wrapper.py:139] From train_mask_gan.py:247: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0310 00:29:20.693761 139858729412480 module_wrapper.py:139] From train_mask_gan.py:249: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "W0310 00:29:20.698447 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:596: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0310 00:29:20.698750 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:94: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0310 00:29:20.704561 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:104: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0310 00:29:20.705522 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/regularization/variational_dropout.py:37: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0310 00:29:20.715392 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:116: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0310 00:29:20.734066 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:71: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0310 00:29:20.748658 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:155: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0310 00:29:20.790999 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "W0310 00:29:20.796634 139858729412480 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0310 00:29:20.920504 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:253: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
            "Instructions for updating:\n",
            "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
            "\n",
            "W0310 00:29:20.931468 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0310 00:29:20.951458 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:310: The name tf.matrix_transpose is deprecated. Please use tf.linalg.matrix_transpose instead.\n",
            "\n",
            "W0310 00:29:21.026355 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:362: __init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "W0310 00:29:21.028361 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/distributions/categorical.py:242: __init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "W0310 00:29:21.029144 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0310 00:29:21.041198 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:322: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "W0310 00:29:26.624068 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_losses.py:41: The name tf.losses.sigmoid_cross_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid_cross_entropy instead.\n",
            "\n",
            "W0310 00:29:26.672734 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_losses.py:126: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0310 00:29:27.007671 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_losses.py:57: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\n",
            "\n",
            "W0310 00:29:28.286993 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_optimization.py:114: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0310 00:29:28.287225 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_optimization.py:118: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "\n",
            "Optimizing Generator vars:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69849, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(550, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69849,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1100, 550) dtype=float32_ref>\n",
            "\n",
            "Optimizing Discriminator vars:\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(69849, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(550, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1100, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(550, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Optimizing Critic vars:\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(550, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "W0310 00:29:35.844297 139858729412480 module_wrapper.py:139] From train_mask_gan.py:377: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0310 00:29:35.858165 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/helper.py:38: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0310 00:29:36.524502 139858729412480 module_wrapper.py:139] From train_mask_gan.py:423: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0310 00:29:36.532145 139858729412480 module_wrapper.py:139] From train_mask_gan.py:425: The name tf.summary.text is deprecated. Please use tf.compat.v1.summary.text instead.\n",
            "\n",
            "W0310 00:29:36.533526 139858729412480 module_wrapper.py:139] From train_mask_gan.py:428: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "\n",
            "Trainable Variables in Graph:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69849, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(550, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69849,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1100, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(69849, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(550, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1100, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(550, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(550, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "W0310 00:29:36.597843 139858729412480 deprecation.py:323] From train_mask_gan.py:546: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2020-03-10 00:29:38.478163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-03-10 00:29:38.519224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.519790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-10 00:29:38.522501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-03-10 00:29:38.534383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-03-10 00:29:38.537804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-03-10 00:29:38.544079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-03-10 00:29:38.552803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-03-10 00:29:38.558878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-03-10 00:29:38.571123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-10 00:29:38.571253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.571840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.572324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-10 00:29:38.572715: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-03-10 00:29:38.586189: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz\n",
            "2020-03-10 00:29:38.588355: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562f16d119c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-10 00:29:38.588384: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-03-10 00:29:38.743741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.744475: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562f16d11800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-10 00:29:38.744516: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-03-10 00:29:38.744695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.745229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-10 00:29:38.745294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-03-10 00:29:38.745318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-03-10 00:29:38.745338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-03-10 00:29:38.745357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-03-10 00:29:38.745376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-03-10 00:29:38.745394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-03-10 00:29:38.745413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-10 00:29:38.745500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.746054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.746575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-10 00:29:38.750602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-03-10 00:29:38.751849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-10 00:29:38.751879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-10 00:29:38.751890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-10 00:29:38.754809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.755509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.756064: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-03-10 00:29:38.756125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0310 00:29:38.759409 139858729412480 saver.py:1284] Restoring parameters from /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt-268\n",
            "W0310 00:30:03.681040 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0310 00:30:03.683342 139858729412480 session_manager.py:500] Running local_init_op.\n",
            "I0310 00:30:04.021142 139858729412480 session_manager.py:502] Done running local_init_op.\n",
            "I0310 00:30:12.559847 139858729412480 supervisor.py:737] Starting standard services.\n",
            "I0310 00:30:12.858433 139858729412480 supervisor.py:743] Starting queue runners.\n",
            "I0310 00:30:12.858711 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "I0310 00:30:13.155273 139852867667712 supervisor.py:1099] global_step/sec: 0\n",
            "W0310 00:30:17.906004 139852876060416 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "2020-03-10 00:30:19.279675: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.289919: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.299557: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.310697: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.320860: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.331114: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.338795: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.343931: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.344554: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.349462: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.355707: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.361273: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.363668: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.366602: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.371070: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.376041: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.380194: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.380993: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.385354: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.390051: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.394991: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.396537: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.400071: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.404401: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.408975: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.412921: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.414001: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.418385: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.423006: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.427727: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.428692: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.431413: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.448513: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.478311: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.504829: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.523465: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.538105: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.552130: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.565963: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.579754: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.713277: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.718199: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.722775: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.727297: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.731603: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.736085: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.740445: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.744304: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.748120: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.752016: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.755932: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.759738: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.763527: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.767332: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.771096: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.774812: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.778650: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.782514: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.786315: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.789077: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:24.401118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 0 1 1 1 1 1 1 0 1...]...][[8 23354 69849 8689 58348 120 26 5755 1110 69849...]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 0 1 1 1 1 1 1 0 1...]...][[8 23354 69849 8689 58348 120 26 5755 1110 69849...]...][[23354 1255 8689 58348 120 26 5755 1110 198 2685...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 0 0 0 1 1 1 0 0...]...][[577 5337 12 69849 69849 69849 81 354 39 69849...]...][[5337 12 361 5 2 81 354 39 12 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 0 0 0 1 1 1 0 0...]...][[577 5337 12 69849 69849 69849 81 354 39 69849...]...][[5337 12 53229 5 0 81 354 39 0 4...]...]\n",
            "2020-03-10 00:30:27.853293: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.854308: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.856206: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.858064: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.859805: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.861565: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.861847: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.864262: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.866332: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.868211: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.869955: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.870502: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.872469: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.874347: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.876084: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.877288: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.878118: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.880099: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.881831: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.883731: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.884750: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.886078: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.888076: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.889879: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.891190: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.891544: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.894154: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.896440: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.898582: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.900674: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.902753: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.905118: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.907375: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.909456: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.911560: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.913713: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.915798: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.917881: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.920001: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.922068: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.964572: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.966083: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.967554: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.969013: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.970738: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.972174: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.973607: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.975057: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.976582: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.978075: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.979550: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.980961: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.982413: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.983989: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.985508: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.986924: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.988340: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.989791: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.991254: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.992197: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[137 13 590 379 11 1057 8 55 225 2911...]...]\n",
            "2020-03-10 00:30:35.151716: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.161725: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.170964: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.180760: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.190259: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.200046: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.206936: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.211323: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.214767: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.216006: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.220594: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.224618: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.228588: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.229821: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.232958: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.236881: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.240517: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.244048: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.244426: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.248513: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.252325: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.256263: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.258408: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.260343: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.264735: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.268611: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.272325: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.272932: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.276802: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.280804: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.283686: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.285639: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.315128: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.326541: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.337763: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.349044: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.360900: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.372071: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.383353: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.394924: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.505060: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.508098: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.511305: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.514421: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.517572: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.520714: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.523883: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.526929: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.530071: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.533017: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.535920: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.539054: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.542062: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.545002: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.547900: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.550864: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.553877: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.556839: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.559660: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.562041: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[17 13 37 379 11 9 307 55 4 2911...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[50 13 0 379 11 10 1 55 1 2911...]...]\n",
            "2020-03-10 00:30:52.349923: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.350886: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.351583: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.352191: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.352845: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.353443: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.354054: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.354660: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.355261: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.355884: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.356499: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.357095: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.357704: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.358312: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.358930: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.359542: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.360146: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.360777: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.361434: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.362039: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "W0310 00:30:52.892658 139858729412480 module_wrapper.py:139] From train_mask_gan.py:756: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "global_step: 274\n",
            " perplexity: 968.750\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            " percent of 3-grams captured: 0.131.\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            " percent of 2-grams captured: 0.546.\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.114.\n",
            " arithmetic_avg: 0.233.\n",
            "global_step: 274\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66360\n",
            " G train loss: 147.60037\n",
            "2020-03-10 00:30:55.191697: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.195860: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.200898: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.205640: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.210044: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.214197: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.216390: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.217996: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.219434: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.219861: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.221264: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.222795: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.224348: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.225143: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.226026: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.227629: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.229244: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.230816: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.231080: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.232447: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.234039: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.235718: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.236979: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.237614: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.239058: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.240618: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.242092: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.242617: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.243760: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.245263: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.246389: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.247114: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.248554: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.249899: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.251248: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.252620: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.254002: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.255429: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.256951: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.258394: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.290995: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.292294: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.293500: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.294795: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.296050: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.297256: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.298514: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.300014: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.301269: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.302853: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.304083: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.305276: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.306529: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.307795: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.309017: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.310251: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.312104: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.313466: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.314879: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.315831: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[17 13 37 379 11 9 307 55 4 2911...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[110 13 7511 379 11 346 949 55 294 2911...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  seen                0.577        3.706        -6.379       -0.549       -2.798       -0.304       -2.494       \n",
            "   [1]  was                 0.566        0.000        0.000        0.000        -2.525       -0.379       -0.000       \n",
            "   [0]  2009                0.473        5.275        -8.489       -0.749       -2.835       -0.396       -2.439       \n",
            "   [1]  awful               0.433        0.000        0.000        0.000        -2.342       -0.366       -0.000       \n",
            "   [1]  that                0.467        0.000        0.000        0.000        -2.630       -0.352       -0.000       \n",
            "   [0]  home                0.495        3.563        -7.490       -0.702       -2.952       -0.365       -2.588       \n",
            "   [0]  write               0.440        6.522        -7.781       -0.820       -2.526       -0.386       -2.140       \n",
            "   [1]  up                  0.429        0.000        0.000        0.000        -1.915       -0.374       -0.000       \n",
            "   [0]  during              0.393        4.099        -7.952       -0.933       -2.150       -0.367       -1.783       \n",
            "   [1]  halfway             0.401        0.000        0.000        0.000        -1.366       -0.353       -0.000       \n",
            "   [0]  never               0.445        7.438        -6.896       -0.810       -1.534       -0.356       -1.177       \n",
            "   [0]  very                0.444        8.406        -5.831       -0.813       -0.813       -0.377       -0.436       \n",
            "   [1]  for                 0.478        0.000        0.000        0.000        0.000        -0.378       0.000        \n",
            "   [1]  it                  0.477        0.000        0.000        0.000        0.000        -0.394       0.000        \n",
            "   [1]  to                  0.509        0.000        0.000        0.000        0.000        -0.391       0.000        \n",
            "   [1]  start               0.493        0.000        0.000        0.000        0.000        -0.403       0.000        \n",
            "   [1]  for                 0.519        0.000        0.000        0.000        0.000        -0.399       0.000        \n",
            "   [1]  the                 0.567        0.000        0.000        0.000        0.000        -0.415       0.000        \n",
            "   [1]  plot                0.497        0.000        0.000        0.000        0.000        -0.444       0.000        \n",
            "   [1]  characters          0.449        0.000        0.000        0.000        0.000        -0.409       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  are                 0.581        3.895        -5.248       -0.543       -2.304       -0.309       -1.995       \n",
            "   [1]  easy                0.530        0.000        0.000        0.000        -1.977       -0.359       -0.000       \n",
            "   [1]  to                  0.547        0.000        0.000        0.000        -2.219       -0.367       -0.000       \n",
            "   [0]  i                   0.560        5.470        -2.948       -0.580       -2.491       -0.378       -2.113       \n",
            "   [1]  that                0.611        0.000        0.000        0.000        -2.146       -0.391       -0.000       \n",
            "   [1]  risa                0.586        0.000        0.000        0.000        -2.409       -0.406       -0.000       \n",
            "   [1]  garcia              0.558        0.000        0.000        0.000        -2.705       -0.405       -0.000       \n",
            "   [0]  had                 0.512        4.309        -5.663       -0.669       -3.037       -0.401       -2.635       \n",
            "   [0]  and                 0.541        3.051        -3.010       -0.615       -2.658       -0.387       -2.271       \n",
            "   [0]  a                   0.578        12.625       -3.075       -0.548       -2.294       -0.392       -1.901       \n",
            "   [0]  great               0.591        10.067       -6.569       -0.527       -1.960       -0.406       -1.554       \n",
            "   [1]  director            0.572        0.000        0.000        0.000        -1.609       -0.413       -0.000       \n",
            "   [0]  fantasy             0.490        2.985        -8.143       -0.714       -1.806       -0.409       -1.397       \n",
            "   [0]  2                   0.519        4.857        -7.097       -0.656       -1.226       -0.387       -0.839       \n",
            "   [1]  a                   0.558        0.000        0.000        0.000        -0.641       -0.392       -0.000       \n",
            "   [1]  screenwriter        0.598        0.000        0.000        0.000        -0.719       -0.408       -0.000       \n",
            "   [0]  have                0.611        5.160        -5.241       -0.493       -0.808       -0.424       -0.384       \n",
            "   [1]  dozens              0.627        0.000        0.000        0.000        -0.354       -0.432       0.000        \n",
            "   [1]  of                  0.671        0.000        0.000        0.000        -0.397       -0.440       0.000        \n",
            "   [0]  like                0.640        8.059        -5.355       -0.446       -0.446       -0.453       0.007        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  been                0.564        2.915        -6.245       -0.573       -3.072       -0.304       -2.768       \n",
            "   [1]  is                  0.596        0.000        0.000        0.000        -2.805       -0.330       -0.000       \n",
            "   [0]  this                0.614        8.964        -3.151       -0.488       -3.150       -0.359       -2.791       \n",
            "   [0]  be                  0.660        11.951       -5.448       -0.415       -2.988       -0.378       -2.610       \n",
            "   [1]  knows               0.638        0.000        0.000        0.000        -2.888       -0.417       -0.000       \n",
            "   [1]  how                 0.569        0.000        0.000        0.000        -3.242       -0.414       -0.000       \n",
            "   [0]  every               0.526        3.841        -7.313       -0.643       -3.640       -0.377       -3.263       \n",
            "   [0]  so                  0.459        8.684        -4.864       -0.779       -3.365       -0.353       -3.012       \n",
            "   [1]  a                   0.467        0.000        0.000        0.000        -2.903       -0.316       -0.000       \n",
            "   [0]  of                  0.516        8.465        -3.883       -0.661       -3.260       -0.316       -2.943       \n",
            "   [0]  well                0.500        7.945        -6.170       -0.693       -2.918       -0.342       -2.576       \n",
            "   [0]  tv                  0.487        5.821        -6.887       -0.719       -2.498       -0.340       -2.157       \n",
            "   [1]  best                0.478        0.000        0.000        0.000        -1.996       -0.337       -0.000       \n",
            "   [0]  just                0.421        7.786        -5.960       -0.866       -2.241       -0.338       -1.903       \n",
            "   [1]  was                 0.374        0.000        0.000        0.000        -1.544       -0.310       -0.000       \n",
            "   [1]  noises              0.353        0.000        0.000        0.000        -1.733       -0.284       -0.000       \n",
            "   [1]  off                 0.321        0.000        0.000        0.000        -1.946       -0.276       -0.000       \n",
            "   [0]  a                   0.346        9.003        -3.602       -1.062       -2.185       -0.267       -1.918       \n",
            "   [0]  heart               0.283        8.537        -7.568       -1.261       -1.261       -0.284       -0.977       \n",
            "   [1]  is                  0.286        0.000        0.000        0.000        0.000        -0.251       0.000        \n",
            "Samples\n",
            "Sample 0 .  seen was 2009 awful that home write up during halfway never very for it to start for the plot characters\n",
            "Sample 1 .  are easy to i that risa garcia had and a great director fantasy 2 a screenwriter have dozens of like\n",
            "Sample 2 .  been is this be knows how every so a of well tv best just was noises off a heart is\n",
            "\n",
            "\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 0 1 0 0 0 1...]...][[16 1803 69849 45 110 69849 703 69849 69849 69849...]...][[1803 34 45 110 0 703 206 15 5359 751...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 0 1 0 0 0 1...]...][[16 1803 69849 45 110 69849 703 69849 69849 69849...]...][[1803 24 45 110 46 703 402 1 492 751...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[0 0 1 0 1 0 0 0 0 1...]...][[7 69849 69849 174 69849 12 69849 69849 69849 69849...]...][[5 23 174 19 12 301 4 22800 20 15795...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[0 0 1 0 1 0 0 0 0 1...]...][[7 69849 69849 174 69849 12 69849 69849 69849 69849...]...][[147 12 174 2182 12 17874 1 981 165 15795...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 31 607 120 26 207 1110 3 47331...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 8 2 120 26 1 1110 3 169...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "global_step: 279\n",
            " perplexity: 962.597\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            " percent of 3-grams captured: 0.138.\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            " percent of 2-grams captured: 0.539.\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.115.\n",
            " arithmetic_avg: 0.232.\n",
            "global_step: 279\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66140\n",
            " G train loss: 158.66170\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 64063 16274 120 26 467 1110 3 18...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  pushover            0.504        0.000        0.000        0.000        -2.723       -0.306       -0.000       \n",
            "   [1]  fred                0.525        0.000        0.000        0.000        -3.057       -0.352       -0.000       \n",
            "   [0]  shana               0.516        11.697       -13.213      -0.661       -3.432       -0.400       -3.031       \n",
            "   [0]  har                 0.502        11.908       -11.601      -0.690       -3.111       -0.424       -2.687       \n",
            "   [1]  off                 0.450        0.000        0.000        0.000        -2.718       -0.433       -0.000       \n",
            "   [1]  his                 0.387        0.000        0.000        0.000        -3.051       -0.411       -0.000       \n",
            "   [0]  loved               0.381        11.243       -7.323       -0.966       -3.425       -0.357       -3.068       \n",
            "   [1]  portrayal           0.353        0.000        0.000        0.000        -2.761       -0.333       -0.000       \n",
            "   [1]  of                  0.407        0.000        0.000        0.000        -3.100       -0.304       -0.000       \n",
            "   [0]  but                 0.466        8.271        -5.832       -0.764       -3.480       -0.330       -3.150       \n",
            "   [0]  his                 0.433        11.746       -6.165       -0.837       -3.050       -0.383       -2.667       \n",
            "   [1]  the                 0.466        0.000        0.000        0.000        -2.484       -0.377       -0.000       \n",
            "   [0]  this                0.483        11.735       -5.122       -0.728       -2.788       -0.409       -2.379       \n",
            "   [0]  one                 0.493        11.356       -6.230       -0.708       -2.314       -0.439       -1.875       \n",
            "   [0]  another             0.484        8.796        -7.066       -0.726       -1.802       -0.464       -1.339       \n",
            "   [1]  from                0.504        0.000        0.000        0.000        -1.208       -0.469       -0.000       \n",
            "   [0]  eddie               0.463        10.258       -8.965       -0.769       -1.356       -0.490       -0.866       \n",
            "   [1]  indemnity           0.409        0.000        0.000        0.000        -0.659       -0.470       -0.000       \n",
            "   [1]  and                 0.440        0.000        0.000        0.000        -0.740       -0.425       -0.000       \n",
            "   [0]  garbage             0.436        8.624        -8.460       -0.831       -0.831       -0.434       -0.396       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  movie               0.535        3.034        -3.332       -0.626       -3.217       -0.306       -2.911       \n",
            "   [1]  nutshell            0.549        0.000        0.000        0.000        -2.909       -0.370       -0.000       \n",
            "   [0]  at                  0.526        5.072        -6.011       -0.643       -3.266       -0.413       -2.853       \n",
            "   [0]  is                  0.534        4.427        -3.000       -0.628       -2.945       -0.422       -2.523       \n",
            "   [0]  christmas           0.489        5.293        -8.267       -0.715       -2.601       -0.430       -2.171       \n",
            "   [1]  don                 0.458        0.000        0.000        0.000        -2.117       -0.418       -0.000       \n",
            "   [1]  t                   0.457        0.000        0.000        0.000        -2.377       -0.408       -0.000       \n",
            "   [0]  movie               0.503        4.870        -4.278       -0.687       -2.669       -0.409       -2.260       \n",
            "   [0]  very                0.491        3.650        -5.796       -0.711       -2.225       -0.428       -1.797       \n",
            "   [0]  to                  0.523        7.002        -3.647       -0.648       -1.700       -0.426       -1.273       \n",
            "   [1]  the                 0.573        0.000        0.000        0.000        -1.181       -0.438       -0.000       \n",
            "   [0]  was                 0.550        8.455        -4.313       -0.597       -1.326       -0.461       -0.865       \n",
            "   [1]  of                  0.593        0.000        0.000        0.000        -0.818       -0.461       -0.000       \n",
            "   [0]  new                 0.559        5.573        -6.731       -0.582       -0.919       -0.473       -0.445       \n",
            "   [1]  crap                0.538        0.000        0.000        0.000        -0.378       -0.462       0.000        \n",
            "   [1]  if                  0.512        0.000        0.000        0.000        -0.424       -0.454       0.000        \n",
            "   [1]  you                 0.543        0.000        0.000        0.000        -0.476       -0.439       -0.000       \n",
            "   [1]  don                 0.534        0.000        0.000        0.000        -0.535       -0.447       -0.000       \n",
            "   [1]  t                   0.535        0.000        0.000        0.000        -0.601       -0.448       -0.000       \n",
            "   [0]  blood               0.510        7.819        -8.254       -0.674       -0.674       -0.453       -0.222       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  i                   0.507        12.780       -4.688       -0.679       -4.747       -0.306       -4.441       \n",
            "   [0]  elements            0.507        6.521        -7.634       -0.679       -4.568       -0.360       -4.208       \n",
            "   [1]  however             0.492        0.000        0.000        0.000        -4.366       -0.387       -0.000       \n",
            "   [0]  here                0.490        6.125        -6.897       -0.713       -4.902       -0.402       -4.500       \n",
            "   [0]  missing             0.433        12.862       -8.649       -0.837       -4.703       -0.408       -4.295       \n",
            "   [0]  run                 0.451        6.121        -8.332       -0.797       -4.340       -0.394       -3.946       \n",
            "   [1]  name                0.381        0.000        0.000        0.000        -3.978       -0.399       -0.000       \n",
            "   [0]  although            0.342        4.733        -7.572       -1.073       -4.466       -0.384       -4.082       \n",
            "   [0]  rambling            0.327        4.400        -11.380      -1.118       -3.810       -0.368       -3.442       \n",
            "   [0]  as                  0.353        12.439       -5.514       -1.042       -3.022       -0.356       -2.666       \n",
            "   [1]  because             0.337        0.000        0.000        0.000        -2.223       -0.358       -0.000       \n",
            "   [0]  questions           0.371        6.108        -8.839       -0.992       -2.496       -0.355       -2.141       \n",
            "   [0]  silent              0.367        7.304        -7.675       -1.004       -1.688       -0.363       -1.325       \n",
            "   [1]  sends               0.386        0.000        0.000        0.000        -0.768       -0.370       -0.000       \n",
            "   [1]  opposing            0.407        0.000        0.000        0.000        -0.862       -0.382       -0.000       \n",
            "   [1]  messages            0.431        0.000        0.000        0.000        -0.968       -0.394       -0.000       \n",
            "   [1]  to                  0.473        0.000        0.000        0.000        -1.086       -0.405       -0.000       \n",
            "   [0]  eugenie             0.477        7.787        -12.733      -0.740       -1.220       -0.425       -0.795       \n",
            "   [1]  on                  0.502        0.000        0.000        0.000        -0.539       -0.434       -0.000       \n",
            "   [0]  for                 0.546        5.878        -5.865       -0.605       -0.605       -0.451       -0.154       \n",
            "Samples\n",
            "Sample 0 .  pushover fred shana har off his loved portrayal of but his the this one another from eddie indemnity and garbage\n",
            "Sample 1 .  movie nutshell at is christmas don t movie very to the was of new crap if you don t blood\n",
            "Sample 2 .  i elements however here missing run name although rambling as because questions silent sends opposing messages to eugenie on for\n",
            "\n",
            "\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 1 1 0 0 1 0 1 1...]...][[10 141 27 77 126 69849 69849 2 69849 2431...]...][[141 27 77 126 73 126 2 1023 2431 2347...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 1 1 0 0 1 0 1 1...]...][[10 141 27 77 126 69849 69849 2 69849 2431...]...][[141 27 77 126 91 2 2 9665 2431 2347...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[0 0 0 0 0 0 0 1 0 1...]...][[10 69849 69849 69849 69849 69849 69849 69849 91 69849...]...][[17 5 634 2 181 17 31 91 2075 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[0 0 0 0 0 0 0 1 0 1...]...][[10 69849 69849 69849 69849 69849 69849 69849 91 69849...]...][[28499 417 699 46 8 38 4 91 1365 7...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 27 81 2251 3 12 5...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 2 81 354 39 12 5...]...]\n",
            "I0310 00:31:12.858674 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 11 81 20963 0 12 5...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "global_step: 284\n",
            " perplexity: 958.371\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            " percent of 3-grams captured: 0.148.\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            " percent of 2-grams captured: 0.550.\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            " percent of 4-grams captured: 0.035.\n",
            " geometric_avg: 0.142.\n",
            " arithmetic_avg: 0.244.\n",
            "global_step: 284\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66193\n",
            " G train loss: 161.67604\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 2 81 354 39 12 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 30 81 3943 50 12 5...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  cronenberg          0.517        0.000        0.000        0.000        -1.870       -0.309       -0.000       \n",
            "   [1]  s                   0.525        0.000        0.000        0.000        -2.099       -0.357       -0.000       \n",
            "   [1]  camera              0.459        0.000        0.000        0.000        -2.356       -0.393       -0.000       \n",
            "   [1]  is                  0.458        0.000        0.000        0.000        -2.645       -0.401       -0.000       \n",
            "   [0]  all                 0.544        3.576        -5.808       -0.610       -2.970       -0.403       -2.568       \n",
            "   [1]  great               0.570        0.000        0.000        0.000        -2.650       -0.421       -0.000       \n",
            "   [0]  intrigued           0.574        7.961        -11.489      -0.555       -2.975       -0.437       -2.538       \n",
            "   [0]  can                 0.560        5.912        -6.329       -0.580       -2.717       -0.443       -2.274       \n",
            "   [1]  s                   0.564        0.000        0.000        0.000        -2.399       -0.443       -0.000       \n",
            "   [1]  is                  0.581        0.000        0.000        0.000        -2.694       -0.448       -0.000       \n",
            "   [1]  only                0.574        0.000        0.000        0.000        -3.024       -0.453       -0.000       \n",
            "   [0]  detailed            0.509        7.339        -9.716       -0.676       -3.395       -0.446       -2.949       \n",
            "   [0]  present             0.378        6.668        -8.713       -0.973       -3.053       -0.429       -2.624       \n",
            "   [0]  i                   0.361        4.451        -3.886       -1.018       -2.335       -0.402       -1.933       \n",
            "   [1]  the                 0.411        0.000        0.000        0.000        -1.479       -0.383       -0.000       \n",
            "   [0]  ve                  0.421        4.491        -6.755       -0.866       -1.661       -0.378       -1.282       \n",
            "   [0]  son                 0.410        9.906        -8.591       -0.892       -0.892       -0.383       -0.510       \n",
            "   [1]  s                   0.419        0.000        0.000        0.000        0.000        -0.382       0.000        \n",
            "   [1]  makes               0.419        0.000        0.000        0.000        0.000        -0.389       0.000        \n",
            "   [1]  it                  0.403        0.000        0.000        0.000        0.000        -0.393       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  have                0.496        0.000        0.000        0.000        -2.610       -0.301       -0.000       \n",
            "   [1]  watched             0.442        0.000        0.000        0.000        -2.931       -0.339       -0.000       \n",
            "   [0]  it                  0.393        3.587        -4.360       -0.933       -3.290       -0.354       -2.937       \n",
            "   [0]  is                  0.390        4.743        -3.703       -0.943       -2.646       -0.351       -2.295       \n",
            "   [1]  several             0.356        0.000        0.000        0.000        -1.913       -0.349       -0.000       \n",
            "   [0]  character           0.286        8.160        -7.101       -1.252       -2.147       -0.351       -1.796       \n",
            "   [1]  and                 0.315        0.000        0.000        0.000        -1.005       -0.342       -0.000       \n",
            "   [1]  aided               0.310        0.000        0.000        0.000        -1.129       -0.331       -0.000       \n",
            "   [1]  by                  0.321        0.000        0.000        0.000        -1.267       -0.329       -0.000       \n",
            "   [1]  a                   0.363        0.000        0.000        0.000        -1.423       -0.334       -0.000       \n",
            "   [1]  gap                 0.387        0.000        0.000        0.000        -1.597       -0.338       -0.000       \n",
            "   [1]  of                  0.471        0.000        0.000        0.000        -1.793       -0.347       -0.000       \n",
            "   [0]  a                   0.532        3.677        -3.677       -0.631       -2.013       -0.355       -1.658       \n",
            "   [1]  few                 0.561        0.000        0.000        0.000        -1.552       -0.373       -0.000       \n",
            "   [0]  but                 0.608        6.391        -5.293       -0.498       -1.742       -0.387       -1.355       \n",
            "   [0]  director            0.601        7.552        -7.384       -0.509       -1.396       -0.408       -0.988       \n",
            "   [1]  viewings            0.628        0.000        0.000        0.000        -0.996       -0.424       -0.000       \n",
            "   [0]  his                 0.590        4.006        -5.422       -0.527       -1.118       -0.439       -0.679       \n",
            "   [0]  office              0.515        3.689        -8.859       -0.663       -0.663       -0.439       -0.224       \n",
            "   [1]  bad                 0.554        0.000        0.000        0.000        0.000        -0.425       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  who                 0.525        3.861        -6.667       -0.644       -3.493       -0.301       -3.192       \n",
            "   [0]  peevishness         0.514        5.825        -12.236      -0.665       -3.199       -0.350       -2.849       \n",
            "   [1]  sucked              0.486        0.000        0.000        0.000        -2.845       -0.371       -0.000       \n",
            "   [1]  from                0.503        0.000        0.000        0.000        -3.194       -0.379       -0.000       \n",
            "   [0]  efx                 0.470        4.203        -12.742      -0.754       -3.586       -0.392       -3.194       \n",
            "   [1]  police              0.368        0.000        0.000        0.000        -3.179       -0.388       -0.000       \n",
            "   [1]  response            0.366        0.000        0.000        0.000        -3.569       -0.359       -0.000       \n",
            "   [1]  to                  0.405        0.000        0.000        0.000        -4.007       -0.352       -0.000       \n",
            "   [0]  well                0.387        4.058        -6.751       -0.948       -4.498       -0.363       -4.136       \n",
            "   [1]  bitter              0.358        0.000        0.000        0.000        -3.985       -0.359       -0.000       \n",
            "   [0]  demos               0.344        7.255        -12.763      -1.067       -4.474       -0.349       -4.125       \n",
            "   [0]  if                  0.317        4.264        -6.503       -1.148       -3.826       -0.346       -3.480       \n",
            "   [1]  was                 0.297        0.000        0.000        0.000        -3.006       -0.338       -0.000       \n",
            "   [1]  really              0.410        0.000        0.000        0.000        -3.375       -0.332       -0.000       \n",
            "   [0]  happy               0.303        6.311        -8.029       -1.193       -3.789       -0.365       -3.424       \n",
            "   [0]  what                0.343        3.863        -6.512       -1.070       -2.915       -0.339       -2.576       \n",
            "   [0]  it                  0.342        5.959        -4.268       -1.072       -2.071       -0.348       -1.723       \n",
            "   [0]  truth               0.326        5.595        -8.507       -1.121       -1.121       -0.343       -0.778       \n",
            "   [1]  believe             0.353        0.000        0.000        0.000        0.000        -0.341       0.000        \n",
            "   [1]  i                   0.378        0.000        0.000        0.000        0.000        -0.350       0.000        \n",
            "Samples\n",
            "Sample 0 .  cronenberg s camera is all great intrigued can s is only detailed present i the ve son s makes it\n",
            "Sample 1 .  have watched it is several character and aided by a gap of a few but director viewings his office bad\n",
            "Sample 2 .  who peevishness sucked from efx police response to well bitter demos if was really happy what it truth believe i\n",
            "\n",
            "\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[0 1 0 0 1 1 1 1 1 1...]...][[10 69849 2 69849 69849 3 478 2124 338 98...]...][[5 2 1563 120 3 478 2124 338 98 38...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[0 1 0 0 1 1 1 1 1 1...]...][[10 69849 2 69849 69849 3 478 2124 338 98...]...][[250 2 5 2826 3 478 2124 338 98 38...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[0 1 1 0 1 1 1 1 0 0...]...][[84 69849 30 287 69849 42 136 11 99 69849...]...][[3 30 287 71 42 136 11 99 816 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[0 1 1 0 1 1 1 1 0 0...]...][[84 69849 30 287 69849 42 136 11 99 69849...]...][[19 30 287 163 42 136 11 99 160 25...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 0 45 110 0 12 0 874 5359 751...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 34 45 110 0 703 206 15 5359 751...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 5 45 110 0 859 1101 504 5359 751...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "global_step: 289\n",
            " perplexity: 981.123\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            " percent of 3-grams captured: 0.146.\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            " percent of 2-grams captured: 0.555.\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            " percent of 4-grams captured: 0.020.\n",
            " geometric_avg: 0.118.\n",
            " arithmetic_avg: 0.241.\n",
            "global_step: 289\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66787\n",
            " G train loss: 158.88863\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 34 45 110 0 703 206 15 5359 751...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 2881 45 110 0 1095 2319 5447 5359 751...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  somebody            0.471        0.000        0.000        0.000        -2.266       -0.294       -0.000       \n",
            "   [0]  sinister            0.464        6.584        -9.573       -0.769       -2.543       -0.324       -2.220       \n",
            "   [1]  has                 0.428        0.000        0.000        0.000        -1.993       -0.341       -0.000       \n",
            "   [1]  seen                0.501        0.000        0.000        0.000        -2.237       -0.340       -0.000       \n",
            "   [1]  the                 0.582        0.000        0.000        0.000        -2.512       -0.363       -0.000       \n",
            "   [0]  credit              0.548        8.626        -9.187       -0.601       -2.820       -0.387       -2.432       \n",
            "   [0]  pleasant            0.422        6.367        -8.478       -0.863       -2.491       -0.375       -2.115       \n",
            "   [0]  reflects            0.333        5.469        -8.773       -1.101       -1.827       -0.333       -1.495       \n",
            "   [1]  pierre              0.361        0.000        0.000        0.000        -0.816       -0.298       -0.000       \n",
            "   [1]  richard             0.363        0.000        0.000        0.000        -0.916       -0.304       -0.000       \n",
            "   [0]  get                 0.358        3.479        -6.618       -1.028       -1.028       -0.313       -0.715       \n",
            "   [1]  gerard              0.342        0.000        0.000        0.000        0.000        -0.324       0.000        \n",
            "   [1]  depardieu           0.364        0.000        0.000        0.000        0.000        -0.331       0.000        \n",
            "   [1]  this                0.401        0.000        0.000        0.000        0.000        -0.347       0.000        \n",
            "   [1]  is                  0.431        0.000        0.000        0.000        0.000        -0.364       0.000        \n",
            "   [1]  the                 0.503        0.000        0.000        0.000        0.000        -0.379       0.000        \n",
            "   [1]  worst               0.454        0.000        0.000        0.000        0.000        -0.395       0.000        \n",
            "   [1]  remake              0.468        0.000        0.000        0.000        0.000        -0.379       0.000        \n",
            "   [1]  of                  0.565        0.000        0.000        0.000        0.000        -0.378       0.000        \n",
            "   [1]  a                   0.629        0.000        0.000        0.000        0.000        -0.401       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  has                 0.467        0.000        0.000        0.000        -2.186       -0.290       -0.000       \n",
            "   [0]  cowboys             0.436        4.939        -12.818      -0.830       -2.454       -0.305       -2.149       \n",
            "   [1]  be                  0.490        0.000        0.000        0.000        -1.823       -0.311       -0.000       \n",
            "   [1]  one                 0.511        0.000        0.000        0.000        -2.046       -0.358       -0.000       \n",
            "   [1]  of                  0.597        0.000        0.000        0.000        -2.298       -0.382       -0.000       \n",
            "   [0]  to                  0.652        6.058        -4.604       -0.428       -2.579       -0.433       -2.147       \n",
            "   [1]  all                 0.721        0.000        0.000        0.000        -2.415       -0.473       -0.000       \n",
            "   [0]  extreme             0.635        6.411        -8.262       -0.454       -2.712       -0.538       -2.173       \n",
            "   [0]  such                0.611        10.481       -6.822       -0.493       -2.534       -0.482       -2.052       \n",
            "   [1]  despite             0.588        0.000        0.000        0.000        -2.292       -0.453       -0.000       \n",
            "   [1]  its                 0.509        0.000        0.000        0.000        -2.573       -0.432       -0.000       \n",
            "   [1]  cheekiness          0.493        0.000        0.000        0.000        -2.888       -0.373       -0.000       \n",
            "   [0]  collection          0.391        4.491        -8.374       -0.939       -3.243       -0.354       -2.888       \n",
            "   [0]  ladylove            0.364        7.987        -12.416      -1.011       -2.586       -0.292       -2.293       \n",
            "   [1]  model               0.376        0.000        0.000        0.000        -1.768       -0.268       -0.000       \n",
            "   [1]  reproductions       0.387        0.000        0.000        0.000        -1.984       -0.276       -0.000       \n",
            "   [0]  hope                0.313        4.627        -7.610       -1.163       -2.228       -0.289       -1.939       \n",
            "   [1]  oil                 0.308        0.000        0.000        0.000        -1.196       -0.256       -0.000       \n",
            "   [0]  worst               0.261        12.490       -6.671       -1.343       -1.343       -0.251       -1.092       \n",
            "   [1]  its                 0.198        0.000        0.000        0.000        0.000        -0.229       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  japanese            0.586        5.612        -9.167       -0.535       -2.665       -0.297       -2.368       \n",
            "   [1]  anyone              0.586        0.000        0.000        0.000        -2.392       -0.338       -0.000       \n",
            "   [1]  else                0.477        0.000        0.000        0.000        -2.686       -0.355       -0.000       \n",
            "   [0]  boredom             0.461        6.181        -12.436      -0.774       -3.015       -0.354       -2.661       \n",
            "   [0]  and                 0.520        5.703        -3.825       -0.654       -2.516       -0.359       -2.157       \n",
            "   [0]  just                0.471        5.517        -5.452       -0.753       -2.091       -0.368       -1.723       \n",
            "   [0]  movie               0.517        10.122       -4.350       -0.660       -1.502       -0.364       -1.138       \n",
            "   [1]  that                0.589        0.000        0.000        0.000        -0.945       -0.376       -0.000       \n",
            "   [1]  all                 0.682        0.000        0.000        0.000        -1.061       -0.378       -0.000       \n",
            "   [1]  the                 0.732        0.000        0.000        0.000        -1.192       -0.387       -0.000       \n",
            "   [0]  the                 0.765        7.858        -3.477       -0.267       -1.338       -0.389       -0.949       \n",
            "   [1]  characters          0.739        0.000        0.000        0.000        -1.202       -0.386       -0.000       \n",
            "   [1]  ended               0.758        0.000        0.000        0.000        -1.349       -0.369       -0.000       \n",
            "   [0]  slowmotion          0.744        6.398        -13.106      -0.296       -1.515       -0.369       -1.146       \n",
            "   [0]  scott               0.710        4.545        -8.531       -0.343       -1.368       -0.361       -1.007       \n",
            "   [1]  the                 0.739        0.000        0.000        0.000        -1.151       -0.348       -0.000       \n",
            "   [0]  above               0.741        7.843        -8.703       -0.300       -1.292       -0.350       -0.941       \n",
            "   [1]  spots               0.680        0.000        0.000        0.000        -1.114       -0.347       -0.000       \n",
            "   [0]  century             0.534        5.275        -7.695       -0.627       -1.251       -0.329       -0.922       \n",
            "   [0]  circumvented        0.497        5.278        -13.683      -0.700       -0.700       -0.306       -0.394       \n",
            "Samples\n",
            "Sample 0 .  somebody sinister has seen the credit pleasant reflects pierre richard get gerard depardieu this is the worst remake of a\n",
            "Sample 1 .  has cowboys be one of to all extreme such despite its cheekiness collection ladylove model reproductions hope oil worst its\n",
            "Sample 2 .  japanese anyone else boredom and just movie that all the the characters ended slowmotion scott the above spots century circumvented\n",
            "\n",
            "\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 0 0 1 0 1 1 0 1 0...]...][[99 42 69849 69849 78 69849 19 9 69849 4...]...][[42 1125 228 78 10 19 9 1739 4 663...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 0 0 1 0 1 1 0 1 0...]...][[99 42 69849 69849 78 69849 19 9 69849 4...]...][[42 13 8 78 296 19 9 92 4 9...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 0 0 0 0 0 0 1 1...]...][[25473 69849 45 69849 69849 69849 69849 69849 69849 55...]...][[185 45 329 40 894 564 3 1902 55 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 0 0 0 0 0 0 1 1...]...][[25473 69849 45 69849 69849 69849 69849 69849 69849 55...]...][[953 45 11 5 630 2 13 12 55 15...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 7 174 2 1 301 4 22800 3 15795...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 23 174 19 12 301 4 22800 20 15795...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 37 174 1591 1077 301 4 22800 3176 15795...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "global_step: 294\n",
            " perplexity: 959.387\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            " percent of 3-grams captured: 0.154.\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            " percent of 2-grams captured: 0.578.\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            " percent of 4-grams captured: 0.024.\n",
            " geometric_avg: 0.129.\n",
            " arithmetic_avg: 0.252.\n",
            "global_step: 294\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67172\n",
            " G train loss: 155.09949\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 23 174 19 12 301 4 22800 20 15795...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 21 174 5 2 301 4 22800 69518 15795...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  is                  0.489        0.000        0.000        0.000        -2.289       -0.299       -0.000       \n",
            "   [0]  t                   0.491        5.964        -5.596       -0.711       -2.569       -0.339       -2.230       \n",
            "   [1]  every               0.459        0.000        0.000        0.000        -2.087       -0.364       -0.000       \n",
            "   [0]  is                  0.469        4.980        -3.890       -0.757       -2.343       -0.363       -1.980       \n",
            "   [0]  a                   0.508        5.113        -3.988       -0.677       -1.780       -0.366       -1.413       \n",
            "   [1]  job                 0.576        0.000        0.000        0.000        -1.238       -0.379       -0.000       \n",
            "   [1]  to                  0.606        0.000        0.000        0.000        -1.390       -0.402       -0.000       \n",
            "   [1]  stimulate           0.597        0.000        0.000        0.000        -1.561       -0.419       -0.000       \n",
            "   [0]  nbb                 0.571        5.730        -12.585      -0.560       -1.752       -0.417       -1.335       \n",
            "   [1]  superficially       0.557        0.000        0.000        0.000        -1.338       -0.407       -0.000       \n",
            "   [1]  i                   0.565        0.000        0.000        0.000        -1.502       -0.401       -0.000       \n",
            "   [0]  i                   0.576        6.754        -4.285       -0.552       -1.686       -0.407       -1.280       \n",
            "   [1]  take                0.574        0.000        0.000        0.000        -1.273       -0.414       -0.000       \n",
            "   [1]  an                  0.605        0.000        0.000        0.000        -1.429       -0.411       -0.000       \n",
            "   [1]  ambitious           0.606        0.000        0.000        0.000        -1.605       -0.425       -0.000       \n",
            "   [0]  guy                 0.524        8.801        -7.565       -0.645       -1.802       -0.430       -1.372       \n",
            "   [1]  over                0.455        0.000        0.000        0.000        -1.298       -0.401       -0.000       \n",
            "   [0]  premchand           0.457        4.373        -12.504      -0.783       -1.457       -0.370       -1.088       \n",
            "   [0]  squeak              0.469        11.999       -12.635      -0.757       -0.757       -0.364       -0.393       \n",
            "   [1]  market              0.416        0.000        0.000        0.000        0.000        -0.366       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  getting             0.447        3.668        -8.000       -0.806       -4.678       -0.300       -4.378       \n",
            "   [0]  17                  0.424        6.258        -8.677       -0.857       -4.347       -0.297       -4.050       \n",
            "   [0]  this                0.453        5.438        -3.687       -0.792       -3.918       -0.300       -3.618       \n",
            "   [1]  the                 0.512        0.000        0.000        0.000        -3.509       -0.319       -0.000       \n",
            "   [0]  a                   0.560        6.465        -3.268       -0.580       -3.940       -0.356       -3.584       \n",
            "   [0]  takashi             0.505        4.440        -9.204       -0.682       -3.771       -0.387       -3.385       \n",
            "   [0]  phishing            0.488        4.033        -13.751      -0.718       -3.468       -0.371       -3.097       \n",
            "   [1]  ve                  0.497        0.000        0.000        0.000        -3.088       -0.359       -0.000       \n",
            "   [0]  in                  0.545        6.356        -4.486       -0.607       -3.467       -0.364       -3.103       \n",
            "   [0]  tan                 0.520        6.510        -12.785      -0.653       -3.211       -0.385       -2.826       \n",
            "   [0]  i                   0.538        5.741        -4.093       -0.620       -2.872       -0.379       -2.493       \n",
            "   [1]  i                   0.562        0.000        0.000        0.000        -2.528       -0.383       -0.000       \n",
            "   [0]  day                 0.551        6.288        -7.985       -0.596       -2.838       -0.391       -2.447       \n",
            "   [0]  time                0.556        7.844        -6.487       -0.587       -2.517       -0.385       -2.132       \n",
            "   [1]  lots                0.532        0.000        0.000        0.000        -2.167       -0.384       -0.000       \n",
            "   [0]  suffocates          0.495        4.286        -13.097      -0.702       -2.433       -0.373       -2.060       \n",
            "   [0]  and                 0.537        5.536        -4.489       -0.622       -1.942       -0.354       -1.588       \n",
            "   [0]  on                  0.580        7.340        -5.914       -0.545       -1.482       -0.369       -1.113       \n",
            "   [0]  disengagement       0.570        5.849        -12.845      -0.561       -1.052       -0.394       -0.658       \n",
            "   [0]  one                 0.576        3.942        -5.317       -0.551       -0.551       -0.399       -0.151       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  movie               0.510        0.000        0.000        0.000        -2.517       -0.308       -0.000       \n",
            "   [1]  was                 0.501        0.000        0.000        0.000        -2.826       -0.359       -0.000       \n",
            "   [1]  lacking             0.483        0.000        0.000        0.000        -3.173       -0.390       -0.000       \n",
            "   [0]  where               0.494        3.327        -6.635       -0.706       -3.562       -0.403       -3.159       \n",
            "   [0]  thrive              0.456        5.898        -10.996      -0.786       -3.207       -0.409       -2.798       \n",
            "   [0]  oddly               0.385        8.328        -8.872       -0.955       -2.718       -0.403       -2.315       \n",
            "   [1]  and                 0.430        0.000        0.000        0.000        -1.979       -0.376       -0.000       \n",
            "   [0]  hard                0.477        3.264        -7.914       -0.740       -2.222       -0.360       -1.862       \n",
            "   [0]  an                  0.530        11.120       -5.594       -0.634       -1.664       -0.371       -1.293       \n",
            "   [0]  movie               0.564        3.765        -4.277       -0.572       -1.157       -0.401       -0.756       \n",
            "   [1]  realism             0.588        0.000        0.000        0.000        -0.656       -0.433       -0.000       \n",
            "   [1]  the                 0.621        0.000        0.000        0.000        -0.737       -0.463       -0.000       \n",
            "   [1]  dialogue            0.557        0.000        0.000        0.000        -0.827       -0.490       -0.000       \n",
            "   [1]  was                 0.537        0.000        0.000        0.000        -0.928       -0.481       -0.000       \n",
            "   [1]  muddled             0.530        0.000        0.000        0.000        -1.042       -0.464       -0.000       \n",
            "   [0]  m                   0.486        9.725        -6.185       -0.721       -1.170       -0.451       -0.719       \n",
            "   [1]  and                 0.529        0.000        0.000        0.000        -0.505       -0.434       -0.000       \n",
            "   [1]  trying              0.496        0.000        0.000        0.000        -0.566       -0.432       -0.000       \n",
            "   [0]  a                   0.529        6.172        -3.545       -0.636       -0.636       -0.425       -0.211       \n",
            "   [1]  hard                0.561        0.000        0.000        0.000        0.000        -0.427       0.000        \n",
            "Samples\n",
            "Sample 0 .  is t every is a job to stimulate nbb superficially i i take an ambitious guy over premchand squeak market\n",
            "Sample 1 .  getting 17 this the a takashi phishing ve in tan i i day time lots suffocates and on disengagement one\n",
            "Sample 2 .  movie was lacking where thrive oddly and hard an movie realism the dialogue was muddled m and trying a hard\n",
            "\n",
            "\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 0 1 1 1 1 1 0 0 0...]...][[761 9365 69849 451 8 1844 15 26 69849 69849...]...][[9365 21650 451 8 1844 15 26 183 518 5809...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 0 1 1 1 1 1 0 0 0...]...][[761 9365 69849 451 8 1844 15 26 69849 69849...]...][[9365 35987 451 8 1844 15 26 188 33 5...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[1 1 0 0 1 1 0 0 1 1...]...][[31 84 9 69849 69849 2277 2 69849 69849 75...]...][[84 9 67 0 2277 2 171 3 75 319...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[1 1 0 0 1 1 0 0 1 1...]...][[31 84 9 69849 69849 2277 2 69849 69849 75...]...][[84 9 236 39326 2277 2 1 378 75 319...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 10 19 1 0 2 1023 31 664...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 77 126 73 126 2 1023 2431 2347...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 9 17 4 71 2 1023 13 44...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "global_step: 299\n",
            " perplexity: 935.495\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            " percent of 3-grams captured: 0.116.\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            " percent of 2-grams captured: 0.557.\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            " percent of 4-grams captured: 0.022.\n",
            " geometric_avg: 0.112.\n",
            " arithmetic_avg: 0.232.\n",
            "global_step: 299\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67236\n",
            " G train loss: 154.68829\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 77 126 73 126 2 1023 2431 2347...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 2152 74 30 71 2 1023 689 3...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  should              0.470        0.000        0.000        0.000        -2.845       -0.297       -0.000       \n",
            "   [1]  have                0.454        0.000        0.000        0.000        -3.194       -0.325       -0.000       \n",
            "   [0]  delivered           0.493        6.159        -10.063      -0.708       -3.586       -0.343       -3.243       \n",
            "   [0]  bad                 0.525        8.279        -6.468       -0.645       -3.230       -0.373       -2.858       \n",
            "   [0]  all                 0.576        6.308        -5.675       -0.552       -2.903       -0.398       -2.505       \n",
            "   [0]  me                  0.538        8.267        -6.061       -0.620       -2.640       -0.436       -2.203       \n",
            "   [1]  a                   0.549        0.000        0.000        0.000        -2.268       -0.427       -0.000       \n",
            "   [1]  former              0.558        0.000        0.000        0.000        -2.546       -0.419       -0.000       \n",
            "   [0]  problems            0.564        11.092       -8.606       -0.572       -2.858       -0.414       -2.444       \n",
            "   [0]  of                  0.619        12.053       -3.815       -0.480       -2.567       -0.414       -2.153       \n",
            "   [0]  karloff             0.559        12.150       -10.915      -0.581       -2.342       -0.443       -1.899       \n",
            "   [0]  already             0.540        13.425       -7.979       -0.616       -1.977       -0.418       -1.558       \n",
            "   [1]  the                 0.572        0.000        0.000        0.000        -1.528       -0.395       -0.000       \n",
            "   [1]  murder              0.553        0.000        0.000        0.000        -1.715       -0.400       -0.000       \n",
            "   [0]  the                 0.578        3.832        -3.213       -0.547       -1.926       -0.395       -1.531       \n",
            "   [0]  through             0.545        3.707        -7.755       -0.606       -1.548       -0.406       -1.142       \n",
            "   [0]  to                  0.567        11.002       -4.129       -0.567       -1.057       -0.387       -0.669       \n",
            "   [1]  dentist             0.557        0.000        0.000        0.000        -0.549       -0.392       -0.000       \n",
            "   [0]  have                0.540        3.942        -5.213       -0.617       -0.617       -0.387       -0.229       \n",
            "   [1]  the                 0.570        0.000        0.000        0.000        0.000        -0.377       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  is                  0.494        7.603        -2.572       -0.706       -3.508       -0.298       -3.210       \n",
            "   [0]  a                   0.515        11.092       -3.083       -0.663       -3.146       -0.334       -2.812       \n",
            "   [1]  is                  0.516        0.000        0.000        0.000        -2.787       -0.359       -0.000       \n",
            "   [0]  my                  0.540        11.951       -6.195       -0.616       -3.129       -0.374       -2.755       \n",
            "   [0]  film                0.556        11.712       -4.572       -0.588       -2.822       -0.388       -2.434       \n",
            "   [1]  to                  0.566        0.000        0.000        0.000        -2.508       -0.401       -0.000       \n",
            "   [1]  some                0.535        0.000        0.000        0.000        -2.816       -0.411       -0.000       \n",
            "   [0]  rated               0.446        4.027        -7.718       -0.807       -3.161       -0.405       -2.756       \n",
            "   [0]  gore                0.450        3.389        -9.041       -0.797       -2.642       -0.367       -2.275       \n",
            "   [0]  dance               0.445        12.322       -9.129       -0.810       -2.071       -0.344       -1.728       \n",
            "   [1]  in                  0.487        0.000        0.000        0.000        -1.416       -0.335       -0.000       \n",
            "   [0]  girlfriend          0.475        6.652        -9.216       -0.744       -1.590       -0.345       -1.244       \n",
            "   [1]  yuzna               0.492        0.000        0.000        0.000        -0.950       -0.356       -0.000       \n",
            "   [1]  has                 0.474        0.000        0.000        0.000        -1.066       -0.363       -0.000       \n",
            "   [1]  been                0.466        0.000        0.000        0.000        -1.197       -0.360       -0.000       \n",
            "   [0]  was                 0.473        8.693        -4.574       -0.748       -1.344       -0.350       -0.994       \n",
            "   [1]  as                  0.492        0.000        0.000        0.000        -0.669       -0.345       -0.000       \n",
            "   [0]  hate                0.472        3.532        -7.693       -0.751       -0.751       -0.349       -0.402       \n",
            "   [1]  director            0.474        0.000        0.000        0.000        0.000        -0.349       0.000        \n",
            "   [1]  society             0.416        0.000        0.000        0.000        0.000        -0.350       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  someone             0.447        0.000        0.000        0.000        -3.606       -0.300       -0.000       \n",
            "   [0]  brochures           0.433        6.688        -13.140      -0.836       -4.049       -0.325       -3.724       \n",
            "   [0]  movies              0.432        6.473        -6.023       -0.840       -3.606       -0.337       -3.269       \n",
            "   [1]  been                0.428        0.000        0.000        0.000        -3.106       -0.347       -0.000       \n",
            "   [0]  little              0.456        12.363       -7.199       -0.785       -3.487       -0.351       -3.136       \n",
            "   [0]  premiere            0.400        5.831        -9.143       -0.915       -3.033       -0.363       -2.670       \n",
            "   [1]  her                 0.377        0.000        0.000        0.000        -2.378       -0.351       -0.000       \n",
            "   [1]  life                0.387        0.000        0.000        0.000        -2.670       -0.342       -0.000       \n",
            "   [0]  what                0.461        3.778        -6.384       -0.775       -2.997       -0.342       -2.655       \n",
            "   [1]  is                  0.483        0.000        0.000        0.000        -2.495       -0.369       -0.000       \n",
            "   [0]  10                  0.461        7.126        -6.841       -0.775       -2.801       -0.390       -2.410       \n",
            "   [1]  a                   0.487        0.000        0.000        0.000        -2.274       -0.387       -0.000       \n",
            "   [0]  and                 0.515        8.849        -3.775       -0.663       -2.553       -0.384       -2.169       \n",
            "   [1]  teacher             0.515        0.000        0.000        0.000        -2.122       -0.388       -0.000       \n",
            "   [0]  waste               0.483        3.785        -8.174       -0.728       -2.382       -0.384       -1.999       \n",
            "   [0]  no                  0.492        3.635        -6.198       -0.709       -1.857       -0.370       -1.487       \n",
            "   [0]  much                0.475        8.407        -6.116       -0.745       -1.289       -0.363       -0.926       \n",
            "   [1]  for                 0.510        0.000        0.000        0.000        -0.611       -0.356       -0.000       \n",
            "   [0]  70mm                0.504        3.633        -13.663      -0.686       -0.686       -0.363       -0.323       \n",
            "   [1]  dance               0.477        0.000        0.000        0.000        0.000        -0.365       0.000        \n",
            "Samples\n",
            "Sample 0 .  should have delivered bad all me a former problems of karloff already the murder the through to dentist have the\n",
            "Sample 1 .  is a is my film to some rated gore dance in girlfriend yuzna has been was as hate director society\n",
            "Sample 2 .  someone brochures movies been little premiere her life what is 10 a and teacher waste no much for 70mm dance\n",
            "\n",
            "\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 1 0 1 1 1 1 1 0 0...]...][[47 2 1349 69849 19 10 5 92 8 69849...]...][[2 1349 115 19 10 5 92 8 5290 22...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 1 0 1 1 1 1 1 0 0...]...][[47 2 1349 69849 19 10 5 92 8 69849...]...][[2 1349 164 19 10 5 92 8 318 366...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 1 1 1 0 0 1 0...]...][[157 69849 4 69849 6756 439 78 69849 69849 33...]...][[4140 4 2666 6756 439 78 0 797 33 575...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 1 1 1 0 0 1 0...]...][[157 69849 4 69849 6756 439 78 69849 69849 33...]...][[3463 4 97 6756 439 78 5313 5 33 8183...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 85 7...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 2075 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 143 7...]...]\n",
            "I0310 00:32:12.858704 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "I0310 00:32:15.366347 139852867667712 supervisor.py:1099] global_step/sec: 0.28639\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "global_step: 304\n",
            " perplexity: 931.961\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            " percent of 3-grams captured: 0.139.\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            " percent of 2-grams captured: 0.547.\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            " percent of 4-grams captured: 0.023.\n",
            " geometric_avg: 0.120.\n",
            " arithmetic_avg: 0.236.\n",
            "global_step: 304\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67418\n",
            " G train loss: 154.74989\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 2075 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 8 7...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  movie               0.506        0.000        0.000        0.000        -1.113       -0.303       -0.000       \n",
            "   [1]  is                  0.507        0.000        0.000        0.000        -1.250       -0.343       -0.000       \n",
            "   [1]  basically           0.542        0.000        0.000        0.000        -1.403       -0.361       -0.000       \n",
            "   [1]  a                   0.556        0.000        0.000        0.000        -1.575       -0.374       -0.000       \n",
            "   [1]  action              0.590        0.000        0.000        0.000        -1.769       -0.380       -0.000       \n",
            "   [1]  movie               0.583        0.000        0.000        0.000        -1.985       -0.391       -0.000       \n",
            "   [1]  at                  0.565        0.000        0.000        0.000        -2.229       -0.396       -0.000       \n",
            "   [1]  its                 0.511        0.000        0.000        0.000        -2.503       -0.395       -0.000       \n",
            "   [0]  in                  0.537        14.286       -3.796       -0.621       -2.810       -0.373       -2.437       \n",
            "   [1]  it                  0.530        0.000        0.000        0.000        -2.457       -0.372       -0.000       \n",
            "   [0]  this                0.546        13.844       -4.003       -0.605       -2.758       -0.367       -2.391       \n",
            "   [0]  i                   0.549        8.995        -3.233       -0.599       -2.417       -0.370       -2.048       \n",
            "   [1]  depth               0.549        0.000        0.000        0.000        -2.042       -0.375       -0.000       \n",
            "   [0]  ed                  0.489        3.231        -8.504       -0.716       -2.292       -0.380       -1.912       \n",
            "   [0]  movie               0.504        6.274        -4.645       -0.684       -1.769       -0.369       -1.400       \n",
            "   [1]  but                 0.534        0.000        0.000        0.000        -1.218       -0.372       -0.000       \n",
            "   [0]  show                0.553        14.108       -6.652       -0.592       -1.367       -0.380       -0.987       \n",
            "   [1]  that                0.567        0.000        0.000        0.000        -0.870       -0.386       -0.000       \n",
            "   [0]  of                  0.611        4.928        -3.779       -0.493       -0.977       -0.386       -0.591       \n",
            "   [0]  noir                0.581        6.645        -7.773       -0.544       -0.544       -0.398       -0.146       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  guess               0.498        0.000        0.000        0.000        -2.146       -0.300       -0.000       \n",
            "   [1]  i                   0.495        0.000        0.000        0.000        -2.409       -0.357       -0.000       \n",
            "   [1]  can                 0.472        0.000        0.000        0.000        -2.705       -0.394       -0.000       \n",
            "   [0]  skepticism          0.446        6.278        -9.522       -0.808       -3.037       -0.395       -2.642       \n",
            "   [0]  cheesing            0.463        3.551        -13.221      -0.770       -2.502       -0.376       -2.126       \n",
            "   [1]  point               0.419        0.000        0.000        0.000        -1.945       -0.373       -0.000       \n",
            "   [1]  this                0.451        0.000        0.000        0.000        -2.183       -0.342       -0.000       \n",
            "   [1]  guy                 0.401        0.000        0.000        0.000        -2.451       -0.345       -0.000       \n",
            "   [0]  start               0.394        5.030        -7.789       -0.932       -2.752       -0.324       -2.428       \n",
            "   [1]  trying              0.377        0.000        0.000        0.000        -2.043       -0.312       -0.000       \n",
            "   [1]  to                  0.426        0.000        0.000        0.000        -2.294       -0.298       -0.000       \n",
            "   [1]  make                0.419        0.000        0.000        0.000        -2.575       -0.323       -0.000       \n",
            "   [0]  actors              0.426        5.674        -7.147       -0.852       -2.891       -0.332       -2.558       \n",
            "   [0]  suspense            0.376        6.066        -7.602       -0.979       -2.289       -0.342       -1.946       \n",
            "   [1]  why                 0.418        0.000        0.000        0.000        -1.471       -0.319       -0.000       \n",
            "   [1]  did                 0.418        0.000        0.000        0.000        -1.651       -0.336       -0.000       \n",
            "   [1]  it                  0.412        0.000        0.000        0.000        -1.854       -0.342       -0.000       \n",
            "   [0]  for                 0.455        5.335        -5.345       -0.788       -2.081       -0.338       -1.743       \n",
            "   [0]  d                   0.478        4.014        -7.408       -0.739       -1.451       -0.364       -1.087       \n",
            "   [0]  childhood           0.450        5.784        -8.887       -0.799       -0.799       -0.393       -0.407       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  am                  0.481        0.000        0.000        0.000        -2.965       -0.303       -0.000       \n",
            "   [0]  worst               0.423        9.052        -6.452       -0.859       -3.329       -0.343       -2.986       \n",
            "   [0]  somewhat            0.459        4.269        -8.281       -0.780       -2.772       -0.357       -2.415       \n",
            "   [1]  what                0.520        0.000        0.000        0.000        -2.237       -0.365       -0.000       \n",
            "   [0]  the                 0.547        10.158       -3.799       -0.604       -2.511       -0.388       -2.123       \n",
            "   [0]  cooney              0.526        9.632        -12.620      -0.643       -2.141       -0.407       -1.734       \n",
            "   [1]  was                 0.514        0.000        0.000        0.000        -1.682       -0.409       -0.000       \n",
            "   [1]  using               0.482        0.000        0.000        0.000        -1.889       -0.402       -0.000       \n",
            "   [0]  all                 0.537        4.586        -6.156       -0.621       -2.121       -0.389       -1.731       \n",
            "   [0]  new                 0.517        3.749        -6.747       -0.660       -1.684       -0.400       -1.284       \n",
            "   [1]  movie               0.527        0.000        0.000        0.000        -1.149       -0.407       -0.000       \n",
            "   [1]  and                 0.545        0.000        0.000        0.000        -1.290       -0.409       -0.000       \n",
            "   [1]  also                0.549        0.000        0.000        0.000        -1.448       -0.415       -0.000       \n",
            "   [1]  the                 0.568        0.000        0.000        0.000        -1.626       -0.418       -0.000       \n",
            "   [1]  caliber             0.578        0.000        0.000        0.000        -1.826       -0.422       -0.000       \n",
            "   [0]  doltish             0.554        4.325        -12.859      -0.591       -2.050       -0.432       -1.618       \n",
            "   [1]  the                 0.571        0.000        0.000        0.000        -1.637       -0.431       -0.000       \n",
            "   [0]  mustachioed         0.554        11.643       -12.785      -0.590       -1.838       -0.428       -1.410       \n",
            "   [0]  both                0.504        4.996        -8.102       -0.685       -1.401       -0.423       -0.978       \n",
            "   [0]  chinese             0.447        6.325        -8.598       -0.804       -0.804       -0.402       -0.402       \n",
            "Samples\n",
            "Sample 0 .  movie is basically a action movie at its in it this i depth ed movie but show that of noir\n",
            "Sample 1 .  guess i can skepticism cheesing point this guy start trying to make actors suspense why did it for d childhood\n",
            "Sample 2 .  am worst somewhat what the cooney was using all new movie and also the caliber doltish the mustachioed both chinese\n",
            "\n",
            "\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 1 1 0 0 1 1 0 0...]...][[264 69849 69849 10 17 69849 69849 16 804 69849...]...][[9 1281 10 17 2 223 16 804 2 605...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 1 1 0 0 1 1 0 0...]...][[264 69849 69849 10 17 69849 69849 16 804 69849...]...][[94 27 10 17 619 949 16 804 13221 484...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 1 1 0 1 1 1 0 1 1...]...][[9 139 110 30 69849 3 0 98 69849 10...]...][[139 110 30 668 3 0 98 8 10 202...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 1 1 0 1 1 1 0 1 1...]...][[9 139 110 30 69849 3 0 98 69849 10...]...][[139 110 30 123 3 0 98 153 10 202...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 153 1563 642 3 478 85 88 47357 38...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 2 1563 120 3 478 2124 338 98 38...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 480 1563 17552 3 478 817 838 7532 38...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "global_step: 309\n",
            " perplexity: 931.322\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            " percent of 3-grams captured: 0.158.\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            " percent of 2-grams captured: 0.540.\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            " percent of 4-grams captured: 0.023.\n",
            " geometric_avg: 0.126.\n",
            " arithmetic_avg: 0.240.\n",
            "global_step: 309\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67473\n",
            " G train loss: 154.00789\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 2 1563 120 3 478 2124 338 98 38...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 430 1563 526 3 478 17 1 23 38...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  is                  0.503        0.000        0.000        0.000        -3.021       -0.292       -0.000       \n",
            "   [0]  problem             0.510        3.807        -9.277       -0.674       -3.391       -0.335       -3.056       \n",
            "   [1]  rip                 0.419        0.000        0.000        0.000        -3.051       -0.370       -0.000       \n",
            "   [0]  self                0.369        6.917        -8.235       -0.997       -3.425       -0.292       -3.134       \n",
            "   [1]  of                  0.470        0.000        0.000        0.000        -2.726       -0.232       -0.000       \n",
            "   [1]  already             0.481        0.000        0.000        0.000        -3.060       -0.328       -0.000       \n",
            "   [0]  movie               0.490        10.877       -4.120       -0.714       -3.436       -0.361       -3.074       \n",
            "   [0]  and                 0.513        8.196        -3.820       -0.668       -3.056       -0.384       -2.672       \n",
            "   [0]  not                 0.503        5.661        -5.055       -0.687       -2.681       -0.408       -2.273       \n",
            "   [1]  like                0.497        0.000        0.000        0.000        -2.239       -0.397       -0.000       \n",
            "   [0]  format              0.447        8.300        -9.052       -0.806       -2.513       -0.379       -2.134       \n",
            "   [1]  and                 0.483        0.000        0.000        0.000        -1.917       -0.319       -0.000       \n",
            "   [1]  i                   0.499        0.000        0.000        0.000        -2.152       -0.341       -0.000       \n",
            "   [0]  version             0.496        6.725        -7.660       -0.702       -2.416       -0.367       -2.049       \n",
            "   [0]  about               0.513        6.425        -5.584       -0.667       -1.924       -0.368       -1.556       \n",
            "   [0]  overly              0.441        5.439        -9.091       -0.819       -1.412       -0.393       -1.019       \n",
            "   [1]  did                 0.436        0.000        0.000        0.000        -0.666       -0.311       -0.000       \n",
            "   [1]  last                0.406        0.000        0.000        0.000        -0.748       -0.285       -0.000       \n",
            "   [0]  locations           0.432        8.800        -10.326      -0.840       -0.840       -0.256       -0.584       \n",
            "   [1]  the                 0.484        0.000        0.000        0.000        0.000        -0.286       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  bland               0.493        11.335       -11.153      -0.708       -2.669       -0.291       -2.378       \n",
            "   [1]  is                  0.494        0.000        0.000        0.000        -2.202       -0.327       -0.000       \n",
            "   [1]  a                   0.516        0.000        0.000        0.000        -2.472       -0.345       -0.000       \n",
            "   [1]  jewel               0.479        0.000        0.000        0.000        -2.776       -0.368       -0.000       \n",
            "   [0]  reviews             0.461        10.422       -7.713       -0.774       -3.116       -0.346       -2.770       \n",
            "   [1]  trying              0.427        0.000        0.000        0.000        -2.630       -0.338       -0.000       \n",
            "   [0]  strange             0.408        4.939        -7.861       -0.896       -2.953       -0.318       -2.635       \n",
            "   [1]  break               0.415        0.000        0.000        0.000        -2.310       -0.304       -0.000       \n",
            "   [0]  b                   0.417        6.633        -7.445       -0.875       -2.593       -0.308       -2.285       \n",
            "   [1]  society             0.370        0.000        0.000        0.000        -1.929       -0.316       -0.000       \n",
            "   [0]  hot                 0.367        4.924        -8.179       -1.003       -2.165       -0.294       -1.871       \n",
            "   [1]  the                 0.427        0.000        0.000        0.000        -1.304       -0.300       -0.000       \n",
            "   [1]  last                0.393        0.000        0.000        0.000        -1.464       -0.342       -0.000       \n",
            "   [1]  of                  0.483        0.000        0.000        0.000        -1.644       -0.326       -0.000       \n",
            "   [0]  for                 0.524        11.498       -5.480       -0.647       -1.846       -0.368       -1.478       \n",
            "   [0]  scream              0.477        11.556       -8.517       -0.740       -1.346       -0.386       -0.960       \n",
            "   [0]  times               0.506        4.358        -7.807       -0.680       -0.680       -0.341       -0.339       \n",
            "   [1]  1937                0.504        0.000        0.000        0.000        0.000        -0.352       0.000        \n",
            "   [1]  film                0.522        0.000        0.000        0.000        0.000        -0.350       0.000        \n",
            "   [1]  from                0.534        0.000        0.000        0.000        0.000        -0.361       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  br                  0.548        0.000        0.000        0.000        -3.304       -0.293       -0.000       \n",
            "   [1]  br                  0.571        0.000        0.000        0.000        -3.709       -0.333       -0.000       \n",
            "   [0]  is                  0.560        6.265        -2.995       -0.580       -4.164       -0.360       -3.804       \n",
            "   [0]  the                 0.578        6.315        -2.774       -0.549       -4.024       -0.362       -3.662       \n",
            "   [0]  feel                0.560        7.659        -8.548       -0.580       -3.902       -0.363       -3.539       \n",
            "   [0]  around              0.534        3.641        -8.658       -0.628       -3.729       -0.354       -3.375       \n",
            "   [1]  save                0.483        0.000        0.000        0.000        -3.482       -0.339       -0.000       \n",
            "   [0]  i                   0.503        5.784        -2.763       -0.688       -3.909       -0.322       -3.587       \n",
            "   [0]  point               0.458        3.458        -7.958       -0.780       -3.616       -0.327       -3.289       \n",
            "   [0]  pretty              0.504        4.696        -7.254       -0.684       -3.184       -0.326       -2.858       \n",
            "   [0]  i                   0.523        5.437        -2.753       -0.648       -2.806       -0.339       -2.467       \n",
            "   [0]  i                   0.530        5.435        -2.752       -0.634       -2.423       -0.356       -2.067       \n",
            "   [0]  as                  0.540        6.060        -4.694       -0.616       -2.009       -0.366       -1.642       \n",
            "   [1]  needs               0.540        0.000        0.000        0.000        -1.564       -0.367       -0.000       \n",
            "   [0]  last                0.490        3.612        -6.601       -0.713       -1.756       -0.364       -1.392       \n",
            "   [0]  should              0.482        5.534        -7.303       -0.729       -1.171       -0.348       -0.823       \n",
            "   [1]  many                0.484        0.000        0.000        0.000        -0.495       -0.339       -0.000       \n",
            "   [1]  more                0.486        0.000        0.000        0.000        -0.556       -0.342       -0.000       \n",
            "   [0]  for                 0.536        15.002       -5.183       -0.625       -0.625       -0.346       -0.279       \n",
            "   [1]  between             0.567        0.000        0.000        0.000        0.000        -0.358       0.000        \n",
            "Samples\n",
            "Sample 0 .  is problem rip self of already movie and not like format and i version about overly did last locations the\n",
            "Sample 1 .  bland is a jewel reviews trying strange break b society hot the last of for scream times 1937 film from\n",
            "Sample 2 .  br br is the feel around save i point pretty i i as needs last should many more for between\n",
            "\n",
            "\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[0 0 1 1 1 0 0 0 1 1...]...][[29 69849 69849 250 98 123 69849 69849 69849 8...]...][[3 0 250 98 123 92 23 64 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[0 0 1 1 1 0 0 0 1 1...]...][[29 69849 69849 250 98 123 69849 69849 69849 8...]...][[5 123 250 98 123 0 28963 0 8 0...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 0 1 0 1 1 0 0 0...]...][[10 2789 343 69849 451 69849 4 91 69849 69849...]...][[2789 343 201 451 55 4 91 6380 7745 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 0 1 0 1 1 0 0 0...]...][[10 2789 343 69849 451 69849 4 91 69849 69849...]...][[2789 343 16 451 2707 4 91 811 1255 6...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 5 11 99 147 1090...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 136 11 99 816 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 2040 11 99 49722 44...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "global_step: 314\n",
            " perplexity: 923.597\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            " percent of 3-grams captured: 0.139.\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            " percent of 2-grams captured: 0.579.\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            " percent of 4-grams captured: 0.027.\n",
            " geometric_avg: 0.130.\n",
            " arithmetic_avg: 0.249.\n",
            "global_step: 314\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67461\n",
            " G train loss: 151.97369\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 136 11 99 816 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 0 11 99 58313 1...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  of                  0.574        0.000        0.000        0.000        -1.013       -0.300       -0.000       \n",
            "   [1]  all                 0.618        0.000        0.000        0.000        -1.137       -0.379       -0.000       \n",
            "   [1]  let                 0.615        0.000        0.000        0.000        -1.277       -0.420       -0.000       \n",
            "   [1]  me                  0.582        0.000        0.000        0.000        -1.434       -0.410       -0.000       \n",
            "   [1]  just                0.533        0.000        0.000        0.000        -1.609       -0.368       -0.000       \n",
            "   [0]  the                 0.571        6.453        -3.033       -0.560       -1.807       -0.322       -1.485       \n",
            "   [1]  that                0.589        0.000        0.000        0.000        -1.400       -0.338       -0.000       \n",
            "   [1]  after               0.574        0.000        0.000        0.000        -1.572       -0.345       -0.000       \n",
            "   [0]  digressive          0.560        8.290        -14.036      -0.580       -1.764       -0.332       -1.432       \n",
            "   [0]  and                 0.577        4.420        -3.427       -0.550       -1.330       -0.320       -1.011       \n",
            "   [0]  old                 0.530        4.829        -6.489       -0.634       -0.876       -0.333       -0.543       \n",
            "   [1]  i                   0.540        0.000        0.000        0.000        -0.271       -0.303       0.000        \n",
            "   [1]  seriously           0.505        0.000        0.000        0.000        -0.304       -0.314       0.000        \n",
            "   [1]  thought             0.489        0.000        0.000        0.000        -0.342       -0.303       -0.000       \n",
            "   [1]  about               0.517        0.000        0.000        0.000        -0.384       -0.311       -0.000       \n",
            "   [1]  writing             0.484        0.000        0.000        0.000        -0.431       -0.345       -0.000       \n",
            "   [1]  my                  0.532        0.000        0.000        0.000        -0.484       -0.337       -0.000       \n",
            "   [1]  own                 0.546        0.000        0.000        0.000        -0.543       -0.366       -0.000       \n",
            "   [1]  screenplay          0.499        0.000        0.000        0.000        -0.610       -0.380       -0.000       \n",
            "   [0]  poorly              0.504        5.913        -8.697       -0.684       -0.684       -0.353       -0.332       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  is                  0.557        6.965        -2.945       -0.586       -3.331       -0.301       -3.030       \n",
            "   [1]  seeing              0.579        0.000        0.000        0.000        -3.082       -0.337       -0.000       \n",
            "   [0]  from                0.588        4.007        -5.657       -0.531       -3.460       -0.353       -3.106       \n",
            "   [0]  sj                  0.569        3.853        -12.887      -0.563       -3.289       -0.350       -2.938       \n",
            "   [0]  if                  0.553        5.050        -6.510       -0.592       -3.060       -0.336       -2.724       \n",
            "   [0]  top                 0.514        10.374       -7.854       -0.665       -2.771       -0.319       -2.452       \n",
            "   [1]  when                0.541        0.000        0.000        0.000        -2.364       -0.316       -0.000       \n",
            "   [0]  who                 0.562        6.820        -6.257       -0.577       -2.654       -0.324       -2.330       \n",
            "   [1]  32                  0.566        0.000        0.000        0.000        -2.332       -0.335       -0.000       \n",
            "   [0]  ms                  0.601        4.964        -11.232      -0.510       -2.618       -0.336       -2.282       \n",
            "   [1]  chicago             0.525        0.000        0.000        0.000        -2.367       -0.330       -0.000       \n",
            "   [0]  act                 0.518        4.957        -8.592       -0.658       -2.658       -0.312       -2.346       \n",
            "   [1]  broadcasting        0.484        0.000        0.000        0.000        -2.245       -0.315       -0.000       \n",
            "   [0]  compared            0.486        4.023        -8.161       -0.721       -2.521       -0.328       -2.193       \n",
            "   [1]  final               0.489        0.000        0.000        0.000        -2.021       -0.346       -0.000       \n",
            "   [0]  experience          0.521        7.880        -7.929       -0.652       -2.269       -0.370       -1.899       \n",
            "   [0]  cite                0.514        4.396        -13.381      -0.665       -1.815       -0.390       -1.425       \n",
            "   [0]  vhs                 0.489        6.675        -8.824       -0.715       -1.291       -0.395       -0.896       \n",
            "   [0]  film                0.524        6.840        -5.093       -0.647       -0.647       -0.396       -0.251       \n",
            "   [1]  school              0.568        0.000        0.000        0.000        0.000        -0.392       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  douglas             0.451        2.946        -8.774       -0.797       -3.447       -0.297       -3.151       \n",
            "   [1]  a                   0.491        0.000        0.000        0.000        -2.975       -0.315       -0.000       \n",
            "   [1]  horrible            0.437        0.000        0.000        0.000        -3.340       -0.351       -0.000       \n",
            "   [0]  tv                  0.423        7.815        -6.796       -0.860       -3.750       -0.348       -3.402       \n",
            "   [0]  atmosphere          0.438        8.391        -9.871       -0.825       -3.245       -0.356       -2.889       \n",
            "   [1]  but                 0.482        0.000        0.000        0.000        -2.717       -0.372       -0.000       \n",
            "   [1]  somehow             0.457        0.000        0.000        0.000        -3.050       -0.390       -0.000       \n",
            "   [0]  series              0.435        4.086        -7.208       -0.832       -3.424       -0.384       -3.040       \n",
            "   [0]  and                 0.473        4.259        -3.960       -0.748       -2.910       -0.376       -2.534       \n",
            "   [1]  also                0.497        0.000        0.000        0.000        -2.427       -0.388       -0.000       \n",
            "   [0]  all                 0.560        7.470        -5.894       -0.579       -2.725       -0.390       -2.334       \n",
            "   [0]  keeps               0.472        8.172        -8.678       -0.751       -2.409       -0.406       -2.003       \n",
            "   [0]  cinematography      0.423        6.015        -7.966       -0.860       -1.862       -0.367       -1.495       \n",
            "   [1]  are                 0.459        0.000        0.000        0.000        -1.125       -0.352       -0.000       \n",
            "   [0]  and                 0.505        6.940        -3.974       -0.683       -1.263       -0.371       -0.892       \n",
            "   [1]  kinds               0.496        0.000        0.000        0.000        -0.651       -0.390       -0.000       \n",
            "   [0]  fact                0.482        4.223        -7.301       -0.731       -0.731       -0.385       -0.346       \n",
            "   [1]  sci                 0.482        0.000        0.000        0.000        0.000        -0.374       0.000        \n",
            "   [1]  fi                  0.476        0.000        0.000        0.000        0.000        -0.368       0.000        \n",
            "   [1]  good                0.420        0.000        0.000        0.000        0.000        -0.364       0.000        \n",
            "Samples\n",
            "Sample 0 .  of all let me just the that after digressive and old i seriously thought about writing my own screenplay poorly\n",
            "Sample 1 .  is seeing from sj if top when who 32 ms chicago act broadcasting compared final experience cite vhs film school\n",
            "Sample 2 .  douglas a horrible tv atmosphere but somehow series and also all keeps cinematography are and kinds fact sci fi good\n",
            "\n",
            "\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[1 0 0 0 0 1 1 0 1 1...]...][[10 17 69849 69849 69849 69849 436 1 69849 868...]...][[17 32756 199 3895 5998 436 1 4242 868 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[1 0 0 0 0 1 1 0 1 1...]...][[10 17 69849 69849 69849 69849 436 1 69849 868...]...][[17 980 545 583 31 436 1 58 868 6...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 1 0 1 1 0 1 0 0 1...]...][[9 140 2 69849 3 193 69849 5920 69849 69849...]...][[140 2 340 3 193 156 5920 257 14340 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 1 0 1 1 0 1 0 0 1...]...][[9 140 2 69849 3 193 69849 5920 69849 69849...]...][[140 2 810 3 193 16 5920 598 959 1...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 10 333 10 160 9 91 4 663...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 228 78 10 19 9 1739 4 663...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 45 11 10 20 9 31269 4 663...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "global_step: 319\n",
            " perplexity: 914.941\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            " percent of 3-grams captured: 0.147.\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            " percent of 2-grams captured: 0.574.\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.121.\n",
            " arithmetic_avg: 0.247.\n",
            "global_step: 319\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67553\n",
            " G train loss: 151.54874\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 228 78 10 19 9 1739 4 663...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 0 27 10 0 9 3 4 663...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  just                0.466        0.000        0.000        0.000        -3.300       -0.298       -0.000       \n",
            "   [1]  15                  0.353        0.000        0.000        0.000        -3.705       -0.322       -0.000       \n",
            "   [0]  the                 0.390        8.635        -2.694       -0.942       -4.160       -0.339       -3.821       \n",
            "   [0]  have                0.382        6.764        -5.255       -0.962       -3.613       -0.380       -3.233       \n",
            "   [1]  this                0.411        0.000        0.000        0.000        -2.976       -0.402       -0.000       \n",
            "   [0]  the                 0.456        4.694        -2.466       -0.786       -3.341       -0.412       -2.929       \n",
            "   [1]  i                   0.464        0.000        0.000        0.000        -2.869       -0.407       -0.000       \n",
            "   [0]  of                  0.560        8.813        -3.502       -0.579       -3.221       -0.388       -2.834       \n",
            "   [1]  to                  0.593        0.000        0.000        0.000        -2.966       -0.374       -0.000       \n",
            "   [1]  miss                0.574        0.000        0.000        0.000        -3.330       -0.338       -0.000       \n",
            "   [0]  remarkably          0.485        11.893       -11.390      -0.723       -3.739       -0.294       -3.445       \n",
            "   [0]  me                  0.477        13.317       -6.261       -0.740       -3.385       -0.252       -3.134       \n",
            "   [0]  has                 0.469        4.441        -5.775       -0.757       -2.970       -0.255       -2.714       \n",
            "   [0]  not                 0.485        8.147        -4.877       -0.724       -2.484       -0.276       -2.208       \n",
            "   [1]  more                0.489        0.000        0.000        0.000        -1.976       -0.307       -0.000       \n",
            "   [0]  director            0.504        15.442       -6.930       -0.686       -2.218       -0.324       -1.894       \n",
            "   [0]  not                 0.512        5.975        -4.877       -0.669       -1.721       -0.337       -1.385       \n",
            "   [0]  is                  0.523        4.229        -3.992       -0.648       -1.181       -0.343       -0.838       \n",
            "   [0]  wish                0.550        8.159        -9.598       -0.599       -0.599       -0.341       -0.258       \n",
            "   [1]  at                  0.564        0.000        0.000        0.000        0.000        -0.339       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  taking              0.446        0.000        0.000        0.000        -3.087       -0.296       -0.000       \n",
            "   [0]  seen                0.513        7.017        -6.392       -0.667       -3.466       -0.308       -3.158       \n",
            "   [1]  for                 0.566        0.000        0.000        0.000        -3.142       -0.361       -0.000       \n",
            "   [0]  it                  0.542        3.257        -4.457       -0.613       -3.528       -0.390       -3.138       \n",
            "   [1]  santa               0.456        0.000        0.000        0.000        -3.273       -0.363       -0.000       \n",
            "   [0]  1981                0.470        11.911       -10.900      -0.755       -3.674       -0.314       -3.360       \n",
            "   [0]  again               0.448        4.731        -7.078       -0.804       -3.277       -0.315       -2.962       \n",
            "   [0]  to                  0.503        8.002        -4.253       -0.686       -2.776       -0.310       -2.466       \n",
            "   [1]  i                   0.522        0.000        0.000        0.000        -2.346       -0.346       -0.000       \n",
            "   [0]  version             0.511        6.516        -7.318       -0.672       -2.634       -0.359       -2.275       \n",
            "   [0]  in                  0.558        7.725        -4.625       -0.583       -2.203       -0.347       -1.856       \n",
            "   [1]  of                  0.658        0.000        0.000        0.000        -1.818       -0.358       -0.000       \n",
            "   [0]  tradition           0.560        4.164        -8.553       -0.579       -2.041       -0.393       -1.649       \n",
            "   [1]  movie               0.561        0.000        0.000        0.000        -1.642       -0.321       -0.000       \n",
            "   [0]  i                   0.580        7.880        -3.971       -0.545       -1.843       -0.311       -1.533       \n",
            "   [0]  occupation          0.575        5.651        -13.035      -0.553       -1.457       -0.322       -1.136       \n",
            "   [0]  schwentke           0.551        7.759        -10.529      -0.596       -1.015       -0.319       -0.696       \n",
            "   [1]  i                   0.567        0.000        0.000        0.000        -0.471       -0.307       -0.000       \n",
            "   [1]  read                0.557        0.000        0.000        0.000        -0.528       -0.320       -0.000       \n",
            "   [0]  aircrew             0.553        4.295        -13.120      -0.593       -0.593       -0.314       -0.279       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  characters          0.477        4.746        -7.458       -0.740       -3.251       -0.297       -2.954       \n",
            "   [0]  minute              0.426        3.766        -7.885       -0.852       -2.818       -0.335       -2.483       \n",
            "   [1]  more                0.417        0.000        0.000        0.000        -2.207       -0.366       -0.000       \n",
            "   [0]  hollywood           0.466        8.336        -7.665       -0.764       -2.478       -0.385       -2.093       \n",
            "   [0]  lord                0.506        7.073        -8.659       -0.681       -1.924       -0.383       -1.541       \n",
            "   [0]  would               0.527        7.854        -6.585       -0.641       -1.395       -0.378       -1.018       \n",
            "   [1]  movies              0.517        0.000        0.000        0.000        -0.846       -0.359       -0.000       \n",
            "   [1]  is                  0.525        0.000        0.000        0.000        -0.950       -0.346       -0.000       \n",
            "   [1]  definitely          0.518        0.000        0.000        0.000        -1.067       -0.328       -0.000       \n",
            "   [1]  smart               0.402        0.000        0.000        0.000        -1.198       -0.321       -0.000       \n",
            "   [1]  house               0.407        0.000        0.000        0.000        -1.345       -0.355       -0.000       \n",
            "   [1]  this                0.457        0.000        0.000        0.000        -1.510       -0.366       -0.000       \n",
            "   [0]  when                0.493        8.542        -6.236       -0.706       -1.695       -0.368       -1.327       \n",
            "   [1]  a                   0.535        0.000        0.000        0.000        -1.110       -0.376       -0.000       \n",
            "   [1]  family              0.519        0.000        0.000        0.000        -1.246       -0.367       -0.000       \n",
            "   [0]  collection          0.411        8.720        -7.905       -0.888       -1.399       -0.360       -1.039       \n",
            "   [1]  a                   0.458        0.000        0.000        0.000        -0.574       -0.381       -0.000       \n",
            "   [1]  contest             0.486        0.000        0.000        0.000        -0.644       -0.369       -0.000       \n",
            "   [0]  tyson               0.485        4.048        -11.524      -0.723       -0.723       -0.372       -0.351       \n",
            "   [1]  live                0.423        0.000        0.000        0.000        0.000        -0.376       0.000        \n",
            "Samples\n",
            "Sample 0 .  just 15 the have this the i of to miss remarkably me has not more director not is wish at\n",
            "Sample 1 .  taking seen for it santa 1981 again to i version in of tradition movie i occupation schwentke i read aircrew\n",
            "Sample 2 .  characters minute more hollywood lord would movies is definitely smart house this when a family collection a contest tyson live\n",
            "\n",
            "\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[1 1 0 0 0 0 1 0 1 1...]...][[16 2 366 69849 69849 69849 69849 149 69849 17...]...][[2 366 9 80 23 395 149 10 17 8386...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[1 1 0 0 0 0 1 0 1 1...]...][[16 2 366 69849 69849 69849 69849 149 69849 17...]...][[2 366 5 112 5 8 149 173 17 8386...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[0 0 0 1 0 0 0 1 1 0...]...][[8 69849 69849 69849 36433 69849 69849 69849 1309 0...]...][[47 5 1640 36433 117 166 4 1309 0 4853...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[0 0 0 1 0 0 0 1 1 0...]...][[8 69849 69849 69849 36433 69849 69849 69849 1309 0...]...][[3041 3 76 36433 924 22530 0 1309 0 7...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[529 45 329 40 717 36 3 48 714 15...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[185 45 329 40 894 564 3 1902 55 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[13890 45 329 40 9 26 3 4 9 15...]...]\n",
            "I0310 00:33:12.858769 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "global_step: 324\n",
            " perplexity: 907.234\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            " percent of 3-grams captured: 0.143.\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            " percent of 2-grams captured: 0.564.\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.119.\n",
            " arithmetic_avg: 0.243.\n",
            "global_step: 324\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67459\n",
            " G train loss: 154.13297\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[185 45 329 40 894 564 3 1902 55 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[6 45 329 40 2645 19 3 1156 67 15...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  br                  0.542        7.584        -5.646       -0.612       -2.750       -0.304       -2.446       \n",
            "   [1]  has                 0.503        0.000        0.000        0.000        -2.400       -0.364       -0.000       \n",
            "   [1]  used                0.472        0.000        0.000        0.000        -2.695       -0.365       -0.000       \n",
            "   [1]  her                 0.437        0.000        0.000        0.000        -3.025       -0.365       -0.000       \n",
            "   [0]  25                  0.361        9.599        -9.186       -1.020       -3.396       -0.365       -3.031       \n",
            "   [0]  film                0.417        7.678        -5.095       -0.874       -2.668       -0.365       -2.303       \n",
            "   [1]  of                  0.551        0.000        0.000        0.000        -2.015       -0.401       -0.000       \n",
            "   [0]  starring            0.519        8.102        -7.681       -0.656       -2.262       -0.437       -1.825       \n",
            "   [0]  had                 0.495        6.391        -5.805       -0.703       -1.804       -0.409       -1.394       \n",
            "   [1]  with                0.597        0.000        0.000        0.000        -1.235       -0.374       -0.000       \n",
            "   [1]  two                 0.565        0.000        0.000        0.000        -1.387       -0.384       -0.000       \n",
            "   [1]  autistic            0.582        0.000        0.000        0.000        -1.557       -0.351       -0.000       \n",
            "   [0]  show                0.607        7.795        -6.861       -0.499       -1.748       -0.338       -1.410       \n",
            "   [0]  real                0.576        5.192        -7.357       -0.551       -1.402       -0.335       -1.067       \n",
            "   [0]  or                  0.613        3.995        -5.587       -0.489       -0.955       -0.307       -0.648       \n",
            "   [0]  think               0.593        11.136       -6.704       -0.523       -0.523       -0.309       -0.214       \n",
            "   [1]  she                 0.511        0.000        0.000        0.000        0.000        -0.298       0.000        \n",
            "   [1]  describes           0.477        0.000        0.000        0.000        0.000        -0.273       0.000        \n",
            "   [1]  as                  0.497        0.000        0.000        0.000        0.000        -0.280       0.000        \n",
            "   [1]  a                   0.540        0.000        0.000        0.000        0.000        -0.317       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  in                  0.535        0.000        0.000        0.000        -1.856       -0.301       -0.000       \n",
            "   [1]  mind                0.505        0.000        0.000        0.000        -2.084       -0.327       -0.000       \n",
            "   [0]  of                  0.596        4.887        -4.066       -0.517       -2.339       -0.339       -2.001       \n",
            "   [0]  seeing              0.621        6.419        -7.063       -0.476       -2.046       -0.313       -1.732       \n",
            "   [0]  bastardizes         0.603        7.961        -13.648      -0.506       -1.762       -0.294       -1.468       \n",
            "   [0]  romance             0.524        5.566        -8.028       -0.646       -1.410       -0.270       -1.140       \n",
            "   [1]  always              0.554        0.000        0.000        0.000        -0.858       -0.263       -0.000       \n",
            "   [1]  serious             0.584        0.000        0.000        0.000        -0.964       -0.259       -0.000       \n",
            "   [1]  ha                  0.585        0.000        0.000        0.000        -1.082       -0.256       -0.000       \n",
            "   [1]  ha                  0.583        0.000        0.000        0.000        -1.214       -0.253       -0.000       \n",
            "   [0]  steven              0.618        9.170        -9.122       -0.481       -1.363       -0.251       -1.113       \n",
            "   [1]  ha                  0.615        0.000        0.000        0.000        -0.991       -0.250       -0.000       \n",
            "   [0]  to                  0.641        9.164        -4.094       -0.444       -1.113       -0.248       -0.865       \n",
            "   [0]  makes               0.628        9.163        -7.805       -0.465       -0.751       -0.240       -0.511       \n",
            "   [1]  ha                  0.621        0.000        0.000        0.000        -0.320       -0.235       -0.000       \n",
            "   [1]  ha                  0.616        0.000        0.000        0.000        -0.359       -0.229       -0.000       \n",
            "   [1]  ha                  0.613        0.000        0.000        0.000        -0.404       -0.226       -0.000       \n",
            "   [1]  ha                  0.610        0.000        0.000        0.000        -0.453       -0.228       -0.000       \n",
            "   [0]  s                   0.601        4.510        -4.809       -0.509       -0.509       -0.233       -0.276       \n",
            "   [1]  was                 0.598        0.000        0.000        0.000        0.000        -0.245       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  making              0.484        4.446        -8.293       -0.726       -2.719       -0.290       -2.429       \n",
            "   [0]  1997                0.454        4.743        -10.547      -0.790       -2.237       -0.318       -1.919       \n",
            "   [0]  minutes             0.461        6.628        -7.587       -0.774       -1.625       -0.331       -1.294       \n",
            "   [1]  to                  0.512        0.000        0.000        0.000        -0.956       -0.352       -0.000       \n",
            "   [1]  be                  0.554        0.000        0.000        0.000        -1.073       -0.388       -0.000       \n",
            "   [1]  said                0.614        0.000        0.000        0.000        -1.205       -0.414       -0.000       \n",
            "   [1]  about               0.623        0.000        0.000        0.000        -1.353       -0.435       -0.000       \n",
            "   [1]  high                0.584        0.000        0.000        0.000        -1.519       -0.416       -0.000       \n",
            "   [0]  city                0.561        8.386        -7.996       -0.578       -1.705       -0.359       -1.346       \n",
            "   [1]  confidential        0.552        0.000        0.000        0.000        -1.266       -0.320       -0.000       \n",
            "   [0]  of                  0.660        3.452        -3.897       -0.416       -1.421       -0.304       -1.117       \n",
            "   [1]  teen                0.573        0.000        0.000        0.000        -1.129       -0.372       -0.000       \n",
            "   [1]  exploitation        0.560        0.000        0.000        0.000        -1.267       -0.301       -0.000       \n",
            "   [0]  not                 0.565        4.683        -4.839       -0.571       -1.422       -0.288       -1.134       \n",
            "   [1]  from                0.595        0.000        0.000        0.000        -0.956       -0.298       -0.000       \n",
            "   [0]  makes               0.593        2.940        -7.710       -0.522       -1.074       -0.323       -0.751       \n",
            "   [1]  end                 0.543        0.000        0.000        0.000        -0.619       -0.325       -0.000       \n",
            "   [1]  of                  0.648        0.000        0.000        0.000        -0.695       -0.294       -0.000       \n",
            "   [0]  i                   0.649        2.941        -3.321       -0.432       -0.780       -0.364       -0.416       \n",
            "   [0]  the                 0.676        11.574       -2.938       -0.391       -0.391       -0.366       -0.025       \n",
            "Samples\n",
            "Sample 0 .  br has used her 25 film of starring had with two autistic show real or think she describes as a\n",
            "Sample 1 .  in mind of seeing bastardizes romance always serious ha ha steven ha to makes ha ha ha ha s was\n",
            "Sample 2 .  making 1997 minutes to be said about high city confidential of teen exploitation not from makes end of i the\n",
            "\n",
            "\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[0 0 1 0 1 1 1 0 0 1...]...][[10 69849 69849 677 69849 1753 4924 4924 69849 69849...]...][[17 5 677 6798 1753 4924 4924 15 1753 1761...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[0 0 1 0 1 1 1 0 0 1...]...][[10 69849 69849 677 69849 1753 4924 4924 69849 69849...]...][[10 40 677 64 1753 4924 4924 9 95 1761...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 1 1 0 1 1 0 1 1 0...]...][[9 509 127 738 69849 884 127 69849 34 25...]...][[509 127 738 255 884 127 738 34 25 23...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 1 1 0 1 1 0 1 1 0...]...][[9 509 127 738 69849 884 127 69849 34 25...]...][[509 127 738 23 884 127 645 34 25 232...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 1102 20 1844 9 26 183 518 5809...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 451 8 1844 15 26 183 518 5809...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 167 813 1844 1382 26 183 518 5809...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "global_step: 329\n",
            " perplexity: 906.757\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            " percent of 3-grams captured: 0.153.\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            " percent of 2-grams captured: 0.569.\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            " percent of 4-grams captured: 0.035.\n",
            " geometric_avg: 0.145.\n",
            " arithmetic_avg: 0.252.\n",
            "global_step: 329\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67495\n",
            " G train loss: 154.34732\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 451 8 1844 15 26 183 518 5809...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 34 2 1844 33 26 183 518 5809...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  maxwell             0.532        0.000        0.000        0.000        -2.481       -0.296       -0.000       \n",
            "   [1]  caulfield           0.494        0.000        0.000        0.000        -2.785       -0.330       -0.000       \n",
            "   [0]  who                 0.511        8.791        -5.925       -0.672       -3.127       -0.341       -2.786       \n",
            "   [0]  a                   0.545        4.106        -3.028       -0.608       -2.756       -0.349       -2.407       \n",
            "   [1]  england             0.361        0.000        0.000        0.000        -2.412       -0.349       -0.000       \n",
            "   [0]  by                  0.342        5.060        -5.571       -1.072       -2.708       -0.330       -2.378       \n",
            "   [1]  his                 0.347        0.000        0.000        0.000        -1.836       -0.355       -0.000       \n",
            "   [1]  young               0.373        0.000        0.000        0.000        -2.062       -0.392       -0.000       \n",
            "   [1]  daughter            0.309        0.000        0.000        0.000        -2.315       -0.423       -0.000       \n",
            "   [1]  melissa             0.309        0.000        0.000        0.000        -2.599       -0.436       -0.000       \n",
            "   [0]  the                 0.368        9.415        -2.840       -1.000       -2.917       -0.452       -2.466       \n",
            "   [1]  savage              0.385        0.000        0.000        0.000        -2.153       -0.468       -0.000       \n",
            "   [1]  a                   0.426        0.000        0.000        0.000        -2.417       -0.464       -0.000       \n",
            "   [1]  recent              0.305        0.000        0.000        0.000        -2.714       -0.450       -0.000       \n",
            "   [0]  it                  0.289        12.641       -3.526       -1.242       -3.047       -0.425       -2.622       \n",
            "   [0]  world               0.338        3.871        -7.269       -1.084       -2.026       -0.427       -1.599       \n",
            "   [1]  16                  0.343        0.000        0.000        0.000        -1.058       -0.442       -0.000       \n",
            "   [0]  truly               0.305        8.942        -6.811       -1.187       -1.187       -0.452       -0.735       \n",
            "   [1]  tom                 0.141        0.000        0.000        0.000        0.000        -0.457       0.000        \n",
            "   [1]  is                  0.145        0.000        0.000        0.000        0.000        -0.487       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  were                0.537        3.564        -6.449       -0.622       -3.101       -0.306       -2.794       \n",
            "   [1]  am                  0.512        0.000        0.000        0.000        -2.783       -0.355       -0.000       \n",
            "   [0]  david               0.548        3.304        -8.661       -0.602       -3.124       -0.374       -2.750       \n",
            "   [1]  fan                 0.549        0.000        0.000        0.000        -2.832       -0.387       -0.000       \n",
            "   [0]  at                  0.559        3.759        -5.573       -0.582       -3.179       -0.384       -2.795       \n",
            "   [0]  i                   0.556        2.850        -3.252       -0.586       -2.916       -0.382       -2.534       \n",
            "   [0]  film                0.586        13.910       -4.659       -0.535       -2.615       -0.379       -2.236       \n",
            "   [0]  she                 0.501        13.647       -6.827       -0.692       -2.335       -0.381       -1.954       \n",
            "   [1]  ben                 0.535        0.000        0.000        0.000        -1.846       -0.366       -0.000       \n",
            "   [1]  stiller             0.571        0.000        0.000        0.000        -2.072       -0.368       -0.000       \n",
            "   [0]  detectives          0.559        6.956        -14.167      -0.581       -2.326       -0.379       -1.947       \n",
            "   [0]  i                   0.555        12.550       -3.301       -0.589       -1.959       -0.376       -1.583       \n",
            "   [0]  i                   0.553        9.603        -3.302       -0.592       -1.538       -0.375       -1.163       \n",
            "   [1]  movies              0.543        0.000        0.000        0.000        -1.061       -0.376       -0.000       \n",
            "   [0]  of                  0.645        5.053        -3.911       -0.438       -1.192       -0.376       -0.816       \n",
            "   [0]  kids                0.636        2.856        -8.469       -0.453       -0.846       -0.392       -0.454       \n",
            "   [0]  about               0.643        12.637       -5.688       -0.441       -0.441       -0.386       -0.056       \n",
            "   [1]  was                 0.625        0.000        0.000        0.000        0.000        -0.385       0.000        \n",
            "   [1]  awful               0.629        0.000        0.000        0.000        0.000        -0.374       0.000        \n",
            "   [1]  br                  0.673        0.000        0.000        0.000        0.000        -0.371       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  burke               0.522        0.000        0.000        0.000        -1.904       -0.298       -0.000       \n",
            "   [0]  s                   0.516        3.122        -4.336       -0.662       -2.137       -0.337       -1.801       \n",
            "   [1]  an                  0.560        0.000        0.000        0.000        -1.656       -0.355       -0.000       \n",
            "   [0]  of                  0.660        13.715       -3.749       -0.415       -1.860       -0.358       -1.502       \n",
            "   [0]  slow                0.548        8.806        -8.399       -0.601       -1.622       -0.337       -1.284       \n",
            "   [1]  sent                0.626        0.000        0.000        0.000        -1.145       -0.302       -0.000       \n",
            "   [0]  was                 0.627        3.736        -4.527       -0.466       -1.286       -0.291       -0.995       \n",
            "   [1]  investigate         0.677        0.000        0.000        0.000        -0.920       -0.282       -0.000       \n",
            "   [1]  a                   0.698        0.000        0.000        0.000        -1.033       -0.280       -0.000       \n",
            "   [1]  prison              0.553        0.000        0.000        0.000        -1.160       -0.268       -0.000       \n",
            "   [0]  at                  0.581        7.333        -5.712       -0.543       -1.302       -0.256       -1.047       \n",
            "   [1]  inmates             0.568        0.000        0.000        0.000        -0.853       -0.269       -0.000       \n",
            "   [0]  i                   0.580        5.894        -3.417       -0.545       -0.958       -0.290       -0.667       \n",
            "   [1]  inexplicably        0.615        0.000        0.000        0.000        -0.464       -0.311       -0.000       \n",
            "   [1]  dying               0.670        0.000        0.000        0.000        -0.521       -0.324       -0.000       \n",
            "   [0]  with                0.737        7.508        -5.090       -0.305       -0.584       -0.323       -0.261       \n",
            "   [1]  t                   0.728        0.000        0.000        0.000        -0.314       -0.303       -0.000       \n",
            "   [0]  iffy                0.703        7.892        -11.331      -0.352       -0.352       -0.268       -0.085       \n",
            "   [1]  too                 0.653        0.000        0.000        0.000        0.000        -0.238       0.000        \n",
            "   [1]  shabby              0.607        0.000        0.000        0.000        0.000        -0.225       0.000        \n",
            "Samples\n",
            "Sample 0 .  maxwell caulfield who a england by his young daughter melissa the savage a recent it world 16 truly tom is\n",
            "Sample 1 .  were am david fan at i film she ben stiller detectives i i movies of kids about was awful br\n",
            "Sample 2 .  burke s an of slow sent was investigate a prison at inmates i inexplicably dying with t iffy too shabby\n",
            "\n",
            "\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 1 1 0 0 0 0 0 0 1...]...][[1840 1943 12 368 69849 69849 69849 69849 69849 69849...]...][[1943 12 368 755 654 78 2 368 1115 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 1 1 0 0 0 0 0 0 1...]...][[1840 1943 12 368 69849 69849 69849 69849 69849 69849...]...][[1943 12 368 43 17 970 2198 0 2 19...]...]\n",
            "targets[[7 12 2069 0 3572 1 33 0 984 5 37 3056 1306 5 143 18 24 12 3056 9][2907 12 84 81 19 3687 6405 13 2 19 11 13 5536 4 28 3047 120 17790 1 12295][353 0 298 8 60 646 713 1 99 70...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[69 7 12 2069 0 3572 1 33 0 984...]...][[0 1 0 1 1 0 0 1 0 1...]...][[69 69849 12 69849 0 3572 69849 69849 0 69849...]...][[7 12 2069 0 3572 1 33 0 984 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[69 7 12 2069 0 3572 1 33 0 984...]...][[0 1 0 1 1 0 0 1 0 1...]...][[69 69849 12 69849 0 3572 69849 69849 0 69849...]...][[112 12 0 0 3572 87 62 0 82 5...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[4 9 19 1 2277 3381 171 3 75 319...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[84 9 67 0 2277 2 171 3 75 319...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[12 9 16 2 2277 120 171 3 75 319...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "global_step: 334\n",
            " perplexity: 898.917\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            " percent of 3-grams captured: 0.148.\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            " percent of 2-grams captured: 0.582.\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.121.\n",
            " arithmetic_avg: 0.250.\n",
            "global_step: 334\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67411\n",
            " G train loss: 154.51425\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[84 9 67 0 2277 2 171 3 75 319...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[74 9 98 801 2277 1224 171 3 75 319...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  bad                 0.523        5.849        -6.683       -0.648       -2.662       -0.300       -2.362       \n",
            "   [1]  i                   0.501        0.000        0.000        0.000        -2.262       -0.346       -0.000       \n",
            "   [0]  movies              0.486        5.726        -6.117       -0.722       -2.539       -0.356       -2.183       \n",
            "   [0]  means               0.518        3.568        -8.856       -0.658       -2.041       -0.350       -1.691       \n",
            "   [1]  reaction            0.558        0.000        0.000        0.000        -1.552       -0.357       -0.000       \n",
            "   [0]  slasher             0.471        3.703        -8.171       -0.753       -1.742       -0.366       -1.376       \n",
            "   [1]  lot                 0.415        0.000        0.000        0.000        -1.110       -0.327       -0.000       \n",
            "   [1]  of                  0.545        0.000        0.000        0.000        -1.246       -0.313       -0.000       \n",
            "   [1]  people              0.566        0.000        0.000        0.000        -1.399       -0.358       -0.000       \n",
            "   [1]  left                0.580        0.000        0.000        0.000        -1.571       -0.359       -0.000       \n",
            "   [1]  with                0.666        0.000        0.000        0.000        -1.764       -0.349       -0.000       \n",
            "   [1]  after               0.639        0.000        0.000        0.000        -1.980       -0.362       -0.000       \n",
            "   [1]  seeing              0.662        0.000        0.000        0.000        -2.223       -0.317       -0.000       \n",
            "   [0]  together            0.606        4.473        -8.371       -0.501       -2.496       -0.307       -2.189       \n",
            "   [0]  good                0.525        4.807        -5.794       -0.645       -2.240       -0.270       -1.969       \n",
            "   [0]  in                  0.570        10.010       -4.234       -0.562       -1.790       -0.238       -1.552       \n",
            "   [0]  up                  0.585        4.118        -6.228       -0.536       -1.378       -0.270       -1.108       \n",
            "   [1]  fat                 0.563        0.000        0.000        0.000        -0.945       -0.287       -0.000       \n",
            "   [0]  movie               0.559        6.688        -4.521       -0.582       -1.061       -0.289       -0.772       \n",
            "   [0]  best                0.584        13.328       -6.349       -0.538       -0.538       -0.299       -0.239       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  movie               0.504        0.000        0.000        0.000        -7.035       -0.304       -0.000       \n",
            "   [0]  biased              0.280        4.454        -8.238       -1.273       -7.898       -0.346       -5.000       \n",
            "   [1]  so                  0.221        0.000        0.000        0.000        -7.438       -0.339       -0.000       \n",
            "   [0]  yet                 0.240        7.313        -7.834       -1.427       -8.350       -0.374       -5.000       \n",
            "   [1]  it                  0.225        0.000        0.000        0.000        -7.772       -0.436       -0.000       \n",
            "   [0]  kyu                 0.211        7.449        -12.920      -1.554       -8.726       -0.469       -5.000       \n",
            "   [1]  me                  0.194        0.000        0.000        0.000        -8.052       -0.483       -0.000       \n",
            "   [1]  wish                0.216        0.000        0.000        0.000        -9.039       -0.489       -0.000       \n",
            "   [0]  particular          0.159        4.443        -8.771       -1.842       -10.148      -0.501       -5.000       \n",
            "   [1]  had                 0.143        0.000        0.000        0.000        -9.326       -0.485       -0.000       \n",
            "   [0]  credits             0.107        6.368        -8.040       -2.236       -10.470      -0.489       -5.000       \n",
            "   [0]  mvp                 0.077        13.267       -11.236      -2.568       -9.244       -0.488       -5.000       \n",
            "   [0]  computer            0.066        9.124        -9.580       -2.717       -7.494       -0.503       -5.000       \n",
            "   [0]  of                  0.124        4.469        -4.380       -2.086       -5.363       -0.530       -4.834       \n",
            "   [1]  deaf                0.083        0.000        0.000        0.000        -3.680       -0.563       -0.000       \n",
            "   [1]  as                  0.086        0.000        0.000        0.000        -4.131       -0.538       -0.000       \n",
            "   [0]  of                  0.160        3.924        -4.375       -1.835       -4.638       -0.531       -4.107       \n",
            "   [0]  into                0.194        7.766        -6.728       -1.641       -3.148       -0.537       -2.610       \n",
            "   [0]  blossomed           0.184        4.473        -12.846      -1.692       -1.692       -0.509       -1.183       \n",
            "   [1]  watched             0.177        0.000        0.000        0.000        0.000        -0.468       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  in                  0.539        7.249        -4.899       -0.617       -4.732       -0.296       -4.436       \n",
            "   [0]  hopefully           0.415        11.304       -8.591       -0.879       -4.619       -0.334       -4.285       \n",
            "   [0]  movie               0.417        7.072        -3.236       -0.874       -4.199       -0.341       -3.858       \n",
            "   [0]  is                  0.444        3.081        -3.382       -0.813       -3.732       -0.355       -3.377       \n",
            "   [1]  screenplay          0.361        0.000        0.000        0.000        -3.278       -0.370       -0.000       \n",
            "   [0]  to                  0.421        3.694        -3.995       -0.866       -3.680       -0.379       -3.301       \n",
            "   [0]  dallas              0.464        3.351        -11.473      -0.767       -3.160       -0.388       -2.772       \n",
            "   [1]  horror              0.511        0.000        0.000        0.000        -2.686       -0.392       -0.000       \n",
            "   [0]  ways                0.430        7.453        -8.527       -0.845       -3.016       -0.390       -2.626       \n",
            "   [0]  i                   0.429        3.862        -3.793       -0.847       -2.437       -0.371       -2.066       \n",
            "   [0]  is                  0.456        9.594        -4.174       -0.785       -1.786       -0.366       -1.419       \n",
            "   [1]  at                  0.496        0.000        0.000        0.000        -1.123       -0.370       -0.000       \n",
            "   [1]  tiffany             0.480        0.000        0.000        0.000        -1.261       -0.375       -0.000       \n",
            "   [1]  s                   0.484        0.000        0.000        0.000        -1.416       -0.373       -0.000       \n",
            "   [1]  you                 0.508        0.000        0.000        0.000        -1.589       -0.372       -0.000       \n",
            "   [0]  style               0.384        6.875        -8.000       -0.957       -1.784       -0.372       -1.412       \n",
            "   [0]  s                   0.395        6.680        -4.413       -0.929       -0.929       -0.365       -0.564       \n",
            "   [1]  is                  0.429        0.000        0.000        0.000        0.000        -0.375       0.000        \n",
            "   [1]  going               0.381        0.000        0.000        0.000        0.000        -0.389       0.000        \n",
            "   [1]  to                  0.445        0.000        0.000        0.000        0.000        -0.392       0.000        \n",
            "Samples\n",
            "Sample 0 .  bad i movies means reaction slasher lot of people left with after seeing together good in up fat movie best\n",
            "Sample 1 .  movie biased so yet it kyu me wish particular had credits mvp computer of deaf as of into blossomed watched\n",
            "Sample 2 .  in hopefully movie is screenplay to dallas horror ways i is at tiffany s you style s is going to\n",
            "\n",
            "\n",
            "targets[[50418 12 330 8991 8 0 328 45 4 28 29 3 4895 12 117 39 5 983 3 600][17 13 1136 9 59 395 7 4 255 73 126 72 47 9 67 478 7231 7 13 401][9546 288 21 4095 1669 12 872 17 18 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[601 50418 12 330 8991 8 0 328 45 4...]...][[0 1 1 0 1 0 0 0 1 1...]...][[601 69849 12 330 69849 8 69849 69849 69849 4...]...][[50418 12 330 8991 8 0 328 45 4 28...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[601 50418 12 330 8991 8 0 328 45 4...]...][[0 1 1 0 1 0 0 0 1 1...]...][[601 69849 12 330 69849 8 69849 69849 69849 4...]...][[582 12 330 54583 8 1 55377 165 4 28...]...]\n",
            "targets[[17 13 42 39307 33 3566 1 15 49 293 84 1 7096 85 7 5 2 814 17 1156][26862 6675 2698 188 4 28 2 737 3840 34679 5638 7071 469 75 25 4437 38 4573 187 40][42 215 10 19 16 0 13340 57 9 367...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 42 39307 33 3566 1 15 49...]...][[0 0 1 1 1 0 1 0 1 1...]...][[10 69849 69849 42 39307 33 69849 1 69849 49...]...][[17 13 42 39307 33 3566 1 15 49 293...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 42 39307 33 3566 1 15 49...]...][[0 0 1 1 1 0 1 0 1 1...]...][[10 69849 69849 42 39307 33 69849 1 69849 49...]...][[504 2529 42 39307 33 9995 1 38126 49 293...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 1001 13 10 307 5 92 0 5290 739...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 1349 115 19 10 5 92 8 5290 22...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 33 8 24391 19 5 92 4 5290 344...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "global_step: 339\n",
            " perplexity: 894.665\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            " percent of 3-grams captured: 0.147.\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            " percent of 2-grams captured: 0.553.\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            " percent of 4-grams captured: 0.018.\n",
            " geometric_avg: 0.114.\n",
            " arithmetic_avg: 0.240.\n",
            "global_step: 339\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67260\n",
            " G train loss: 156.59065\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 1349 115 19 10 5 92 8 5290 22...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 3896 19 43 140 5 92 3415 5290 12...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  a                   0.501        0.000        0.000        0.000        -3.350       -0.298       -0.000       \n",
            "   [0]  tonight             0.436        8.645        -8.824       -0.830       -3.761       -0.340       -3.422       \n",
            "   [0]  film                0.472        6.951        -4.326       -0.751       -3.291       -0.342       -2.949       \n",
            "   [0]  about               0.516        4.376        -5.940       -0.661       -2.852       -0.362       -2.490       \n",
            "   [0]  m                   0.419        3.779        -5.854       -0.869       -2.459       -0.378       -2.081       \n",
            "   [1]  is                  0.436        0.000        0.000        0.000        -1.785       -0.352       -0.000       \n",
            "   [1]  made                0.512        0.000        0.000        0.000        -2.004       -0.358       -0.000       \n",
            "   [0]  beating             0.495        4.393        -13.860      -0.702       -2.250       -0.376       -1.874       \n",
            "   [1]  1973                0.549        0.000        0.000        0.000        -1.738       -0.362       -0.000       \n",
            "   [0]  s                   0.550        5.390        -4.780       -0.598       -1.951       -0.364       -1.587       \n",
            "   [1]  what                0.638        0.000        0.000        0.000        -1.519       -0.351       -0.000       \n",
            "   [0]  because             0.559        4.597        -6.287       -0.582       -1.705       -0.360       -1.345       \n",
            "   [1]  presumably          0.524        0.000        0.000        0.000        -1.261       -0.319       -0.000       \n",
            "   [0]  any                 0.475        3.364        -7.016       -0.744       -1.416       -0.299       -1.117       \n",
            "   [0]  after               0.470        5.954        -6.752       -0.754       -0.754       -0.294       -0.461       \n",
            "   [1]  low                 0.496        0.000        0.000        0.000        0.000        -0.309       0.000        \n",
            "   [1]  budget              0.528        0.000        0.000        0.000        0.000        -0.336       0.000        \n",
            "   [1]  this                0.558        0.000        0.000        0.000        0.000        -0.351       0.000        \n",
            "   [1]  two                 0.498        0.000        0.000        0.000        0.000        -0.353       0.000        \n",
            "   [1]  hander              0.516        0.000        0.000        0.000        0.000        -0.333       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  you                 0.537        0.000        0.000        0.000        -6.521       -0.297       -0.000       \n",
            "   [0]  friday              0.406        7.969        -7.874       -0.901       -7.321       -0.344       -5.000       \n",
            "   [0]  at                  0.438        12.710       -5.331       -0.825       -7.208       -0.306       -5.000       \n",
            "   [1]  excess              0.454        0.000        0.000        0.000        -7.165       -0.345       -0.000       \n",
            "   [0]  force               0.219        3.623        -7.919       -1.518       -8.045       -0.369       -5.000       \n",
            "   [1]  movie               0.220        0.000        0.000        0.000        -7.327       -0.264       -0.000       \n",
            "   [0]  competition         0.219        8.800        -11.792      -1.516       -8.226       -0.300       -5.000       \n",
            "   [0]  into                0.259        4.952        -6.648       -1.350       -7.533       -0.350       -5.000       \n",
            "   [0]  for                 0.339        6.323        -5.033       -1.080       -6.942       -0.403       -5.000       \n",
            "   [1]  mean                0.178        0.000        0.000        0.000        -6.580       -0.449       -0.000       \n",
            "   [0]  stars               0.249        6.264        -7.852       -1.390       -7.388       -0.353       -5.000       \n",
            "   [0]  non                 0.175        4.066        -8.211       -1.742       -6.733       -0.388       -5.000       \n",
            "   [0]  on                  0.240        5.462        -5.209       -1.428       -5.604       -0.352       -5.000       \n",
            "   [0]  that                0.271        5.536        -4.461       -1.304       -4.689       -0.402       -4.287       \n",
            "   [1]  attempt             0.261        0.000        0.000        0.000        -3.800       -0.411       -0.000       \n",
            "   [0]  name                0.177        4.450        -7.710       -1.734       -4.266       -0.392       -3.874       \n",
            "   [1]  made                0.246        0.000        0.000        0.000        -2.843       -0.340       -0.000       \n",
            "   [0]  if                  0.255        4.081        -6.025       -1.368       -3.192       -0.390       -2.802       \n",
            "   [0]  matthau             0.310        12.190       -10.884      -1.173       -2.047       -0.399       -1.649       \n",
            "   [0]  somewhat            0.374        3.431        -9.127       -0.982       -0.982       -0.416       -0.566       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  of                  0.601        9.138        -3.848       -0.509       -2.298       -0.312       -1.985       \n",
            "   [0]  of                  0.685        11.709       -3.647       -0.379       -2.008       -0.380       -1.628       \n",
            "   [1]  obviously           0.681        0.000        0.000        0.000        -1.829       -0.412       -0.000       \n",
            "   [0]  the                 0.685        9.062        -2.900       -0.379       -2.054       -0.394       -1.660       \n",
            "   [0]  after               0.650        5.316        -6.948       -0.431       -1.881       -0.367       -1.514       \n",
            "   [1]  the                 0.669        0.000        0.000        0.000        -1.627       -0.326       -0.000       \n",
            "   [0]  interesting         0.647        3.424        -7.044       -0.435       -1.827       -0.314       -1.513       \n",
            "   [1]  it                  0.612        0.000        0.000        0.000        -1.562       -0.295       -0.000       \n",
            "   [0]  own                 0.631        4.326        -7.509       -0.460       -1.754       -0.276       -1.478       \n",
            "   [0]  i                   0.605        3.308        -3.456       -0.502       -1.452       -0.286       -1.166       \n",
            "   [1]  joke                0.662        0.000        0.000        0.000        -1.067       -0.290       -0.000       \n",
            "   [0]  setting             0.576        6.322        -8.812       -0.551       -1.198       -0.318       -0.880       \n",
            "   [1]  bad                 0.614        0.000        0.000        0.000        -0.726       -0.295       -0.000       \n",
            "   [0]  way                 0.605        3.510        -7.884       -0.502       -0.815       -0.309       -0.506       \n",
            "   [1]  is                  0.604        0.000        0.000        0.000        -0.352       -0.313       -0.000       \n",
            "   [1]  and                 0.610        0.000        0.000        0.000        -0.395       -0.311       -0.000       \n",
            "   [0]  bad                 0.642        6.854        -5.898       -0.443       -0.443       -0.310       -0.133       \n",
            "   [1]  one                 0.628        0.000        0.000        0.000        0.000        -0.319       0.000        \n",
            "   [1]  would               0.632        0.000        0.000        0.000        0.000        -0.309       0.000        \n",
            "   [1]  review              0.591        0.000        0.000        0.000        0.000        -0.306       0.000        \n",
            "Samples\n",
            "Sample 0 .  a tonight film about m is made beating 1973 s what because presumably any after low budget this two hander\n",
            "Sample 1 .  you friday at excess force movie competition into for mean stars non on that attempt name made if matthau somewhat\n",
            "Sample 2 .  of of obviously the after the interesting it own i joke setting bad way is and bad one would review\n",
            "\n",
            "\n",
            "targets[[10 5015 231 570 3 0 599 2707 8323 33 0 2497 269 35 2816 1152 3 2281 405 1909][4994 7264 578 8 0 200 6183 15 1133 2730 2769 451 1 436 4 4248 10 122 968 1262][27 307 10 19 440 154 143 1 51 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 10 5015 231 570 3 0 599 2707 8323...]...][[0 1 1 1 0 0 0 1 1 0...]...][[15 69849 5015 231 570 69849 69849 69849 2707 8323...]...][[10 5015 231 570 3 0 599 2707 8323 33...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 10 5015 231 570 3 0 599 2707 8323...]...][[0 1 1 1 0 0 0 1 1 0...]...][[15 69849 5015 231 570 69849 69849 69849 2707 8323...]...][[29 5015 231 570 0 295 100 2707 8323 708...]...]\n",
            "targets[[5 0 244 3 3140 3788 3495 439 50 80 208 9 169 7 3214 11 286 15 142 115][1018 2 222 3 10 2889 22 1008 729 1 693 9 67 4 27 7 0 921 198 295][7 12 2 222 3 2 658 19 37 46...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 0 244 3 3140 3788 3495 439 50...]...][[1 1 0 0 0 1 0 0 1 0...]...][[10 5 0 69849 69849 69849 3788 69849 69849 50...]...][[5 0 244 3 3140 3788 3495 439 50 80...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 0 244 3 3140 3788 3495 439 50...]...][[1 1 0 0 0 1 0 0 1 0...]...][[10 5 0 69849 69849 69849 3788 69849 69849 50...]...][[5 0 799 5 853 3788 20 123 50 243...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[0 4 2521 38 17 2 0 797 26211 575...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[4140 4 2666 6756 439 78 0 797 33 575...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[23 4 694 2117 7 9995 0 797 2 575...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "global_step: 344\n",
            " perplexity: 892.171\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            " percent of 3-grams captured: 0.169.\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            " percent of 2-grams captured: 0.590.\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            " percent of 4-grams captured: 0.029.\n",
            " geometric_avg: 0.142.\n",
            " arithmetic_avg: 0.263.\n",
            "global_step: 344\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67138\n",
            " G train loss: 158.14488\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[4140 4 2666 6756 439 78 0 797 33 575...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[139 4 270 0 9 81 0 797 14 575...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  ve                  0.483        13.815       -6.214       -0.728       -3.233       -0.303       -2.930       \n",
            "   [1]  to                  0.538        0.000        0.000        0.000        -2.812       -0.344       -0.000       \n",
            "   [0]  set                 0.521        13.490       -7.111       -0.652       -3.157       -0.363       -2.794       \n",
            "   [0]  the                 0.559        10.190       -2.529       -0.582       -2.812       -0.361       -2.452       \n",
            "   [0]  i                   0.517        9.304        -3.238       -0.660       -2.504       -0.357       -2.147       \n",
            "   [0]  great               0.512        6.688        -5.853       -0.669       -2.070       -0.352       -1.718       \n",
            "   [1]  the                 0.554        0.000        0.000        0.000        -1.573       -0.353       -0.000       \n",
            "   [1]  theater             0.517        0.000        0.000        0.000        -1.766       -0.355       -0.000       \n",
            "   [0]  as                  0.538        5.689        -4.920       -0.619       -1.982       -0.348       -1.635       \n",
            "   [1]  including           0.528        0.000        0.000        0.000        -1.530       -0.348       -0.000       \n",
            "   [0]  the                 0.566        8.199        -2.449       -0.569       -1.718       -0.348       -1.369       \n",
            "   [1]  actors              0.521        0.000        0.000        0.000        -1.289       -0.347       -0.000       \n",
            "   [1]  from                0.541        0.000        0.000        0.000        -1.447       -0.336       -0.000       \n",
            "   [0]  the                 0.581        3.805        -2.474       -0.543       -1.625       -0.339       -1.286       \n",
            "   [0]  say                 0.625        4.716        -6.551       -0.470       -1.214       -0.340       -0.875       \n",
            "   [0]  are                 0.628        3.976        -5.483       -0.466       -0.836       -0.337       -0.499       \n",
            "   [1]  a                   0.629        0.000        0.000        0.000        -0.415       -0.319       -0.000       \n",
            "   [1]  war                 0.623        0.000        0.000        0.000        -0.466       -0.302       -0.000       \n",
            "   [1]  story               0.560        0.000        0.000        0.000        -0.524       -0.291       -0.000       \n",
            "   [0]  was                 0.556        5.058        -4.194       -0.588       -0.588       -0.282       -0.306       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  is                  0.572        4.046        -2.858       -0.558       -3.279       -0.304       -2.975       \n",
            "   [1]  begin               0.546        0.000        0.000        0.000        -3.055       -0.338       -0.000       \n",
            "   [1]  the                 0.570        0.000        0.000        0.000        -3.429       -0.356       -0.000       \n",
            "   [1]  special             0.438        0.000        0.000        0.000        -3.850       -0.354       -0.000       \n",
            "   [1]  effects             0.410        0.000        0.000        0.000        -4.323       -0.375       -0.000       \n",
            "   [0]  and                 0.440        7.397        -2.980       -0.821       -4.853       -0.405       -4.448       \n",
            "   [1]  be                  0.490        0.000        0.000        0.000        -4.526       -0.421       -0.000       \n",
            "   [0]  already             0.437        8.686        -8.867       -0.827       -5.081       -0.417       -4.665       \n",
            "   [1]  special             0.318        0.000        0.000        0.000        -4.776       -0.415       -0.000       \n",
            "   [0]  it                  0.291        13.446       -2.812       -1.234       -5.362       -0.445       -4.917       \n",
            "   [0]  that                0.320        6.044        -3.979       -1.138       -4.635       -0.483       -4.152       \n",
            "   [0]  movies              0.322        2.369        -6.117       -1.133       -3.926       -0.502       -3.425       \n",
            "   [1]  director            0.319        0.000        0.000        0.000        -3.136       -0.514       -0.000       \n",
            "   [1]  shouted             0.331        0.000        0.000        0.000        -3.521       -0.519       -0.000       \n",
            "   [1]  action              0.442        0.000        0.000        0.000        -3.953       -0.513       -0.000       \n",
            "   [0]  local               0.289        3.224        -8.268       -1.242       -4.438       -0.474       -3.964       \n",
            "   [0]  very                0.266        9.541        -5.690       -1.325       -3.588       -0.476       -3.113       \n",
            "   [0]  elizabeth           0.247        6.150        -9.390       -1.397       -2.541       -0.489       -2.053       \n",
            "   [0]  and                 0.277        8.747        -2.813       -1.284       -1.284       -0.520       -0.764       \n",
            "   [1]  indicated           0.233        0.000        0.000        0.000        0.000        -0.533       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  when                0.511        5.951        -6.239       -0.671       -7.569       -0.303       -5.000       \n",
            "   [1]  many                0.490        0.000        0.000        0.000        -7.745       -0.351       -0.000       \n",
            "   [1]  words               0.412        0.000        0.000        0.000        -8.695       -0.359       -0.000       \n",
            "   [1]  describe            0.384        0.000        0.000        0.000        -9.762       -0.351       -0.000       \n",
            "   [1]  this                0.417        0.000        0.000        0.000        -10.959      -0.366       -0.000       \n",
            "   [0]  around              0.394        4.621        -8.258       -0.932       -12.304      -0.387       -5.000       \n",
            "   [0]  anymore             0.227        5.156        -8.345       -1.481       -12.767      -0.386       -5.000       \n",
            "   [0]  cool                0.144        6.220        -8.533       -1.939       -12.670      -0.376       -5.000       \n",
            "   [0]  i                   0.135        4.820        -3.423       -2.002       -12.048      -0.442       -5.000       \n",
            "   [0]  a                   0.154        6.838        -2.880       -1.872       -11.278      -0.549       -5.000       \n",
            "   [0]  that                0.171        13.392       -4.366       -1.765       -10.561      -0.632       -5.000       \n",
            "   [0]  actually            0.139        3.174        -7.005       -1.976       -9.875       -0.664       -5.000       \n",
            "   [0]  t                   0.144        6.442        -4.866       -1.935       -8.868       -0.668       -5.000       \n",
            "   [0]  the                 0.171        4.568        -2.559       -1.766       -7.784       -0.681       -5.000       \n",
            "   [0]  has                 0.160        4.568        -5.371       -1.832       -6.756       -0.680       -5.000       \n",
            "   [1]  crap                0.186        0.000        0.000        0.000        -5.528       -0.652       -0.000       \n",
            "   [0]  i                   0.170        4.568        -3.426       -1.774       -6.207       -0.636       -5.000       \n",
            "   [0]  into                0.195        4.569        -6.392       -1.634       -4.976       -0.606       -4.369       \n",
            "   [0]  employed            0.183        6.018        -10.967      -1.701       -3.752       -0.593       -3.159       \n",
            "   [0]  science             0.100        6.959        -7.352       -2.303       -2.303       -0.569       -1.734       \n",
            "Samples\n",
            "Sample 0 .  ve to set the i great the theater as including the actors from the say are a war story was\n",
            "Sample 1 .  is begin the special effects and be already special it that movies director shouted action local very elizabeth and indicated\n",
            "Sample 2 .  when many words describe this around anymore cool i a that actually t the has crap i into employed science\n",
            "\n",
            "\n",
            "targets[[1022 37 10 5 47 30 0 50896 13 43 3543 69 9 140 251 11 0 673 16 91][0 2195 3902 2130 22 2 1717 2841 11 5 42 43 4 1382 3504 55 112 50 28 448][439 39 327 25 53 173 741 4 367 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[48 1022 37 10 5 47 30 0 50896 13...]...][[1 1 1 1 1 0 1 0 1 0...]...][[48 1022 37 10 5 47 69849 0 69849 13...]...][[1022 37 10 5 47 30 0 50896 13 43...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[48 1022 37 10 5 47 30 0 50896 13...]...][[1 1 1 1 1 0 1 0 1 0...]...][[48 1022 37 10 5 47 69849 0 69849 13...]...][[1022 37 10 5 47 360 0 1216 13 863...]...]\n",
            "targets[[196 0 17 257 0 109 784 2 171 3 166 0 756 3 0 17 1386 31548 1 11405][17 4 28 509 33 30 0 231 42 106 44 16 48 9619 1 2 115 537 39 68][0 834 3 3341 338 5 52 3 2 3225...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 196 0 17 257 0 109 784 2 171...]...][[1 1 0 0 0 1 1 0 0 0...]...][[9 196 0 69849 69849 69849 109 784 69849 69849...]...][[196 0 17 257 0 109 784 2 171 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 196 0 17 257 0 109 784 2 171...]...][[1 1 0 0 0 1 1 0 0 0...]...][[9 196 0 69849 69849 69849 109 784 69849 69849...]...][[196 0 442 2 2 109 784 8 2 91...]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[5 1326 5 0 2 362 16 804 1326 605...]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[9 1281 10 17 2 223 16 804 2 605...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[17 0 13 4857 2 1 16 804 1644 605...]...]\n",
            "I0310 00:34:12.858808 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "I0310 00:34:12.885633 139852867667712 supervisor.py:1099] global_step/sec: 0.382916\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "global_step: 349\n",
            " perplexity: 891.778\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            " percent of 3-grams captured: 0.170.\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            " percent of 2-grams captured: 0.602.\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            " percent of 4-grams captured: 0.031.\n",
            " geometric_avg: 0.147.\n",
            " arithmetic_avg: 0.268.\n",
            "global_step: 349\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67020\n",
            " G train loss: 159.19322\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[9 1281 10 17 2 223 16 804 2 605...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[10 43 13 67 2 146 16 804 1197 605...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  this                0.540        4.357        -3.582       -0.616       -3.918       -0.302       -3.615       \n",
            "   [0]  about               0.594        7.659        -6.163       -0.521       -3.707       -0.351       -3.356       \n",
            "   [0]  was                 0.579        3.715        -3.967       -0.546       -3.577       -0.388       -3.189       \n",
            "   [0]  had                 0.533        3.825        -5.574       -0.629       -3.403       -0.379       -3.024       \n",
            "   [1]  a                   0.545        0.000        0.000        0.000        -3.114       -0.358       -0.000       \n",
            "   [0]  those               0.476        7.817        -6.835       -0.742       -3.496       -0.357       -3.139       \n",
            "   [1]  for                 0.581        0.000        0.000        0.000        -3.092       -0.341       -0.000       \n",
            "   [1]  showing             0.447        0.000        0.000        0.000        -3.471       -0.373       -0.000       \n",
            "   [0]  showed              0.385        3.811        -8.364       -0.954       -3.897       -0.345       -3.552       \n",
            "   [1]  complete            0.354        0.000        0.000        0.000        -3.304       -0.337       -0.000       \n",
            "   [0]  road                0.282        8.680        -8.858       -1.264       -3.709       -0.344       -3.365       \n",
            "   [1]  of                  0.423        0.000        0.000        0.000        -2.745       -0.352       -0.000       \n",
            "   [1]  effort              0.297        0.000        0.000        0.000        -3.081       -0.404       -0.000       \n",
            "   [1]  in                  0.325        0.000        0.000        0.000        -3.459       -0.388       -0.000       \n",
            "   [1]  trying              0.249        0.000        0.000        0.000        -3.884       -0.403       -0.000       \n",
            "   [0]  meets               0.262        4.189        -9.406       -1.339       -4.360       -0.393       -3.967       \n",
            "   [0]  the                 0.306        11.452       -3.432       -1.185       -3.392       -0.411       -2.981       \n",
            "   [0]  years               0.276        3.860        -6.557       -1.287       -2.477       -0.434       -2.044       \n",
            "   [1]  quality             0.190        0.000        0.000        0.000        -1.337       -0.428       -0.000       \n",
            "   [0]  the                 0.223        7.442        -3.435       -1.501       -1.501       -0.411       -1.090       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  had                 0.477        7.889        -5.645       -0.739       -6.270       -0.303       -5.000       \n",
            "   [0]  it                  0.430        5.897        -3.940       -0.844       -6.209       -0.343       -5.000       \n",
            "   [0]  extreme             0.265        2.333        -8.103       -1.330       -6.024       -0.372       -5.000       \n",
            "   [1]  movie               0.263        0.000        0.000        0.000        -5.269       -0.424       -0.000       \n",
            "   [0]  the                 0.312        7.160        -2.183       -1.166       -5.916       -0.498       -5.000       \n",
            "   [1]  t                   0.317        0.000        0.000        0.000        -5.333       -0.549       -0.000       \n",
            "   [1]  cause               0.142        0.000        0.000        0.000        -5.987       -0.567       -0.000       \n",
            "   [0]  very                0.123        6.486        -5.775       -2.093       -6.722       -0.612       -5.000       \n",
            "   [0]  in                  0.149        8.698        -3.790       -1.906       -5.197       -0.683       -4.514       \n",
            "   [1]  but                 0.199        0.000        0.000        0.000        -3.694       -0.741       -0.000       \n",
            "   [0]  ripoff              0.220        3.216        -13.563      -1.513       -4.148       -0.756       -3.391       \n",
            "   [1]  did                 0.192        0.000        0.000        0.000        -2.958       -0.737       -0.000       \n",
            "   [1]  cause               0.077        0.000        0.000        0.000        -3.320       -0.707       -0.000       \n",
            "   [0]  after               0.073        2.265        -6.710       -2.616       -3.728       -0.737       -2.991       \n",
            "   [1]  director            0.079        0.000        0.000        0.000        -1.248       -0.799       -0.000       \n",
            "   [1]  a                   0.087        0.000        0.000        0.000        -1.401       -0.859       -0.000       \n",
            "   [1]  lot                 0.058        0.000        0.000        0.000        -1.573       -0.885       -0.000       \n",
            "   [1]  of                  0.102        0.000        0.000        0.000        -1.766       -0.917       -0.000       \n",
            "   [0]  to                  0.138        7.945        -3.690       -1.982       -1.982       -0.903       -1.080       \n",
            "   [1]  to                  0.175        0.000        0.000        0.000        0.000        -0.850       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  combination         0.505        0.000        0.000        0.000        -1.771       -0.304       -0.000       \n",
            "   [1]  of                  0.619        0.000        0.000        0.000        -1.988       -0.342       -0.000       \n",
            "   [1]  reading             0.540        0.000        0.000        0.000        -2.232       -0.334       -0.000       \n",
            "   [0]  the                 0.564        2.329        -2.329       -0.573       -2.506       -0.315       -2.192       \n",
            "   [0]  he                  0.572        15.578       -6.325       -0.558       -2.171       -0.297       -1.873       \n",
            "   [0]  was                 0.558        3.145        -3.680       -0.583       -1.810       -0.280       -1.530       \n",
            "   [1]  viewing             0.509        0.000        0.000        0.000        -1.378       -0.262       -0.000       \n",
            "   [1]  this                0.541        0.000        0.000        0.000        -1.547       -0.260       -0.000       \n",
            "   [1]  film                0.562        0.000        0.000        0.000        -1.736       -0.263       -0.000       \n",
            "   [1]  has                 0.545        0.000        0.000        0.000        -1.950       -0.261       -0.000       \n",
            "   [1]  inspired            0.588        0.000        0.000        0.000        -2.189       -0.257       -0.000       \n",
            "   [0]  i                   0.546        5.586        -3.407       -0.605       -2.457       -0.251       -2.207       \n",
            "   [0]  dvd                 0.512        8.001        -7.786       -0.670       -2.080       -0.242       -1.837       \n",
            "   [1]  and                 0.535        0.000        0.000        0.000        -1.582       -0.241       -0.000       \n",
            "   [1]  i                   0.498        0.000        0.000        0.000        -1.777       -0.246       -0.000       \n",
            "   [0]  use                 0.396        3.649        -8.590       -0.927       -1.995       -0.253       -1.742       \n",
            "   [1]  new                 0.364        0.000        0.000        0.000        -1.198       -0.278       -0.000       \n",
            "   [0]  toronto             0.261        13.003       -8.721       -1.345       -1.345       -0.324       -1.021       \n",
            "   [1]  recently            0.223        0.000        0.000        0.000        0.000        -0.397       0.000        \n",
            "   [1]  i                   0.213        0.000        0.000        0.000        0.000        -0.480       0.000        \n",
            "Samples\n",
            "Sample 0 .  this about was had a those for showing showed complete road of effort in trying meets the years quality the\n",
            "Sample 1 .  had it extreme movie the t cause very in but ripoff did cause after director a lot of to to\n",
            "Sample 2 .  combination of reading the he was viewing this film has inspired i dvd and i use new toronto recently i\n",
            "\n",
            "\n",
            "targets[[5 23 127 737 184 600 19 8 189 2 171 3 0 184 5 319 4 127 1629 11][552 7231 10 17 256 509 0 298 13 53 671 14 0 17 352 319 44 88 3 0][20 159 21 367 10 17 352 127 330 41...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 23 127 737 184 600 19 8 189...]...][[1 0 0 0 1 0 1 1 0 1...]...][[10 5 69849 69849 69849 184 69849 19 8 69849...]...][[5 23 127 737 184 600 19 8 189 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 23 127 737 184 600 19 8 189...]...][[1 0 0 0 1 0 1 1 0 1...]...][[10 5 69849 69849 69849 184 69849 19 8 69849...]...][[5 109 3 38 184 4 19 8 688 2...]...]\n",
            "targets[[5 29 3 60 519 231 98 467 7 51 9 13 115 1 7 133 1765 55 15 71][55687 9149 7617 554 360 341 19 11 45 14399 3 10822 245 4 865 144 9 76 0 754][67 110 10 19 95 143 8 0 1001 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 29 3 60 519 231 98 467 7...]...][[1 1 1 0 1 0 1 0 0 0...]...][[10 5 29 3 69849 519 69849 98 69849 69849...]...][[5 29 3 60 519 231 98 467 7 51...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 29 3 60 519 231 98 467 7...]...][[1 1 1 0 1 0 1 0 0 0...]...][[10 5 29 3 69849 519 69849 98 69849 69849...]...][[5 29 3 59 519 17 98 8 3 7...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 2343 30 723 3 14223 98 8 10 202...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 110 30 668 3 0 98 8 10 202...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 58 30 1197 3 108 98 8 10 202...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "global_step: 354\n",
            " perplexity: 892.873\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            " percent of 3-grams captured: 0.148.\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            " percent of 2-grams captured: 0.603.\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            " percent of 4-grams captured: 0.028.\n",
            " geometric_avg: 0.136.\n",
            " arithmetic_avg: 0.260.\n",
            "global_step: 354\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66893\n",
            " G train loss: 159.21686\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 110 30 668 3 0 98 8 10 202...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 2867 30 834 3 1568 98 8 10 202...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  ve                  0.539        0.000        0.000        0.000        -2.881       -0.295       -0.000       \n",
            "   [0]  disbelief           0.572        6.856        -11.000      -0.559       -3.234       -0.331       -2.903       \n",
            "   [1]  all                 0.617        0.000        0.000        0.000        -3.004       -0.356       -0.000       \n",
            "   [0]  premise             0.506        7.515        -7.926       -0.682       -3.372       -0.363       -3.009       \n",
            "   [1]  of                  0.611        0.000        0.000        0.000        -3.020       -0.319       -0.000       \n",
            "   [0]  collection          0.420        3.212        -8.660       -0.868       -3.391       -0.338       -3.053       \n",
            "   [1]  movies              0.439        0.000        0.000        0.000        -2.833       -0.285       -0.000       \n",
            "   [1]  in                  0.473        0.000        0.000        0.000        -3.181       -0.327       -0.000       \n",
            "   [1]  this                0.503        0.000        0.000        0.000        -3.571       -0.363       -0.000       \n",
            "   [1]  series              0.431        0.000        0.000        0.000        -4.009       -0.371       -0.000       \n",
            "   [0]  it                  0.381        8.543        -4.005       -0.964       -4.501       -0.348       -4.152       \n",
            "   [1]  one                 0.391        0.000        0.000        0.000        -3.971       -0.352       -0.000       \n",
            "   [0]  can                 0.332        11.696       -6.467       -1.104       -4.458       -0.384       -4.074       \n",
            "   [0]  famous              0.383        10.658       -7.537       -0.959       -3.766       -0.403       -3.363       \n",
            "   [1]  and                 0.405        0.000        0.000        0.000        -3.151       -0.447       -0.000       \n",
            "   [0]  buffered            0.392        10.655       -12.907      -0.936       -3.538       -0.458       -3.080       \n",
            "   [0]  with                0.511        5.655        -5.403       -0.672       -2.922       -0.443       -2.478       \n",
            "   [0]  1970s               0.427        3.413        -8.690       -0.850       -2.525       -0.446       -2.079       \n",
            "   [0]  tells               0.353        9.289        -8.006       -1.041       -1.881       -0.389       -1.492       \n",
            "   [0]  into                0.390        4.406        -6.766       -0.943       -0.943       -0.382       -0.561       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  just                0.481        2.872        -5.811       -0.731       -5.205       -0.310       -4.895       \n",
            "   [1]  a                   0.492        0.000        0.000        0.000        -5.023       -0.363       -0.000       \n",
            "   [0]  in                  0.512        6.390        -3.892       -0.669       -5.639       -0.397       -5.000       \n",
            "   [1]  cool                0.354        0.000        0.000        0.000        -5.580       -0.408       -0.000       \n",
            "   [0]  in                  0.381        3.426        -3.492       -0.966       -6.264       -0.466       -5.000       \n",
            "   [0]  extreme             0.235        2.904        -8.639       -1.449       -5.948       -0.510       -5.000       \n",
            "   [1]  enjoyed             0.270        0.000        0.000        0.000        -5.052       -0.610       -0.000       \n",
            "   [1]  it                  0.228        0.000        0.000        0.000        -5.671       -0.673       -0.000       \n",
            "   [1]  immensely           0.281        0.000        0.000        0.000        -6.367       -0.732       -0.000       \n",
            "   [1]  and                 0.300        0.000        0.000        0.000        -7.148       -0.746       -0.000       \n",
            "   [0]  seriously           0.237        6.838        -8.058       -1.439       -8.025       -0.734       -5.000       \n",
            "   [0]  cinematography      0.176        3.280        -8.445       -1.738       -7.394       -0.746       -5.000       \n",
            "   [1]  every               0.141        0.000        0.000        0.000        -6.350       -0.802       -0.000       \n",
            "   [1]  time                0.136        0.000        0.000        0.000        -7.129       -0.879       -0.000       \n",
            "   [0]  used                0.106        3.289        -8.097       -2.245       -8.003       -0.943       -5.000       \n",
            "   [1]  is                  0.118        0.000        0.000        0.000        -6.465       -1.019       -0.000       \n",
            "   [0]  now                 0.118        5.004        -7.533       -2.137       -7.258       -1.057       -5.000       \n",
            "   [1]  tv                  0.097        0.000        0.000        0.000        -5.750       -1.071       -0.000       \n",
            "   [0]  fast                0.041        4.723        -8.230       -3.204       -6.455       -1.094       -5.000       \n",
            "   [0]  together            0.026        4.723        -8.310       -3.651       -3.651       -1.214       -2.436       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  intriguing          0.595        0.000        0.000        0.000        -5.362       -0.310       -0.000       \n",
            "   [0]  worth               0.398        7.608        -7.664       -0.922       -6.019       -0.354       -5.000       \n",
            "   [1]  of                  0.550        0.000        0.000        0.000        -5.723       -0.357       -0.000       \n",
            "   [0]  cents               0.442        10.544       -9.172       -0.816       -6.425       -0.411       -5.000       \n",
            "   [1]  drawn               0.411        0.000        0.000        0.000        -6.297       -0.412       -0.000       \n",
            "   [1]  fantasy             0.248        0.000        0.000        0.000        -7.069       -0.440       -0.000       \n",
            "   [1]  come                0.235        0.000        0.000        0.000        -7.936       -0.484       -0.000       \n",
            "   [0]  a                   0.250        3.902        -3.650       -1.387       -8.910       -0.580       -5.000       \n",
            "   [0]  one                 0.262        7.197        -5.497       -1.338       -8.447       -0.663       -5.000       \n",
            "   [0]  the                 0.285        4.178        -3.176       -1.256       -7.981       -0.709       -5.000       \n",
            "   [0]  fire                0.203        3.699        -10.463      -1.592       -7.550       -0.725       -5.000       \n",
            "   [1]  child               0.123        0.000        0.000        0.000        -6.688       -0.728       -0.000       \n",
            "   [1]  s                   0.129        0.000        0.000        0.000        -7.509       -0.774       -0.000       \n",
            "   [0]  because             0.095        11.631       -6.312       -2.357       -8.430       -0.862       -5.000       \n",
            "   [0]  cassi               0.111        9.474        -10.974      -2.194       -6.818       -0.938       -5.000       \n",
            "   [0]  young               0.122        8.238        -7.595       -2.101       -5.191       -1.006       -4.186       \n",
            "   [0]  for                 0.186        3.627        -5.146       -1.681       -3.469       -1.034       -2.435       \n",
            "   [1]  imagine             0.179        0.000        0.000        0.000        -2.008       -1.020       -0.000       \n",
            "   [1]  the                 0.198        0.000        0.000        0.000        -2.254       -0.957       -0.000       \n",
            "   [0]  alien               0.080        8.678        -8.265       -2.530       -2.530       -0.899       -1.631       \n",
            "Samples\n",
            "Sample 0 .  ve disbelief all premise of collection movies in this series it one can famous and buffered with 1970s tells into\n",
            "Sample 1 .  just a in cool in extreme enjoyed it immensely and seriously cinematography every time used is now tv fast together\n",
            "Sample 2 .  intriguing worth of cents drawn fantasy come a one the fire child s because cassi young for imagine the alien\n",
            "\n",
            "\n",
            "targets[[414 5 35 486 3 1817 5680 2 19 111 174 102 30378 613 3206 4 2695 1442 10597 1861][17 5 37 1149 2 297 12998 368 6 6 9 402 1453 78 10 17 14 2 11244 8][3157 41 15548 14 7 12 11184 542 4 646...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16638 414 5 35 486 3 1817 5680 2 19...]...][[1 0 0 1 1 1 0 1 0 1...]...][[16638 414 69849 69849 486 3 1817 69849 2 69849...]...][[414 5 35 486 3 1817 5680 2 19 111...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16638 414 5 35 486 3 1817 5680 2 19...]...][[1 0 0 1 1 1 0 1 0 1...]...][[16638 414 69849 69849 486 3 1817 69849 2 69849...]...][[414 5 473 486 3 1817 25 2 1 111...]...]\n",
            "targets[[24205 5 2 81 1 1031 153 9 723 21 110 238 332 33 86 239 72 10 0 6754][3006 8799 3 417 652 510 4133 8 537 25 254 8 952 1265 8 0 484 3 21845 8][1550 5 0 117 3 0 733 1541 1012 98...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1882 24205 5 2 81 1 1031 153 9 723...]...][[0 0 1 0 1 0 1 1 0 0...]...][[1882 69849 69849 2 69849 1 69849 153 9 69849...]...][[24205 5 2 81 1 1031 153 9 723 21...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1882 24205 5 2 81 1 1031 153 9 723...]...][[0 0 1 0 1 0 1 1 0 0...]...][[1882 69849 69849 2 69849 1 69849 153 9 69849...]...][[13 348 2 369 1 413 153 9 12 1036...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 51 26 1071...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 64 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 1 3 48...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "global_step: 359\n",
            " perplexity: 891.595\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            " percent of 3-grams captured: 0.153.\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            " percent of 2-grams captured: 0.560.\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            " percent of 4-grams captured: 0.020.\n",
            " geometric_avg: 0.119.\n",
            " arithmetic_avg: 0.244.\n",
            "global_step: 359\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66811\n",
            " G train loss: 161.81311\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 64 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 0 42 242...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  of                  0.613        0.000        0.000        0.000        -1.283       -0.291       -0.000       \n",
            "   [1]  the                 0.617        0.000        0.000        0.000        -1.440       -0.344       -0.000       \n",
            "   [1]  worst               0.513        0.000        0.000        0.000        -1.617       -0.332       -0.000       \n",
            "   [1]  movies              0.534        0.000        0.000        0.000        -1.815       -0.283       -0.000       \n",
            "   [1]  ever                0.524        0.000        0.000        0.000        -2.038       -0.297       -0.000       \n",
            "   [1]  made                0.619        0.000        0.000        0.000        -2.288       -0.311       -0.000       \n",
            "   [1]  not                 0.632        0.000        0.000        0.000        -2.568       -0.345       -0.000       \n",
            "   [0]  the                 0.636        6.107        -2.743       -0.452       -2.884       -0.332       -2.551       \n",
            "   [0]  just                0.593        4.055        -5.551       -0.522       -2.730       -0.306       -2.423       \n",
            "   [0]  am                  0.589        2.752        -6.874       -0.529       -2.478       -0.263       -2.216       \n",
            "   [0]  best                0.622        12.989       -6.436       -0.474       -2.189       -0.263       -1.925       \n",
            "   [0]  real                0.550        7.780        -7.743       -0.598       -1.925       -0.276       -1.649       \n",
            "   [1]  which               0.582        0.000        0.000        0.000        -1.490       -0.252       -0.000       \n",
            "   [0]  one                 0.588        9.730        -5.385       -0.531       -1.673       -0.273       -1.399       \n",
            "   [1]  an                  0.630        0.000        0.000        0.000        -1.282       -0.278       -0.000       \n",
            "   [0]  now                 0.619        7.732        -7.365       -0.479       -1.439       -0.293       -1.146       \n",
            "   [0]  certainly           0.535        13.179       -7.670       -0.625       -1.078       -0.280       -0.798       \n",
            "   [0]  or                  0.601        3.712        -5.775       -0.509       -0.509       -0.246       -0.262       \n",
            "   [1]  cinematic           0.580        0.000        0.000        0.000        0.000        -0.270       0.000        \n",
            "   [1]  dung                0.617        0.000        0.000        0.000        0.000        -0.259       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  rhonda              0.493        2.845        -14.078      -0.708       -6.108       -0.312       -5.000       \n",
            "   [1]  name                0.343        0.000        0.000        0.000        -6.062       -0.358       -0.000       \n",
            "   [0]  it                  0.293        5.718        -4.260       -1.228       -6.806       -0.420       -5.000       \n",
            "   [1]  karate              0.214        0.000        0.000        0.000        -6.263       -0.483       -0.000       \n",
            "   [0]  so                  0.189        8.960        -5.700       -1.668       -7.031       -0.570       -5.000       \n",
            "   [0]  a                   0.200        6.573        -2.808       -1.608       -6.021       -0.646       -5.000       \n",
            "   [0]  the                 0.227        5.759        -2.744       -1.483       -4.955       -0.702       -4.253       \n",
            "   [0]  with                0.342        5.414        -4.859       -1.074       -3.897       -0.728       -3.169       \n",
            "   [0]  said                0.459        6.537        -8.156       -0.779       -3.170       -0.696       -2.474       \n",
            "   [0]  time                0.409        8.398        -5.810       -0.893       -2.684       -0.622       -2.062       \n",
            "   [1]  this                0.447        0.000        0.000        0.000        -2.010       -0.560       -0.000       \n",
            "   [1]  movie               0.430        0.000        0.000        0.000        -2.256       -0.496       -0.000       \n",
            "   [0]  dovetails           0.422        4.641        -14.411      -0.864       -2.533       -0.461       -2.072       \n",
            "   [0]  is                  0.464        2.981        -4.644       -0.768       -1.874       -0.444       -1.430       \n",
            "   [1]  perfect             0.507        0.000        0.000        0.000        -1.242       -0.436       -0.000       \n",
            "   [0]  cathartic           0.496        8.496        -14.037      -0.701       -1.394       -0.422       -0.972       \n",
            "   [1]  of                  0.628        0.000        0.000        0.000        -0.779       -0.413       -0.000       \n",
            "   [0]  s                   0.617        2.993        -4.613       -0.483       -0.874       -0.376       -0.498       \n",
            "   [1]  brainless           0.577        0.000        0.000        0.000        -0.440       -0.346       -0.000       \n",
            "   [0]  the                 0.610        13.130       -2.810       -0.494       -0.494       -0.327       -0.167       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  spoiler             0.320        9.215        -6.922       -1.138       -4.677       -0.304       -4.373       \n",
            "   [1]  as                  0.364        0.000        0.000        0.000        -3.974       -0.342       -0.000       \n",
            "   [0]  your                0.382        14.055       -7.701       -0.962       -4.461       -0.399       -4.062       \n",
            "   [1]  the                 0.404        0.000        0.000        0.000        -3.929       -0.444       -0.000       \n",
            "   [0]  have                0.365        13.121       -4.874       -1.007       -4.411       -0.468       -3.943       \n",
            "   [0]  disbelief           0.413        4.062        -11.690      -0.884       -3.822       -0.472       -3.350       \n",
            "   [1]  a                   0.410        0.000        0.000        0.000        -3.298       -0.476       -0.000       \n",
            "   [0]  bugs                0.336        11.142       -9.779       -1.092       -3.703       -0.468       -3.235       \n",
            "   [0]  between             0.390        13.760       -7.343       -0.942       -2.931       -0.459       -2.472       \n",
            "   [0]  going               0.295        5.209        -7.352       -1.220       -2.233       -0.473       -1.760       \n",
            "   [1]  an                  0.367        0.000        0.000        0.000        -1.137       -0.484       -0.000       \n",
            "   [1]  armless             0.402        0.000        0.000        0.000        -1.276       -0.511       -0.000       \n",
            "   [1]  circus              0.355        0.000        0.000        0.000        -1.433       -0.521       -0.000       \n",
            "   [0]  horror              0.447        12.483       -6.903       -0.805       -1.609       -0.520       -1.089       \n",
            "   [1]  wielder             0.492        0.000        0.000        0.000        -0.902       -0.522       -0.000       \n",
            "   [1]  he                  0.506        0.000        0.000        0.000        -1.013       -0.507       -0.000       \n",
            "   [0]  was                 0.513        10.046       -4.263       -0.668       -1.137       -0.485       -0.652       \n",
            "   [1]  by                  0.468        0.000        0.000        0.000        -0.527       -0.455       -0.000       \n",
            "   [1]  throwing            0.553        0.000        0.000        0.000        -0.591       -0.438       -0.000       \n",
            "   [0]  i                   0.515        12.927       -3.587       -0.664       -0.664       -0.439       -0.225       \n",
            "Samples\n",
            "Sample 0 .  of the worst movies ever made not the just am best real which one an now certainly or cinematic dung\n",
            "Sample 1 .  rhonda name it karate so a the with said time this movie dovetails is perfect cathartic of s brainless the\n",
            "Sample 2 .  spoiler as your the have disbelief a bugs between going an armless circus horror wielder he was by throwing i\n",
            "\n",
            "\n",
            "targets[[84 151 9 196 99 318 10 17 13 87 5 10 614 11 0 8561 165 386 2 1686][81 11360 3 584 24951 5 74 19 8 174 95 0 225 0 5054 1069 0 608 3 1132][263 214 8 26 17196 590 284 9695 45 8084...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 84 151 9 196 99 318 10 17 13...]...][[0 0 1 0 0 1 1 1 0 0...]...][[0 69849 69849 9 69849 69849 318 10 17 69849...]...][[84 151 9 196 99 318 10 17 13 87...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 84 151 9 196 99 318 10 17 13...]...][[0 0 1 0 0 1 1 1 0 0...]...][[0 69849 69849 9 69849 69849 318 10 17 69849...]...][[711 95 9 3 58308 318 10 17 628 13...]...]\n",
            "targets[[12 56 6072 7 10140 978 2243 317 5 394 0 116 5 37 74 11 20 232 3454 1][5 23 64 0 4144 3 30 1224 98 18 0 872 184 17 123 284 3467 1 9402 2000][194 227 2041 26370 35 554 1031 279 34 1193...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[39 12 56 6072 7 10140 978 2243 317 5...]...][[1 1 0 0 0 1 1 1 1 1...]...][[39 12 56 69849 69849 69849 978 2243 317 5...]...][[12 56 6072 7 10140 978 2243 317 5 394...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[39 12 56 6072 7 10140 978 2243 317 5...]...][[1 1 0 0 0 1 1 1 1 1...]...][[39 12 56 69849 69849 69849 978 2243 317 5...]...][[12 56 12 1515 126 978 2243 317 5 394...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 201 67518 10 657 6380 458 23266...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 451 55 4 91 6380 7745 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 128 1027 4 4 6380 71 95...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "global_step: 364\n",
            " perplexity: 894.439\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            " percent of 3-grams captured: 0.166.\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            " percent of 2-grams captured: 0.580.\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            " percent of 4-grams captured: 0.030.\n",
            " geometric_avg: 0.143.\n",
            " arithmetic_avg: 0.259.\n",
            "global_step: 364\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66622\n",
            " G train loss: 162.32452\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 451 55 4 91 6380 7745 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 1027 459 150 162 6380 15 17...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  indie               0.594        0.000        0.000        0.000        -3.443       -0.293       -0.000       \n",
            "   [1]  sex                 0.516        0.000        0.000        0.000        -3.865       -0.361       -0.000       \n",
            "   [1]  comedy              0.509        0.000        0.000        0.000        -4.339       -0.333       -0.000       \n",
            "   [0]  expecting           0.549        10.884       -8.336       -0.600       -4.872       -0.344       -4.528       \n",
            "   [0]  4                   0.444        6.503        -8.170       -0.812       -4.796       -0.364       -4.431       \n",
            "   [0]  doesn               0.342        3.682        -7.321       -1.073       -4.472       -0.309       -4.164       \n",
            "   [0]  makes               0.378        6.604        -7.702       -0.972       -3.816       -0.288       -3.528       \n",
            "   [1]  billing             0.398        0.000        0.000        0.000        -3.193       -0.357       -0.000       \n",
            "   [0]  with                0.519        13.681       -5.266       -0.657       -3.584       -0.407       -3.177       \n",
            "   [0]  movie               0.464        3.053        -3.861       -0.767       -3.287       -0.481       -2.806       \n",
            "   [1]  caroms              0.414        0.000        0.000        0.000        -2.828       -0.431       -0.000       \n",
            "   [1]  builds              0.546        0.000        0.000        0.000        -3.175       -0.380       -0.000       \n",
            "   [1]  up                  0.561        0.000        0.000        0.000        -3.565       -0.432       -0.000       \n",
            "   [0]  word                0.421        3.694        -7.874       -0.864       -4.002       -0.406       -3.596       \n",
            "   [0]  been                0.391        2.758        -6.417       -0.938       -3.522       -0.306       -3.216       \n",
            "   [1]  humorous            0.335        0.000        0.000        0.000        -2.901       -0.305       -0.000       \n",
            "   [0]  i                   0.330        3.091        -3.469       -1.110       -3.257       -0.308       -2.949       \n",
            "   [0]  be                  0.433        13.270       -5.330       -0.838       -2.411       -0.346       -2.065       \n",
            "   [0]  only                0.429        12.906       -6.265       -0.846       -1.766       -0.432       -1.334       \n",
            "   [0]  some                0.356        5.985        -5.589       -1.034       -1.034       -0.432       -0.602       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  s                   0.571        0.000        0.000        0.000        -4.367       -0.306       -0.000       \n",
            "   [1]  probably            0.522        0.000        0.000        0.000        -4.903       -0.349       -0.000       \n",
            "   [0]  movie               0.498        6.729        -3.184       -0.697       -5.504       -0.367       -5.000       \n",
            "   [1]  point               0.395        0.000        0.000        0.000        -5.397       -0.382       -0.000       \n",
            "   [0]  living              0.431        4.236        -9.609       -0.842       -6.060       -0.397       -5.000       \n",
            "   [1]  trying              0.343        0.000        0.000        0.000        -5.858       -0.423       -0.000       \n",
            "   [1]  to                  0.397        0.000        0.000        0.000        -6.577       -0.452       -0.000       \n",
            "   [1]  describe            0.406        0.000        0.000        0.000        -7.384       -0.476       -0.000       \n",
            "   [0]  it                  0.332        14.520       -2.907       -1.102       -8.290       -0.483       -5.000       \n",
            "   [1]  teen                0.209        0.000        0.000        0.000        -8.070       -0.480       -0.000       \n",
            "   [0]  i                   0.196        14.295       -2.918       -1.632       -9.060       -0.497       -5.000       \n",
            "   [0]  trouble             0.164        8.502        -9.919       -1.810       -8.340       -0.540       -5.000       \n",
            "   [0]  the                 0.178        15.379       -2.824       -1.725       -7.331       -0.588       -5.000       \n",
            "   [0]  this                0.201        4.028        -4.314       -1.604       -6.294       -0.628       -5.000       \n",
            "   [0]  that                0.213        3.654        -3.932       -1.546       -5.264       -0.644       -4.620       \n",
            "   [0]  but                 0.280        7.726        -5.045       -1.273       -4.175       -0.629       -3.546       \n",
            "   [0]  this                0.301        5.397        -4.314       -1.200       -3.258       -0.602       -2.656       \n",
            "   [0]  horrid              0.346        9.312        -11.464      -1.062       -2.311       -0.560       -1.751       \n",
            "   [0]  quality             0.246        5.127        -7.955       -1.402       -1.402       -0.512       -0.889       \n",
            "   [1]  already             0.207        0.000        0.000        0.000        0.000        -0.476       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  wont                0.595        0.000        0.000        0.000        -0.900       -0.294       -0.000       \n",
            "   [1]  believe             0.663        0.000        0.000        0.000        -1.010       -0.336       -0.000       \n",
            "   [0]  be                  0.709        3.653        -5.792       -0.344       -1.134       -0.353       -0.781       \n",
            "   [1]  i                   0.661        0.000        0.000        0.000        -0.887       -0.340       -0.000       \n",
            "   [1]  was                 0.670        0.000        0.000        0.000        -0.996       -0.286       -0.000       \n",
            "   [1]  16                  0.665        0.000        0.000        0.000        -1.119       -0.264       -0.000       \n",
            "   [1]  when                0.705        0.000        0.000        0.000        -1.256       -0.251       -0.000       \n",
            "   [0]  bring               0.692        3.467        -9.254       -0.368       -1.410       -0.260       -1.150       \n",
            "   [0]  bat                 0.716        6.731        -10.780      -0.335       -1.170       -0.243       -0.927       \n",
            "   [0]  s                   0.712        5.902        -4.575       -0.340       -0.938       -0.241       -0.697       \n",
            "   [1]  proudly             0.754        0.000        0.000        0.000        -0.671       -0.230       -0.000       \n",
            "   [1]  we                  0.780        0.000        0.000        0.000        -0.753       -0.237       -0.000       \n",
            "   [1]  hail                0.748        0.000        0.000        0.000        -0.845       -0.233       -0.000       \n",
            "   [1]  thirty              0.722        0.000        0.000        0.000        -0.949       -0.198       -0.000       \n",
            "   [0]  one                 0.726        6.538        -5.350       -0.320       -1.066       -0.171       -0.895       \n",
            "   [1]  later               0.734        0.000        0.000        0.000        -0.837       -0.180       -0.000       \n",
            "   [0]  real                0.676        3.566        -6.836       -0.392       -0.940       -0.190       -0.750       \n",
            "   [0]  we                  0.732        3.761        -7.208       -0.312       -0.615       -0.176       -0.439       \n",
            "   [0]  unlocking           0.712        3.391        -13.670      -0.339       -0.339       -0.218       -0.122       \n",
            "   [1]  remembered          0.777        0.000        0.000        0.000        0.000        -0.219       0.000        \n",
            "Samples\n",
            "Sample 0 .  indie sex comedy expecting 4 doesn makes billing with movie caroms builds up word been humorous i be only some\n",
            "Sample 1 .  s probably movie point living trying to describe it teen i trouble the this that but this horrid quality already\n",
            "Sample 2 .  wont believe be i was 16 when bring bat s proudly we hail thirty one later real we unlocking remembered\n",
            "\n",
            "\n",
            "targets[[619 4398 282 1065 11 56 534 96 123 2228 2460 13989 4 0 6392 3 2 243 507 16][89 21 121 87 7 195 49 2921 9 89 21 456 46 7 13 2 297 63 9 856][70 651 8 1806 2591 374 4784 1 681 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 619 4398 282 1065 11 56 534 96 123...]...][[0 1 1 0 0 0 1 1 1 1...]...][[2 69849 4398 282 69849 69849 69849 534 96 123...]...][[619 4398 282 1065 11 56 534 96 123 2228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 619 4398 282 1065 11 56 534 96 123...]...][[0 1 1 0 0 0 1 1 1 1...]...][[2 69849 4398 282 69849 69849 69849 534 96 123...]...][[19 4398 282 18 341 49 534 96 123 2228...]...]\n",
            "targets[[1116 19 36 5051 8 61 0 1500 3 6845 14867 303 377 25 6039 15 2 160 30388 4357][139 77 2754 2 394 19 6 6 815 9 232 987 11 233 9 140 460 1 27 67][140 23 2 8791 147 882 123 27 77 18...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2952 1116 19 36 5051 8 61 0 1500 3...]...][[1 1 1 1 1 0 0 0 1 1...]...][[2952 1116 19 36 5051 8 69849 69849 69849 3...]...][[1116 19 36 5051 8 61 0 1500 3 6845...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2952 1116 19 36 5051 8 61 0 1500 3...]...][[1 1 1 1 1 0 0 0 1 1...]...][[2952 1116 19 36 5051 8 69849 69849 69849 3...]...][[1116 19 36 5051 8 539 7 4 3 6845...]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[353 32756 199 3895 62324 436 1 4242 0 6...]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[17 32756 199 3895 5998 436 1 4242 868 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[32 32756 199 3895 35 436 1 4242 29 6...]...]\n",
            "I0310 00:35:12.858817 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "global_step: 369\n",
            " perplexity: 900.280\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            " percent of 3-grams captured: 0.162.\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            " percent of 2-grams captured: 0.583.\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            " percent of 4-grams captured: 0.026.\n",
            " geometric_avg: 0.135.\n",
            " arithmetic_avg: 0.257.\n",
            "global_step: 369\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66527\n",
            " G train loss: 161.99614\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[17 32756 199 3895 5998 436 1 4242 868 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[1100 32756 199 3895 21 436 1 4242 422 6...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  u                   0.450        2.911        -8.144       -0.798       -1.597       -0.303       -1.294       \n",
            "   [1]  alternated          0.497        0.000        0.000        0.000        -0.898       -0.365       -0.000       \n",
            "   [1]  between             0.567        0.000        0.000        0.000        -1.008       -0.394       -0.000       \n",
            "   [1]  captivating         0.609        0.000        0.000        0.000        -1.131       -0.384       -0.000       \n",
            "   [0]  t                   0.604        14.627       -4.634       -0.504       -1.270       -0.350       -0.920       \n",
            "   [1]  drama               0.547        0.000        0.000        0.000        -0.860       -0.310       -0.000       \n",
            "   [1]  and                 0.576        0.000        0.000        0.000        -0.965       -0.284       -0.000       \n",
            "   [1]  formulaic           0.707        0.000        0.000        0.000        -1.084       -0.271       -0.000       \n",
            "   [0]  episode             0.723        8.940        -8.156       -0.325       -1.217       -0.233       -0.984       \n",
            "   [1]  br                  0.753        0.000        0.000        0.000        -1.001       -0.179       -0.000       \n",
            "   [1]  br                  0.780        0.000        0.000        0.000        -1.124       -0.120       -0.000       \n",
            "   [0]  nobody              0.749        2.629        -10.245      -0.289       -1.262       -0.063       -1.199       \n",
            "   [0]  and                 0.758        14.451       -3.007       -0.277       -1.092       -0.017       -1.075       \n",
            "   [1]  drama               0.723        0.000        0.000        0.000        -0.915       0.010        -0.000       \n",
            "   [1]  part                0.687        0.000        0.000        0.000        -1.027       0.017        -0.000       \n",
            "   [0]  i                   0.678        3.930        -3.196       -0.389       -1.153       -0.004       -1.150       \n",
            "   [0]  shall               0.593        13.278       -8.729       -0.523       -0.858       -0.046       -0.813       \n",
            "   [0]  about               0.686        8.769        -5.581       -0.376       -0.376       -0.110       -0.267       \n",
            "   [1]  by                  0.648        0.000        0.000        0.000        0.000        -0.157       0.000        \n",
            "   [1]  the                 0.652        0.000        0.000        0.000        0.000        -0.195       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  philmore            0.497        5.701        -13.267      -0.700       -3.743       -0.296       -3.447       \n",
            "   [0]  reviews             0.473        3.437        -7.318       -0.749       -3.417       -0.327       -3.090       \n",
            "   [1]  gun                 0.433        0.000        0.000        0.000        -2.995       -0.348       -0.000       \n",
            "   [0]  in                  0.431        3.947        -4.571       -0.842       -3.362       -0.361       -3.001       \n",
            "   [0]  captain             0.527        7.417        -11.088      -0.641       -2.829       -0.370       -2.459       \n",
            "   [0]  making              0.506        6.640        -7.633       -0.682       -2.457       -0.367       -2.090       \n",
            "   [1]  forgotten           0.315        0.000        0.000        0.000        -1.993       -0.348       -0.000       \n",
            "   [0]  a                   0.319        6.947        -3.506       -1.143       -2.237       -0.361       -1.876       \n",
            "   [1]  but                 0.426        0.000        0.000        0.000        -1.229       -0.395       -0.000       \n",
            "   [1]  caused              0.410        0.000        0.000        0.000        -1.379       -0.409       -0.000       \n",
            "   [0]  first               0.465        3.567        -6.094       -0.767       -1.548       -0.406       -1.143       \n",
            "   [1]  minor               0.446        0.000        0.000        0.000        -0.878       -0.390       -0.000       \n",
            "   [1]  storm               0.508        0.000        0.000        0.000        -0.985       -0.369       -0.000       \n",
            "   [1]  of                  0.583        0.000        0.000        0.000        -1.106       -0.351       -0.000       \n",
            "   [0]  misfortune          0.614        10.645       -10.571      -0.487       -1.242       -0.319       -0.923       \n",
            "   [1]  interest            0.542        0.000        0.000        0.000        -0.847       -0.283       -0.000       \n",
            "   [0]  the                 0.553        7.220        -3.222       -0.592       -0.951       -0.249       -0.703       \n",
            "   [1]  in                  0.545        0.000        0.000        0.000        -0.404       -0.242       -0.000       \n",
            "   [1]  1955                0.490        0.000        0.000        0.000        -0.453       -0.243       -0.000       \n",
            "   [0]  directed            0.601        6.207        -8.062       -0.509       -0.509       -0.255       -0.255       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  bodyguard           0.579        0.000        0.000        0.000        -2.557       -0.285       -0.000       \n",
            "   [1]  is                  0.555        0.000        0.000        0.000        -2.871       -0.317       -0.000       \n",
            "   [0]  been                0.494        12.960       -6.128       -0.705       -3.223       -0.292       -2.931       \n",
            "   [0]  the                 0.509        2.951        -3.207       -0.675       -2.827       -0.264       -2.563       \n",
            "   [1]  love                0.504        0.000        0.000        0.000        -2.416       -0.260       -0.000       \n",
            "   [0]  own                 0.579        6.138        -6.904       -0.547       -2.712       -0.260       -2.452       \n",
            "   [0]  on                  0.641        7.942        -5.556       -0.445       -2.431       -0.265       -2.166       \n",
            "   [0]  and                 0.629        7.336        -3.455       -0.464       -2.229       -0.249       -1.980       \n",
            "   [0]  and                 0.625        13.362       -3.465       -0.471       -1.982       -0.205       -1.776       \n",
            "   [1]  characters          0.561        0.000        0.000        0.000        -1.696       -0.173       -0.000       \n",
            "   [1]  with                0.659        0.000        0.000        0.000        -1.904       -0.150       -0.000       \n",
            "   [0]  done                0.516        7.068        -7.770       -0.661       -2.138       -0.166       -1.972       \n",
            "   [0]  a                   0.512        6.844        -3.263       -0.670       -1.658       -0.130       -1.528       \n",
            "   [1]  idiosyncrasies      0.525        0.000        0.000        0.000        -1.110       -0.152       -0.000       \n",
            "   [0]  boring              0.539        3.303        -7.819       -0.619       -1.246       -0.179       -1.067       \n",
            "   [0]  of                  0.631        5.815        -3.491       -0.461       -0.704       -0.195       -0.509       \n",
            "   [1]  is                  0.602        0.000        0.000        0.000        -0.273       -0.208       -0.000       \n",
            "   [1]  intriguing          0.662        0.000        0.000        0.000        -0.306       -0.179       -0.000       \n",
            "   [1]  enough              0.681        0.000        0.000        0.000        -0.344       -0.170       -0.000       \n",
            "   [0]  this                0.680        4.097        -3.741       -0.386       -0.386       -0.141       -0.245       \n",
            "Samples\n",
            "Sample 0 .  u alternated between captivating t drama and formulaic episode br br nobody and drama part i shall about by the\n",
            "Sample 1 .  philmore reviews gun in captain making forgotten a but caused first minor storm of misfortune interest the in 1955 directed\n",
            "Sample 2 .  bodyguard is been the love own on and and characters with done a idiosyncrasies boring of is intriguing enough this\n",
            "\n",
            "\n",
            "targets[[17 5 2 997 36 0 457 4 0 129 0 101 80 1 136 47 0 543 491 90][74 19 53 53 53 74 19 7 12 2 10027 18 7 5 23 273 3299 185 10 1071][203 66 16 74 17 4603 247 528 2 461...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 2 997 36 0 457 4 0...]...][[0 1 0 1 0 0 0 1 0 1...]...][[10 69849 5 69849 997 69849 69849 69849 4 69849...]...][[17 5 2 997 36 0 457 4 0 129...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 2 997 36 0 457 4 0...]...][[0 1 0 1 0 0 0 1 0 1...]...][[10 69849 5 69849 997 69849 69849 69849 4 69849...]...][[60 5 41 997 633 184 16 4 42463 129...]...]\n",
            "targets[[5095 1070 4 479 966 12 3115 1667 1056 201 78 2 3631 917 725 15 0 914 107 193][39 20 27 7 157 24170 22 60 2619 105 165 84 3 30 58 152 9 38 4 103][6625 5 437 379 0 109 5 53 348 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5256 5095 1070 4 479 966 12 3115 1667 1056...]...][[0 1 0 1 1 1 0 0 0 1...]...][[5256 69849 1070 69849 479 966 12 69849 69849 69849...]...][[5095 1070 4 479 966 12 3115 1667 1056 201...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5256 5095 1070 4 479 966 12 3115 1667 1056...]...][[0 1 0 1 1 1 0 0 0 1...]...][[5256 69849 1070 69849 479 966 12 69849 69849 69849...]...][[37 1070 3752 479 966 12 74 23 782 201...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 60 1288 3 193 156 5920 257 656 1...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 2 340 3 193 156 5920 257 14340 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 4 12 3 193 156 5920 257 7 1...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "global_step: 374\n",
            " perplexity: 900.888\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            " percent of 3-grams captured: 0.163.\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            " percent of 2-grams captured: 0.598.\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            " percent of 4-grams captured: 0.032.\n",
            " geometric_avg: 0.146.\n",
            " arithmetic_avg: 0.264.\n",
            "global_step: 374\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66555\n",
            " G train loss: 161.04105\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 2 340 3 193 156 5920 257 14340 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 10 13 3 193 156 5920 257 2181 1...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  m                   0.457        0.000        0.000        0.000        -3.166       -0.299       -0.000       \n",
            "   [0]  this                0.506        2.975        -3.006       -0.682       -3.554       -0.313       -3.242       \n",
            "   [0]  was                 0.504        7.266        -4.098       -0.684       -3.225       -0.350       -2.875       \n",
            "   [1]  of                  0.586        0.000        0.000        0.000        -2.853       -0.360       -0.000       \n",
            "   [1]  both                0.455        0.000        0.000        0.000        -3.203       -0.389       -0.000       \n",
            "   [1]  actors              0.413        0.000        0.000        0.000        -3.596       -0.321       -0.000       \n",
            "   [1]  singers             0.445        0.000        0.000        0.000        -4.037       -0.298       -0.000       \n",
            "   [1]  especially          0.386        0.000        0.000        0.000        -4.532       -0.329       -0.000       \n",
            "   [0]  vehicle             0.191        15.217       -8.166       -1.655       -5.088       -0.330       -4.758       \n",
            "   [1]  and                 0.237        0.000        0.000        0.000        -3.854       -0.292       -0.000       \n",
            "   [1]  when                0.313        0.000        0.000        0.000        -4.327       -0.372       -0.000       \n",
            "   [0]  and                 0.314        3.216        -2.862       -1.158       -4.858       -0.455       -4.403       \n",
            "   [0]  i                   0.278        6.152        -3.215       -1.279       -4.154       -0.467       -3.687       \n",
            "   [0]  or                  0.348        9.452        -6.048       -1.056       -3.227       -0.444       -2.784       \n",
            "   [0]  if                  0.352        3.913        -6.629       -1.045       -2.438       -0.452       -1.986       \n",
            "   [0]  group               0.373        4.096        -8.518       -0.985       -1.563       -0.430       -1.133       \n",
            "   [1]  and                 0.373        0.000        0.000        0.000        -0.649       -0.416       -0.000       \n",
            "   [1]  watch               0.393        0.000        0.000        0.000        -0.729       -0.392       -0.000       \n",
            "   [0]  are                 0.441        2.397        -5.143       -0.818       -0.818       -0.382       -0.436       \n",
            "   [1]  trailer             0.273        0.000        0.000        0.000        0.000        -0.386       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  elements            0.505        7.588        -9.221       -0.683       -4.274       -0.300       -3.974       \n",
            "   [0]  is                  0.508        3.459        -2.365       -0.677       -4.032       -0.332       -3.700       \n",
            "   [0]  a                   0.485        6.862        -2.215       -0.724       -3.767       -0.348       -3.419       \n",
            "   [0]  the                 0.491        9.010        -2.351       -0.712       -3.415       -0.351       -3.065       \n",
            "   [1]  the                 0.492        0.000        0.000        0.000        -3.035       -0.355       -0.000       \n",
            "   [0]  the                 0.489        6.427        -2.280       -0.715       -3.407       -0.357       -3.051       \n",
            "   [1]  to                  0.522        0.000        0.000        0.000        -3.023       -0.356       -0.000       \n",
            "   [0]  take                0.494        7.628        -8.566       -0.705       -3.394       -0.359       -3.034       \n",
            "   [0]  the                 0.493        8.308        -2.319       -0.707       -3.019       -0.348       -2.671       \n",
            "   [0]  the                 0.492        2.550        -2.329       -0.709       -2.595       -0.346       -2.249       \n",
            "   [0]  premise             0.411        8.220        -8.480       -0.889       -2.117       -0.347       -1.771       \n",
            "   [1]  of                  0.516        0.000        0.000        0.000        -1.379       -0.345       -0.000       \n",
            "   [1]  any                 0.442        0.000        0.000        0.000        -1.548       -0.379       -0.000       \n",
            "   [0]  i                   0.419        2.824        -3.265       -0.871       -1.738       -0.370       -1.367       \n",
            "   [1]  the                 0.435        0.000        0.000        0.000        -0.974       -0.381       -0.000       \n",
            "   [1]  movies              0.474        0.000        0.000        0.000        -1.093       -0.396       -0.000       \n",
            "   [1]  i                   0.437        0.000        0.000        0.000        -1.227       -0.407       -0.000       \n",
            "   [0]  i                   0.413        5.040        -3.266       -0.885       -1.378       -0.397       -0.981       \n",
            "   [1]  seen                0.565        0.000        0.000        0.000        -0.554       -0.396       -0.000       \n",
            "   [0]  the                 0.537        4.906        -2.356       -0.621       -0.621       -0.413       -0.209       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  movie               0.500        12.428       -2.564       -0.693       -2.412       -0.296       -2.116       \n",
            "   [1]  in                  0.486        0.000        0.000        0.000        -1.930       -0.335       -0.000       \n",
            "   [1]  my                  0.575        0.000        0.000        0.000        -2.166       -0.361       -0.000       \n",
            "   [1]  opinion             0.479        0.000        0.000        0.000        -2.432       -0.366       -0.000       \n",
            "   [1]  is                  0.494        0.000        0.000        0.000        -2.731       -0.356       -0.000       \n",
            "   [1]  a                   0.480        0.000        0.000        0.000        -3.066       -0.360       -0.000       \n",
            "   [0]  it                  0.413        5.818        -3.706       -0.884       -3.442       -0.366       -3.076       \n",
            "   [0]  mujhse              0.419        14.015       -13.854      -0.870       -2.872       -0.383       -2.489       \n",
            "   [0]  directing           0.420        11.877       -9.327       -0.868       -2.248       -0.409       -1.838       \n",
            "   [1]  movie               0.398        0.000        0.000        0.000        -1.549       -0.435       -0.000       \n",
            "   [1]  one                 0.407        0.000        0.000        0.000        -1.739       -0.462       -0.000       \n",
            "   [1]  of                  0.490        0.000        0.000        0.000        -1.952       -0.478       -0.000       \n",
            "   [1]  the                 0.467        0.000        0.000        0.000        -2.191       -0.469       -0.000       \n",
            "   [1]  greatest            0.464        0.000        0.000        0.000        -2.460       -0.447       -0.000       \n",
            "   [0]  if                  0.471        7.257        -6.141       -0.752       -2.762       -0.425       -2.337       \n",
            "   [0]  lightest            0.452        5.600        -12.676      -0.793       -2.256       -0.403       -1.853       \n",
            "   [0]  episode             0.511        4.260        -7.755       -0.671       -1.643       -0.388       -1.255       \n",
            "   [0]  of                  0.575        4.256        -3.868       -0.553       -1.091       -0.377       -0.715       \n",
            "   [0]  the                 0.546        4.333        -2.986       -0.605       -0.605       -0.350       -0.255       \n",
            "   [1]  my                  0.614        0.000        0.000        0.000        0.000        -0.318       0.000        \n",
            "Samples\n",
            "Sample 0 .  m this was of both actors singers especially vehicle and when and i or if group and watch are trailer\n",
            "Sample 1 .  elements is a the the the to take the the premise of any i the movies i i seen the\n",
            "Sample 2 .  movie in my opinion is a it mujhse directing movie one of the greatest if lightest episode of the my\n",
            "\n",
            "\n",
            "targets[[29 3 0 88 901 1211 1 4316 1146 98 22 0 3246 123 92 10 5 229 1674 4][317 2502 36 35 1914 3505 4 168 31 0 3615 20 59 103 7 12 43 2 461 2217][312 118 23 942 47 2 1594 10 17 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[437 29 3 0 88 901 1211 1 4316 1146...]...][[0 0 0 0 0 0 1 0 1 1...]...][[437 69849 69849 69849 69849 69849 69849 1 69849 1146...]...][[29 3 0 88 901 1211 1 4316 1146 98...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[437 29 3 0 88 901 1211 1 4316 1146...]...][[0 0 0 0 0 0 1 0 1 1...]...][[437 69849 69849 69849 69849 69849 69849 1 69849 1146...]...][[167 21 53234 43 379 53 1 19 1146 98...]...]\n",
            "targets[[793 4 93 71 262 11 0 834 3 10 29385 2333 184 680 13 1403 33 0 851 227][371 856 137 397 36 10 17 35 3309 177 3 1031 1 155 3154 141 27 68216 69 6][3320 1704 8 112 15 4197 15829 10 122 982...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[75 793 4 93 71 262 11 0 834 3...]...][[0 0 0 0 0 1 0 0 0 1...]...][[75 69849 69849 69849 69849 69849 11 69849 69849 69849...]...][[793 4 93 71 262 11 0 834 3 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[75 793 4 93 71 262 11 0 834 3...]...][[0 0 0 0 0 1 0 0 0 1...]...][[75 69849 69849 69849 69849 69849 11 69849 69849 69849...]...][[543 16616 33 11 139 11 2977 58431 121 10...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[17 307 467 80 23 395 149 312 17 0...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[2 366 9 80 23 395 149 10 17 8386...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[1543 343 18 80 23 395 149 104 17 19...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "global_step: 379\n",
            " perplexity: 901.623\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            " percent of 3-grams captured: 0.159.\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            " percent of 2-grams captured: 0.590.\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            " percent of 4-grams captured: 0.028.\n",
            " geometric_avg: 0.139.\n",
            " arithmetic_avg: 0.259.\n",
            "global_step: 379\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66490\n",
            " G train loss: 160.79492\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[2 366 9 80 23 395 149 10 17 8386...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[724 563 1100 80 23 395 149 21 17 48...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  falls               0.308        3.656        -8.599       -1.178       -4.594       -0.294       -4.300       \n",
            "   [0]  voice               0.317        7.240        -7.861       -1.148       -3.835       -0.339       -3.496       \n",
            "   [0]  u                   0.304        3.347        -8.176       -1.190       -3.017       -0.374       -2.644       \n",
            "   [1]  do                  0.411        0.000        0.000        0.000        -2.051       -0.426       -0.000       \n",
            "   [1]  not                 0.465        0.000        0.000        0.000        -2.303       -0.452       -0.000       \n",
            "   [1]  recommend           0.300        0.000        0.000        0.000        -2.585       -0.456       -0.000       \n",
            "   [1]  watching            0.458        0.000        0.000        0.000        -2.902       -0.443       -0.000       \n",
            "   [0]  t                   0.482        3.843        -5.367       -0.729       -3.259       -0.426       -2.832       \n",
            "   [1]  movie               0.436        0.000        0.000        0.000        -2.840       -0.417       -0.000       \n",
            "   [0]  some                0.376        13.770       -5.990       -0.979       -3.188       -0.398       -2.790       \n",
            "   [1]  the                 0.395        0.000        0.000        0.000        -2.481       -0.384       -0.000       \n",
            "   [1]  movie               0.387        0.000        0.000        0.000        -2.785       -0.387       -0.000       \n",
            "   [1]  follows             0.475        0.000        0.000        0.000        -3.127       -0.397       -0.000       \n",
            "   [0]  let                 0.538        2.724        -8.344       -0.620       -3.510       -0.399       -3.112       \n",
            "   [0]  monday              0.387        9.298        -8.694       -0.949       -3.245       -0.390       -2.855       \n",
            "   [0]  has                 0.432        4.789        -5.645       -0.840       -2.578       -0.368       -2.210       \n",
            "   [0]  very                0.473        2.739        -5.658       -0.749       -1.951       -0.355       -1.595       \n",
            "   [0]  this                0.502        11.515       -3.929       -0.690       -1.349       -0.356       -0.993       \n",
            "   [0]  in                  0.477        9.785        -4.154       -0.740       -0.740       -0.353       -0.387       \n",
            "   [1]  warrior             0.489        0.000        0.000        0.000        0.000        -0.351       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  non                 0.372        8.451        -7.854       -0.989       -5.657       -0.294       -5.000       \n",
            "   [0]  tim                 0.272        5.021        -8.442       -1.304       -5.240       -0.314       -4.926       \n",
            "   [1]  rachel              0.337        0.000        0.000        0.000        -4.420       -0.347       -0.000       \n",
            "   [0]  by                  0.370        14.692       -5.342       -0.993       -4.962       -0.401       -4.561       \n",
            "   [1]  are                 0.434        0.000        0.000        0.000        -4.456       -0.433       -0.000       \n",
            "   [1]  among               0.311        0.000        0.000        0.000        -5.002       -0.448       -0.000       \n",
            "   [1]  the                 0.332        0.000        0.000        0.000        -5.616       -0.425       -0.000       \n",
            "   [0]  very                0.370        12.217       -5.434       -0.994       -6.305       -0.426       -5.000       \n",
            "   [0]  lee                 0.315        8.822        -8.230       -1.155       -5.963       -0.426       -5.000       \n",
            "   [0]  i                   0.320        3.174        -2.927       -1.141       -5.398       -0.411       -4.987       \n",
            "   [1]  have                0.349        0.000        0.000        0.000        -4.780       -0.412       -0.000       \n",
            "   [0]  dutched             0.308        6.218        -14.758      -1.179       -5.366       -0.418       -4.948       \n",
            "   [0]  a                   0.309        5.420        -3.107       -1.174       -4.701       -0.413       -4.288       \n",
            "   [0]  the                 0.324        9.642        -2.738       -1.127       -3.960       -0.414       -3.547       \n",
            "   [0]  why                 0.383        7.438        -6.795       -0.959       -3.181       -0.415       -2.766       \n",
            "   [1]  live                0.257        0.000        0.000        0.000        -2.495       -0.419       -0.000       \n",
            "   [0]  that                0.328        3.331        -4.010       -1.113       -2.801       -0.395       -2.406       \n",
            "   [0]  autopsies           0.332        4.742        -14.522      -1.102       -1.895       -0.406       -1.489       \n",
            "   [0]  of                  0.410        14.804       -3.226       -0.890       -0.890       -0.410       -0.480       \n",
            "   [1]  that                0.435        0.000        0.000        0.000        0.000        -0.412       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  agree               0.545        2.915        -8.801       -0.608       -1.788       -0.299       -1.489       \n",
            "   [1]  was                 0.530        0.000        0.000        0.000        -1.325       -0.343       -0.000       \n",
            "   [1]  such                0.550        0.000        0.000        0.000        -1.488       -0.355       -0.000       \n",
            "   [0]  to                  0.570        2.799        -3.628       -0.562       -1.670       -0.375       -1.295       \n",
            "   [1]  blast               0.640        0.000        0.000        0.000        -1.244       -0.382       -0.000       \n",
            "   [1]  it                  0.537        0.000        0.000        0.000        -1.397       -0.414       -0.000       \n",
            "   [1]  has                 0.564        0.000        0.000        0.000        -1.568       -0.357       -0.000       \n",
            "   [1]  that                0.603        0.000        0.000        0.000        -1.761       -0.356       -0.000       \n",
            "   [1]  feel                0.646        0.000        0.000        0.000        -1.977       -0.365       -0.000       \n",
            "   [0]  is                  0.624        5.510        -3.877       -0.472       -2.219       -0.388       -1.832       \n",
            "   [0]  but                 0.684        8.089        -4.986       -0.380       -1.962       -0.372       -1.589       \n",
            "   [0]  dead                0.589        9.274        -8.043       -0.530       -1.776       -0.394       -1.382       \n",
            "   [0]  that                0.634        4.168        -3.815       -0.455       -1.399       -0.338       -1.061       \n",
            "   [1]  your                0.663        0.000        0.000        0.000        -1.060       -0.343       -0.000       \n",
            "   [0]  sucks               0.594        8.853        -8.585       -0.521       -1.190       -0.369       -0.821       \n",
            "   [1]  attitude            0.686        0.000        0.000        0.000        -0.751       -0.341       -0.000       \n",
            "   [0]  movie               0.638        3.836        -4.090       -0.449       -0.843       -0.396       -0.446       \n",
            "   [0]  against             0.643        8.664        -9.008       -0.441       -0.441       -0.380       -0.062       \n",
            "   [1]  me                  0.621        0.000        0.000        0.000        0.000        -0.378       0.000        \n",
            "   [1]  to                  0.647        0.000        0.000        0.000        0.000        -0.360       0.000        \n",
            "Samples\n",
            "Sample 0 .  falls voice u do not recommend watching t movie some the movie follows let monday has very this in warrior\n",
            "Sample 1 .  non tim rachel by are among the very lee i have dutched a the why live that autopsies of that\n",
            "Sample 2 .  agree was such to blast it has that feel is but dead that your sucks attitude movie against me to\n",
            "\n",
            "\n",
            "targets[[51 9 215 10 17 31 2 2166 9 3580 42 87 3091 7 13 11 15830 27086 2663 8][24192 1 888 27 3428 137 11 5 53 1247 131 483 2 734 201 15 3989 7745 1 21208][291 60 563 3040 5 634 43 275 183 362...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[58 51 9 215 10 17 31 2 2166 9...]...][[0 1 0 0 0 1 0 0 0 0...]...][[58 69849 9 69849 69849 69849 31 69849 69849 69849...]...][[51 9 215 10 17 31 2 2166 9 3580...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[58 51 9 215 10 17 31 2 2166 9...]...][[0 1 0 0 0 1 0 0 0 0...]...][[58 69849 9 69849 69849 69849 31 69849 69849 69849...]...][[38 9 60 15049 169 31 43 3 4 12...]...]\n",
            "targets[[5 51 8309 269 55 36 0 3614 6 6 9 118 23 512 73 36 8309 10 501 1606][3778 7250 17994 732 8 2 209 3642 40 1966 6 6 2 323 2573 7314 34 45 115 116][19 5 1009 4 0 220 3 107 11612 1058...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2994 5 51 8309 269 55 36 0 3614 6...]...][[1 0 0 1 1 0 0 0 0 1...]...][[2994 5 69849 69849 269 55 69849 69849 69849 69849...]...][[5 51 8309 269 55 36 0 3614 6 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2994 5 51 8309 269 55 36 0 3614 6...]...][[1 0 0 1 1 0 0 0 0 1...]...][[2994 5 69849 69849 269 55 69849 69849 69849 69849...]...][[5 1338 205 269 55 307 882 9 318 6...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 752 37 36433 117 166 0 1309 0 23...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 5 1640 36433 117 166 4 1309 0 4853...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 29742 43938 36433 117 166 6648 1309 0 1973...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "global_step: 384\n",
            " perplexity: 903.853\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            " percent of 3-grams captured: 0.165.\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            " percent of 2-grams captured: 0.597.\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            " percent of 4-grams captured: 0.030.\n",
            " geometric_avg: 0.143.\n",
            " arithmetic_avg: 0.264.\n",
            "global_step: 384\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66407\n",
            " G train loss: 160.13162\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 5 1640 36433 117 166 4 1309 0 4853...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 76 1712 36433 117 166 33 1309 0 118...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  what                0.624        0.000        0.000        0.000        -2.800       -0.293       -0.000       \n",
            "   [0]  get                 0.575        3.266        -6.440       -0.554       -3.143       -0.367       -2.776       \n",
            "   [0]  asked               0.459        8.451        -8.780       -0.778       -2.907       -0.360       -2.547       \n",
            "   [1]  sandlers            0.484        0.000        0.000        0.000        -2.390       -0.304       -0.000       \n",
            "   [1]  best                0.550        0.000        0.000        0.000        -2.683       -0.293       -0.000       \n",
            "   [1]  work                0.546        0.000        0.000        0.000        -3.013       -0.325       -0.000       \n",
            "   [0]  by                  0.523        4.163        -5.753       -0.649       -3.382       -0.330       -3.052       \n",
            "   [1]  date                0.369        0.000        0.000        0.000        -3.069       -0.325       -0.000       \n",
            "   [1]  the                 0.410        0.000        0.000        0.000        -3.445       -0.282       -0.000       \n",
            "   [0]  did                 0.442        13.715       -7.314       -0.817       -3.868       -0.304       -3.564       \n",
            "   [0]  mirjana             0.409        12.406       -13.473      -0.893       -3.425       -0.336       -3.089       \n",
            "   [0]  many                0.384        8.629        -6.391       -0.957       -2.842       -0.348       -2.494       \n",
            "   [1]  happy               0.144        0.000        0.000        0.000        -2.117       -0.354       -0.000       \n",
            "   [1]  gilmore             0.253        0.000        0.000        0.000        -2.377       -0.320       -0.000       \n",
            "   [0]  a                   0.267        6.981        -3.689       -1.321       -2.668       -0.385       -2.283       \n",
            "   [1]  join                0.305        0.000        0.000        0.000        -1.513       -0.468       -0.000       \n",
            "   [0]  petite              0.341        3.087        -12.087      -1.076       -1.698       -0.522       -1.177       \n",
            "   [1]  professional        0.399        0.000        0.000        0.000        -0.699       -0.553       -0.000       \n",
            "   [1]  golfers             0.369        0.000        0.000        0.000        -0.785       -0.567       -0.000       \n",
            "   [0]  sexy                0.414        8.917        -9.618       -0.881       -0.881       -0.547       -0.334       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  is                  0.497        0.000        0.000        0.000        -3.632       -0.300       -0.000       \n",
            "   [0]  did                 0.496        6.186        -7.041       -0.702       -4.078       -0.336       -3.742       \n",
            "   [0]  well                0.462        7.889        -7.118       -0.771       -3.791       -0.351       -3.440       \n",
            "   [0]  movie               0.436        2.285        -2.881       -0.831       -3.390       -0.341       -3.049       \n",
            "   [0]  br                  0.526        8.030        -5.349       -0.643       -2.873       -0.339       -2.534       \n",
            "   [1]  surprise            0.484        0.000        0.000        0.000        -2.503       -0.368       -0.000       \n",
            "   [0]  the                 0.475        4.173        -2.270       -0.744       -2.811       -0.358       -2.453       \n",
            "   [1]  german              0.457        0.000        0.000        0.000        -2.320       -0.345       -0.000       \n",
            "   [1]  postwar             0.553        0.000        0.000        0.000        -2.605       -0.335       -0.000       \n",
            "   [0]  saw                 0.567        4.373        -5.791       -0.567       -2.924       -0.358       -2.566       \n",
            "   [0]  film                0.562        7.773        -4.384       -0.575       -2.646       -0.366       -2.281       \n",
            "   [0]  but                 0.639        7.717        -4.803       -0.449       -2.325       -0.352       -1.973       \n",
            "   [0]  this                0.629        5.756        -3.910       -0.463       -2.107       -0.363       -1.743       \n",
            "   [0]  the                 0.603        2.382        -2.382       -0.506       -1.845       -0.351       -1.494       \n",
            "   [0]  movie               0.560        14.303       -3.677       -0.579       -1.503       -0.325       -1.178       \n",
            "   [1]  mr                  0.523        0.000        0.000        0.000        -1.037       -0.304       -0.000       \n",
            "   [0]  set                 0.554        9.751        -7.129       -0.591       -1.165       -0.285       -0.880       \n",
            "   [0]  i                   0.525        15.741       -3.130       -0.644       -0.644       -0.302       -0.342       \n",
            "   [1]  who                 0.530        0.000        0.000        0.000        0.000        -0.312       0.000        \n",
            "   [1]  usually             0.408        0.000        0.000        0.000        0.000        -0.328       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  mentally            0.347        3.290        -9.635       -1.057       -3.864       -0.286       -3.578       \n",
            "   [0]  was                 0.386        5.163        -4.031       -0.953       -3.151       -0.283       -2.869       \n",
            "   [1]  jess                0.467        0.000        0.000        0.000        -2.468       -0.311       -0.000       \n",
            "   [1]  franco              0.538        0.000        0.000        0.000        -2.771       -0.356       -0.000       \n",
            "   [0]  went                0.505        4.679        -7.509       -0.683       -3.111       -0.383       -2.728       \n",
            "   [1]  best                0.549        0.000        0.000        0.000        -2.726       -0.366       -0.000       \n",
            "   [0]  it                  0.449        4.770        -3.460       -0.800       -3.061       -0.355       -2.706       \n",
            "   [0]  probably            0.408        5.587        -7.319       -0.897       -2.537       -0.311       -2.227       \n",
            "   [1]  it                  0.356        0.000        0.000        0.000        -1.842       -0.290       -0.000       \n",
            "   [1]  s                   0.393        0.000        0.000        0.000        -2.068       -0.285       -0.000       \n",
            "   [1]  certainly           0.360        0.000        0.000        0.000        -2.322       -0.319       -0.000       \n",
            "   [1]  not                 0.439        0.000        0.000        0.000        -2.606       -0.341       -0.000       \n",
            "   [1]  the                 0.421        0.000        0.000        0.000        -2.926       -0.375       -0.000       \n",
            "   [0]  me                  0.389        6.651        -6.013       -0.943       -3.285       -0.380       -2.905       \n",
            "   [0]  the                 0.387        8.285        -3.034       -0.951       -2.629       -0.372       -2.257       \n",
            "   [0]  short               0.439        6.214        -8.708       -0.823       -1.884       -0.366       -1.518       \n",
            "   [1]  doesn               0.309        0.000        0.000        0.000        -1.192       -0.376       -0.000       \n",
            "   [1]  t                   0.380        0.000        0.000        0.000        -1.338       -0.350       -0.000       \n",
            "   [0]  seeing              0.482        6.402        -7.076       -0.729       -1.502       -0.367       -1.136       \n",
            "   [0]  rock                0.420        6.607        -8.538       -0.868       -0.868       -0.399       -0.470       \n",
            "Samples\n",
            "Sample 0 .  what get asked sandlers best work by date the did mirjana many happy gilmore a join petite professional golfers sexy\n",
            "Sample 1 .  is did well movie br surprise the german postwar saw film but this the movie mr set i who usually\n",
            "Sample 2 .  mentally was jess franco went best it probably it s certainly not the me the short doesn t seeing rock\n",
            "\n",
            "\n",
            "targets[[7983 5 2 153 622 104 25 245 4 3919 24 12 92 275 3 60 30 57 519 104][17 5 60 519 29 9 27 307 7 52 72 163 214 9 1207 121 30 0 410 174][6724 2570 5262 16 3345 18 11750 3 12713 505...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4086 7983 5 2 153 622 104 25 245 4...]...][[0 0 1 0 0 0 0 1 1 0...]...][[4086 69849 69849 2 69849 69849 69849 69849 245 4...]...][[7983 5 2 153 622 104 25 245 4 3919...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4086 7983 5 2 153 622 104 25 245 4...]...][[0 0 1 0 0 0 0 1 1 0...]...][[4086 69849 69849 2 69849 69849 69849 69849 245 4...]...][[2 5 2 964 228 33 1425 245 4 2181...]...]\n",
            "targets[[125 9 470 4 38 10 17 7 45 2 81 177 1 7 5 2 385 1633 19 1][21 76 71 356 9 424 9142 1512 18 10 29 13 73 126 0 63 13 126 58 0][3 0 2983 5067 6198 45 7961 73 690 43...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 125 9 470 4 38 10 17 7 45...]...][[1 0 0 1 0 0 0 1 0 1...]...][[431 125 69849 69849 4 69849 69849 69849 7 69849...]...][[125 9 470 4 38 10 17 7 45 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 125 9 470 4 38 10 17 7 45...]...][[1 0 0 1 0 0 0 1 0 1...]...][[431 125 69849 69849 4 69849 69849 69849 7 69849...]...][[125 418 363 4 3 22 1250 7 811 2...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 0 14590 1753 4924 2042 704 1753 3...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 677 6798 1753 4924 4924 15 1753 1761...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 3830 913 1753 4924 815 7003 1753 1652...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "global_step: 389\n",
            " perplexity: 903.220\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            " percent of 3-grams captured: 0.149.\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            " percent of 2-grams captured: 0.592.\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            " percent of 4-grams captured: 0.023.\n",
            " geometric_avg: 0.126.\n",
            " arithmetic_avg: 0.254.\n",
            "global_step: 389\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66350\n",
            " G train loss: 161.36528\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 677 6798 1753 4924 4924 15 1753 1761...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 0 193 1753 4924 49 1731 1753 190...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  movie               0.435        0.000        0.000        0.000        -3.734       -0.292       -0.000       \n",
            "   [1]  is                  0.438        0.000        0.000        0.000        -4.192       -0.331       -0.000       \n",
            "   [0]  the                 0.420        9.776        -2.783       -0.867       -4.706       -0.355       -4.351       \n",
            "   [0]  both                0.334        15.012       -8.563       -1.098       -4.310       -0.374       -3.936       \n",
            "   [1]  guns                0.414        0.000        0.000        0.000        -3.607       -0.405       -0.000       \n",
            "   [1]  chicks              0.402        0.000        0.000        0.000        -4.049       -0.432       -0.000       \n",
            "   [0]  good                0.312        13.428       -5.400       -1.165       -4.546       -0.449       -4.097       \n",
            "   [0]  redeeming           0.330        4.631        -8.592       -1.110       -3.795       -0.473       -3.322       \n",
            "   [1]  guns                0.400        0.000        0.000        0.000        -3.015       -0.500       -0.000       \n",
            "   [0]  however             0.461        12.319       -8.127       -0.775       -3.385       -0.513       -2.871       \n",
            "   [0]  bad                 0.531        9.194        -6.517       -0.634       -2.930       -0.494       -2.436       \n",
            "   [0]  it                  0.409        10.744       -3.389       -0.893       -2.578       -0.438       -2.141       \n",
            "   [1]  getting             0.249        0.000        0.000        0.000        -1.892       -0.391       -0.000       \n",
            "   [0]  be                  0.372        14.440       -5.785       -0.989       -2.124       -0.409       -1.715       \n",
            "   [0]  isn                 0.443        5.495        -7.848       -0.814       -1.274       -0.446       -0.827       \n",
            "   [1]  black               0.380        0.000        0.000        0.000        -0.516       -0.465       -0.000       \n",
            "   [1]  chicks              0.374        0.000        0.000        0.000        -0.579       -0.464       -0.000       \n",
            "   [1]  who                 0.377        0.000        0.000        0.000        -0.651       -0.464       -0.000       \n",
            "   [0]  but                 0.482        13.468       -5.326       -0.730       -0.730       -0.469       -0.261       \n",
            "   [1]  guns                0.523        0.000        0.000        0.000        0.000        -0.454       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  or                  0.530        3.405        -6.136       -0.634       -6.884       -0.296       -5.000       \n",
            "   [1]  was                 0.487        0.000        0.000        0.000        -7.017       -0.303       -0.000       \n",
            "   [0]  pleasant            0.266        7.233        -8.645       -1.324       -7.878       -0.324       -5.000       \n",
            "   [0]  than                0.302        7.888        -5.870       -1.197       -7.357       -0.442       -5.000       \n",
            "   [0]  s                   0.324        3.530        -4.760       -1.127       -6.916       -0.490       -5.000       \n",
            "   [0]  their               0.400        5.577        -7.164       -0.917       -6.499       -0.543       -5.000       \n",
            "   [0]  a                   0.356        6.739        -3.598       -1.033       -6.267       -0.536       -5.000       \n",
            "   [1]  some                0.295        0.000        0.000        0.000        -5.876       -0.560       -0.000       \n",
            "   [0]  brought             0.143        8.707        -8.213       -1.945       -6.597       -0.596       -5.000       \n",
            "   [0]  hilarious           0.181        6.044        -8.317       -1.709       -5.223       -0.760       -4.463       \n",
            "   [1]  before              0.243        0.000        0.000        0.000        -3.946       -0.807       -0.000       \n",
            "   [1]  the                 0.229        0.000        0.000        0.000        -4.430       -0.824       -0.000       \n",
            "   [0]  am                  0.273        13.511       -7.309       -1.299       -4.974       -0.857       -4.116       \n",
            "   [1]  is                  0.272        0.000        0.000        0.000        -4.125       -0.834       -0.000       \n",
            "   [0]  death               0.296        3.277        -8.487       -1.216       -4.631       -0.831       -3.800       \n",
            "   [0]  into                0.341        5.887        -6.315       -1.075       -3.835       -0.806       -3.029       \n",
            "   [0]  in                  0.317        9.676        -4.388       -1.150       -3.098       -0.768       -2.330       \n",
            "   [0]  antoinette          0.317        7.032        -13.403      -1.149       -2.187       -0.774       -1.413       \n",
            "   [0]  and                 0.312        6.196        -3.565       -1.166       -1.166       -0.779       -0.387       \n",
            "   [1]  though              0.230        0.000        0.000        0.000        0.000        -0.801       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  guy                 0.380        8.232        -7.419       -0.968       -5.386       -0.297       -5.000       \n",
            "   [1]  direct              0.241        0.000        0.000        0.000        -4.959       -0.338       -0.000       \n",
            "   [0]  their               0.349        3.982        -7.820       -1.052       -5.568       -0.378       -5.000       \n",
            "   [0]  if                  0.377        8.019        -6.230       -0.975       -5.070       -0.396       -4.674       \n",
            "   [0]  as                  0.392        8.048        -4.841       -0.937       -4.597       -0.416       -4.181       \n",
            "   [1]  movie               0.343        0.000        0.000        0.000        -4.110       -0.419       -0.000       \n",
            "   [0]  based               0.423        8.076        -7.680       -0.860       -4.614       -0.424       -4.190       \n",
            "   [1]  the                 0.392        0.000        0.000        0.000        -4.214       -0.411       -0.000       \n",
            "   [0]  direction           0.324        7.429        -9.103       -1.127       -4.731       -0.405       -4.326       \n",
            "   [0]  did                 0.354        4.341        -6.677       -1.039       -4.047       -0.400       -3.647       \n",
            "   [0]  s                   0.346        7.569        -4.545       -1.061       -3.376       -0.393       -2.983       \n",
            "   [1]  good                0.270        0.000        0.000        0.000        -2.599       -0.402       -0.000       \n",
            "   [1]  there               0.244        0.000        0.000        0.000        -2.918       -0.420       -0.000       \n",
            "   [1]  were                0.343        0.000        0.000        0.000        -3.276       -0.435       -0.000       \n",
            "   [1]  a                   0.298        0.000        0.000        0.000        -3.678       -0.442       -0.000       \n",
            "   [0]  1970s               0.205        6.902        -8.615       -1.586       -4.129       -0.458       -3.672       \n",
            "   [0]  movie               0.205        3.492        -4.108       -1.586       -2.855       -0.479       -2.376       \n",
            "   [1]  first               0.313        0.000        0.000        0.000        -1.425       -0.503       -0.000       \n",
            "   [1]  rate                0.227        0.000        0.000        0.000        -1.599       -0.512       -0.000       \n",
            "   [0]  it                  0.166        7.465        -3.319       -1.796       -1.796       -0.532       -1.263       \n",
            "Samples\n",
            "Sample 0 .  movie is the both guns chicks good redeeming guns however bad it getting be isn black chicks who but guns\n",
            "Sample 1 .  or was pleasant than s their a some brought hilarious before the am is death into in antoinette and though\n",
            "Sample 2 .  guy direct their if as movie based the direction did s good there were a 1970s movie first rate it\n",
            "\n",
            "\n",
            "targets[[656 898 67 2 2190 3 163 678 10 13 2 394 432 3 443 437 56 4096 56 2615][12 161 160 477 12931 48 3 0 126 2601 236 28 98 38 0 960 12001 3 7946 908][5 29 3 60 519 98 295 8 7 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 656 898 67 2 2190 3 163 678 10...]...][[1 0 0 1 0 1 0 0 0 0...]...][[9 656 69849 69849 2 69849 3 69849 69849 69849...]...][[656 898 67 2 2190 3 163 678 10 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 656 898 67 2 2190 3 163 678 10...]...][[1 0 0 1 0 1 0 0 0 0...]...][[9 656 69849 69849 2 69849 3 69849 69849 69849...]...][[656 65 354 2 176 3 11 125 23 7...]...]\n",
            "targets[[2185 22 0 17 796 6 6 7 13 23 49 23 49 31 30 2240 7 13 81 9][42 389 143 36 2 1689 768 816 3 10 322 832 827 19 22802 7 12 393 5 23632][343 1 868 201 9137 0 868 5 7543 33...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[60 2185 22 0 17 796 6 6 7 13...]...][[1 1 1 1 1 1 0 1 1 0...]...][[60 2185 22 0 17 796 6 69849 7 13...]...][[2185 22 0 17 796 6 6 7 13 23...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[60 2185 22 0 17 796 6 6 7 13...]...][[1 1 1 1 1 1 0 1 1 0...]...][[60 2185 22 0 17 796 6 69849 7 13...]...][[2185 22 0 17 796 6 1693 7 13 21...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 177 738 255 884 9 5 246 2 22...]...]\n",
            "I0310 00:36:12.858728 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "I0310 00:36:12.862085 139852867667712 supervisor.py:1099] global_step/sec: 0.375074\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 127 738 255 884 127 738 34 25 23...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 3 738 255 884 15 8 35 250 3...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "global_step: 394\n",
            " perplexity: 908.134\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            " percent of 3-grams captured: 0.180.\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            " percent of 2-grams captured: 0.603.\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            " percent of 4-grams captured: 0.030.\n",
            " geometric_avg: 0.148.\n",
            " arithmetic_avg: 0.271.\n",
            "global_step: 394\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66263\n",
            " G train loss: 162.69452\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 127 738 255 884 127 738 34 25 23...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 144 738 255 884 60 29 0 11 196...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  enjoyed             0.528        0.000        0.000        0.000        -1.711       -0.301       -0.000       \n",
            "   [0]  through             0.612        7.921        -8.410       -0.491       -1.921       -0.344       -1.576       \n",
            "   [1]  review              0.577        0.000        0.000        0.000        -1.605       -0.363       -0.000       \n",
            "   [1]  anyone              0.584        0.000        0.000        0.000        -1.801       -0.329       -0.000       \n",
            "   [1]  reading             0.548        0.000        0.000        0.000        -2.022       -0.292       -0.000       \n",
            "   [0]  my                  0.681        7.617        -5.471       -0.384       -2.271       -0.262       -2.009       \n",
            "   [0]  one                 0.643        8.394        -5.184       -0.442       -2.118       -0.269       -1.849       \n",
            "   [0]  the                 0.623        5.535        -2.173       -0.474       -1.882       -0.236       -1.646       \n",
            "   [0]  that                0.643        5.313        -4.090       -0.441       -1.581       -0.207       -1.374       \n",
            "   [0]  thought             0.598        5.562        -6.634       -0.514       -1.280       -0.191       -1.089       \n",
            "   [1]  consumers           0.715        0.000        0.000        0.000        -0.860       -0.168       -0.000       \n",
            "   [0]  that                0.701        3.298        -4.094       -0.355       -0.966       -0.193       -0.772       \n",
            "   [1]  metal               0.664        0.000        0.000        0.000        -0.686       -0.178       -0.000       \n",
            "   [1]  will                0.735        0.000        0.000        0.000        -0.770       -0.148       -0.000       \n",
            "   [1]  learn               0.491        0.000        0.000        0.000        -0.864       -0.154       -0.000       \n",
            "   [1]  that                0.580        0.000        0.000        0.000        -0.970       -0.100       -0.000       \n",
            "   [0]  is                  0.588        4.025        -3.591       -0.531       -1.089       -0.136       -0.953       \n",
            "   [1]  topic               0.451        0.000        0.000        0.000        -0.626       -0.189       -0.000       \n",
            "   [1]  could               0.529        0.000        0.000        0.000        -0.703       -0.210       -0.000       \n",
            "   [0]  movie               0.454        8.705        -4.016       -0.790       -0.790       -0.270       -0.520       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  saw                 0.516        0.000        0.000        0.000        -3.165       -0.295       -0.000       \n",
            "   [1]  a                   0.440        0.000        0.000        0.000        -3.553       -0.333       -0.000       \n",
            "   [0]  simply              0.274        8.151        -7.711       -1.294       -3.989       -0.318       -3.671       \n",
            "   [1]  of                  0.347        0.000        0.000        0.000        -3.026       -0.314       -0.000       \n",
            "   [1]  this                0.346        0.000        0.000        0.000        -3.397       -0.375       -0.000       \n",
            "   [1]  last                0.189        0.000        0.000        0.000        -3.814       -0.416       -0.000       \n",
            "   [1]  year                0.374        0.000        0.000        0.000        -4.282       -0.434       -0.000       \n",
            "   [0]  thriller            0.311        3.251        -7.727       -1.169       -4.807       -0.509       -4.298       \n",
            "   [1]  all                 0.343        0.000        0.000        0.000        -4.085       -0.498       -0.000       \n",
            "   [1]  i                   0.273        0.000        0.000        0.000        -4.586       -0.490       -0.000       \n",
            "   [1]  can                 0.242        0.000        0.000        0.000        -5.148       -0.463       -0.000       \n",
            "   [0]  of                  0.281        7.144        -3.719       -1.270       -5.780       -0.457       -5.000       \n",
            "   [1]  is                  0.262        0.000        0.000        0.000        -5.063       -0.471       -0.000       \n",
            "   [0]  women               0.174        4.250        -7.587       -1.749       -5.684       -0.474       -5.000       \n",
            "   [0]  has                 0.219        5.019        -5.811       -1.518       -4.417       -0.476       -3.942       \n",
            "   [1]  br                  0.276        0.000        0.000        0.000        -3.255       -0.515       -0.000       \n",
            "   [0]  tv                  0.178        3.546        -6.925       -1.723       -3.655       -0.537       -3.118       \n",
            "   [0]  old                 0.114        8.410        -6.883       -2.168       -2.168       -0.514       -1.654       \n",
            "   [1]  they                0.124        0.000        0.000        0.000        0.000        -0.525       0.000        \n",
            "   [1]  can                 0.106        0.000        0.000        0.000        0.000        -0.587       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  film                0.500        8.273        -3.897       -0.693       -2.852       -0.296       -2.557       \n",
            "   [1]  this                0.492        0.000        0.000        0.000        -2.424       -0.324       -0.000       \n",
            "   [0]  this                0.487        2.932        -3.298       -0.719       -2.722       -0.334       -2.388       \n",
            "   [0]  is                  0.491        3.032        -3.018       -0.712       -2.249       -0.337       -1.912       \n",
            "   [0]  just                0.498        7.898        -5.779       -0.698       -1.725       -0.338       -1.388       \n",
            "   [1]  for                 0.587        0.000        0.000        0.000        -1.153       -0.332       -0.000       \n",
            "   [1]  its                 0.485        0.000        0.000        0.000        -1.295       -0.310       -0.000       \n",
            "   [1]  sheer               0.700        0.000        0.000        0.000        -1.454       -0.276       -0.000       \n",
            "   [0]  and                 0.626        12.219       -2.971       -0.468       -1.632       -0.247       -1.385       \n",
            "   [0]  rox                 0.600        3.348        -14.733      -0.510       -1.307       -0.175       -1.131       \n",
            "   [1]  must                0.590        0.000        0.000        0.000        -0.894       -0.122       -0.000       \n",
            "   [0]  of                  0.624        12.841       -3.691       -0.471       -1.004       -0.098       -0.906       \n",
            "   [1]  my                  0.700        0.000        0.000        0.000        -0.598       -0.090       -0.000       \n",
            "   [0]  before              0.707        7.751        -7.630       -0.347       -0.671       -0.076       -0.596       \n",
            "   [1]  by                  0.669        0.000        0.000        0.000        -0.364       -0.040       -0.000       \n",
            "   [1]  way                 0.741        0.000        0.000        0.000        -0.409       0.002        -0.000       \n",
            "   [0]  on                  0.769        3.699        -5.346       -0.263       -0.459       0.031        -0.490       \n",
            "   [0]  through             0.802        13.008       -7.639       -0.220       -0.220       0.070        -0.291       \n",
            "   [1]  that                0.787        0.000        0.000        0.000        0.000        0.125        -0.000       \n",
            "   [1]  i                   0.742        0.000        0.000        0.000        0.000        0.191        -0.000       \n",
            "Samples\n",
            "Sample 0 .  enjoyed through review anyone reading my one the that thought consumers that metal will learn that is topic could movie\n",
            "Sample 1 .  saw a simply of this last year thriller all i can of is women has br tv old they can\n",
            "Sample 2 .  film this this is just for its sheer and rox must of my before by way on through that i\n",
            "\n",
            "\n",
            "targets[[11663 13 335 22 351 3 26 458 51 24 326 10 19 7 5 35 6548 5358 19 368][2 57 51 0 176 5 37 2890 22 0 1849 8044 8 3854 0 151 11 5 88 731][307 10 3529 22 6 6 9 140 23 251...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1644 11663 13 335 22 351 3 26 458 51...]...][[0 0 0 0 1 0 1 1 1 0...]...][[1644 69849 69849 69849 69849 351 69849 26 458 51...]...][[11663 13 335 22 351 3 26 458 51 24...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1644 11663 13 335 22 351 3 26 458 51...]...][[0 0 0 0 1 0 1 1 1 0...]...][[1644 69849 69849 69849 69849 351 69849 26 458 51...]...][[2 17 381 14 351 586 26 458 51 109...]...]\n",
            "targets[[318 10 609 9 42 96 23 1017 545 36 13049 60 333 420 21 1643 4 1775 1 9696][111 80 9 875 10 17 1168 60 114 0 109 13 11000 601 20260 45 44 226 313 2537][5 23 29 3 146 104 7 5 29 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 318 10 609 9 42 96 23 1017 545...]...][[1 1 1 1 0 1 1 0 0 1...]...][[99 318 10 609 9 69849 96 23 69849 69849...]...][[318 10 609 9 42 96 23 1017 545 36...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 318 10 609 9 42 96 23 1017 545...]...][[1 1 1 1 0 1 1 0 0 1...]...][[99 318 10 609 9 69849 96 23 69849 69849...]...][[318 10 609 9 34 96 23 123 46 36...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 1057 368 755 654 78 2 376 3 51935...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 12 368 755 654 78 2 368 1115 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 12 368 755 654 78 2 125 9 27...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "global_step: 399\n",
            " perplexity: 916.458\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            " percent of 3-grams captured: 0.158.\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            " percent of 2-grams captured: 0.597.\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            " percent of 4-grams captured: 0.014.\n",
            " geometric_avg: 0.110.\n",
            " arithmetic_avg: 0.256.\n",
            "global_step: 399\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66241\n",
            " G train loss: 161.55048\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 12 368 755 654 78 2 368 1115 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 242 368 755 654 78 2 176 10 17...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  hopes               0.425        0.000        0.000        0.000        -3.134       -0.293       -0.000       \n",
            "   [0]  am                  0.533        4.296        -6.309       -0.629       -3.518       -0.300       -3.218       \n",
            "   [1]  classic             0.568        0.000        0.000        0.000        -3.245       -0.357       -0.000       \n",
            "   [1]  tale                0.441        0.000        0.000        0.000        -3.643       -0.377       -0.000       \n",
            "   [1]  turned              0.371        0.000        0.000        0.000        -4.089       -0.329       -0.000       \n",
            "   [1]  into                0.486        0.000        0.000        0.000        -4.591       -0.294       -0.000       \n",
            "   [1]  a                   0.426        0.000        0.000        0.000        -5.155       -0.340       -0.000       \n",
            "   [0]  world               0.435        7.926        -7.086       -0.833       -5.787       -0.334       -5.000       \n",
            "   [0]  this                0.443        9.960        -3.815       -0.814       -5.561       -0.339       -5.000       \n",
            "   [0]  movie               0.386        4.507        -4.387       -0.952       -5.330       -0.340       -4.990       \n",
            "   [1]  with                0.483        0.000        0.000        0.000        -4.915       -0.328       -0.000       \n",
            "   [0]  in                  0.449        14.234       -3.865       -0.802       -5.518       -0.362       -5.000       \n",
            "   [0]  his                 0.441        13.912       -5.949       -0.818       -5.295       -0.346       -4.949       \n",
            "   [0]  i                   0.394        5.029        -3.262       -0.931       -5.026       -0.333       -4.693       \n",
            "   [0]  anyway              0.196        3.102        -8.757       -1.629       -4.598       -0.315       -4.283       \n",
            "   [1]  king                0.379        0.000        0.000        0.000        -3.333       -0.254       -0.000       \n",
            "   [1]  and                 0.324        0.000        0.000        0.000        -3.742       -0.336       -0.000       \n",
            "   [0]  country             0.184        5.955        -7.977       -1.691       -4.202       -0.346       -3.856       \n",
            "   [0]  there               0.185        12.514       -6.266       -1.690       -2.819       -0.305       -2.514       \n",
            "   [0]  always              0.282        11.992       -7.852       -1.267       -1.267       -0.320       -0.947       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  i                   0.494        2.990        -4.329       -0.706       -3.451       -0.294       -3.157       \n",
            "   [0]  go                  0.535        5.118        -7.159       -0.625       -3.081       -0.329       -2.753       \n",
            "   [0]  before              0.592        13.102       -7.746       -0.524       -2.758       -0.344       -2.414       \n",
            "   [1]  of                  0.585        0.000        0.000        0.000        -2.508       -0.347       -0.000       \n",
            "   [0]  like                0.542        5.190        -5.690       -0.612       -2.816       -0.332       -2.484       \n",
            "   [0]  suspense            0.372        8.346        -7.840       -0.990       -2.474       -0.309       -2.165       \n",
            "   [0]  few                 0.487        7.881        -6.577       -0.720       -1.666       -0.301       -1.365       \n",
            "   [1]  one                 0.486        0.000        0.000        0.000        -1.063       -0.318       -0.000       \n",
            "   [1]  way                 0.590        0.000        0.000        0.000        -1.193       -0.341       -0.000       \n",
            "   [1]  passage             0.628        0.000        0.000        0.000        -1.339       -0.350       -0.000       \n",
            "   [1]  is                  0.577        0.000        0.000        0.000        -1.504       -0.341       -0.000       \n",
            "   [1]  likely              0.689        0.000        0.000        0.000        -1.688       -0.314       -0.000       \n",
            "   [1]  the                 0.642        0.000        0.000        0.000        -1.895       -0.294       -0.000       \n",
            "   [0]  if                  0.653        6.355        -6.668       -0.426       -2.128       -0.260       -1.867       \n",
            "   [1]  underrated          0.400        0.000        0.000        0.000        -1.911       -0.225       -0.000       \n",
            "   [0]  antidote            0.448        8.083        -12.934      -0.803       -2.145       -0.206       -1.939       \n",
            "   [0]  the                 0.477        8.322        -3.588       -0.741       -1.506       -0.235       -1.271       \n",
            "   [0]  year                0.628        4.305        -7.570       -0.466       -0.859       -0.286       -0.573       \n",
            "   [0]  are                 0.643        6.033        -5.755       -0.441       -0.441       -0.326       -0.115       \n",
            "   [1]  time                0.639        0.000        0.000        0.000        0.000        -0.322       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  s                   0.492        0.000        0.000        0.000        -2.738       -0.292       -0.000       \n",
            "   [0]  same                0.413        7.276        -8.217       -0.884       -3.073       -0.317       -2.757       \n",
            "   [1]  bad                 0.551        0.000        0.000        0.000        -2.458       -0.311       -0.000       \n",
            "   [0]  that                0.543        3.767        -4.739       -0.611       -2.759       -0.351       -2.408       \n",
            "   [1]  film                0.558        0.000        0.000        0.000        -2.412       -0.352       -0.000       \n",
            "   [0]  not                 0.591        7.319        -5.431       -0.526       -2.708       -0.349       -2.359       \n",
            "   [0]  o                   0.465        5.419        -8.041       -0.766       -2.449       -0.350       -2.100       \n",
            "   [0]  soul                0.532        13.057       -9.048       -0.631       -1.890       -0.307       -1.583       \n",
            "   [0]  seem                0.554        3.669        -8.211       -0.591       -1.414       -0.314       -1.100       \n",
            "   [0]  solar               0.573        12.876       -12.074      -0.556       -0.925       -0.329       -0.596       \n",
            "   [1]  time                0.620        0.000        0.000        0.000        -0.413       -0.337       -0.000       \n",
            "   [1]  the                 0.594        0.000        0.000        0.000        -0.464       -0.343       -0.000       \n",
            "   [1]  idea                0.694        0.000        0.000        0.000        -0.521       -0.334       -0.000       \n",
            "   [0]  my                  0.758        7.648        -6.184       -0.277       -0.585       -0.355       -0.230       \n",
            "   [0]  really              0.822        5.625        -7.117       -0.195       -0.346       -0.374       0.028        \n",
            "   [1]  been                0.780        0.000        0.000        0.000        -0.169       -0.402       0.000        \n",
            "   [1]  good                0.711        0.000        0.000        0.000        -0.190       -0.350       0.000        \n",
            "   [1]  but                 0.805        0.000        0.000        0.000        -0.213       -0.281       0.000        \n",
            "   [0]  in                  0.787        4.573        -4.463       -0.240       -0.240       -0.301       0.061        \n",
            "   [1]  wasn                0.687        0.000        0.000        0.000        0.000        -0.290       0.000        \n",
            "Samples\n",
            "Sample 0 .  hopes am classic tale turned into a world this movie with in his i anyway king and country there always\n",
            "Sample 1 .  i go before of like suspense few one way passage is likely the if underrated antidote the year are time\n",
            "Sample 2 .  s same bad that film not o soul seem solar time the idea my really been good but in wasn\n",
            "\n",
            "\n",
            "targets[[8 23319 11477 30354 12 1054 1081 673 64862 4572 1437 13 35 2004 20904 2783 4989 0 2486 1][11 12 30 11 50 28 300 43 10 19 10 1525 21 58 28 1267 2 662 4 0][13 2 1049 10323 249 8 407 8600 608 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6159 8 23319 11477 30354 12 1054 1081 673 64862...]...][[0 0 1 0 1 0 0 1 1 1...]...][[6159 69849 69849 11477 69849 12 69849 69849 673 64862...]...][[8 23319 11477 30354 12 1054 1081 673 64862 4572...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6159 8 23319 11477 30354 12 1054 1081 673 64862...]...][[0 0 1 0 1 0 0 1 1 1...]...][[6159 69849 69849 11477 69849 12 69849 69849 673 64862...]...][[25446 17 11477 22 12 27 2494 673 64862 4572...]...]\n",
            "targets[[293 0 277 5198 3 10 19 25 8 316 1 460 5 85 1276 50 76 62 933 22][206 5140 699 246 5453 23406 36 2170 13 142 35 1373 369 11 7 465 1099 4 93 2][45 0 3509 10259 3 43801 1050 15 15164 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 293 0 277 5198 3 10 19 25 8...]...][[0 0 1 1 1 0 1 1 1 0...]...][[0 69849 69849 277 5198 3 69849 19 25 8...]...][[293 0 277 5198 3 10 19 25 8 316...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 293 0 277 5198 3 10 19 25 8...]...][[0 0 1 1 1 0 1 1 1 0...]...][[0 69849 69849 277 5198 3 69849 19 25 8...]...][[108 4 277 5198 3 2 19 25 8 140...]...]\n",
            "targets[[7 12 2069 0 3572 1 33 0 984 5 37 3056 1306 5 143 18 24 12 3056 9][2907 12 84 81 19 3687 6405 13 2 19 11 13 5536 4 28 3047 120 17790 1 12295][353 0 298 8 60 646 713 1 99 70...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[69 7 12 2069 0 3572 1 33 0 984...]...][[1 1 1 1 1 1 1 0 0 1...]...][[69 7 12 2069 0 3572 1 33 69849 69849...]...][[7 12 2069 0 3572 1 33 977 0 5...]...]\n",
            "Training raising FloatingPoinError.\n",
            "I0310 00:36:42.084112 139858729412480 coordinator.py:224] Error reported to Coordinator: <type 'exceptions.FloatingPointError'>, Training infinite perplexity: nan\n",
            "Traceback (most recent call last):\n",
            "  File \"train_mask_gan.py\", line 1161, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"train_mask_gan.py\", line 1143, in main\n",
            "    data_ngram_counts)\n",
            "  File \"train_mask_gan.py\", line 743, in train_model\n",
            "    'Training infinite perplexity: %.3f' % perplexity)\n",
            "FloatingPointError: Training infinite perplexity: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8hYuMZmmkcm",
        "colab_type": "text"
      },
      "source": [
        "## Generate samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw-SWqvTmpsi",
        "colab_type": "code",
        "outputId": "48d429b7-dffd-49d9-8b63-2bb1c7d4dd5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/yesweGAN/maskgan_colab\n",
        "!python generate_samples.py \\\n",
        "--data_dir='/content/yesweGAN/maskgan_colab/dataset/imdb' \\\n",
        "--data_set='imdb' \\\n",
        "--batch_size=128 \\\n",
        "--sequence_length=20 \\\n",
        "--base_directory='/content/yesweGAN/maskgan_colab/maskGAN' \\\n",
        "--hparams=\"gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,gen_vd_keep_prob=0.33971\" \\\n",
        "--generator_model='seq2seq_vd' \\\n",
        "--discriminator_model='seq2seq_vd' \\\n",
        "--is_present_rate=0.5 \\\n",
        "--maskgan_ckpt='/content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt-72' \\\n",
        "--seq2seq_share_embedding=true \\\n",
        "--dis_share_embedding=true \\\n",
        "--attention_option=luong \\\n",
        "--output_path='/content/yesweGAN/maskgan_colab' \\\n",
        "--baseline_method=critic \\\n",
        "--number_epochs=1"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yesweGAN/maskgan_colab\n",
            "Vocab size: 69849\n",
            "\n",
            "Optimizing Generator vars:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69849, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69849,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "\n",
            "Optimizing Discriminator vars:\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Optimizing Critic vars:\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "WARNING:tensorflow:From generate_samples.py:181: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2020-03-11 04:07:49.504312: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
            "2020-03-11 04:07:49.572497: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-11 04:07:49.573213: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \n",
            "name: Tesla K80 major: 3 minor: 7 memoryClockRate(GHz): 0.8235\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 11.17GiB freeMemory: 11.11GiB\n",
            "2020-03-11 04:07:49.573259: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n",
            "2020-03-11 04:07:49.858605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 10769 MB memory) -> physical GPU (device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7)\n",
            "Restoring Generator from /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt-72.\n",
            "Asserting Generator is a seq2seq-variant.\n",
            "Restoring Discriminator from /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt-72.\n",
            "Epoch number: 0\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            "targets[[7 12 2069 0 3572 1 33 0 984 5 37 3056 1306 5 143 18 24 12 3056 9][2907 12 84 81 19 3687 6405 13 2 19 11 13 5536 4 28 3047 120 17790 1 12295][353 0 298 8 60 646 713 1 99 70]...]\n",
            "targets[[50418 12 330 8991 8 0 328 45 4 28 29 3 4895 12 117 39 5 983 3 600][17 13 1136 9 59 395 7 4 255 73 126 72 47 9 67 478 7231 7 13 401][9546 288 21 4095 1669 12 872 17 18 7]...]\n",
            "targets[[17 13 42 39307 33 3566 1 15 49 293 84 1 7096 85 7 5 2 814 17 1156][26862 6675 2698 188 4 28 2 737 3840 34679 5638 7071 469 75 25 4437 38 4573 187 40][42 215 10 19 16 0 13340 57 9 367]...]\n",
            "targets[[10 5015 231 570 3 0 599 2707 8323 33 0 2497 269 35 2816 1152 3 2281 405 1909][4994 7264 578 8 0 200 6183 15 1133 2730 2769 451 1 436 4 4248 10 122 968 1262][27 307 10 19 440 154 143 1 51 9]...]\n",
            "targets[[5 0 244 3 3140 3788 3495 439 50 80 208 9 169 7 3214 11 286 15 142 115][1018 2 222 3 10 2889 22 1008 729 1 693 9 67 4 27 7 0 921 198 295][7 12 2 222 3 2 658 19 37 46]...]\n",
            "targets[[1022 37 10 5 47 30 0 50896 13 43 3543 69 9 140 251 11 0 673 16 91][0 2195 3902 2130 22 2 1717 2841 11 5 42 43 4 1382 3504 55 112 50 28 448][439 39 327 25 53 173 741 4 367 10]...]\n",
            "targets[[196 0 17 257 0 109 784 2 171 3 166 0 756 3 0 17 1386 31548 1 11405][17 4 28 509 33 30 0 231 42 106 44 16 48 9619 1 2 115 537 39 68][0 834 3 3341 338 5 52 3 2 3225]...]\n",
            "targets[[5 23 127 737 184 600 19 8 189 2 171 3 0 184 5 319 4 127 1629 11][552 7231 10 17 256 509 0 298 13 53 671 14 0 17 352 319 44 88 3 0][20 159 21 367 10 17 352 127 330 41]...]\n",
            "targets[[5 29 3 60 519 231 98 467 7 51 9 13 115 1 7 133 1765 55 15 71][55687 9149 7617 554 360 341 19 11 45 14399 3 10822 245 4 865 144 9 76 0 754][67 110 10 19 95 143 8 0 1001 12]...]\n",
            "targets[[414 5 35 486 3 1817 5680 2 19 111 174 102 30378 613 3206 4 2695 1442 10597 1861][17 5 37 1149 2 297 12998 368 6 6 9 402 1453 78 10 17 14 2 11244 8][3157 41 15548 14 7 12 11184 542 4 646]...]\n",
            "targets[[24205 5 2 81 1 1031 153 9 723 21 110 238 332 33 86 239 72 10 0 6754][3006 8799 3 417 652 510 4133 8 537 25 254 8 952 1265 8 0 484 3 21845 8][1550 5 0 117 3 0 733 1541 1012 98]...]\n",
            "targets[[84 151 9 196 99 318 10 17 13 87 5 10 614 11 0 8561 165 386 2 1686][81 11360 3 584 24951 5 74 19 8 174 95 0 225 0 5054 1069 0 608 3 1132][263 214 8 26 17196 590 284 9695 45 8084]...]\n",
            "targets[[12 56 6072 7 10140 978 2243 317 5 394 0 116 5 37 74 11 20 232 3454 1][5 23 64 0 4144 3 30 1224 98 18 0 872 184 17 123 284 3467 1 9402 2000][194 227 2041 26370 35 554 1031 279 34 1193]...]\n",
            "targets[[619 4398 282 1065 11 56 534 96 123 2228 2460 13989 4 0 6392 3 2 243 507 16][89 21 121 87 7 195 49 2921 9 89 21 456 46 7 13 2 297 63 9 856][70 651 8 1806 2591 374 4784 1 681 0]...]\n",
            "targets[[1116 19 36 5051 8 61 0 1500 3 6845 14867 303 377 25 6039 15 2 160 30388 4357][139 77 2754 2 394 19 6 6 815 9 232 987 11 233 9 140 460 1 27 67][140 23 2 8791 147 882 123 27 77 18]...]\n",
            "targets[[17 5 2 997 36 0 457 4 0 129 0 101 80 1 136 47 0 543 491 90][74 19 53 53 53 74 19 7 12 2 10027 18 7 5 23 273 3299 185 10 1071][203 66 16 74 17 4603 247 528 2 461]...]\n",
            "targets[[5095 1070 4 479 966 12 3115 1667 1056 201 78 2 3631 917 725 15 0 914 107 193][39 20 27 7 157 24170 22 60 2619 105 165 84 3 30 58 152 9 38 4 103][6625 5 437 379 0 109 5 53 348 1]...]\n",
            "targets[[29 3 0 88 901 1211 1 4316 1146 98 22 0 3246 123 92 10 5 229 1674 4][317 2502 36 35 1914 3505 4 168 31 0 3615 20 59 103 7 12 43 2 461 2217][312 118 23 942 47 2 1594 10 17 13]...]\n",
            "targets[[793 4 93 71 262 11 0 834 3 10 29385 2333 184 680 13 1403 33 0 851 227][371 856 137 397 36 10 17 35 3309 177 3 1031 1 155 3154 141 27 68216 69 6][3320 1704 8 112 15 4197 15829 10 122 982]...]\n",
            "targets[[51 9 215 10 17 31 2 2166 9 3580 42 87 3091 7 13 11 15830 27086 2663 8][24192 1 888 27 3428 137 11 5 53 1247 131 483 2 734 201 15 3989 7745 1 21208][291 60 563 3040 5 634 43 275 183 362]...]\n",
            "targets[[5 51 8309 269 55 36 0 3614 6 6 9 118 23 512 73 36 8309 10 501 1606][3778 7250 17994 732 8 2 209 3642 40 1966 6 6 2 323 2573 7314 34 45 115 116][19 5 1009 4 0 220 3 107 11612 1058]...]\n",
            "targets[[7983 5 2 153 622 104 25 245 4 3919 24 12 92 275 3 60 30 57 519 104][17 5 60 519 29 9 27 307 7 52 72 163 214 9 1207 121 30 0 410 174][6724 2570 5262 16 3345 18 11750 3 12713 505]...]\n",
            "targets[[125 9 470 4 38 10 17 7 45 2 81 177 1 7 5 2 385 1633 19 1][21 76 71 356 9 424 9142 1512 18 10 29 13 73 126 0 63 13 126 58 0][3 0 2983 5067 6198 45 7961 73 690 43]...]\n",
            "targets[[656 898 67 2 2190 3 163 678 10 13 2 394 432 3 443 437 56 4096 56 2615][12 161 160 477 12931 48 3 0 126 2601 236 28 98 38 0 960 12001 3 7946 908][5 29 3 60 519 98 295 8 7 5]...]\n",
            "targets[[2185 22 0 17 796 6 6 7 13 23 49 23 49 31 30 2240 7 13 81 9][42 389 143 36 2 1689 768 816 3 10 322 832 827 19 22802 7 12 393 5 23632][343 1 868 201 9137 0 868 5 7543 33]...]\n",
            "targets[[11663 13 335 22 351 3 26 458 51 24 326 10 19 7 5 35 6548 5358 19 368][2 57 51 0 176 5 37 2890 22 0 1849 8044 8 3854 0 151 11 5 88 731][307 10 3529 22 6 6 9 140 23 251]...]\n",
            "targets[[318 10 609 9 42 96 23 1017 545 36 13049 60 333 420 21 1643 4 1775 1 9696][111 80 9 875 10 17 1168 60 114 0 109 13 11000 601 20260 45 44 226 313 2537][5 23 29 3 146 104 7 5 29 3]...]\n",
            "targets[[8 23319 11477 30354 12 1054 1081 673 64862 4572 1437 13 35 2004 20904 2783 4989 0 2486 1][11 12 30 11 50 28 300 43 10 19 10 1525 21 58 28 1267 2 662 4 0][13 2 1049 10323 249 8 407 8600 608 3]...]\n",
            "targets[[293 0 277 5198 3 10 19 25 8 316 1 460 5 85 1276 50 76 62 933 22][206 5140 699 246 5453 23406 36 2170 13 142 35 1373 369 11 7 465 1099 4 93 2][45 0 3509 10259 3 43801 1050 15 15164 1]...]\n",
            "targets[[12 60 660 11 51 20 1134 4 148 93 2 53 49 19 20 141 12432 4 80 126][2 19112 665 1 178 1249 1768 284 1188 30313 1543 7867 1 26 312 3753 12 30313 36272 33641][63 3 4022 1756 45 77 41802 8 4849 1]...]\n",
            "targets[[9017 2324 267 143 78 0 2494 4 586 276 15859 36 1540 17535 9834 1321 159 21 6100 5985][13 433 19 7902 1 19 2921 334 72 1719 357 9 215 0 9356 748 733 5991 3 574][3044 8 5 23 155 23 734 23 1009 7]...]\n",
            "targets[[2990 6088 4860 9381 5 133 2301 33 0 311 24 1963 0 3337 1479 461 2095 27212 601 34][439 25 37 6966 37425 32 27 1291 0 217 5273 3432 344 1 25661 39 12 437 56 813][353 30 131 829 22 128 43 87 10 5]...]\n",
            "targets[[13 475 10837 8 10 19 36 0 84 4 227 782 7 5 2130 326 15 745 3 212][19 45 13830 142 3983 2761 30 3 20 42 353 0 798 11 9 140 180 1520 146 3][84 307 10 19 8 35 407 687 1860 472]...]\n",
            "targets[[12 212 4 66 47 2974 5293 5204 12 590 13 8 167 1420 4356 22 0 130 8 10][1315 3 1144 992 10 17 5 1373 9 67 56 321 11 4955 389 44 37 407 264 9][1508 6451 30939 13223 11838 3901 107 2 1508 24]...]\n",
            "targets[[0 1191 317 5 2 1157 1223 0 116 5 3704 0 225 5 10508 1 0 224 5 379][481 9 139 421 3063 33 149 1432 49 98 8 1187 154 18 10 13 33 229 0 250][1267 545 4 28 180 7572 257 51 9 106]...]\n",
            "targets[[4 2 74 1212 63 1 286 12 1969 10 184 755 635 2 4757 985 568 284 3479 1][140 11652 11 56 29 188 4 402 14331 8362 31 219 8 0 2348 32 89 21 4831 0][45 77 60 894 519 298 123 233 9 353]...]\n",
            "targets[[2591 328 36 5912 5 29 3 0 10103 928 41 873 104 9 139 123 2982 531 22 188][7457 6371 12 237 5 7 42 71 41 124 10 216 213 982 4 13909 17839 8 62 98][6288 3623 543 6840 22458 21200 22452 11 5 4143]...]\n",
            "targets[[88 3 60 355 1180 10 3578 4 71 2318 32 1667 3383 2 246 122 4 142 303 2846][5 65 23 2 53 49 201 264 9 27 509 149 14 2 3254 1 22 142 201 1380][817 725 45 77 2 1201 477 233 0 2845]...]\n",
            "targets[[604 66 134 1035 5249 10 17 6 6 0 5010 17 15 5283 1 15060 5 221 2 368][84 9 96 23 262 601 12999 13 391 2543 4722 18 24 5 330 22 15 26 1110 24][1 556 314 6945 30774 19436 34225 34 1065 513]...]\n",
            "targets[[9 121 126 72 4 512 2 1051 1470 3 2 673 51 338 218 1017 3 7 9 13][0 544 1916 703 153 1452 5359 12570 1 5352 3948 17456 1825 1 254 32 4983 2 171 3][242 1957 8962 4 66 2 938 3 463 798]...]\n",
            "targets[[124 69 128 257 1078 10 5 64 40 3987 19 10 29 5 404 8886 85 7 724 199][5 56 813 11 10693 12525 5 2 3256 8 1680 1 10564 2185 13019 8 0 650 4515 547][5 2 681 5324 4927 869 11 113 982 4]...]\n",
            "targets[[17 43 105 1071 887 34 389 4 2563 15 62 231 5 42 81 6 6 7 210 21][20 25 475 78 2293 343 968 39 25 173 2168 128 783 240 36 10 2227 92 432 3][0 63 5 2088 2594 1 0 842 271 5]...]\n",
            "targets[[5616 12313 5582 0 3556 2388 5377 40319 25972 13 15058 8 20264 0 4243 45 113 77 254 10][321 839 2097 6 6 128 25 0 9335 9 50 136 43 10 17 0 321 3 2 705][103 91 57 16 3069 4 138 4960 78 0]...]\n",
            "targets[[139 110 10 17 51 9 13 3650 8 3207 9 254 7 871 4 65 387 7740 1087 1][27 56 321 47 0 1104 3 0 7728 68 259 4 80 18 0 914 2400 16 392 0][3 0 117 98 16 30 2220 20 83 113]...]\n",
            "targets[[10190 2247 1606 39 5 56 145 95 4 1823 0 105 190 233 2247 1606 5 2 637 11][17 5 12 1286 629 1846 1066 88 3 0 17 165 1073 16 7 4 875 6 6 0][4 0 3401 16 10 122 9 329 4 2655]...]\n",
            "targets[[155 87 50 255 3968 10 4 6257 6629 11 5 437 630 39 25 56 915 10 5 23][644 389 14 2 938 842 23 256 2 2374 14 4 47 7 13 30 43 7 1499 120][307 10 19 108 214 1 367 7 42 14]...]\n",
            "targets[[54 2164 55 8 47 108 59 1652 14 2 2914 755 176 10 17 274 11 23 30 338][5 2 65 81 247 19 11 2107 15 48 3 0 1429 3 57 1899 78 0 501 46][18 9 89 21 1046 15 0 75 128 0]...]\n",
            "targets[[5 0 84 44 3 0 9509 3982 202 1 5 29 3 0 52 3202 104 44 3 0][10328 10328 36 17322 57550 682 10 5 2 19 43 1700 43 11 305 244 3 1700 3 0][13 770 31 0 1152 3 1370 14113 1264 181]...]\n",
            "targets[[19 529 2 3927 667 3 0 472 3 0 759 1328 1708 8 15379 7 406 0 507 193][734 1 641 2013 436 1015 4 18639 73 573 9 65 509 0 2468 135 873 7 188 0][10 5 247 3268 20 27 4 28 8 0]...]\n",
            "targets[[57 9 66 1406 5679 7 12 14 46 2 6547 316 2184 45 3257 1 9 140 1983 21677][418 4 10 17 31 2 177 1 888 122 1090 60 425 916 5 2 1252 22 0 17][108 3 131 82 829 27 2963 11 2 171]...]\n",
            "targets[[27 23 353 0 673 10 19 13 427 22 18 9 118 1059 106 0 2514 339 132 0][19 5 44060 3 4550 98 0 705 4550 3912 12 6726 0 1258 4550 0 1787 6568 1620 0][21 28 7584 43 3995 10 17 9 112 7]...]\n",
            "targets[[657 25 576 0 2051 25 2281 1 0 101 25 1344 54109 18 9 420 21 535 36 1014][11893 20 25 1213 7442 34 150 21 121 2 151 36 0 81 20094 41736 10 125 45 77][2260 55 8 9777 1 94 307 91 954 78]...]\n",
            "targets[[499 8 5582 8 4569 111 316 1 8948 2252 7127 1017 3989 11 12 111 32 2384 55 190][13 10403 31 47 35 379 369 10 13 1078 1769 12 6028 2535 376 3611 1 116 5 2][215 10 42 43 414 19 51 7 7980 31]...]\n",
            "targets[[46 20 148 2 243 34 12 195 13241 1205 20 236 38 10 17 722 127 2824 82 10][4585 3079 45 3521 2 765 6339 43 7 73 38 314 2520 11 12 85 39 25 108 4034][64 2065 10 17 43 3106 1816 602 51 9]...]\n",
            "targets[[9 353 10 298 1 13 2229 4 66 0 17 4 66 47 32 59 6 6 9 67][9 27 110 10 17 1 59 38 4 198 3823 1808 29 39 5 161 43 10 17 4][42251 5 2 81 19 16 0 5438 7 971]...]\n",
            "targets[[1430 1022 10 17 210 21 497 7 12 165 2 977 49 17 30513 20 139 113 110 555][37 32 350 4 93 10 38 35 851 145 176 122 1 94 7 267 17 22 178 1][7497 12 6441 4 0 276 443 5 4064 26]...]\n",
            "targets[[42 110 22538 10 17 389 4 333 4376 0 105 9 103 10 29 13 52 1097 0 4032][181 2702 201 11 12 1034 53 3186 882 155 10 10362 4798 1109 18998 947 49869 144 2 5803][215 11 17 173 483 602 10 17 5 37]...]\n",
            "targets[[9 970 970 629 5 0 8714 19 123 92 9 50 21 262 60 324 1291 10 609 10][183 10132 342 2384 14 921 1 350 4 76 78 2 176 4165 28200 1162 199 7156 1 27559][0 20230 3 0 1278 12505 1 3 0 11674]...]\n",
            "targets[[5 0 447 19 9 139 581 31 8 2 194 57 0 116 5149 36 379 4 2975 18][49 6 6 2 1043 641 7681 7407 261 125 1455 22 0 1350 3 0 4794 3 0 89][78 2 3061 10600 624 31 311 1 136 1785]...]\n",
            "targets[[1996 1 3566 319 44 5 0 117 172 1 64 913 130 3 10 873 2062 17 39 5][36 22764 1 1192 2095 10 19 5 2 1792 4 0 531 15 323 5501 5883 36 12 2059][300 2 940 11 92 71 53 671 8 40]...]\n",
            "targets[[3 0 881 1996 27 446 24579 774 1099 1 27 1712 134 7 13 92 60 864 16 131][19 702 2 1571 63 0 503 1 0 101 25 37 145 1 1011 37 69 20 50 21][140 2 669 340 3 0 9196 3 13691 246]...]\n",
            "targets[[6 815 37 9 139 555 3 0 21756 3 12410 10 17 45 2034 9 139 555 87 10][84 215 970 11406 42091 10761 12 786 31358 29341 125 6611 821 43 720 154 602 1 9 13][188 4 28 1 145 6115 5 0 1406 9]...]\n",
            "targets[[373 1 2 518 409 8 2 1040 346 391 1409 3 855 11842 1 148 24540 0 1497 3][50 106 2 49 2019 19 147 1 94 9 139 110 48 186 1213 528 190 10 5 29][84 1018 0 17 22 91 84 488 22 3685]...]\n",
            "targets[[13 1262 671 33 10 17 9 27 1059 1291 193 202 22 277 14 9 67 110 0 84][9 84 215 8434 13264 12 397 2835 0 773 20577 9 1147 30 18 0 227 969 228 10][389 596 10 19 445 0 412 885 7417 31]...]\n",
            "targets[[5 0 117 2499 19 123 92 7 1886 52 94 47 7 195 31 0 4076 2555 16 179][4326 12 673 3342 1 5576 5 208 2 813 29 3 60 519 1177 1 147 429 7 45][53 413 807 17 36 2563 46 20 112 0]...]\n",
            "targets[[65 470 4 38 10 17 18 7 13 42 54404 0 116 13 3104 5646 0 109 13 612][196 7 13 2 186 49 17 0 116 13 49 1 0 63 13 69 585 7 12 29][13 8 2 394 95 194 167 0 276 389]...]\n",
            "targets[[5611 52 72 537 0 372 25 42 4160 2400 7977 12 324 31 0 457 3 0 19 0][24799 14083 3828 23 58 49 16 0 2922 12307 55 0 346 965 37 32 810 9511 7797 7][7 13 29 3 0 117 98 9 27 110]...]\n",
            "targets[[0 457 70 50 66 1102 3 4895 739 1077 1 6059 4925 550 10 5 74 7 12 2][1473 285 2 12231 44 18 2780 1141 22 0 2504 16 2 1479 461 34 45 2 13938 433][140 2 200 340 3 3585 98 1 4733 9]...]\n",
            "targets[[5826 9716 1721 550 6884 2 941 12 531 1621 1281 181 24 12 23 356 245 6388 5 2][83 198 20 105 996 16 149 10 832 827 13416 43 2 10337 5593 11 5199 4 8071 44][422 141 27 77 3308 196 44 440 214 1]...]\n",
            "targets[[203 27 77 440 154 99 7 13 645 37 89 21 121 134 7 13 31 0 98 18][17 5 37 948 7 5 221 2863 126 542 14 2724 1947 0 24994 817 10 487 494 4][429 195 545 270 55 22 5074 643 277 2255]...]\n",
            "targets[[4881 3905 12 88 1571 237 1 1675 698 0 243 45 654 8 2 173 294 40 590 9][0 245 6388 1141 547 3 20634 15617 1 4172 7852 27 13139 4 443 38 2 1670 8 2][1426 291 158 396 33927 5 1448 36 2292 4]...]\n",
            "targets[[221 283 9 1818 89 21 940 22 104 9 723 21 110 30 0 95 144 18 233 9][5 42 29 3 146 3256 2446 28 540 328 2684 6540 92 8 1916 9 80 23 121 134][1 338 67 77 22 0 5257 14 0 6498]...]\n",
            "targets[[10 5 29 3 200 12 117 685 12 666 209 8 7321 3182 78 105 16 0 1248 66453][204 23 28 0 29 4 738 10 17 85 99 3595 228 3 1051 3203 1 3057 9 654][1959 2087 47 1117 2166 491 4 106 41 58]...]\n",
            "targets[[276 4743 491 4 1738 2 1437 22 2 2727 958 37 24 45 286 13287 86 1 26 939][483 99 318 29307 14030 9 242 133 8 4313 11 100 522 3 75 59 1143 37 73 57][112 2 19 11 150 21 0 109 1 0]...]\n",
            "targets[[46 20 27 2 115 119 35 541 4 496 1 169 2856 4 28 97 1094 9 207 1478][467 10 17 7 12 371 1185 554 155 6288 1799 7 162 56 266 4 380 43 0 10047][17 50 2222 4 28 2 360 360 341 600]...]\n",
            "targets[[371 337 63 15 2 1534 43 20239 112 4471 10 1055 577 2146 19 10 13 257 1055 85][113 188 4 11642 71 87 74 104 50 165 28 18 10 5 35 2449 4 1648 87 124][194 602 9 1065 60 990 14 4 47 40013]...]\n",
            "targets[[5 0 64 17 11 60 312 1 9 27 123 2518 44 22 475 2144 70 215 7 8][215 7 31 0 33993 15 2 425 8 0 122 497 6356 13 884 31 0 143 3 0][10 5 2 994 7 5 2 4588 19 6]...]\n",
            "targets[[389 85 9 555 7 13 2 1484 199 6963 0 1341 9687 0 1 9379 9 2464 85 7][5 35 212 297 63 3 10475 3541 7721 34 8793 3 107 35 51 24 13 2 493 357][103 10 17 5 117 7870 55 14 1109 852]...]\n",
            "targets[[84 202 3 400 4211 120 15 2 3729 1207 1 1366 29460 8 1069 10 204 27 278 48][17 5 730 4 0 304 5343 27776 1325 408 33 10670 7117 0 109 3 2 1212 312 1][69 69 32773 42944 5 29 3 0 1536 156]...]\n",
            "targets[[5 0 88 1211 19 9 27 110 8 2 194 57 46 20 38 10 17 138 818 7442][3 2 3063 945 527 34 747 4 2132 36748 1 2025 4 0 1395 652 3 26 160 303][27121 65 4231 4 28 11120 16 10 371 19940]...]\n",
            "targets[[471 5 23 74 1101 4 3862 2124 184 7 274 0 457 1 2450 3 258 519 184 314][89 21 103 100 1837 8 338 472 4939 14 194 14 577 6375 118 358 88 3 0 849][17 13 53 777 7 13 4610 1 155 31]...]\n",
            "targets[[242 715 10 202 2 163 23 85 3 0 109 34 12 8789 18 16 0 1136 12319 1][89 21 182 4 198 10 308 314 9 470 4 38 7 18 7 1201 22 37 108 5611][5 241 29 3 0 250 98 9 27 123]...]\n",
            "targets[[210 21 2 414 19 165 1225 740 107 414 18 1321 10 17 210 21 2 1223 352 9][3895 755 3 2 363 3536 4 0 4131 3473 1 1410 44 0 1282 13 23 606 14 6364][21 28 4374 33 238 11 20 27 123 353]...]\n",
            "targets[[0 3988 1 407 3821 8286 5421 13 5363 1267 14 29 3 0 872 184 964 123 4 1656][1 1588 8433 198 49 347 14 105 14897 556 292 4 8071 44 0 2507 1238 0 366 3][22 12 160 7553 31 459 11205 22 38298 205]...]\n",
            "targets[[20 38 832 827 1890 1 2291 4749 94 20 83 112 10 17 6 6 0 305 290 25][267 4 0 69 29 97 108 214 14 1859 34 45 110 0 206 115 7267 83 230 5721][36607 46 20 159 21 58 7155 31 11 390]...]\n",
            "targets[[548 10 122 19754 5745 36 0 705 298 1 17 8 189 9 424 10 10 246 122 0][5 2 53674 19 4 106 2 6469 311 15 127 355 62 158 20264 5439 690 5 65 137][30 276 13364 11668 5 161 18 2 10754 8081]...]\n",
            "Traceback (most recent call last):\n",
            "  File \"generate_samples.py\", line 281, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow/python/platform/app.py\", line 126, in run\n",
            "    _sys.exit(main(argv))\n",
            "  File \"generate_samples.py\", line 277, in main\n",
            "    generate_samples(hparams, data_set, id_to_word, log_dir, output_file)\n",
            "  File \"generate_samples.py\", line 233, in generate_samples\n",
            "    generate_logs(sess, model, output_file, id_to_word, eval_feed)\n",
            "  File \"generate_samples.py\", line 138, in generate_logs\n",
            "    samples = write_masked_log(log, id_to_word, sequence_eval, p)\n",
            "  File \"generate_samples.py\", line 115, in write_masked_log\n",
            "    FLAGS.batch_size)\n",
            "  File \"generate_samples.py\", line 95, in convert_to_human_readable\n",
            "    sample.append('*' + str(id_to_word[index]))\n",
            "KeyError: 69848\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOwDbEL3c3B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9aZeEuOc3kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huMuu2c6c4OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHl8sFSttCHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRPevP8JsZ3J",
        "colab_type": "text"
      },
      "source": [
        "## Save and Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiPsRppGw1CK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FB7DYX0w17O",
        "colab_type": "code",
        "outputId": "7f24f459-61d6-4e1e-a74c-9d9e241829db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# #copy checkpoints to drive\n",
        "%cd /content\n",
        "%cp -av maskgan /content/gdrive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "'maskgan' -> '/content/gdrive/My Drive/maskgan'\n",
            "'maskgan/checkpoint_convert.py' -> '/content/gdrive/My Drive/maskgan/checkpoint_convert.py'\n",
            "'maskgan/sample_shuffler.py' -> '/content/gdrive/My Drive/maskgan/sample_shuffler.py'\n",
            "'maskgan/pretrain_mask_gan.py' -> '/content/gdrive/My Drive/maskgan/pretrain_mask_gan.py'\n",
            "'maskgan/README.md' -> '/content/gdrive/My Drive/maskgan/README.md'\n",
            "'maskgan/dataset' -> '/content/gdrive/My Drive/maskgan/dataset'\n",
            "'maskgan/dataset/iccv' -> '/content/gdrive/My Drive/maskgan/dataset/iccv'\n",
            "'maskgan/dataset/iccv/ptb.valid.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv/ptb.valid.txt'\n",
            "'maskgan/dataset/iccv/ptb.train.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv/ptb.train.txt'\n",
            "'maskgan/dataset/iccv/ptb.test.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv/ptb.test.txt'\n",
            "'maskgan/dataset/iccv2017' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017'\n",
            "'maskgan/dataset/iccv2017/ptb.train.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017/ptb.train.txt'\n",
            "'maskgan/dataset/iccv2017/ptb.valid.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017/ptb.valid.txt'\n",
            "'maskgan/dataset/iccv2017/ptb.test.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017/ptb.test.txt'\n",
            "'maskgan/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/.ipynb_checkpoints'\n",
            "'maskgan/pretrain_mask_gan.pyc' -> '/content/gdrive/My Drive/maskgan/pretrain_mask_gan.pyc'\n",
            "'maskgan/data' -> '/content/gdrive/My Drive/maskgan/data'\n",
            "'maskgan/data/imdb_loader.py' -> '/content/gdrive/My Drive/maskgan/data/imdb_loader.py'\n",
            "'maskgan/data/ptb_loader.py' -> '/content/gdrive/My Drive/maskgan/data/ptb_loader.py'\n",
            "'maskgan/data/__init__.py' -> '/content/gdrive/My Drive/maskgan/data/__init__.py'\n",
            "'maskgan/data/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/data/__init__.pyc'\n",
            "'maskgan/data/ptb_loader.pyc' -> '/content/gdrive/My Drive/maskgan/data/ptb_loader.pyc'\n",
            "'maskgan/data/imdb_loader.pyc' -> '/content/gdrive/My Drive/maskgan/data/imdb_loader.pyc'\n",
            "'maskgan/nas_utils' -> '/content/gdrive/My Drive/maskgan/nas_utils'\n",
            "'maskgan/nas_utils/configs.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/configs.py'\n",
            "'maskgan/nas_utils/variational_dropout.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/variational_dropout.py'\n",
            "'maskgan/nas_utils/__init__.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/__init__.py'\n",
            "'maskgan/nas_utils/custom_cell.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/custom_cell.py'\n",
            "'maskgan/nas_utils/custom_cell.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/custom_cell.pyc'\n",
            "'maskgan/nas_utils/variational_dropout.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/variational_dropout.pyc'\n",
            "'maskgan/nas_utils/configs.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/configs.pyc'\n",
            "'maskgan/nas_utils/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/__init__.pyc'\n",
            "'maskgan/models' -> '/content/gdrive/My Drive/maskgan/models'\n",
            "'maskgan/models/cnn.py' -> '/content/gdrive/My Drive/maskgan/models/cnn.py'\n",
            "'maskgan/models/attention_utils.py' -> '/content/gdrive/My Drive/maskgan/models/attention_utils.py'\n",
            "'maskgan/models/rollout.py' -> '/content/gdrive/My Drive/maskgan/models/rollout.py'\n",
            "'maskgan/models/bidirectional_zaremba.py' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_zaremba.py'\n",
            "'maskgan/models/rnn_nas.py' -> '/content/gdrive/My Drive/maskgan/models/rnn_nas.py'\n",
            "'maskgan/models/seq2seq_vd.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_vd.py'\n",
            "'maskgan/models/feedforward.py' -> '/content/gdrive/My Drive/maskgan/models/feedforward.py'\n",
            "'maskgan/models/critic_vd.py' -> '/content/gdrive/My Drive/maskgan/models/critic_vd.py'\n",
            "'maskgan/models/__init__.py' -> '/content/gdrive/My Drive/maskgan/models/__init__.py'\n",
            "'maskgan/models/bidirectional.py' -> '/content/gdrive/My Drive/maskgan/models/bidirectional.py'\n",
            "'maskgan/models/seq2seq_zaremba.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_zaremba.py'\n",
            "'maskgan/models/seq2seq.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq.py'\n",
            "'maskgan/models/rnn_vd.py' -> '/content/gdrive/My Drive/maskgan/models/rnn_vd.py'\n",
            "'maskgan/models/bidirectional_vd.py' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_vd.py'\n",
            "'maskgan/models/seq2seq_nas.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_nas.py'\n",
            "'maskgan/models/rnn.py' -> '/content/gdrive/My Drive/maskgan/models/rnn.py'\n",
            "'maskgan/models/rnn_zaremba.py' -> '/content/gdrive/My Drive/maskgan/models/rnn_zaremba.py'\n",
            "'maskgan/models/evaluation_utils.py' -> '/content/gdrive/My Drive/maskgan/models/evaluation_utils.py'\n",
            "'maskgan/models/bidirectional.pyc' -> '/content/gdrive/My Drive/maskgan/models/bidirectional.pyc'\n",
            "'maskgan/models/evaluation_utils.pyc' -> '/content/gdrive/My Drive/maskgan/models/evaluation_utils.pyc'\n",
            "'maskgan/models/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/models/__init__.pyc'\n",
            "'maskgan/models/rnn_zaremba.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn_zaremba.pyc'\n",
            "'maskgan/models/bidirectional_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_vd.pyc'\n",
            "'maskgan/models/rnn.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn.pyc'\n",
            "'maskgan/models/bidirectional_zaremba.pyc' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_zaremba.pyc'\n",
            "'maskgan/models/seq2seq_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_vd.pyc'\n",
            "'maskgan/models/attention_utils.pyc' -> '/content/gdrive/My Drive/maskgan/models/attention_utils.pyc'\n",
            "'maskgan/models/cnn.pyc' -> '/content/gdrive/My Drive/maskgan/models/cnn.pyc'\n",
            "'maskgan/models/seq2seq.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq.pyc'\n",
            "'maskgan/models/seq2seq_nas.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_nas.pyc'\n",
            "'maskgan/models/rnn_nas.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn_nas.pyc'\n",
            "'maskgan/models/seq2seq_zaremba.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_zaremba.pyc'\n",
            "'maskgan/models/rollout.pyc' -> '/content/gdrive/My Drive/maskgan/models/rollout.pyc'\n",
            "'maskgan/models/rnn_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn_vd.pyc'\n",
            "'maskgan/models/critic_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/critic_vd.pyc'\n",
            "'maskgan/models/feedforward.pyc' -> '/content/gdrive/My Drive/maskgan/models/feedforward.pyc'\n",
            "'maskgan/regularization' -> '/content/gdrive/My Drive/maskgan/regularization'\n",
            "'maskgan/regularization/variational_dropout.py' -> '/content/gdrive/My Drive/maskgan/regularization/variational_dropout.py'\n",
            "'maskgan/regularization/zoneout.py' -> '/content/gdrive/My Drive/maskgan/regularization/zoneout.py'\n",
            "'maskgan/regularization/__init__.py' -> '/content/gdrive/My Drive/maskgan/regularization/__init__.py'\n",
            "'maskgan/regularization/zoneout.pyc' -> '/content/gdrive/My Drive/maskgan/regularization/zoneout.pyc'\n",
            "'maskgan/regularization/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/regularization/__init__.pyc'\n",
            "'maskgan/regularization/variational_dropout.pyc' -> '/content/gdrive/My Drive/maskgan/regularization/variational_dropout.pyc'\n",
            "'maskgan/losses' -> '/content/gdrive/My Drive/maskgan/losses'\n",
            "'maskgan/losses/losses.py' -> '/content/gdrive/My Drive/maskgan/losses/losses.py'\n",
            "'maskgan/losses/__init__.py' -> '/content/gdrive/My Drive/maskgan/losses/__init__.py'\n",
            "'maskgan/losses/losses.pyc' -> '/content/gdrive/My Drive/maskgan/losses/losses.pyc'\n",
            "'maskgan/losses/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/losses/__init__.pyc'\n",
            "'maskgan/train_mask_gan.py' -> '/content/gdrive/My Drive/maskgan/train_mask_gan.py'\n",
            "'maskgan/model_utils' -> '/content/gdrive/My Drive/maskgan/model_utils'\n",
            "'maskgan/model_utils/model_losses.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_losses.py'\n",
            "'maskgan/model_utils/variable_mapping.py' -> '/content/gdrive/My Drive/maskgan/model_utils/variable_mapping.py'\n",
            "'maskgan/model_utils/model_optimization.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_optimization.py'\n",
            "'maskgan/model_utils/model_utils.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_utils.py'\n",
            "'maskgan/model_utils/helper.py' -> '/content/gdrive/My Drive/maskgan/model_utils/helper.py'\n",
            "'maskgan/model_utils/__init__.py' -> '/content/gdrive/My Drive/maskgan/model_utils/__init__.py'\n",
            "'maskgan/model_utils/n_gram.py' -> '/content/gdrive/My Drive/maskgan/model_utils/n_gram.py'\n",
            "'maskgan/model_utils/variable_mapping.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/variable_mapping.pyc'\n",
            "'maskgan/model_utils/n_gram.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/n_gram.pyc'\n",
            "'maskgan/model_utils/model_utils.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_utils.pyc'\n",
            "'maskgan/model_utils/helper.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/helper.pyc'\n",
            "'maskgan/model_utils/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/__init__.pyc'\n",
            "'maskgan/model_utils/model_losses.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_losses.pyc'\n",
            "'maskgan/model_utils/model_optimization.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_optimization.pyc'\n",
            "'maskgan/model_utils/model_construction.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_construction.py'\n",
            "'maskgan/model_utils/model_construction.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_construction.pyc'\n",
            "'maskgan/train_mask_gan.pyc' -> '/content/gdrive/My Drive/maskgan/train_mask_gan.pyc'\n",
            "'maskgan/generate_samples.py' -> '/content/gdrive/My Drive/maskgan/generate_samples.py'\n",
            "'maskgan/maskGAN' -> '/content/gdrive/My Drive/maskgan/maskGAN'\n",
            "'maskgan/maskGAN/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/maskGAN/.ipynb_checkpoints'\n",
            "'maskgan/maskGAN/train-log.txt' -> '/content/gdrive/My Drive/maskgan/maskGAN/train-log.txt'\n",
            "'maskgan/maskGAN/maskGAN_mle' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle'\n",
            "'maskgan/maskGAN/maskGAN_mle/train-log.txt' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train-log.txt'\n",
            "'maskgan/maskGAN/maskGAN_mle/train' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/.ipynb_checkpoints'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/checkpoint' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/checkpoint'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.index' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.index'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.data-00000-of-00001' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.data-00000-of-00001'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.meta' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.meta'\n",
            "'maskgan/maskGAN/train' -> '/content/gdrive/My Drive/maskgan/maskGAN/train'\n",
            "'maskgan/maskGAN/train/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/.ipynb_checkpoints'\n",
            "'maskgan/maskGAN/train/graph.pbtxt' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/graph.pbtxt'\n",
            "'maskgan/maskGAN/train/model.ckpt-1059.index' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/model.ckpt-1059.index'\n",
            "'maskgan/maskGAN/train/model.ckpt-1059.data-00000-of-00001' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/model.ckpt-1059.data-00000-of-00001'\n",
            "'maskgan/maskGAN/train/checkpoint' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/checkpoint'\n",
            "'maskgan/maskGAN/train/model.ckpt-1059.meta' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/model.ckpt-1059.meta'\n",
            "'maskgan/maskGANj_working.ipynb' -> '/content/gdrive/My Drive/maskgan/maskGANj_working.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fV7rJwxZ0Xa",
        "colab_type": "code",
        "outputId": "6b8d2052-fc97-4b59-b923-1b5f40e55c1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# copy from drive into runtime\n",
        "%cd /content/gdrive/My\\ Drive\n",
        "%cp -r maskgan /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXXGgoq0aZo8",
        "colab_type": "code",
        "outputId": "a89c677f-572e-46da-9978-e07b0e2b7edb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/maskgan/maskGAN/train\n",
        "from google.colab import files\n",
        "files.download('model.ckpt-1542.data-00000-of-00001') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/maskgan/maskGAN/train\n",
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 35792, 0, 0)\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 293, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 321, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 334, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 657, in __init__\n",
            "    self.finish()\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 716, in finish\n",
            "    self.wfile.close()\n",
            "  File \"/usr/lib/python2.7/socket.py\", line 283, in close\n",
            "    self.flush()\n",
            "  File \"/usr/lib/python2.7/socket.py\", line 307, in flush\n",
            "    self._sock.sendall(view[write_offset:write_offset+buffer_size])\n",
            "error: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AcystW6oRMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJLAY-ot-rAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcqrMZ7W-uEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dMi-HZO_Leb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}