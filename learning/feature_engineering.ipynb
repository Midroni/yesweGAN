{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "First, we list the features generated by the successful TALOS team.\n",
    "\n",
    "1. **Basic Count Features**: turns unigrams, bi-grams, and tri-grams into various counts and ratios which mark the *relationship between a headline and its body text*. Features generated include:\n",
    "    - Number of unique grams in the headline & body\n",
    "    - Number of times a gram appears in the headline & body\n",
    "    - Ratio of gram appearances over number of unique grams ( for headline and body)\n",
    "    - How many grams in the headline also appear in the body text (overlapping grams)\n",
    "    - Number of overlapping grams normalized by the number of grams in the headline\n",
    "\n",
    "2. **TF-IDF Features**: constructs sparse vector representations of the headline and body by calculating the Term Frequency score (TF) of each gram, and normalizing it by its Inverse-Document Frequency score (IDF).\n",
    "    - Calculates the cosine similarity between the headline and body tfidf vector\n",
    "\n",
    "3. **SVD Features**: this model applies Singular-Value Decomposiiton (SVD) to the sparse matrices resulting from the TF-IDF analysis, and obtains a more compact vector representation of the headline and body. This is a well known procedure used for Topic Modelling. As such, its output is used to understand which topics in the corpus represent the headline or body. Features include:\n",
    "    - Latent topics from the corpus which represent headline/body text\n",
    "    - The cosine similarities between the SVD features of headline and body text\n",
    "\n",
    "4. **Word2Vec Features**: Using pre-trained word vectors from public sources, *vector representations of the headline and body* are built. These word vectors were trained on a Google News corpus. *Vector representation features may overcome the use of synonyms instead of exact words*. \n",
    "\n",
    "5. **Sentiment Features**: Used the Sentiment Analyzer in the NLTK package to assign a sentiment polarity to the headline and body separately. Features include:\n",
    "    - Returned negative or positive scores on sentiment for the headline and body\n",
    "    \n",
    "Other features we can add:\n",
    "6. **Bag of Words Features**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#standard imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#nlp imports\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line george orwell novel 1984 predict power sm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17 year old girl name alyssa carson train nasa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1988 author roald dahl pen open letter urge pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>come fight terrorism another thing know work b...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                claim         claimant  \\\n",
       "id                                                                       \n",
       "0   line george orwell novel 1984 predict power sm...              NaN   \n",
       "1   maine legislature candidate leslie gibson insu...              NaN   \n",
       "4   17 year old girl name alyssa carson train nasa...              NaN   \n",
       "5   1988 author roald dahl pen open letter urge pa...              NaN   \n",
       "6   come fight terrorism another thing know work b...  Hillary Clinton   \n",
       "\n",
       "         date  label                            related_articles  \n",
       "id                                                                \n",
       "0  2017-07-17      0            [122094, 122580, 130685, 134765]  \n",
       "1  2018-03-17      2                    [106868, 127320, 128060]  \n",
       "4  2018-07-18      1                    [132130, 132132, 149722]  \n",
       "5  2019-02-04      2                    [123254, 123418, 127464]  \n",
       "6  2016-03-22      2  [41099, 89899, 72543, 82644, 95344, 88361]  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Locate Vedant's pickle ;)\n",
    "#os.listdir('../playground/vedant/pre_processing/')\n",
    "df = pd.read_pickle('../playground/vedant/pre_processing/claims.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Count Features\n",
    "### ngram Count\n",
    "First we will attempt to create n-gram counts for {1,2,3}-grams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "socialist teacher south charlotte middle school put message fuck kavanaugh school sign\n"
     ]
    }
   ],
   "source": [
    "test_claim = df['claim'].values[11]\n",
    "print(test_claim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate a list of ngrams\n",
    "def generate_ngrams(string, n):\n",
    "    #break text in tokens, not counting empty tokens\n",
    "    tokens = [token for token in string.split(\" \") if token != \"\"]\n",
    "    \n",
    "    #use the zip function to generate the desired n_gram list\n",
    "    ngrams = zip(*[tokens[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['socialist teacher',\n",
       " 'teacher south',\n",
       " 'south charlotte',\n",
       " 'charlotte middle',\n",
       " 'middle school',\n",
       " 'school put',\n",
       " 'put message',\n",
       " 'message fuck',\n",
       " 'fuck kavanaugh',\n",
       " 'kavanaugh school',\n",
       " 'school sign']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_ngrams(test_claim,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also use the nltk ngrams function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('socialist', 'teacher'),\n",
       " ('teacher', 'south'),\n",
       " ('south', 'charlotte'),\n",
       " ('charlotte', 'middle'),\n",
       " ('middle', 'school'),\n",
       " ('school', 'put'),\n",
       " ('put', 'message'),\n",
       " ('message', 'fuck'),\n",
       " ('fuck', 'kavanaugh'),\n",
       " ('kavanaugh', 'school'),\n",
       " ('school', 'sign')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = [token for token in test_claim.split(\" \") if token != \"\"]\n",
    "list(ngrams(tokens,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_ngram_count(df,col,n):\n",
    "    '''\n",
    "    Adds the word_count feature for a desired column in a pandas dataframe.\n",
    "    '''\n",
    "    if not isinstance(col,str):\n",
    "        raise ValueError('col must be of type str')\n",
    "    \n",
    "    df['count_%sgram_%s' %(n,col)] = df[col].apply(lambda x: len(generate_ngrams(x,n)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add ngram counts to the desired column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feature_ngram_count(df,'claim',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>count_1gram_claim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line george orwell novel 1984 predict power sm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17 year old girl name alyssa carson train nasa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-07-18</td>\n",
       "      <td>1</td>\n",
       "      <td>[132130, 132132, 149722]</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1988 author roald dahl pen open letter urge pa...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2019-02-04</td>\n",
       "      <td>2</td>\n",
       "      <td>[123254, 123418, 127464]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>come fight terrorism another thing know work b...</td>\n",
       "      <td>Hillary Clinton</td>\n",
       "      <td>2016-03-22</td>\n",
       "      <td>2</td>\n",
       "      <td>[41099, 89899, 72543, 82644, 95344, 88361]</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                claim         claimant  \\\n",
       "id                                                                       \n",
       "0   line george orwell novel 1984 predict power sm...              NaN   \n",
       "1   maine legislature candidate leslie gibson insu...              NaN   \n",
       "4   17 year old girl name alyssa carson train nasa...              NaN   \n",
       "5   1988 author roald dahl pen open letter urge pa...              NaN   \n",
       "6   come fight terrorism another thing know work b...  Hillary Clinton   \n",
       "\n",
       "         date  label                            related_articles  \\\n",
       "id                                                                 \n",
       "0  2017-07-17      0            [122094, 122580, 130685, 134765]   \n",
       "1  2018-03-17      2                    [106868, 127320, 128060]   \n",
       "4  2018-07-18      1                    [132130, 132132, 149722]   \n",
       "5  2019-02-04      2                    [123254, 123418, 127464]   \n",
       "6  2016-03-22      2  [41099, 89899, 72543, 82644, 95344, 88361]   \n",
       "\n",
       "    count_1gram_claim  \n",
       "id                     \n",
       "0                   8  \n",
       "1                  14  \n",
       "4                  11  \n",
       "5                  12  \n",
       "6                  12  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unique ngram Count & Ratio of Unique ngram\n",
    "Now, piggyback off this function to add a count of **unique ngrams** and the **ratio of unique ngrams to total ngrams**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_unique_ngram_count(df,col,n):\n",
    "    '''\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(col,str):\n",
    "        raise ValueError('col must be of type str')\n",
    "    \n",
    "    df['count_unique_%sgram_%s' %(n,col)] = df.apply(lambda x: len(set(generate_ngrams(x[col],n))),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_divide(a,b):\n",
    "    try:\n",
    "        return a/b\n",
    "    except ZeroDivisionError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_unique_ngram_ratio(df,col,n):\n",
    "    '''\n",
    "    Adds a column of ratios which indicate the proportion of the feature's text which is unique.\n",
    "    '''\n",
    "    if not isinstance(col,str):\n",
    "        raise ValueError('col must be of type str')\n",
    "    \n",
    "    df['ratio_unique_%sgram_%s' %(n,col)] = \\\n",
    "    list(map(try_divide, df['count_unique_%sgram_%s' %(n,col)], df['count_%sgram_%s' %(n,col)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feature_unique_ngram_count(df,'claim',1)\n",
    "add_feature_unique_ngram_ratio(df,'claim',1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of a headline with a ratio less than 1.0\n",
      "['state federal government help run health care marketplace average american 50 different plan choose different level coverage']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example of a headline with a ratio less than 1.0\")\n",
    "print(df.loc[df.ratio_unique_1gram_claim != 1.0].sample(1).claim.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overlapping ngram\n",
    "Now we will attempt to make a count of **overlapping ngrams**. As we only have one sentence of per sample (the \n",
    "claim), we will use a custom sentence to make sure this function works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'socialist teacher south charlotte middle school put message fuck kavanaugh school sign'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "artificial_headline = 'teacher in south charlotte school'\n",
    "#this should result in an overlap count of 5 (because school appears twice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['socialist',\n",
       " 'teacher',\n",
       " 'south',\n",
       " 'charlotte',\n",
       " 'middle',\n",
       " 'school',\n",
       " 'put',\n",
       " 'message',\n",
       " 'fuck',\n",
       " 'kavanaugh',\n",
       " 'school',\n",
       " 'sign']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Slowly work our way towards the end function for sanity's sake\n",
    "#here df.iloc[11][\"claim\"] is the standin for an x in a lambda function\n",
    "[s for s in generate_ngrams(df.iloc[11][\"claim\"],1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0, 1.0, 1.0, 1.0]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#add 1.0 if an ngram in is in the set of ngrams of the artificial headline\n",
    "[1.0 for s in generate_ngrams(df.iloc[11][\"claim\"],1) if s in set(generate_ngrams(artificial_headline,1))] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overlapping ngrams between artificial_headline and test_claim\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Overlapping ngrams between artificial_headline and test_claim\")\n",
    "sum([1.0 for s in generate_ngrams(df.iloc[11][\"claim\"],1) if s in set(generate_ngrams(artificial_headline,1))] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_overlap_ngrams(df,col1,col2,n):\n",
    "    '''\n",
    "    Adds the word_count feature for a desired column in a pandas dataframe.\n",
    "    '''\n",
    "    if not isinstance(col1,str):\n",
    "        raise ValueError('col must be of type str')\n",
    "    \n",
    "    df['overlap_%sgram_%s_%s' %(n,col1,col2)] = \\\n",
    "    list(df.apply(lambda x: sum([1.0 for s in generate_ngrams(x[col1],n) if s in set(generate_ngrams(x[col2],n))]),axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To use this effectively, we will need the dataframe to have some comparison claim. Let's make a row which has the first five words from each claim. This way, the overlap_ngram feature is expected to return a uniform number across every claim (not accounting for double words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'socialist teacher south charlotte'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Join the first four words together using the below operations\n",
    "\" \".join(df.iloc[11].claim.split(\" \")[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pseudo_headline'] = df.apply(lambda x: \" \".join(x[\"claim\"].split(\" \")[:4]),axis=1) #need to set axis=1 to be able to treat x as a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feature_overlap_ngrams(df,\"claim\",\"pseudo_headline\",1)\n",
    "add_feature_overlap_ngrams(df,\"claim\",\"pseudo_headline\",2)\n",
    "add_feature_overlap_ngrams(df,\"claim\",\"pseudo_headline\",3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mission accomplished!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>count_1gram_claim</th>\n",
       "      <th>count_unique_1gram_claim</th>\n",
       "      <th>ratio_unique_1gram_claim</th>\n",
       "      <th>pseudo_headline</th>\n",
       "      <th>overlap_1gram_claim_pseudo_headline</th>\n",
       "      <th>overlap_2gram_claim_pseudo_headline</th>\n",
       "      <th>overlap_3gram_claim_pseudo_headline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line george orwell novel 1984 predict power sm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>line george orwell novel</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>maine legislature candidate leslie</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                claim claimant       date  \\\n",
       "id                                                                          \n",
       "0   line george orwell novel 1984 predict power sm...      NaN 2017-07-17   \n",
       "1   maine legislature candidate leslie gibson insu...      NaN 2018-03-17   \n",
       "\n",
       "    label                  related_articles  count_1gram_claim  \\\n",
       "id                                                               \n",
       "0       0  [122094, 122580, 130685, 134765]                  8   \n",
       "1       2          [106868, 127320, 128060]                 14   \n",
       "\n",
       "    count_unique_1gram_claim  ratio_unique_1gram_claim  \\\n",
       "id                                                       \n",
       "0                          8                       1.0   \n",
       "1                         14                       1.0   \n",
       "\n",
       "                       pseudo_headline  overlap_1gram_claim_pseudo_headline  \\\n",
       "id                                                                            \n",
       "0             line george orwell novel                                  4.0   \n",
       "1   maine legislature candidate leslie                                  4.0   \n",
       "\n",
       "    overlap_2gram_claim_pseudo_headline  overlap_3gram_claim_pseudo_headline  \n",
       "id                                                                            \n",
       "0                                   3.0                                  2.0  \n",
       "1                                   3.0                                  2.0  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of sentences in the claim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "def add_feature_sentence_count(df,col):\n",
    "    '''\n",
    "    Adds the word_count feature for a desired column in a pandas dataframe.\n",
    "    '''\n",
    "    if not isinstance(col,str):\n",
    "        raise ValueError('col must be of type str')\n",
    "    \n",
    "    df['num_sents_%s' %(col)] = df[col].apply(lambda x: len(sent_tokenize(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feature_sentence_count(df,'claim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a list of columns which contain the features we just generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_names = [ n for n in df.columns \\\n",
    "                if \"count\" in n \\\n",
    "                or \"ratio\" in n \\\n",
    "                or \"num_sent\" in n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['count_1gram_claim',\n",
       " 'count_unique_1gram_claim',\n",
       " 'ratio_unique_1gram_claim',\n",
       " 'num_sents_claim']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Basic Count Features to Explore\n",
    "- Count of target words (i.e. if we wish to make a count of known negative, positive, or topical words). Inspired from TALOS's (discarded) 'refute_words' list, which counted words which would indicate a refutation mid-sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Features\n",
    "The primary feature generated through TF-IDF processes is the metric of **cosine similarity** between headline TF-IDF features and body TF-IDF features. Thus the features we create will be `headlineTFIDF`,`bodyTFIDF`, and `similarityTFIDF`\n",
    "\n",
    "Until our dataframe has the articles associated with each claim, we will use the following test corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'if you\\'re from Syria and you\\'re a Christian, you cannot come into this country as a refugee says Trump',\n",
    "    'video shows federal troops in armored vehicles patrolling the streets of Chicago on President Trump\\'s orders.',\n",
    "    'Donald Trump wrote in \\The Art of the Deal\\ that he punched out his second grade music teacher.',\n",
    "    'Actor Denzel Washington said electing President Trump saved the US from becoming an \\\"Orwellian police state.\\\"',\n",
    "    \"President Trump fired longtime White House butler Cecil Gaines for disobedience.\",\n",
    "    'Congress has approved the creation of a taxpayer-funded network called \\\"Trump TV.\\\"',\n",
    "    'The Islamic State \"just built a hotel in Syria\", according to President Donald Trump',\n",
    "    \"A Gucci ensemble worn by Trump counselor Kellyanne Conway to the inauguration closely resembled a 1970s 'Simplicity' pattern.,\"\n",
    "    'Says Melania Trump hired exorcist to \\\"cleanse White House of Obama demons.\\\"',\n",
    "    '\"\\\"Russia, Iran, Syria & many others are not happy\\\" about US troops leaving Syria according to the US President.'\n",
    "    \"In January 2019, President Donald Trump ordered FEMA to stop or cancel funding for its disaster assistance efforts in California.\",\n",
    "    \"Trump looking to open up E Coast & new areas for offshore oil drilling when Congress has passed no new safety standards since BP\",\n",
    "    '\"You were here long before any of us were here, although we have a representative in Congress who, they say, was here a long time ago. They call her Pocahontas.\", said Trump',\n",
    "    \"A photograph shows an elephant carrying a lion cub.\",\n",
    "    \"Elephant carrying thirsty lion cub\",\n",
    "    \"Nike makes their sneakers with elephant skins.\",\n",
    "    \"A photograph shows a jumping baby elephant.\",\n",
    "    \"Is this a video of an elephant trampling a man to death in India?\",\n",
    "    \"The lion used for the original MGM logo killed its trainer and his assistants.\",\n",
    "    \"A friend and I are arguing about the origin of the photo of Donald Trump, Ivanka, and Barron with Barron sitting on the stuffed lion. She swears it's a photo-shopped, fake photo. Do you have any idea who took it or published it first?\",\n",
    "    \"A photograph shows a real baby platypus.\",\n",
    "    \"Photograph shows a drop bear cub being fed human blood.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_preprocess_corpus(corpus,stop_words):\n",
    "    '''\n",
    "    Returns a simply cleaned corpus with basic lemmatization, lower casing, and stopword removal.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus:\n",
    "        An uncleaned corpus of shape [num_documents,]. The 2nd dimension contains the text sample for that document.\n",
    "    stop_words:\n",
    "        An nltk.corpus.stopwords object which contains the desired stopwords to be removed in preprocessing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    corpus:\n",
    "        A corpus of shape [num_docments,] which has been processed through lower casing, stopword removal, lematization.\n",
    "    '''\n",
    "    cleaned_corpus = list()\n",
    "    for i in range(len(corpus)):\n",
    "        # remove symbols and numbers\n",
    "        document = re.sub('[^a-zA-Z]', ' ', corpus[i])\n",
    "        # change to lower case\n",
    "        document = document.lower()\n",
    "        # convert string to a list of strings\n",
    "        document = document.split()    \n",
    "        # remove stopwords and perform lemmatisation\n",
    "        lem = WordNetLemmatizer()\n",
    "        document = [lem.lemmatize(word) for word in document if (not word in  \n",
    "                stop_words) and (len(word) > 1)]\n",
    "        cleaned_corpus.append(document)\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "corpus = basic_preprocess_corpus(corpus,stop_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will create a dictionary which has the keys as the term, and the values is a list of documents which it is found in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_dict(corpus, threshold=False):\n",
    "    '''\n",
    "    Returns the term dictionary of a corpus.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    corpus:\n",
    "        A text corpus of shape [num_documents,1]. The 2nd dimension contains the text sample for that document.\n",
    "    threshold:\n",
    "        The minimum number of documents a term must appear in for it to be added to the term dictionary.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    term_dict:\n",
    "        A dictionary for which the key stores all unique terms, and the value for each key is a list of \n",
    "        indices for the documents in the corpus which contain the term. Duplicate indices indicate \n",
    "        multiple appearances within the same document.\n",
    "    '''\n",
    "    term_dict = {}\n",
    "    for idx, document in enumerate(corpus):\n",
    "        for term in document:\n",
    "            if term in set(term_dict.keys()): #if the term is already in the keys, append\n",
    "                term_dict[term].append(idx)\n",
    "            else: #otherwise, add new key\n",
    "                term_dict[term] = [idx]\n",
    "    \n",
    "    #automatically remove terms which don't appear in 'threshold' documents\n",
    "    if threshold:\n",
    "        for term in list(term_dict): #list allows us to change dictionary in iteration\n",
    "            if len(term_dict[term]) <= threshold:\n",
    "                term_dict.pop(term) #delete that term\n",
    "    return term_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Term dictionary with single appearance terms removed\n",
      "\n",
      "{'syria': [0, 6, 8, 8], 'christian': [0], 'cannot': [0], 'come': [0], 'country': [0], 'refugee': [0], 'say': [0, 7, 10], 'trump': [0, 1, 2, 3, 4, 5, 6, 7, 7, 8, 9, 10, 17], 'video': [1, 15], 'show': [1, 11, 14, 18, 19], 'federal': [1], 'troop': [1, 8], 'armored': [1], 'vehicle': [1], 'patrolling': [1], 'street': [1], 'chicago': [1], 'president': [1, 3, 4, 6, 8, 8], 'order': [1], 'donald': [2, 6, 8, 17], 'wrote': [2], 'art': [2], 'deal': [2], 'punched': [2], 'second': [2], 'grade': [2], 'music': [2], 'teacher': [2], 'actor': [3], 'denzel': [3], 'washington': [3], 'said': [3, 10], 'electing': [3], 'saved': [3], 'u': [3, 8, 8, 10], 'becoming': [3], 'orwellian': [3], 'police': [3], 'state': [3, 6], 'fired': [4], 'longtime': [4], 'white': [4, 7], 'house': [4, 7], 'butler': [4], 'cecil': [4], 'gaines': [4], 'disobedience': [4], 'congress': [5, 9, 10], 'approved': [5], 'creation': [5], 'taxpayer': [5], 'funded': [5], 'network': [5], 'called': [5], 'tv': [5], 'islamic': [6], 'built': [6], 'hotel': [6], 'according': [6, 8], 'gucci': [7], 'ensemble': [7], 'worn': [7], 'counselor': [7], 'kellyanne': [7], 'conway': [7], 'inauguration': [7], 'closely': [7], 'resembled': [7], 'simplicity': [7], 'pattern': [7], 'melania': [7], 'hired': [7], 'exorcist': [7], 'cleanse': [7], 'obama': [7], 'demon': [7], 'russia': [8], 'iran': [8], 'many': [8], 'others': [8], 'happy': [8], 'leaving': [8], 'january': [8], 'ordered': [8], 'fema': [8], 'stop': [8], 'cancel': [8], 'funding': [8], 'disaster': [8], 'assistance': [8], 'effort': [8], 'california': [8], 'looking': [9], 'open': [9], 'coast': [9], 'new': [9, 9], 'area': [9], 'offshore': [9], 'oil': [9], 'drilling': [9], 'passed': [9], 'safety': [9], 'standard': [9], 'since': [9], 'bp': [9], 'long': [10, 10], 'although': [10], 'representative': [10], 'time': [10], 'ago': [10], 'call': [10], 'pocahontas': [10], 'photograph': [11, 14, 18, 19], 'elephant': [11, 12, 13, 14, 15], 'carrying': [11, 12], 'lion': [11, 12, 16, 17], 'cub': [11, 12, 19], 'thirsty': [12], 'nike': [13], 'make': [13], 'sneaker': [13], 'skin': [13], 'jumping': [14], 'baby': [14, 18], 'trampling': [15], 'man': [15], 'death': [15], 'india': [15], 'used': [16], 'original': [16], 'mgm': [16], 'logo': [16], 'killed': [16], 'trainer': [16], 'assistant': [16], 'friend': [17], 'arguing': [17], 'origin': [17], 'photo': [17, 17, 17], 'ivanka': [17], 'barron': [17, 17], 'sitting': [17], 'stuffed': [17], 'swears': [17], 'shopped': [17], 'fake': [17], 'idea': [17], 'took': [17], 'published': [17], 'first': [17], 'real': [18], 'platypus': [18], 'drop': [19], 'bear': [19], 'fed': [19], 'human': [19], 'blood': [19]}\n"
     ]
    }
   ],
   "source": [
    "term_dict = create_term_dict(corpus)\n",
    "print('Term dictionary with single appearance terms removed\\n')\n",
    "print(term_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that words which appear twice in the same document appear in `term_dict` as indices with repeated numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_term_doc_matrix(term_dict,corpus):\n",
    "    '''\n",
    "    Returns the term-document matrix of a corpus of shape [num_documents,].\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    term_dict:\n",
    "        The term dictionary of corpus.\n",
    "    corpus:\n",
    "         The corpus of text from which the term_dictionary has been made.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    term_doc:\n",
    "        A term-document matrix of shape [num_documents, num_terms] with \n",
    "        (rows, columns) corresponding to (documents, terms). Values at the [i,j]th\n",
    "        index indicate the number of times term j appears in document i.   \n",
    "    '''\n",
    "    A = np.zeros([len(corpus),len(term_dict)]) #rows x col = doc x terms\n",
    "    for idx, term in enumerate(term_dict):\n",
    "        for d in term_dict[term]:\n",
    "            A[d,idx] += 1 \n",
    "    return np.asarray(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 157)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc = create_term_doc_matrix(term_dict,corpus)\n",
    "term_doc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_matrix(term_doc):\n",
    "    '''\n",
    "    Returns the term-frequency-inverse-document-frequency matrix of a term-document matrix.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    term_doc:\n",
    "        The term-document matrix of a corpus of words. \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tfidf_matrix:\n",
    "        A matrix of tf-idf values for each term-document relationship with\n",
    "        (rows, columns) corresponding to (documents, terms). \n",
    "    '''\n",
    "    col_sums = np.sum(term_doc, axis=0)\n",
    "    A = np.zeros(term_doc.shape)\n",
    "    B = np.copy(term_doc)\n",
    "    for i in range(term_doc.shape[0]):\n",
    "        for j in range(term_doc.shape[1]):\n",
    "            term_count = col_sums[j] #number of terms in the document j\n",
    "            tf = B[i,j] / term_count #divide all rows by the frequency per term\n",
    "            \n",
    "            row_i = list(B[i]) \n",
    "            row_i = [d for d in row_i if d>0] #filter out docs that don't have the term\n",
    "            nt = len(row_i) #nt is the number of documents which have the term\n",
    "            \n",
    "            idf = log(float(term_doc.shape[1])) / nt\n",
    "            A[i,j] = tf*idf\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_mat = tfidf_matrix(term_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.15800768, 0.63203073, 0.63203073, ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
       "        0.        ],\n",
       "       [0.        , 0.        , 0.        , ..., 0.63203073, 0.63203073,\n",
       "        0.63203073]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use SVD to Create a Low-Rank Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-**n_components**: the desired dimensionality of the output data\n",
    "-**n_iter**: the number of iteratoins for randomized SVD solver to take. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=50, n_iter=15,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=50, n_iter=15)\n",
    "svd.fit(tfidf_mat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20, 20)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svd_output= svd.transform(tfidf_mat)\n",
    "svd_output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, assuming that we have a second corpus of text to perform this pipeline on, we can return the following feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(A,B):\n",
    "    '''\n",
    "    Return the cosine similarity of two matrices.\n",
    "    '''\n",
    "    return cosine_similarity(A,B)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the case in which we have many svd outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_outputs = [svd_output for i in range(5)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998, 0.9999999999999998]\n"
     ]
    }
   ],
   "source": [
    "svd_similarities = list(map(cos_sim,svd_outputs,svd_outputs))\n",
    "print(svd_similarities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sklearn Library TfidfVectorizer\n",
    "Now that we have explored the creation of the SVD similarity feature on some toy data, let's implement it on our claims and pseudo_headlines. Here, we will use the big guns and use sklearns `TfidfVectorizer` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,3),max_df=0.8,min_df=2)\n",
    "vectorizer.fit(df['claim']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is the mapping of terms to feature vector indices\n",
    "vocabulary = vectorizer.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "claim_vectorizer = TfidfVectorizer(ngram_range=(1,3),max_df=0.8,\n",
    "                                   min_df=2, vocabulary=vocabulary)\n",
    "claims_tfidf = claim_vectorizer.fit_transform(df['claim'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_vectorizer = TfidfVectorizer(ngram_range=(1, 3), max_df=0.8,\n",
    "                                      min_df=2, vocabulary=vocabulary)\n",
    "headline_tfidf= headline_vectorizer.fit_transform(df['pseudo_headline'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_tfidf = list(map(cos_sim, claims_tfidf,headline_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15555,)\n"
     ]
    }
   ],
   "source": [
    "print(np.squeeze(sim_tfidf).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that we have one similarity score per entry!\n",
    "\n",
    "### Functionise TFIDF-SVD Cosine Similarity Feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_tfidf_svd_similarity(df,col1,col2,vec):\n",
    "    '''\n",
    "    Adds the cosine similarity feature for the tfidf matrices of each column\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        The dataframe which the row is added to.\n",
    "    col1:\n",
    "        The first column of text for the similarity measurements. \n",
    "    col2:\n",
    "        The second column of text for the similarity measurements.\n",
    "    n:\n",
    "        The ngram number to include in the similarity comparison.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    N/A \n",
    "    '''\n",
    "    vec.fit(np.add(df[col1].values,df[col2].values)) #train on all text contained in the two columns\n",
    "    vocabulary = vectorizer.vocabulary_ \n",
    "    \n",
    "    new_vec= TfidfVectorizer(ngram_range=vec.ngram_range,max_df=vec.max_df,\n",
    "                            min_df=vec.min_df,vocabulary=vec.vocabulary_)\n",
    "    tfidf1 = new_vec.fit_transform(df[col1])\n",
    "    new_vec= TfidfVectorizer(ngram_range=vec.ngram_range,max_df=vec.max_df,\n",
    "                            min_df=vec.min_df,vocabulary=vec.vocabulary_)\n",
    "    tfidf2 = new_vec.fit_transform(df[col2])\n",
    "    \n",
    "    df['sim_tfidf_%s_to_%sgram_%s_%s' %(vec.ngram_range[0],vec.ngram_range[1],col1,col2)] = list(map(cos_sim, claims_tfidf,headline_tfidf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1,3),max_df=0.8,min_df=2)\n",
    "add_feature_tfidf_svd_similarity(df,'claim','pseudo_headline',vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>claim</th>\n",
       "      <th>claimant</th>\n",
       "      <th>date</th>\n",
       "      <th>label</th>\n",
       "      <th>related_articles</th>\n",
       "      <th>count_1gram_claim</th>\n",
       "      <th>count_unique_1gram_claim</th>\n",
       "      <th>ratio_unique_1gram_claim</th>\n",
       "      <th>pseudo_headline</th>\n",
       "      <th>overlap_1gram_claim_pseudo_headline</th>\n",
       "      <th>overlap_2gram_claim_pseudo_headline</th>\n",
       "      <th>overlap_3gram_claim_pseudo_headline</th>\n",
       "      <th>num_sents_claim</th>\n",
       "      <th>sim_tfidf_1_to_3gram_claim_pseudo_headline</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>line george orwell novel 1984 predict power sm...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2017-07-17</td>\n",
       "      <td>0</td>\n",
       "      <td>[122094, 122580, 130685, 134765]</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1.0</td>\n",
       "      <td>line george orwell novel</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.708021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maine legislature candidate leslie gibson insu...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2018-03-17</td>\n",
       "      <td>2</td>\n",
       "      <td>[106868, 127320, 128060]</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1.0</td>\n",
       "      <td>maine legislature candidate leslie</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.349062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                claim claimant       date  \\\n",
       "id                                                                          \n",
       "0   line george orwell novel 1984 predict power sm...      NaN 2017-07-17   \n",
       "1   maine legislature candidate leslie gibson insu...      NaN 2018-03-17   \n",
       "\n",
       "    label                  related_articles  count_1gram_claim  \\\n",
       "id                                                               \n",
       "0       0  [122094, 122580, 130685, 134765]                  8   \n",
       "1       2          [106868, 127320, 128060]                 14   \n",
       "\n",
       "    count_unique_1gram_claim  ratio_unique_1gram_claim  \\\n",
       "id                                                       \n",
       "0                          8                       1.0   \n",
       "1                         14                       1.0   \n",
       "\n",
       "                       pseudo_headline  overlap_1gram_claim_pseudo_headline  \\\n",
       "id                                                                            \n",
       "0             line george orwell novel                                  4.0   \n",
       "1   maine legislature candidate leslie                                  4.0   \n",
       "\n",
       "    overlap_2gram_claim_pseudo_headline  overlap_3gram_claim_pseudo_headline  \\\n",
       "id                                                                             \n",
       "0                                   3.0                                  2.0   \n",
       "1                                   3.0                                  2.0   \n",
       "\n",
       "    num_sents_claim  sim_tfidf_1_to_3gram_claim_pseudo_headline  \n",
       "id                                                               \n",
       "0                 1                                    0.708021  \n",
       "1                 1                                    0.349062  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if our function worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.subtract(sim_tfidf,df.sim_tfidf_1_to_3gram_claim_pseudo_headline.values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The difference between our function and the step-by-step process is the exact same AKA SUCCESS!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec Features\n",
    "Now we will generate the **Word2Vec features** as indicated by the Talos team. They used pre-trained word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Cooper/anaconda3/envs/home/lib/python3.7/site-packages/smart_open/smart_open_lib.py:398: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['pseudo_headline_unigram_vec'] = \\\n",
    "df['pseudo_headline'].map(lambda x: generate_ngrams(x,1))\n",
    "df['claim_unigram_vec'] = \\\n",
    "df['claim'].map(lambda x: generate_ngrams(x,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `functools.reduce` tool is used to apply a function to a sequence of elements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sum of the list elements is : 15\n",
      "The maximum element of the list is : 5\n"
     ]
    }
   ],
   "source": [
    "from functools import reduce\n",
    "lis = [1,2,3,4,5]\n",
    "\n",
    "# using reduce to compute sum of list \n",
    "print (\"The sum of the list elements is : \",end=\"\") \n",
    "print (reduce(lambda a,b : a+b,lis)) \n",
    "\n",
    "#using reduce to compute maximum element from list \n",
    "print (\"The maximum element of the list is : \",end=\"\") \n",
    "print (reduce(lambda a,b : a if a > b else b,lis)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "headline_unigram_arr = df.pseudo_headline_unigram_vec.values\n",
    "claim_unigram_arr = df.claim_unigram_vec.values\n",
    "headline_vecs = map(lambda x: reduce(np.add, [model[gram] for gram in x if gram in model], [0.]*300), headline_unigram_arr)\n",
    "claim_vecs = map(lambda x: reduce(np.add, [model[gram] for gram in x if gram in model], [0.]*300), claim_unigram_arr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to break down what is happening in our `reduce` line as there is A LOT going on.\n",
    "1. We are mapping (i.e. applying) a lambda function to each element of the `claim_unigram_arr`. Thus the lambda function acts on each claim's list of unigrams.\n",
    "2. The function which is applied is an `np.add` function, which adds vectors element-wise. What is it adding? It is summing the list of vectors generated by the list comprehension [model[y] for y in x if y in model]\n",
    "3. What is this list comprehension doing? It is applying the model to each 'unigram' in 'x' (which is the list of total unigrams for that claim). Thus this list comprehension returns a list of vector embeddings for each unigram in that claim's unigram list.\n",
    "4. What does the presence of `[0.]*300` mean? It means we are `np.add`-ing the results of summing the list of vector embedded unigrams to a 300-dimensional array of zeros.\n",
    "\n",
    "SO! What can we conclude? The output of this gloriously pythonic sentence is a list which has a vector embedding for each claim - created by adding up the individual vectors for each unigram found in the claim.\n",
    "\n",
    "Now, what do we do with these?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of headline vectors: (15555, 300)\n",
      "Shape of claim vectors: (15555, 300)\n"
     ]
    }
   ],
   "source": [
    "headline_vecs = list(headline_vecs)\n",
    "claim_vecs = list(claim_vecs)\n",
    "print('Shape of headline vectors:',np.asarray(headline_vecs).shape)\n",
    "print('Shape of claim vectors:',np.asarray(claim_vecs).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are able to compute the cosine similarity between the headline and body Word2Vec features. We will get the cosine similarity for each of our 15K rows.\n",
    "\n",
    "After much (much!) struggle and iteration, it has been determined that we must pass our vectors in with the shape (num_samples, 1, dimension_of_vector). WHY? Well thanks for asking. The `cosine_similarity` function requires that single samples be passed in as a 2-D vector of shape (1, dimensions). When we use a neat pythonic tool such as the `map(func, input_list_a, input_list_b)`, we are returned a list of the function applied to the pairwise inputs from each list. In other words, we receive a list of the function applied to inputs of the same indices.\n",
    "\n",
    "THUS, we must make sure that each 'input' in input list is of the appropriate shape, which is in the shape (1, dimensions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "vecs_h = [np.reshape(x,newshape=[1,-1]) for x in headline_vecs]\n",
    "vecs_c =[np.reshape(x,newshape=[1,-1]) for x in claim_vecs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_sims = np.squeeze(list(map(cos_sim, vecs_h,vecs_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.71542829 0.74194606 0.65782847 ... 0.8944385  0.49021315 0.79096662]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_sims)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Future work:\n",
    "- Similarity metrics for different word2vec models!\n",
    "\n",
    "### Functionise Word2Vec Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_w2v_similarity(df,col1,col2,model,n):\n",
    "    '''\n",
    "    Adds the Word2Vec cosine similarity feature to the dataframe between two specific columns.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        The dataframe which the row is added to.\n",
    "    col1:\n",
    "        The first column of text for the similarity measurements. \n",
    "    col2:\n",
    "        The second column of text for the similarity measurements.\n",
    "    model:\n",
    "        A gensim model loaded with Word2Vec embeddings.\n",
    "    n:\n",
    "        The ngram number to include in the similarity comparison.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    N/A \n",
    "    '''\n",
    "    gram_list1 = df[col1].map(lambda x: generate_ngrams(x,n)).values\n",
    "    gram_list2 = df[col2].map(lambda x: generate_ngrams(x,n)).values\n",
    "    \n",
    "    vecs1 = map(lambda x: reduce(np.add, [model[gram] for gram in x if gram in model], [0.]*300), gram_list1)\n",
    "    vecs2 = map(lambda x: reduce(np.add, [model[gram] for gram in x if gram in model], [0.]*300), gram_list2)\n",
    "    #get values from map by calling the generator in a list\n",
    "    vecs1 = list(vecs1)\n",
    "    vecs2 = list(vecs2)\n",
    "    #shape into necessary form\n",
    "    vecs1 = [np.reshape(x,newshape=[1,-1]) for x in vecs1]\n",
    "    vecs2 =[np.reshape(x,newshape=[1,-1]) for x in vecs2]\n",
    "    df['sim_w2v_%s_%s' %(col1,col2)] = w2v_sims = np.squeeze(list(map(cos_sim, vecs_h,vecs_c)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feature_w2v_similarity(df,'claim','pseudo_headline',model,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "id\n",
       "0    0.715428\n",
       "1    0.741946\n",
       "4    0.657828\n",
       "5    0.420630\n",
       "6    0.741683\n",
       "Name: sim_w2v_claim_pseudo_headline, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sim_w2v_claim_pseudo_headline.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_sentiment(sentences,average=False):\n",
    "    '''\n",
    "    Computes the sentiment of the sentences with nltk SentimentIntensityAnalyzer().\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sentences:\n",
    "        Either a list of single sentences or a list of sentence lists, where each inner list is\n",
    "        composed of all the sentences tokenized per document.\n",
    "    average:\n",
    "        A binary True/False value of whether or not to average the sentiment.\n",
    "        Sentiment should be averaged if your sentences parameter is a list of lists.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A dataframe of the resulting sentiments.\n",
    "    '''\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        vs = sid.polarity_scores(sentence)\n",
    "        result.append(vs)\n",
    "    if average:\n",
    "        return pd.DataFrame(result).mean()\n",
    "    else:\n",
    "        return pd.DataFrame(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>compound</th>\n",
       "      <th>neg</th>\n",
       "      <th>neu</th>\n",
       "      <th>pos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.4939</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.9022</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   compound    neg    neu    pos\n",
       "0    0.3182  0.000  0.753  0.247\n",
       "1   -0.4939  0.297  0.573  0.130\n",
       "2    0.0000  0.000  1.000  0.000\n",
       "3    0.0000  0.000  1.000  0.000\n",
       "4   -0.9022  0.552  0.448  0.000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sents = compute_sentiment(df.claim.values)\n",
    "sents.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_feature_sentiment_sid(df,col,avg=False):\n",
    "    '''\n",
    "    Adds features from the sentiment of the sentences as processed by nltk SentimentIntensityAnalyzer().\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df:\n",
    "        The dataframe which the row is added to.\n",
    "    col:\n",
    "        The column of text to be analyzed. \n",
    "    avg:\n",
    "        A binary True/False value of whether or not to average the sentiment.\n",
    "        Sentiment should be averaged if your sentences parameter is a list of lists.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    N/A\n",
    "    '''\n",
    "    sentences = df[col].values\n",
    "    result = list()\n",
    "    for sentence in sentences:\n",
    "        vs = sid.polarity_scores(sentence)\n",
    "        result.append(vs)\n",
    "    if avg:\n",
    "        df2 = pd.DataFrame(result).mean()\n",
    "        columns = ['sent_'+ x + '_' + col for x in df2.columns]\n",
    "        df2.columns = columns\n",
    "        for key in df2:\n",
    "            df[key] = df2[key]\n",
    "    else:\n",
    "        df2 = pd.DataFrame(result)\n",
    "        columns = ['sent_'+ x + '_' + col for x in df2.columns]\n",
    "        df2.columns = columns\n",
    "        for key in df2:\n",
    "            df[key] = df2[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_feature_sentiment_sid(df,'claim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent_compound_claim</th>\n",
       "      <th>sent_neg_claim</th>\n",
       "      <th>sent_neu_claim</th>\n",
       "      <th>sent_pos_claim</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.3182</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.753</td>\n",
       "      <td>0.247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.4939</td>\n",
       "      <td>0.297</td>\n",
       "      <td>0.573</td>\n",
       "      <td>0.130</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.9022</td>\n",
       "      <td>0.552</td>\n",
       "      <td>0.448</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>-0.9171</td>\n",
       "      <td>0.443</td>\n",
       "      <td>0.557</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>-0.4767</td>\n",
       "      <td>0.341</td>\n",
       "      <td>0.659</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sent_compound_claim  sent_neg_claim  sent_neu_claim  sent_pos_claim\n",
       "id                                                                     \n",
       "0                0.3182           0.000           0.753           0.247\n",
       "1               -0.4939           0.297           0.573           0.130\n",
       "4               -0.9022           0.552           0.448           0.000\n",
       "5               -0.9171           0.443           0.557           0.000\n",
       "6               -0.4767           0.341           0.659           0.000"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[[col for col in df.columns if 'sent_' in col]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Everything matches, we're good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
