Semantic Video CNNs Through Representation Warping   Semantic Video CNNs through Representation Warping  Raghudeep GaddeN,N, Varun JampaniN,N and Peter V
GehlerN,N,N  NMPI for Intelligent Systems, NUniversity of Würzburg NBernstein Center for Computational Neuroscience, NNVIDIA  {raghudeep.gadde,varun.jampani,peter.gehler}@tuebingen.mpg.de  Abstract  In this work, we propose a technique to convert CNN  models for semantic segmentation of static images into  CNNs for video data
We describe a warping method that  can be used to augment existing architectures with very little extra computational cost
This module is called NetWarp and we demonstrate its use for a range of network  architectures
The main design principle is to use optical flow of adjacent frames for warping internal network  representations across time
A key insight of this work is  that fast optical flow methods can be combined with many  different CNN architectures for improved performance and  end-to-end training
Experiments validate that the proposed  approach incurs only little extra computational cost, while  improving performance, when video streams are available
 We achieve new state-of-the-art results on the CamVid and  Cityscapes benchmark datasets and show consistent improvements over different baseline networks
Our code and  models are available at http://segmentation.is
 tue.mpg.de  N
Introduction  It is fair to say that the empirical performance of semantic image segmentation techniques has seen dramatic improvement in the recent years with the onset of Convolutional Neural Network (CNN) methods
The driver of this  development have been large image segmentation datasets  and the natural next challenge is to develop fast and accurate  video segmentation methods
 The number of proposed CNN models for semantic image segmentation by far outnumbers those for video data
A  naive way to use a single image CNN for video is to apply it  frame-by-frame, effectively ignoring the temporal information altogether
However, frame-by-frame application often  yields to jittering across frames, especially at object boundaries
Alternative approaches include the use of conditional  random field (CRF) models on video data to fuse the predicted label information across frames or the development  of tailored CNN architectures for videos
A separate CRF  applied to the CNN predictions has the limitation, that it  has no access to internal representations of the CNNs
Thus  the CRF operates on a representations (the labels) that has  already been condensed
Furthermore, existing CRFs for  video data are often too slow for practical purposes
 We aim to develop a video segmentation technique that  makes use of temporal coherence in video frames and reuse strong single image segmentation CNNs
For this, we  propose a conceptually simple approach to convert existing  image CNNs into video CNNs that uses only very little extra computational resources
We achieve this by ‘NetWarp’,  a neural network module that warps the intermediate CNN  representations of the previous frame to the corresponding  representations of the current frame
Specifically, the NetWarp module uses the optical flow between two adjacent  frames and then learns to transform the intermediate CNN  representations through an extra set of operations
Multiple NetWarp modules can be used at different layers of the  CNN hierarchies to warp deep intermediate representations  across time, as depicted in Fig
N
 Our implementation of NetWarp takes only about N.N  milliseconds to process an intermediate CNN representation of NNN × NNN with N0NN feature channels
It is fully differentiable and can be learned using standard back propagation techniques during training of the entire CNN network
In addition, the resulting video CNN model with  NetWarp modules processes the frames in an online fashion, i.e., the system has access only to the present and previous frames when predicting the segmentation of the present  frame
 We augmented several existing state-of-the-art image  segmentation CNNs using NetWarp
On the current standard video segmentation benchmarks of CamVid [N] and  Cityscapes [N], we consistently observe performance improvements in comparison to base network that is applied  in a frame-by-frame mode
Our video CNNs also outperformed other recently proposed (CRF-)architectures and  video propagation techniques setting up a new state-of-theart on both CamVid and Cityscapes datasets
 In Section N, we discuss the related works on video segmentation
In Section N, we describe the NetWarp module  and how it is used to convert image CNNs into video CNNs
 In Section N, experiments on CamVid and Cityscapes are  presented
We conclude with a discussion in Section N
 NNNNN  http://segmentation.is.tue.mpg.de http://segmentation.is.tue.mpg.de   Frame(t−N) Segmentation(t−N)  Image Segmentation CNN  Framet Segmentationt  Net Wa  rpa  Net Wa  rpb  Net Wa  rpc  Image Segmentation CNN  Figure N
Schematic of the proposed video CNN with NetWarp modules
This illustration depicts the use of NetWarp modules in three  different layers of a image CNN
The video CNN is applied in an online fashion, looking back only one frame
The CNN filter activations  for the current frame are modified by the corresponding representations of the previous frame via NetWarp modules
 N
Related Works  We limit our discussion of the literature on semantic  segmentation to those works concerning the video data
 Most semantic video segmentation approaches implement  the strategy to first obtain a single frame predictions using a  classifier such as random forest or CNN, and then propagate  this information using CRFs or filtering techniques to make  the result temporally more consistent
 One possibility to address semantic video segmentation  is by means of the ND scene structure
Some works [N, NN,  NN] build models that use ND point clouds that have been  obtained with structure from motion
Based on these geometrical and/or motion features, semantic segmentation is  improved
More recent works [NN, NN] propose the joint  estimation of ND semantics and ND reconstruction of the  scenes from the video data
While ND information is very  informative, it is also costly to obtain and comes with prediction errors that are hard to recover from
 A more popular route [N0, N, N, NN, NN, NN, NN] is to construct large graphical models that connect different video  pixels to achieve temporal consistency across frames
The  work of [N] proposes a Perturb-and-MAP random field  model with spatio-temporal energy terms based on Potts  model
[N] used dynamic temporal links between the frames  but optimizes for a ND CRF with temporal energy terms
A  ND dense CRF across video frames is constructed in [NN]  and optimized using mean-field approximate inference
The  work of [NN] proposed a joint model for predicting semantic labels for supervoxels, object tracking and geometric relationship between the objects
Recently, [NN] proposed a  technique for optimizing the feature spaces for ND dense  CRF across video pixels
The resulting CRF model is applied on top of the unary predictions obtained with CNN  or some other techniques
In [NN], a joint model to estimate both optical flow and semantic segmentation is designed
[NN] proposed a CRF model and an effiecient inference technique to fuse CNN unaries with long range spatiotemporal cues estimated by recurrent temporal restricted  Boltzmann machine
We avoid the CRF construction and  filter the intermediate CNN representations directly
This  results in fast runtime and a natural way to train any augmented model by means of gradient descent
 More related to our technique are fast filtering techniques
For example, [NN] learns a similarity function between pixels of consecutive frames to propagate predictions  across time
The approach of [NN] implements a neural network that uses learnable bilateral filters [NN] for long-range  propagation of information across video frames
These filtering techniques propagate information after the semantic  labels are computed for each frame, whereas in contrast, our  approach does filtering based propagation across intermediate CNN representations making it more integrated into  CNN training
 The use of CNNs (e.g., [NN, N]) resulted in a surge of performance in semantic segmentation
But, most CNN techniques work on single images
The authors of [NN] observed  that the semantics change slowly across time and re-use  some intermediate representations from the previous frames  while computing segmentation for the present frame
This  results in faster runtime but a loss in accuracy
In contrast,  our approach uses adjacent frame deep representations for  consistent predictions across frames resulting in improved  prediction accuracy
 Although several works proposed neural network approaches for processing several video frames together, they  are mostly confined to video level tasks such as classificaNNNN    NetWarpFrame(t−N)  Compute Flow  Transform Flow  Warp Combine  CNNN···k z k  (t−N)  Framet  CNNN···k z k t  CNNk+N···end  Segmentationt  Ft Λ(Ft) ẑ k (t−N)  z̃ k t  Figure N
Illustration of computations in a NetWarp module
First, optical flow Ft is computed between two video frames at time steps t  and t−N
Then the NetWarp module transforms the flow Λ(Ft) with few convolutional layers; warps the activations z k  (t−N) of the previous  frame and and combines the warped representations with those of the present frame zkt 
The resulting representation z̃ k  t is then passed onto  the remaining CNN layers for semantic segmentation
 tion or captioning
The works of [N0, NN] use ND convolutions across frames for action recognition
In [N], LSTMs  are used in a recurrent network for recognition and captioning
Two stream optical flow and image CNNs [N0, NN, NN]  are among the state-of-the-art approaches for visual action  recognition
Unlike video level tasks, pixel-level semantic video segmentation requires filtering at pixel-level
This  work proposes a way of doing local information propagation across video frames
 A related task to semantic video segmentation is video  object segmentation
Like in semantic video segmentation literature, several works [NN, N0, NN, NN, NN] aim to  reduce the complexity of graphical model structure with  spatio-temporal superpixels
Some other works use nearest  neighbor fields [NN] or optical flow [N] for estimating correspondence between different frame pixels
These works  use pixel correspondences across frames to refine or propagate labels, whereas the proposed approach refines the intermediate CNN representations with a module that is easy  to integrate into current CNN frameworks
 N
Warping Image CNNs to Video CNNs  Our aim is to convert a given CNN network architecture, designed to work on single images into a segmentation  CNN for video data
Formally, given a sequence of n video frames denoted as IN, IN, · · · , In, the task is to predict se- mantic segmentation for every video frame
Our aim is to  process the video frames online, i.e., the system has access  only to previous frames when predicting the segmentation  of the present frame
 The main building block will be the NetWarp module  that warps the intermediate (kth layer) CNN representations zkt−N of the previous frame and then combines with those  of the present frame zkt , where z N t , z  N t , · · · , z  m t denote the  intermediate representations of a given image CNN with m layers
 Motivation The design of the NetWarp module is based  on two specific insights from the recent semantic segmentation literature
The authors of [NN] showed that intermediate CNN representations change only slowly over adjacent  frames, especially for deeper CNN layers
This inspired the  design of the clockwork convnet architecture [NN]
In [NN],  a bilateral inception module is constructed to average intermediate CNN representations for locations across the image  that are spatially and photometrically close
There, the authors use super-pixels based on runtime considerations and  demonstrated improved segmentation results when applied  to different CNN architectures
Given these findings, in this  work, we ask the question: Does the combination of temporally close representations also leads to more stable and  consistent semantic predictions?  We find a positive answer to this question
Using pixel  correspondences, provided by optical flow, to combine intermediate CNN representations of adjacent frames consistently improves semantic predictions for a number of CNN  architectures
Especially at object boundaries and thin object structures, we observe a solid improvement
Further,  this warping can be performed at different layers in CNN  architectures, as illustrated in Fig
N and incurs only a tiny  extra computation cost to the entire pipeline
 N.N
NetWarp  The NetWarp module consists of multiple separate steps,  a flowchart overview is depicted in Fig
N
It takes as input,  an estimate of dense optical flow field and then performs  N
flow transformation, N
representation warping, and N
 combination of representations
In the following, we will  first discuss the optical flow computation followed by the  NNNN    description of each of the three separate steps
 Flow Computation We use existing optical flow algorithms to obtain dense pixel correspondences (denoted as  Ft) across frames
We chose a particular fast optical flow method to keep the runtime small
We found that DISFlow [NN], which takes only about Nms to compute flow  per image pair (with size NN0 × NN0) on a CPU, works well for our purpose
Additionally, we experimented with the  more accurate but slower FlowFields [N] method that requires several seconds per image pair to compute flow
Formally, given an image pair, It and I(t−N), the optical flow algorithm computes the pixel displacement (u, v) for every pixel location (x, y) in It to the spatial locations (x  ′, y′) in I(t−N)
That is, (x  ′, y′) = (x + u, y + v)
u and v are floating point numbers and represent pixel displacements  in horizontal and vertical directions respectively
Note that  we compute the reverse flow mapping the present frame locations to locations in previous frame as we want to warp  previous frame representations
 Flow Transformation Correspondences obtained with  traditional optical flow methods might not be optimal for  propagating representations across video frames
So, we  use a small CNN to transform the pre-computed optical  flow, which we will refer to as FlowCNN and denote the  transformation as Λ(Ft)
We concatenate the original two channel flow, the previous and present frame images, and  the difference of the two frames
This results in a NN channel tensor as an input to the FlowCNN
The network itself  is composed of N convolution layers interleaved with ReLU  nonlinearities
All the convolution layers are made up of N  × N filters and the number of output channels for the first N layers are NN, NN and N respectively
The output of the third  layer is then concatenated (skip connection) with the original pre-computed flow which is then passed on to the last N  × N convolution layer to obtain final transformed flow
This network architecture is loosely inspired from the residual  blocks in ResNet [NN] architectures
Other network architectures are conceivable
All parameters of FlowCNN are  learned via standard back-propagation
Learning is done on  semantic segmentation only and we do not include any supervised flow data as we are mainly interested in semantic  video segmentation
Figure N in the experimental section  shows how the flow transforms with the FlowCNN
We observe significant transformations in the original flow with  the FlowCNN and we will discuss more about these changes  in the experimental section
 Warping Representations The FlowCNN transforms a  dense correspondence field from frame It to the previous frame I(t−N)
Let us assume that we want to apply the NetWarp module on the kth layer of the image CNN and the filter activations for the adjacent frames are zkt and z  k  (t−N)  (as in Fig N)
For notational convenience, we drop k and  refer to these as zt and z(t−N) respectively
The representations of the previous frame z(t−N) are warped to align with  the corresponding present frame representations:  ẑ(t−N) = Warp(z(t−N),Λ(Ft)), (N)  where ẑ(t−N) denotes the warped representations, Ft is the dense correspondence field and Λ(·) represents the FlowCNN described above
Lets say we want to compute  the warped representations ẑ(t−N) at a present frame’s pixel  location (x, y) which is mapped to the location (x′, y′) in the previous frame by the transformed flow
We implement Warp as a bilinear interpolation of z(t−N) at the de- sired points (x′, y′)
Let (xN, yN), (xN, yN), (xN, yN) and (xN, yN) be the corner points of the previous frame’s grid cell where (x′, y′) falls
Then the warping of z(t−N) to ob- tain ẑ(t−N)(x, y) is given as standard bilinear interpolation:  ẑ(t−N)(x, y) = z(t−N)(x ′, y′)  = N  η  [ xN − x  ′  x′ − xN  ]⊤ [ z(t−N)(xN, yN) z(t−N)(xN, yN) z(t−N)(xN, yN) z(t−N)(xN, yN)  ] [ yN − y  ′  y′ − yN  ]  (N)  where η = N/(xN − xN)(yN − yN)
In case (x ′, y′) lies outside the spatial domain of z(t−N), we back-project (x ′, y′)  to the nearest border in z(t−N)
The above warping function is differentiable at all the points except when the flow  values are integer numbers
Intuitively, this is because the  the corner points used for the interpolation suddenly change  when (x′, y′) moves across from one grid cell to another
To avoid the non-differentiable case, we add a small ǫ of 0.000N to the transformed flow
This makes the warping module  differentiable with respect to both the previous frame representations and the transformed flow
We implement gradients using standard matrix calculus
Due to strided pooling,  deeper CNN representations are typically of smaller resolution in comparison to the image signal
The same strides  are used for the transformed optical flow to obtain the pixel  correspondences at the desired resolution
 Combination of Representations Once the warped activations of the previous frame ẑk(t−N) are computed with  the above mentioned procedure, they are linearly combined  with the present frame representations zkt  z̃kt = wN ⊙ z k t +wN ⊙ ẑ  k  (t−N), (N)  where wN and wN are weight vectors with the same length  as the number of channels in zk; and ⊙ represents per- channel scalar multiplication
In other words, each channel  of the frame t and the corresponding channel of the warped representations in the previous frame t−N are linearly com- bined
The parameters wN,wN are learned via standard back-propagation
The result z̃kt is then passed on to the  remaining image CNN layers
Different computations in  the NetWarp module are illustrated in Fig
N
 NNNN    PlayData-CNN (IoU: NN.N) ConvN N ConvN N ConvN N ConvN N ConvN N FCN FCN FCN FCN + FCN  + NetWarp (without FlowCNN) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  + NetWarp (with FlowCNN) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N  Table N
The effect of where NetWarp modules are inserted
Shown are test IoU scores on CamVid for augmented versions of the  PlayData-CNN
We observe an improvement (frame-by-frame results NN.N) independent of where a NetWarp is included
Refining the flow  estimate typically leads to slightly better results
 Usage and Training The inclusion of NetWarp modules  still allows end-to-end training
It can be easily integrated  in different deep learning architectures
Note that backpropagating a loss from frame t will affect image CNN lay- ers (those preceding NetWarp modules) for the present and  also previous frames
We use shared filter weights for the  image CNN across the frames
Training is possible also  when ground truth label information is available for only  some and not all frames, which is generally the case
 Due to GPU memory constraints, we make an approximation and only use two frames at a time
Filter activations  from frame t − N would receive updates from t − N when unrolling the architecture in time, but we ignore this effect because of the hardware memory limitations
The NetWarp module can be included at different depths and multiple NetWarp modules can be used to form a video CNN
 In our experiments, we used the same flow transformation  Λ(·) when multiple NetWarp modules are used
We used the Adam [NN] stochastic gradient descent method for optimizing the network parameters
Combination weights are  initialized with wN = N and wN = 0, so the initial video network is identical to the single image CNN
Other NetWarp parameters are initialized randomly with a Gaussian  noise
All our experiments and runtime analysis were performed using a Nvidia TitanX GPU and a N core Intel iNNNN0K CPU clocked at N.N0GHz machine
Our implementation that builds on the Caffe [NN] framework is available  at http://segmentation.is.tue.mpg.de
 N
Experiments  We evaluated the NetWarp modules using the two challenging semantic video segmentation benchmarks for which  video frames and/or annotations are available: CamVid [N]  and Cityscapes [N]
Both datasets contain real world video  sequences of street scenes
We choose different popular  CNN architectures of [NN, NN, NN] and augmented them  with the NetWarp modules at different places across the  network
We follow standard protocols and report the standard Intersection over Union (IoU) score which is defined in  terms of true-positives (TP), false-positives (FP) and falsenegatives (FP): “TP / (TP + FP + FN)” and additionally  the instance-level iIoU for Cityscapes [N]
We are particularly interested in the segmentation effects around the  boundaries
Methods like the influential DenseCRFs [NN]  are particularly good in segmentation of the object boundaries
Therefore, we adopt the methodologies from [NN, NN]  and measure the IoU performance only in a narrow band  around the ground truth label changes (see Fig.NN in [NN])
 We vary the width of this band and refer to this measure as  trimap-IoU (tIoU)
In all the experiments, unless specified,  we use a default trimap band of N pixels
 N.N
CamVid Dataset  The CamVid dataset contains N videos with ground-truth  labelling available for every N0th frame
Overall, the dataset  has N0N frames with ground-truth
For direct comparisons  with previous works, we used the same train, validation and  test splits as in [NN, NN, NN]
In all our models, we use only  the train split for training and report the performance on the  test split
We introduced NetWarp modules in two popular  segmentation CNNs for this dataset: One is PlayData-CNN  from [NN] and another is Dilation-CNN from [NN]
Unless  otherwise mentioned, we used DIS-Flow [NN] for the experiments on this dataset
 With PlayData-CNN [NN] as the base network, we first  study how the NetWarp module performs when introduced  at different stages of the network
The network architecture of PlayData-CNN is made of five convolutional blocks,  each with N or N convolutional layers, followed by three  N×N convolution layers (FC layers)
We add the Net- Warp module to the following layers at various depths of  the network: ConvN N, ConvN N, ConvN N, ConvN N, ConvN N,  FCN, FCN and FCN layers
The corresponding IoU scores  are reported in Tab
N
We find a consistent improvement  over the PlayData-CNN performance of NN.N% irrespective of the NetWarp locations in the network
Since NetWarp modules at FCN and FCN performed slightly better,  we chose to insert NetWarp at both the locations in our final  model with this base network
We also observe consistent  increase in IoU with the use of flow transformation across  different NetWarp locations
Our best model with two NetWarp modules yields an IoU score of N0.N% which is a new  state-of-the-art on this dataset
 Adding NetWarp modules to CNN introduces very few  additional parameters
For example, two NetWarp modules at FCN and FCN in the PlayData-CNN have about  NNK parameters, a mere 0.0NN% of all NNN.NM parameters
 The experiments in Tab
N indicate that improvements can  be contributed to the temporal information propagation at  multiple-depths
As a baseline, concatenating corresponding FCN and FCN features resulted in only 0.N% IoU improvement compared to N.N% using NetWarp modules
 NNNN  http://segmentation.is.tue.mpg.de   Input Frame Ground Truth PlayData-CNN NetWarp-CNN  Figure N
Qualitative results from the CamVid dataset
Notice  how NetWarp-CNN recovers some thin structures in the top row  and corrects some regions (on cyclist) in the second row
 IoU tIoU  PlayData-CNN [NN] NN.N NN.0  + NetWarp (with DIS-Flow [NN]) N0.N NN.N  + NetWarp (with FlowFields [N]) N0.N N0.N  + VPN [NN] NN.N Table N
CamVid Results using PlayData-CNN
Shown are the  IoU and tIoU scores from different methods using a fast flow from  DIS[NN] and an accurate flow from [N] for NetWarp augmented  PlayData-CNN
 IoU tIoU Runtime (ms)  Dilation-CNN [NN] NN.N NN.N NN0  + NetWarp(Ours) NN.N NN.N NNN  + FSO-CRF [NN] NN.N - > N0k  + VPN [NN] NN.N NN.N NN0  Table N
CamVid Results using Dilation-CNN
IoU, tIoU scores  and runtimes (in milliseconds) for different methods
 In Tab
N, we show the effect of using the more accurate but slower optical flow method of Flowfields [N]
Results indicate that there is only a 0.N% improvement in IoU  with Flowfields but this incurs a much higher runtime for  flow computation
Results also indicate that our approach  outperformed the current state-of-the-art approach of VPN  from [NN] by a significant margin of 0.N% IoU, while being  faster
 As a second network, we choose the dilation CNN  from [NN]
This network consists of a standard CNN followed by a context module with N dilated convolutional  layers
For this network, we apply the NetWarp module  on the output of each of these N dilated convolutions
Table N shows the performance and runtime comparisons with  the dilation CNN and other related techniques
With a runtime increase of NN milliseconds, we observe significant improvements in the order of N.N% IoU
The runtime increase  assumes that the result of the previous frame is already computed, which is the case for video segmentation
 N.N
Cityscapes Dataset  The Cityscapes dataset [N] comes with a total of N000  video sequences of high-quality images (N0NN×N0NN res- olution), partitioned into NNNN train, N00 validation and  NNNN test sequences
The videos are captured in different  weather conditions across N0 different cities in Germany  and Switzerland
In addition to the IoU and tIoU performance metrics, we report the instance-level IoU score
 Since IoU score is dominated by large objects/regions (such  as road) in the scene, the makers of this dataset proposed the  iIoU score that takes into account the relative size of different objects/regions
The iIoU score is given as iTP/(iTP +  FP + iFN), where iTP and iFN are the modified true-positive  and false-negative scores which are computed by weighting  the contribution of each pixel by the ratio of the average  class instance size to the size of the respective ground truth  instance
This measures how well the semantic labelling  represents the individual instances in the scene
For more  details on this metric, please refer to the original dataset  paper [N]
For this dataset, we used DIS-Flow [NN] for all  networks augmented with NetWarp modules
 We choose the recently proposed Pyramid Scene Parsing  Network (PSPNet) from [NN]
Because of high-resolution  images in Cityscapes and GPU memory limitations, PSPNet is applied in a sliding window fashion with a window  size of NNN×NNN
To achieve higher segmentation perfor- mance, the authors of [NN] also evaluated a multi-scale version
Applying the same PSPNet on N different scales of an  input image results to an improvement of N.N% IoU over the  single-scale variant
This increased performance comes at  the cost of increased runtime
In the single-scale setting, the  network is evaluated on N different windows to get a full image result, whereas in the multi-scale setting, the network is  evaluated NN times leading to N0 times increase in runtime
 We refer to the single-scale and multi-scale evaluations as  PSPNet-SSc and PSPNet-MSc respectively
 The architecture of PSPNet is a ResNetN0N [NN] network  variant with pyramid style pooling layers
We insert NetWarp modules before and after the pyramid pooling layers
More precisely, NetWarp modules are added on both  the ConvN NN and ConvN N representations
The network is  then fine-tuned with NNNN train video sequences without any  data augmentation
We evaluate both the single-scale and  multi-scale variants
Table N summarizes the quantitative  results with and without NetWarp modules, on validation  data scenes of Cityscapes
We find an improvement of the  IoU (by N.N), tIoU (by N.N) and iIoU (by N.N) respectively  over the single image PSPNet-SSc variant
These improvements come with a low runtime overhead of NN milliseconds
Also the augmented multi-scale network improves all  measures: IoU by 0.N, tIoU by N.N, and iIoU by N.N%
 We chose to submit the results of the best performing method from the validation set to the Cityscapes test  server
Results are shown in Tab
N
We find that the NetWarp augmented PSP multi-scale variant is on par with the  current top performing method [NN] (N0.N vs
N0.N) and  out-performs current top models in terms of iIoU by a sigNNNN    IoU iIoU tIoU Runtime (s)  PSPNet-SSc [NN] NN.N N0.N NN.N N.00  +NetWarp N0.N NN.N NN.N N.0N  PSPNet-MSc [NN] N0.N NN.N NN.N N0.N  +NetWarp NN.N NN.N NN.N N0.N  Table N
Performance of PSPNet with NetWarp modules on the  Cityscapes validation dataset
We observe consistent improvements with NetWarp modules across all three performance measures in both the single-scale (SSc) and multi-scale (MSc) settings,  while only adding little time overhead
 nificant margin of N.N%
In summary, at submission time,  our result is best performing method in iIoU and second on  IoUN
Surprisingly, it is the only approach that uses video  data
A closer look into class IoU score in Tab
N shows  that our approach works particularly well for parsing thin  structures like poles, traffic lights, traffic signs etc
The improvement is around N% IoU for the pole class
Another observation is that adding NetWarp modules resulted in slight  IoU performance decrease for few broad object classes such  as car, truck etc
However, we find consistent improvements  across most of the classes in other performance measures
 The classwise iIoU scores that are computed on broad object classes like car, bus etc show better performance on N  out of N classes for NetWarp
Further, analysing the classwise tIoU scores on the validation set, we find that NetWarpperforms better on NN out of NN classes
Visual results in Fig
N also indicate that the thin structures are better  parsed with the introduction of NetWarp modules
Qualitatively, we find improved performance near boundaries compared to baseline CNN (see supplementary video)
Visual  results in Fig
N also indicate that NetWarp helps in rectifying the segmentation of big regions as well
 In Fig
N, we show the relative improvement of the NetWarp augmented PSPNet for different widths in the trimapIoU
From this analysis we can conclude that the IoU improvement is especially due to better performance near object boundary
This is true for both the single and the multiscale version of the network
Image CNNs, in general, are  very good at segmenting large regions or objects like road or  cars
However, thin and fine structures are difficult to parse  as information is lost due to strided operations inside CNN
 In part this is recovered by NetWarp CNNs that use the temporal context to recover thin structures
In Fig
N, some  qualitative results with static image CNN and our video  CNN are shown
We observe that the NetWarp module correct for thin structures but also frequently correct larger regions of wrong segmentations
This is possible since representations for the same regions are combined over different  frames leading to a more robust classification
 Next, we analyze how the DIS-Flow is transformed by  the FlowCNN
Figure N shows some visualizations of the  Nhttps://www.cityscapes-dataset.com/benchmarks/  Width of Trimap (Pixels)  0 N N0 NN N0  Im p  r o  v e m  e n  t  in   e Io  U  0.N  N  N.N  N  N.N NetWarp-PSPNet-SSc  NetWarp-PSPNet-MSc  Figure N
tIoU improvement
Relative improvements of IoU  within trimaps as a function of trimap width
Most improvement  is in regions close to object boundaries
 Input Frame DIS Flow Transformed Flow  C a m V id  C it y s c a p e s  Figure N
Effect of flow transformation
Example results of input and transformed flow, after training for semantic segmentation
 There is a qualitative difference between CamVid and CityScapes
 Best viewed on screen
 transformed flow along with the original DIS Flow fields
 We can clearly observe that, in both CamVid and Cityscapes  images, the structure of the scene is much more pronounced  in the transformed flow indicating that the traditionally  computed flow might not be optimal to find pixel correspondences for semantic segmentation
 N
Conclusions and Outlook  We presented NetWarp, an efficient and conceptually  easy way to transform image CNNs into video CNNs
The  main concept is to transfer intermediate CNN filter activNNNN  https://www.cityscapes-dataset.com/benchmarks/   Input Frame Ground Truth PSPNet-MSc NetWarp-PSPNet  Figure N
Qualitative results from the Cityscapes dataset
Observe how NetWarp-PSPNet is able to correct larger parts of wrong  segmentation (top two rows) by warping activations across frames
The bottom row shows an example of improved segmentation of a thin  structure
All results shown are obtained with the multi-scale versions
 iIo U  Io U  ro ad  si de  w al  k  bu ild  in g  w al  l  fe nc  e po  le  tra ffi  cl ig  ht  tra ffi  cs ig  n  ve ge  ta tio  n  te rr ai  n sk  y pe  rs on  rid er  ca r  tru ck  bu s  tra in  m ot  or cy  cl e  bi cy  cl e  PSPNet-MSc [NN] NN.N N0.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N N0.N N0.N NN.N NN.N  +NetWarp(Ours) NN.N N0.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N N0.0 NN.N NN.0 NN.N NN.N NN.N NN.N NN.N N0.0 NN.N N0.N NN.N  ResNet-NN [NN] NN.N N0.N NN.N NN.0 NN.N N0.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  TuSimple [NN] NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N N0.N NN.N NN.N N0.N NN.N NN.N NN.N  LRR-NX [NN] NN.N NN.N NN.0 NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.0 NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  RefineNet [NN] NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0  Table N
Results on the Cityscapes test dataset
Average iIoU, IoU and class IoU scores of base PSPNet, with NetWarp modules and  also other top performing methods taken from the Cityscapes benchmark website at the time of submission
The NetWarp augmented  PSPNet-MSc version achieves highest iIoU and is about on par with [NN] on IoU
Our video methods performs particularly well on small  classes such as poles, traffic lights etc
 ities of adjacent frames based on transformed optical flow  estimate
The resulting video CNN is end-to-end trainable,  runs in an online fashion and has only a small computation  overhead in comparison to the frame-by-frame application
 Experiments on the current standard benchmark datasets  CityScapes and CamVid show improved performance for  several strong baseline methods
The final model sets a  new state-of-the-art performance on both CityScapes and  CamVid
 Extensive experimental analysis provide insights into the  workings of the NetWarp module
First, we demonstrate  consistent performance improvements across different image CNN hierarchies
Second, we find more temporally  consistent semantic predictions and better coverage of thin  structures such as poles and traffic signs
Third, we observed that the flow changed radically after the transformation (FlowCNN) trained with the segmentation loss
From  the qualitative results, it seems that the optical flow at the  object boundaries is important for semantic segmentation
 An interesting future work is to systematically study what  properties of optical flow estimation are necessary for semantic segmentation and the impact of different types of  interpolation in a NetWarp module
 Another future direction is to scale the video CNNs to  use multiple frames
Due to GPU memory limitations and  to keep training easier, here, we trained with only two adjacent frames at a time
In part this is due to the memory  demanding base models like ResNetN0N
Memory optimizing the CNN training would alleviate some of the problems  and enables training with many frames together
We also  envision that the findings of this paper are interesting for  the design of video CNNs for different tasks other than semantic segmentation
 Acknowledgements
This work was supported by Max Planck  ETH Center for Learning Systems and the research program of  the Bernstein Center for Computational Neuroscience, Tübingen,  funded by the German Federal Ministry of Education and Research (BMBF; FKZ: 0NGQN00N)
We thank Thomas Nestmeyer  and Laura Sevilla for help with the supplementary
 NNN0    References  [N] C
Bailer, B
Taetz, and D
Stricker
Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation
In Proceedings of the IEEE International  Conference on Computer Vision, N0NN
N, N  [N] G
J
Brostow, J
Fauqueur, and R
Cipolla
Semantic object  classes in video: A high-definition ground truth database
 Pattern Recognition Letters, N0(N):NN–NN, N00N
N, N  [N] G
J
Brostow, J
Shotton, J
Fauqueur, and R
Cipolla
Segmentation and recognition using structure from motion point  clouds
In European Conference on Computer Vision, pages  NN–NN
Springer, N00N
N  [N] A
Y
Chen and J
J
Corso
Temporally consistent multiclass video-object segmentation with the video graph-shifts  algorithm
In IEEE Winter Conference on Applications of  Computer Vision, pages NNN–NNN
IEEE, N0NN
N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected crfs
arXiv preprint  arXiv:NNNN.N0NN, N0NN
N  [N] Y.-Y
Chuang, A
Agarwala, B
Curless, D
H
Salesin, and  R
Szeliski
Video matting of complex scenes
ACM Transactions on Graphics (ToG), NN(N):NNN–NNN, N00N
N  [N] M
Cordts, M
Omran, S
Ramos, T
Rehfeld, M
Enzweiler,  R
Benenson, U
Franke, S
Roth, and B
Schiele
The  cityscapes dataset for semantic urban scene understanding
 In IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N, N  [N] R
de Nijs, S
Ramos, G
Roig, X
Boix, L
Van Gool, and  K
Kühnlenz
On-line semantic perception using uncertainty
 In N0NN IEEE/RSJ International Conference on Intelligent  Robots and Systems, pages NNNN–NNNN
IEEE, N0NN
N  [N] J
Donahue, L
Anne Hendricks, S
Guadarrama,  M
Rohrbach, S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual  recognition and description
In Proceedings of the IEEE  conference on computer vision and pattern recognition,  pages NNNN–NNNN, N0NN
N  [N0] A
Ess, T
Mueller, H
Grabner, and L
J
Van Gool
 Segmentation-based urban traffic scene understanding
In  British Machine Vision Conference, N00N
N  [NN] Q
Fan, F
Zhong, D
Lischinski, D
Cohen-Or, and B
Chen
 Jumpcut: non-successive mask transfer and interpolation  for video cutout
ACM Transactions on Graphics (ToG),  NN(N):NNN, N0NN
N  [NN] G
Floros and B
Leibe
Joint Nd-Nd temporally consistent  semantic segmentation of street scenes
In IEEE Conference  on Computer Vision and Pattern Recognition, pages NNNN–  NNN0, N0NN
N  [NN] R
Gadde, V
Jampani, M
Kiefel, D
Kappler, and P
Gehler
 Superpixel convolutional networks using bilateral inceptions
In European Conference on Computer Vision
 Springer, N0NN
N  [NN] G
Ghiasi and C
C
Fowlkes
Laplacian pyramid reconstruction and refinement for semantic segmentation
In European  Conference on Computer Vision, pages NNN–NNN
Springer,  N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In IEEE Conference on Computer  Vision and Pattern Recognition, pages NN0–NNN, N0NN
N, N  [NN] J
Hur and S
Roth
Joint optical flow and temporally consistent semantic segmentation
In European Conference on  Computer Vision, pages NNN–NNN
Springer, N0NN
N  [NN] S
D
Jain and K
Grauman
Supervoxel-consistent foreground propagation in video
In European Conference on  Computer Vision, pages NNN–NNN
Springer, N0NN
N  [NN] V
Jampani, R
Gadde, and P
V
Gehler
Video propagation  networks
In IEEE Conference on Computer Vision and Pattern Recognition, N0NN
N, N  [NN] V
Jampani, M
Kiefel, and P
V
Gehler
Learning sparse high  dimensional filters: Image filtering, dense crfs and bilateral  neural networks
In IEEE Conference on Computer Vision  and Pattern Recognition, June N0NN
N  [N0] S
Ji, W
Xu, M
Yang, and K
Yu
Nd convolutional neural  networks for human action recognition
IEEE transactions  on pattern analysis and machine intelligence, NN(N):NNN–  NNN, N0NN
N  [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional  architecture for fast feature embedding
In Proceedings of  the ACM International Conference on Multimedia, pages  NNN–NNN
ACM, N0NN
N  [NN] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In IEEE Conference on Computer  Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N  [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
arXiv preprint arXiv:NNNN.NNN0, N0NN
N  [NN] P
Kohli, P
H
Torr, et al
Robust higher order potentials for  enforcing label consistency
International Journal of Computer Vision, NN(N):N0N–NNN, N00N
N  [NN] P
Krähenbühl and V
Koltun
Efficient inference in fully  connected crfs with gaussian edge potentials
In Advances in  neural information processing systems, N0NN
N  [NN] T
Kroeger, R
Timofte, D
Dai, and L
V
Gool
Fast optical  flow using dense inverse search
In European Conference on  Computer Vision, pages NNN–NNN
Springer, N0NN
N, N, N  [NN] A
Kundu, Y
Li, F
Dellaert, F
Li, and J
M
Rehg
Joint semantic segmentation and Nd reconstruction from monocular  video
In European Conference on Computer Vision, pages  N0N–NNN
Springer, N0NN
N  [NN] A
Kundu, V
Vineet, and V
Koltun
Feature space optimization for semantic video segmentation
In IEEE Conference  on Computer Vision and Pattern Recognition, pages NNNN–  NNNN, N0NN
N, N, N  [NN] P
Lei and S
Todorovic
Recurrent temporal deep field for  semantic video labeling
In European Conference on Computer Vision, pages N0N–NNN
Springer, N0NN
N  [N0] Y
Li, J
Sun, and H.-Y
Shum
Video object cut and  paste
ACM Transactions on Graphics (ToG), NN(N):NNN–  N00, N00N
N  [NN] G
Lin, A
Milan, C
Shen, and I
Reid
RefineNet: Multipath refinement networks for high-resolution semantic segmentation
In CVPR, N0NN
N  NNNN    [NN] B
Liu, X
He, and S
Gould
Multi-class semantic video  segmentation with exemplar-based object reasoning
In Applications of Computer Vision (WACV), N0NN IEEE Winter  Conference on, pages N0NN–N0NN
IEEE, N0NN
N, N  [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In IEEE Conference  on Computer Vision and Pattern Recognition, pages NNNN–  NNN0, N0NN
N  [NN] O
Miksik, D
Munoz, J
A
Bagnell, and M
Hebert
Efficient  temporal consistency for streaming video scene analysis
In  Robotics and Automation (ICRA), N0NN IEEE International  Conference on, pages NNN–NNN
IEEE, N0NN
N  [NN] B
L
Price, B
S
Morse, and S
Cohen
Livecut: Learningbased interactive video segmentation by evaluation of multiple propagated cues
In Proceedings of the IEEE International Conference on Computer Vision, pages NNN–NNN
 IEEE, N00N
N  [NN] M
Reso, B
Scheuermann, J
Jachalsky, B
Rosenhahn, and  J
Ostermann
Interactive segmentation of high-resolution  video content using temporally coherent superpixels and  graph cut
In International Symposium on Visual Computing, pages NNN–NNN
Springer, N0NN
N  [NN] S
R
Richter, V
Vineet, S
Roth, and V
Koltun
Playing  for data: Ground truth from computer games
In European  Conference on Computer Vision, pages N0N–NNN
Springer,  N0NN
N, N  [NN] S
Sengupta, E
Greveson, A
Shahrokni, and P
H
Torr
Urban Nd semantic modelling using stereo vision
In Robotics  and Automation (ICRA), N0NN IEEE International Conference on, pages NN0–NNN
IEEE, N0NN
N  [NN] E
Shelhamer, K
Rakelly, J
Hoffman, and T
Darrell
Clockwork convnets for video semantic segmentation
arXiv  preprint arXiv:NN0N.0NN0N, N0NN
N, N  [N0] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In Advances  in neural information processing systems, pages NNN–NNN,  N0NN
N  [NN] P
Sturgess, K
Alahari, L
Ladicky, and P
H
Torr
Combining appearance and structure from motion features for road  scene understanding
In British Machine Vision Conference
 BMVA, N00N
N  [NN] S
Tripathi, S
Belongie, Y
Hwang, and T
Nguyen
Semantic  video segmentation: Exploring inference efficiency
In N0NN  International SoC Design Conference (ISOCC), pages NNN–  NNN
IEEE, N0NN
N  [NN] J
Wang, P
Bhat, R
A
Colburn, M
Agrawala, and M
F
Cohen
Interactive video cutout
ACM Transactions on Graphics (ToG), NN(N):NNN–NNN, N00N
N  [NN] L
Wang, Y
Xiong, Z
Wang, Y
Qiao, D
Lin, X
Tang, and  L
Van Gool
Temporal segment networks: towards good  practices for deep action recognition
In European Conference on Computer Vision, pages N0–NN
Springer, N0NN
N  [NN] P
Wang, P
Chen, Y
Yuan, D
Liu, Z
Huang, X
Hou, and  G
Cottrell
Understanding convolution for semantic segmentation
arXiv preprint arXiv:NN0N.0NN0N, N0NN
N  [NN] Z
Wu, C
Shen, and A
v
d
Hengel
Wider or deeper: Revisiting the resnet model for visual recognition
arXiv preprint  arXiv:NNNN.N00N0, N0NN
N, N  [NN] F
Yu and V
Koltun
Multi-scale context aggregation by dilated convolutions
In International Conference on Learning  Representations, N0NN
N, N  [NN] H
Zhao, J
Shi, X
Qi, X
Wang, and J
Jia
Pyramid scene  parsing network
arXiv preprint arXiv:NNNN.0NN0N, N0NN
N,  N, N, N  NNNNDetail-Revealing Deep Video Super-Resolution   Detail-revealing Deep Video Super-resolution∗  Xin TaoN Hongyun GaoN Renjie LiaoN,N Jue WangN Jiaya JiaN,N  NThe Chinese University of Hong Kong NUniversity of Toronto NUber Advanced Technologies Group NMegvii Inc
NYoutu Lab, Tencent  {xtao,hygao}@cse.cuhk.edu.hk rjliao@cs.toronto.edu  arphid@gmail.com leojiaN@gmail.com  Abstract  Previous CNN-based video super-resolution approaches  need to align multiple frames to the reference
In this paper, we show that proper frame alignment and motion compensation is crucial for achieving high quality results
We  accordingly propose a “sub-pixel motion compensation”  (SPMC) layer in a CNN framework
Analysis and experiments show the suitability of this layer in video SR
The  final end-to-end, scalable CNN framework effectively incorporates the SPMC layer and fuses multiple frames to reveal  image details
Our implementation can generate visually  and quantitatively high-quality results, superior to current  state-of-the-arts, without the need of parameter tuning
 N
Introduction  As one of the fundamental problems in image processing  and computer vision, video or multi-frame super-resolution  (SR) aims at recovering high-resolution (HR) images from a  sequence of low-resolution (LR) ones
In contrast to singleimage SR where details have to be generated based on only  external examples, an ideal video SR system should be  able to correctly extract and fuse image details in multiple  frames
To achieve this goal, two important sub-problems  are to be answered: (N) how to align multiple frames to construct accurate correspondence; and (N) how to effectively  fuse image details for high-quality outputs
 Motion Compensation While large motion between consecutive frames increases the difficulty to locate corresponding image regions, subtle sub-pixel motion contrarily benefits restoration of details
Most previous methods  compensate inter-frame motion by estimating optical flow  [N, N, NN, N0, NN] or applying block-matching [NN]
After  motion is estimated, traditional methods [N, N0, NN] reconstruct the HR output based on various imaging models and  ∗This work is in part supported by a grant from the Research Grants  Council of the Hong Kong SAR (project No
NNNNNN)
 image priors, typically under an iterative estimation framework
Most of these methods involve rather intensive caseby-case parameter-tuning and costly computation
 Recent deep-learning-based video SR methods [N, NN]  compensate inter-frame motion by aligning all other frames  to the reference one, using backward warping
We show that  such a seemingly reasonable technical choice is actually not  optimal for video SR, and improving motion compensation  can directly lead to higher quality SR results
In this paper,  we achieve this by proposing a sub-pixel motion compensation (SPMC) strategy, which is validated by both theoretical  analysis and extensive experiments
 Detail Fusion Besides motion compensation, proper image detail fusion from multiple frames is the key to the success of video SR
We propose a new CNN framework that  incorporates the SPMC layer, and effectively fuses image  information from aligned frames
Although previous CNNbased video SR systems can produce sharp-edge images, it  is not entirely clear whether the image details are those inherent in input frames, or learned from external data
In  many practical applications such as face or text recognition,  only true HR details are useful
In this paper we provide  insightful ablation study to verify this point
 Scalability A traditionally-overlooked but practicallymeaningful property of SR systems is the scalability
In  many previous learning-based SR systems, the network  structure is closely coupled with SR parameters, making  them less flexible when new SR parameters need to be applied
For example, ESPCN [NN] output channel number is  determined by the scale factor
VSRnet [NN] and VESPCN  [N] can only take a fixed number of temporal frames as input, once trained
 In contrast, our system is fully scalable
First, it can  take arbitrary-size input images
Second, the new SPMC  layer does not contain trainable parameters and can be applied for arbitrary scaling factors during testing
Finally, the  ConvLSTM-based [NN] network structure makes it possible  to accept an arbitrary number of frames for SR in testing  NNNNN    phase
 N.N
Related Work  Deep Super-resolution With the seminal work of SRCNN [N], a majority of recent SR methods employ deep  neural networks [N, NN, NN, NN, NN, NN]
Most of them  resize input frames before sending them to the network  [N, NN, NN, NN], and use very deep [NN], recursive [NN]  or other networks to predict HR results
Shi et al
[NN]  proposed a subpixel network, which directly takes lowresolution images as input, and produces a high-res one with  subpixel location
Ledig et al
[NN] used a trainable deconvolution layer instead
 For deep video SR, Liao et al
[NN] adopted a separate  step to construct high-resolution SR-drafts, which are obtained under different flow parameters
Kappeler et al
[NN]  estimated optical flow and selected corresponding patches  across frames to train a CNN
In both methods, motion estimation is separated from training
Recently, Caballero et  al
[N] proposed the first end-to-end video SR framework,  which incorporates motion compensation as a submodule
 Motion Estimation Deep neural networks were also used  to solve motion estimation problems
Zbontar and LeCun  [NN] and Luo et al
[NN] used CNNs to learn a patch distance  measure for stereo matching
Fischer et al
[N] and Mayer  et al
[NN] proposed end-to-end networks to predict optical  flow and stereo disparity
 Progress was made in spatial transformer networks [N0]  where a differentiable layer warps images according to predicted affine transformation parameters
Based on it, WarpNet [NN] used a similar scheme to extract sparse correspondence
Yu et al
[N0] warped output based on predicted  optical flow as a photometric loss for unsupervised optical  flow learning
Different from these strategies, we introduce  a Sub-pixel Motion Compensation (SPMC) layer, which is  suitable for the video SR task
 N
Sub-pixel Motion Compensation (SPMC)  We first introduce our notations for video SR
It takes a  sequence of NF = (NT + N) LR images as input (T is the size of temporal span in terms of number of frames), where  ΩL = {I L −T , · · · , I  L 0 , · · · , I  L T }
The output HR image I  H 0  corresponds to center reference frame IL0 
 LR Imaging Model The classical imaging model for LR  images [N, NN, N0, NN] is expressed as  ILi = SKW0→iI H 0 + ni, (N)  where W0→i is the warping operator to warp from the 0th to ith frame
K and S are downsampling blur and deci- mation operators, respectively
ni is the additive noise to frame i
For simplicity’s sake, we neglect operator K in the following analysis, since it can be absorbed by S
 a c e g  a a b d e g g h  a b c d e f g h  (a)  (b) a b c d e f g h  a+b c d e f+g h  S S T  W  W T  a b c d e f g h a c e g  WS S T  W T  (c)  Figure N
Visualization of operators in image formation
(a)  Decimation operator S (N×) reduces the input ND signal to its half- size
The transpose ST corresponds to zero-upsampling
(b) With  arrows indicating motion, warping operator W produces the blue  signal from the gray one through backward warping
WT produces the green signal through forward warping
(c) Illustration  of matrices S, ST , W and WT 
Grayed and white blocks indicate  values N and 0 respectively
 Flow Direction and Transposed Operators Operator  W0→i indicates the warping process
To compute it, one needs to first calculate the motion field Fi→0 (from the ith to 0th frame), and then perform backward warping to pro- duce the warped image
However, current deep video SR  methods usually align other frames back to IL0 , which actu- ally makes use of flow F0→i
 More specifically, directly minimizing the LN-norm re- construction error  ∑  i ‖SW0→iI H 0 − I  L i ‖  N results in  IH0 = ( ∑  i  WT0→iS TSW0→i)  −N( ∑  i  WT0→iS T ILi )
(N)  With certain assumptions [N, N], WT0→iS TSW0→i becomes  a diagonal matrix
The solution to Eq
(N) reduces to a feedforward generation process of  IH0 =  ∑  i W T 0→iS  T ILi ∑  i W T 0→iS  TN , (N)  where N is an all-one vector with the same size as ILi 
The operators that are actually applied to ILi are S  T and  WT0→i
S T is the transposed decimation corresponding to  zero-upsampling
WT0→i is the transposed forward warping using flow Fi→0
A ND signal example for these operators is shown in Fig
N
We will further analyze the difference of  forward and backward warping after explaining our system
 N
Our Method  Our method takes a sequence of NF LR images as in- put and produces one HR image IH0 
It is an end-to-end fully trainable framework that comprises of three modules:  motion estimation, motion compensation and detail fusion
 NNNN    SPMC  Conv  LSTM  ME  NNNN NNN NNN NNN NN NN  H×W  H/ ×N W N/  H/ ×N W N/  H×W H×W H×W  H/ ×N W N/  NN  F i→0  J i  H  Motion Estimation SPMC Layer Detail Fusion Net  Convolution (stride N) Convolution (stride N) Deconvolution Skip connection  I 0  L  I i  L  Figure N
Our framework
Network configuration for the ith time step
 F i  Grid Generator  Y0  (a) (b)  


 


 I i L  Sampler  I -T  L  IT L  I0 L  FT  F0  F -T  Figure N
Subpixel Motion Compensation layer (×N)
(a) Layer diagram
(b) Illustration of the SPMC layer (×N)
 They are respectively responsible for motion field estimation between frames; aligning frames by compensating motion; and finally increasing image scale and adding image  details
We elaborate on each module in the following
 N.N
Motion Estimation  The motion estimation module takes two LR frames as  input and produces a LR motion field as  Fi→j = NetME(I L i , I  L j ; θME), (N)  where Fi→j = (ui→j , vi→j) is the motion field from frame ILi to I  L j 
θME is the set of module parameters
 Using neural networks for motion estimation is not a new  idea, and existing work [N, N, NN, N0] already achieves good  results
We have tested FlowNet-S [N] and the motion compensation transformer (MCT) module from VESPCN [N]  for our task
We choose MCT because it has less parameters and accordingly less computation cost
It can process  N00+ single-channel image pairs (N00 × N00 in pixels) per second
The result quality is also acceptable in our system
 N.N
SPMC Layer  According to the analysis in Sec
N, we propose a  novel layer to utilize sub-pixel information from motion  and simultaneously achieve sub-pixel motion compensation  (SPMC) and resolution enhancement
It is defined as  JH = LayerSPMC(J L, F ;α), (N)  where JL and JH are input LR and output HR images, F is optical flow used for transposed warping and α is the scal- ing factor
The layer contains two submodules
 Sampling Grid Generator In this step, transformed coordinates are first calculated according to estimated flow  F = (u, v) as  (  xsp ysp  )  = WF ;α  (  xp yp  )  = α  (  xp + up yp + vp  )  , (N)  where p indexes pixels in LR image space
xp and yp are the two coordinates of p
up and vp are the flow vectors estimated from previous stage
We denote transform of coordinates as operator WF ;α, which depends on flow field F and scale factor α
xsp and y  s p are the transformed coordinates in an enlarged image space, as shown in Fig
N
 Differentiable Image Sampler Output image is constructed in the enlarged image space according to xsp and  ysp
The resulting image J H q is  JHq = ∑  p=N  JLp M(x s p − xq)M(y  s p − yq), (N)  where q indexes HR image pixels
xq and yq are the two coordinates for pixel q in the HR grid
M(·) is the sampling kernel, which defines the image interpolation methods (e.g
 bicubic, bilinear, and nearest-neighbor)
 We further investigate differentiability of this layer
As  indicated in Eq
(N), the SPMC layer takes one LR image  JL and one flow field F = (u, v) as input, without other trainable parameters
For each output pixel, partial derivaNNNN    tive with respect to each input pixel is  ∂JHq ∂JLp  = ∑  p=N  M(xsp − xq)M(y s p − yq)
(N)  It is similar to calculating partial derivatives with respect to  flow field (up, vp) using the chain rule as  ∂JHq ∂up  = ∂JHq ∂xsp  · ∂xsp ∂up  = α ∑  p=N  JLp M ′(xsp − xq)M(y  s p − yq),  (N)  where M ′(·) is the gradient of sampling kernel M(·)
Similar derivatives can be derived for ∂Jq ∂vp  
We choose M(x) =  max(0, N− |x|), which corresponds to the bilinear interpo- lation kernel, because of its simplicity and convenience to  calculate gradients
Our final layer is fully differentiable, allowing back-propagating loss to flow fields smoothly
The  advantages of having this type of layers is threefold
 • This layer can simultaneously achieve motion com- pensation and resolution enhancement
Note in most  previous work, they are separate steps (e.g
backward  warping + bicubic interpolation)
• This layer is parameter free and fully differentiable,  which can be effectively incorporated into neural networks with almost no additional cost
• The rationale behind this layer roots from accurate LR  imaging model, which ensures good performance in  theory
It also demonstrates good results in practice,  as we will present later
 N.N
Detail Fusion Net  The SPMC layer produces a series of motion compensated frames {JHi } expressed as  JHi = LayerSPMC(I L i , Fi→0;α)
(N0)  Design of the following network is non-trivial due to the  following considerations
First, {JHi } are already HR-size images that produce large feature maps, thus computational  cost becomes an important factor
 Second, due to the property of forward warping and zeroupsampling, {JHi } is sparse and majority of the pixels are zero-valued (e.g
about NN/NN are zeros for scale factor N×)
This requires the network to have large receptive fields to  capture image patterns in JHi 
Using simple interpolation to fill these holes is not a good solution because interpolated  values would dominate during training
 Finally, special attention needs to be paid to the use of  the reference frame
On the one hand, we rely on the reference frame as the guidance for SR so that the output HR  image is consistent with the reference frame in terms of image structures
On the other hand, over-emphasizing the  reference frame could impose an adverse effect of neglecting information in other frames
The extreme case is that  the system behaves like a single-image SR one
 Network Architecture We design an encoder-decoder  [NN] style structure with skip-connections (see Fig
N) to  tackle above issues
This type of structure has been proven  to be effective in many image regression tasks [NN, NN, NN]
 The encoder sub-network reduces the size of input HR image to N/N of it in our case, leading to reduced computation cost
It also makes the feature maps less sparse so that information can be effectively aggregated without the need of  employing very deep networks
Skip-connections are used  for all stages to accelerate training
 A ConvLSTM module [NN] is inserted in the middle  stage as a natural choice for sequential input
The network  structure includes  fi = NetE(J H i ; θE)  gi, si = ConvLSTM(fi, si−N; θLSTM ) (NN)  I (i) 0 = NetD(gi, S  E i ; θD) + I  L↑ 0  where NetE and NetD are encoder and decoder CNNs  with parameters θE and θD
fi is the output of encoder net
gi is the input of decoder net
si is the hidden state for  LSTM at the ith step
SEi for all i are intermediate feature  maps of NetE , used for skip-connection
I L↑ 0 is the bicubic  upsampled IL0 
I (i) 0 is the ith time step output
 The first layer of NetE and the last layer of NetD have  kernel size N × N
All other convolution layers use kernel size N × N, including those inside ConvLSTM
Deconvolu- tion layers are with kernel size N × N and stride N
Rec- tified Linear Units (ReLU) are used for every conv/deconv  layer as the activation function
For skip-connection, we use  SUM operator between connected layers
Other parameters  are labeled in Fig
N
 N.N
Training Strategy  Our framework consists of three major components, each  has a unique functionality
Training the whole system in an  end-to-end fashion with random initialization would result  in zero flow in motion estimation, making the final results  similar to those of single-image SR
We therefore separate  training into three phases
 Phase N We only consider NetME in the beginning of  training
Since we do not have ground truth flow, unsupervised warping loss is used as [NN, N0]  LME =  T ∑  i=−T  ‖ILi − Ĩ L 0→i‖N + λN‖∇Fi→0‖N, (NN)  where ĨL0→i is the backward warped I L 0 according to estimated flow Fi→0, using a differentiable layer similar to spatial transformer [N0]
Note that this image is in low resolution, aligned with ILi 
‖∇Fi→0‖N is the total variation term on each (u, v)-component of flow Fi→0
λN is the reg- ularization weight
We set λN = 0.0N in all experiments
 NNNN    (a) Bicubic ×N (b) Using BW #N (c) Using BW #N (d) Using BW #N  (e) Ground truth (f) Using SPMC #N (g) Using SPMC #N (h) Using SPMC #N  Figure N
Effectiveness of SPMC Layer (FN-×N)
(a) Bicubic ×N
(b)-(d) Output for each time step using BW
(e) Ground truth
(f)-(h) Outputs using SPMC
 Phase N We then fix the learned weights θME and only train NetDF 
This time we use Euclidean loss between our  estimated HR reference frame and the ground truth as  LSR =  T ∑  i=−T  κi‖I H 0 − I  (i) 0 ‖  N N, (NN)  where I (i) 0 is our network output in the ith time step, corresponding to reference frame IL0 
{κi} are the weights for each time step
We empirically set κ−T = 0.N and κT = N.0, and linearly interpolate intermediate values
 Phase N In the last stage, we jointly tune the whole system  using the total loss as  L = LSR + λNLME , (NN)  where λN is the weight balancing two losses
 N
Experiments  We conduct our experiments on a PC with an Intel Xeon  EN CPU and an NVIDIA Titan X GPU
We implement our  framework on the TensorFlow platform [N], which enables  us to easily develop our special layers and experiment with  different network configurations
 Data Preparation For the super-resolution task, training  data needs to be of high-quality without noise while containing rich fine details
To our knowledge, there is no  such publicly available video dataset that is large enough  to train our deep networks
We thus collect NNN sequences  from high-quality N0N0p HD video clips
Most of them are  commercial videos shot with high-end cameras and contain  both natural-world and urban scenes that have rich details
 Each sequence contains NN frames following the configuration of [NN, N0, NN]
We downsample the original frames to  NN0× NN0 pixels as HR ground truth using bicubic interpo- lation
LR input is obtained by further downsampling HR  frames to NN0 × NN0, NN0 × NN0 and NNN × NN0 sizes
We randomly choose NNN of them as training data, and the rest  N0 sequences are for validation and testing
 Model Training For model training, we use Adam solver  [NN] with learning rate of 0.000N, βN = 0.N and βN = 0.NNN
We apply gradient clip only to weights of ConvLSTM module (clipped by global norm N) to stabilize the training process
At each iteration, we randomly sample NF consecu- tive frames (e.g
NF = N, N, N) from one sequence, and ran- domly crop a N00×N00 image region as training input
The corresponding ground truth is accordingly cropped from the  reference frame with size N00α× N00α where α is the scal- ing factor
Above parameters are fixed for all experiments
 Batch size varies according to different settings, which is  determined as the maximal value allowed by GPU memory
 We first train the motion estimation module using only  loss LME in Eq
(NN) with λN = 0.0N
After about N0,000 it- erations, we fix the parameters θME and train the system us- ing only loss LSR in Eq
(NN) for N0,000 iterations
Finally, all parameters are trained using total loss L in Eq
(NN), λN is empirically chosen as 0.0N
All trainable variables are  initialized using Xavier methods [N]
 In the following analysis and experiments, we train sevNNNN    Table N
Performance of baseline models  Model (FN) BW DF-Bic DF-0up Ours  SPMCS (×N) NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN  eral models under different settings
For simplicity, we use  ×(·) to denote scaling factors (e.g
×N, ×N, and ×N)
And F(·) is used as the number of input frames (e.g
FN, FN, and FN)
Moreover, our ConvLSTM based DF net produces multiple outputs (one for each time step), we use  {#N,#N, · · · } to index output
 N.N
Effectiveness of SPMC Layer  We first evaluate the effectiveness of the proposed SPMC  layer
For comparison, a baseline model BW (FN-×N) is used
It is achieved by fixing our system in Fig
N, except  replacing the SPMC layer with backward warping, followed  by bicubic interpolation, which is a standard alignment procedure
An example is shown in Fig
N
In Fig
N(a), bicubic ×N for reference frame contains severe aliasing for the tile patterns
Baseline model BW produces N outputs corresponding to three time steps in Fig
N(b)-(d)
Although  results are sharper when more frames are used, tile patterns  are obviously wrong compared to ground truth in Fig
N(e)
 This is due to loss of sub-pixel information as analyzed in  Section N
The results are similar to the output of single  image SR, where the reference frame dominates
 As shown in Fig
N(f), if we only use one input image in our method, the recovered pattern is also similar to  Fig
N(a)-(d)
However, with more input frames fed into the  system, the restored images dramatically improve, as shown  in Fig
N(g)-(h), which are both sharper and closer to the  ground truth
Quantitative values on our validation set are  listed in Table N
 N.N
Detail Fusion vs
Synthesis  We further investigate if our recovered details truly exist  in original frames
One example is already shown in Fig
N
 Here we conduct a more illustrative experiment by replacing all input frames with the same reference frame
Specifically, Fig
N(f)-(h) are outputs using N consecutive frames  (FN-×N)
The numbers and logo are recovered nicely
How- ever, if we only use N copies of the same reference frame as  input and test them on the same pre-trained model, the results are almost the same as using only one frame
This  manifests that our final result shown in Fig
N(h) is truly  recovered from the N different input frames based on their  internal detail information, rather than synthesized from external examples because if the latter holds, the synthesized  details should also appear even if we use only one reference  frame
 (a) Bicubic ×N (b) DF 0up(c) DF Bic- (d) Ours  Figure N
Detail fusion net with various inputs
 N.N
DF-Net with Various Inputs  Our proposed detail fusion (DF) net takes only JHi as input
To further evaluate if the reference frame is needed,  we design two baseline models
Model DF-bic and DF-0up  respectively add bicubic and zero-upsampled IL0 as another channel of input to DF net
Visual comparison in Fig
N  shows that although all models can recover reasonable details, the emphasis on the reference frame may mislead detail recovery and slightly degrade results quantitatively on  the evaluation set (see Table N)
 N.N
Comparisons with Video SR Methods  We compare our method with previous video SR ones  on the evaluation dataset
BayesSR [N0, NN] is an important method that iteratively estimates motion flow, blur kernel, noise and the HR image
DESR [NN] ensembles “draft”  based on estimated flow, which makes it an intermediate  solution between traditional and CNN-based methods
We  also include a recent deep-learning-based method VSRnet  [NN] in comparison
We use author-provided implementation for all these methods
VESPCN [N] did not provide  code or pre-trained model, so we only list their reported  PSNR/SSIM on the N-video dataset VIDN [N0]
The quantitative results are listed in Table N
Visual comparison is  shown in Fig
N
 N.N
Comparisons with Single Image SR  Since our framework is flexible, we set NF = N to turn it into a single image SR solution
We compare this  approach with three recent image SR methods: SRCNN  [N], FSRCNN [N] and VDSR [NN], on dataset SetN [N] and  SetNN [NN]
To further compare the performance of using  one and multiple frames, we also run all single-image SR  methods and ours under FN setting on our evaluation dataset  SPMCS
The quantitative results are listed in Table N
 NNNN    (a) Bicubic ×N (b) Using copied frames #N (c) Using copied frames #N (d) Using copied  frames #N  (e) Ground truth (f) Using N frames #N (g) Using N frames #N (h) Using N frames #N  Figure N
SR using multiple frames (FN-×N)
(a) Bicubic ×N
(b)-(d) Output for each time step using N reference frames that are with the same content
(e) Ground truth
(f)-(h) Output using N consecutive frames
 (b)BayesSR (c) DESR (e) Ours FN ×N( - )(a) Bicubic ×N (d) VSRnet  Figure N
Comparisons with video SR methods
 NNNN    (a) Bicubic ×N (b) Ours (c) Bicubic ×N (d) Ours  Figure N
Real-world examples under configuration (FN-×N)
 Table N
Comparison with video SR methods (PSNR/SSIM)  Method (FN) Bicubic BayesSR DESR VSRnet Ours (FN)  SPMCS×N NN.NN / 0.NN NN.NN / 0.NN - NN.NN / 0.NN NN.NN / 0.NN SPMCS×N NN.NN / 0.NN NN.NN / 0.NN - NN.NN / 0.NN NN.NN / 0.N0 SPMCS×N NN.0N / 0.NN NN.NN / 0.N0 NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN  Method (FN) Bicubic BayesSR DESR VSRNet Ours (FN)  SPMCS×N NN.NN / 0.NN NN.NN / 0.NN - NN.NN / 0.NN NN.NN / 0.NN SPMCS×N NN.NN / 0.NN NN.NN / 0.NN - N0.NN / 0.NN NN.N0 / 0.N0 SPMCS×N NN.0N / 0.NN NN.0N / 0.NN NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN  Method (FN) BayesSR DESR VSRNet VESPCN Ours (FN)  VidN×N NN.NN / 0.N0 - NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN VidN×N NN.NN / 0.NN NN.N0 / 0.NN NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN  (a) Bicubic ×N (b) SRCNN (c) FSRCNN  (d) VDSR (e) Ours (FN) (f) Ours (FN)  Figure N
Comparisons with single image SR methods
(a) Bicubic ×N
(b)-(d) Output from image SR methods
(e) Our result using N frame
(f) Our result using N frames
 For the FN setting on SetN and SetNN, our method produces comparable or slightly lower-PSNR or -SSIM results
 Under the FN setting, our method outperforms them by a  large margin, indicating that our multi-frame setting can effectively fuse information in multiple frames
An example  is shown in Fig
N, where single image SR cannot recover  the tiled structure of the building
In contrast, our FN model  can faithfully restore it
 Table N
Comparison with image SR methods (PSNR/SSIM)  Method SRCNN FSRCNN VDSR Ours (FN) Ours (FN)  Set N (×N) NN.NN / 0.NN NN.00 / 0.NN NN.NN / 0.NN NN.NN / 0.NN - Set N (×N) NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN - Set N (×N) N0.NN / 0.NN N0.NN / 0.NN NN.NN / 0.NN N0.NN / 0.NN Set NN (×N) NN.NN / 0.NN NN.NN / 0.NN NN.0N / 0.NN NN.N0 / 0.NN - Set NN (×N) NN.N0 / 0.NN NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN - Set NN (×N) NN.NN / 0.NN NN.NN / 0.NN NN.0N / 0.NN NN.NN / 0.NN SPMCS (×N) NN.N0 / 0.NN NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN SPMCS (×N) N0.NN / 0.NN N0.NN / 0.NN NN.NN / 0.NN NN.NN / 0.NN NN.NN / 0.N0 SPMCS (×N) NN.NN / 0.NN NN.NN / 0.NN NN.N0 / 0.NN NN.N0 / 0.N0 NN.NN / 0.NN  N.N
Real-World Examples  The LR images in the above evaluation are produced  though downsampling (bicubic interpolation)
Although  this is a standard approach for evaluation [N, N, NN, NN, NN,  N0], the generated LR images may not fully resemble the  real-world cases
To verify the effectiveness of our method  on real-world data, we captured four examples as shown  in Fig
N
For each object, we capture a short video using  a hand-held cellphone camera, and extract NN consecutive  frames from it
We then crop a NNN × NN0 region from the center frame, and use TLD tracking [NN] to track and crop  the same region from all other frames as the input data to our  system
Fig
N shows the SR result of the center frame for  each sequence
Our method faithfully recovers the textbook  characters and fine image details using the FN-×N model
More examples are included in our supplementary material
 N.N
Model Complexity and Running Time  Using our un-optimized TensorFlow code, the FN-×N model takes about 0.NNs to process N input images with size NN0× NN0 for one HR output
In comparison, reported tim- ings for other methods (FNN) are N hours for Liu et al
[N0],  N0 min
for Ma et al
[NN], and N min
for DESR [NN]
VSRnet [NN] requires ≈N0s for FN configuration
Our method is further accelerated to 0.NNs for FN and 0.NNs for FN
 N
Concluding Remarks  We have proposed a new deep-learning-based approach  for video SR
Our method includes a sub-pixel motion compensation layer that can better handle inter-frame motion for  this task
Our detail fusion (DF) network that can effectively  fuse image details from multiple images after SPMC alignment
We have conducted extensive experiments to validate  the effectiveness of each module
Results show that our  method can accomplish high-quality results both qualitatively and quantitatively, at the same time flexible regarding  scaling factors and numbers of input frames
 References  [N] M
Bevilacqua, A
Roumy, C
Guillemot, and M
L
AlberiMorel
Low-complexity single-image super-resolution based  on nonnegative neighbor embedding
N0NN
N  NNNN    [N] J
Caballero, C
Ledig, A
Aitken, A
Acosta, J
Totz,  Z
Wang, and W
Shi
Real-time video super-resolution with  spatio-temporal networks and motion compensation
arXiv  preprint arXiv:NNNN.0NNN0, N0NN
N, N, N, N  [N] C
Dong, C
C
Loy, K
He, and X
Tang
Learning a deep  convolutional network for image super-resolution
In ECCV,  pages NNN–NNN, N0NN
N, N, N  [N] C
Dong, C
C
Loy, and X
Tang
Accelerating the superresolution convolutional neural network
In ECCV, pages  NNN–N0N
Springer, N0NN
N, N, N  [N] M
Elad and Y
Hel-Or
A fast super-resolution reconstruction algorithm for pure translational motion and common  space-invariant blur
IEEE Transactions on Image Processing, N0(N):NNNN–NNNN, N00N
N  [N] M
A
et
al
TensorFlow: Large-scale machine learning on  heterogeneous systems, N0NN
Software available from tensorflow.org
N  [N] S
Farsiu, M
D
Robinson, M
Elad, and P
Milanfar
Fast  and robust multiframe super resolution
IEEE Transactions  on Image Processing, NN(N0):NNNN–NNNN, N00N
N, N  [N] P
Fischer, A
Dosovitskiy, E
Ilg, P
Häusser, C
Hazırbaş,  V
Golkov, P
van der Smagt, D
Cremers, and T
Brox
 Flownet: Learning optical flow with convolutional networks
 ICCV, N0NN
N, N  [N] X
Glorot and Y
Bengio
Understanding the difficulty of  training deep feedforward neural networks
In Aistats, volume N, pages NNN–NNN, N0N0
N  [N0] M
Jaderberg, K
Simonyan, A
Zisserman, et al
Spatial  transformer networks
In NIPS, pages N0NN–N0NN, N0NN
N,  N  [NN] J
Johnson, A
Alahi, and L
Fei-Fei
Perceptual losses for  real-time style transfer and super-resolution
In ECCV, N0NN
 N  [NN] Z
Kalal, K
Mikolajczyk, and J
Matas
Trackinglearning-detection
IEEE Trans
Pattern Anal
Mach
Intell.,  NN(N):NN0N–NNNN, N0NN
N  [NN] A
Kanazawa, D
W
Jacobs, and M
Chandraker
Warpnet:  Weakly supervised matching for single-view reconstruction
 CVPR, N0NN
N  [NN] A
Kappeler, S
Yoo, Q
Dai, and A
K
Katsaggelos
 Video super-resolution with convolutional neural networks
 IEEE Transactions on Computational Imaging, N(N):N0N–  NNN, N0NN
N, N, N, N  [NN] J
Kim, J
Kwon Lee, and K
Mu Lee
Accurate image superresolution using very deep convolutional networks
In CVPR,  June N0NN
N, N, N  [NN] J
Kim, J
Kwon Lee, and K
Mu Lee
Deeply-recursive  convolutional network for image super-resolution
In CVPR,  June N0NN
N  [NN] D
P
Kingma and J
Ba
Adam: A method for stochastic  optimization
In ICLR, N0NN
N  [NN] C
Ledig, L
Theis, F
Huszár, J
Caballero, A
Aitken, A
Tejani, J
Totz, Z
Wang, and W
Shi
Photo-realistic single image super-resolution using a generative adversarial network
 arXiv preprint arXiv:NN0N.0NN0N, N0NN
N  [NN] R
Liao, X
Tao, R
Li, Z
Ma, and J
Jia
Video superresolution via deep draft-ensemble learning
In ICCV, pages  NNN–NNN, N0NN
N, N, N, N, N  [N0] C
Liu and D
Sun
A bayesian approach to adaptive video  super resolution
In CVPR, pages N0N–NNN
IEEE, N0NN
N,  N, N, N, N  [NN] Z
Liu, R
Yeh, X
Tang, Y
Liu, and A
Agarwala
Video  frame synthesis using deep voxel flow
N0NN
N  [NN] W
Luo, A
G
Schwing, and R
Urtasun
Efficient deep learning for stereo matching
In CVPR, pages NNNN–NN0N, N0NN
 N  [NN] Z
Ma, R
Liao, X
Tao, L
Xu, J
Jia, and E
Wu
Handling  motion blur in multi-frame super-resolution
In CVPR, pages  NNNN–NNNN, N0NN
N, N, N, N, N  [NN] X
Mao, C
Shen, and Y.-B
Yang
Image restoration using very deep convolutional encoder-decoder networks with  symmetric skip connections
In NIPS, pages NN0N–NNN0,  N0NN
N  [NN] N
Mayer, E
Ilg, P
Häusser, P
Fischer, D
Cremers,  A
Dosovitskiy, and T
Brox
A large dataset to train convolutional networks for disparity, optical flow, and scene flow  estimation
In CVPR, N0NN
N, N  [NN] W
Shi, J
Caballero, F
Huszár, J
Totz, A
P
Aitken,  R
Bishop, D
Rueckert, and Z
Wang
Real-time single image and video super-resolution using an efficient sub-pixel  convolutional neural network
In CVPR, pages NNNN–NNNN,  N0NN
N, N  [NN] S
Su, M
Delbracio, J
Wang, G
Sapiro, W
Heidrich,  and O
Wang
Deep video deblurring
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
N  [NN] H
Takeda, P
Milanfar, M
Protter, and M
Elad
Superresolution without explicit subpixel motion estimation
IEEE  Transactions on Image Processing, NN(N):NNNN–NNNN, N00N
 N  [NN] S
Xingjian, Z
Chen, H
Wang, D.-Y
Yeung, W.-k
Wong,  and W.-c
Woo
Convolutional lstm network: A machine  learning approach for precipitation nowcasting
In NIPS,  pages N0N–NN0, N0NN
N, N  [N0] J
J
Yu, A
W
Harley, and K
G
Derpanis
Back to  basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N, N  [NN] J
Zbontar and Y
LeCun
Stereo matching by training a convolutional neural network to compare image patches
Journal of Machine Learning Research, NN:N–NN, N0NN
N  [NN] R
Zeyde, M
Elad, and M
Protter
On single image scale-up  using sparse-representations
In International Conference on  Curves and Surfaces, pages NNN–NN0
Springer, N0N0
N  NNN0Spatial-Aware Object Embeddings for Zero-Shot Localization and Classification of Actions   Spatial-Aware Object Embeddings for Zero-Shot Localization  and Classification of Actions  Pascal Mettes and Cees G
M
Snoek  University of Amsterdam  Abstract  We aim for zero-shot localization and classification of  human actions in video
Where traditional approaches  rely on global attribute or object classification scores for  their zero-shot knowledge transfer, our main contribution  is a spatial-aware object embedding
To arrive at spatial awareness, we build our embedding on top of freely  available actor and object detectors
Relevance of objects  is determined in a word embedding space and further enforced with estimated spatial preferences
Besides local  object awareness, we also embed global object awareness  into our embedding to maximize actor and object interaction
Finally, we exploit the object positions and sizes in  the spatial-aware embedding to demonstrate a new spatiotemporal action retrieval scenario with composite queries
 Action localization and classification experiments on four  contemporary action video datasets support our proposal
 Apart from state-of-the-art results in the zero-shot localization and classification settings, our spatial-aware embedding is even competitive with recent supervised action localization alternatives
 N
Introduction  We strive for the localization and classification of human actions like Walking a dog and Skateboarding without the need for any video training examples
The common  approach in this challenging zero-shot setting is to transfer action knowledge via a semantic embedding build from  attributes [NN, N0, NN] or objects [N, NN, NN]
As the semantic embeddings are defined by image or video classifiers,  they are unable, nor intended, to capture the spatial interactions an actor has with its environment
Hence, it is hard  to distinguish who is Throwing a baseball and who is Hitting a baseball when both actions occur within the same  video
We propose a spatial-aware object embedding for localization and classification of human actions in video, see  Figure N
 We draw inspiration from the supervised action classification literature, where the spatial connection between  Kicking a ball  Actors Relevant objects Spa�al rela�ons  Ac�on localiza�on  Figure N: Spatial-aware object embedding
Actions are  localized and classified by information about actors, relevant objects, and their spatial relations
 actors and objects has been well recognized, e.g
[NN, NN,  NN, NN]
Early work focused on capturing actors and objects implicitly in a low-level descriptor [N, N0], while more  recently the benefit of explicitly representing detected objects [N], their scores, and spatial properties was proven effective [NN, NN, NN, NN]
Both [N] and [N0] demonstrate the  benefit of temporal actor and object interaction, by linking  detected bounding boxes over time via trackers
By doing  so, they are also capable of (supervised) action localization
 We also detect actors and objects, and link them over time to  capture spatio-temporal interactions
Different from all of  the above works, we do no rely on any action class and/or  action video supervision to get to our recognition
Instead,  we introduce an embedding built upon actor and object detectors that allows for zero-shot action classification and localization in video
 Our main contribution is a spatial-aware object embedding for zero-shot action localization and classification
The  spatial-aware embedding incorporates word embeddings,  NNNNN    box locations for actors and objects, as well as their spatial relations, to generate action tubes
This enables us to  both classify videos and to precisely localize where actions  occur
Our spatial-aware embedding is naturally extended  with contextual awareness from global objects
We furthermore show how our embedding generalizes to any query  involving objects, spatial relations, and their sizes in a new  spatio-temporal action retrieval scenario
Action localization and classification experiments on four contemporary  action video datasets support our proposal
 N
Related work  N.N
Supervised action localization and classification  A wide range of works have proposed representations  to classify actions given video examples
Such representations include local spatio-temporal interest points and  features [NN, NN, NN] and local trajectories [N, N0], typically aggregated into VLAD or Fisher vector representations [NN, NN]
Recent works focus on learning global  representations from deep networks, pre-trained on optical  flow [NN] or large-scale object annotations [N0, NN, NN]
We  also rely on deep representations for our global objects, but  we emphasize on local objects and we aim to classify and  localize actions without the need for any video example
 For spatio-temporal action localization, a popular approach is to split videos into action proposals; spatiotemporal tubes in videos likely to contain an action
Annotated tubes from example videos are required to train a  model to select the best action proposals at test time
Action  proposal methods include merging supervoxels [NN, NN],  merging trajectories [N, NN], and detecting actors [NN]
The  current state-of-the-art action localizers employ Faster RCNN [NN] trained on bounding box annotations of actions in  video frames [NN, NN]
We are inspired by the effectiveness  of actor detections and Faster R-CNN for localization, but  we prefer commonly available detectors trained on images
 We employ these detectors as input to our spatial-aware embedding for localization in video in a zero-shot setting
 N.N
Zero-shot action localization and classification  Inspired by zero-shot image classification [NN], several  works have performed zero-shot action classification by  learning a mapping of actions to attributes [NN, N0, NN]
 Models are trained for the attributes from training videos  of other actions and used to compare test videos to unseen  actions
Attribute-based classification has been extended  e.g
using transductive learning [N, NN] and domain adaption [NN, NN]
Due to the necessity to manually map each  action to global attributes a priori, these approaches do not  generalize to arbitrary zero-shot queries and are unable to  localize actions, which is why we do not employ attributes  in our work
 Rather than mapping actions to attributes, test actions  can also be mapped directly to actions used for training
 Li et al
[NN] map visual video features to a semantic space  shared by training and test actions
Gan et al
[NN] train  a classifier for an unseen action by relating the action to  training actions at several levels of relatedness
Although  the need for attributes is relieved with such mappings, this  approach still requires videos of other actions for training  and is only able to classify actions
We localize and classify  actions without using any videos of actions during training
 A number of works have proposed zero-shot classification by exploiting large amounts of image and object labels [N]
Given deep networks trained on image data, these  approaches map object scores in videos to actions e.g
using word vectors [N, NN, NN, NN] or auxiliary textual descriptions [N0, NN, NN]
Objects as the basis for actions results  in effective zero-shot classification and generalizes to arbitrary actions
However, these approaches are holistic; object  scores are computed over whole videos
In this work, we  take the object-based perspective to a local level, which allows us to model the spatial interaction between actors and  objects for action localization, classification, and retrieval
 The work of Jain et al
[NN] has previously performed  zero-shot action localization
Their approach first generates  action proposals
In a second pass, each proposal is represented with object classification scores
The proposals best  matching the action name in wordNvec space are selected
 Their approach does not use any object detectors, nor is  there any explicit notion of spatial-awarenes inside each action proposal
Finally, spatial relations between actors and  objects are ignored
As we will show in the experiments,  inclusion of our spatial-awareness solves these limitations  and leads to a better zero-shot action localization
 N
Spatial-aware object embeddings  In our zero-shot formulation, we are given a set of test  videos V and a set of action class names Z 
We aim to clas- sify each video to its correct class and to discover the spatiotemporal tubes encapsulating each action in all videos
To  that end, we propose a spatial-aware embedding; scored action tubes from interactions between actors and local objects
We present our embeddings in three steps: (i) gathering prior knowledge on actions, actors, objects, and their  interactions, (ii) computing spatial-aware embedding scores  for bounding boxes, and (iii) linking boxes into action tubes
 N.N
Prior knowledge  Local object detectors
We first gather a set of local detectors pre-trained on images
Let O = {OD, ON} denote the objects with detectors OD and names ON 
Furthermore,  let A = {AD,actor} denote the actor detector
Each de- tector outputs a set of bounding boxes with corresponding  object probability scores per video frame
 NNNN    (a) Skateboard
(b) Bicycle
(c) Traffic light
 min max  Figure N: Examples of preferred spatial relations of objects relative to actors
In line with our intuition, skateboards are typically on or below the actor, while bicycles  are typically to the left or right of actors and traffic lights  are above the actors
 Textual embedding
Given an action class name Z ∈ Z , we aim to select a sparse subset of objects OZ ⊂ O relevant for the action
For the selection, we rely on semantic textual  representations as provided by wordNvec [NN]
The similarity between object o and the action class name is given as:  w(o, Z) = cos(e(oN ), e(Z)), (N)  where e(·) states the wordNvec representation of the name
We select the objects with maximum similarity to the action
 Actor-object relations
We exploit that actors interact  with objects in preferred spatial relations
To do so, we explore where objects tend to occur relative to the actor
Since  we can not learn precise spatial relations between actors and  objects from examples, we aim to use common spatial relations between actors and objects, as can be mined from  large-scale image data sets
We discretize the spatial relations into nine relative positions, representing the preposition in front of and the eight basic prepositions around  the actor, i.e
left of, right of, above, below, and the four  corners (e.g
above left)
For each object, we obtain a  nine-dimensional distribution specifying its expected location relative to the actor, as detailed in Figure N
 N.N
Scoring actor boxes with object interaction  We exploit our sources of prior knowledge to compute a  score for the detected bounding boxes in all frames of each  test video V ∈ V 
Given a bounding box b in frame F of video V , we define a score function that incorporates the  presence of (i) actors, (ii) relevant local objects, and (iii) the  preferred spatial relation between actors and objects
A visual overview of the three components is shown in Figure N
 More formally, we define a score function for box b given  an action class Z as:  s(b, F, Z) = p(AD|b) + ∑  o∈OZ  r(o, b, F, Z), (N)  where p(AD|b) is the probability of an actor being present in bounding box b as specified by the detector AD
The  function r expresses the object presence and relation to the  actor, it is defined as:  r(o, b, F, Z) = w(o, Z) ·  (  max f∈Fn  p(oD|f) ·m(o,A, b, f)  )  ,  (N)  where w(o, Z) states the semantic relation score between object o and action Z and Fn states all bounding boxes  within the neighourhood of box b in frame F 
The second part of Equation N states that we are looking for a box  f around b that maximizes the joint probability of the presence of object o (the function p(oD|f)), the match between the spatial relations of (b, f) and the prior relations of the actor and object o (the function m)
We define the spatial  relation match as:  m(o,A, b, f) = N− JSDN(d(A, o)||d(b, f)), (N)  where JSDN(·||·) ∈ [0, N] denotes the Jensen-Shannon Di- vergence with base N logarithm [NN]
Intuitively, the JensenShannon Divergence, a symmetrized and bounded variant  of the Kullback-Leibler divergence, determines to what extent the two N-dimensional distributions match
The more  similar the distributions, the lower the divergence, hence the  need for the inversion as we aim for maximization
 N.N
Linking spatial-aware boxes  The score function of Equation N provides a spatialaware embedding score for each bounding box in each  frame of a video
We apply the score function to the boxes  of all actor detections in each frame
We form tubes from  the individual box scores by linking them over time [NN]
 We link those boxes over time that by themselves have a  high score from our spatial-aware embedding and have a  high overlap amongst each other
This maximization problem is solved using dynamic programming with the Viterbi  algorithm
Once we have a tube from the optimization, we  remove all boxes from that tube and compute the next tube  from the remaining boxes
 Let T denote a discovered action tube in a video
The  corresponding score is given as:  temb(T, Z) = N  |T |  ∑  t∈T  s(tb, tF , Z), (N)  where tb and tF denote a bounding box and the corresponding frame in tube T 
 In summary, we propose spatial-aware object embeddings for actions; tubes through videos by linking boxes  based on the zero-shot likelihood from the presence of actors, the presence of relevant objects around the actors, and  the expected spatial relations between objects and actors
 N
Local and global object interaction  To distinguish tubes from different videos in a collection,  contextual awareness in the form of relevant global object  NNNN    (a) Video frame
(b) Actor detection
(c) Object detection (horse)
(d) Spatial relation match
 Figure N: Example of our spatial-aware embedding
The actor sitting on the left horse (green box) is most relevant for the  action Riding horse based on the actor detection, horse detection, and spatial relations between actors and horses
 classifiers is also a viable source of information
Here, we  first outline how to obtain video-level scores based on object classifiers
Then, we show how to compute spatial- and  global-aware embeddings for action localization, classification, and retrieval
 N.N
Scoring videos with global objects  Let G = {GC , GN} denote the set of global objects with corresponding classifiers and names
Different from the local objects O, these objects provide classifier scores over a whole video
Given an action class name Z, we again select  the top relevant objects GZ ⊂ G using the textual embed- ding
The score of a video V is then computed as a linear  combination of the wordNvec similarity and classifier probabilities over the top relevant objects:  tglobal(V, Z) = ∑  g∈GZ  w(g, Z) · p(g|V ), (N)  where p(g|V ) denotes the probability of global object g of being in video V 
 N.N
Spatial- and global-aware embedding  The information from local and global objects is combined into a spatial- and global-aware embedding
Here,  we show how this embedding is employed for spatial-aware  action localization, classification, and retrieval
 Action localization
For localization, we combine the  tube score from our spatial-aware embedding with the video  score from the global objects into a score for each individual  tube T as:  t(T, V, Z) = temb(T, Z) + tglobal(V, Z)
(N)  We note that incorporating scores from global objects does  not distinguish tubes from the same video
The global  scores are however discriminative for distinguishing tubes  from different videos in a collection V 
We compute the fi- nal score for all tubes of all videos in V using Equation N
We then select the top scoring tubes per video, and rank the  tubes over all videos based on their scores for localization
 Action classification
For classification purposes, we  are no longer concerned about the precise location of the  tubes from the spatial-aware embeddings
Therefore, we  compute the score of a video V given an action class name  Z using a max-pooling operation over the scores from all  tubes TV in the video
The max-pooled score is then combined with the video score from the global objects
The  predicted class for video V is determined as the class with  the highest combined score:  c∗V = argmax Z∈Z  (  max T∈TV  temb(T, Z) + tglobal(V, Z)  )  
(N)  Spatial-aware action retrieval
Spatial-aware action retrieval from user queries resembles action localization, i.e
 rank the most relevant tubes the highest
However, different  from localization, we now have the opportunity to specify  actor and object relations via the search query
Given the  effectiveness of size in actor-object interactions [N], we can  also allow users to specify a relative object size r
By altering the size of queries objects, different localizations can  be retrieved of the same action
To facilitate spatial-aware  action retrieval, we alter the spatial relation match of Equation N with a match for a specified relative object size:  q(o,A, b, f, r) = m(o,A, b, f) +  (  N− | s(b)  s(f) − r|  )  , (N)  where s(·) denotes the size of a bounding box
Substituting the spatial relation match with Equation N, we again rank  top scoring tubes, but now by maximizing a match to userspecified objects, spatial relations, and relative size
 N
Experimental setup  N.N
Datasets  UCF Sports consists of NN0 videos from N0 sport action  categories, such as Skateboarding, Horse riding, and Walking [NN]
We employ the test split as suggested in [NN]
 UCF N0N consists of NN,NN0 videos from N0N action categories, such as Skiing, Basketball dunk, and Surfing [NN]
 We use this dataset for classification and use the test splits  as provided in [NN], unless stated otherwise
 J-HMDB consists of NNN videos from NN actions, such  as Sitting, Laughing, and Dribbling [NN]
We use the  NNNN    Localization (mAP @ 0.N) Classification (mean accuracy)  # local objects # local objects  0 N N N 0 N N N  Embedding I: Actor-only 0.0NN - - - 0.N00 - - Embedding II: Actors and objects - 0.NNN 0.NNN 0.NNN - 0.N0N 0.NNN 0.NNN  Embedding III: Spatial-aware - 0.NNN 0.N0N 0.NNN - 0.NN0 0.NNN 0.NNN  Table N: Influence of spatial awareness
On UCF Sports we compare our spatial-aware object embedding to two other  embeddings; using only the actors and using actors with objects, while ignoring their spatial relations
Our spatial-aware  embedding is preferred for both localization (one object per action) and classification (five objects per action)
 bounding box around the binary action masks as the spatiotemporal annotations for localization
We use the test split  as suggested in [NN]
 HollywoodNTubes consists of N,N0N videos from the  HollywoodN dataset [NN], supplemented with spatiotemporal annotations for localization [NN]
Actions include  Fighting with a person, Eating, and Getting out of a car
 We use the test split as suggested in [NN]
 N.N
Implementation details  Textual embedding
To map the semantics of actions to objects, we employ the skip-gram network of  wordNvec [NN] trained on the metadata of the images and  videos from the YFCCN00M dataset [NN]
This model outputs a N00-dimensional representation for each word
If an  action or object consists of multiple words, we average the  representations of the individual words [NN]
 Actor and object detection
For the detection of both  the actors and the local objects, we use Faster R-CNN [NN],  pre-trained on the MS-COCO dataset [NN]
This network  consists of the actor class and NN other objects, such as  snowboard, horse, and toaster
After non-maximum suppression, we obtain roughly N0 detections for each object  per frame
We apply the network to each frame (UCF  Sports, J-HMDB), or each Nth frame (UCF N0N, Holly- woodNTubes) followed by linear interpolation
 Spatial relations
The spatial relations between actors  and objects are also estimated from the MS-COCO dataset
 For each object instance, we examine the spatial relations  with the closest actor (if any actor is close to the object)
 We average the relations over all instances for each object
 Object classification
For the global objects, we employ a GoogLeNet network [NN], pre-trained on a NN,NNNcategory shuffle [NN] of ImageNet [N]
This network is applied to each Nth frame of each video
For each frame, we obtain the object probabilities at the softmax layer and average the probabilities over the entire video
Following [NN],  we select the top N00 most relevant objects per action
 Evaluation
For localization, we compute the spatiotemporal intersection-over-union between top ranked actor  tubes and ground truth tubes
We report results using both  the (mean) Average Precision and AUC metrics
For classification, we evaluate with mean class accuracy
 N
Experimental results  N.N
Spatial-aware embedding properties  In the first experiment, we focus on the properties of our  spatial-aware embedding, namely the number of local objects to select and the influence of the spatial relations
We  also evaluate qualitatively the effect of selecting relevant  objects per action
We evaluate these properties on the UCF  Sports dataset for both localization and classification
 Influence of local objects
We evaluate the performance  using three settings of our embeddings
The first setting  is using solely the actor detections for scoring bounding  boxes
The second setting uses both the actor and the top  relevant objects(s), but ignores the spatial relations between  actors and objects
The third setting is our spatial-aware  embedding, which combines the information from actors,  objects, and their spatial relations
 In Table N, we provide both the localization and classification results
For localization using only the actor results  in tubes that might overlap well with the action of interest,  but there is no direct means to separate tubes containing different actions
This results in low Average Precision scores
 For classification, using only the actor results in weak accuracy scores
This is because there is again no mechanism to  discriminate videos containing different actions
 The second row of Table N shows the result when incorporating local object detections
For both localization  and classification, there is a considerable increase in performance, indicating the importance of detections of relevant  objects for zero-shot action localization and classification
 In the third row of Table N, we show the performance  of our spatial-aware embedding
The embedding outperforms the other settings for both localization and classification
This result shows that gathering and capturing information about the relative spatial locations of objects and actors provides valuable information about actions in videos
 NNNN    Skateboarding  Top object: skateboard  Riding a horse  Top object: horse  Swinging on a bar  Top object: table  Kicking  Top object: tie  Figure N: Qualitative action localization results
For Skateboarding and Riding a horse, relevant objects (blue) aid our  localization (red)
For Swinging on a bar and Kicking, incorrectly selected objects result in incorrect localizations
We  expect that including more object detectors into our embedding will further improve results
 0.N 0.N 0.N 0.N 0.N 0.N Overlap threshold  0.0  0.N  0.N  0.N  0.N  0.N  A U  C  Spatial-aware embeding  Spatial- and global-aware embeddding  Figure N: Local and global object interaction effect on  localization
Adding global object awarereness further improves our spatial-aware object embedding on UCF Sports,  especially at low overlap thresholds
 The spatial-aware embedding is most beneficial for the action Riding a horse (from 0.0N to 0.NN mAP), due to the  consistent co-occurrence of actors and horses
Contrarily,  the performance for Running remains unaltered, which is  because no object relevant to the action is amongst the available detectors
 We have additionally performed an experiment with finer  grid sizes on UCF Sports
For localization with the top-N  objects, we reach an mAP of 0.NN0 (NxN grid) and 0.NNN  (NxN grid), compared to a score of 0.NNN with the NxN grid
 Overall, the scores descrease slightly with finer grid sizes,  indicating that coarse spatial relations are preferred over  fine spatial relations
 How many local objects? In Table N we also consider  how many relevant local objects to maintain per action
For  localization, we observe a peak in performance using the  top-N local object per action, with a mean Average Precision (mAP) of 0.NNN at an overlap threshold of 0.N; a sharp  increase in performance over the 0.0NN mAP using only  the actor
When more objects are used, the performance  of our embeddings degrades slightly, indicating that actors  are more likely to interact with a single object than multiple  objects on a local level
At least for the UCF Sports dataset
 For classification, we observe a reverse correlation; the  more local objects in our embedding, the higher the classification accuracy
This result indicates that for classification,  we want to aggregate more information about object presence in videos, rather than exploit the single most relevant  object per action
This is because a precise overlap with the  action in each video is no longer required for classification
 We exploit this relaxation with the max-pooling operation  in the video-level scoring of Equation N
 Selecting relevant objects
In our zero-shot formulation, a correct action recognition depends on detecting objects relevant to the action
We highlight the effect of detecting relevant objects in Figure N
For successful actions  such as Skateboarding and Riding a horse, the detection of  respectively skateboards and horses help to generate a desirable action localization
For the actions Swinging on a bar  and Kicking, the top selected objects are however incorrect,  either because no relevant object is available or because of  ambiguity in the wordNvec representations
 Conclusions
We conclude from this experiment that our  spatial-aware embedding is preferred over only using the  actor and using actors and objects without spatial relations
 Throughout the rest of the experiments, we will employ the  spatial-aware embedding, using the top-N object for localization and the top-N for classification
 N.N
Local and global object interaction  In the second experiment, we focus on the localization  and classification performance when incorporating contextual awareness from global object scores into the spatialaware embedding
We perform the evaluation on the UCF  Sports dataset
 Effect on localization
In Figure N, we show the AUC  scores across several overlap thresholds
We show the results using our spatial-aware embedding and the combined  spatial- and global-aware embedding
 We observe that across all overlap thresholds, adding  global object classifier scores to our spatial-aware embedding improves the localization performance
This result inNNNN    Backpack (0.NN) on actor Sports ball (0.N0) right of actor Sports ball (0.NN) right of actor  Figure N: Spatial-aware action retrieval
Top retrieved results on J-HMDB given specified queries
Our retrieved localizations (red) reflect the prescribed object (blue), spatial relation, and object size
 Accuracy  Random 0.N00  Jain et al
[NN] 0.NNN  Spatial-aware embedding 0.NNN  Spatial- and global-aware embedding 0.NNN  Table N: Local and global object interaction for classification
Adding global object awarereness improves our  spatial-aware embedding considerably on UCF Sports
 dicates the importance of global object classification scores  for discriminating tubes from different videos
The increase  in performance is most notable at lower overlap thresholds,  which we attribute to the fact that no localization information is provided by the global objects
The higher the overlap threshold, the more important selecting the right tube in  each video becomes, and consequently, the less important  the global object scores become
 Effect on classification
In Table N, we show the classification accuracies on the UCF Sports dataset
We first observe that our spatial-aware embedding yields results competitive to the global object approach of Jain et al
[NN], who  also report zero-shot classification on UCF Sports
We also  observe a big leap in performance when using our spatialand global-aware embedding, with an accuracy of 0.NNN
 We note that the big improvement is partially due to our  deep network for the global object classifiers, namely a  GoogleNet trained on NNk objects [NN]
We have therefore  also performed an experiment with our spatial- and globalaware embedding using the network of [NN]
We achieved  a classification accuracy of 0.NNN, still a considerable improvement over the accuracy of 0.NNN reported in [NN]
 Conclusion
We conclude from this experiment that including global object classification scores into our spatialaware embedding improves both the zero-shot localization  and classification performance
We will use this embedding  for our comparison to related zero-shot action works
 N.N
Spatial-aware action retrieval  For the third experiment, we show qualitatively that our  spatial-aware embedding is not restricted to specific action  queries and spatial relations
We show that any object, any  spatial relation, and any object size can be specified as a  query for spatial-aware action retrieval
For this experiment,  we rely on the test videos from J-HMDB
In Figure N, we  show three example queries and their top retrieved actions
 The example on the left shows how we can search for  a specific combination of actor, object, and spatial relation
 The examples in the middle and right show that specifying  different sizes for the query object leads to a different retrieval
The examples show an interaction with a baseball  (middle) and a soccer ball (right), which matches with the  desired object sizes in the queries
 We conclude from this experiment that our embedding  can provide spatio-temporal action retrieval results for arbitrarily specified objects, spatial relations, and object sizes
 N.N
Comparison to state-of-the-art  For the fourth experiment, we perform a comparative  evaluation of our approach to the state-of-the-art in zeroshot action classification and localization
For localization,  we also compare our results to supervised approaches, to  highlight the effectiveness of our approach
 Action classification
In Table N, we provide the zeroshot classification results on the UCF-N0N dataset, which  provides the most comparisons to related zero-shot approaches
Many different data splits and evaluation setups  have been proposed, making a direct comparison difficult
 We have therefore applied our approach to the three most  common types of zero-shot setups, namely using the standard supervised test splits, using N0 randomly selected actions for testing, and using N0 actions randomly for testing
 In Table N, we first compare our approach to Jain et  al
[NN], who like us do not require training videos
With  an accuracy of 0.NNN we ouperform their approach (0.N0N)
 We also compare to approaches that require training videos  for their zero-shot transfer, using author suggested splits
 For the (random) NN/N0 splits for training and testing, we  NNNN    Train Test Splits Accuracy  Jain et al
[NN] – N0N N 0.N0N ± 0.00 Ours – N0N N 0.NNN ± 0.00  Kodirov et al
[NN] NN N0 N0 0.NN0 ± 0.0N Liu et al
[N0] NN N0 N 0.NNN ± 0.0N Xu et al
[NN] NN N0 N0 0.NNN ± 0.0N Xu et al
[NN] NN N0 N0 0.NNN ± 0.0N Xu et al
[NN] NN N0 N0 0.NNN ± 0.0N Li et al
[NN] NN N0 N0 0.NNN ± 0.0N Ours – N0 N0 0.N0N ± 0.0N  Kodirov et al
[NN] NN N0 N0 0.NNN ± 0.0N Gan et al
[NN] NN N0 N0 0.NNN ± 0.0N Ours – N0 N0 0.NNN ± 0.0N  Table N: Comparison to state-of-the-art for zero-shot action classification on UCFN0N
For all protocols and test  splits we outperform the state-of-the-art, even without us  needing any training videos for action transfer
 obtain an accuracy of 0.N0N
Outperform the next best zeroshot approach (0.NNN) considerably
We like to stress that  all other approaches in this regime use the videos from the  training split to guide their zero-shot transfer, while we  ignore these videos
When using N0 actions for testing,  the difference to other zero-shot approaches increases from  0.NNN [NN] and 0.NNN [NN] to 0.NNN
The lower the number  of actions compared to the number of objects in our embedding, the more beneficial for our approach
 Action localization
In Table N, we provide the localization results on the UCF Sports, HollywoodNTubes, and  J-HMDB datasets
We first compare our result to Jain et  al
[NN] on UCF Sports in Table Na, which is the only zeroshot action localization work in the literature we are aware  of
Across all overlap thresholds, we clearly outperform  their approach
At the challenging overlap threshold of 0.N,  we obtain an AUC score of 0.NNN, compared to 0.0NN for  Jain et al
[NN]; a considerable improvement
 Given the lack of comparison for zero-shot localization,  we also compare our approach to several supervised localization approaches on UCF Sports (Table Na) and HollywoodNTubes (Table Nb)
We observe that we can achieve results competitive to supervised approaches [N, NN, NN], especially at high overlaps
Naturally, the state-of-the-art supervised approach [NN] performs better, but requires thousands  of hard to obtain video tube annotations for training
Our  achieved performance indicates the effectiveness of our approach, even though no training examples of action videos  or bounding boxes are required
Finally, to highlight our  performance across multiple datasets, we provide the first  zero-shot localization results on J-HMDB in Table Nc
 Conclusion
For classification, we outperform other  zero-shot approaches across all common evaluation setups
 UCF Sports  0.N 0.N 0.N 0.N 0.N  Supervised  Gkioxari et al
[NN] 0.NN0 0.NN0 0.NN0 0.NN0 0.NNN  Jain et al
[NN] 0.NN0 0.NNN 0.NN0 0.NN0 0.NN0  Tian et al
[NN] 0.NNN 0.NNN 0.NNN 0.NNN 0.NN0  Cinbis et al
[N] 0.NNN 0.NNN 0.NNN 0.N0N 0.0NN  Zero-shot  Jain et al
[NN] 0.NNN 0.NNN 0.NNN 0.0NN 0.0NN  Ours 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  (a)  HollywoodNTubes  0.N 0.N 0.N 0.N 0.N  Supervised  Mettes et al
[NN] 0.NNN 0.NN0 0.NNN 0.0NN 0.0NN  Cinbis et al
[N] 0.NNN 0.0NN 0.0N0 0.00N 0.00N  Zero-shot  Ours 0.NN0 0.NNN 0.0NN 0.0NN 0.0N0  (b)  J-HMDB  0.N 0.N 0.N 0.N 0.N  Zero-shot  Ours 0.NNN 0.NNN 0.N0N 0.NNN 0.NN0  (c)  Table N: Comparison to state-of-the-art for zero-shot action localization on (a) UCF Sports, (b) HollywoodNTubes,  and (c) J-HMDB
The only other zero-shot action localization approach is [NN], which we outperform considerably
 We also compare with several supervised alternatives
We  are competitive, especially at high overlaps thresholds
 For localization, we outperform the zero-shot localization  of [NN], while even being competitive to several supervised  action localization alternatives
 N
Conclusions  We introduce a spatial-aware embedding for localizing  and classifying actions without using any action video during training
The embedding captures information from actors, relevant local objects, and their spatial relations
The  embedding further profits from contextual awareness by  global objects
Experiments show the benefit of our embeddings, resulting in state-of-the-art zero-shot action localization and classification
Finally, we demonstrate our embedding in a new spatio-temporal action retrieval scenario with  queries containing object positions and sizes
 Acknowledgements This research is supported by the STW STORY project
 NNN0    References  [N] B
B
Amor, J
Su, and A
Srivastava
Action recognition  using rate-invariant analysis of skeletal shape trajectories
 TPAMI, NN(N):N–NN, N0NN
N  [N] S
Cappallo, T
Mensink, and C
Snoek
Video stream retrieval of unseen queries using semantic memory
BMVC,  N0NN
N, N  [N] W
Chen and J
J
Corso
Action detection by implicit intentional motion clustering
In ICCV, N0NN
N  [N] W
Choi, K
Shahid, and S
Savarese
What are they doing?  : Collective activity classification using spatio-temporal relationship among people
In ICCV wshop, N0NN
N  [N] R
G
Cinbis, J
Verbeek, and C
Schmid
Multi-fold mil  training for weakly supervised object localization
In CVPR,  N0NN
N  [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
ImageNet: A large-scale hierarchical image database
 In CVPR, N00N
N, N  [N] V
Escorcia and J
C
Niebles
Spatio-temporal human-object  interactions for action recognition in videos
In ICCV wshop,  N0NN
N, N  [N] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and D
Ramanan
Object detection with discriminatively trained part  based models
TPAMI, N0N0
N  [N] Y
Fu, T
Hospedales, T
Xiang, Z
Fu, and S
Gong
Transductive multi-view embedding for zero-shot recognition and  annotation
In ECCV, N0NN
N  [N0] C
Gan, M
Lin, Y
Yang, G
de Melo, and A
G
Hauptmann
Concepts not alone: Exploring pairwise relationships  for zero-shot video activity recognition
In AAAI, N0NN
N  [NN] C
Gan, T
Yang, and B
Gong
Learning attributes equals  multi-source domain generalization
In CVPR, N0NN
N, N  [NN] C
Gan, Y
Yang, L
Zhu, D
Zhao, and Y
Zhuang
Recognizing an action using its name: A knowledge-based approach
 IJCV, N0NN
N  [NN] G
Gkioxari and J
Malik
Finding action tubes
In CVPR,  N0NN
N, N, N  [NN] A
Gupta and L
S
Davis
Objects in action: An approach  for combining action understanding and object perception
 In CVPR, N00N
N  [NN] A
Habibian, T
Mensink, and C
Snoek
VideoNvec embeddings recognize events when examples are scarce
TPAMI,  N0NN
N  [NN] D
Han, L
Bo, and C
Sminchisescu
Selection and context  for action recognition
In ICCV, N00N
N  [NN] N
Inoue and K
Shinoda
Adaptation of word vectors using  tree structure for visual semantics
In MM, N0NN
N  [NN] M
Jain, J
an Gemert, H
Jégou, P
Bouthemy, and C
Snoek
 Action localization with tubelets from motion
In CVPR,  N0NN
N, N  [NN] M
Jain, J
van Gemert, T
Mensink, and C
Snoek
ObjectsNaction: Classifying and localizing actions without any  video example
In ICCV, N0NN
N, N, N, N, N  [N0] M
Jain, J
C
van Gemert, and C
Snoek
What do NN,000  object categories tell us about classifying and localizing actions? In CVPR, N0NN
N  [NN] H
Jhuang, J
Gall, S
Zuffi, C
Schmid, and M
J
Black
 Towards understanding action recognition
In ICCV, N0NN
 N, N  [NN] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In CVPR, N0NN
N  [NN] E
Kodirov, T
Xiang, Z
Fu, and S
Gong
Unsupervised  domain adaptation for zero-shot learning
In ICCV, N0NN
N,  N, N  [NN] C
H
Lampert, H
Nickisch, and S
Harmeling
Attributebased classification for zero-shot visual object categorization
TPAMI, N0NN
N  [NN] T
Lan, Y
Wang, and G
Mori
Discriminative figure-centric  models for joint action localization and recognition
In  ICCV, N0NN
N  [NN] I
Laptev, M
Marszalek, C
Schmid, and B
Rozenfeld
 Learning realistic human actions from movies
In CVPR,  N00N
N  [NN] Y
Li, S.-H
Hu, and B
Li
Recognizing unseen actions in a  domain-adapted embedding space
In ICIP, N0NN
N, N  [NN] J
Lin
Divergence measures based on the shannon entropy
 Trans
on Inf
Theory, NNNN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV, N0NN
N  [N0] J
Liu, B
Kuipers, and S
Savarese
Recognizing human actions by attributes
In CVPR, N0NN
N, N, N  [NN] L
Liu, L
Shao, X
Li, and K
Lu
Learning spatio-temporal  representations for action recognition: A genetic programming approach
Trans
Cybernetics, NN(N):NNN–NN0, N0NN
 N  [NN] M
Marszałek, I
Laptev, and C
Schmid
Actions in context
 In CVPR, N00N
N  [NN] P
Mettes, D
C
Koelma, and C
Snoek
The imagenet shuffle: Reorganized pre-training for video event detection
In  ICMR, N0NN
N, N  [NN] P
Mettes, C
Snoek, and S.-F
Chang
Localizing actions  from video labels and pseudo-annotations
In BMVC, N0NN
 N  [NN] P
Mettes, J
van Gemert, and C
Snoek
Spot on: Action  localization from pointly-supervised proposals
In ECCV,  N0NN
N, N, N  [NN] T
Mikolov, I
Sutskever, K
Chen, G
S
Corrado, and  J
Dean
Distributed representations of words and phrases  and their compositionality
In NIPS, N0NN
N, N  [NN] D
J
Moore, I
A
Essay, and M
H
Hayes III
Exploiting  human actions and object context for recognition tasks
In  ICCV, NNNN
N  [NN] D
Oneata, J
Verbeek, and C
Schmid
Action and event  recognition with fisher vectors on a compact feature set
In  ICCV, N0NN
N  [NN] X
Peng, C
Zou, Y
Qiao, and Q
Peng
Action recognition  with stacked fisher vectors
In ECCV, N0NN
N  [N0] A
Prest, V
Ferrari, and C
Schmid
Explicit modeling of  human-object interactions in realistic videos
TPAMI, N0NN
 N  NNNN    [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
N, N  [NN] M
D
Rodriguez, J
Ahmed, and M
Shah
Action MACH:  a spatio-temporal maximum average correlation height filter  for action recognition
In CVPR, N00N
N  [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In NIPS, N0NN
N  [NN] K
Soomro, H
Idrees, and M
Shah
Action localization in  videos through context walk
In ICCV, N0NN
N  [NN] K
Soomro, A
R
Zamir, and M
Shah
UcfN0N: A dataset  of N0N human actions classes from videos in the wild
 arXiv:NNNN.0N0N, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
N  [NN] B
Thomee, D
A
Shamma, G
Friedland, B
Elizalde, K
Ni,  D
Poland, D
Borth, and L.-J
Li
YfccN00m: The new data  in multimedia research
CACM, N0NN
N  [NN] Y
Tian, R
Sukthankar, and M
Shah
Spatiotemporal deformable part models for action detection
In CVPR, N0NN
 N  [NN] M
M
Ullah, S
N
Parizi, and I
Laptev
Improving bag-offeatures action recognition with non-local cues
In BMVC,  N0N0
N  [N0] H
Wang and C
Schmid
Action recognition with improved  trajectories
In ICCV, N0NN
N  [NN] P
Weinzaepfel, Z
Harchaoui, and C
Schmid
Learning to  track for spatio-temporal action localization
In ICCV, N0NN
 N  [NN] G
Willems, T
Tuytelaars, and L
Van Gool
An efficient  dense and scale-invariant spatio-temporal interest point detector
In ECCV, N00N
N  [NN] J
Wu, A
Osuntogun, T
Choudhury, M
Philipose, and J
M
 Rehg
A scalable approach to activity recognition based on  object use
In CVPR, N00N
N  [NN] S
Wu, S
Bondugula, F
Luisier, X
Zhuang, and P
Natarajan
Zero-shot event detection using multi-modal fusion of  weakly supervised concepts
In CVPR, N0NN
N  [NN] Z
Wu, Y
Fu, Y.-G
Jiang, and L
Sigal
Harnessing object  and scene semantics for large-scale video understanding
In  CVPR, N0NN
N, N  [NN] X
Xu, T
Hospedales, and S
Gong
Semantic embedding  space for zero-shot action recognition
In ICIP, N0NN
N  [NN] X
Xu, T
Hospedales, and S
Gong
Multi-task zero-shot  action recognition with prioritised data augmentation
In  ECCV, N0NN
N, N, N  [NN] X
Xu, T
Hospedales, and S
Gong
Transductive zero-shot  action recognition by word-vector embedding
IJCV, N0NN
 N, N  [NN] Z
Xu, Y
Yang, and A
G
Hauptmann
A discriminative cnn  video representation for event detection
In CVPR, N0NN
N  [N0] B
Yao and L
Fei-Fei
Grouplet: A structured image representation for recognizing human and object interactions
In  CVPR, N0N0
N  [NN] B
Yao and L
Fei-Fei
Modeling mutual context of object  and human pose in human-object interaction activities
In  CVPR, N0N0
N  [NN] B
Yao, A
Khosla, and L
Fei-Fei
Classifying actions and  measuring action similarity by modeling the mutual context  of objects and human poses
In ICML, N0NN
N  [NN] G
Yu and J
Yuan
Fast action proposals for human action  detection and search
In CVPR, N0NN
N  [NN] Z
Zhang, C
Wang, B
Xiao, W
Zhou, and S
Liu
Robust  relative attributes for human action recognition
PAA, N0NN
 N  NNNNTurning Corners Into Cameras: Principles and Methods   Turning Corners into Cameras: Principles and Methods  Katherine L
BoumanN Vickie YeN Adam B
YedidiaN Frédo DurandN  Gregory W
WornellN Antonio TorralbaN William T
FreemanN,N  NDept
of Electrical Engineering and Computer Science, Massachusetts Institute of Technology NGoogle Research  A+BA  a n g u la r	 p o si ti o n  A  A+BA  B  (c)	Original	Frame (d)	Color	Magnified (e)	Reconstructed	ND	Video	of	Hidden	Scene  time  (b)	the	hidden	scene	as	you	move	in	a	circle	around	the	wall’s	edge  (a)  Figure N: We construct a N-D video of an obscured scene using RGB video taken with a consumer camera
The stylized diagram in (a) shows a typical scenario: two people—one wearing red and the other blue—are hidden from the camera’s view by a wall
Only the region shaded in yellow is visible to the  camera
To an observer walking around the occluding edge (along the magenta arrow), light from different parts of the hidden scene becomes visible at  different angles (see sequence (b))
Ultimately, this scene information is captured in the intensity and color of light reflected from the corresponding patch of  ground near the corner
Although these subtle irradiance variations are invisible to the naked eye (c), they can be extracted and interpreted from a camera  position from which the entire obscured scene is hidden from view
Image (d) visualizes these subtle variations in the highlighted corner region
We use  temporal frames of these radiance variations on the ground to construct a N-D video of motion evolution in the hidden scene
Specifically, (e) shows the  trajectories over time that specify the angular position of hidden red and blue subjects illuminated by a diffuse light
 Abstract  We show that walls, and other obstructions with edges,  can be exploited as naturally-occurring “cameras” that  reveal the hidden scenes beyond them
In particular, we  demonstrate methods for using the subtle spatio-temporal  radiance variations that arise on the ground at the base of  a wall’s edge to construct a one-dimensional video of the  hidden scene behind the wall
The resulting technique can be  used for a variety of applications in diverse physical settings
 From standard RGB video recordings, we use edge cameras  to recover N-D videos that reveal the number and trajectories  of people moving in an occluded scene
We further show  that adjacent wall edges, such as those that arise in the case  of an open doorway, yield a stereo camera from which the  N-D location of hidden, moving objects can be recovered
 We demonstrate our technique in a number of indoor and  outdoor environments involving varied floor surfaces and  illumination conditions
 N
Introduction  The ability to see around obstructions would prove valuable in a wide range of applications
As just two examples,  remotely sensing occupants in a room would be valuable in  search and rescue operations, and the ability to detect hidden,  oncoming vehicles and/or pedestrians would be valuable in  collision avoidance systems [N]
Although often not visible  to the naked eye, in many environments, light from obscured  portions of a scene is scattered over many of the observable  surfaces
This reflected light can be used to recover information about the hidden scene (see Fig
N)
In this work, we  exploit the vertical edge at the corner of a wall to construct  a “camera” that sees beyond the wall
Since vertical wall  edges are ubiquitous, such cameras can be found in many  environments
 The radiance emanating from the ground in front of a  corner, e.g., at the base of a building, is influenced by many  factors: the albedo, shape, and BRDF of its surface, as  well as the light coming from the full hemisphere above it
 Assuming the ground has a significant diffuse component, a  majority of the reflected light comes from the surroundings  NNNN0    that are easily seen from the observer’s position next to the  occluding wall (the visible region is shaded in yellow in  Fig
N(a))
However, emitted and reflected light from behind  the corner, hidden from the observer, also has a small effect  on the ground’s radiance in the form of a subtle gradient of  light encircling the corner; this is not a shadow, but is instead  what is referred to as a penumbra
 The faint penumbra on the ground is caused by the reflection of an increasing amount of light from the hidden scene
 To illustrate this, imagine standing with your shoulder up  against the building’s wall (refer to the leftmost picture of  Fig
N(b))
At this position you are unable to see any of the  scene behind the corner
However, as you slowly move away  from the wall, walking along the magenta circle shown in  Fig
N(a), you see an increasing amount of the scene
Eventually, the hidden scene comes fully into view
Similarly,  different points on the ground reflect light integrated from  differently-sized fractions of the hidden scene
 Now imagine someone has entered the hidden portion of  the scene
This person would introduce a small change to  the light coming from an angular slice of the room
From  behind the corner this change would often not be perceptible  to the naked eye
However, it would result in a subtle change  to the penumbra; see Fig
N(c) and (d)
We use these subtle  changes, recorded from standard video cameras, to construct  a N-D version of how the hidden scene beyond the corner  evolves with time; see Fig
N(e)
 Section N summarizes related work that puts the present  contribution in context
Section N shows how, using our  proposed methods, it is possible to identify the number and  location of people in a hidden scene
Section N shows how  parallax created by a pair of adjacent edges, such as in a  doorway, can be used to triangulate the ND position of moving people over time
Experimental results (in the paper and  supplemental material) are shown for a number of indoor  and outdoor environments with varied flooring, including  carpet, tile, hardwood, concrete, and brick
 N
Related Work  In this section we describe previous non-line-of-sight  (NLoS) methods
Previous methods used to see past or  through occluders have ranged from using WiFi signals [N] to  exploiting random specular surfaces [NN, N]
In this summary,  we emphasize a few active and passive approaches that have  previously been used to see past occluders and image hidden  scenes
 Recovery under Active Illumination: Past approaches  to see around corners have largely involved using time-offlight (ToF) cameras [NN, N0, N0, N]
These methods involve  using a laser to illuminate a point that is visible to both  the observable and hidden scene, and measuring how long  it takes for the light to return [N0, NN]
By measuring the  light’s time of flight, one can infer the distance to objects  in the hidden scene, and by measuring the light’s intensity,  one can often learn about the reflectance and curvature of  the objects [NN]
Past work has used ToF methods to infer  the location [N], size and motion [NN, N], and shape [NN] of  objects in the hidden scene
These methods have also been  used to count hidden people [NN]
 ToF cameras work well in estimating the depths of hidden  objects, however, they have some limitations
First, they  require specialized and comparatively expensive detectors  with fine temporal resolution
Second, they are limited in  how much light they can introduce in the scene to support  imaging
Third, they are vulnerable to interference from  ambient outdoor illumination
By contrast, our proposed realtime passive technique operates in unpredictable indoor and  outdoor environments with inexpensive consumer cameras,  without additional illumination
 In [N] a laser is used to indirectly illuminate an object  behind an occluder
Using a standard camera the authors are  then able to identify the position of the hidden object
Similar  to our proposed work, [N] uses a standard camera; however,  their proposed system has a number of limitations
Namely,  they require controlled conditions where the geometry of the  unknown moving object is rigid, and its shape and material  are either known or can be closely modeled by a single  oriented surface element
In contrast, our method requires  minimal prior information, is completely passive, and has  been shown to work in many natural settings
 Passive Recovery: Other work has previously considered  the possibility of using structures naturally present in the  real world as cameras
Naturally occurring pinholes (such  as windows) or pinspecks have been previously used for  non-line-of-sight imaging [NN, N]
In addition, specular reflections off of human eyes have been used to image hidden  scenes [NN]
Although these accidental cameras can be used  to reconstruct N-D images, they require a more specialized  accidental camera scenario than the simple edges we propose  to use in this work
 The technique presented in [NN] also detects and visualizes small, often imperceptible, color changes in video
 However, in this work, rather than just visualize these tiny  color changes, we interpret them in order to reconstruct a  video of a hidden scene
 N
Edge Cameras  An edge camera system consists of four components:  the visible and hidden scenes, the occluding edge, and the  ground, which reflects light from both scenes
We refer to  the (ground) plane perpendicular to the occluding edge as  the observation plane
By analyzing subtle variations in the  penumbra at the base of an edge, we are able to deduce a  hidden subject’s pattern of motion
 NNNN    The reflected light from a surface at point p, with normal n̂, is a function of the incoming light L′i as well as the surface’s albedo a and BRDF β
Specifically,  L′o(p, v̂o) = a(p)  ∫ L′i(p, v̂i)β(v̂i, v̂o, n̂) γ(v̂i, n̂) dv̂i,  (N)  where v̂i and v̂o denote the incoming and outgoing unit vectors of light at position p = (r, θ), respectively, and γ(v̂i, n̂) = v̂i · n̂
We parameterize p in polar coordinates, with the origin centered at the occluding edge and θ = 0 corresponding to the angle parallel to the wall coming from the corner (refer to Fig
N)
For simplicity, we assume  the observation plane is Lambertian, and that the visible  and hidden scene are modeled as light emitted from a large  celestial sphere, parameterized by right ascension α and declination δ
Under these assumptions, we simplify (N):  L′o(r, θ) = a(r, θ)  ∫ Nπ  α=0  ∫ π/N  δ=0  Li(α, δ) dα dδ (N)  where Li = L ′  iγ
Furthermore, since the occluding edge blocks light from [π + θ, Nπ] at the radial line θ,  L′o(r, θ) = a(r, θ)  [ Lv +  ∫ θ  φ=0  Lh(φ) dφ  ] (N)  for Lv = ∫ π α=0  ∫ π/N δ=0  Li(α, δ) dα dδ and Lh(φ) = ∫ π/N δ=0  Li(π +  φ, δ) dδ
By inspecting (N) we can see that the intensity  of light on the penumbra is explained by a constant term,  Lv, which is the contribution due to light visible to the observer (shaded in yellow in Fig
N(a)), and a varying angle  dependent term which integrates the light in the hidden scene,  Lh
For instance, a radial line at θ = 0 only integrates the light from the scene visible to the observer, while the radial  line θ = π/N reflects the integral of light over the entire visible and hidden scenes
 Then, if we assume that ddθa(r, θ) ≈ 0 N, the derivative of  the observed penumbra recovers the N-D angular projection  of the hidden scene:  d  dθ L′o(r, θ) ≈ a(r, θ)Lh(θ)
(N)  But what happens if someone walks into the hidden scene  at time t, changing L0h(θ) to L t h(θ)? In this case, the spatial  derivative of the temporal difference encodes the angular  change in lighting:  d  dθ  [ L′to (r, θ)− L  ′0 o (r, θ)  ] = a(r, θ)  [ Lth(θ)− L  0 h(θ)  ]  (N)  NIn practice, we subtract a background frame to substantially remove  per-pixel albedo variations
Refer to Section N.N.N  =  y O b se rv a ti o n s:  Transfer	Matrix:A  x (b)	Sample	Estimation	Gain	Image  Direction	of	  Light	Integration  W a ll  N -D 	H id d e n 	S ce n e :  Positive  Negative θ = 0  (a)	Constructing	Transfer	Matrix  Figure N: In (a), the transfer matrix, A, is shown for a toy situation in which observations lie along circles around the edge
In this case, A  would simply be a repeated lower triangular matrix
(b) contains an example  estimation gain image, which describes the matrix operation performed on  observations y(t) to estimate x(t)
As predicted, the image indicates that  we are essentially performing an angular derivative in recovering a frame of  the N-D video
 In other words, the angular derivative of the penumbra’s difference from the reference frame is a signal that indicates the  angular change in the hidden scene over time
In practice, we  obtain good results assuming a(r, θ) = N and using the cam- eras’ native encoded intensity values while subtracting the  temporal mean as a background frame (see Section N.N.N)
 N.N
Method  Using a video recording of the observation plane, we  generate a N-D video indicating the changes in a hidden  scene over time
These N-D angular projections of the hidden  scene, viewed over many time-steps, reveal the trajectory of  a moving object behind the occluding edge
 Likelihood: At each time t, we relate the observed M - pixels on the projection plane, y(t), to the N-D angular projection of the hidden scene, L (t) h (θ)
We formulate a discrete  approximation to our edge camera system by describing the  continuous image L (t) h (θ) using N terms, x  (t)
The observations y(t) then relate to the unknown parameters x(t) and  L (t) v by a linear matrix operation:  y(t) = L(t)v +Ax (t) +w(t), w(t) ∼ N (0, λNI),  where the M ×N matrix A is defined by the geometry of the system
More explicitly, each row m of A integrates the portion of the hidden scene visible from observation m,  y (t) m 
In the simplified case of observations that lie on a circle  around the occluding edge, A would simply be a constant  lower-triangular matrix; see Fig
N(a)
 Let Ã be the column augmented matrix [N A]
We can then express the likelihood of an observation given x(t) and  L (t) v as:  p(y(t)|x(t), L(t)v ) = N  ( Ã [ L(t)v x  (t)T ]T  , λNN  ) 
(N)  NNNN    Prior: The signal we are trying to extract is very small  relative to the total light intensity on the observation plane
 Therefore, to improve the quality of results, we enforce spatial smoothness of x(t)
We use a simple LN smoothness  regularization over adjacent parameters in x(t)
This corresponds, for a gradient matrix G, to using the prior  p(x(t)) ∝ N−N∏  n=N  exp  [ −  N  NσNN ‖x(t)[n]− x(t)[n− N]‖NN  ]  N∏  n=N  exp  [ −  N  NσNN ‖x(t)[n]‖NN  ] (N)  = N (0, σNN(G TG)−N + σNNN)
(N)  Inference: We seek a maximum a posteriori (MAP) estimate of the hidden image coefficients, x(t), given M ob- servations, y(t), measured by the camera
By combining  the defined Gaussian likelihood and prior distributions, we  obtain a Gaussian posterior distribution of x(t) and L (t) v ,  p(x(t), L(t)v |y (t)) = N  ([ L̂(t)v x̂  (t)T ]T  ,Σ(t) )  Σ(t) =  [ λ−NÃT Ã+  ( 0 0  0 G T G  σN N  + N σN N  )] −N  [ L̂(t)v x̂  (t)T ]T  = Σ(t)λ−NÃTy(t) (N)  where the maximum a posteriori estimate is given by x̂(t)
 To better understand the operation that is being performed  to obtain the N-D reconstruction, we visualize each row  of the matrix Σ(t)λ−NÃT 
We refer to each reshaped row of this matrix as the estimation gain image
An example  estimation gain image is shown in Fig
Nb
As expected, the  matrix operation is computing an angular derivative over the  observation plane
Note that although earlier we assumed d dθa(r, θ) ≈ 0, in reality the albedo simply needs to be orthogonal to the zero-mean pie-wedges in each estimation  gain image
We expect violations from this assumption to be  small
 N.N.N Implementation Details  Rectification: All of our analysis thus far has assumed  we are observing the floor parallel to the occluding edge
 However, in most situations, the camera will be observing  the projection plane at an angle
In order to make the construction of the matrix A easier, we begin by rectifying our  images using a homography
In these results, we assume the  ground is perpendicular to the occluding edge, and estimate  the homography using either a calibration grid or regular  patterns, such as tiles, that naturally appear on the ground
 Alternatively, a known camera calibration could be used
 Background Subtraction: Since we are interested in  identifying temporal differences in a hidden scene due to a  moving subject, we must remove the effect of the scene’s  background illumination
Although this could be accomplished by first subtracting a background frame, L0o, taken without the subject, we avoid requiring the availability of  such a frame
Instead, we assume the subject’s motion is  roughly uniform over the video, and use the video’s mean  image in lieu of a true background frame
We found that in  sequences containing people moving naturally, background  subtraction using the average video frame worked well
 Temporal Smoothness: In addition to spatial smoothness  we could also impose temporal smoothness on our MAP estimate
x̂(t)
This helps to further regularize our result, at  the cost of some temporal blurring
However, to emphasize the coherence among results, we do not impose this  additional constraint
Each N-D image, x(t), that we show  is independently computed
Results obtained with temporal smoothness constraints are shown in the supplemental  material
 Parameter Selection: The noise parameter λN is set for each video as the median variance of estimated sensor noise
 The regularization parameters σN and σN are empirically set to 0.N for all results
 N.N
Experiments and Results  Our algorithm reconstructs a N-D video of a hidden scene  from behind an occluding edge, allowing users to track the  motions of obscured, moving objects
In all results shown,  the subject was not visible to an observer at the camera
 We present results as space-time images
These images  contain curves that indicate the angular trajectories of moving people
All results, unless specified otherwise, were  generated from standard, compressed video taken with a  SLR camera
Please refer to the supplemental video for full  sequences and additional results
 N.N.N Environments  We show several applications of our algorithm in various  indoor and outdoor environments
For each environment, we  show the reconstructions obtained when one or two people  were moving in the hidden scene
 Indoor: In Fig
N(e) we show a result obtained from a  video recorded in a mostly dark room
A large diffuse light  illuminated two hidden subjects wearing red and blue clothing
As the subjects walked around the room, their clothing  reflected light, allowing us to reconstruct a N-D video of colored trajectories
As correctly reflected in our reconstructed  video, the subject in blue occludes the subject in red three  times before the subject in red becomes the occluder
 Fig
N shows additional examples of N-D videos recovered  from indoor edge cameras
In these sequences, the environment was well-lit
The subjects occluded the bright ambient  NNNN    Hidden	Scene  time  N Person N	PeopleVideo	FrameSetup  θ  Figure N: One-dimensional reconstructed videos of indoor, hidden scenes
Results are shown as space-time images for sequences where one or two people were walking behind the corner
In these reconstructions, the angular position of a person, as well as the number of people, can be clearly identified
Bright  vertical line artifacts are caused by additional shadows appearing on the penumbra
We believe horizontal line artifacts result from sampling on a square grid
 Setup  S u n n y  C lo u d y 	&  W e t	 G ro u n d  C lo u d y 	  & R a in y  time  N Person N	PeopleVideo	Frame  Hidden	Scene  θ  Figure N: N-D reconstructed videos of a common outdoor, hidden scene under various weather conditions
Results are shown as space-time images
The last row shows results from sequences taken while it was beginning to rain
Although artifacts appear due to the appearing raindrops, motion trajectories can be  identified in all reconstructions
 light, resulting in the reconstruction’s dark trajectory
Note  that in all the reconstructions, it is possible to count the number of people in the hidden scene, and to recover important  information such as their angular position and speed, and the  characteristics of their motion
 Outdoor: In Fig
N we show the results of a number of  videos taken at a common outdoor location, but in different weather conditions
The top sequences were recorded  during a sunny day, while the bottom two sequences were  recorded while it was cloudy
Additionally, in the bottom sequence, raindrops appeared on the ground during recording,  while in the middle sequence the ground was fully saturated  with water
Although the raindrops cause artifacts in the  reconstructed space-time images, you can still discern the  trajectory of people hidden behind the wall
 N.N.N Video Quality:  In all experiments shown thus far we have used standard,  compressed video captured using a consumer camera
However, video compression can create large, correlated noise  that may affect our signal
We have explored the effect video  quality has on results
To do this, we filmed a common  scene using N different cameras: an iPhone Ns, a Sony Alpha  Ns, and a uncompressed RGB Point Grey
Fig
N shows the  results of this experiment assuming different levels of i.i.d
 noise
Each resulting N-D image was reconstructed from a  single frame
The cell phone camera’s compressed videos  resulted in the noisiest reconstructions, but even those results  still capture key features of the subject’s path
 N.N.N Velocity Estimation  The derivative of a person’s trajectory over time, θ(t), in- dicates their angular velocity
Fig
N shows an example of  the estimated angular velocity obtained from a single edge  camera when the hidden subject was walking roughly in a  circle
Note that the person’s angular size and speed are both  larger when the person is closer to the corner
Such cues can  help approximate the subject’s N-D position over time
 N.N
Estimated Signal Strength  In all of our presented reconstructions we show images  with an intensity range of 0.N
As these results were obtained  from N-bit videos, our target signal is less than 0.N% of the  video’s original pixel intensities
 To better understand the signal measurement requirements, we have developed a simple model of the edge camera system that both explains experimental performance and  enables the study of asymptotic limits
 We consider three sources of emitted or reflected light: a  cylinder (proxying for a person), a hemisphere of ambient  light (the surrounding scene), and an infinitely tall half-plane  (the occluding wall)
If all surfaces are Lambertian, the  NNNN    iP h o n e  S o n y  α 	N s  P o in t	  G re y  (c)	Walking	from	N	to	NN	feet	at	a	NN° Angle	 (d)	Walking	Randomly  Sony	α	Ns  Point	Grey  iPhone  timeN	ft NN	ft  (a)	Setup (b)	Noise  λ = N.N  λ = N.N  λ = 0.N  θ  Figure N: The result of using different cameras on the reconstruction of the same sequence in an indoor setting
Three different N-bit cameras (an iPhone Ns, a Sony Alpha Ns, and an uncompressed RGB Point Grey) simultaneously recorded the carpeted floor
Each camera introduced a different level of sensor  noise
The estimated standard deviation of per-pixel sensor noise, λ, is shown in (b)
We compare the quality of two sequences in (c) and (d)
In (c), we have  reconstructed a video from a sequence of a single person walking directly away from the corner from N to NN feet at a NN degree angle from the occluded wall
 This experiment helps to illustrate how signal strength varies with distance from the corner
In (d), we have done a reconstruction of a single person walking  in a random pattern
In (c) the hidden person does not change in angular position
Thus, for these results, we subtract an average background frame computed  from a different portion of the video sequence
 R a d ia n s	 p e r	 S e co n d  Seconds  time  φ  0 N N0 NN N0 NN N0 -0.N  -0.N  0  0.N  0.N  0.N  Figure N: A subject’s reconstructed angular velocity relative to the corner as a function of time
In this sequence, a person was walking in circles far  from the corner
 brightness change of the observation plane due the presence of the cylinder around the corner can be computed  analytically for this simple system
See the supplementary  document
 For reasonable assumed brightnesses of the cylinder,  hemisphere, and half-plane (NN0, N00, and N00, respectively,  in arbitrary linear units), the brightness change on the observation plane due to the cylinder will be an extremum of  -N.N out of a background of N0N0 units
This is commensurate with our experimental observations of ∼ 0.N% change of brightness over the penumbra region
Our model shows  novel asymptotic behavior of the edge camera
Namely, at  large distances from the corner, brightness changes in the  penumbra decrease faster than would otherwise be expected  from a N-D camera
This is because the arrival angle of the  rays from a distant cylinder are close to grazing with the  ground, lessening their influence on the penumbra
However,  within N0 meters of the corner, such effects are small
 N
Stereo Edge Cameras  Although the width of a track recovered in the method of  the previous section can give some indication of a hidden  person’s relative range, more accurate methods are possible  by exploiting adjacent walls
For example, when a hidden  scene is behind a doorway, the pair of vertical doorway  Left	Wall Right	Wall  N N N  N  Left	Wall Right	Wall  N N  tim eN N  θL θR  Hidden	Scene  Figure N: The four edges of a doorway contain penumbras that can be used to reconstruct a NN0◦ view of a hidden scene
The top diagram indicates  the penumbras and the corresponding region they describe
Parallax occurs  in the reconstructions from the left and right wall
This can be seen in  the bottom reconstruction of two people hidden behind a doorway
Numbers/colors indicate the penumbras used for each N0◦ space-time image
 wall edges yield a pair of corner cameras
By treating the  observation plane at the base of each edge as a camera,  we can obtain stereo N-D images that we can then use to  triangulate the absolute position of a subject over time
 N.N
Method  A single edge camera allows us to reconstruct a N0◦ angu- lar image of an occluded scene
We now consider a system  composed of four edge cameras, such as an open doorway,  as illustrated in Fig
N
Each side of the doorway contains  two adjacent edge cameras, whose reconstructions together  create a NN0◦ view of the hidden scene
 The two sides of the doorway provide two views of the  same hidden scene, but from different positions
This causes  NNNN    Bw Left	Wall Right	Wall  Px  P z  θL θR  Figure N: A hidden person will introduce an intensity change on the left  and right wall penumbras at angles of θ (t) L and θ  (t) R , respectively
Once  these angles have been identified, we can recover the hidden person’s twodimensional location using Eq
NN
 an offset in the projected angular position of the same person  (see Fig
N)
Our aim is to use this angular parallax to triangulate the location of a hidden person over time
Assume we  are observing the base of a doorway, with walls of width w  separated by a distance B
A hidden person will introduce an  intensity change on the left and right wall penumbras at angles of θ (t) L and θ  (t) R , respectively
From this correspondence,  we can triangulate their N-D location
 P (t)z = B − η(t)  cot θ (t) L + cot θ  (t) R  (N0)  P (t)x = P (t) z cot θ  (t) L (NN)  η(t) =      w cot(θR) Px ≤ 0  0 0 ≤ Px ≤ B  w cot(θL) Px ≥ B  (NN)  where (Px, Pz) are the x- and z-coordinate of the person
We define the top corner of the left doorway, corner N in  Fig
N, as (Px, Pz) = (0, 0)
Assuming the wall is sufficiently thin compared to the  depth of moving objects in the hidden scene, the η(t) term  can be ignored
In this case, the relative position of the  person can be reconstructed without any knowledge of the  absolute geometry of the doorway (e.g
B or w)
In all  results shown in this paper, we have made this assumption
 Identifying Trajectories: While automatic contour tracing methods exist [N], for simplicity, in our stereo results,  we identify the trajectories of objects in the hidden scene  manually by tracing a path on the reconstructed space-time  images
 N.N
Experiments and Results  We demonstrate the ability of our method to localize  the two-dimensional position of a hidden object using four  edge cameras, such as in a doorway
We present a series  of experiments in both controlled and uncontrolled settings
 Full sequences, indicating the ground truth motions, and  additional results can be found in the supplemental material
 Controlled Environment: To demonstrate the ability to  infer depth from stereo edge cameras we constructed a controlled experiment
A monitor displaying a slowly moving  tim e  Left	Wall Right	Wall Inferred	Positon	Over	Time  tim e  S te re o 	E d g e 	C a m e ra 	A  S te re o 	E d g e 	C a m e ra 	B  Hidden	Scene	ASetup Hidden	Scene	B  ↓ Baseline  Z -P o sitio  n  ↓ Baseline  X-Position  Z -P o sitio  n  X-Position  Z -P o sitio  n  θL θR  Figure N: The results of our stereo experiments in a natural setting
Each sequence consists of a single person walking in a roughly circular pattern  behind a doorway
The N-D inferred locations over time are shown as a line  from blue to red
Error bars indicating one standard deviation of error have  been drawn around a subset of the points
Our inferred depths capture the  hidden subject’s cyclic motion, but are currently subject to large error
A  subset of B’s inferred N-D locations have been cut out of this figure, but can  be seen in full in the supplemental material
 green line was placed behind two walls, separated by a baseline of N0 cm, at a distance of roughly NN, N0, N0, and NN  cm
Fig
N0(b) shows sample space-time reconstructions of  each NN0◦ edge camera
The depth of the green line was then estimated from manually identified trajectories obtained  from these space-time images
Empirically estimated error  ellipses are shown in red for a subset of the depth estimates
 Natural Environment: Fig
N shows the results of estimating N-D positions from doorways in natural environments
 The hidden scene consists of a single person walking in a circular pattern behind the doorway
Although our reconstructions capture the cyclic nature of the subject’s movements,  they are sensitive to error in the estimated trajectories
Refer to Section N.N
Ellipses indicating empirically estimated  error have been drawn around a subset of the points
 N.N
Error Analysis  There are multiple sources of error that can introduce  biases into location estimates
Namely, inaccuracy in localizing the projected trajectories, and mis-calibration of the  scene cause error in the estimates
We discuss the effects of  some of these errors below
Further derivations and analysis  can be seen in our supplemental material
 Trajectory Localization: Because Pz scales inversely  with cot(θL) + cot(θR), small errors in the estimated pro- jected angles of the person in the left and right may cause  NNNN    N0	cm  N 0 0 	c m  Left	Wall Right	Wall  Left	Wall Right	Wall  Monitor  Lo ca ti o n 	N  Lo ca ti o n 	N  -N0 -N0 0 N0 N0 N0  X Position  0  N0  N0  N0  N0  N00  NN0  NN0  Z  P  o s it io  n  ↓ Baseline  Location N  Location N  Location N  Location N  (a)	Controlled	Setup (b)	Sample	Stereo	Reconstructions (c)	Estimated	Depth  θL θR  Figure N0: Controlled experiments were performed to demonstrate the ability to infer depth from stereo edge cameras
A monitor displaying a moving green line was placed behind an artificial doorway (a) at four locations corresponding to NN, N0, N0, and NN cm, respectively
(b) shows sample reconstructions  done of the edge cameras for the left and right wall when the monitor was placed at NN and NN cm
Using tracks obtained from these reconstructions, the N-D  position of the green line in each sequence was estimated over time (c)
The inferred position is plotted with empirically computed error ellipses (indicating  one standard deviation of noise)
 large errors in the estimated position of the hidden person,  particularly at larger depths
Assuming Gaussian uncertainty  in the left and right angular trajectories, σθL and σθR , the  uncertainty in the estimated position of the hidden person  will not be Gaussian
However, the standard deviation of  empirical distributions through sampling, as seen in Figs
N  and N0, can be informative
Additionally, by using standard  error propagation of independent variables, we can compute  a first order approximation of the uncertainty
For instance,  the uncertainty in the z position, σPz , is  σPz = B  √ σNθL csc  N θL + σNθR csc N θR  (cot θL + cot θR)N (NN)  -N0 -N0 0 N0 N0 N0  X Position  0  N0  N0  N0  N0  N00  Z  P  o s it io  n  # Baseline  Figure NN: The empirical means plus or minus one standard deviation of the estimated Pz as a function of its x-coordinate, assuming true Pz of N0,  N0, N0, and N0
Here, the two corner location errors at each of the boundaries  of the doorway are independent and subject to σN∆x = σ N ∆z = 0.0N
 Corner Identification: Misidentifying the corner of each  occluding edge will cause systematic error to the estimated  N-D position
To determine how erroneously identifying a  corner affects our results, we consider the following situation:  a doorway of baseline B = N0 obscuring a bright object at angular position θ in an otherwise dark scene
 Assuming the offset from the true corner location is drawn  from an independent Gaussian distribution, we can calculate  the error between the estimated and true angular position,  and then subsequently use these offsets to calculate the error  in depth
Fig
NN shows the error as a function of depth for  a stereo camera setup in which the corner offset has been  drawn from a Gaussian distribution with variance 0.0N
 N
Conclusion We show how to turn corners into cameras, exploiting a  common, but overlooked, visual signal
The vertical edge  of a corner’s wall selectively blocks light to let the ground  nearby display an angular integral of light from around the  corner
The resulting penumbras from people and objects  are invisible to the eye – typical contrasts are 0.N% above background – but are easy to measure using consumer-grade  cameras
We produce N-D videos of activity around the corner, measured indoors, outdoors, in both sunlight and shade,  from brick, tile, wood, and asphalt floors
The resulting  N-D videos reveal the number of people moving around the  corner, their angular sizes and speeds, and a temporal summary of activity
Open doorways, with two vertical edges,  offer stereo views inside a room, viewable even away from  the doorway
Since nearly every corner now offers a N-D  view around the corner, this opens potential applications for  automotive pedestrian safety, search and rescue, and public  safety
This ever-present, but previously unnoticed, 0.N% signal may invite other novel camera measurement methods
 Acknowledgments This work was supported in part by  the DARPA REVEAL Program under Contract No
HR00NNNN-C-00N0, NSF Grant NNNNNNN, Shell Research, and an  NDSEG Fellowship (to ABY)
We thank Yoav Schechner,  Jeff Shapiro, Franco Wang, and Vivek Goyal for helpful  discussions
 NNNN    References  [N] F
Adib and D
Katabi
See through walls with wifi! ACM,  NN(N):NN–NN, N0NN
N  [N] P
Borges, A
Tews, and D
Haddon
Pedestrian detection in  industrial environments: Seeing around corners
N0NN
N  [N] A
L
Cohen
Anti-pinhole imaging
Optica Acta: International Journal of Optics, NN(N):NN–NN, NNNN
N  [N] R
Fergus, A
Torralba, and W
Freeman
Random lens imaging
N00N
N  [N] G
Gariepy, F
Tonolini, R
Henderson, J
Leach, and D
Faccio
Detection and tracking of moving objects hidden from  view
Nature Photonics, N0NN
N  [N] F
Heide, L
Xiao, W
Heidrich, and M
B
Hullin
Diffuse  mirrors: Nd reconstruction from diffuse indirect illumination  using inexpensive time-of-flight sensors
In N0NN IEEE Conference on Computer Vision and Pattern Recognition, pages  NNNN–NNNN, June N0NN
N  [N] A
Kadambi, H
Zhao, B
Shi, and R
Raskar
Occluded  imaging with time-of-flight sensors, N0NN
ACM Transactions  on Graphics
N  [N] M
Kass, A
Witkin, and D
Terzopoulos
Snakes: Active  contour models
International journal of computer vision,  N(N):NNN–NNN, NNNN
N  [N] J
Klein, C
Peters, J
Martı́n, M
Laurenzis, and M
B
Hullin
 Tracking objects outside the line of sight using Nd intensity  images
Scientific reports, N, N0NN
N  [N0] M
Laurenzis, A
Velten, and J
Klein
Dual-mode optical  sensing: three-dimensional imaging and seeing around a corner
Optical Engineering, N0NN
N  [NN] K
Nishino and S
Nayar
Corneal imaging system: Environment from eyes
International Journal of Computer Vision,  N0(N):NN–N0, N00N
N  [NN] R
Pandharkar, A
Velten, A
Bardagjy, B
M
Lawson, E., and  R
Raskar
Estimating motion and size of moving non-line-ofsight objects in cluttered environments, N0NN
In Computer  Vision and Pattern Recognition (CVPR), N0NN IEEE Conference on (pp
NNN-NNN)
N  [NN] D
Shin, A
Kirmani, V
Goyal, and J
Shapiro
Computational  Nd and reflectivity imaging with high photon efficiency
Image Processing (ICIP), N0NN IEEE International Conference,  N0NN
N  [NN] D
Shin, A
Kirmani, V
Goyal, and J
Shapiro
Photonefficient computational N-d and reflectivity imaging with  single-photon detectors
IEEE Transactions on Computational Imaging, N0NN
N  [NN] S
Shrestha, F
Heide, W
Heidrich, and G
Wetzstein
Computational imaging with multi-camera time-of-flight systems
 ACM Transactions on Graphics (TOG), N0NN
N  [NN] A
Torralba and W
T
Freeman
Accidental pinhole and  pinspeck cameras: Revealing the scene outside the picture
 Computer Vision and Pattern Recognition (CVPR)
IEEE.,  pages NNN–NNN, N0NN
N  [NN] A
Velten, T
Willwacher, O
Gupta, A
Veeraraghavan,  M
Bawendi, and R
Raskar
Recovering three-dimensional  shape around a corner using ultrafast time-of-flight imaging
 Nature Communications, N(N):NNN, N0NN
ACM Transactions  on Graphics
N  [NN] H
Wu, M
Rubinstein, E
Shih, J
Guttag, F
Durand, and  W
Freeman
Eulerian video magnification for revealing subtle  changes in the world
IEEE Signal Processing Letters, N0NN
 N  [NN] L
Xia, C
Chen, and J
Aggarwal
Human detection using  depth information by kinect
Computer Vision and Pattern  Recognition Workshops (CVPRW), N0NN
N  [N0] F
Xu, D
Shin, D
Venkatraman, R
Lussana, F
Villa,  F
Zappa, V
Goyal, F
Wong, and J
Shapiro
Photon-efficient  computational imaging with a single-photon camera
Computational Optical Sensing and Imaging, N0NN
N  [NN] Z
Zhang, P
Isola, and E
Adelson
Sparklevision: Seeing the  world through random specular microfacets
N0NN
N  NNNNLinear Differential Constraints for Photo-Polarimetric Height Estimation   Linear Differential Constraints for Photo-polarimetric Height Estimation  Silvia Tozza  Sapienza - Università di Roma  tozza@mat.uniromaN.it  William A
P
Smith  University of York  william.smith@york.ac.uk  Dizhong Zhu  University of York  dzNNN@york.ac.uk  Ravi Ramamoorthi  UC San Diego  ravir@cs.ucsd.edu  Edwin R
Hancock  University of York  edwin.hancock@york.ac.uk  Abstract  In this paper we present a differential approach to photopolarimetric shape estimation
We propose several alternative differential constraints based on polarisation and photometric shading information and show how to express them  in a unified partial differential system
Our method uses the  image ratios technique to combine shading and polarisation  information in order to directly reconstruct surface height,  without first computing surface normal vectors
Moreover,  we are able to remove the non-linearities so that the problem reduces to solving a linear differential problem
We also  introduce a new method for estimating a polarisation image  from multichannel data and, finally, we show it is possible  to estimate the illumination directions in a two source setup,  extending the method into an uncalibrated scenario
From  a numerical point of view, we use a least-squares formulation of the discrete version of the problem
To the best of  our knowledge, this is the first work to consider a unified  differential approach to solve photo-polarimetric shape estimation directly for height
Numerical results on synthetic  and real-world data confirm the effectiveness of our proposed method
 N
Introduction  A recent trend in photometric [N, NN, NN, NN, NN, N0]  and physics-based [NN] shape recovery has been to develop  methods that solve directly for surface height, rather than  first estimating surface normals and then integrating them  into a height map
Such methods are attractive since: N
 they only need solve for a single height value at each pixel  (as opposed to the two components of surface orientation),  N
integrability is guaranteed, N
errors do not accumulate  through a two step pipeline of shape estimation and integration and N
it enables combination with cues that provide  depth information directly [N0]
In both photometric stereo  [NN, NN, NN] and recently in shape-from-polarisation (SfP)  [NN], such a direct solution was made possible by deriving  equations that are linear in the unknown surface gradient
 In this paper, we explore the combination of SfP  constraints with photometric constraints (i.e
photopolarimetric shape estimation) provided by one or two light  sources
Photometric stereo with three or more light sources  is a very well studied problem with robust solutions available under a range of different assumptions
Two source  photometric stereo is still considered a difficult problem  [NN] even when the illumination is calibrated and albedo is  known
We show that various formulations of one and two  source photo-polarimetric stereo lead to the same general  problem (in terms of surface height), that illumination can  be estimated and that certain combinations of constraints  lead to an albedo invariant formulation
Hence, with only  modest additional data capture requirements (a polarisation  image rather than an intensity image), we arrive at an approach for uncalibrated two source photometric stereo
We  make the following novel contributions:  • We show how to estimate a polarisation image from multichannel data such as from colour images, multiple light source data or both (Sec
N.N)
 • We show how polarisation and photometric constraints (Sec
N) can be expressed in a unified formulation (of  which previous work [NN] is a special case) and that  various combinations of these constraints provide different practical advantages (Sec
N)
 • We show how to estimate the illumination directions in two source photo-polarimetric data leading to an uncalibrated solution (Sec
N)
 N.N
Related Work  The polarisation state of light reflected by a surface provides a cue to the material properties of the surface and,  via a relationship with surface orientation, the shape
Polarisation has been used for a number of applications, inNNNNN    cluding early work on material segmentation [NN] and diffuse/specular reflectance separation [NN]
However, there  has been a resurgent interest [NN, N0, NN, NN] in using polarisation information for shape estimation
 Shape-from-polarisation The degree to which light is linearly polarised and the orientation associated with maximum reflection are related to the two degrees of freedom  of surface orientation
In theory, this polarisation information alone restricts the surface normal at each pixel to two  possible directions
Both Atkinson and Hancock [N] and  Miyazaki et al
[NN] solve the problem of disambiguating  these polarisation normals via propagation from the boundary under an assumption of global convexity
Huynh et al
 [N] also disambiguate polarisation normals with a global  convexity assumption but estimate refractive index in addition
These works all used a diffuse polarisation model  while Morel et al
[NN] use a specular polarisation model  for metals
Recently, Taamazyan et al
[NN] introduced a  mixed specular/diffuse polarisation model
All of these  methods estimate surface normals that must be integrated  into a height map
Moreover, since they rely entirely on the  weak shape cue provided by polarisation and do not enforce  integrability, the results are extremely sensitive to noise
 Photo-polarimetric methods There have been a number  of attempts to combine photometric constraints with polarisation cues
Mahmoud et al
[NN] used a shape-fromshading cue with assumptions of known light source direction, known albedo and Lambertian reflectance to disambiguate the polarisation normals
Atkinson and Hancock [N]  used calibrated, three source Lambertian photometric stereo  for disambiguation but avoiding an assumption of known  albedo
Smith et al
[NN] showed how to express polarisation and shading constraints directly in terms of surface  height, leading to a robust and efficient linear least squares  solution
They also show how to estimate the illumination,  up to a binary ambiguity, making the method uncalibrated
 However, they require known or uniform albedo
We explore variants of this method by introducing additional constraints that arise when a second light source is introduced,  allowing us to relax the uniform albedo assumption
We  also give an explanation for why the matrix they consider is  full-rank except in a unique case
Recently, Ngo et al
[NN]  derived constraints that allowed surface normals, light directions and refractive index to be estimated from polarisation images under varying lighting
However, this approach  requires at least N lights
All of the above methods operate  on single channel images and do not exploit the information  available in colour images
 Polarisation with additional cues Rahmann and Canterakis [NN] combined a specular polarisation model with  stereo cues
Similarly, Atkinson and Hancock [N] used polarisation normals to segment an object into patches, simplifying stereo matching
Stereo polarisation cues have also  been used for transparent surface modelling [NN]
Huynh  et al
[N] extended their earlier work to use multispectral  measurements to estimate both shape and refractive index
 Drbohlav and Sara [N] showed how the Bas-relief ambiguity [N] in uncalibrated photometric stereo could be resolved  using polarisation
However, this approach requires a polarised light source
Recently, Kadambi et al
[N0] proposed  an interesting approach in which a single polarisation image  is combined with a depth map obtained by an RGBD camera
The depth map is used to disambiguate the normals and  provide a base surface for integration
 N
Representing Polarisation Information  We place a camera at the origin of a three-dimensional  coordinate system (Oxyz) in such a way that Oxy coincides  with the image plane and Oz with the optical axis
In Sec
N  we propose a unified formulation for a variety of methods,  all of which assume a) orthographic projection, b) known  refractive index of the surface
Other assumptions will be  given later on, depending on the specific problem at hand
 We denote by v the viewer direction, by s a general light  source direction with v N= s
We only require the third com- ponents of these unit vectors to be greater than zero (i.e
all  the vectors belong to the upper hemisphere)
We will denote  by t a second light source where required
We parametrise  the unknown surface height by the function z(x), where x = (x, y) is an image location, and the unit normal to the surface at the point x is given by:  n(x) = n̂(x)  |n̂(x)| = [−zx,−zy, N]T √  N + |∇z(x)|N , (N)  where n̂(x) is the outgoing normal vector and zx, zy de- notes the partial derivative of z(x) w.r.t
x and y, respec- tively, so that ∇z(x) = (zx, zy)
We now introduce rele- vant polarization theory, describing how we can estimate a  polarisation image from multichannel data
 N.N
Polarisation image  When unpolarised light is reflected by a surface it becomes partially polarised [NN]
A polarisation image can  be estimated by capturing a sequence of images in which  a linear polarising filter in front of the camera lens is rotated through a sequence of P ≥ N different angles ϑj , j ∈ {N, 


, P}
The measured intensity at a pixel varies sinusoidally with the polariser angle:  iϑj (x) = iun(x) (  N + ρ(x) cos(Nϑj − Nφ(x)) )  
(N)  The polarisation image is thus obtained by decomposing the  sinusoid at every pixel location into three quantities [NN]:  the phase angle, φ(x), the degree of polarisation, ρ(x), and the unpolarised intensity, iun(x)
The parameters of the si- nusoid can be estimated from the captured image sequence  NNN0    using non-linear least squares [N], linear methods [N] or via  a closed form solution [NN] for the specific case of P = N, ϑ ∈ {0◦, NN◦, N0◦}
N.N
Multichannel polarisation image estimation  A polarisation image is usually computed by fitting the  sinusoid in (N) to observed data in a least squares sense
 Hence, from P ≥ N measurements we estimate iun, ρ and φ
In practice, we may have access to multichannel mea- surements
For example, we may capture colour images  (N channels), polarisation images with two different light  source directions (N channels) or both (N channels)
Since  ρ and φ depend only on surface geometry (assuming that, in the case of colour images, the refractive index does not  vary with wavelength), then we expect these quantities to  be constant over the channels
On the other hand, iun will vary between channels either because of a shading change  caused by the different lighting or because the albedo or  light source intensity is different in the different colour  channels
Hence, in a multichannel setting with C channels, we have C + N unknowns and CP observations
If we use information across all channels simultaneously, the system  is more constrained and the solution will be more robust  to noise
Moreover, we do not need to make an arbitrary  choice about the channel from which we estimate the polarisation image
This idea shares something in common with  that of Narasimhan et al
[NN], though their material/shape  separation was not in the context of polarisation
 Specifically, we can express the multichannel observations in channel c with polariser angle ϑj as  icϑj (x) = i c un(x)(N + ρ(x) cos(Nϑj − Nφ(x)))
(N)  The system of equations is linear in the unpolarised intensities and, by a change of variables, can be made linear in  ρ and φ [N]
Hence, we wish to solve a bilinear system and do so in a least squares sense using interleaved alternating  minimisation
Specifically, we a) fix ρ and φ and then solve linearly for the unpolarised intensity in each channel and  b) then fix the unpolarised intensities and solve linearly for  ρ and φ using all channels simultaneously
Concretely, for a single pixel, we obtain the unpolarised intensities across  channels by solving:  min iNun(x),...,i  C un(x)  ∥  ∥  ∥ CI  [  iNun(x), 


, i C un(x)  ]T − dI ∥  ∥  ∥  N  , (N)  where CI ∈ RCP×C is given by  CI =        (N + ρ(x) cos(NϑN − Nφ(x)))IC ..
 (N + ρ(x) cos(NϑP − Nφ(x)))IC       , (N)  with IC denoting the C×C identity matrix, and dI ∈ RCP is given by  dI = [  i N ϑN(x), 


, i  C ϑN(x), i  N ϑN(x), 


, i  C ϑP  (x) ]T  
 Then, with the unpolarised intensities fixed, we solve for ρ and φ using the following linearisation:  min a,b  ∥  ∥  ∥  ∥  Cρφ  [  a b  ]  − dρφ ∥  ∥  ∥  ∥  N  , (N)  where [a b]T = [ρ(x) cos(Nφ(x)), ρ(x) sin(Nφ(x))]T , and Cρφ ∈ RCP×N is given by  Cρφ =                        iNun(x) cos(NϑN) i N un(x) sin(NϑN)  ..
..
 iNun(x) cos(NϑP ) i N un(x) sin(NϑP )  iNun(x) cos(NϑN) i N un(x) sin(NϑN)  ..
..
 iCun(x) cos(NϑP ) i C un(x) sin(NϑP )                        , (N)  and dρφ ∈ RCP is given by:  dρφ =                        iNϑN(x)− i N un(x)  ..
 iNϑP (x)− i N un(x)  iNϑN(x)− i N un(x)  ..
 iCϑP (x)− i C un(x)                        
(N)  We estimate ρ and φ from the linear parameters using φ(x) = NNatanN(b, a) and ρ(x) =  √ aN + bN
 We initialise by computing a polarisation image from  one channel using linear least squares, as in [N], and  then use the estimated ρ and φ to begin alternating inter- leaved optimisation by solving for the unpolarised intensities across channels
We interleave and alternate the two  steps until convergence
In practice, we find that this approach not only dramatically reduces noise in the polarisation images but also removes the ad hoc step of choosing  an arbitrary channel to process
We show an example of  the results obtained in Figure N
The multichannel result is  visibly less noisy than the single channel performance
 N
Photo-polarimetric height constraints  In this section we describe the different constraints provided by photo-polarimetric information and then show  how to combine them to arrive at linear equations in the  unknown surface height
 N.N
Degree of polarisation constraint  A polarisation image provides a constraint on the surface normal direction at each pixel
The exact nature of the  constraint depends on the polarisation model used
In this  paper we will consider diffuse polarisation, due to subsurface scattering (see [N] for more details)
The degree of diffuse polarisation ρd(x) at each point x can be expressed in  NNNN    Input Single channel estimation Multichannel estimation  0  0.N  N  N.N  N  N.N  N  0  0.N  0.N  0.N  0.N  0.N  0.N  0  0.N  N  N.N  N  N.N  N  0  0.N  0.N  0.N  0.N  Figure N
Multichannel polarisation image estimation
Left to right: an image from the input sequence; phase angle (φ) and degree of  polarisation (ρ) estimated from a single channel; phase angle (φ) and degree of polarisation (ρ) estimated from three colour channels and  two light source directions
 terms of the refractive index η and the surface zenith angle θ ∈ [0, πN ] as follows (Cf
[N]):  ρd(x) = (N)  (η − N/η)N sinN(θ) N+NηN−(η+N/η)N sinN(θ)+N cos(θ)  √  ηN− sinN(θ) 
 Recall that the zenith angle is the angle between the unit surface normal vector n(x) and the viewing direction v
If we know the degree of polarisation ρd(x) and the refractive index η (or have good estimates of them at hand), equation (N) can be rewritten with respect to the cosine of the zenith angle, and expressed in terms of the function, f(ρd(x), η), that depends on the measured degree of polarisation and the refractive index:  cos(θ) = n(x) · v = f(ρd(x), η) = (N0) √  ηN(N−ρNd)+Nη N(NρNd+ρd−N)+ρ  N d+Nρd−Nη  Nρd √  N−ρNd+N  (ρd + N)N (ηN + N) + NηN(NρNd + Nρd − N)  where we drop the dependency of ρd on (x) for brevity
 N.N
Shading constraint  The unpolarised intensity provides an additional constraint on the surface normal direction via an appropriate reflectance model
We assume that pixels have been labelled  as diffuse or specular dominant and restrict consideration to  diffuse shading
In practice, we deal with specular pixels  in the same way as [NN] and simply assume that they point  in the direction of the halfway vector between s and v
For  the diffuse pixels, we therefore assume that light is reflected  according to the Lambert’s law
Hence, the unpolarised intensity is related to the surface normal by:  iun(x) = γ(x) cos(θi) = γ(x)n(x) · s, (NN)  where γ(x) is the albedo
Writing n(x) in terms of the gradient of z as reported in (N), (NN) can be rewritten as follows:  iun(x) = γ(x) −∇z(x) · s̃+ sN √  N + |∇z(x)|N , (NN)  with s̃ = (sN, sN)
This is a non-linear equation, but we will see in Sec
N.N and N.N how it is possible to remove the  non-linearity by using the ratios technique
 N.N
Phase angle constraint  An additional constraint comes from the phase angle,  which determines the azimuth angle of the surface normal  α(x) ∈ [0, Nπ] up to a NN0◦ ambiguity
This constraint can be rewritten as a collinearity condition [NN], that is satisfied  by either of the two possible azimuth angles implied by the  phase angle measurement
Specifically, for diffuse pixels  we require the projection of the surface normal into the x-y plane, [nx ny], and a vector in the image plane pointing in the phase angle direction, [sin(φ) cos(φ)], to be collinear
This corresponds to requiring  n(x) · [cos(φ(x)) − sin(φ(x)) 0]T = 0
(NN)  In terms of the surface gradient, using (N), it is equivalent to  (− cosφ, sinφ) · ∇z = 0
(NN)  A similar expression can be obtained for specular pixels,  substituting in the πN -shifted phase angles
The advantage  of doing this will become clear in Sec
N.N
 N.N
Degree of polarisation ratio constraint  Combining the two constraints illustrated in Sec
N.N and  N.N, we can arrive at a linear equation, that we refer to as  the DOP ratio constraint
Recall that cos(θ) = n(x) ·v and that we can express n in terms of the gradient of z by using (N), then isolating the non-linear term in (N0) we obtain  √  N + |∇z(x)|N = −∇z(x) · ṽ + vN f(ρd(x), η)  , (NN)  where ṽ = (vN, vN)
On the other hand, considering the shading information contained in (NN), and again isolating  the non-linearity we arrive at the following  √  N + |∇z(x)|N = γ(x)−∇z(x) · s̃+ sN iun(x)  
(NN)  Note that we are supposing s N= v, and iun(x) N= 0, f(ρd(x), η) N= 0
Inspecting Eqs
(NN) and (NN) we obtain  −∇z(x) · ṽ + vN f(ρd(x), η)  = γ(x) −∇z(x) · s̃+ sN  iun(x) 
(NN)  NNNN    We thus arrive at the following partial differential equation  (PDE):  b(x) · ∇z(x) = h(x), (NN)  where  b(x) := b(f,iun) = iun(x)ṽ − γ(x)f(ρd(x), η) s̃, (NN)  and  h(x) := h(f,iun) = iun(x)vN − γ(x)f(ρd(x), η) sN
(N0)  N.N
Intensity ratio constraint  Finally, we construct an intensity ratio constraint by considering two unpolarised images, iun,N, iun,N, taken from two different light source directions, s, t
We construct our con- straint equation by applying (NN) twice, once for each light  source
We can remove the non-linearity as before and take  a ratio, arriving at the following equation:  iun,N(−∇z(x) · s̃+ sN) = iun,N(−∇z(x) · t̃+ tN)
(NN)  The above equation is independent of albedo, light source  intensity and non-linear normalisation term
Again as before, we can rewrite (NN) as a PDE in the form of (NN) with  b(x) := b(iun,N,iun,N) = iun,N(x)s̃− iun,N(x) t̃, (NN)  where t̃ = (tN, tN), and  h(x) := h(iun,N,iun,N) = iun,N(x)sN − iun,N(x) tN
(NN)  N
A unified PDE formulation  Commencing from the constraints introduced in Sec
N,  in this section we show how to solve several different problems in photo-polarimetric shape estimation
The common  feature is that these are all linear in the unknown height, and  are expressed in a unified formulation in terms of a system  of PDEs in the same general form:  B(x)∇z(x) = h(x), (NN)  where B : Ω̄ → RJ×N, h : Ω̄ → RJ×N, denoting by Ω the reconstruction domain and being J = N, N or N depend- ing on the cases
(NN) is a compact and general equation,  suitable for describing several cases in a unified differential  formulation that solves directly for surface height
 Different combinations of the three constraints described  in Sec
N that are linear in the surface gradient can be combined in the formulation of (NN)
Each corresponds to different assumptions and have different pros and cons
We  explore three variants and show that [NN] is a special case  of our formulation
We summarise the alternative formulations in Tab
N
 Phase DOP Intensity  Method angle ratio ratio  [NN] X X  Proposed N X X  Proposed N X X  Proposed N X X X Table N
Summary of the different formulations  N.N
Single light and polarisation formulation  This case has been studied in [NN]
It uses a single polarisation image, requires known illumination (though [NN]  show how this can be estimated if unknown) and assumes  that albedo is known or uniform
This last assumption is  quite restrictive, since it can only be applied to objects with  homogeneous surfaces
With just a single illumination condition, only the phase angle and DOP ratio constraints are  available
This thus becomes a special case of our general  unified formulation (NN), where B and h are defined as  B =  [  b (f,iun) N b  (f,iun) N  − cosφ sinφ  ]  , h = [h(f,iun), 0]T , (NN)  with b(f,iun) and h(f,iun) defined by (NN) and (N0), with uni- form γ(x) and v = [0, 0, N]T 
 N.N
Proposed N: Albedo invariant formulation  Our first proposed method uses the phase angle con- straint (NN) and two unpolarised images, taken from two different light source directions, obtained through (NN) and combined as in (NN)
In this case the problem studied is described by the system of PDEs (NN) with  B(x) =  [  b (iun,N,iun,N)  N b (iun,N,iun,N)  N  − cosφ sinφ  ]  ,h(x) =  [  h(iun,N,iun,N)  0  ]  ,  (NN)  where b(iun,N,iun,N) and h(iun,N,iun,N) defined as in (NN) and (NN)
The phase angle does not depend on albedo and the  intensity ratio constraint is invariant to albedo
As a result, this formulation is particularly powerful because it allows albedo invariant height estimation
Moreover, the light  source directions in the two images can be estimated (again,  in an albedo invariant manner) using the method in Sec
N
 Once surface height has been estimated, we can compute  the surface normal at each pixel and it is then straightforward to estimate an albedo map using (NN)
Where we have  two diffuse observations, we can compute albedo from two  equations of the form of (NN) in a least squares sense
In  real data, where we have specular pixel labels, we use only  the diffuse observations at each pixel
To avoid artifacts at  the boundary of specular regions, we introduce a gradient  consistency term into the albedo estimation
We encourage  the gradient of the albedo map to match the gradients of the  intensity image for diffuse pixels
 NNNN    N.N
Proposed N: Phase invariant formulation  Our second proposed method uses only the DOP ratio  and the intensity ratio constraints
This means that phase  angle estimates are not used
The advantage of this is  that phase angles are subject to a shift of πN at specular  reflections when compared to diffuse reflections
So, the  phase angle constraint relies upon having accurate per-pixel  specularity labels, which classify reflections as either dominantly specular or diffuse (or alternatively use a mixed polarisation model [NN] with a four way ambiguity)
In this  case we need a) two unpolarised intensity images, taken  with two different light source directions, s and t, obtained  through (NN), b) polarisation information from the function  f(ρ, η) and c) knowledge of the albedo map
We need s, t,v non-coplanar in order to have the matrix field B not singular
Note that the function f , obtained from polariza- tion information (as in (N0)), is the same for the two required images
The reason for this is that it does not depend  on the light source directions but only on the viewer direction v which does not change
This formulation can be deduced starting from (NN) and (NN), arriving at a PDE system  as in (NN) with  B = [b(f,iun,N),b(f,iun,N),b(iun,N,iun,N)]T , (NN)  and h = [h(f,iun,N), h(f,iun,N), h(iun,N,iun,N)]T , using (NN), (N0), (NN), (NN) to define the vector fields b and the scalar fields  h that appear in B and h
 N.N
Proposed N: Most constrained formulation  Our final proposed method combines all of the previous  constraints, leading to a problem of the form (NN) with  B=            b (f,iun,N) N b  (f,iun,N) N  b (f,iun,N) N b  (f,iun,N) N  b (iun,N,iun,N) N b  (iun,N,iun,N) N  − cosφ sinφ            , h =          h(f,iun,N)  h(f,iun,N)  h(iun,N,iun,N)  0          
 (NN)  This formulation uses the most information and so is potentially the most robust method
However, it requires known  albedo in order to use the DOP ratio constraint
Nevertheless, it is possible to first apply proposed method N, estimate the albedo and then re-estimate surface height using  the maximally constrained formulation and the estimated  albedo map
In fact, the best performance is obtained by iterating these two steps, alternately using the surface height  estimate to compute albedo and then using the updated  albedo to re-compute surface height
 N.N
Extension to colour images  We now consider how to extend the above systems of  equations when colour information is available
If a surface is lit by a coloured point source, then each pixel provides three equations of the form in (NN)
In principle, this  provides no more information than a grayscale observation  since the surface normal and light source direction are fixed  across colour channels
However, in the presence of noise  using all three observations improves robustness
In particular, if the albedo value at a pixel is lower in one colour  channel, the signal to noise ratio will be worse in that channel than the others
For a multicoloured object, it is impossible to choose a single colour channel that provides the best  signal to noise ratio across the whole object
For this reason, we propose to use information from all colour channels  where available
 We already exploit colour information in the estimation  of the polarisation image in Sec
N.N
Hence, the phase angle  estimates have already benefited from the improved robustness
Both the DOP ratio and intensity ratio constraints can  also exploit colour information by repeating each constraint  three times, once for each colour channel
In the case of the  intensity ratio, the colour albedo once again cancels if ratios  are taken between the same colour channels under different  light source directions
 N
Height estimation via linear least squares  We have seen that each of the variants illustrated in the  previous section, each with different advantages, can be  written as a PDE system (NN)
Denoting by M the num- ber of pixels, we discretise the gradient in (NN) via finite  differences, arriving at the following linear system in z  Az = h̄, (NN)  where A = B̄G, with G ∈ RNM×M the matrix of finite dif- ference gradients
B̄ ∈ RJM×NM is the discrete per-pixel version of the matrix B(x), hence A ∈ RJM×M , where J depends on the various proposed cases reported in Sec
N  (J = N for (NN) and (NN), J = N for (NN) and J = N for (NN))
h̄ is the discrete per-pixel version of the function  h(x), h̄ ∈ RJM×N, and z ∈ RM×N the vector of the un- known height values
The resulting discrete system is large,  since we have JM linear equations in M unknowns, but sparse, since A has few non-zero values for each row, and  has as unknowns the height values
The per-pixel matrix A  is a full-rank matrix, for each choice of B̄ that comes from  the proposed formulations in Sec
N, under the different assumptions specified for each case
The per-pixel matrix A  related to [NN] is full-rank except in one case: when the  first two components of the light vector s are non-zero and  sN = −sN and it happens that the phase angle is φ = π/N at least in one pixel
In that case, the matrix has a rankdeficiency (though in practice φ assuming a value of exactly π/N, up to numerical tolerance, is unlikely)
 We want to find a solution of (NN) in the least-squares  sense, i.e
find a vector z ∈ RM such that  ||Az− h̄||NN ≤ ||Ay − h̄||NN, ∀y ∈ RM 
(N0)  NNNN    Considering the associated system of normal equations  AT (Az− h̄) = 0, (NN)  it is well-known that if there exists z ∈ RM that satisfies (NN), then z is also solution of the least-squares problem,  i.e
z satisfies (N0)
Since A is a full-rank matrix, then the  matrix ATA is not singular, hence there exists a unique solution z of (NN) for each data term h̄
Since neither B nor  h depend on z in (NN), the solution can be computed only up to an additive constant (which is consistent with the orthographic projection assumption)
To resolve the unknown  constant, knowledge of z at just one pixel is sufficient
In our implementation, we remove the height of one pixel from  the variables and substitute its zero value elsewhere
 N
Two source lighting estimation  Our three proposed shape estimation methods require  knowledge of the two light source directions
Previously,  Smith et al
[NN] showed that a single polarisation image  can be used to estimate illumination conditions up to a binary ambiguity
However, to do so, they assumed that the  albedo was known or uniform, and they also worked only  with a single colour channel
In a two source setting, we  show that it is possible to estimate both light source directions simultaneously, and do so in an albedo invariant manner
Moreover, we can exploit information across different  colour channels to improve robustness to noise
Hence, our  three methods can be used in an uncalibrated setting
 The intensity ratio (NN) provides one equation per pixel  relating unpolarised intensities, surface gradient and light  source directions
Given two polarisation images with different light directions, we have one such equation per pixel  and six unknowns in total
We assume that ambiguous surface gradient estimates are known from ρ and φ, and then use (NN) to estimate the light source directions
 The intensity ratio (NN) is homogeneous in s and t and so has a trivial solution s = t = [0 0 0]T 
If we assume that the intensity of the light source remains constant in each colour channel across the two images, then this intensity di- vides out when taking an intensity ratio and so the length of the light source vectors is arbitrary
We therefore constrain them to unit length (avoiding the trivial solution), and rep- resent them by spherical coordinates (θs, αs) and (θt, αt), such that [sN, sN, sN] = [cosαs sin θs, sinαs sin θs, cos θs] and [tN, tN, tN] = [cosαt sin θt, sinαt sin θt, cos θt]
This reduces the number of unknowns to four
We can now write the residual at each pixel given an estimate of the light source directions
There are two possible residuals, depend- ing on which of the two ambiguous polarisation normals we use
From the phase angle and the zenith angle estimated from the degree of polarisation using (N0), we have two possible surface normal directions at each pixel and there- fore two possible gradients: zx(x) ≈ ± cosφ(x) tan θ(x), zy(x) ≈ ± sinφ(x) tan θ(x)
Hence, the residuals at pixel  xj in channel c are given by either:  rj,c(θs, αs, θt, αt) =i c un,N(xj)(−zx(xj)tN − zy(xj)tN + tN)−  i c un,N(xj)(−zx(xj)sN − zy(xj)sN + sN),  or  qj,c(θs, αs, θt, αt) =i c un,N(xj)(zx(xj)tN + zy(xj)tN + tN)−  i c un,N(xj)(zx(xj)sN + zy(xj)sN + sN)
 We can now write a minimisation problem for light source  direction estimation by summing the minimum of the two  residuals over all pixels and colour channels:  min θs,αs,θt,αt  ∑  j,c  min[rNj,c(θs, αs, θt, αt), q N j,c(θs, αs, θt, αt)]
 The minimum of two convex functions is not itself convex and so this optimisation is non-convex
However, we  find that, even with a random initialisation, it almost always  converges to the global minimum
As in [NN], the solution is still subject to a binary ambiguity, in that if (s, t) is a solution then (Ts,Tt) is also a solution (with T = diag([−N,−N, N])), corresponding to the convex/concave ambiguity
We resolve this simply by choosing the maximal solution when surface height is later recovered
 N
Experiments  We begin by using synthetic data generated from the  Mozart height map (Fig
N)
We differentiate to obtain surface normals and compute unpolarised intensities by rendering the surface using light sources s = [N, 0, N]T and t = [−N,−N, N]T according to (NN)
We experiment with both uniform albedo and varying albedo for which we use  a checkerboard pattern
We simulate the effect of polarisation according to (N), varying the polariser angle between  0◦ and NN0◦ in N0◦ increments
Next, we corrupt this data by adding Gaussian noise with zero mean and standard deviation σ, saturate and quantise to N bits
This noisy data provides the input to our reconstruction
First, we estimate a  polarisation image using the method in Sec
N.N, then apply  each of the proposed methods or the state-of-the-art comparison method [NN] to recover the height map
 In Tab
N we report Root-Mean-Square (RMS) error in  the surface height (in pixels) and mean angular error (in degrees) in the surface normals obtained by differentiating the  estimated surface height
In Fig
N we show a sample of  qualitative results from this experiment
In all cases, more  than one of our proposed methods outperform [NN]
When  albedo is uniform, our phase invariant (Prop
N) or maximally constrained solution (Prop
N) provides the best results
When albedo is non-uniform, the albedo invariant  method (Prop
N) provides much better performance
Although the combination of the albedo invariant method followed by the maximally constrained method (Prop
N+N)  NNNN    Input	 Es)mated	Normals	 Es)mated	Albedo	 Reillumina)on	 Es)mated	surface	 [NN]	  Figure N
Qualitative results on real objects with varying albedo obtained by using Prop
N+N and comparison to [NN] (zoom for detail)
 Input Input Ground  (uniform albedo) (varying albedo) truth height  [NN] Prop
N Prop
N Prop
N Prop
N+N  U n  if o rm  a lb  ed o  V a ry  in g  a lb  ed o  Figure N
Qualitative results on synthetic data
 does not give quantitatively the best performance, we find  that on real world data containing more complex noise and  specular reflections, this approach is most robust
 In Fig
N we show qualitative results on two real objects  with spatially varying albedo
From left to right we show:  an image from the input sequence; the surface normals of  the estimated height map (inset sphere shows how orientation is visualised as colour); the estimated albedo map; a  re-rendering of the estimated surface and albedo map under  novel lighting with Blinn-Phong reflectance [N]; a rotated  view of the estimated surface; and, for comparison, reconstructions of the same surfaces using [NN]
The results of  [NN] are highly distorted in the presence of varying albedo
 Our approach avoids transfer of albedo details into the recovered shape, leading to convincing relighting results
 N
Conclusions  In this paper we have introduced a unifying formulation  for recovering height from photo-polarimetric data and proposed a variety of methods that use different combinations  of linear constraints
We proposed a more robust way to  estimate a polarisation image from multichannel data and  σ = 0% σ = 0.N% σ = N%  Setting Method Height Normal Height Normal Height Normal  (pix) (deg) (pix) (deg) (pix) (deg)  Uniform  albedo,  known  lighting  [NN] N.NN N.NN N.NN N.NN N.0N NN.NN  Prop
N N.NN N.NN N.NN N.N0 N.NN N.NN  Prop
N 0.NN N.NN 0.N0 N.N0 N.N0 N.NN  Prop
N 0.NN N.0N 0.NN N.NN N.NN N.NN  Prop
N+N N.NN N.NN N.NN N.0N N.NN N.NN  Uniform  albedo,  estimated  lighting  [NN] N.N0 N.NN N.NN N.NN N.NN NN.NN  Prop
N N.NN N.NN N.NN N.NN N.0N N.NN  Prop
N 0.NN N.NN 0.NN N.NN N.NN N.NN  Prop
N 0.NN N.0N 0.NN N.NN N.NN N.NN  Prop
N+N N.NN N.NN N.NN N.NN N.NN N.NN  Unknown  albedo,  known  lighting  [NN] NN.N0 NN.0N NN.NN NN.NN N0.NN NN.NN  Prop
N N.NN N.NN N.NN N.NN N.NN NN.NN  Prop
N NNN.NN NN.NN NN0.0N NN.NN NNN.NN NN.NN  Prop
N NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  Prop
N+N N.NN N.NN N.N0 NN.NN N.NN NN.N0  Unknown  albedo,  estimated  lighting  [NN] N.NN NN.N0 N.N0 NN.NN N.NN NN.NN  Prop
N N.NN N.NN N.NN N.NN N.NN NN.NN  Prop
N NN0.NN NN.NN NNN.NN NN.NN NN.NN NN.NN  Prop
N NN.NN NN.NN NN.0N NN.NN N0.NN NN.NN  Prop
N+N N.NN N.NN N.NN NN.0N N.NN NN.NN  Table N
Height and surface normal errors on synthetic data
 showed how to estimate lighting from two source photopolarimetric images
Together, our methods provide uncalibrated, albedo invariant shape estimation with only two  light sources
Since our unified differential formulation  does not depend on a specific camera setup or a chosen reflectance model, the most obvious target for future work is  to move to a perspective projection, considering more complex reflectance models, exploiting better the information  available in specular reflection and polarisation
In addition,  since our methods directly estimate surface height, it would  be straightforward to incorporate positional constraints, for  example provided by binocular stereo
 Acknowledgements  This work was supported mainly by the “GNCS - INdAM”,  in part by ONR grant N000NNNNNN0NN and the UC San  Diego Center for Visual Computing
W
Smith was supported by EPSRC grant EP/N0NNNNN/N
 NNNN    References  [N] G
A
Atkinson and E
R
Hancock
Recovery of surface  orientation from diffuse polarization
IEEE Transactions on  Image processing, NN(N):NNNN–NNNN, N00N
N, N, N  [N] G
A
Atkinson and E
R
Hancock
Shape estimation using polarization and shading from two views
IEEE Trans
 Pattern Anal
Mach
Intell., NN(NN):N00N–N0NN, N00N
N  [N] G
A
Atkinson and E
R
Hancock
Surface reconstruction  using polarization and photometric stereo
In Proc
CAIP,  pages NNN–NNN, N00N
N  [N] P
N
Belhumeur, D
J
Kriegman, and A
Yuille
The Basrelief ambiguity
Int
J
Comput
Vision, NN(N):NN–NN, NNNN
 N  [N] J
F
Blinn
Models of light reflection for computer synthesized pictures
Computer Graphics, NN(N):NNN–NNN, NNNN
 N  [N] O
Drbohlav and R
Šára
Unambiguous determination of  shape from photometric stereo with unknown light sources
 In Proc
ICCV, pages NNN–NNN, N00N
N  [N] A
Ecker and A
D
Jepson
Polynomial shape from shading
 In Proc
CVPR, pages NNN–NNN
IEEE, N0N0
N  [N] C
P
Huynh, A
Robles-Kelly, and E
Hancock
Shape and  refractive index recovery from single-view polarisation images
In Proc
CVPR, pages NNNN–NNNN, N0N0
N, N  [N] C
P
Huynh, A
Robles-Kelly, and E
R
Hancock
Shape  and refractive index from single-view spectro-polarimetric  images
Int
J
Comput
Vision, N0N(N):NN–NN, N0NN
N  [N0] A
Kadambi, V
Taamazyan, B
Shi, and R
Raskar
Polarized  ND: High-quality depth sensing with polarization cues
In  Proc
ICCV, N0NN
N, N  [NN] A
H
Mahmoud, M
T
El-Melegy, and A
A
Farag
Direct  method for shape recovery from polarization and shading
In  Proc
ICIP, pages NNNN–NNNN, N0NN
N  [NN] R
Mecca and M
Falcone
Uniqueness and approximation  of a photometric shape-from-shading model
SIAM Journal  on Imaging Sciences, N(N):NNN–NNN, N0NN
N  [NN] R
Mecca and Y
Quéau
Unifying diffuse and specular  reflections for the photometric stereo problem
In IEEE  Workshop on Applications of Computer Vision (WACV), Lake  Placid, USA, N0NN, N0NN
N  [NN] D
Miyazaki, M
Kagesawa, and K
Ikeuchi
Transparent  surface modeling from a pair of polarization images
IEEE  Trans
Pattern Anal
Mach
Intell., NN(N):NN–NN, N00N
N  [NN] D
Miyazaki, R
T
Tan, K
Hara, and K
Ikeuchi
 Polarization-based inverse rendering from a single view
In  Proc
ICCV, pages NNN–NNN, N00N
N  [NN] O
Morel, F
Meriaudeau, C
Stolz, and P
Gorria
Polarization imaging applied to ND reconstruction of specular metallic surfaces
In Proc
EI N00N, pages NNN–NNN, N00N
N  [NN] S
G
Narasimhan, V
Ramesh, and S
Nayar
A class of photometric invariants: Separating material from shape and illumination
In Proc
ICCV, volume N, pages NNNN – NNNN,  N00N
N  [NN] S
Nayar, X
Fang, , and T
Boult
Separation of reflection  components using color and polarization
Int
J
Comput
 Vision, NN(N):NNN–NNN, NNNN
N  [NN] T
T
Ngo, H
Nagahara, and R
Taniguchi
Shape and light  directions from shading and polarization
In Proc
CVPR,  pages NNN0–NNNN, N0NN
N  [N0] Y
Quéau, R
Mecca, and J
D
Durou
Unbiased photometric  stereo for colored surfaces: A variational approach
In N0NN  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNNN–NNNN, June N0NN
N  [NN] Y
Quéau, R
Mecca, J.-D
Durou, and X
Descombes
Photometric stereo with only two images: A theoretical study and  numerical resolution
Image and Vision Computing, NN:NNN–  NNN, N0NN
N  [NN] S
Rahmann and N
Canterakis
Reconstruction of specular  surfaces using polarization imaging
In Proc
CVPR, N00N
 N  [NN] W
Smith and F
Fang
Height from photometric ratio with  model-based light source selection
Computer Vision and  Image Understanding, NNN:NNN – NNN, N0NN
N  [NN] W
A
P
Smith, R
Ramamoorthi, and S
Tozza
Linear depth  estimation from an uncalibrated, monocular polarisation image
In Computer Vision - ECCV N0NN, volume NNNN of Lecture Notes in Computer Science, pages N0N–NNN, N0NN
N, N,  N, N, N, N, N  [NN] V
Taamazyan, A
Kadambi, and R
Raskar
Shape from  mixed polarization
arXiv preprint arXiv:NN0N.0N0NN, N0NN
 N, N  [NN] S
Tozza, R
Mecca, M
Duocastella, and A
Del Bue
Direct differential photometric stereo shape recovery of diffuse  and specular surfaces
Journal of Mathematical Imaging and  Vision, NN(N):NN–NN, N0NN
N  [NN] L
B
Wolff
Polarization vision: a new sensory approach to  image understanding
Image Vision Comput., NN(N):NN–NN,  NNNN
N, N  [NN] L
B
Wolff and T
E
Boult
Constraining object features  using a polarization reflectance model
IEEE Trans
Pattern  Anal
Mach
Intell., NN(N):NNN—NNN, NNNN
N  NNNNPlaying for Benchmarks   Playing for Benchmarks  Stephan R
Richter  TU Darmstadt  Zeeshan Hayder  ANU  Vladlen Koltun  Intel Labs  Figure N
Data for several tasks in our benchmark suite
Clockwise from top left: input video frame, semantic segmentation, semantic  instance segmentation, ND scene layout, visual odometry, optical flow
Each task is presented on a different image
 Abstract  We present a benchmark suite for visual perception
The  benchmark is based on more than NN0K high-resolution  video frames, all annotated with ground-truth data for both  low-level and high-level vision tasks, including optical flow,  semantic instance segmentation, object detection and tracking, object-level ND scene layout, and visual odometry
 Ground-truth data for all tasks is available for every frame
 The data was collected while driving, riding, and walking a total of NNN kilometers in diverse ambient conditions  in a realistic virtual world
To create the benchmark, we  have developed a new approach to collecting ground-truth  data from simulated worlds without access to their source  code or content
We conduct statistical analyses that show  that the composition of the scenes in the benchmark closely  matches the composition of corresponding physical environments
The realism of the collected data is further validated via perceptual experiments
We analyze the performance of state-of-the-art methods for multiple tasks, providing reference baselines and highlighting challenges for  future research
 N
Introduction  Visual perception is believed to be a compound process  that involves recognizing objects, estimating their threedimensional layout in the environment, tracking their motion and the observer’s own motion through time, and integrating this information into a predictive model that supports planning and action [NN]
Advanced biological vision systems have a number of salient characteristics
The  component processes of visual perception (including motion perception, shape perception, and recognition) operate  in tandem and support each other
Visual perception integrates information over time
And it is robust to transformations of visual appearance due to environmental conditions,  such as time of day and weather
 These characteristics are recognized by the computer  vision and robotics communities
Researchers have argued that diverse computer vision tasks should be tackled  in concert [NN] and developed models that could support  this [N, NN, NN]
The importance of video and motion cues  is widely recognized [N, NN, N0, NN]
And robustness to environmental conditions is a long-standing challenge in the  field [NN, NN]
 We present a benchmark suite that is guided by these  NNNN    considerations
The input modality is high-resolution video
 Comprehensive and accurate ground truth is provided for  low-level tasks such as visual odometry and optical flow  as well as higher-level tasks such as semantic instance segmentation, object detection and tracking, and ND scene layout, all on the same data
Ground truth for all tasks is available for every video frame, with pixel-level segmentations  and subpixel-accurate correspondences
The data was collected in diverse environmental conditions, including night,  rain, and snow
This combination of characteristics in a single benchmark aims to support the development of broadcompetence visual perception systems that construct and  maintain comprehensive models of their environments
 Collecting a large-scale dataset with all of these properties in the physical world would have been impossible with  known techniques
To create the benchmark, we have used  an open-world computer game with realistic content and appearance
The game world simulates a living city and its  surroundings
The rendering engine incorporates comprehensive modeling of image formation
The training, validation, and test sets in the dataset comprise continuous video  sequences with NNN,0NN fully annotated frames, collected  while driving, riding, and walking a total of NNN kilometers  in different environmental conditions
 To create the benchmark, we have developed a new  methodology for collecting data from simulated worlds  without access to their source code or content
Our approach integrates dynamic software updating, bytecode  rewriting, and bytecode analysis, going significantly beyond prior work to allow video-rate collection of ground  truth for all tasks, including subpixel-accurate dense correspondences and instance-level ND layouts
 We conduct extensive experiments that evaluate the performance of state-of-the-art models for semantic segmentation, semantic instance segmentation, visual odometry, and  optical flow estimation on the presented benchmark
The  results indicate that the benchmark is challenging and creates new opportunities for progress
Detailed performance  analyses point to promising directions for future work
 N
Background  Progress in computer vision has been driven by the systematic application of the common task framework [NN]
 The Pascal VOC benchmark supported the development  of object detection techniques that broadly influenced the  field [NN]
The ImageNet benchmark was instrumental  in the development of deep networks for visual recognition [NN]
The Microsoft COCO benchmark provides  data for object detection and semantic instance segmentation [NN]
The SUN and Places datasets support research  on scene recognition [NN, NN]
The Middlebury benchmarks for stereo, multi-view stereo, and optical flow played  a pivotal role in the development of low-level vision algorithms [N, NN, NN, NN]
 The KITTI benchmark suite provides ground truth for  visual odometry, stereo reconstruction, optical flow, scene  flow, and object detection and tracking [NN]
It is an important precursor to our work because it provides video input  and evaluates both low-level and high-level vision tasks on  the same data
It also highlights the limitations of conventional ground-truth acquisition techniques [NN]
For example, optical flow ground truth is sparse and is based on fitting approximate CAD models to moving objects [NN], object detection and segmentation data is available for only a  small number of frames and a small set of classes [NN], and  the dataset as a whole represents the appearance of a single  town in fair weather
 The Cityscapes benchmark evaluates semantic segmentation and semantic instance segmentation models on images acquired while driving around N0 cities in Europe [N0]
 The quality of the annotations is very high and the dataset  has become the default benchmark for semantic segmentation
It also highlights the challenges of annotating realworld images: only a sparse set of N,000 frames is annotated  at high accuracy, and average annotation time was more  than N0 minutes per frame
Data for tasks other than semantic segmentation and semantic instance segmentation is  not provided, and data was only acquired during the day and  in fair weather
 The importance of robustness to different environmental conditions is recognized as a key challenge in the autonomous driving community
The HCI benchmark suite  offers recordings of the same street block on six days  distributed over three seasons [NN]
The Oxford RobotCar dataset provides more than N00 recordings of a route  through central Oxford collected over a year, including  recordings made at night, under heavy rain, and in the presence of snow [NN]
These datasets aim to address an important gap, but are themselves limited in ways that are indicative of the challenges of ground-truth data collection in  the physical world
The HCI benchmark lacks ground truth  for moving objects, does not address tasks beyond stereo  and optical flow, and is limited to a single N00-meter street  section
The Oxford RobotCar dataset only provides GPS,  IMU, and LIDAR traces with minimal post-processing and  does not contain the ground-truth data that would be necessary to benchmark most tasks considered in this paper
 Our benchmark combines and extends some of the most  compelling characteristics of prior benchmarks and datasets  for visual perception in urban environments: a broad range  of tasks spanning low-level and high-level vision evaluated  on the same data (KITTI), highly accurate instance-level semantic annotations (Cityscapes), and diverse environmental  conditions (Oxford)
We integrate these characteristics in a  single large-scale dataset for the first time, and go beyond  by providing temporally consistent object-level semantic  NNNN    ground truth and ND scene layouts at video rate, as well as  subpixel-accurate dense correspondences
To achieve this,  we use three-dimensional virtual worlds
 Simulated worlds are commonly used to benchmark optical flow algorithms [N, N, N] and visual odometry systems [NN, NN, NN]
Virtual worlds have been used to evaluate  the robustness of feature descriptors [NN], test visual surveillance systems [N0], evaluate multi-object tracking models [NN], and benchmark UAV target tracking [NN]
Virtual  environments have also been used to collect training data for  pedestrian detection [N0], stereo reconstruction and optical  flow estimation [NN], and semantic segmentation [NN, NN]
 A key challenge in scaling up this approach to comprehensive evaluation of broad-competence visual perception  systems is populating virtual worlds with content: acquiring and laying out geometric models, applying surface materials, configuring the lighting, and realistically animating  all objects and their interactions over time
Realism on a  large scale is primarily a content creation problem
While  all computer vision researchers have access to open-source  engines that incorporate the latest advances in real-time rendering [N0], there are no open-source virtual worlds with  content that approaches the scale and realism of commercial productions
 Our approach is inspired by recent research that has  demonstrated that ground-truth data for semantic segmentation can be produced for images from computer games without direct access to their source code or content [NN]
The  data collection techniques developed in this prior work are  not sufficient for our purposes: they cannot generate data  at video rate, do not produce instance-level segmentations,  and do not produce dense correspondences, among other  limitations
To create the presented benchmark, we have  developed a new methodology for collecting ground-truth  data from computer games, described in the next section
 N
Data Collection Methodology  To create the presented benchmark, we use Grand Theft  Auto V, a modern game that simulates a functioning city  and its surroundings in a photorealistic three-dimensional  world
(The publisher of Grand Theft Auto V allows noncommercial use of footage from the game as long as certain conditions are met, such as non-commercial use and not  distributing spoilers [NN, NN].) A known way to extract data  from such simulations is to inject a middleware between the  game and its underlying graphics library via detouring [NN]
 The middleware acts as the graphics library and receives  all rendering commands from the game
Previous work has  adapted graphics debugging software for this purpose [NN]
 Since such software was designed for different use-cases,  previous methods were limited in the frequency and granularity of data that could be captured
To create the presented  benchmark, we have developed dedicated middleware that  captures only data that is relevant to ground-truth synthesis,  enabling it to operate at video rate and collect much richer  datasets
 This work builds on fairly detailed understanding of realtime rendering
We provide an introductory overview of  real-time rendering pipelines in the supplement and refer  the reader to a comprehensive reference for more details [N]
 Slicing shaders
To capture resources at video rate, we augment the game’s shaders with additional inputs, outputs, and  instructions
This enables tagging each pixel in real time  with resource IDs as well as depth and transparency values
In order to augment the shaders at runtime, we employ  dynamic software updating [NN]
Dynamic software updates are used for patching critical software systems without  downtime
Instead of shutting down a system, updating it,  and bringing it back up, inactive parts are identified at runtime and replaced by new versions, while also transitioning the program state
This can be extremely challenging  for complex software systems
We leverage the fact that  shaders are distributed as bytecode blocks in order to be  executed on the GPU, which simplifies static analysis and  modification in comparison to arbitrary compiled binaries
 In particular, we start by slicing pixel shaders [NN]
That  is, we identify unused slots for inputs and outputs and instructions that modify transparency values, and ignore the  remaining parts of the program
Note that source code  for the shaders is not available and we operate directly on  the bytecode
We broadcast an identifier for rendering resources as well as the z-coordinate to all affected pixels
 To capture alpha values, we copy instructions that write to  the alpha channel of the original G-buffer, redirect them to  our G-buffer, and insert them right after the original instructions
We materialize our modifications via bytecode rewriting, a technique used for runtime modification of Java programs [NN]
 Identifying objects
The resources used to render an object  correlate with the object’s semantic class
By tracking resources, rendered images can be segmented into temporally  consistent patches that lie within object parts and are associated with semantic class labels [NN]
This produces annotations for semantic segmentation, but does not segment individual object instances
To segment the scene at the level  of instances, we track transformation matrices used to place  objects and object parts in the world
Parts that make up  the same object share transformation matrices
By clustering patches that share transformation matrices, individual  object instances can be identified and segmented
 Identifying object instances in this way has the added  benefit of resolving conflicting labels for rendering resources that are used across multiple semantic classes
 (E.g., wheels that are used on both buses and trucks.) To  determine the semantic label for an object instance, we aggregate the semantic classes of all patches that make up the  NNNN    instance and take the majority vote for the semantic class of  the object
 Capturing ND scene layout
We record the meshes used  for rendering (specifically, vertex and index buffers as well  as the input layout for vertex buffers) and transformation  matrices for each mesh
This enables us to later recover the  camera position from the matrices, thus obtaining ground  truth for visual odometry
We further use the vertex buffers  to compute ND bounding boxes of objects and transform  the bounding boxes to the camera’s reference frame: this  produces the ground-truth data for ND scene layout
 Tracking objects
Tracking object instances and computing optical flow requires associating meshes across successive frames
This poses a set of challenges:  N
Meshes can appear and disappear
 N
Multiple meshes can have the same segment ID
 N
The camera and the objects are moving
 N
Due to camera motion, meshes may be replaced by  versions at different levels of detail, which can change  the mesh, segment ID, and position associated with the  same object in consecutive frames [NN]
 We tackle the first challenge by also recording the rendering  resources used for meshes that either failed the depth test  or lie outside the view frustum
We address the remaining  challenges by formulating the association of meshes across  frames as a weighted matching problem [N0]
 Let Vf , Vg be nodes representing meshes rendered in two  consecutive frames f and g, and let G = (Vf ∪ Vg, E) be a graph with edges E = {ei,j = (vi ∈ Vf , vj ∈ Vg)}, con- necting every mesh in one frame to all meshes in the other
 Let s : V → S be the mapping from a mesh to its seg- ment ID, C the set of semantic class labels, D ⊆ C the set of semantic class labels that represent dynamic objects, and  c : V → C the mapping from meshes to semantic classes
Let p : V → RN be the mapping from a mesh to its position in the game world, given by the world matrix W that transforms the mesh
Let d(vi, vj) = ‖p(vi)− p(vj)‖ be the dis- tance between positions of two meshes in the game world,  and let w : C → R+0 be a function for the class-dependent maximum speed of instances
Let Ec = {ei,j : c(vi) = c(vj)} and Ek = {ei,j : s(vi) = s(vj)} be the sets of edges associating meshes that share the same semantic class  or segment ID, respectively
We define the sets Ed = {ei,j : c(vi) ∈ D}∩Ec∩Ek and Es = {ei,j : c(vi) ∈ C \D}∩Ec of edges associating meshes of dynamic objects and static  objects, respectively
Finally, we define a set  Em = {ei,j : d(vi, vj) < w(c(vi))} ∩ (Ed ∪ Es) (N)  and define a weight function on the graph:  w(vi, vj) =  {  d(vi, vj) ei,j ∈ Em  0 otherwise (N)  This leverages the previously obtained mappings of segments to semantic classes
Intuitively, we associate meshes  by minimizing their motion between frames and prune associations of mismatched classes and mismatched segment  IDs in the case of dynamic objects
Additionally, we cap  motions depending on the mapped semantic classes (first  term of Eq
N)
By solving the maximum weight matching problem on the graph, we associate meshes across pairs  of consecutive frames
For keeping track of objects that  are invisible for multiple frames, we extrapolate their last  recorded motion linearly for a fixed number of frames and  add their meshes to Vf and Vg 
 Dense correspondences
So far we have established correspondences between whole meshes, allowing us to track  objects across frames
We now extend this to dense correspondences over the surfaces of rigid objects
To this  end, we need to acquire dense ND coordinates in the object’s local coordinate system, trace transformations applied  by shaders along the rendering pipeline in different frames,  and invert these transformations to associate pixel coordinates in different frames with stable ND coordinates in the  object’s original coordinate frame
To simplify the inversion, we disable tessellation shaders
 Let x∈RN be a surface point in object space, represented in homogenous coordinates
The transformation pipeline  maps x to a camera-space point s via a sequence of linear  transformations: s = CPVWx
For fast capture, we only record the world, view, and projection matrices W,V, P  for each mesh and the z-component (depth) of s for each  pixel, as the clipping matrix C and the x, y-components of  s can be derived from the image resolution
By setting the  w-component of s to N and inverting the matrices, we re- cover x for each pixel
To compute dense correspondences  across frames f and g, we obtain object-space points x from  image-space points in f , and then use the transformation  matrices of g (obtained by associating meshes across f and  g) to obtain corresponding camera-space points in g
 Inverting shader slices
We now tackle dense correspondences over the surfaces of nonrigid objects
Nonrigid  transformations are applied by vertex shaders
Unlike rigid  correspondences, these transformations are not communicated to the graphics library in the form of matrices represented in standard format, and can be quite complex
Consider the set of vertices in the mesh of a person, represented  in its local coordinate frame in a neutral pose
To transform  the person to sit on a bench, the vertices are transformed individually by vertex shaders
The rasterizer interpolates the  transformed mesh to produce ND coordinates for every pixel  depicting the person
To invert these black-box transformations, we have initially considered recording all input and  output buffers in the pipeline and learning a non-parametric  mapping from output to input via random forests [NN]
This  would have yielded approximate model-space coordinates  NNNN    for each pixel, rather than the precise, subpixel-accurate  correspondences we seek
 We have instead developed an exact solution based on  selectively executing the shader slices offline
The key idea  is to use slices of a vertex shader for transforming meshes  and inverting the rasterizer stage to map pixels back to their  corresponding mesh location
We analyze the data flow  within a vertex shader towards the output register of the  transformed ND points
Since we are only interested in the  ND coordinates, we can ignore (and remove) the computation of texture coordinates and other attributes, producing a slice of the vertex shader, which applies a potentially  non-rigid transform to a mesh
Each transform, however,  is rigid at the level of triangles
Furthermore, the order of  vertices is preserved in the output of a shader slice, making  the mapping from output vertices to input vertices trivial
 The remaining step is to map pixels back to the transformed  triangles, which reduces to inverting the linear interpolation  of the rasterizer
Combining these steps, we map dense ND  points in camera coordinates to the object’s local coordinate  frame
As in the case of rigid transformations, this inversion is the crucial step
By chaining inverse transformations  from one frame and forward transformations from another,  we can establish dense subpixel-accurate correspondences  over nonrigidly moving objects across arbitrary baselines
 N
Dataset  We have used the approach described in Section N to  collect video sequences with a total of NNN,0NN frames at  NNN0×N0N0 resolution
The sequences were captured in five different ambient conditions: day (overcast), sunset,  rain, snow, and night
All sequences are annotated with the  following types of ground truth, for every video frame: pixelwise semantic categories (semantic segmentation), dense  pixelwise semantic instance segmentations, instance-level  semantic boundaries, object detection and tracking (the semantic instance IDs are consistent over time), ND scene  layout (each instance is accompanied by an oriented and  localized ND bounding box, also consistent over time),  dense subpixel-accurate optical flow, and ego-motion (visual odometry)
In addition, the metadata associated with  each frame enables creating ground truth for additional  tasks, such as subpixel-accurate correspondences across  wide baselines (non-consecutive frames) and relative pose  estimation for widely separated images
 The dataset is split into training, validation, and test sets,  containing NNNK, N0K, and N0K frames, respectively
The  split was performed such that the sets cover geographically  distinct areas (no geographic overlap between train/val/test)  and such that each set contains a roughly balanced distribution of data acquired in different conditions (day/night/etc.)  and different types of scenes (suburban/downtown/etc.)
To  perform the split, we clustered the recorded frames by geographic position and manually assigned clusters to the three  sets
The dataset and associated benchmark suite are referred to as the VIsual PERception benchmark (VIPER)
 Statistical analysis
We compare the statistics of VIPER  to three benchmark datasets: Cityscapes [N0], KITTI [NN],  and MS COCO [NN]
The results of multiple analyses are  summarized in Figure N
First we evaluate the realism of  the simulated world by analyzing the distributions of the  number of categories and the number of instances present in  each image in the dataset
For these statistics, our reference  is the Cityscapes dataset, since the annotations in Cityscapes are the most accurate and comprehensive
As shown in  Figure N(a,b), the distribution of the number of categories  per image in VIPER is almost identical to the distribution  in Cityscapes
The Jensen-Shannon divergence (JSD) be- tween these two distributions is 0.00N: two orders of magnitude tighter than JSD (Cityscapes ‖ COCO) = 0.NN and JSD (Cityscapes ‖ KITTI) = 0.NN
The distribution of the number of instances per image in VIPER also matches  the Cityscapes distribution more closely by an order of  magnitude than the other datasets: JSD (Cityscapes ‖ ·) = (0.0N; 0.NN; 0.N0) for · = (VIPER;COCO;KITTI)
Further details are provided in the supplement
 Next we analyze the number of instances per semantic  class
As shown in Figure N(c), the number of semantic categories with instance-level labels in our dataset is NN, compared to N0 in Cityscapes and N in KITTI, while the number  of instances labeled with pixel-level segmentation masks for  each class is more than an order of magnitude higher
 Finally, we evaluate the realism of ND scene layouts  in our dataset
Figure N(d) reports the distribution of vehicles as a function of distance from the camera in the  three datasets for which this information could be obtained
 The Cityscapes dataset again serves as our reference due  to its comprehensive nature (data from N0 cities)
The distance distribution in VIPER closely matches that of Cityscapes: JSD (Cityscapes ‖ VIPER) = 0.0N, compared to JSD (Cityscapes ‖ KITTI) = 0.NN
 Perceptual experiment
To assess the realism of VIPER in  comparison to other synthetic datasets, we conduct a perceptual experiment
We sampled N00 random images each  from VIPER, SYNTHIA [NN], Virtual KITTI [NN], the Freiburg driving sequence [NN], and the Urban Canyon dataset  (regular perspective camera) [NN], as well as Cityscapes as a  real-world reference
Pairs of images from separate datasets  were selected at random and shown to Amazon Mechanical  Turk (MTurk) workers who were asked to pick the more realistic image in each pair
Each MTurk job involved a batch  of ∼N00 pairwise comparisons, balanced across conditions  and randomized, along with sentinel pairs that test whether  the worker is attentive and diligent
Each job is performed  by N0 different workers, and jobs in which any sentinel pair  is ranked incorrectly are pruned
Each pair is shown for  NNNN    N N0 NN N0 NN  Number of categories  0  N0  N0  N0  N0  N0  P e  rc e  n ta  g e   o f   im a  g e  s  a) categories per image  Cityscapes  VIPER  KITTI  MS COCO  N N0 NN N0 NN N0 NN N0  Number of instances  0  N  N0  NN  N0  P e  rc e  n ta  g e   o f   im a  g e  s  b) instances per image  Cityscapes  VIPER  KITTI  MS COCO  N00 N0N N0N N0N  Number of categories  N00  N0N  N0N  N0N  N0N  N0N  N0N  In s ta  n c e s  p  e r   c a te  g o ry  c) instances per category  Cityscapes  VIPER  KITTI  MS COCO  N0 N00 NN0 N00 NN0 N00  Distance [m]  0  0.N  N  N.N  N  N.N  N  P e rc  e n ta  g e  o  f  in  s ta  n c e s  d) vehicle distance  Cityscapes  VIPER  KITTI  Figure N
Statistical analysis of the VIPER dataset in comparison  to Cityscapes, KITTI, and MS COCO
(a,b) Distributions of the  number of categories and the number of instances present in each  image
(c) Number of instances per semantic class
(d) Distributions of vehicles as a function of distance from the camera
 a timespan chosen at random from { N N , N N , N N , N, N, N, N} seconds
For each timespan and pair of datasets, at least N0  distinct image pairs were rated
This experimental protocol  was adopted from concurrent work on photographic image  synthesis [N]
 The results are shown in Figure N
VIPER images were  rated more realistic than all other synthetic datasets
The  difference is already apparent at NNN milliseconds, when the  VIPER images are rated more realistic than other synthetic  datasets in N0% (vs Freiburg) to NN% (vs SYNTHIA) of the  comparisons
This is comparable to the relative realism rate  of real Cityscapes images vs VIPER at this time (NN%)
At  N seconds, VIPER images are rated more realistic than all  other synthetic datasets in NN% (vs Virtual KITTI) to NN%  (vs SYNTHIA) of the comparisons
Surprisingly, the votes  for real Cityscapes images versus VIPER at N seconds are  only at NN%, lower than the rates for VIPER versus two of  the other four synthetic datasets
 N
Baselines and Analysis  We have set up and analyzed the performance of representative methods for semantic segmentation, semantic instance segmentation, visual odometry, and optical flow
Our  primary goals in this process were to further validate the realism of the VIPER benchmark, to assess its difficulty relative to other benchmarks, to provide reference baselines for  a number of tasks, and to gain additional insight into the  performance characteristics of state-of-the-art methods
 NNN NN0 N00 N000 N000 N000 N000  Time for comparison [ms]  0  N0  N0  N0  N0  N00  P e rc  e n ta  g e  o  f  c o m  p a ri s o n s  VIPER is more realistic  SYNTHIA  Canyon  Freiburg  Virtual KITTI  Cityscapes  Figure N
Perceptual experiment
Results of pairwise A/B tests  with MTurk workers, who were asked to select the more realistic  image in a pair of images, given different timespans
The dashed  line represents chance
VIPER images were rated more realistic  than images from other synthetic datasets
 Semantic segmentation
Our first task is semantic segmentation and our primary measure is mean intersection  over union (IoU), averaged over semantic classes [N0, NN]
 We have evaluated two semantic segmentation models
 First, we benchmarked a fully-convolutional setup of the  ResNet-N0 network [NN, NN]
Second, we benchmarked the  Pyramid Scene Parsing Network (PSPNet), an advanced semantic segmentation system [NN]
The results are summarized in Table N
 Method d ay  su n se  t  ra in  sn o w  n ig  h t  al l  FCN-ResNet NN.N NN.N NN.N NN.N NN.N NN.N  PSPNet [NN] NN.N NN.N NN.N NN.N N0.N NN.N  Table N
Semantic segmentation
Mean IoU in each environmental condition, as well as over the whole test set
 We draw several conclusions
First, the relative performance of the two baselines on VIPER is consistent with  their relative performance on the Cityscapes validation set  (PSPNet is ahead by ∼NN points on both datasets)
Second, VIPER is more challenging than Cityscapes: while PSPNet is above N0% on Cityscapes, it’s at NN% on VIPER
 We view this additional headroom as a benefit to the community, given that the performance of the leading methods  on Cityscapes rose by more than N0 points in the last year
 Third, the relative performance in different conditions is  broadly consistent across the two methods: for example,  day is easier than sunset, which is easier than rain
Fourth,  the methods do not fail in any condition, indicating that contemporary semantic segmentation systems, which are based  on convolutional networks, are quite robust when diverse  training data is available
 Semantic instance segmentation
To measure semantic  instance segmentation accuracy, we compute region-level  average precision (AP) for each class, and average across  NNNN    classes and across N0 overlap thresholds between 0.N and  0.NN [N0, NN]
We benchmark the performance of Multi-task  Network Cascades (MNC) [NN] and of Boundary-aware Instance Segmentation (BAIS) [NN]
Each system was trained  in two conditions: (a) on the complete training set and (b)  on day and sunset images only
The results are shown in  Table N
 Method Train condition d ay  su n  se t  ra in  sn o w  n ig  h t  al l  MNC [NN] day & sunset N.0 N.N N.N N.N N.N N.N  MNC [NN] all N.N N.N N.N N.N N.N N.N  BAIS [NN] day & sunset NN.N NN.N NN.N N.N N.N N.N  BAIS [NN] all NN.N NN.N NN.N N0.N NN.N NN.N  Table N
Semantic instance segmentation
We report AP in each  environmental condition as well as over the whole test set
Models  trained only on day and sunset images do not generalize well to  other conditions
 We first observe that the relative performance of MNC  and BAIS on VIPER is consistent with their relative performance on Cityscapes, and that VIPER is more challenging  than Cityscapes in this task as well (NN.N AP for BAIS on  VIPER vs NN.N on Cityscapes)
Furthermore, we see that  the performance of systems that are only trained on day  and sunset images drops in other conditions
The performance drop is present in all conditions and is particularly  dramatic at night
Note that we matched the number of  training iterations in the ‘day & sunset’ and ‘all’ regimes,  so the ‘day & sunset’ models are trained for a proportionately larger number of epochs to compensate for the smaller  number of images
 A performance analysis of instance segmentation accuracy as a function of objects’ distance from the camera is  provided in Figure N
 Optical flow
To evaluate the accuracy of optical flow algorithms, we use a new robust measure, the Weighted Area  Under the Curve
The measure evaluates the inlier rates for  a range of thresholds, from 0 to N px, and integrates these  rates, giving higher weight to lower-threshold rates
The  thresholds and their weights are inversely proportional
The  precise definition is provided in the supplement
This measure is a continuous analogue of the measure used for the  KITTI optical flow leaderboard [NN]
Instead of selecting a  specific threshold (N px), we integrate over all thresholds between 0 and N px and reward more accurate estimates within  this range
 We benchmark the performance of four well-known optical flow algorithms: LDOF [N], EpicFlow [NN], FlowFields [N], and FlowNet [NN]
For all methods, we used the  publicly available implementations with default parameters
 The results are summarized in Table N
The relative ranking of the four methods on VIPER is consistent with their  Method d ay  su n  se t  ra in  sn o w  n ig  h t  al l  FlowNet [NN] NN.N NN.N NN.N N0.N NN.N NN.N  LDOF [N] NN.N N0.N NN.N NN.0 NN.N NN.N  EpicFlow [NN] NN.N NN.N NN.N NN.N NN.N NN.N  FlowFields [N] NN.N NN.N NN.N NN.N N0.N NN.0  Table N
Optical flow
We report the weighted area under the  curve, a robust measure that is analogous to the inlier rate at a  given threshold but integrates over a range of thresholds (0 to N  px) and assigns higher weight to lower thresholds
Higher is better
 relative accuracy on the KITTI optical flow dataset
The  poor performance of FlowNet is likely due to its exclusive  training on a different dataset; we expect that training this  model (or its successor [NN]) on VIPER will yield much better results
Overall the results indicate that VIPER is more  challenging than KITTI, even in the daytime condition
We  attribute this to the more varied and complex nature of our  scenes (see Figure N), and the density and precision of our  ground truth (including on nonrigidly moving objects, on  thin structures, around boundaries, etc.)
For all methods,  accuracy degrades markedly in the rain, in the presence of  snow, and at night
 We further analyze the performance of EpicFlow, which  is commonly used as a building block in other optical flow  pipelines (e.g., FlowFields)
Specifically, we investigate the  accuracy of optical flow estimation as a function of object  type, object size, and displacement magnitude
The results of this analysis are shown in Figure N
We see that  the most significant challenges are posed by very large displacements, very large objects (primarily people and vehicles that are close to the camera), ground and vehicle motion, and adverse weather
 Visual odometry
For evaluating the accuracy of visual  odometry algorithms, we follow Geiger et al
[NN] and measure the rotation errors of sequences of different lengths and  speeds
We benchmark two state-of-the-art systems that  represent different approaches to monocular visual odometry: ORB-SLAMN, which tracks sparse features [NN], and  DSO, which optimizes a photometric objective defined over  image intensities [NN]
For fair comparison we run both  methods without loop-closure detection
We tuned hyperparameters for both methods on the training set
To account  for non-deterministic behavior due to threading, we ran all  configurations N0 times on all test sequences
 The results are summarized in Table N
The performance  of the tested methods is highest in the easy day setting and  decreases in adverse conditions
We identified a number of  phenomena that affect performance
For example, detecting keypoints is harder with less contrast (snow), in areas  NNNN    N0 N0 N00 N00  Distance [m]  N  N0  NN  N0  NN  N0  NN  N0  NN  M e a n  A  v e ra  g e  P  re c is  io n  day  N0 N0 N00 N00  Distance [m]  N  N0  NN  N0  NN  N0  NN  N0  NN  M e a n  A  v e ra  g e  P  re c is  io n  sunset  N0 N0 N00 N00  Distance [m]  N  N0  NN  N0  NN  N0  NN  N0  NN  M e a n  A  v e ra  g e  P  re c is  io n  rain  N0 N0 N00 N00  Distance [m]  N  N0  NN  N0  NN  N0  NN  N0  NN  M e a n  A  v e ra  g e  P  re c is  io n  snow  BAIS all  BAIS day & sunset  MNC all  MNC day & sunset  N0 N0 N00 N00  Distance [m]  N  N0  NN  N0  NN  N0  NN  N0  NN  M e a n  A  v e ra  g e  P  re c is  io n  night  Figure N
Analysis of instance segmentation performance as a function of the objects’ distance from the camera, in different conditions
 All methods perform best on objects within N0 meters
Accuracy deteriorates with distance
 0 N N N N N  Endpoint Error  0  N0  N0  N0  N0  N00  P e rc  e n ta  g e  o  f  p ix  e ls  Environmental Conditions  day  sunset  snow  night  rain  0 N N N N N  Endpoint Error  0  N0  N0  N0  N0  N00  P e rc  e n ta  g e  o  f  p ix  e ls  Semantic Class  static  ground  vehicle  0 N N N N N  Endpoint Error  0  N0  N0  N0  N0  N00  P e rc  e n ta  g e  o  f  p ix  e ls  Object Size  small  medium  large  very large N  N  N  N  N  E n  d p  o in  t  E  rr o  r  N0-N N00 N0N N0N  Displacement [px]  Displacement (matched pixels)  0  N0  N0  N0  N0  N00  Figure N
Analysis of optical flow performance, conducted on EpicFlow
Left to right: effect of environmental condition, object type,  object size, and displacement
 Method day sunset rain snow night all  ORB-SLAMN 0.NNN 0.NN0 0.NNN 0.NNN 0.NNN 0.NNN  DSO 0.N0N 0.NN0 0.NNN 0.NNN 0.NN0 0.NNN  Table N
Visual odometry
We report rotation errors (in deg/m)  for ORB-SLAMN [NN] and for DSO [NN]
 lit by moving headlights (night), or in the presence of lens  flares (sunset)
Reflections on puddles (rain) often yield  large sets of keypoints that are consistent with incorrect  pose hypotheses
In all conditions, the accuracy is much  lower than corresponding results on KITTI, indicating that  the VIPER visual odometry benchmark is far more challenging
We attribute this to the different composition of  VIPER scenes, which include wider streets with fewer features at close range, more variation in camera speed, and a  higher concentration of dynamic objects (see Figure N)
 An interesting opportunity for future work is to integrate  visual odometry with semantic analysis of the scene, which  can help prune destabilizing keypoints on dynamic objects  and can restrain scale drift by estimating the scale of objects  in the scene [NN]
The presented benchmark provides integrated ground-truth data that can facilitate the development  of such techniques
 N
Conclusion  We have presented a new benchmark suite for visual perception
The benchmark is enabled by ground-truth data  for both low-level and high-level vision tasks, collected for  more than NN0 thousand video frames in different environmental conditions
We hope that the presented benchmark  will support the development of techniques that leverage  the temporal structure of visual data and the complementary nature of different visual perception tasks
We hope  that the availability of ground-truth data for all tasks on the  same video sequences will support the development of robust broad-competence visual perception systems that construct and maintain effective models of their environments
 The dataset will be released upon publication
Groundtruth data for the test set will be withheld and will be used  to set up a public evaluation server and leaderboard
In addition to the baselines presented in the paper, we plan to  provide reference baselines for additional tasks, such as ND  layout estimation, and to set up challenges that evaluate performance on integrated problems, such as temporally consistent semantic instance segmentation, tracking, and ND  layout estimation in video
 Acknowledgements  We thank Qifeng Chen for the MTurk experiment code,  Fisher Yu for the FCN-ResNet baseline, the authors of PSPNet for evaluating their system on our data, and Jia Xu and  René Ranftl for help with optical flow and visual odometry  analysis
SRR was supported in part by the European Research Council under the European Union’s Seventh Framework Programme (FP/N00N-N0NN) / ERC Grant Agreement  No
N0NNNN
 NNN0    References  [N] P
Agrawal, J
Carreira, and J
Malik
Learning to see by  moving
In ICCV, N0NN
N  [N] T
Akenine-Möller, E
Haines, and N
Hoffman
Real-Time  Rendering
A K Peters, Nrd edition, N00N
N  [N] C
Bailer, B
Taetz, and D
Stricker
Flow fields: Dense correspondence fields for highly accurate large displacement optical flow estimation
In ICCV, N0NN
N  [N] S
Baker, D
Scharstein, J
P
Lewis, S
Roth, M
J
Black, and  R
Szeliski
A database and evaluation methodology for optical flow
International Journal of Computer Vision, NN(N),  N0NN
N, N  [N] J
L
Barron, D
J
Fleet, and S
S
Beauchemin
Performance  of optical flow techniques
International Journal of Computer Vision, NN(N), NNNN
N  [N] H
Bilen and A
Vedaldi
Integrated perception with recurrent  multi-task neural networks
In NIPS, N0NN
N  [N] T
Brox and J
Malik
Large displacement optical flow: Descriptor matching in variational motion estimation
Pattern  Analysis and Machine Intelligence, NN(N), N0NN
N  [N] D
J
Butler, J
Wulff, G
B
Stanley, and M
J
Black
A  naturalistic open source movie for optical flow evaluation
 In ECCV, N0NN
N  [N] Q
Chen and V
Koltun
Photographic image synthesis with  cascaded refinement networks
In ICCV, N0NN
N  [N0] M
Cordts, M
Omran, S
Ramos, T
Rehfeld, M
Enzweiler,  R
Benenson, U
Franke, S
Roth, and B
Schiele
The Cityscapes dataset for semantic urban scene understanding
In  CVPR, N0NN
N, N, N, N  [NN] A
Criminisi, J
Shotton, and E
Konukoglu
Decision forests:  A unified framework for classification, regression, density  estimation, manifold learning and semi-supervised learning
 Foundations and Trends in Computer Graphics and Vision,  N(N-N), N0NN
N  [NN] J
Dai, K
He, and J
Sun
Instance-aware semantic segmentation via multi-task network cascades
In CVPR, N0NN
N  [NN] D
Donoho
N0 years of data science
In Tukey Centennial  Workshop, N0NN
N  [NN] A
Dosovitskiy, P
Fischer, E
Ilg, P
Häusser, C
Hazirbas,  V
Golkov, P
van der Smagt, D
Cremers, and T
Brox
 FlowNet: Learning optical flow with convolutional networks
In ICCV, N0NN
N  [NN] J
Engel, V
Koltun, and D
Cremers
Direct sparse odometry
 Pattern Analysis and Machine Intelligence, NN, N0NN
N, N  [NN] M
Everingham, S
M
A
Eslami, L
J
V
Gool, C
K
I
 Williams, J
M
Winn, and A
Zisserman
The Pascal visual object classes challenge: A retrospective
International  Journal of Computer Vision, NNN(N), N0NN
N, N  [NN] A
Gaidon, Q
Wang, Y
Cabon, and E
Vig
Virtual worlds  as proxy for multi-object tracking analysis
In CVPR, N0NN
 N, N  [NN] F
Galasso, N
S
Nagaraja, T
J
Cardenas, T
Brox, and  B
Schiele
A unified video segmentation benchmark: Annotation, metrics and analysis
In ICCV, N0NN
N  [NN] A
Geiger, P
Lenz, and R
Urtasun
Are we ready for autonomous driving? The KITTI vision benchmark suite
In  CVPR, N0NN
N, N, N  [N0] A
V
Goldberg and R
Kennedy
An efficient cost scaling  algorithm for the assignment problem
Mathematical Programming, NN, NNNN
N  [NN] R
Haeusler and R
Klette
Analysis of KITTI data for stereo  analysis with stereo confidence measures
In ECCV Workshops, N0NN
N  [NN] A
Handa, R
A
Newcombe, A
Angeli, and A
J
Davison
 Real-time camera tracking: When is high frame-rate best?  In ECCV, N0NN
N  [NN] A
Handa, V
Patraucean, V
Badrinarayanan, S
Stent, and  R
Cipolla
Understanding real world indoor scenes with  synthetic data
In CVPR, N0NN
N  [NN] A
Handa, T
Whelan, J
McDonald, and A
J
Davison
A  benchmark for RGB-D visual odometry, ND reconstruction  and SLAM
In ICRA, N0NN
N  [NN] Z
Hayder, X
He, and M
Salzmann
Boundary-aware instance segmentation
In CVPR, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N  [NN] M
W
Hicks and S
Nettles
Dynamic software updating
 ACM Transactions on Programming Languages and Systems, NN(N), N00N
N  [NN] G
Hunt and D
Brubacher
Detours: Binary interception  of WinNN functions
In USENIX Windows NT Symposium,  NNNN
N  [NN] E
Ilg, N
Mayer, T
Saikia, M
Keuper, A
Dosovitskiy, and  T
Brox
FlowNet N.0: Evolution of optical flow estimation  with deep networks
CVPR, N0NN
N  [N0] D
Jayaraman and K
Grauman
Learning image representations tied to ego-motion
In ICCV, N0NN
N  [NN] B
Kaneva, A
Torralba, and W
T
Freeman
Evaluation of  image features using a photorealistic virtual world
In ICCV,  N0NN
N  [NN] B
Kitt, F
Moosmann, and C
Stiller
Moving on to dynamic  environments: Visual odometry using feature classification
 In IROS, N0N0
N  [NN] I
Kokkinos
UberNet: Training a ‘universal’ convolutional  neural network for low-, mid-, and high-level vision using  diverse datasets and limited memory
In CVPR, N0NN
N  [NN] D
Kondermann, R
Nair, K
Honauer, K
Krispin, J
Andrulis, A
Brock, B
Güssefeld, M
Rahimimoghaddam,  S
Hofmann, C
Brenner, and B
Jähne
The HCI benchmark suite: Stereo and flow ground truth with uncertainties  for urban autonomous driving
In CVPR Workshops, N0NN
N  [NN] T
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft COCO: Common objects in context
In ECCV, N0NN
N, N, N  [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
N  [NN] D
Luebke, M
Reddy, J
D
Cohen, A
Varshney, B
Watson,  and R
Huebner
Level of Detail for ND Graphics
Morgan  Kaufmann, N00N
N  [NN] W
Maddern, G
Pascoe, C
Linegar, and P
Newman
N year,  N000 km: The Oxford RobotCar dataset
International Journal of Robotics Research, NN(N), N0NN
N, N  NNNN    [NN] J
Malik, P
A
Arbeláez, J
Carreira, K
Fragkiadaki, R
B
 Girshick, G
Gkioxari, S
Gupta, B
Hariharan, A
Kar, and  S
Tulsiani
The three R’s of computer vision: Recognition,  reconstruction and reorganization
Pattern Recognition Letters, NN, N0NN
N  [N0] J
Marı́n, D
Vázquez, D
Gerónimo, and A
M
López
 Learning appearance in virtual scenarios for pedestrian detection
In CVPR, N0N0
N  [NN] N
Mayer, E
Ilg, P
Häusser, P
Fischer, D
Cremers,  A
Dosovitskiy, and T
Brox
A large dataset to train convolutional networks for disparity, optical flow, and scene flow  estimation
In CVPR, N0NN
N, N  [NN] M
Menze and A
Geiger
Object scene flow for autonomous  vehicles
In CVPR, N0NN
N  [NN] I
Misra, A
Shrivastava, A
Gupta, and M
Hebert
Crossstitch networks for multi-task learning
In CVPR, N0NN
N  [NN] M
Müller, N
Smith, and B
Ghanem
A benchmark and  simulator for UAV tracking
In ECCV, N0NN
N  [NN] R
Mur-Artal, J
Montiel, and J
D
Tardós
ORB-SLAM:  A versatile and accurate monocular SLAM system
IEEE  Transactions on Robotics, NN(N), N0NN
N, N  [NN] S
K
Nayar and S
G
Narasimhan
Vision in bad weather
 In ICCV, NNNN
N  [NN] A
Orso, A
Rao, and M
J
Harrold
A technique for dynamic  updating of Java software
In International Conference on  Software Maintenance, N00N
N  [NN] S
E
Palmer
Vision Science: Photons to Phenomenology
 MIT Press, NNNN
N  [NN] D
Pathak, R
Girshick, P
Dollár, T
Darrell, and B
Hariharan
Learning features by watching objects move
In CVPR,  N0NN
N  [N0] W
Qiu and A
L
Yuille
UnrealCV: Connecting computer  vision to Unreal engine
In ECCV Workshops, N0NN
N  [NN] J
Revaud, P
Weinzaepfel, Z
Harchaoui, and C
Schmid
 EpicFlow: Edge-preserving interpolation of correspondences for optical flow
In CVPR, N0NN
N  [NN] S
R
Richter, V
Vineet, S
Roth, and V
Koltun
Playing for  data: Ground truth from computer games
In ECCV, N0NN
N  [NN] Rockstar Games
PC single-player mods
http://  tinyurl.com/ycNkqNvn
N  [NN] Rockstar Games
Policy on posting copyrighted Rockstar  Games material
http://tinyurl.com/pjfoqoN
N  [NN] G
Ros, L
Sellart, J
Materzynska, D
Vázquez, and A
M
 Lopez
The SYNTHIA dataset: A large collection of synthetic images for semantic segmentation of urban scenes
In  CVPR, N0NN
N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
S
Bernstein,  A
C
Berg, and F
Li
ImageNet large scale visual recognition challenge
International Journal of Computer Vision,  NNN(N), N0NN
N  [NN] D
Scharstein, H
Hirschmüller, Y
Kitajima, G
Krathwohl,  N
Nesic, X
Wang, and P
Westling
High-resolution stereo  datasets with subpixel-accurate ground truth
In GCPR,  N0NN
N  [NN] D
Scharstein and R
Szeliski
A taxonomy and evaluation of  dense two-frame stereo correspondence algorithms
International Journal of Computer Vision, NN(N-N), N00N
N [NN] S
M
Seitz, B
Curless, J
Diebel, D
Scharstein, and  R
Szeliski
A comparison and evaluation of multi-view  stereo reconstruction algorithms
In CVPR, N00N
N  [N0] G
R
Taylor, A
J
Chosak, and P
C
Brewer
OVVV: Using  virtual worlds to design and evaluate surveillance systems
 In CVPR, N00N
N  [NN] F
Tip
A survey of program slicing techniques
Journal of  Programming Languages, N, NNNN
N  [NN] J
Xiao, K
A
Ehinger, J
Hays, A
Torralba, and A
Oliva
 SUN database: Exploring a large collection of scene categories
International Journal of Computer Vision, NNN(N),  N0NN
N  [NN] Z
Zhang, S
Fidler, and R
Urtasun
Instance-level segmentation for autonomous driving with deep densely connected  MRFs
In CVPR, N0NN
N  [NN] Z
Zhang, H
Rebecq, C
Forster, and D
Scaramuzza
Benefit  of large field-of-view cameras for visual odometry
In ICRA,  N0NN
N, N  [NN] H
Zhao, J
Shi, X
Qi, X
Wang, and J
Jia
Pyramid scene  parsing network
In CVPR, N0NN
N  [NN] B
Zhou, À
Lapedriza, J
Xiao, A
Torralba, and A
Oliva
 Learning deep features for scene recognition using Places  database
In NIPS, N0NN
N  NNNN  http://tinyurl.com/ycNkqNvn http://tinyurl.com/ycNkqNvn http://tinyurl.com/pjfoqoNLearning to Synthesize a ND RGBD Light Field From a Single Image   Learning to Synthesize a ND RGBD Light Field from a Single Image  Pratul P
SrinivasanN, Tongzhou WangN, Ashwin SreelalN, Ravi RamamoorthiN, Ren NgN  NUniversity of California, Berkeley NUniversity of California, San Diego  Abstract  We present a machine learning algorithm that takes as  input a ND RGB image and synthesizes a ND RGBD light  field (color and depth of the scene in each ray direction)
 For training, we introduce the largest public light field  dataset, consisting of over NN00 plenoptic camera light  fields of scenes containing flowers and plants
Our synthesis pipeline consists of a convolutional neural network  (CNN) that estimates scene geometry, a stage that renders  a Lambertian light field using that geometry, and a second  CNN that predicts occluded rays and non-Lambertian effects
Our algorithm builds on recent view synthesis methods, but is unique in predicting RGBD for each light field  ray and improving unsupervised single image depth estimation by enforcing consistency of ray depths that should  intersect the same scene point
 N
Introduction  We focus on a problem that we call “local light field synthesis”, which we define as the promotion of a single photograph to a plenoptic camera light field
One can think of  this as expansion from a single view to a dense ND patch  of views
We argue that local light field synthesis is a  core visual computing problem with high potential impact
 First, it would bring light field benefits such as synthetic  apertures and refocusing to everyday photography
Furthermore, local light field synthesis would systematically lower  the sampling rate of photographs needed to capture large  baseline light fields, by “filling the gap” between discrete  viewpoints
This is a path towards making light field capture for virtual and augmented reality (VR and AR) practical
In this work, we hope to convince the community that  local light field synthesis is actually a tractable problem
 From an alternative perspective, the light field synthesis  task can be used as an unsupervised learning framework for  estimating scene geometry from a single image
Without  any ground-truth geometry for training, we can learn to estimate the geometry that minimizes the difference between  the light field rendered with that geometry and the groundtruth light field
 Light field synthesis is a severely ill-posed problem,  since the goal is to reconstruct a ND light field given just  a single image, which can be interpreted as a ND slice of the  ND light field
To alleviate this, we use a machine learning  approach that is able to utilize prior knowledge of natural  light fields
In this paper, we focus on scenes of flowers and  plants, because they contain interesting and complex occlusions as well as a wide range of relative depths
Our specific  contributions are the introduction of the largest available  light field dataset, the prediction of ND ray depths with a  novel depth consistency regularization to improve unsupervised depth estimation, and a learning framework to synthesize a light field from a single image
 Light Field Dataset We collect the largest available light  field dataset (Sec
N), contaning NNNN light fields of flowers and plants, taken with the Lytro Illum camera
Our  dataset limits us to synthesizing light fields with camerascale baselines, but we note that our model can generalize  to light fields of any scene and baseline given the appropriate datasets
 Ray Depths and Regularization Current view synthesis  methods generate each view separately
Instead, we propose to concurrently predict the entire ND light field by estimating a separate depth map for each viewpoint, which is  equivalent to estimating a depth for each ray in the ND light  field (Sec
N)
We introduce a novel physically-based regularization that encourages the predicted depth maps to be  consistent across viewpoints, alleviating typical problems  that arise in depths created by view synthesis (Fig
N)
We  demonstrate that our algorithm can predict depths from a  single image that are comparable or better than depths estimated by a state-of-the-art physically-based non-learning  method that uses the entire light field [NN] (Fig
N)
 CNN Framework We create and study an end-to-end  convolutional neural network (CNN) framework, visualized  in Fig
N, that factorizes the light field synthesis problem  into the subproblems of estimating scene depths for every  ray (Fig
N, Sec
N) (we use depth and disparity interchangeably, since they are closely related in structured light fields),  rendering a Lambertian light field (Sec
N.N), and predicting  occluded rays and non-Lambertian effects (Sec
N.N)
This  makes the learning process more tractable and allows us to  NNNN    Figure N
We propose a CNN framework that factors the light field synthesis problem into estimating depths for each ray in the light field,  rendering a Lambertian approximation to the light field, and refining this approximation by predicting occluded rays and non-Lambertian  effects (incorrect rays that are refined, in this case red rays that should be the color of the background instead of the flower, are marked with  blue arrows)
We train this network end-to-end by minimizing the reconstruction errors of the Lambertian and predicted light fields, along  with a novel physically-based depth regularization
We demonstrate that we can predict convincing ND light fields and ray depths from a  single ND image
We visualize synthesized light fields as a predicted corner view along with epipolar slices in both the u and v directions of different spatial segments
Please view our supplementary video for compelling animations of our light fields and ray depths
 estimate scene depths, even though our network is trained  without any access to the ground truth depths
Finally, we  demonstrate that it is possible to synthesize high-quality ray  depths and light fields of flowers and plants from a single  image (Fig
N, Fig
N, Fig
N, Fig
N0, Sec
N)
 N
Related Work  Light Fields The ND light field [NN] is the total spatioangular distribution of light rays passing through a region  of free space
Previous work has demonstrated exciting applications of light fields, including rendering images from  new viewpoints [NN], changing the focus and depth-of-field  of photographs after capture [NN], correcting lens aberrations [NN], and estimating scene flow [NN]
 View Synthesis from Light Fields Early work on light  field rendering [NN] captures a densely-sampled ND light  field of a scene, and renders images from new viewpoints  as ND slices of the light field
Closely related work on the  Lumigraph [NN] uses approximate geometry information to  refine the rendered slices
The unstructured Lumigraph rendering framework [N] extends these approaches to use a set  of unstructured (not axis-aligned in the angular dimensions)  ND slices of the light field
In contrast to these pioneering  works which capture many ND slices of the light field to render new views, we propose to synthesize a dense sampling  of new views from just a single slice of the light field
 View Synthesis without Geometry Estimation Alternative approaches synthesize images from new viewpoints  without explicitly estimating geometry
The work of Shi  et al
[NN] uses the observation that light fields are sparse  in the continuous Fourier domain to reconstruct a full light  field from a carefully-constructed ND collection of views
 Didyk et al
[N] and Zhang et al
[NN] reconstruct ND light  fields from pairs of ND slices using phase-based approaches
 Recent works have trained CNNs to synthesize slices of  the light field that have dramatically different viewpoints  than the input slices
Tatarchenko et al
[NN] and Yang et  al
[NN] train CNNs to regress from a single input ND view  to another ND view, given the desired camera rotation
The  exciting work of Zhou et al
[NN] predicts a flow field that  rearranges pixels from the input views to synthesize novel  views that are sharper than directly regressing to pixel values
These methods are trained on synthetic images rendered from large databases of ND models of objects such as  cars and chairs [N], while we train on real light fields
Additionally, they are not able to explicitly take advantage of  geometry because they attempt to synthesize views at arbitrary rotations with potentially no shared geometry between  the input and target views
We instead focus on the problem  of synthesizing a dense sampling of views around the input view, so we can explicitly estimate geometry to produce  higher quality results
 View Synthesis by Geometry Estimation Other methods perform view interpolation by first estimating geometry  from input ND slices of the light field, and then warping the  input views to reconstruct new views
These include view  interpolation algorithms [N, NN] which use wider baseline  unstructured stereo pairs to estimate geometry using multiview stereo algorithms
 More recently, CNN-based view synthesis methods  been proposed, starting with the inspiring DeepStereo  method that uses unstructured images from Google’s Street  View [N0] to synthesize new views
This idea has been extended to view interpolation for light fields given N corner  views [NN], and the prediction of one image from a stereo  pair given the other image [NN, NN, NN]
 NNNN    Figure N
Two equivalent interpretations of the local light field synthesis problem
Left: Given an input image of a scene, with the  field-of-view marked in green, our goal is to synthesize a dense  grid of surrounding views, with field-of-views marked in black
 The u dimension represents the center-of-projection of each vir- tual viewpoint, and the x axis represents the optical conjugate of the sensor plane
Right: Given an input image, which is a ND slice  of the ND flatland light field (ND slice of the full ND light field), our  goal is to synthesize the entire light field
In our light field parameterization, vertical lines correspond to points in focus, and lines at  a slope of NN degrees correspond to points at the farthest distance  that is within the depth of field of each sub-aperture image
 We take inspiration from the geometry-based view synthesis algorithms discussed above, and also predict geometry to warp an input view to novel views
However, unlike  previous methods, we synthesize an entire ND light field  from just a single image
Furthermore, we synthesize all  views and corresponding depths at once, as opposed to the  typical strategy of predicting a single ND view at a time, and  leverage this to produce better depth estimations
 ND Representation Inference from a Single Image Instead of synthesizing new imagery, many excellent works  address the general inverse rendering problem of inferring  the scene properties that produce an observed ND image
 The influential algorithm of Barron and Malik [N] solves an  optimization problem with priors on reflectance, shape, and  illumination to infer these from a single image
Other interesting works [N, NN] focus on inferring just the ND structure  of the scene, and train on ground-truth geometry captured  with ND scanners or the Microsoft Kinect
A number of exciting works extend this idea to infer a ND voxel [N, NN, NN]  or point set [N] representation from a synthetic ND image  by training CNNs on large databases of ND CAD models
 Finally, recent methods [NN, N0, NN] learn to infer ND voxel  grids from a ND image without any ND supervision by using  a rendering or projection layer within the network and minimizing the error of the rendered view
Our work is closely  related to unsupervised ND representation learning methods, but we represent geometry as ND ray depths instead of  voxels, and train on real light fields instead of views from  synthetic ND models of single objects
 N
Light Field Synthesis  Given an image from a single viewpoint, our goal is to  synthesize views from a densely-sampled grid around the  input view
This is equivalent to synthesizing a ND light  field, given a central ND slice of the light field, and both of  these interpretations are visualized in Fig
N
We do this by  learning to approximate a function f :  L̂(x,u) = f(L(x,0)) (N)  where L̂ is the predicted light field, x is spatial coordinate (x, y), u is angular coordinate (u, v), and L(x,u) is the ground-truth light field, with input central view L(x,0)
 Light field synthesis is severely ill-posed, but certain redundancies in the light field as well as prior knowledge of  scene statistics enable us to infer other slices of the light  field from just a single ND slice
Figure N illustrates that  scene points at a specific depth lie along lines with corresponding slopes in the light field
Furthermore, the colors  along these lines are constant for Lambertian reflectance,  and only change due to occlusions or non-Lambertian reflectance effects
 We factorize the problem of light field synthesis into  the subproblems of estimating the depth at each coordinate  (x,u) in the light field, rendering a Lambertian approxi- mation of the light field using the input image and these  estimated depths, and finally predicting occluded rays and  non-Lambertian effects
This amounts to factorizing the  function f in Eq
N into a composition of N functions: d to estimate ray depths, r to render the approximate light field from the depths and central ND slice, and o to predict occluded rays and non-Lambertian effects from the approximate light field and predicted depths:  D(x,u) =d(L(x,0))  Lr(x,u) =r(L(x,0), D(x,u))  L̂(x,u) =o(Lr(x,u), D(x,u))  (N)  where D(x,u) represents predicted ray depths, and Lr rep- resents the rendered Lambertian approximate light field
 This factorization lets the network learn to estimate scene  depths from a single image in an unsupervised manner
 The rendering function r (Sec
N.N) is physically-based, while the depth estimation function d (Sec
N) and occlu- sion prediction function o (Sec
N.N) are both structured as CNNs, due to their state-of-the-art performance across  many function approximation problems in computer vision
 The CNN parameters are learned end-to-end by minimizing the sum of the reconstruction error of the Lambertian  approximate light field, the reconstruction error of the predicted light field, and regularization losses for the predicted  depths, for all training tuples:  min θd,θo  ∑  S  [  ||Lr − L||N + ||L̂− L||N  +λcψc(D) + λtvψtv(D) ]  (N)  where θd and θo are the parameters for the depth estimation and occlusion prediction networks
ψc and ψtv are consis- tency and total variation regularization losses for the predicted ray depths, discussed below in Sec
N
S is the set of  NNNN    all training tuples, each consisting of an input central view  L(x,0) and ground truth light field L(x,u)
We include the reconstruction errors for both the Lambertian light field and the predicted light field in our loss to  prevent the occlusion prediction network from attempting to  learn the full light field prediction function by itself, which  would prevent the depth estimation network from properly  learning a depth estimation function
 N
Light Field Dataset  To train our model, we collected NNNN light fields of  flowers and plants with the Lytro Illum camera, randomly  split into NNNN for training and N00 for testing
We captured  all light fields using a focal length of N0 mm and f/N aper- ture
Other camera parameters including the shutter speed,  ISO, and white balance were set automatically by the camera
We decoded the sensor data from the Illum camera using the Lytro Power Tools Beta decoder, which demosaics  the color sensor pattern and calibrates the lenslet locations
 Each light field has NNNxNNN spatial samples, and NNxNN  angular samples
Many of the corner angular samples lie  outside the camera’s aperture, so we used an NxN grid of  angular samples in our experiments, corresponding to the  angular samples that lie fully within the aperture
 This dataset includes light fields of several varieties of  roses, poppies, thistles, orchids, lillies, irises, and other  plants, all of which contain complex occlusions
Furthermore, these light fields were captured in various locations  and times of day with different natural lighting conditions
 Figure N illustrates the diversity of our dataset, and the geometric complexity in our dataset can be visualized in the  epipolar slices
To quantify the geometric diversity of our  dataset, we compute a histogram of the disparities across  the full aperture using our trained depth estimation network,  since we do not have ground truth depths
The left peak  of this histogram corresponds to background points, which  have large negative disparities, and the right peak of the  histogram corresponds to the photograph subjects (typically  flowers) which are in focus and have small disparities
 We hope this dataset will be useful for future investigations into various problems including light field synthesis,  single view synthesis, and unsupervised geometry learning
 N
Synthesizing ND Ray Depths  We learn the function d to predict depths by minimiz- ing the reconstruction error of the rendered Lambertian light  field, along with our novel depth regularization
 Two prominent errors arise when learning to predict  depth maps by minimizing the reconstruction error of synthesized views, and we visualize these in Fig
N
In textureless regions, the depth can be incorrect and depth-based  warping will still synthesize the correct image
Therefore,  the minimization in Eq
N has no incentive to predict the  correct depth
Second, depths for scene points that are occluded from the input view are also typically incorrect, because predicting the correct depth would cause the synthesized view to sample pixels from the occluder
 Incorrect depths are fine if we only care about the synthesized views
However, the quality of these depths must  be improved to consider light field synthesis as an unsupervised learning algorithm to infer depth from a single ND image
It is difficult to capture large datasets of ground-truth  depths for real scenes, especially outdoors, while it is much  easier to use capture scenes with a plenoptic camera
We  believe that light field synthesis is a promising way to train  algorithms to estimate depths from a single image, and we  present a strategy to address these depth errors
 We predict depths for every ray in the light field, which is  equivalent to predicting a depth map for each view
This enables us to introduce a novel regularization that encourages  the predicted depths to be consistent across views and accounts for occlusions, which is a light field generalization of  the left-right consistency used in methods such as [NN, NN]
 Essentially, depths should be consistent for rays coming  from the same scene points, which means that the ray depths  should be consistent along lines with the same slope:  D(x,u) = D(x+ kD(x,u),u− k) (N)  for any continuous value of k, as illustrated in Fig
N
 To regularize the predicted depth maps, we minimize the  LN norm of finite-difference gradients along these sheared lines by setting k = N, which both encourages the predicted depths to be consistent across views and encourages occluders to be sparse:  ψc(D(x,u)) = ||D(x,u)−D(x+D(x,u),u− N)||N (N)  where ψc is the consistency regularization loss for predicted ray depths D(x,u)
 Benefits of this regularization are demonstrated in Fig
N
 It encourages consistent depths in texture-less areas as well  as for rays occluded from the input view, because predicting  the incorrect depths would result in higher gradients along  sheared lines as well as new edges in the ray depths
 We additionally use total variation regularization in the  spatial dimensions for the predicted depth maps, to encourage them to be sparse in the spatial gradient domain:  ψtv(D(x,u)) = ||∇xD(x,u)||N (N)  Depth Estimation Network We model the function d to estimate ND ray depths from the input view as a CNN
We  use dilated convolutions [NN], which allow the receptive  NNNN    Figure N
We introduce the largest available light field dataset, containing NNNN light fields of scenes of flowers and plants captured with  the Lytro Illum camera in various locations and lighting settings
These light fields contain complex occlusions and wide ranges of relative  depths, as visualized in the example epipolar slices
No ground truth depths are available, so we use our algorithm to predict a histogram  of disparities in the dataset to demonstrate the rich depth complexity in our dataset
We will make this dataset available upon publication
 Figure N
Top: In a Lambertian approximation of the light field, the  color of a scene point is constant along the line corresponding to  its depth
Given estimated disparities D(x, u) and a central view L(x, 0), we can render the flatland light field as L(x, u) = L(x+ uD(x, u), 0) (D(x, u) is negative in this example)
In white, we illustrate two prominent problems that arise when estimating depth  by minimizing the reconstruction error of novel views
It is difficult to estimate the correct depth for points occluded from the input view, because warping the input view using the correct depth  does not properly reconstruct the novel views
Additionally, it is  difficult to estimate the correct depth in texture-less regions, because many possible depths result in the same synthesized novel  views
Bottom: Analogous to the Lambertian color consistency,  rays from the same scene point should have the same depth
This  can be represented as D(x, u) = D(x+kD(x, u), u−k) for any continuous value of k
We visualize ray depths using a colormap where darker colors correspond to further objects
 field of the network to increase exponentially as a function of the network depth
Hence, each of the predicted  ray depths has access to the entire input image without the  resolution loss caused by spatial downsampling or pooling
 Every convolution layer except for the final layer consists  of a NxN filter, followed by batch normalization [NN] and an  exponential linear unit activation function (ELU) [N]
The  last layer is followed by a scaled tanh activation function  Figure N
Our proposed phyiscally-based depth consistency regularization produces higher-quality estimated depths
Here, we  visualize example sub-aperture depth maps where our novel regularization improves the estimated depths for texture-less regions
 Blue arrows indicate incorrect depths and depths that are inconsistent across views, as shown in the epipolar slices
 instead of an ELU to constrain the possible disparities to  [−NN, NN] pixels
Please refer to our supplementary mate- rial for a more detailed network architecture description
 N
Synthesizing the ND Light Field  N.N
Lambertian Light Field Rendering  We render an approximate Lambertian light field by using the predicted depths to warp the input view as:  Lr(x,u) = L(x+ uD(x,u),0) (N)  where D(x,u) is the predicted depth for each ray in the light field
Figure N illustrates this relationship
 This formulation amounts to using the predicted depths  for each ray to render the ND light field by sampling the input central view image
Since our depth regularization encourages the ray depths to be consistent across views, this  effectively encourages different views of the same scene  point to sample the same pixel in the input view, resulting  in a Lambertian approximation to the light field
 N.N
Occlusions and Non-Lambertian Effects  Although predicting a depth for each ray, combined with  our depth regularization, allows the network to learn to  NNNN    model occlusions, the Lambertian light fields rendered using these depths are not able to correctly synthesize the values of rays that are occluded from the input view, as demonstrated in Fig
N
Furthermore, this depth-based rendering is  not able to accurately predict non-Lambertian effects
 We model the function o to predict occluded rays and non-Lambertian effects as a residual block [NN]:  o(Lr(x,u), D(x,u)) = õ(Lr(x,u), D(x,u)) + Lr(x,u) (N)  where õ is modeled as a ND CNN
We stack all sub-aperture images along one dimension and use a ND CNN so each filter has access to every ND view
This ND CNN predicts  a residual that, when added to the approximate Lambertian light field, best predicts the training example true light  fields
Structuring this network as a residual block ensures  that decreases in the loss are driven by correctly predicting  occluded rays and non-Lambertian effects
Additionally, by  providing the predicted depths, this network has the information necessary to understand which rays in the approximate light field are incorrect due to occlusions
Figure N  quantitatively demonstrates that this network improves the  reconstruction error of the synthesized light fields
 We simply concatenate the estimated depths to the Lambertian approximate light field as the input to a ND CNN  that contains N layers of ND convolutions with NxNxN filters  (height x width x color channels), batch normalization, and  ELU activation functions
The last convolutional layer is  followed by a tanh activation function instead of an ELU,  to constrain the values in the predicted light field to [−N, N]
Please refer to our supplementary material for a more detailed network architecture description
 N.N
Training  We generate training examples by randomly selecting  NNNxNNNxNxN crops from the training light fields, and spatially downsampling them to NNxNNxNxN
We use bilinear interpolation to sample the input view for the Lambertian depth-based rendering, so our network is fully differentiable
We train our network end-to-end using the first-order  Adam optimization algorithm [N0] with default parameters  βN = 0.N, βN = 0.NNN, ǫ = Ne−0N, a learning rate of 0.00N, a minibatch size of N examples, and depth regularization parameters λc = 0.00N and λtv = 0.0N
 N
Results  We validate our light field synthesis algorithm using our  testing dataset, and demonstrate that we are able to synthesize compelling ND ray depths and light fields with complex  occlusions and relative depths
It is difficult to fully appreciate ND light fields in a paper format, so we request readers  to view our supplementary video for animations that fully  convey the quality of our synthesized light fields
No other  Figure N
We validate our ray depths against the state-of-the-art  light field depth estimation
We give Jeon et al
[NN] a distinct advantage by providing them a ground-truth ND light field to predict  ND depths, while we use a single ND image to predict ND depths
 Our estimated depths are comprable, and in some cases superior,  to their estimated depths, as shown by the detailed varying depths  of the flower petals, leaves, and fine stem structures
 methods have attempted to synthesize a full ND light field  or ND ray depths from a single ND image, so we separately  compare our estimated depths to a state-of-the-art light field  depth estimation algorithm and our synthesized light fields  to a state-of-the-art view synthesis method
 Depth Evaluation We compare our predicted depths to  Jeon et al
[NN], which is a physically-based non-learning  depth estimation technique
Note that their algorithm uses  the entire ground-truth light field to estimate a ND depth  map, while our algorithm estimates ND ray depths from a  single ND image
Figure N qualitatively demonstrates that  our unsupervised depth estimation algorithm produces results that are comprable to Jeon et al., and even more detailed in many cases
 Synthesized Light Field Evaluation We compare our  synthesized light fields to the alternative of using the appearance flow method [NN], a state-of-the-art view synthesis  method that predicts a flow field to warp an input image to  NNNN    Figure N
We compare our synthesized light fields to the appearance flow method [NN]
Qualitatively, appearance flow has difficulties correctly predicting rays occluded from the input view, resulting in artifacts around the edges of the flowers
These types  of edge artifacts are highly objectionable perceptually, and the  improvement provided by our algorithm subjectively exceeds the  quantitative improvement given in Fig
N
 an image from a novel viewpoint
Other recent view synthesis methods are designed for predicting a held-out image  from a stereo pair, so it is unclear how to adapt them to predict a ND light field
On the other hand, it is straightforward  to adapt the appearance flow method to synthesize a full  ND light field by modifying our depth estimation network  to instead predict x and y flow fields to synthesize each subaperture image from the input view
We train this network  on our training dataset
While appearance flow can be used  to synthesize a light field, it does not produce any explicit  geometry representation, so unlike our method, appearance  flow cannot be used as a strategy for unsupervised geometry  learning from light fields
 Figure N illustrates that appearance flow has trouble synthesizing rays occluded from the input view, resulting in  artifacts around occlusion boundaries
Our method is able  to synthesize plausible occluded rays and generate convincing light fields
Intuitively, the correct strategy to flow observed rays into occluded regions will change dramatically  for flowers with different colors and shapes, so it is difficult to learn
Our approach separates the problems of depth  prediction and occluded ray prediction, so the depth prediction network can focus on estimating depth correctly without needing to correctly predict all occluded rays
 To quantitatively evaluate our method, we display histograms for the mean LN error on our test dataset for our predicted light fields, our Lambertian light fields, and the  appearance flow light fields in Fig
N
We calculate this error over the outermost generated views, since these are the  most difficult to synthesize from a central input view
Our  predicted light fields have the lowest mean error, and both  our predicted and Lambertian approximate light fields have  a lower mean error than the appearance flow light fields
 We also plot the mean LN error as a function of the view position in u, and show that while all methods are best at synthesizing views close to the input view ((u, v) = 0),  Figure N
To quantitatively validate our results, we visualize histograms of the LN errors on the testing dataset for the outermost views of our predicted light fields L̂, our Lambertian light fields Lr , and the light fields predicted by appearance flow
Our pre- dicted light fields and Lambertian light fields both have lower errors than those of appearance flow
We also compute the mean LN errors as a function of view position u, and demonstrate that our algorithm consistently outperforms appearance flow
 both our predicted and Lambertian light fields consistently  outperform the light fields generated by appearance flow
 We also tested a CNN that directly regresses from an input  image to an output light field, and found that our model outperforms this network with a mean LN error of 0.0NN versus 0.0NN across all views
Please refer to our supplementary  material for more quantitative evaluation
 Encouragingly, our single view light field synthesis  method performs only slightly worse than the light field interpolation method of [NN] that takes N corner views as input, with a mean LN error of 0.0NNN compared to 0.0NNN for  a subset of output views not input to either method
 Figure N displays example light fields synthesized by our  method, and demonstrates that we can use our synthesized  light fields for photographic effects
Our algorithm is able to  predict convincing light fields with complex occlusions and  depth ranges, as visualized in the epipolar slices
Furthermore, we can produce realistic photography effects, including extending the aperture from f/NN (aperture of the input view) to f/N.N for synthetic defocus blur, and refocusing the full-aperture image from the flower to the background
 Finally, we note that inference is fast, and it takes under  N second to synthesize a NNNxNN0xNxN light field and ray  depths on a machine with a single Titan X GPU
 Generalization Figure N0 demonstrates our method’s  ability to generalize to input images from a cell phone camera
We show that we can generate convincing ray depths, a  high-quality synthesized light field, and interesting photography effects from an image taken with an iPhone Ns
 Finally, we investigate our framework’s ability to generalize to other scene classes by collecting a second dataset,  consisting of NNNN light fields of various types of toys including cars, figurines, stuffed animals, and puzzles
Figure NN displays an example result from the test set of toys
 Although our performance on toys is quantitatively similar  to our performance on flowers (the mean LN error on the test dataset over all views is 0.0NN for toys and 0.0NN for  NNNN    Figure N
We visualize our synthesized light fields as a corner view  crop, along with several epipolar slice crops
The epipolar slices  demonstrate that our synthesized light fields contain complex occlusions and relative depths
We additionally demonstrate that our  light field generated from a single ND image can be used for synthetic defocus blur, increasing the aperture from f/NN to f/N.N
Moreover, we can use our light fields to convincingly refocus the  full-aperture image from the flowers to the background
 flowers), we note that the toys results are perceptually not  quite as impressive
The class of toys is much more diverse  than that of flowers, and this suggests that a larger and more  diverse dataset would be useful for this scene category
 N
Conclusion  We have shown that consumer light field cameras enable  the practical capture of datasets large enough for training  machine learning algorithms to synthesize local light fields  of specific scenes from single photographs
It is viable to  extend this approach to other niches, as we demonstrate  with toys, but it is an open problem to generalize this to  the full diversity of everyday scenes
We believe that our  work opens up two exciting avenues for future exploration
 First, light field synthesis is an exciting strategy for unsupervised geometry estimation from a single image, and we  hope that our dataset and algorithm enable future progress  Figure N0
Our pipeline applied to cell phone photographs
We  demonstrate that our network can generalize to synthesize light  fields from pictures taken with an iPhone Ns
We synthesize realistic depth variations and occlusions, as shown in the epipolar  slices
Furthermore, we can synthetically increase the iPhone aperture size and refocus the full-aperture image
 Figure NN
We demonstrate that our approach can generalize to  scenes of toys, and we display an example test set result
 in this area
In particular, the notion of enforcing consistent geometry for rays that intersect the same scene point  can be used for geometry representations other than ray  depths, including voxels, point clouds, and meshes
Second, synthesizing dense light fields is important for capturing VR/AR content, and we believe that this work enables  future progress towards generating immersive VR/AR content from sparsely-sampled images
 Acknowledgments This work was supported in part by  ONR grants N000NNNNN0NN and N000NNNNNNNNN, NSF  grant NNNNNNN, NSF fellowship DGE NN0NN00, a Google  Research Award, the UC San Diego Center for Visual Computing, and a generous GPU donation from NVIDIA
 NNN0    References  [N] J
T
Barron and J
Malik
Shape, illumination, and reflectance from shading
In IEEE Transactions on Pattern  Analysis and Machine Intelligence, N0NN
N  [N] C
Buehler, M
Bosse, L
McMillan, S
Gortler, and M
Cohen
Unstructured lumigraph rendering
In SIGGRAPH,  N00N
N  [N] A
X
Chang, T
Funkhouser, L
Guibas, P
Hanrahan,  Q
Huang, Z
Li, S
Savarese, M
Savva, S
Song, H
Su,  J
Xiao, L
Yi, and F
Yu
ShapeNet: An Information-Rich  ND Model Repository
In arXiv:NNNN.0N0NN, N0NN
N  [N] G
Chaurasia, S
Duchêne, O
Sorkine-Hornung, and  G
Drettakis
Depth synthesis and local warps for plausible  image-based navigation
In ACM Transactions on Graphics,  N0NN
N  [N] C
B
Choy, D
Xu, J
Gwak, K
Chen, and S
Savarese
NDRNNN: A unified approach for single and multi-view ND object reconstruction
In ECCV, N0NN
N  [N] D.-A
Clevert, T
Unterthiner, and S
Hochreiter
Fast and  accurate deep network learning by exponential linear units  (ELUs)
In ICLR, N0NN
N  [N] P
Didyk, P
Sitthi-Amorn, W
T
Freeman, F
Durand, and  W
Matusik
Joint view expansion and filtering for automultiscopic ND displays
In ACM Transactions on Graphics,  N0NN
N  [N] D
Eigen and R
Fergus
Predicting depth, surface normals  and semantic labels with a common multi-scale convolutional architecture
In ICCV, N0NN
N  [N] H
Fan, H
Su, and L
Guibas
A point set generation network  for ND object reconstruction from a single image
In CVPR,  N0NN
N  [N0] J
Flynn, I
Neulander, J
Philbin, and N
Snavely
DeepStereo: learning to predict new views from the world’s imagery
In CVPR, N0NN
N  [NN] R
Garg, V
Kumar BG, G
Carneiro, and I
Reid
Unsupervised CNN for single view depth estimation: geometry to the  rescue
In ECCV, N0NN
N  [NN] R
Girdhar, D
F
Fouhey, M
Rodriguez, and A
Gupta
 Learning a predictable and generative vector representation  for objects
In ECCV, N0NN
N  [NN] C
Godard, O
M
Aodha, and G
J
Brostow
Unsupervised  monocular depth estimation with left-right consistency
In  CVPR, N0NN
N, N  [NN] M
Goesele, J
Ackermann, S
Fuhrmann, C
Haubold,  R
Klowsky, D
Steedly, and R
Szeliski
Ambient point  clouds for view interpolation
In ACM Transactions on  Graphics, N0N0
N  [NN] S
J
Gortler, R
Grzeszczuk, R
Szeliski, and M
Cohen
The  lumigraph
In SIGGRAPH, NNNN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N  [NN] S
Ioffe and C
Szegedy
Batch normalization: accelerating  deep network training by reducing internal covariate shift
In  Journal of Machine Learning Research, N0NN
N  [NN] H.-G
Jeon, J
Park, G
Choe, J
Park, Y
Bok, Y.-W
Tai, and  I
S
Kweon
Accurate depth map estimation from a lenslet  light field camera
In CVPR, N0NN
N, N  [NN] N
K
Kalantari, T.-C
Wang, and R
Ramamoorthi
 Learning-based view synthesis for light field cameras
In  ACM Transactions on Graphics, N0NN
N, N  [N0] D
Kingma and J
Ba
Adam: a method for stochastic optimization
In ICLR, N0NN
N  [NN] M
Levoy and P
Hanrahan
Light field rendering
In SIGGRAPH, NNNN
N  [NN] G
Lippmann
La photographie intégrale
In ComptesRendus, Académie des Sciences, NN0N
N  [NN] R
Ng and P
Hanrahan
Digital correction of lens aberrations  in light field photography
In SPIE International Optical Design, N00N
N  [NN] R
Ng, M
Levoy, M
Bredif, G
Duval, M
Horowitz, and  P
Hanrahan
Light field photographhy with a hand-held  plenoptic camera
CSTR N00N-0N, N00N
N  [NN] D
J
Rezende, S
M
A
Eslami, S
Mohamed, P
Battaglia,  M
Jaderberg, and N
Heess
Unsupervised learning of ND  structure from images
In NIPS, N0NN
N  [NN] A
Saxena, M
Sun, and A
Y
Ng
MakeND: learning N-D  scene structure from a single still image
In IEEE Transactions on Pattern Analysis and Machine Intelligence, N00N
N  [NN] L
Shi, H
Hassanieh, A
Davis, D
Katabi, and F
Durand
 Light field reconstruction using sparsity in the continuous  Fourier domain
In ACM Transactions on Graphics, N0NN
N  [NN] P
P
Srinivasan, M
W
Tao, R
Ng, and R
Ramamoorthi
 Oriented light-field windows for scene flow
In ICCV, N0NN
 N  [NN] M
Tatarchenko, A
Dosovitskiy, and T
Brox
Multi-view  ND models from single images wih a convolutional network
 In ECCV, N0NN
N  [N0] S
Tulsiani, T
Zhou, A
A
Efros, and J
Malik
Multi-view  supervision for single-view reconstruction via differentiable  ray consistency
In CVPR, N0NN
N  [NN] J
Wu, C
Zhang, T
Xue, W
T
Freeman, and J
B
Tenenbaum
Learning a probabilistic latent space of object shapes  via ND generative-adversarial modeling
In NIPS, N0NN
N  [NN] J
Xie, R
Girshick, and A
Farhadi
DeepND: fully automatic  ND-to-ND video conversion with deep convolutional neural  networks
In ECCV, N0NN
N  [NN] X
Yan, J
Yang, E
Yumer, Y
Guo, and H
Lee
Perspective  transformer nets: learning single-view ND object reconstruction without ND supervision
In NIPS, N0NN
N  [NN] J
Yang, S
E
Reed, M.-H
Yang, and H
Lee
Weaklysupervised disentangling with recurrent transformations for  ND view synthesis
In NIPS, N0NN
N  [NN] F
Yu and V
Koltun
Multi-scale context aggregation by dilated convolutions
In ICLR, N0NN
N  [NN] Z
Zhang, Y
Liu, and Q
Dai
Light field from micro-baseline  image pair
In CVPR, N0NN
N  [NN] T
Zhou, S
Tulsiani, W
Sun, J
Malik, and A
A
Efros
View  synthesis by appearance flow
In ECCV, N0NN
N, N, N  [NN] C
L
Zitnick, S
B
Kang, M
Uyttendaele, S
Winder, and  R
Szeliski
High-quality video view interpolation using a  layered representation
In ACM Transactions on Graphics,  N00N
N  NNNNUnpaired Image-To-Image Translation Using Cycle-Consistent Adversarial Networks   Unpaired Image-to-Image Translation  using Cycle-Consistent Adversarial Networks  Jun-Yan Zhu∗ Taesung Park∗ Phillip Isola Alexei A
Efros Berkeley AI Research (BAIR) laboratory, UC Berkeley  Zebras Horses  horse        zebra  zebra        horse  Summer Winter  summer        winter  winter        summer  Photograph Van Gogh CezanneMonet Ukiyo-e  Monet        Photos  Monet        photo  photo       Monet  Figure N: Given any two unordered image collections X and Y , our algorithm learns to automatically “translate” an image from one into the other and vice  versa
Example application (bottom): using a collection of paintings of a famous artist, learn to render a user’s photograph into their style
 Abstract  Image-to-image translation is a class of vision and graphics problems where the goal is to learn the mapping between  an input image and an output image using a training set of  aligned image pairs
However, for many tasks, paired training data will not be available
We present an approach for  learning to translate an image from a source domain X to a  target domain Y in the absence of paired examples
Our goal  is to learn a mapping G : X → Y such that the distribution of images from G(X) is indistinguishable from the distribu- tion Y using an adversarial loss
Because this mapping is  highly under-constrained, we couple it with an inverse mapping F : Y → X and introduce a cycle consistency loss to push F (G(X)) ≈ X (and vice versa)
Qualitative results are presented on several tasks where paired training data does  not exist, including collection style transfer, object transfiguration, season transfer, photo enhancement, etc
Quantitative  comparisons against several prior methods demonstrate the  superiority of our approach
 N
Introduction  What did Claude Monet see as he placed his easel by the  bank of the Seine near Argenteuil on a lovely spring day in  NNNN (Figure N, top-left)? A color photograph, had it been  invented, may have documented a crisp blue sky and a glassy  river reflecting it
Monet conveyed his impression of this same  scene through wispy brush strokes and a bright palette
What  if Monet had happened upon the little harbor in Cassis on a  cool summer evening (Figure N, bottom-left)? A brief stroll  through a gallery of Monet paintings makes it easy to imagine  how he would have rendered the scene: perhaps in pastel  shades, with abrupt dabs of paint, and a somewhat flattened  dynamic range
 We can imagine all this despite never having seen a side by  side example of a Monet painting next to a photo of the scene  he painted
Instead we have knowledge of the set of Monet  paintings and of the set of landscape photographs
We can  reason about the stylistic differences between these two sets,  and thereby imagine what a scene might look like if we were  to “translate” it from one set into the other
 * indicates equal contribution  NNNNN    ( )  ⋯  ,  ( )  ⋯  Paired Unpaired  n o  ,  n o  ,  n o  ,  ⋯  X Yxi yi  Figure N: Paired training data (left) consists of training examples  {xi, yi} N i=N, where the yi that corresponds to each xi is given [N0]
We  instead consider unpaired training data (right), consisting of a source set  {xi} N i=N ∈ X and a target set {yj}  M j=N ∈ Y , with no information provided  as to which xi matches which yj 
 In this paper, we present a system that can learn to do the  same: capturing special characteristics of one image collection  and figuring out how these characteristics could be translated  into the other image collection, all in the absence of any paired  training examples
 This problem can be more broadly described as image-toimage translation [N0], converting an image from one representation of a given scene, x, to another, y, e.g., grayscale  to color, image to semantic labels, edge-map to photograph
 Years of research in computer vision, image processing, and  graphics have produced powerful translation systems in the supervised setting, where example image pairs {x, y} are avail- able (Figure N, left), e.g., [N, NN, N0, NN, NN, NN, NN, NN, NN, NN]
 However, obtaining paired training data can be difficult and  expensive
For example, only a couple of datasets exist for  tasks like semantic segmentation (e.g., [N]), and they are relatively small
Obtaining input-output pairs for graphics tasks  like artistic stylization can be even more difficult since the  desired output is highly complex, typically requiring artistic  authoring
For many tasks, like object transfiguration (e.g.,  zebra→horse, Figure N top-middle), the desired output is not even well-defined
 We therefore seek an algorithm that can learn to translate  between domains without paired input-output examples (Figure N, right)
We assume there is some underlying relationship  between the domains – for example, that they are two different  renderings of the same underlying world – and seek to learn  that relationship
Although we lack supervision in the form  of paired examples, we can exploit supervision at the level of  sets: we are given one set of images in domain X and a different set in domain Y 
We may train a mapping G : X → Y such that the output ŷ = G(x), x ∈ X , is indistinguishable from images y ∈ Y by an adversary trained to classify ŷ apart from y
In theory, this objective can induce an output distribution over ŷ that matches the empirical distribution pY (y) (in general, this requires that G be stochastic) [NN]
The optimal  G thereby translates the domain X to a domain Ŷ distributed  identically to Y 
However, such a translation does not guarantee that the individual inputs and outputs x and y are paired  up in a meaningful way – there are infinitely many mappings  G that will induce the same distribution over ŷ
Moreover, in  practice, we have found it difficult to optimize the adversarial  objective in isolation: standard procedures often lead to the  well-known problem of mode collapse, where all input images  map to the same output image and the optimization fails to  make progress [NN]
 These issues call for adding more structure to our objective
 Therefore, we exploit the property that translation should be  “cycle consistent”, in the sense that if we translate, e.g., a  sentence from English to French, and then translate it back  from French to English, we should arrive back at the original  sentence [N]
Mathematically, if we have a translator G : X → Y and another translator F : Y → X , then G and F should be inverses of each other, and both mappings should  be bijections
We apply this structural assumption by training  both the mapping G and F simultaneously, and adding a  cycle consistency loss [N0] that encourages F (G(x)) ≈ x and G(F (y)) ≈ y
Combining this loss with adversarial losses on domains X and Y yields our full objective for unpaired  image-to-image translation
 We apply our method to a wide range of applications, including style transfer, object transfiguration, attribute transfer  and photo enhancement
We also compare against previous  approaches that rely either on hand-defined factorizations of  style and content, or on shared embedding functions, and show  that our method outperforms these baselines
Our code is available at https://github.com/junyanz/CycleGAN
 Check out the full version of the paper at https://arxiv
 org/abs/NN0N.N0NNN
 N
Related work  Generative Adversarial Networks (GANs) [NN, NN] have  achieved impressive results in image generation [N, NN], image  editing [NN], and representation learning [NN, NN, NN]
Recent  methods adopt the same idea for conditional image generation  applications, such as textNimage [NN], image inpainting [NN],  and future prediction [NN], as well as to other domains like  videos [N0] and ND models [NN]
The key to GANs’ success is  the idea of an adversarial loss that forces the generated images  to be, in principle, indistinguishable from real images
This  is particularly powerful for image generation tasks, as this is  exactly the objective that much of computer graphics aims to  optimize
We adopt an adversarial loss to learn the mapping  such that the translated image cannot be distinguished from  images in the target domain
 Image-to-Image Translation The idea of image-to-image  translation goes back at least to Hertzmann et al.’s Image Analogies [NN], who employ a nonparametric texture  model [N] on a single input-output training image pair
More  NNNN  https://github.com/junyanz/CycleGAN https://arxiv.org/abs/NN0N.N0NNN https://arxiv.org/abs/NN0N.N0NNN   X Y  G  F  DYDX  G  F  Ŷ  X Y (  X Y (  G  F  X̂  (a) (b) (c)  cycle-consistency loss  cycle-consistency loss  DY DX  ŷx̂x y  Figure N: (a) Our model contains two mapping functions G : X → Y and F : Y → X , and associated adversarial discriminators DY and DX 
DY encourages G to translate X into outputs indistinguishable from domain Y , and vice versa for DX , F , and X 
To further regularize the mappings, we  introduce two “cycle consistency losses” that capture the intuition that if we translate from one domain to the other and back again we should arrive where we  started: (b) forward cycle-consistency loss: x → G(x) → F (G(x)) ≈ x, and (c) backward cycle-consistency loss: y → F (y) → G(F (y)) ≈ y  recent approaches use a dataset of input-output examples to  learn a parametric translation function using CNNs, e.g
[NN]
 Our approach builds on the “pixNpix” framework of Isola et  al
[N0], which uses a conditional generative adversarial network [NN] to learn a mapping from input to output images
 Similar ideas have been applied to various tasks such as generating photographs from sketches [N0] or from attribute and  semantic layouts [NN]
However, unlike these prior works, we  learn the mapping without paired training examples
 Unpaired Image-to-Image Translation Several other  methods also tackle the unpaired setting, where the goal is to  relate two data domains, X and Y 
Rosales et al
[NN] propose  a Bayesian framework that includes a prior based on a patchbased Markov random field computed from a source image,  and a likelihood term obtained from multiple style images
 More recently, CoupledGANs [NN] and cross-modal scene  networks [N] use a weight-sharing strategy to learn a common  representation across domains
Concurrent to our method,  Liu et al
[NN] extends this framework with a combination  of variational autoencoders [NN] and generative adversarial  networks
Another line of concurrent work [NN, NN, N] encourages the input and output to share certain “content” features  even though they may differ in “style“
They also use adversarial networks, with additional terms to enforce the output  to be close to the input in a predefined metric space, such  as class label space [N], image pixel space [NN], and image  feature space [NN]
 Unlike the above approaches, our formulation does not rely  on any task-specific, predefined similarity function between  the input and output, nor do we assume that the input and output have to lie in the same low-dimensional embedding space
 This makes our method a general-purpose solution for many  vision and graphics tasks
We directly compare against several  prior approaches in Section N.N
Concurrent with our work, in  these same proceedings, Yi et al
[NN] independently introduce  a similar objective for unpaired image-to-image translation,  inspired by dual learning in machine translation [NN]
 Cycle Consistency The idea of using transitivity as a way  to regularize structured data has a long history
In visual tracking, enforcing simple forward-backward consistency has been  a standard trick for decades [NN]
In the language domain,  verifying and improving translations via “back translation and  reconsiliation” is a technique used by human translators [N]  (including, humorously, by Mark Twain [NN]), as well as  by machines [NN]
More recently, higher-order cycle consistency has been used in structure from motion [NN], ND shape  matching [NN], co-segmentation [NN], dense semantic alignment [NN, N0], and depth estimation [NN]
Of these, Zhou et  al
[N0] and Godard et al
[NN] are most similar to our work, as  they use a cycle consistency loss as a way of using transitivity  to supervise CNN training
In this work, we are introducing a  similar loss to push G and F to be consistent with each other
 Neural Style Transfer [NN, NN, NN, N0] is another way  to perform image-to-image translation, which synthesizes a  novel image by combining the content of one image with the  style of another image (typically a painting) by matching the  Gram matrix statistics of pre-trained deep features
Our main  focus, on the other hand, is learning the mapping between two  domains, rather than between two specific images, by trying  to capture correspondences between higher-level appearance  structures
Therefore, our method can be applied to other  tasks, such as painting→ photo, object transfiguration, etc
where single sample transfer methods do not perform well
 We compare these two methods in Section N.N
 N
Formulation  Our goal is to learn mapping functions between two domains X and Y given training samples {xi} N i=N ∈ X and  {yj} M j=N ∈ Y 
As illustrated in Figure N (a), our model includes two mappings G : X → Y and F : Y → X 
In addition, we introduce two adversarial discriminators DX and  DY , where DX aims to distinguish between images {x} and translated images {F (y)}; in the same way, DY aims to dis- criminate between {y} and {G(x)}
Our objective contains kinds of two terms: adversarial losses [NN] for matching the  distribution of generated images to the data distribution in  NNNN    the target domain; and a cycle consistency loss to prevent the  learned mappings G and F from contradicting each other
 N.N
Adversarial Loss  We apply adversarial losses [NN] to both mapping functions
 For the mapping function G : X → Y and its discriminator DY , we express the objective as:  LGAN(G,DY , X, Y ) =Ey∼pdata(y)[logDY (y)]  +Ex∼pdata(x)[log(N−DY (G(x))],  (N)  where G tries to generate images G(x) that look similar to im- ages from domain Y , while DY aims to distinguish between  translated samples G(x) and real samples y
We introduce a similar adversarial loss for the mapping function F : Y → X and its discriminator DX as well: i.e
LGAN(F,DX , Y,X)
 N.N
Cycle Consistency Loss  Adversarial training can, in theory, learn mappings G and  F that produce outputs identically distributed as target domains Y and X respectively (strictly speaking, this requires  G and F to be stochastic functions) [NN]
However, with large  enough capacity, a network can map the same set of input  images to any random permutation of images in the target  domain, where any of the learned mappings can induce an output distribution that matches the target distribution
To further  reduce the space of possible mapping functions, we argue that  the learned mapping functions should be cycle-consistent: as  shown in Figure N (b), for each image x from domain X , the  image translation cycle should be able to bring x back to the  original image, i.e
x → G(x) → F (G(x)) ≈ x
We call this forward cycle consistency
Similarly, as illustrated in Figure N  (c), for each image y from domain Y , G and F should also satisfy backward cycle consistency: y → F (y) → G(F (y)) ≈ y
We can incentivize this behavior using a cycle consistency  loss:  Lcyc(G,F ) =Ex∼pdata(x)[‖F (G(x))− x‖N]  +Ey∼pdata(y)[‖G(F (y))− y‖N]
(N)  In preliminary experiments, we also tried replacing the LN  norm in this loss with an adversarial loss between F (G(x)) and x, and between G(F (y)) and y, but did not observe im- proved performance
The behavior induced by the cycle consistency loss can be observed in the arXiv version
 N.N
Full Objective  Our full objective is:  L(G,F,DX , DY ) =LGAN(G,DY , X, Y )  + LGAN(F,DX , Y,X)  + λLcyc(G,F ), (N)  where λ controls the relative importance of the two objectives
 We aim to solve:  G∗, F ∗ = argmin G,F  max Dx,DY  L(G,F,DX , DY )
(N)  Notice that our model can be viewed as training two “autoencoders” [NN]: we learn one autoencoder F ◦G : X → X jointly with another G ◦ F : Y → Y 
However, these autoen- coders each have special internal structure: they map an image  to itself via an intermediate representation that is a translation of the image into another domain
Such a setup can also  be seen as a special case of “adversarial autoencoders” [N0],  which use an adversarial loss to train the bottleneck layer  of an autoencoder to match an arbitrary target distribution
 In our case, the target distribution for the X → X autoen- coder is that of domain Y 
In Section N.N.N, we compare our  method against ablations of the full objective, and empirically show that both objectives play critical roles in arriving  at high-quality results
 N
Implementation  Network Architecture We adapt the architecture for our  generative networks from Johnson et al
[NN] who have  shown impressive results for neural style transfer and superresolution
This network contains two stride-N convolutions,  several residual blocks [NN], and two NN -strided convolutions
 Similar to Johnson et al
[NN], we use instance normalization [NN]
For the discriminator networks we use N0×N0 Patch- GANs [N0, NN, NN], which aim to classify whether N0 × N0 overlapping image patches are real or fake
Such a patch-level  discriminator architecture has fewer parameters than a fullimage discriminator, and can be applied to arbitrarily-sized  images in a fully convolutional fashion [N0]
 Training details We apply two techniques from recent  works to stabilize our model training procedure
First, for  LGAN (Equation N), we replace the negative log likelihood objective by a least square loss [NN]
This loss performs more  stably during training and generates higher quality results
 Equation N then becomes:  LLSGAN(G,DY , X, Y ) =Ey∼pdata(y)[(DY (y)− N) N]  +Ex∼pdata(x)[DY (G(x)) N], (N)  Second, to reduce model oscillation [NN], we follow Shrivastava et al’s strategy [NN] and update the discriminators DX and DY using a history of generated images rather than the  ones produced by the latest generative networks
We keep an  image buffer that stores the N0 previously generated images
 Please refer to our arXiv paper for more details about the  datasets, architectures and training procedures
 NNNN    CoGANBiGANInput CycleGAN pixNpix Ground truth  Figure N: Different methods for mapping labels→photos trained on cityscapes
From left to right: input, BiGAN/ALI [N, N], CoGAN [NN],  CycleGAN (ours), pixNpix [N0] trained on paired data, and ground truth
 CoGANBiGAN CycleGAN Ground truthInput pixNpix  Figure N: Different methods for mapping aerial photos↔maps on Google Maps
From left to right: input, BiGAN/ALI [N, N], CoGAN [NN], CycleGAN  (ours), pixNpix [N0] trained on paired data, and ground truth
 N
Results  We first compare our approach against recent methods for  unpaired image-to-image translation on paired datasets where  ground truth input-output pairs are available for evaluation
 We then study the importance of both the adversarial loss  and the cycle consistency loss, and compare our full method  against several variants
Finally, we demonstrate the generality of our algorithm on a wide range of applications where  paired data does not exist
For brevity, we refer to our method  as CycleGAN
 N.N
Evaluation  Using the same evaluation datasets and metrics as  “pixNpix” [N0], we compare our method against several baselines both qualitatively and quantitatively
We also perform  ablation study on the full loss function
 N.N.N Baselines  CoGAN [NN] This method learns one GAN generator for  domain X and one for domain Y , with shared weights on the  first few layers for shared latent representation
Translation  from X to Y can be achieved by finding a latent representation that generates image X and then rendering this latent  representation into style Y 
 Pixel loss + GAN [NN] Like our method, Shrivastava et  al
[NN] uses an adversarial loss to train a translation from X  to Y 
The regularization term ‖X− Ŷ ‖N was used to penalize making large changes at pixel level
 Feature loss + GAN We also test a variant of [NN] where  the LN loss is computed over deep image features using a  pretrained network (VGG-NN reluN N [NN]), rather than over  Map → Photo Photo → Map Loss % Turkers labeled real % Turkers labeled real  CoGAN [NN] 0.N% ± 0.N% 0.N% ± 0.N% BiGAN/ALI [N, N] N.N% ± N.0% N.N% ± 0.N% Pixel loss + GAN [NN] 0.N% ± 0.N% N.N% ± N.N% Feature loss + GAN N.N% ± 0.N% 0.N% ± 0.N% CycleGAN (ours) NN.N% ± N.N% NN.N% ± N.N%  Table N: AMT “real vs fake” test on maps↔aerial photos
 Loss Per-pixel acc
Per-class acc
Class IOU  CoGAN [NN] 0.N0 0.N0 0.0N  BiGAN/ALI [N, N] 0.NN 0.0N 0.0N  Pixel loss + GAN [NN] 0.N0 0.N0 0.0N  Feature loss + GAN 0.0N 0.0N 0.0N  CycleGAN (ours) 0.NN 0.NN 0.NN  pixNpix [N0] 0.NN 0.NN 0.NN  Table N: FCN-scores for different methods, evaluated on Cityscapes  labels→photos
 Loss Per-pixel acc
Per-class acc
Class IOU  CoGAN [NN] 0.NN 0.NN 0.0N  BiGAN/ALI [N, N] 0.NN 0.NN 0.0N  Pixel loss + GAN [NN] 0.NN 0.NN 0.0N  Feature loss + GAN 0.N0 0.N0 0.0N  CycleGAN (ours) 0.NN 0.NN 0.NN  pixNpix [N0] 0.NN 0.N0 0.NN  Table N: Classification performance of photo→labels for different methods on cityscapes
 RGB pixel values
 BiGAN/ALI [N, N] Unconditional GANs [NN] learn a generator G : Z → X , that maps random noise Z to images X 
The BiGAN [N] and ALI [N] propose to also learn the inverse  mapping function F : X → Z
Though they were originally designed for mapping a latent vector z to an image x, we  implemented the same objective for mapping a source image  x to a target image y
 pixNpix [N0] We also compare against pixNpix [N0], which  is trained on paired data, to see how close we can get to this  “upper bound” without using paired data
 For fair comparison, we implement all the baselines using  the same architecture and details as our method except for  CoGAN [NN]
We use the public implementation of CoGAN  due to fundametal differences in architecture N
 N.N.N Comparison against baselines  As can be seen in Figure N and Figure N, we were unable to  achieve compelling results with any of the baselines
Our  method, on the other hand, is able to produce translations that  are often of similar quality to the fully supervised pixNpix
 We exclude pixel loss + GAN and feature loss + GAN in  the figures, as both of the methods fail to produce results at  all close to the target domain (full results can be viewed at  https://junyanz.github.io/CycleGAN/)
 In addition, our method and the baselines are quantitatively  compared in three ways
First, we run ”real vs fake” study on  Amazon Mechanical Turk (AMT) workers to assess perceptual realism [N0]
Second, we train photo→label task on the  Nhttps://github.com/mingyuliutw/CoGAN  NNNN  https://junyanz.github.io/CycleGAN/   Ground truthInput GAN aloneCycle alone GAN+forward GAN+backward CycleGAN (ours)  Figure N: Different variants of our method for mapping labels↔photos trained on cityscapes
From left to right: input, cycle-consistency loss alone, adversarial loss alone, GAN + forward cycle-consistency loss (F (G(x)) ≈ x), GAN + backward cycle-consistency loss (G(F (y)) ≈ y), CycleGAN (our full method), and ground truth
Both Cycle alone and GAN + backward fail to produce images similar to the target domain
GAN alone and GAN + forward  suffer from mode collapse, producing identical label maps regardless of the input photo
 Cityscapes dataset, and compare the output label images with  the ground truth using the standard metrics on the Cityscapes  benchmark [N]
Lastly, we train label→photo task on the same dataset and evaluate the output photos using an off-the-shelf  fully-convolutional semantic segmentation network [NN]
We  find that our method significantly outperforms the baselines  in all three experiments
Table N reports performance on the  AMT perceptual realism task
Here, we see that our method  can fool participants on around a quarter of trials, in both  the map→photo direction and the photo→map direction
All baselines almost never fooled participants
Table N and Table N assess the performance of the label↔photo task on the Cityscapes
In both cases, our method again outperforms the  baselines
Detailed procedures and results of each experiment  can be found in our arXiv version
 N.N.N Ablation Study  We compare against ablations of our full loss
Figure N  shows several qualitative examples
Removing the GAN loss  substantially degrades results, as does removing the cycleconsistency loss
We therefore conclude that both terms are  critical to our results
We also evaluate our method with  the cycle loss in only one direction: GAN+forward cycle  loss Ex∼pdata(x)[‖F (G(x))− x‖N], or GAN+backward cycle loss Ey∼pdata(y)[‖G(F (y)) − y‖N] (Equation N) and find that it often incurs training instability and causes mode collapse,  especially for the direction of the mapping that was removed
 We also quantitatively measured the ablations on Cityscapes  photos→label, whose results can be found in our arXiv ver- sion
 N.N
Applications  We demonstrate our method on several applications where  paired training data does not exist.We observe that translations  on training data are often more appealing than those on test  data, and full results of all applications on both training and  test data can be viewed on our project website
 Object transfiguration (Figure N) The model is trained to  translate one object class from Imagenet [NN] to another (each  class contains around N000 training images)
Turmukham- betov et al.[NN] proposes a subspace model to translate one  object into another object of the same category, while our  method focuses on object transfiguration between two visually similar categories
 Season transfer (Figure N) The model is trained on the  winter and summer photos of Yosemite on Flickr
 Collection style transfer (Figure N) We train the model  on landscape photographs downloaded from Flickr and  WikiArt
Note that unlike recent work on “neural style transfer” [NN], our method learns to mimic the style of an entire  set of artworks (e.g
Van Gogh), rather than transferring the  style of a single selected piece of art (e.g
Starry Night)
In  Figure N.N, we compare our results with [NN]
 Photo generation from paintings (Figure N) For  painting→photo, we find that it is helpful to introduce an additional loss to encourage the mapping to preserve color  composition between the input and output
In particular, we  adopt the technique of Taigman et al
[NN] and regularize  the generator to be near an identity mapping when real samples of the target domain are provided as the input to the  generator: i.e
Lidentity(G,F ) = Ey∼pdata(y)[‖G(y) − y‖N] + Ex∼pdata(x)[‖F (x)− x‖N]
 Without Lidentity, the generator G and F are free to change the tint of input images when there is no need to
For example,  when learning the mapping between Monet’s paintings and  Flickr photographs, the generator often maps paintings of  daytime to photographs taken during sunset, because such a  mapping may be equally valid under the adversarial loss and  cycle consistency loss
The effect of this identity mapping  loss can be found in our arXiv paper
 In Figure N, we show additional results translating Monet  NNNN    Input Input OutputOutput  horse → zebra zebra → horse summer Yosemite → winter Yosemite   apple → orange orange → apple winter Yosemite → summer Yosemite  Input Input OutputOutput  Figure N: Results on several translation problems
These images are relatively successful results – please see our website for more comprehensive results
 Ukiyo-eMonetInput Van Gogh Cezanne  Figure N: We transfer input images into different artistic styles
Please see our website for additional examples
 paintings to photographs
This figure shows results on paintings that were included in the training set, whereas for all  other experiments in the paper, we only evaluate and show test  set results
Because the training set does not include paired  data, coming up with a plausible translation for a training set  painting is a nontrivial task
Indeed, since Monet is no longer  able to create new paintings, generalization to unseen, “test  set”, paintings is not a pressing problem
 Photo enhancement (Figure N) We show that our method  can be used to generate photos with shallower depth of field
 We train the model on flower photos downloaded from Flickr
 The source domain consists of photos of flower taken by  smartphones, which usually have deep depth of field due to  a small aperture
The target photos were taken with DSLRs  with a larger aperture
Our model successfully generates  photos with shallower depth of field from the photos taken by  smartphones
 N
Limitations and Discussion  Although our method can achieve compelling results in  many cases, the results are far from uniformly positive
Several typical failure cases are shown in Figure NN
On translation tasks that involve color and texture changes, like many  of those reported above, the method often succeeds
We have  also explored tasks that require geometric changes, with little  success
For example, on the task of dog→cat transfigura- tion, the learned translation degenerates to making minimal  changes to the input (Figure NN)
Handling more varied and  extreme transformations, especially geometric changes, is an  NNNN    Input Output Input Output Input Output  Figure N: Results on mapping Monet paintings to photographs
Please see our website for additional examples
 Figure N0: Photo enhancement: mapping from a set of iPhone snaps to  professional DSLR photographs, the system often learns to produce shallow  focus
Here we show some of the most successful results in our test set –  average performance is considerably worse
Please see our website for more  comprehensive and random examples
 important problem for future work
 Some failure cases are caused by the distribution characteristic of the training datasets
For example, the horse → zebra task of Figure NN has completely failed, because our model  was trained on the wild horse, zebra synsets of ImageNet,  which does not contain images of a person riding horse or  zebra
 We also observe a lingering gap between the results achievable with paired training data and those achieved by our unpaired method
In some cases, this gap may be very hard – or  even impossible – to close: for example, our method sometimes permutes the labels for tree and building in the output  of the photos→labels task
To resolve this ambiguity may require some form of weak semantic supervision
Integrating  weak or semi-supervised data may lead to substantially more  powerful translators, still at a fraction of the annotation cost  of the fully-supervised systems
 Nonetheless, in many cases completely unpaired data is  plentifully available and should be made use of
This paper  pushes the boundaries of what is possible in this “unsupervised” setting
 →  →  →  → Figure NN: We compare our method with neural style transfer [NN]
Left to  right: input images, results from [NN] using single representative image as a  style image, results from [NN] using all the images from the target domain,  and CycleGAN (ours)  apple → orange  dog → cat horse → zebra Figure NN: Some failure cases of our method
 Acknowledgments We thank Aaron Hertzmann, Shiry Ginosar, Deepak Pathak, Bryan Russell, Eli Shechtman, Richard  Zhang, and Tinghui Zhou for many helpful comments
This  work was supported in part by NSF SMA-NNNNNNN, NSF IISNNNNNN0, a Google Research Award, Intel Corp, and hardware  donations from NVIDIA
JYZ is supported by the Facebook  Graduate Fellowship and TP is supported by the Samsung  Scholarship
The photographs used in style transfer were  taken by AE, mostly in France
 NNN0    References  [N] Y
Aytar, L
Castrejon, C
Vondrick, H
Pirsiavash, and  A
Torralba
Cross-modal scene networks
arXiv preprint  arXiv:NNN0.0N00N, N0NN
N  [N] K
Bousmalis, N
Silberman, D
Dohan, D
Erhan, and  D
Krishnan
Unsupervised pixel-level domain adaptation with generative adversarial networks
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
N  [N] R
W
Brislin
Back-translation for cross-cultural research
Journal of cross-cultural psychology, N(N):NNN–  NNN, NNN0
N, N  [N] M
Cordts, M
Omran, S
Ramos, T
Rehfeld, M
Enzweiler, R
Benenson, U
Franke, S
Roth, and B
Schiele
 The cityscapes dataset for semantic urban scene understanding
In CVPR, N0NN
N, N  [N] E
L
Denton, S
Chintala, R
Fergus, et al
Deep generative image models using a laplacian pyramid of adversarial networks
In NIPS, pages NNNN–NNNN, N0NN
 N  [N] J
Donahue, P
Krähenbühl, and T
Darrell
Adversarial  feature learning
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 N  [N] V
Dumoulin, I
Belghazi, B
Poole, A
Lamb, M
Arjovsky, O
Mastropietro, and A
Courville
Adversarially  learned inference
arXiv preprint arXiv:NN0N.00N0N,  N0NN
N  [N] A
A
Efros and T
K
Leung
Texture synthesis by  non-parametric sampling
In ICCV, volume N, pages  N0NN–N0NN
IEEE, NNNN
N  [N] D
Eigen and R
Fergus
Predicting depth, surface normals and semantic labels with a common multi-scale  convolutional architecture
In ICCV, pages NNN0–NNNN,  N0NN
N  [N0] L
A
Gatys, M
Bethge, A
Hertzmann, and E
Shechtman
Preserving color in neural artistic style transfer
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] L
A
Gatys, A
S
Ecker, and M
Bethge
Image style  transfer using convolutional neural networks
CVPR,  N0NN
N, N, N  [NN] C
Godard, O
Mac Aodha, and G
J
Brostow
Unsupervised monocular depth estimation with left-right  consistency
In CVPR, N0NN
N  [NN] I
Goodfellow
Nips N0NN tutorial: Generative adversarial networks
arXiv preprint arXiv:NN0N.00NN0, N0NN
N,  N  [NN] I
Goodfellow, J
Pouget-Abadie, M
Mirza, B
Xu,  D
Warde-Farley, S
Ozair, A
Courville, and Y
Bengio
Generative adversarial nets
In NIPS, N0NN
N, N, N,  N  [NN] D
He, Y
Xia, T
Qin, L
Wang, N
Yu, T
Liu, and W.-Y
 Ma
Dual learning for machine translation
In NIPS,  pages NN0–NNN, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual  learning for image recognition
In CVPR, pages NN0–  NNN, N0NN
N  [NN] A
Hertzmann, C
E
Jacobs, N
Oliver, B
Curless, and  D
H
Salesin
Image analogies
In SIGGRAPH, pages  NNN–NN0
ACM, N00N
N  [NN] G
E
Hinton and R
R
Salakhutdinov
Reducing the  dimensionality of data with neural networks
Science,  NNN(NNNN):N0N–N0N, N00N
N  [NN] Q.-X
Huang and L
Guibas
Consistent shape maps via  semidefinite programming
In Computer Graphics Forum, volume NN, pages NNN–NNN
Wiley Online Library,  N0NN
N  [N0] P
Isola, J.-Y
Zhu, T
Zhou, and A
A
Efros
Image-toimage translation with conditional adversarial networks
 In CVPR, N0NN
N, N, N, N  [NN] J
Johnson, A
Alahi, and L
Fei-Fei
Perceptual losses  for real-time style transfer and super-resolution
In  ECCV, pages NNN–NNN
Springer, N0NN
N, N, N  [NN] L
Karacan, Z
Akata, A
Erdem, and E
Erdem
Learning  to generate images of outdoor scenes from attributes  and semantic layouts
arXiv preprint arXiv:NNNN.00NNN,  N0NN
N  [NN] D
P
Kingma and M
Welling
Auto-encoding variational  bayes
ICLR, N0NN
N  [NN] P.-Y
Laffont, Z
Ren, X
Tao, C
Qian, and J
Hays
 Transient attributes for high-level understanding and  editing of outdoor scenes
ACM Transactions on Graphics (TOG), NN(N):NNN, N0NN
N  [NN] C
Ledig, L
Theis, F
Huszár, J
Caballero, A
Cunningham, A
Acosta, A
Aitken, A
Tejani, J
Totz, Z
Wang,  et al
Photo-realistic single image super-resolution using a generative adversarial network
arXiv preprint  arXiv:NN0N.0NN0N, N0NN
N  [NN] C
Li and M
Wand
Precomputed real-time texture synthesis with markovian generative adversarial networks
 ECCV, N0NN
N  [NN] M.-Y
Liu, T
Breuel, and J
Kautz
Unsupervised  image-to-image translation networks
arXiv preprint  arXiv:NN0N.00NNN, N0NN
N  [NN] M.-Y
Liu and O
Tuzel
Coupled generative adversarial  networks
In NIPS, pages NNN–NNN, N0NN
N, N  [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional networks for semantic segmentation
In CVPR,  pages NNNN–NNN0, N0NN
N, N, N  [N0] A
Makhzani, J
Shlens, N
Jaitly, I
Goodfellow, and  B
Frey
Adversarial autoencoders
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
N  [NN] X
Mao, Q
Li, H
Xie, R
Y
Lau, and Z
Wang
Multiclass generative adversarial networks with the lN loss  function
arXiv preprint arXiv:NNNN.0N0NN, N0NN
N  NNNN    [NN] M
Mathieu, C
Couprie, and Y
LeCun
Deep multiscale video prediction beyond mean square error
ICLR,  N0NN
N  [NN] M
F
Mathieu, J
Zhao, A
Ramesh, P
Sprechmann, and  Y
LeCun
Disentangling factors of variation in deep  representation using adversarial training
In NIPS, pages  N0N0–N0NN, N0NN
N  [NN] D
Pathak, P
Krahenbuhl, J
Donahue, T
Darrell, and  A
A
Efros
Context encoders: Feature learning by  inpainting
CVPR, N0NN
N  [NN] A
Radford, L
Metz, and S
Chintala
Unsupervised representation learning with deep convolutional generative  adversarial networks
arXiv preprint arXiv:NNNN.0NNNN,  N0NN
N  [NN] S
Reed, Z
Akata, X
Yan, L
Logeswaran, B
Schiele,  and H
Lee
Generative adversarial text to image synthesis
arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] R
Rosales, K
Achan, and B
J
Frey
Unsupervised  image translation
In iccv, pages NNN–NNN, N00N
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  et al
Imagenet large scale visual recognition challenge
 IJCV, NNN(N):NNN–NNN, N0NN
N  [NN] T
Salimans, I
Goodfellow, W
Zaremba, V
Cheung,  A
Radford, and X
Chen
Improved techniques for  training gans
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 N  [N0] P
Sangkloy, J
Lu, C
Fang, F
Yu, and J
Hays
Scribbler:  Controlling deep image synthesis with sketch and color
 In CVPR, N0NN
N  [NN] Y
Shih, S
Paris, F
Durand, and W
T
Freeman
Datadriven hallucination of different times of day from a  single outdoor photo
ACM Transactions on Graphics  (TOG), NN(N):N00, N0NN
N  [NN] A
Shrivastava, T
Pfister, O
Tuzel, J
Susskind,  W
Wang, and R
Webb
Learning from simulated and  unsupervised images through adversarial training
arXiv  preprint arXiv:NNNN.0NNNN, N0NN
N, N, N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
arXiv  preprint arXiv:NN0N.NNNN, N0NN
N  [NN] N
Sundaram, T
Brox, and K
Keutzer
Dense point trajectories by gpu-accelerated large displacement optical  flow
In ECCV, pages NNN–NNN
Springer, N0N0
N  [NN] Y
Taigman, A
Polyak, and L
Wolf
Unsupervised cross-domain image generation
arXiv preprint  arXiv:NNNN.0NN00, N0NN
N, N  [NN] D
Turmukhambetov, N
D
Campbell, S
J
Prince, and  J
Kautz
Modeling object appearance using contextconditioned component analysis
In CVPR, pages NNNN–  NNNN, N0NN
N  [NN] M
Twain
The Jumping Frog: in English, then in French,  and then Clawed Back into a Civilized Language Once  More by Patient, Unremunerated Toil
NN0N
N  [NN] D
Ulyanov, V
Lebedev, A
Vedaldi, and V
Lempitsky
 Texture networks: Feed-forward synthesis of textures  and stylized images
In Int
Conf
on Machine Learning  (ICML), N0NN
N  [NN] D
Ulyanov, A
Vedaldi, and V
Lempitsky
Instance normalization: The missing ingredient for fast stylization
 arXiv preprint arXiv:NN0N.0N0NN, N0NN
N  [N0] C
Vondrick, H
Pirsiavash, and A
Torralba
Generating  videos with scene dynamics
In NIPS, pages NNN–NNN,  N0NN
N  [NN] F
Wang, Q
Huang, and L
J
Guibas
Image cosegmentation via consistent functional maps
In ICCV,  pages NNN–NNN, N0NN
N  [NN] X
Wang and A
Gupta
Generative image modeling  using style and structure adversarial networks
ECCV,  N0NN
N  [NN] J
Wu, C
Zhang, T
Xue, B
Freeman, and J
Tenenbaum
 Learning a probabilistic latent space of object shapes  via Nd generative-adversarial modeling
In NIPS, pages  NN–N0, N0NN
N  [NN] S
Xie and Z
Tu
Holistically-nested edge detection
In  ICCV, N0NN
N  [NN] Z
Yi, H
Zhang, T
Gong, Tan, and M
Gong
Dualgan: Unsupervised dual learning for image-to-image  translation
In ICCV, N0NN
N  [NN] C
Zach, M
Klopschitz, and M
Pollefeys
Disambiguating visual relations using loop constraints
In CVPR,  pages NNNN–NNNN
IEEE, N0N0
N  [NN] R
Zhang, P
Isola, and A
A
Efros
Colorful image  colorization
In ECCV, N0NN
N  [NN] J
Zhao, M
Mathieu, and Y
LeCun
Energybased generative adversarial network
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [NN] T
Zhou, Y
Jae Lee, S
X
Yu, and A
A
Efros
Flowweb:  Joint image set alignment by weaving consistent, pixelwise correspondences
In CVPR, pages NNNN–NN00,  N0NN
N  [N0] T
Zhou, P
Krahenbuhl, M
Aubry, Q
Huang, and A
A
 Efros
Learning dense correspondence via Nd-guided  cycle consistency
In CVPR, pages NNN–NNN, N0NN
N, N  [NN] J.-Y
Zhu, P
Krähenbühl, E
Shechtman, and A
A
Efros
 Generative visual manipulation on the natural image  manifold
In ECCV, N0NN
N  NNNNNeural EPI-Volume Networks for Shape From Light Field   Neural EPI-volume Networks for Shape from Light Field  Stefan HeberN Wei YuN Thomas PockN,N  Graz University of TechnologyN  Austrian Institute of TechnologyN  {stefan.heber,wei.yu,pock}@icg.tugraz.at  Abstract  This paper presents a novel deep regression network to  extract geometric information from Light Field (LF) data
 Our network builds upon u-shaped network architectures
 Those networks involve two symmetric parts, an encoding  and a decoding part
In the first part the network encodes relevant information from the given input into a set  of high-level feature maps
In the second part the generated feature maps are then decoded to the desired output
 To predict reliable and robust depth information the proposed network examines ND subsets of the ND LF called  Epipolar Plane Image (EPI) volumes
An important aspect of our network is the use of ND convolutional layers,  that allow to propagate information from two spatial dimensions and one directional dimension of the LF
Compared to  previous work this allows for an additional spatial regularization, which reduces depth artifacts and simultaneously  maintains clear depth discontinuities
Experimental results  show that our approach allows to create high-quality reconstruction results, which outperform current state-of-the-art  Shape from Light Field (SfLF) techniques
The main advantage of the proposed approach is the ability to provide those  high-quality reconstructions at a low computation time
 N
Introduction  This paper investigates the task of reconstructing the geometry of a scene based on captured Light Field (LF) data
 The corresponding problem is referred to as Shape from  Light Field (SfLF)
A LF represents a densely sampled set  of images captured from a regular grid of viewpoints located on a common ND plane
It should be emphasized  that the key difference to the general multi-view stereo setting is the dense and regular sampling of the viewpoints
 Thus compared to a traditional ND image a ND LF provides two additional dimensions, that can be interpreted as  a parametrization of the ND grid of viewpoints
This multiview stereo interpretation of the LF data directly shows that  a LF provides information about the geometry of the obx  ξ  y  x  ξ  y  Figure N
Illustration of the input and output of the proposed network
To the left the figure shows a RGB EPI volume, that is  fed to our network, and to the right the corresponding color-coded  disparity information is shown
 served scene
The encoded geometrical information in the  LF allows to tackle problems that are impossible to solve  based on a single ND image of the scene
Those problems include the geometrical reconstruction of the observed  scene itself [NN, N, NN, N0, NN, NN], the generation of images  with different focus or aperture settings [NN, NN], and the  digital viewpoint manipulation [NN], to name but a few
 SfLF is currently a very active area of research
The LF  research field has grown from a niche topic to an established  part of today’s Computer Vision (CV) research
This development occurred not least because there is a growing commercial interest in LF technology
Nowadays LF or plenoptic cameras are used in industrial applications, like for instance automated optical inspection [NN], and LF technology is used in consumer cameras to provide features like  digital refocusing capabilities [NN]
Moreover, there is also  an increasing number of companies that are looking for  new ways to capture cinematic Virtual Reality (VR) content, where LF imaging might be a perfect solution that allows to take advantage of the freedom of motion offered by  devices like Oculus Rift [NN] or HTC Vive [NN]
 NNNN    In mathematical terms a ND LF is commonly defined via  the so-called two-plane parametrization, where a ray is defined by the intersection points of two parallel planes
Let  Ω ⊆ RN and Π ⊆ RN be two parallel planes (Ω N= Π), then the LF is defined as  L : Ω×Π → R, (p,q) N→ L(p,q) , (N)  where p = (x, y)⊤ ∈ Ω and q = (ξ, η)⊤ ∈ Π represent spatial and directional coordinates
Note that Ω corresponds to the traditional image plane and Π is usually referred to as lens or focal plane
 A LF can be visualized in many different ways
Common  visualizations are sub-aperture images and EPIs
Both represent ND slices through the general ND LF
The representation we work with in this paper is called EPI volume [N],  which is equivalent to an orthogonal ND slice through the  LF
In terms of Equation (N) an EPI volume is obtained  by holding one directional coordinate constant and varying  the remaining coordinates
For instance by choosing a certain directional coordinate η we restrict the ND LF to the ND function  Ση : R N → R, (x, y, ξ) N→ L(x, y, ξ, η) , (N)  that defines the corresponding horizontal EPI volume
Similarly one can also define vertical EPI volumes
Compare  Figure N for a visualization of an EPI volume
EPI volumes  nicely illustrate the linear characteristic of the LF space
 By analyzing the orientations of the individual lines in this  representation one can infer the depth of the corresponding  scene points
This correspondence between depth and orientation has been leveraged in many works on SfLF
 After the huge success of deep learning in a variety of  CV applications it was only a matter of time till deep learning principles have found their application also in LF image  processing
In [NN, NN] it was shown that utilizing learningbased approaches for SfLF has a high potential to reduce  depth artifacts due to occlusions, partial visibility, and specular reflections
The advantages of data-driven approaches  are not only the capability to learn from data how to handle certain artifacts, but also the facilitation for an efficient  implementation of the inference step, which results in a fast  computation time
 This work focuses on one main drawback of the deep  learning architecture proposed in [NN]
We address the lack  of regularization in the disparity prediction provided by this  method
Due to the fact that the network in [NN] is designed  to process a single EPI, the input information fed to the network is limited to one spatial dimension
This results in  streaking artifacts in the respective direction, which can not  be reliably resolved in their proposed framework
To remedy this, we propose to extend the network architecture in  [NN] to predict disparity based on entire EPI volumes, i.e 
 we allow the network to incorporate information from both  spatial dimension
We will show that this simple modification allows to spatially propagate information and avoids  the unwanted streaking artifacts
 N
Related Work  SfLF is a fundamental problem in LF image processing
 However, despite a substantial amount of progress in this  field, ND scene reconstruction from LFs still struggles with  many difficulties, especially in dealing with occlusions, textureless regions, and specular reflections
 There is a wide range of methods for SfLF
The field  can be roughly divided into methods based on EPI analysis  like e.g 
[NN, N], and multi-view stereo matching based approaches like e.g 
[NN, N]
The seminal work of Bolles et  al 
[N] introduced so-called EPI volumes, where they analyze the slopes of lines by line fitting in order to estimate sparse disparity information
In [N] Criminisi et al 
 exploited the high degree of regularity found in the EPIs
 They performed a so-called EPI strip rectification, i.e 
a  shearing of the EPI, to estimate lines with smallest color  variation and hence dense disparity information
The first  order structure tensor was used in [NN, N] to compute the  orientation of lines in the EPIs
In [NN] the authors proposed a matching-term based on Active Wavefront Sampling (AWS), that is used within a variational multi-view  stereo framework
Tao et al 
[N0] suggested to combine correspondence cues with defocus cues to calculate depth
In  order to indicate the probability of occlusions Chen et al 
 [N] introduced a bilateral consistency metric on the surface  camera
In [NN] Heber and Pock defined a new dataterm  based on Robust Principal Component Analysis (RPCA),  that exploits the redundancy of sub-aperture views
Jeon et  al 
[NN] employed the phase-shift theorem to match subaperture images
 While for classical stereo reconstruction for color images  deep learning approaches are gaining ground [NN], SfLF  methods still mainly rely on classical variational principles,  EPI filtering, and other handcrafted solutions
The main  reason for this is the lack of high quality large-scale training  data, which is essential to train deep network architectures
 Hence only few work has been pursued on utilizing Machine Learning (ML) techniques for LF analysis [NN, NN]
 To the best of our knowledge there exist only two relevant  publications that apply ML techniques to SfLF
Heber and  Pock [NN] proposed to apply a conventional Convolutional  Neural Network (CNN) in a sliding window fashion to predict slope orientation in the EPIs
The main motivation of  utilizing ML for SfLF is the fact that phenomena such as  occlusions, specular highlights or reflections manifest as  certain patterns on the EPI space and can be learned in a  data-driven approach
Hence learning-based approaches basically allow to handle cases that are problematic for traNNNN    ditional non-learning based methods
However, because  of the redundancy in overlapping patches, the patch-based  method in [NN] comes with the drawback of high computational costs
Furthermore, it also relies on an additional refinement step to handle textureless or uniform regions
In a  follow up work [NN] the authors addressed those drawbacks  and proposed a more sophisticated network structure that  operates on entire ND EPIs
The network achieved good reconstruction results in combination with low computational  costs
However, one downside of the method is the introduction of streaking artifacts into the final reconstruction  result
In this work we tackle this problem by providing  the network with additional information from neighboring  pixels, i.e 
extending the input dimension of the network by  incorporating the second spatial dimension
Hence we propose to train a network that allows to predict disparity information based on entire EPI volumes
In this work, for the  first time, we unify ideas from ND EPI analysis with spatial matching based approaches by learning ND filters for  disparity estimation based on EPI volumes
 N
Contribution  In this paper we make the following main contributions:  We propose a method for SfLF that builds upon the ushaped architecture proposed in [NN]
In particular, we extend the work of [NN] by introducing additional spatial regularization in terms of ND convolutions
By doing so we  are able to eliminate the main drawback of the network proposed in [NN], which is the tendency to generate visually  unpleasing streaking artifacts
Hence, we transfer recent  success in predicting disparity information based on EPIs to  entire EPI volumes
More specifically, this means that the  proposed network sequentially processes ND subvolumes of  the ND LF instead of ND EPIs
Compared to [NN] this modification allows to propagate information from both spatial  dimensions and thus allows to avoid unwanted depth artifacts in the final ND disparity field
In a fair evaluation we  will show that our learning-based method is able to outperform the current state of the art
It not just allows to reduce  depth artifacts in the final reconstruction, but it also allows  to maintain a low computation time
 N
Methodology  This section describes the methodology of the proposed  deep learning approach for disparity prediction based on  EPI volumes [N]
The proposed approach consists of three  main parts: (i) extending the u-shaped network structure in  [NN] to perform additional spatial regularization, (ii) preparing a dataset for supervised training, and (iii) training the  network using the tensorflow framework [N]
 Our data-driven approach is based on CNNs [NN], that  have been successfully applied to many CV applications
 The popularity of CNNs in CV increased drastically after Krizhevsky et al 
[N0] efficiently applied them for large  scale image classification
Modern CNN architectures basically alternate between convolutions and Rectified Linear  Units (ReLUs) [NN]
Note that convolutions only account  for short-range dependencies, i.e 
that they are limited by  the size of their kernels
There are different ways to allow long-range dependencies without introducing unfeasible fully connected layers
The simplest solution is to make  the network deeper and with that increase the receptive field  to the desired size
Another way is to introduce pooling or  downsampling layer, but this reduces the output resolution  and hence is only one part of the solution
The second part  involves so-called unpooling or upsampling layer, that allow to increase the resolution again to a desired output resolution, see e.g 
[NN, N, NN]
Networks that involve a bottleneck due to downsampling and upsampling operations were  first introduced to model auto-encoder
Those networks are  basically designed to learn a sparse representation of the  given data
Hence they compress the data which also results in a loss of detail when utilized for other prediction  tasks
By introducing connections that skip the downsampling parts of the network it is possible to preserve the high  frequency information and simultaneously allow for longrange dependencies, as demonstrated in e.g 
[NN, N, NN]
 Those skip-connections are denoted as pinhole connections  in the remainder of this paper
 The proposed network is an extension of the network  proposed in [NN] and is mainly designed to remove depth  artifacts occurring in the final ND disparity field
Due to the  fact that the network in [NN] only receives input from one  spatial dimension, the result is not necessarily consistent  w.r.t
the second spatial dimension
This results in depthartifact in ambiguous regions, which appear as streaking artifact in the sub-aperture images of the final reconstruction
 In this work we remedy this problem
We extend the network input by the second spatial dimension
In this way we  can exploit ND convolutional layers to perform an additional  spatial regularization
 In what follows we will present details about the suggested network architectures and the training procedure
 Due to the fact that our method is not based on natural  ND images, we are not able to exploit existing trained networks in terms of transfer learning
The proposed network  is entirely trained from scratch
 Network Architecture
The proposed network is a special version of a Fully Convolutional Network (FCN) [NN],  that has a network structure similar to an auto-encoder [N]  with additional pinhole connections
The overall network  architecture is inspired by [NN] and it is designed to predict disparity based on RGB EPI volumes
Note that an EPI  volume defines a ND subset of a ND LF, i.e 
several netNNNN    NN  N00  N00  N00×N00×NN×NN N00×N00×NN×NN N00×N00×NN×NN N00×N00×NN×NN  NN  N00  N00  N00×N00×NN×NN N00×N00×NN×NN N00×N00×NN×NN N00×N00×NN×NN  N0×N0×NN×NN N0×N0×NN×NN N0×N0×NN×NNN N0×N0×NN×NN  NN×NN×NN×NN NN×NN×NN×NN  Figure N
Illustration of the proposed network architecture
The overall network structure builds upon u-shaped networks
Those networks  involve two symmetric parts, an encoding and a decoding part
The encoding and decoding parts of the network are highlighted in purple  and green, respectively
To preserve high-frequency information the network also uses so-called pinhole connections, marked in blue, that  allow to skip the downsampling parts of the network
 work predictions have to be combined to obtain the final  ND disparity field
Figure N provides an overview of the entire network structure
Contrary to [NN], where the network  only operates on a single EPI, the input of the proposed network is an RGB EPI volume and the output is its corresponding disparity volume
Thus the proposed network extends the network in [NN] by replacing all ND operators with  their ND counterparts
The proposed network consists of  essentially two symmetric parts, an encoding part (c.f 
purple part in Figure N) and a decoding part (c.f 
green part in  Figure N)
Each part is further subdivided into different levels
In the encoding and decoding part those levels are connected via down and up-convolutional layers, respectively
 At each down-convolutional layer we reduce the spatial resolution by a factor of two, and at each up-convolutional  layer we consequently increase the resolution again by the  same factor
In Figure N the down and up-convolutional  layers are indicated with purple and green arrows, respectively
Each level in the network consists of four convolutional layer each followed by a ReLU non-linearity [NN],  σ(x) = max(0,x)
Note that each layer is a four dimen- sional array of size h× w × d× c, where the first three di- mensions correspond to the two spatial and one directional  dimensions of the LF, and c is the feature or channel dimen- sion
The involved convolutional layers perform ND convolutions with filter kernels of size N× N× N, that correspond to the x, y, and ξ dimension of the LF
Each convolutional layer employs max{NN, NN · Nl−N} filter, where l ∈ [N]N de- notes the respective level, i.e 
we start with NN feature chan- nels in the first level of the encoding part and gradually increase them towards higher levels, till we reach a maximum  number of NN feature channels
Likewise in the decoding part we gradually decrease the number of feature channels  at each up-convolutional layer except at the lowest one
At  NNotation: [n] := {N, 


, n}  the very end of the network we use a convolutional layer to  map to one output channel representing the disparity information
Note that the network does not include any pooling  layer, we make use of learned down and up-convolutional  layers instead
An important aspect of this network is the  use of so-called pinhole connections, that connect the levels  from the encoding part with the respective levels in the decoding part of the network
At each pinhole connection (c.f 
 blue arrows in Figure N) we concatenate the output feature  map of the encoding level with the input feature map of the  corresponding decoding level
Those pinhole connections  allow to preserve high frequency information and thus increase the amount of details in the final reconstruction
Due  to the fact that this network belongs to the class of FCNs it  allows to process EPI volumes of arbitrary resolutions
The  main reason for this is the fact that the involved convolutions are inherently translational invariant
Also note that  the network is trained end-to-end and does not make use of  pre- and post-processing complications
 Dataset
In order to train the proposed network a large  amount of training data is needed for supervised training
 For this purpose we leverage the synthetic LF dataset proposed in [NN]
This dataset provides several interesting  features that distinguishes it from other datasets, including highly accurate ground truth depth fields, and a random  scene generator, which makes it easy to scale the dataset  as required
For our current purpose we generated N00 LFs with a spatial resolution of NN0×NN0 and a directional reso- lution of NN×NN
The entire dataset is split up into a training set of NN0 LFs and a test set of N0 LFs
 Data Augmentation
One way to combat overfitting on  the training data is called data augmentation [N0, N]
The  main idea is to train the model such that it gets invariant to  NNNN    Figure N
Illustration of the used data augmentation
The figure  shows slices through the EPI volume, where the original sample is  shown at the top followed by different augmented versions
 certain predefined image deformations
This is done by extending the training set with slightly modified training samples
Although the samples generated via data augmentation are heavily correlated, they allow to increase the robustness of the trained model
We perform a large amount of  data augmentation, including hue, saturation, contrast and  brightness modifications
To modify the hue and saturation  we first convert the RGB images to the HSV color space and  add offsets to the hue and saturation channels
The offsets  are randomly picked from the interval [−0.NN, 0.NN] for the hue channel and from [0.N, N.0] for the saturation channel
After that we convert back to the RGB color space
To augment the contrast, for each channel x we compute the mean  x̄ of the pixel values and calculate the contrast manipulated  result as x̄ + (x − x̄)s, where s denotes a contrast factor randomly picked from the interval [0.N, N.0]
The bright- ness is augmented by adding an offset to each channel, that  is randomly picked from [−0.N, 0.N]
Besides the changes in pixel values we also flip the x and ξ coordinate axes ran- domly with a probability of 0.N
Note, when flipping one of the axes we also need to simultaneously flip and negate the  disparity values of the corresponding labels
Finally we also  add additive Gaussian noise with zeros mean and a standard  deviation of N% of the image dynamic range
Figure N pro- vides an illustration of the implemented data augmentation,  where slices through augmented EPI volumes are shown
 Network Training
In order to train the proposed network we use the tensorflow framework [N], where we chose  Adam [NN] to optimize the ℓN loss
Compared to using an ℓN loss this allows to reduce the effect of blurry predictions
 For the training procedure we implemented an input  pipeline, that performs the following steps
First we load  a random LF from the training set into memory and extract  an RGB EPI volume of size N00 × N00 × NN × N at a ran- dom position
This EPI volume constitutes a single training sample, which is added to an input queue, that holds a  certain amount of samples
When removing samples from  this queue new samples are automatically reloaded
During training we take the first n samples from the queue and triple them using data augmentation
We calculate the  gradient of the resulting mini-patch using back-propagation  and update the network parameters based on the selected  optimization scheme
We use N0 LFs from the test set to monitor overfitting
 Initialization is another important part when training a  network
As suggested in [N0] we initialize the weights of  the network by drawing them from a Gaussian distribution  with standard deviation √  N/N , where N denotes the num- ber of incoming nodes
We train the model for approximately N000 epochs, where we use a mini-batch size of NN
 N
Experiments  We thoroughly evaluate the proposed model on the following datasets
For the synthetic evaluation we use the LF  dataset proposed in [NN]
This dataset provides LFs, with  a spatial resolution of NN0 × NN0 and a directional resolu- tion of NN × NN, that are generated using the ray tracing software POV-Ray [NN]
The rendered LFs in this dataset  are quite challenging because of non-Lambertian surfaces  and a great number of objects that are occluding each other
 For real world evaluation we use the Stanford Light Field  Archive (SLFA) [NN]
The LFs from the SLFA are captured  with a multi-camera array
They have varying spatial resolutions and a fixed directional resolution of NN× NN
We compare against top performing algorithms for SfLF  [NN, N0, NN, NN, NN, NN]
Wanner and Goldluecke [NN]  proposed a method based on EPI analyzes that uses the  ND structure tensor to estimate line orientations
Tao et  al 
[N0] suggested to combine correspondence and defocus cues
Heber and Pock [NN] proposed a sparse coding  method based on RPCA, that shears the ND LF
Jeon et  al 
[NN] utilizes the phase shift theorem to calculate subpixel displacements
In [NN] Heber and Pock presented a  patch-based deep learning approach to predict depth information for given LF data
Their network takes as input  two patches extracted from the vertical and horizontal EPIs  and predicts the orientation of the corresponding ND hyperplane in the domain of the LF
After the pointwise prediction they used an additional ND higher order regularization  step to cope with untextured or uniform regions
They also  used a ND anisotropic diffusion tensor to guide the regularization and a confidence measure to gauge the reliability of the CNN prediction
In a follow up work [NN] the  approach is extended to predict entire ND EPIs at once using u-shaped networks
This allows to get rid of the regularization step needed in [NN] and thus drastically reduces  the overall computation time
However, due to the fact that  they process each EPI separately, their approach introduces  streaking artifact in the sub-aperture images of the final reconstruction
To overcome this drawback the proposed network predicts disparity information based on entire EPI volumes
This allows to propagate information in both spatial  dimensions and thus introduces some kind of spatial regularization
Compared to [NN] we will show that this modifiNNNN    LF ground truth Wanner [NN] Tao [N0]  Heber [NN] Jeon [NN] Heber [NN] proposed  LF ground truth Wanner [NN] Tao [N0]  Heber [NN] Jeon [NN] Heber [NN] proposed  Figure N
Comparison to state-of-the-art methods on the synthetic POV-Ray dataset
The figure shows the center view of the LF, the  color-coded ground truth, the results for five state-of-the-art SfLF methods [NN, N0, NN, NN, NN], followed by the result of the proposed  method
 NNNN    Wanner [NN] Tao [N0] Heber [NN] Jeon [NN] Heber [NN] (CNN) Heber [NN] proposed  RMSE N.NN N.NN N.N0 N.NN N.NN 0.N0 0.NN MAE N.NN N.0N 0.NN 0.NN N.NN 0.NN 0.NN 0.N% NN.00 NN.NN N.NN N.NN NN.NN N.NN N.NN 0.N% NN.NN NN.NN NN.N0 NN.NN NN.NN NN.NN NN.NN Time Nmin NNs NNmin Ns Nmin NNs Nh NNmin N0s NNs N.Ns 0.Ns GPU ✓ ✗ ✓ ✗ ✓ ✓ ✓  Table N
Quantitative results for various SfLF methods averaged over N0 synthetic LFs
The table provides the RMSE, MAE, the percentage  of pixels with a relative disparity error larger than 0.N% and 0.N%, and the computational time of the method
In each row we indicate with  green and yellow the best and second best result, respectively
 cation allows to remove depth artifacts, and simultaneously  preserves the advantages of sharp depth discontinuities and  a fast computation time
 In this section we mainly focus on a quantitative evaluation based on synthetic data
Besides that we also present  some qualitative real world results
Note that for all our experiments we use a horizontal slicing strategy as indicated  in Equation (N)
Overall our evaluations show that the proposed model is superior to the state of the art
 Synthetic Evaluation
For the synthetic evaluation we  use a test set of N0 LFs
The majority of disparity values in this dataset are in the range [−N, N], with a few exception that exceed this range
Figure N provides a visualization of  the disparity results of the proposed method and compares  them to different state-of-the-art methods
For methods relying on precomputed cost volumes, i.e 
[NN, N0, NN], we set  the number of labels to N00 in this experiment
Moreover for those methods the needed disparity range is set based on  the ground truth data
When considering Figure N we see  that the proposed network is able to predict accurate disparity results
Overall the results of the proposed method are  on par with those obtained by the method in [NN]
However, when considering the closeup views we recognize that  the proposed model is able to effectively remove unwanted  streaking artifact, that are prevalent in the results predicted  by the network proposed in [NN]
Also note that the proposed method is barely effected by depth discontinuities
 Table N provides quantitative results, that are averaged  over the N0 LFs used for testing
Note that for the method proposed in [NN] we only compare to the network prediction  and exclude the additional refinement step
The table shows  the RMSE, the MAE, and the percentage of pixels with a  relative disparity error larger than 0.N% and 0.N%
More- over the table also provides the average computation time  for the various methods and an indication if a GPU implementation was used or not
We see that the proposed method  provides an excellent performance, and achieves the overall  best results
Overall it can improve upon the ND method  proposed in [NN]
It provides a low error rate in combination  with low computation times
Furthermore we also observe  that the proposed method is significantly better compared to  non-learning based methods
Especially in terms of computation time we observe a tremendous improvement
 Real World Evaluation
Figure N provides a qualitative  comparison to state-of-the-art methods based on the SLFA
 Note that we had to reduce the directional resolution of the  dataset from NN×NN to NN×NN to be able to compute results for the methods by Jeon et al 
[NN] and Tao et al 
[N0] in a  reasonable timeframe
Moreover the number of labels for  those methods is set to NN
The results show that the pro- posed method allows to predict excellent disparity fields,  although the model was not trained on this specific dataset
 Compared to [NN] we see that the proposed model removes  the streaking artifacts
Especially in regions of depth discontinuities the proposed network is able to reconstruct the  scene more accurately than the competing methods
We  also indicate the computation time for each method at the  bottom right, which shows the main benefit of the learning  based approaches
Compared to the fastest non-learning  based method, the proposed method is able to reconstruct  the scene N00 times faster
Also keep in mind that the pro- posed method at the same time calculates disparity information for an entire EPI volume and not just for the center  view alone
 N
Conclusion  In this paper we have described a new approach for recovering depth information from LF data
We presented an  end-to-end system for SfLF that analyzes EPI volumes
Due  to stacked convolutional operations the proposed network  architecture provides a high efficiency
The suggested network structure extends the u-shaped architecture proposed  in [NN] to perform additional spatial regularization
By doing so we were able to eliminate depth artifacts present in  the results produced with the method in [NN]
 Our experimental results show that the proposed model  is able to predict disparity fields that are significantly better  NNNN    NNmin N0s Nmin NNs Nh Nmin NNs Ns Ns  NNmin NNs Nmin NNs Nh NNmin NNs Ns Ns  NNmin NNs Nmin NNs Nh NNmin N0s Ns Ns  NNmin N0s Nmin N0s Nh Nmin Ns Ns Ns  LF Tao [N0] Heber [NN] Jeon [NN] Heber [NN] proposed  Figure N
Qualitative comparison for LFs from the SLFA
The figure shows from left to right the center view of the LF, followed by the  results for the methods proposed by Tao et al 
[N0], Heber and Pock [NN], Jeon et al 
[NN], and Heber et al 
[NN]
The results to the right  correspond to the proposed method
 than those produced by competing state-of-the-art methods
 The proposed approach combines the two main advantages  of the model proposed in [NN], namely the low error rate  and the low inference time, with an additional spatial regularization that reduces unpleasant depth artifacts
 The presented results suggest that CNNs are well suited  for SfLF
More general, applying deep learning to LF image processing tasks is a promising direction of research  because the progress remaining to achieve in this area is  tremendous
In this paper we focused on SfLF, but the same  type of network architecture can also be used for a large  variety of applications in LF image processing, including  denoising, segmentation, and super-resolution
Testing the  proposed network for the above-mentioned applications is  left as future work
 Acknowledgment
This work was supported by the  Vision+ project Integrating visual information with inde- pendent knowledge, No
NNNNN0
 References  [N] M
Abadi, A
Agarwal, P
Barham, E
Brevdo, Z
Chen,  C
Citro, G
S
Corrado, A
Davis, J
Dean, M
Devin, S
Ghemawat, I
Goodfellow, A
Harp, G
Irving, M
Isard, Y
Jia,  R
Jozefowicz, L
Kaiser, M
Kudlur, J
Levenberg, D
Mané,  R
Monga, S
Moore, D
Murray, C
Olah, M
Schuster,  J
Shlens, B
Steiner, I
Sutskever, K
Talwar, P
Tucker,  V
Vanhoucke, V
Vasudevan, F
Viégas, O
Vinyals, P
Warden, M
Wattenberg, M
Wicke, Y
Yu, and X
Zheng
TensorFlow: Large-scale machine learning on heterogeneous systems, N0NN
Software available from tensorflow.org
 [N] M
aurelio Ranzato, C
Poultney, S
Chopra, and Y
L
Cun
 Efficient learning of sparse representations with an energybased model
In B
Schölkopf, J
C
Platt, and T
Hoffman,  NNNN    editors, Advances in Neural Information Processing Systems  NN, pages NNNN–NNNN
MIT Press, N00N
 [N] R
C
Bolles, H
H
Baker, and D
H
Marimont
Epipolarplane image analysis: An approach to determining structure from motion
International Journal of Computer Vision,  N(N):N–NN, NNNN
 [N] C
Chen, H
Lin, Z
Yu, S
Bing Kang, and J
Yu
Light field  stereo matching using bilateral statistics of surface cameras
 June N0NN
 [N] A
Criminisi, S
B
Kang, R
Swaminathan, R
Szeliski, and  P
Anandan
Extracting layers and analyzing their specular  properties using epipolar-plane-image analysis
Computer  Vision and Image Understanding (CVIU), NN:NN–NN, January  N00N
 [N] A
Dosovitskiy, P
Fischer, E
Ilg, P
Hausser, C
Hazirbas,  V
Golkov, P
van der Smagt, D
Cremers, and T
Brox
 Flownet: Learning optical flow with convolutional networks
 In The IEEE International Conference on Computer Vision  (ICCV), December N0NN
 [N] A
Dosovitskiy, J
T
Springenberg, and T
Brox
Learning  to generate chairs with convolutional neural networks
In  IEEE Conference on Computer Vision and Pattern Recognition, CVPR N0NN, Boston, MA, USA, June N-NN, N0NN, pages  NNNN–NNNN, N0NN
 [N] D
Eigen, C
Puhrsch, and R
Fergus
Depth map prediction from a single image using a multi-scale deep network
 In Z
Ghahramani, M
Welling, C
Cortes, N
Lawrence, and  K
Weinberger, editors, Advances in Neural Information Processing Systems NN, pages NNNN–NNNN
Curran Associates,  Inc., N0NN
 [N] B
Goldluecke and S
Wanner
The variational structure  of disparity and regularization of Nd light fields
In IEEE  Conference on Computer Vision and Pattern Recognition  (CVPR), N0NN
 [N0] K
He, X
Zhang, S
Ren, and J
Sun
Delving deep into  rectifiers: Surpassing human-level performance on imagenet  classification
CoRR, abs/NN0N.0NNNN, N0NN
 [NN] S
Heber and T
Pock
Shape from light field meets robust  PCA
In Proceedings of the NNth European Conference on  Computer Vision, N0NN
 [NN] S
Heber and T
Pock
Convolutional networks for shape  from light field
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June N0NN
 [NN] S
Heber, R
Ranftl, and T
Pock
Variational Shape from  Light Field
In International Conference on Energy Minimization Methods in Computer Vision and Pattern Recognition, N0NN
 [NN] S
Heber, W
Yu, and T
Pock
U-shaped networks for shape  from light field
In Proc
British Machine Vision Conf., N0NN
 [NN] HTC Vive
http://www.vive.com
 [NN] A
Isaksen, L
McMillan, and S
J
Gortler
Dynamically  reparameterized light fields
In SIGGRAPH, pages NNN–N0N,  N000
 [NN] H
G
Jeon, J
Park, G
Choe, J
Park, Y
Bok, Y
W
Tai, and  I
S
Kweon
Accurate depth map estimation from a lenslet  light field camera
In N0NN IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), pages NNNN–NNNN,  June N0NN
 [NN] N
K
Kalantari, T.-C
Wang, and R
Ramamoorthi
 Learning-based view synthesis for light field cameras
ACM  Trans
Graph., NN(N):NNN:N–NNN:N0, Nov
N0NN
 [NN] D
P
Kingma and J
Ba
Adam: A method for stochastic  optimization
CoRR, abs/NNNN.NNN0, N0NN
 [N0] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  F
Pereira, C
Burges, L
Bottou, and K
Weinberger, editors, Advances in Neural Information Processing Systems  NN, pages N0NN–NN0N
Curran Associates, Inc., N0NN
 [NN] Y
Lecun, L
Bottou, Y
Bengio, and P
Haffner
Gradientbased learning applied to document recognition
Proceedings  of the IEEE, NN(NN):NNNN–NNNN, Nov NNNN
 [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In N0NN IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  pages NNNN–NNN0, June N0NN
 [NN] Lytro
https://www.lytro.com/
 [NN] V
Nair and G
E
Hinton
Rectified linear units improve restricted boltzmann machines
In J
Fuernkranz and  T
Joachims, editors, Proceedings of the NNth International  Conference on Machine Learning (ICML-N0), pages N0N–  NNN
Omnipress, N0N0
 [NN] R
Ng
Digital Light Field Photography
Phd thesis, Stanford  University, N00N
 [NN] Oculus
https://www.oculus.com/
 [NN] Pov-ray
http://www.povray.org
 [NN] Raytrix
https://www.raytrix.de/
 [NN] O
Ronneberger, P
Fischer, and T
Brox
U-Net: Convolutional Networks for Biomedical Image Segmentation, pages  NNN–NNN
Springer International Publishing, Cham, N0NN
 [N0] M
W
Tao, S
Hadap, J
Malik, and R
Ramamoorthi
Depth  from combining defocus and correspondence using lightfield cameras
In International Conference on Computer Vision (ICCV), Dec
N0NN
 [NN] T.-C
Wang, J.-Y
Zhu, E
Hiroaki, M
Chandraker, A
A
 Efros, and R
Ramamoorthi
A ND Light-Field Dataset and  CNN Architectures for Material Recognition, pages NNN–  NNN
Springer International Publishing, Cham, N0NN
 [NN] S
Wanner and B
Goldluecke
Globally consistent depth labeling of ND lightfields
In IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), N0NN
 [NN] B
Wilburn, N
Joshi, V
Vaish, E.-V
Talvala, E
Antunez,  A
Barth, A
Adams, M
Horowitz, and M
Levoy
High performance imaging using large camera arrays
ACM Trans
 Graph., NN(N):NNN–NNN, July N00N
 [NN] J
Zbontar and Y
LeCun
Computing the stereo matching cost with a convolutional neural network
CoRR,  abs/NN0N.NNNN, N0NN
 [NN] M
D
Zeiler and R
Fergus
Computer Vision – ECCV N0NN:  NNth European Conference, Zurich, Switzerland, September N-NN, N0NN, Proceedings, Part I, chapter Visualizing  and Understanding Convolutional Networks, pages NNN–NNN
 Springer International Publishing, Cham, N0NN
 NNN0  http://www.vive.com https://www.lytro.com/ https://www.oculus.com/ http://www.povray.org https://www.raytrix.de/Material Editing Using a Physically Based Rendering Network   Material Editing Using a Physically Based Rendering Network  Guilin LiuN,N∗ Duygu CeylanN Ersin YumerN Jimei YangN Jyh-Ming LienN  NGeorge Mason University NAdobe Research NNVIDIA  Abstract  The ability to edit materials of objects in images is desirable by many content creators
However, this is an extremely challenging task as it requires to disentangle intrinsic physical properties of an image
We propose an  end-to-end network architecture that replicates the forward  image formation process to accomplish this task
Specifically, given a single image, the network first predicts intrinsic properties, i.e
shape, illumination, and material, which  are then provided to a rendering layer
This layer performs  in-network image synthesis, thereby enabling the network  to understand the physics behind the image formation process
The proposed rendering layer is fully differentiable,  supports both diffuse and specular materials, and thus can  be applicable in a variety of problem settings
We demonstrate a rich set of visually plausible material editing examples and provide an extensive comparative study
 N
Introduction  One of the main properties of an object that contributes  to its appearance, is material
Hence, many designers desire  to effortlessly alter materials of objects
In case of ND design, however, often the designer only has access to a single  image of the object
Given one such image, e.g
an image  of a fabric sofa, how would you synthesize a new image that  depicts the same sofa with a more specular material?  A typical approach to addressing this image-based material editing problem is to first infer the intrinsic properties  (e.g
shape, material, illumination) from the image which  then enables access to edit them
However, this problem is  highly ambiguous since multiple combinations of intrinsic  properties may result in the same image
Many prior work  has tackled this ambiguity either by assuming at least one  of the intrinsic properties is known [NN] or devising priors  about each of these properties [N]
The recent success of  deep learning based methods, on the other hand, has stimulated research to learn such priors directly from the data
A  feasible approach is to predict each of the intrinsic proper∗This work was done when Guilin Liu was with Adobe and George  Mason University
 input image target material output image  Figure N
We replace the materials of the objects in the input images with the material properties of the objects in the middle column, resulting edited images are shown in the right column
 ties independently from an input image, i.e
using individual neural networks to predict normals, material, and lighting
However, the interaction of these properties during the  image formation process inspires us to devise a joint prediction framework
An alternative common practice of deep  learning methods is to represent the intrinsic properties in  a latent feature space [NN] where an image can be decoded  from implicit feature representations
Directly editing such  feature representations, however, is not trivial since they do  not correspond to any true physical property
On the contrary, the physics of image formation process is well understood and motivates us to replace this black-box decoder  with a physical decoder and thus enables the network to  learn the physics of image formation
 We present an end-to-end network architecture that replicates the image formation process in a physically based rendering layer
Our network is composed of prediction modules that infer each of the intrinsic properties from a single  image of an object
These predictions are then provided  to a rendering layer which re-synthesizes the input image
 We define the loss function as a weighted sum of the error over the individual predictions and perceptual error over  NNNNN    the synthesized images
We provide comparisons with and  without incorporating the perceptual error and show that the  combined loss provides significant improvements (see Figure N and N)
Due to the lack of a large scale dataset of  real images with ground truth normal, material, and lighting annotations, we train our network on a rendering based  synthetic dataset
Nevertheless, this network performs reasonable predictions for real images where the space of materials and lighting is much more diverse
We further refine  our results on real images with a post-optimization process  and show that the network predictions provide a good initialization to this highly non-linear optimization
This paves  the road to plausible editing results (see Figure N)
 Our main contributions are:  N
We present an end-to-end network architecture for  image-based material editing that encapsulates the forward  image formation process in a rendering layer
 N
We present a differentiable rendering layer that supports both diffuse and specular materials and utilizes an environment map to represent a variety of natural illumination conditions
This layer can trivially be integrated into  other networks to enable in-network image synthesis and  thus boost performance
 N
Related Work  Intrinsic Image Decomposition
A closely related  problem to image-based material editing is intrinsic image  decomposition which aims to infer intrinsic properties, e.g
 shape, illumination, and material, from a single image and  thus enables access to edit them
The work of Barrow et  al
[N] is one of the earliest to formulate this problem and  since then several variants have been introduced
 Intrinsic image decomposition is an ill-posed problem  since different combinations of shape, illumination, and  material may result in the same image
Therefore, an important line of work assumes at least one these unknowns  to be given, such as geometry
Patow et al
[N0] provides  a survey of earlier methods proposed to infer illumination,  material, or combination of both for scenes with known geometry
More recent work from Lombardi et al
[NN, NN]  introduces a Bayesian formulation to infer illumination and  material properties from a single image captured under natural illumination
They utilize a material representation  based on Bidirectional Reflectance Distribution Functions  (BRDFs) and thus can handle a wide range of diffuse and  specular materials
Approaches that aim to infer all three  intrinsic properties [N], on the other hand, often make simplified prior assumptions such as diffuse materials and lowfrequency lighting to reduce the complexity of the problem
 An important sub-class of intrinsic image decomposition  is shape from shading (ShS) where the goal is to reconstruct  accurate geometry from shading cues [NN]
Many ShS approaches, however, assume prior knowledge about the material properties [N] or coarse geometry [NN] to be given
 As opposed to these approaches, we assume no prior  knowledge about any of the intrinsic properties to be given  and handle both diffuse and specular materials under natural  illumination
Instead of making assumptions, we aim to infer priors directly from data via a learning based approach
 Material Editing
Some previous methods treat material editing as an image filtering problem without performing explicit intrinsic image decomposition
Khan et al
[N0]  utilize simple heuristics to infer approximate shape and illumination from an image and utilize this knowledge to perform material editing
Boyadzhiev et al
[N] introduce several image filters to change properties such as shininess and  glossiness
While these approaches achieve photo-realistic  results they can provide limited editing scenarios without  explicit knowledge of the intrinsic properties
 Material Modeling via Learning
With the recent success of learning based methods, specifically deep learning,  several data-driven solutions have been proposed to infer  intrinsic properties from an image
Tang et al
[NN] introduce deep lambertian networks to infer diffuse material  properties, a single point light direction, and an orientation  map from a single image
They utilize Gaussian Restricted  Boltzmann Machines to model surface albedo and orientation
Richter et al
[NN] use random forests to extract surface  patches from a database to infer the shape of an object with  diffuse and uniform albedo
Narihira et al
[NN] predict relative lightness of two image patches by training a classifier  on features extracted by deep networks
Similarly, Zhou et  al
[NN] use a convolutional neural network (CNN) to predict relative material properties of two pixels in an image  and then perform a constrained optimization to solve for  the albedo of the entire image
Narihira et al
[NN] propose  a CNN architecture to directly predict albedo and shading  from an image
Kulkarni et al
[NN] use variational autoencoders to disentangle viewpoint, illumination, and other  intrinsic components (e.g
shape, texture) in a single image
One of the limitation of these methods is the ability  to handle diffuse materials only
The recent work of Rematas et al
[NN] predict the combination of material and  illumination from a single image handling both diffuse and  specular materials
In a follow-up work [N], they propose  two independent network architectures to further disentangle material and illumination from such a combined representation
However, any error that occurs when inferring  the combined representation is automatically propagated to  the second step
In contrast, we provide an end-to-end network architecture that performs this disentangling in one  pass
Last but not least, several recent work [NN, N] uses neural networks to estimate per-pixel intrinsic properties from  a given image
However, these estimations are aligned with  the input image, thus, it is not easy transfer these properties  across images of different objects as our method does
 NNNN    input image (I )  target material (mt)  normal prediction module  material prediction module  illumination prediction module  surface normals (n’)  material properties (m’)  illumination (L’)  normal loss  material loss  re n  d e  ri n  g  la  ye r  input image loss  synthesized   input image (I’)  synthesized   target image (O’)  output image loss  Figure N
Given a single image of an object, I , we propose a network architecture to predict material (m′), surface normals (n′), and  illumination (L′)
These predictions are provided to a rendering layer which re-synthesizes the input image (I′)
In addition, a desired  target material, mt, is passed to the rendering layer along with n ′ and L′ to synthesize a target image O′ which depicts the object with  the target material from the same viewpoint and under the same illumination
By defining a joint loss that evaluates both the synthesized  images and the individual predictions, we perform robust image decomposition and thus enable material editing applications
 N
Approach  Overview
We present an end-to-end network architecture for image-based material editing
The input is I, a sin- gle image of an object s with material m captured under  illumination L
We assume the object is masked out in the  image
Given a desired target material definition mt, the  goal is to synthesize an output image O that depicts s from the same viewpoint with material mt and illuminated by L
 While inferring illumination, material, and shape from a  single image is an ill-posed problem, once these parameters  are known, the forward process of image synthesis, i.e
rendering is well defined
We propose a network architecture  that encapsulates both this inverse decomposition and the  forward rendering processes as shown in Figure N
The first  part of the network aims to infer illumination (L′), material  (m′), and ND shape (represented as surface normals, n′)  from I via three prediction modules
The output of these modules is then provided to a rendering layer to synthesize  I ′
In addition, the target material mt is passed to the ren- dering layer along with predicted L′ and n′ to synthesize an  output image O′
We define a joint loss that evaluates both  the outputs of the rendering layer and the predictions of two  of the individual modules
We show that this joint loss significantly improves the accuracy of the inverse decomposition and enables compelling material editing results
 We first introduce our render layer (Sec
N.N), prediction  modules providing data to it (Sec
N.N), and the training procedure (Sec
N.N)
Then, we discuss how objects composed  of multiple materials are handled (Sec
N.N)
 N.N
Rendering Layer  The color of each pixel in an image depends on how  the corresponding object surface point reflects and emits incoming light along the viewing direction
Our rendering  layer aims to replicate this process as accurately as possible
Compared to previously proposed differentiable renderers [NN], the main advantage of our rendering layer is to  model surface material properties based on BRDFs to handle both diffuse and specular materials
We also utilize environment maps which provide great flexibility in representing a wide range of illumination conditions
We note that  our rendering layer also makes several moderate assumptions
We assume the image is formed under translationinvariant natural illumination (i.e
incoming light depends  only on the direction)
We also assume there is no emission  and omit complex light interactions like inter-reflections  and subsurface scattering
 Under these conditions, we model the image formation process mathematically similar to Lombardi et al [NN]
 Given per-pixel surface normals n (in camera coordinates),  material properties m and illumination L, the outgoing light  intensity for each pixel p in image I can be written as an in- tegration over all incoming light directions ωi:  Ip(np,m,L) =  ∫ f(ωi, ωo,m)L(ωi)max(0,np ·ωi)dωi,  (N)  where L(ωi) defines the intensity of the incoming light and f(ωi, ωo,m) defines how this light is reflected along the outgoing light direction ωo based on the material properties m
In order to make this formulation differentiable, we  substitute the integral with a sum over a discrete set of incoming light directions defined by the illumination L:  Ip(np,m,L) = ∑ L  f(ωi, ωo,m)L(ωi)max(0,np·ωi)dωi
 (N)  We now describe how we represent each property and reNNNN    fer to the supplementary material about the details of the  forward and back propagation on this rendering layer
 Surface normals (n)
n is represented by a N-channel  image, same size as the input image, where each pixel p  encodes the corresponding per-pixel normal np
 Illumination (L)
We represent illumination with an  HDR environment map of dimension NN× NNN
Each pixel coordinate in this image can be mapped to spherical coordinates and thus corresponds to an incoming light direction  ωi in Equation N
The pixel value stores the intensity of the  light coming from this direction
 Material (m)
We define f(ωi, ωo,m) based on BRDFs [NN] which provide a physically correct description  of pointwise light reflection both for diffuse and specular  surfaces
Non-parametric models [NN] aim to capture the  full spectrum of BRDFs via lookup tables that encode the  ratio of the reflected radiance to the incident radiance given  incoming and outgoing light directions (ωi, ωo)
Although such lookup tables achieve highly realistic results, they are  computationally expensive to store and not differentiable
 Among the various parametric representations, we adopt the  Directional Statistics BRDF (DSBRDF) model [NN] which  is shown to accurately model a wide variety of measured  BRDFs
This model represents each BRDF as a combination of hemispherical exponential power distributions and  the number of parameters depends on the number of distributions utilized
Our experiments show that utilizing N  distributions provides accurate approximations resulting in  N0N parameters per material definition in total
We refer the  reader to the original work [NN] for more details
 N.N
Prediction Modules  We utilize three prediction modules to infer surface normals, material properties, and illumination
The input to  each module is the NNN × NNN input image I
We refer to the supplementary material for the detailed network architecture of each module
 Normal prediction
The normal prediction module follows the same spirit as the recent work of Eigen et al
[N]
 The main difference is that we predict a normal map that  is equal in size to the input image by utilizing a N-scalesubmodule network as opposed to the originally proposed  N-scale-submodule network
The fourth submodule consists  of N convolutional layers where both input and output size  is equal to the size of the input image
We utilize a normalization layer to predict surface normals with unit length
 Illumination prediction
Illumination prediction module is composed of N convolutional layers where the output  of each such layer is half the size of its input
The convolutional layers are followed by N fully connected layers and a  sequence of deconvolutional layers to generate an environment map of size NN × NNN
Each convolutional and fully connected layer is accompanied by a rectifier linear unit
 Fully connected layers are also followed by dropout layers
 Material prediction
Material prediction module is  composed of N convolutional layers where the output of  each such layer is half the size of its input
The convolutional layers are followed by N fully connected layers and a  tanh layer
Each convolutional and fully connected layer is  accompanied by a rectifier linear unit
Fully connected layers are also followed by dropout layers
We note that since  each of the N0N material parameters are defined at different  scales, we normalize each to the range [−0.NN, 0.NN] and remap to their original scales after prediction
 N.N
Training
 We train the proposed network architecture by defining  a joint loss function that evaluates both the predictions of  the individual modules and the images synthesized by the  rendering layer
Specifically, we define a LN normal loss:  lnormal = ∑ p  (np − n ′  p) N, (N)  where np and n ′  p denote the ground-truth and predicted  surface normals for each pixel p respectively
Since both  ground-truth and predicted normals are unit length, this loss  is equivalent to cosine similarity used by Eigen et al
[N]
 We define the material loss as the LN norm between the  ground truth (m) and predicted material parameters (m′):  lmaterial = (m−m ′)N (N)  We also utilize a perceptual loss, lperceptual, to evaluate  the difference between the synthesized {I ′,O′} and ground truth {I,O} images, which helps to recover normal details not captured by the LN normal loss
We use the pre-trained  vggNN network to measure lperceptual as proposed by Johnson et al
[N]
 We define the final loss, l, as a weighted combination of  these individual loss functions:  l = wnlnormal + wmlmaterial + wplperceptual, (N)  where we empirically set wn=N× N0 N, wm=N× N0  N, wp=N
Note that we do not define a loss between ground truth  and predicted illumination due to the limited amount of  available public HDR environment maps for training
Each  pixel in the spherical environment map corresponds to a  light direction
Depending on the viewing direction and surface normals, the contribution of each light direction to the  final rendered image will vary
We leave it as future work to  design a robust loss function that takes this non-uniformity  into account
 N.N
Extension to Multi-Material Case  A typical use case scenario for image-based material  editing is where a user denotes the region of interest of an  NNNN    input image GT output input image GT output input image GT output  Figure N
For each input image, we synthesize an output image with a given target material definition
We provide the ground truth (GT)  target images for reference
The left (middle) column shows cases where the target material is more (less) specular than the input material
 We also provide examples where both the input and the target material are specular in the right column
 object and a desired target material that should be applied  to this region
Our approach trivially supports such use  cases
Given a segmentation mask denoting regions of uniform material, we perform material prediction for each region while predicting a single normal and illumination map  for the entire image
All these predictions are then provided  to the rendering layer
Target materials are defined for each  region of interest separately
The rest of the training and  testing process remains unchanged
 N.N
Refinement with Post-optimization  Due to the lack of a large scale dataset of real images  with ground truth normal, material, and illumination annotations, we train our network with synthetic renderings
 Although large ND shape repositories [N] provide sufficient  shape variation, the network does not see a large spectrum  of material and illumination properties captured in real images
While increasing the variation in the synthetic data  could reduce the discrepancy between the training and test  sets, the gap is almost always there theoretically
Thus,  we refine our network predictions on real images with a  post-optimization
Specifically, using n′,m′,L′, the network predictions of surface normal, material, and illumination for an input image I as initialization, we optimize for n ∗,m∗,L∗ which minimize the following energy function:  argmin n∗,m∗,L∗  ‖ I∗ − I ‖N +a ‖ n∗ − n′ ‖N +b ‖ L∗ − L′ ‖N 
 (N)  I∗ is the image formed by n∗,m∗,L∗ and the first term pe- nalizes the image reconstruction loss
The remaining regularization terms penalize the difference between the network predictions of normal and illumination and their optimized values
We experimentally set a=N, b=N0 as the relative weighting of these regularization terms
We use LBFGS to solve Equation N in an alternative scheme where  only one intrinsic property is updated at each iteration
 N
Evaluation  In this section we provide quantitative and qualitative  evaluations of our method and comparisons to other work
 N.N
Datasets and Training  We train the framework with a large amount of synthetic  data generated for car, chair, sofa, and monitor categories
 We utilize a total of NN0 ND models (NN0 cars, N0 chairs, N0  sofas and N0 monitors) obtained from ShapeNet [N]
For  materials, we use BRDF measurements corresponding to  N0 different materials provided in the MERL database [NN]
 For illumination, we download N0 free HDR environment  maps from the Internet and use random rotations to augment them
 Data generation
For each ND model, material, and illumination combination we render N random views from  a fixed set of pre-sampled NN viewpoints around the object with fixed elevation
We split the data such that no  shape and material is shared between training and test sets
 Specifically, we use N0 cars for pretraining, and N0 shapes  per each category for joint category finetuning, which leaves  N0 shapes per category for testing
Out of the N0 materials, we leave N0 for testing and use the rest in pre-training  and training
This split allows us to generate NN0K pre- training instances, and NN0K multi-category finetuning in- stances
In total, the network is trained with over NN0K unique material-shape-light configurations
 Training procedure
We initialize the weights from a  uniform distribution: [−0.0N, 0.0N]
Normal and material modules are pre-trained using LN loss for a few iterations  and then trained jointly with illumination network
We utilize Adam [NN] optimizer using stochastic gradient descent
 Similarly, we first used momentum parameters βN = 0.N and βN = 0.NNN, and a learning rate of 0.000N
Later we reduced the learning rate to be N× N0−N and N× N0−N
We also reduced βN to 0.N for a more stable training
 N.N
Evaluation on Synthetic Data  To test our network, we randomly select an image corresponding to shape s, environment map L, and material  m from our test set as input
Given a target material mt,  an output image depicting s with material mt and illuminated by L is synthesized
We compare this synthesized  output to ground truth using two metrics
While the LN metNNNN    source image GT output GT output  Figure N
We synthesize an image of an object under some desired  illumination with material properties predicted from a source image
We provide the ground truth (GT) for reference
 ric measures the average pixel-wise error, SSIM metric[NN]  measures the structural difference
We compute the LN error on tone mapped images where each color channel is in  the range [0, NNN] and define per-pixel error as the average of the squared difference of three color channels
We note  that lower (higher) numbers are better for the LN (SSIM)  metric
We provide quantitative results in Table N, last two  columns
We note that our network is trained for all categories jointly
Figure N provides visual results demonstrating that our approach can successfully synthesize specularities when replacing the input material with a more specular  target material
Similarly, specularities in the input image  are successfully removed when replacing the input material  with a more diffuse one
 In Figure N, we show examples where material properties predicted from a source image are transferred to a different object under some desired illumination
Such material transfers alleviates the need to define target materials  explicitly, instead each image becomes an exemplar
 N.N
Evaluation of the Rendering Layer  We evaluate the effectiveness of utilizing the perceptual  loss on the images synthesized by the rendering layer as opposed to independent predictions
We note that for this evaluation, we directly use the pure network predictions without  post-optimization
 Accuracy of Material Coefficients
For this evaluation,  given an input image I, we provide ground truth normals (n) and illumination (L) and use our network to predict only  material
We also train the material prediction module as a  standalone network
While both the standalone module and  our network use the LN loss on material coefficients in spirit  similar to the recent work of Georgoulis et al
[N], the latter  combines this with the perceptual loss on I ′
Note that dur- ing training, the material parameters have been normalized  to the range [-0.NN,0.NN] to reduce the scale issues for LN  loss; while in Table N, we remap to the original scales
N0  materials are used for training and N0 for testing
We provide qualitative and quantitative results in Figure N and Table N respectively
The results demonstrate that the LN loss  is not sufficient in capturing the physical material properties, a small LN error may result in big visual discrepancies  in the rendering output
Incorporation of the rendering layer  GT standalone combined GT standalone combined  Figure N
For each example we show the ground truth image as  well as the renderings obtained by utilizing the material coefficients predicted by the standalone material prediction module and  the combined approach which also uses the rendering layer
 resolves this problem by treating the material coefficients in  accordance with their true physical meaning
 material rendering  LN LN SSIM  standalone N.N0NN NNN.N 0.NNNN  combined N.NNNN NNN.N 0.NNNN Table N
We show the accuracy of the predicted material coefficients and the images rendered using these together with ground  truth normals and illumination
We provide results of the material  prediction module trained standalone vs with the rendering layer
 Accuracy of Surface Normals
For this evaluation,  given an input image I, we provide ground truth material (m) and illumination (L) and use our network to predict  only surface normals
We use the car category for the evaluation
The predicted surface normals along with m and L  are used to synthesize I ′
We also train the normal predic- tion module as a standalone network
While both networks  use the LN loss on the surface normals, our network combines this with the perceptual loss on I ′
 We provide qualitative and quantitative results in Figure N and Table N respectively
We observe that LN loss  helps to get the orientation of the surface normals correct,  and thus results in small errors, but produces blurry output
Adding the perceptual loss helps to preserve sharp features resulting in visually more plausible results
We also  note that, although commonly used, LN and SSIM metrics  are not fully correlated with human perception
Incorporation of the rendering layer results in slim improvements in  quantitative numbers (Table N and N) but significantly better  appearance results as shown in Figure N and N
 standalone combinedinput image  re a  l sy  n th  e ti  c  Figure N
For a synthetic and a real input image, we provide the  surface normals predicted by the normal prediction module when  trained standalone vs with the rendering layer
 NNNN    input Lombardi BaselineN BaselineN BaselineN+opt ours ours+opt GT  Figure N
For each example we show the input and the ground truth target image (GT)
We provide the results obtained by the method of  Lombardi et al
[NN], two baseline methods, and our approach
 normals rendering  cosine LN SSIM  standalone 0.NN0N NNN.N0NN 0.NNNN  combined 0.N0N0 NNN.NNNN 0.NNNN Table N
We show the accuracy of the predicted surface normals  and the images rendered using these together with ground truth  material and illumination
We provide the results of the normal  prediction module trained standalone vs with the rendering layer
 N.N
Multi-Material Examples  We show examples for objects composed of multiple materials
For these examples, we manually segment N00 chair  models to two semantic regions where each region gets its  own material assignment
We generate renderings from N  different viewpoints with random illumination and materials assignments
We fine-tune the model trained on singlematerial examples with these additional renderings
We utilize N0 chairs for fine-tuning and N0 for testing
In Figure N,  we show examples of editing each region with a target material
We also quantitatively evaluate these results with respect to ground truth over N000 examples
We achieve an LN error of NNNN.N and SSIM index of 0.NNNN
No post- optimization is used for the experiments
 input image GT ours input image GT ours  Figure N
Given an image of a multi-material object, we show  how different target material assignments are realized for different parts
We provide ground target (GT) images for reference
 N.N
Comparisons  We compare our method with several baseline methods  and previous approaches which we briefly discuss below  and refer to the supplementary material for more details
 Lombardi et al
[NN] We compare our method to the approach of Lombardi et al
[NN] which uses certain priors to  optimize for illumination and material properties in a single  image of an object with known geometry
We provide the  ground truth surface normals to this method and predict the  material and illumination coefficients
We render the target  image given the target material, ground truth normals, and  the predicted illumination
 Baseline N
Our first baseline is inspired by the work  of Kulkarni et al
[NN] and is based on an encoder-decoder  network
The encoder maps the input image to a feature  representation, f = (fm, fo), with one part corresponding to the material properties (fm) and the remaining part corresponding to other factors (fo) (i.e
normals and illumination)
This disentangling is ensured by passing fm as input  to a regression network which is trained to predict the input material coefficients
fm is then replaced by the feature  representation of the target material
The decoder utilizes  the updated feature representation to synthesize the target  image
We also provide skip connections from the input  image to the decoder to avoid blurry output
Similar to our  approach, baseline N is trained with the perceptual loss between the synthesized and ground truth target image and the  LN loss on the estimated input material coefficients
 Baseline N
For the second baseline, given the input image we train three individual networks to predict the surface normals, material, and illumination separately
All networks are trained with the LN loss
We then render the target image using the predicted surface normals, illumination,  and the target material
 For this comparison we train our method and both of  the baselines on the same training and testing set
We refine the output of baseline N and our network with the postoptimization described in Section N.N
We use N00 images sampled from the testing set as input and generate a new  image with a target material from the test set
We provide  quantitative (see Table N) and visual results (see Figure N)
 For baseline N, the use of LN loss only results in blurry  normal prediction, hence the blurry target images
While  baseline N addresses this issue by use of skip connections,  implicit feature representations fail to capture the true intrinsic properties and result in a large appearance difference  with respect to ground truth
The approach of Lombardi  et al
[NN] performs non-convex optimization over the renNNNN    input image target material Lombardi BaselineN BaselineN+opt ours ours+opt  Figure N
We transfer the target material to the input image using the method of Lombardi et al
[NN], baseline N, baseline N with postoptimization, our network, and our network with post-optimization
 Lombardi BaselineN BaselineN ours  no opt opt no opt opt  LN N0N.N N0NN.N NNN.N NNNN.N N0N.N NN0.N  SSIM 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NN0N 0.NNNN  Table N
We evaluate the accuracy of material transfer results on  synthetic data for Lombardi et al
[NN], baseline N, baseline N with  and without post-optimization, and our method with and without  post-optimization
 input image N input image Nedited image N edited image N  Figure N0
Given two input images, we synthesize new images by  swapping the material properties of the objects
 dering equation which is likely to get stuck in local minimum without a good initialization
The individual predictions from baseline N are quite far from the global minimum or any reasonable local minimum
Thus, we observe  no improvement with post-optimization, in some cases the  optimization in fact gets stuck in a worse local minimum
 Our initial network output performs in par with [NN] with  no assumption on known surface normals
Refining these  predictions with the post-optimization outperforms all other  methods significantly
 N.N
Evaluation on Real Images  We provide visual comparisons of our method with the  method of Lombardi et al
[NN] and baseline N on real product imagesN downloaded from the internet in Figure N
Since  the method of Lombardi et al
[NN] assumes surface normals to be known, we provide the normal prediction of our  network to their optimization
Additionally, we refine the  results of both baseline N (i.e
individual prediction of normal, material, and illumination) and our network with the  NFor real examples we finetune our network using all materials in our  dataset to better generalize to unseen materials
 post-optimization of Section N.N
Even though our network  has been trained only on synthetic data with a limited set  of environment maps and materials, the raw network results  are promising and provide a good initialization to the postoptimization
Independent predictions, on the other hand,  result in the post-optimization to get stuck in a local minimum which does not yield as visually plausible results
 Finally, in Figure N0, we provide material transfer examples on some real images provided by the SMASHINg  challenge dataset [NN] with the combination of our network  and the post optimization
We refer to the supplementary  material for more examples and comparisons
 N
Conclusion and Future Work  We propose an end-to-end network architecture for  image-based material editing that encapsulates the image  formation process in a rendering layer
We demonstrate  various plausible material editing results both for synthetic  and real data
One of the limitations of our work is the  fact that lighting space is learned with a relatively small  dataset, because of the lack of data
This may result in  imperfect decomposition, specifically some lighting effects  being predicted to be part of the surface normals
For multimaterial cases, estimating material for a small segment independently is difficult
For such scenarios, extending the  network to perform per-pixel material predictions will be a  promising direction
Incorporation of advanced light interactions such as subsurface scattering and inter-reflections  in the rendering layer is crucial to generalize the method to  scenes with multiple objects and real images
We expect  the performance boost obtained by the encapsulation of the  rendering layer to stimulate further research in designing  neural networks that can replicate physical processes
 Acknowledgement  We would like to thank Xin Sun, Weilun Sun, Eunbyung  Park, Chunyuan Li and Liwen Hu for helpful discussions
 This work is supported by a gift from Adobe, NSF EFRINNN0NNN, CNS-NN0NNN0 and AFOSR FANNN0-NN-N-00NN
 NNNN    References  [N] J
T
Barron and J
Malik
Shape, illumination, and reflectance from shading
IEEE PAMI, N0NN
N, N  [N] H
G
Barrow and J
M
Tenenbaum
Recovering intrinsic  scene characteristics from images
Technical Report NNN,  AI Center, SRI International, NNN Ravenswood Ave., Menlo  Park, CA NN0NN, Apr NNNN
N  [N] I
Boyadzhiev, K
Bala, S
Paris, and E
Adelson
Bandsifting decomposition for image-based material editing
 ACM TOG, NN(N):NNN:N–NNN:NN, Nov
N0NN
N  [N] A
X
Chang, T
Funkhouser, L
Guibas, P
Hanrahan,  Q
Huang, Z
Li, S
Savarese, M
Savva, S
Song, H
Su,  J
Xiao, L
Yi, and F
Yu
ShapeNet: An Information-Rich  ND Model Repository
Technical Report arXiv:NNNN.0N0NN  [cs.GR], Stanford University — Princeton University —  Toyota Technological Institute at Chicago, N0NN
N  [N] D
Eigen and R
Fergus
Predicting depth, surface normals  and semantic labels with a common multi-scale convolutional architecture
In IEEE ICCV, pages NNN0–NNNN, N0NN
 N  [N] S
Georgoulis, K
Rematas, T
Ritschel, M
Fritz, L
V
 Gool, and T
Tuytelaars
Delight-net: Decomposing reflectance maps into specular materials and natural illumination
arxiv:NN0N.0NNN0 [cs.cv], N0NN
N, N  [N] C
Innamorati, T
Ritschel, T
Weyrich, and N
J
Mitra
Decomposing single images for layered photo retouching
In  Computer Graphics Forum, volume NN, pages NN–NN
Wiley  Online Library, N0NN
N  [N] J
Johnson, A
Alahi, and L
Fei-Fei
Perceptual losses for  real-time style transfer and super-resolution
In IEEE ECCV,  N0NN
N  [N] M
K
Johnson and E
H
Adelson
Shape estimation in natural illumination
In Computer Vision and Pattern Recognition  (CVPR), pages NNNN–NNN0, N0NN
N  [N0] E
A
Khan, E
Reinhard, R
W
Fleming, and H
H
Bülthoff
 Image-based material editing
In ACM SIGGRAPH, pages  NNN–NNN, N00N
N  [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
arXiv preprint arXiv:NNNN.NNN0, N0NN
N  [NN] T
D
Kulkarni, W
F
Whitney, P
Kohli, and J
Tenenbaum
 Deep convolutional inverse graphics network
In NIPS,  pages NNNN–NNNN
N0NN
N, N, N  [NN] S
Lombardi and K
Nishino
Reflectance and natural illumination from a single image
In IEEE ECCV, pages NNN–NNN
 Springer, N0NN
N, N, N, N, N  [NN] S
Lombardi and K
Nishino
Reflectance and illumination  recovery in the wild
IEEE Transactions on Pattern Analysis  and Machine Intelligence, NN(N):NNN–NNN, Jan N0NN
N  [NN] M
M
Loper and M
J
Black
OpenDR: An approximate  differentiable renderer
In Computer Vision – ECCV N0NN,  volume NNNN of Lecture Notes in Computer Science, pages  NNN–NNN
Springer International Publishing, Sept
N0NN
N  [NN] W
Matusik, H
Pfister, M
Brand, and L
McMillan
A datadriven reflectance model
ACM Transactions on Graphics,  NN(N):NNN–NNN, July N00N
N, N  [NN] T
Narihira, M
Maire, and S
X
Yu
Learning lightness from  human judgement on relative reflectance
In IEEE CVPR,  June N0NN
N  [NN] F
E
Nicodemus, J
C
Richmond, J
J
Hsia, I
W
Ginsberg,  and T
Limperis
Radiometry
chapter Geometrical Considerations and Nomenclature for Reflectance, pages NN–NNN
 Jones and Bartlett Publishers, Inc., USA, NNNN
N  [NN] K
Nishino
Directional statistics brdf model
In N00N IEEE  NNth International Conference on Computer Vision, pages  NNN–NNN
IEEE, N00N
N  [N0] G
Patow and X
Pueyo
A survey of inverse rendering problems
In CGF, volume NN, pages NNN–NNN
Wiley Online  Library, N00N
N  [NN] K
Rematas, T
Ritschel, M
Fritz, E
Gavves, and T
Tuytelaars
Deep reflectance maps
In CVPR, N0NN
N, N  [NN] S
R
Richter and S
Roth
Discriminative shape from shading  in uncalibrated illumination
In IEEE CVPR, pages NNNN–  NNNN, June N0NN
N  [NN] J
Shi, Y
Dong, H
Su, and S
X
Yu
Learning nonlambertian object intrinsics across shapenet categories
arXiv  preprint arXiv:NNNN.0NNN0, N0NN
N  [NN] M
M
Takuya Narihira and S
X
Yu
Direct intrinsics:  Learning albedo-shading decomposition by convolutional regression
In International Conference on Computer Vision  (ICCV), N0NN
N  [NN] Y
Tang, R
Salakhutdinov, and G
Hinton
Deep lambertian  networks
In ICML, N0NN
N  [NN] Z
Wang, A
C
Bovik, H
R
Sheikh, and E
P
Simoncelli
 Image quality assessment: from error visibility to structural  similarity
IEEE Trans
on Image Proc., NN(N):N00–NNN,  N00N
N  [NN] L
F
Yu, S
K
Yeung, Y
W
Tai, and S
Lin
Shadingbased shape refinement of rgb-d images
In Computer Vision  and Pattern Recognition (CVPR), N0NN IEEE Conference on,  pages NNNN–NNNN, June N0NN
N  [NN] R
Zhang, P.-S
Tsai, J
E
Cryer, and M
Shah
Shape from  shading: A survey
IEEE PAMI, NN(N):NN0–N0N, NNNN
N  [NN] T
Zhou, P
Krhenbhl, and A
A
Efros
Learning data-driven  reflectance priors for intrinsic image decomposition
In IEEE  ICCV, pages NNNN–NNNN, N0NN
N  NNNNGANs for Biological Image Synthesis   GANs for Biological Image Synthesis  Anton Osokin  INRIA/ENS∗, France  HSE†, Russia  Anatole Chessel  École Polytechnique‡,  France  Rafael E
Carazo Salas  University of Bristol, UK  Federico Vaggi  ENS∗, France  Amazon, USA  Abstract  In this paper, we propose a novel application of Generative Adversarial Networks (GAN) to the synthesis of cells  imaged by fluorescence microscopy
Compared to natural  images, cells tend to have a simpler and more geometric  global structure that facilitates image generation
However,  the correlation between the spatial pattern of different fluorescent proteins reflects important biological functions, and  synthesized images have to capture these relationships to be  relevant for biological applications
We adapt GANs to the  task at hand and propose new models with casual dependencies between image channels that can generate multichannel images, which would be impossible to obtain experimentally
We evaluate our approach using two independent  techniques and compare it against sensible baselines
Finally, we demonstrate that by interpolating across the latent  space we can mimic the known changes in protein localization that occur through time during the cell cycle, allowing  us to predict temporal evolution from static images
 N
Introduction  In the life sciences, the last N0 years saw the rise of light  fluorescence microscopy as a powerful way to probe biological events in living cells and organisms with unprecedented  resolution
The need to analyze quantitatively this deluge of  data has given rise to the field of bioimage informatics [NN]  and is the source of numerous interesting and novel data  analysis problems, which current machine learning developments could, in principle, help solve
 Generative models of natural images are among the most  long-standing and challenging goals in computer vision
 Recently, the community has made significant progress in  this task by adopting neural network machinery
Examples  of recent models include denoising autoencoders [N], variational autoencoders [N0], PixelCNNs [NN] and Generative  Adversarial Networks (GANs) [NN]
 ∗DI École normale supérieure, CNRS, PSL Research University, Paris †National Research University Higher School of Economics, Moscow ‡LOB, École Polytechnique, CNRS, INSERM, Université Paris-Saclay  AlpNN  ArpN  CkiN  MkhN  SidN  TeaN  Proteins Real  images Generated images  Figure N
Real (left) and generated (right) images of fission yeast  cells with protein BgsN depicted in the red channel and N other  proteins depicted in the green channel
The synthetic images were  generated with our star-shaped GAN
The star-shaped model can  generate multiple green channels aligned with the same red channel whereas the training images have only one green channel
 GANs [NN] are family of successful models, which have  recently received widespread attention
Unlike most other  generative models, GANs do not rely on training objectives  connected to the log likelihood
Instead, GAN training can  be seen as a minimax game between two models: the generator aims to output images similar to the training set given  random noise; while the discriminator aims to distinguish  the output of the generator from the training set
 Originally, GANs were applied to the MNIST dataset  of handwritten digits [NN, NN]
The consequent DCGAN  model [NN] was applied to the CelebA dataset [NN] of human faces, the LSUN [NN, NN] and ImageNet [N] datasets  of natural images
We are not aware of any works applying  GANs to biological images
 We work with a recently created bioimage dataset used  to extract functional relationships between proteins, called  the LIN dataset [N] comprising NN0,000 fluorescence microscopy images of cells
In the LIN dataset, each image corresponds to a cell and is composed of signals from  two independent fluorescence imaging channels (“red” and  “green”), corresponding to the two different proteins tagged  with red or green-emitting fluorophores, respectively
 NNNNN    In the LIN dataset, the red channel signal always corresponds to a protein named BgsN, which localizes to the  areas of active growth of cells
The green channel signal instead corresponds to any of NN different “polarity factors”,  that is proteins that mark specific areas of the cells’ cortex  that help define a cell’s geometry
Polarity factors include  proteins like AlpNN, ArpN, CkiN, MkhN, SidN or TeaN  (see Figure N for image examples), each of which controls  the same biological process “cellular polarity” albeit each  in a slightly different way
Each of the green-labeled polarity factors was imaged independently of the others
The  biological aim of the LIN study is to investigate how those  polarity factors (or proteins) interact with one another
 In this paper, we present a novel application of GANs to  generate biological images
Specifically, we want to tackle  two concrete limitations of large scale fluorescent imaging  screens: we want to use the common information contained  in the red channel to learn how to generate a cell with several of the green-labeled proteins together
This would allow us to artificially predict how the localizations of those  (independently imaged) proteins might co-exist in cells if  they had been imaged together and circumvent the current  technical limitations of being able to only image a limited  number of signal channels at the same time
Second, taking  advantage of the relationship between BgsN and the cell  cycle stage, we want to study the dynamical changes in cellular localization that proteins undergo through time as cells  grow and divide
 To accomplish this, we make several contributions
We  modify the standard DCGAN [NN] architecture by substituting the interdependence of the channels with the causal  dependence of the green on the red, allowing us to observe  multiple modes of green signal for a single red setting
Observing the mode collapse effect of GANs [N0, NN] for our  separable architecture, we incorporate the recent Wasserstein GAN (WGAN-GP) objective [N, NN]
We propose  two approaches to generate multi-channel images: regular  WGAN-GP trained on multi-channel images, where extra  channels for training are mined by nearest neighbor search  in the training set, and a novel star-shaped generator trained  directly one the two-channel images
We carefully evaluate our models using two quantitative techniques: the neural network two-sample test (combining ideas from [NN]  and [NN]) and by reconstructing samples in a held out test set  with the optimization approach of [N0]
For reproducibility,  we make the source code and data available online.N  This paper is organized as follows
In Section N, we discuss related works
Section N reviews the relevant biological background for our application
In Section N, we review  GANs and present our modeling contributions
We present  the experimental evaluation in Section N and conclude in  Section N
 Nhttps://github.com/aosokin/biogans  N
Related Work  Generative Adversarial networks (GANs)
Since the  seminal paper by Goodfellow et al
[NN] of N0NN (see  also [NN] for a detailed review), GANs are becoming an increasingly popular model for learning to generate with the  loss functions learned jointly with the model itself
Models with adversarial losses have been used in a wide range  of applications, such as image generation [N, NN], domain  adaptation [NN], text-to-image synthesis [NN], synthesis of  ND shapes [NN] and texture [NN], image-to-image translation [NN], image super resolution [NN] and even generating  radiation patterns in particle physics [N]
However, these  models suffer from issues such as mode collapse and oscillations during training, making them challenging to use in  practice
The community is currently tackling these problems from multiple angles
Extensive effort has been placed  on carefully optimizing the architecture of the network  [NN, N0] and developing best practices to optimize the training procedureN
Another active area of research is improving  the training objective function [NN, N, NN, NN, N, N0, NN, NN]
 In this paper, we build on the DCGAN architecture [NN]  combined with the Wasserstein loss [N, NN], where the latter is used to help with the mode collapse issue, appearing  especially in our separable setting
 Conditioning for GANs
Starting from conditioning on  the class labels [NN, N, NN, NN], researchers have extended  conditioning to user scribbles [NN] and images [NN, NN, NN]
 While the quality of images generated by [NN, NN, NN] is  high, their models suffer from conditional mode collapse,  i.e., given the first (source) image there is very little or no  variety in the second (target image)
This effect might be  related to the fact that the dataset contained only one target image available for each source image, so the model  has only indirect supervision for generating multiple conditioned images
We have applied the pixNpix method of [NN]  to the LIN dataset and it learned to produce high-quality  green images given the red input
However, it was unable to  generate multiple realistic green images for one red input
 Given the difficulty in learning robust latent spaces when  conditioning on an image, we opted for an alternate approach
We propose a new architecture for the generator,  where the red channel and green channels are given independent random noise, and only the red channel is allowed  to influence the green channel, see Figure N (right)
 Factors of variation
Chen et al
[N] and Mathieu et  al
[NN] used unsupervised methods that encourage disentangling factors of variation in the learned latent spaces,  e.g., separating the numerical value of a handwritten digit  from its writing style
In contrast to these works, we do not  rely on unsupervised training to discover factors of variations, but explicitly embed the separation into the model
Nhttps://github.com/soumith/ganhacks  NNNN  https://github.com/aosokin/biogans https://github.com/soumith/ganhacks   Analysis and synthesis of biological images
With large  scale imaging studies becoming more common in biology,  the automated analysis of images is now crucial in many  studies to prove the existence of an effect, process large  datasets or link with models and simulation [NN, N]
Although the field has only recently embraced deep learning,  neural networks are now starting to make a splash, mainly  in classical discriminative settings [NN]
 While, to our knowledge, this work is the first reported  use of GANs on samples from fluorescent microscopy, generative models have been widely used in biology [NN]
For  example, Johnson et al [NN] learned to generate punctuate  patterns in cells (conditional on microtubule localization)  showing the potential of those methods in studying the relative sub-cellular positions of several proteins of interest
 Recently, sharing of large biological datasets has greatly  improved [NN]
Further, EBI has made a large investment  to develop the IDR (Image Data Resource) [NN], a database  built on top of open source tools to facilitate the sharing of  terabyte sized datasets with complex metadata
 N
Biological Background  N.N
Fluorescent Imaging  Fluorescence microscopy is based on fluorescent compounds, i.e., compounds which can absorb light at given  wavelength (the absorption spectrum) and re-emit it almost  immediately at a slightly different wavelength (the emission spectrum)
In the case of fluorescent proteins (FPs),  of which the Green Fluorescent Protein (GFP) [N, NN] is the  first and most widely used one, the fluorescing compound  is attached to the protein of interest via genetic engineering
Many FPs of various absorption and emission spectra  exist, e.g., Red Fluorescent Protein (RFP) [NN]
By genetically tagging different proteins of interest with FPs of different color, one can image them in the same cell at the  same time and thus investigate their co-localization
However, the number of proteins that can be tagged and imaged  at the same time is limited to N-N due to the limited number  of FPs with non-overlapping absorption spectra
 Multi-channel fluorescent images are very different from  natural images
In natural images, color is determined by  the illumination and the properties of a particular material  in the scene
In order to generate realistic natural samples,  a GAN must capture the relationship between the materials  that make up a particular object and its hues
In contrast, in  fluorescent images, the intensity of light in a given channel  corresponds to the local concentration of the tagged protein, and the correlation between signals in different channels represents important information about the relationship  between proteins, but the color does not reflect any intrinsic  property about the protein itself
 N.N
Fission Yeast Cells  Fission yeast (Schizosaccharomyces pombe) cells are rod  shaped unicellular eukaryotes with spherical hemisphere  caps
They are born N µm long and N µm wide, and grow  in length to NN µm while maintaining their width constant
 Newly born fission yeast cells start by growing only at the  pre-existing end until they reach a critical size, and then  switch to bipolar (from the two sides) growth
Bipolar  growth continues until cells reach their final length, when  they stop growing and start to form a cytokinetic ring in the  middle, which is responsible for cleaving the mother cells  into two daughters [NN]
Interestingly, for most of the cell  cycle the length of the cell is a good proxy for its “age”, i.e
 the time it has spent growing since its “birth”
 BgsN, the protein tagged in the red channel, is responsible for cell wall remodeling, and localizes to areas of active  growth (see Figure N for examples of images)
Thus, by observing BgsN, one can accurately infer growth cycle stage,  and predict where cell growth is occurring
 N.N
The LIN Dataset  All experiments in this paper make use of a recent dataset  of images of fission yeast cells, which was originally produced to study polarity networks [N]
The LIN dataset consists of around NN0,000 of images, with each image being centered on one cell; cell segmentation was performed  separately (see [N] for details) and the corresponding outline is also available
Each image is a ND stack of ND  images where each pixel correspond to a physical size of  N00nm; each z-plane is distant by N00nm
Every image is  composed of two channels, informally called the “red” and  the “green”, where light emitted at a precise wavelength is  recorded
In this dataset two types of fluorescent-tagged  proteins are used: BgsN in the red channel, and one of NN  different polarity regulating proteins in the green channel
 A full description of all tagged proteins is beyond the scope  of this paper: we refer interested readers to [NN, N]
 In this paper, we concentrate on a subset of N different  polarity factors, spanning a large set of different cellular localizations
This gives us NN,N0N images of cell, which we,  for simplicity, center crop and resize to resolution of NN×N0
 N
GANs for Image Generation  N.N
Preliminaries  GAN
The framework of generative adversarial networks [NN, NN] is formulated as a minimax two-player game  between two neural networks: generator and discriminator
The generator constructs images given random noise  whereas the discriminator tries to classify if its input image  is real (from the training set) or fake (from the generator)
 The goal of the generator is to trick the discriminator, such  NNNN    that it cannot easily classify
The discriminator is often referred to as the adversarial loss for training the generator
 More formally, consider a data-generating distribution IPd and a training set of images x ∈ X coming from it
The generator G(z; θG) is a neural network parameter- ized by θG that takes random noise z from distribution IPz as input and produces an image xfake ∈ X 
The discrimina- tor D(x; θD) is a neural network parameterized by θD that takes either a training image x or a generated image xfake and outputs a number in the segment [0, N], where zero is associated with fake images and one – with the real images
 As introduced in [NN], the key quantity is the negative crossentropy loss on the discriminator output:  L(θD, θG) = IEx∼IPdata logD(x; θD)  + IEz∼IPz log(N−D(G(z; θG); θD))
(N)  The discriminator maximizes (N) w.r.t
θD and the generator, at the same time, minimizes (N) w.r.t
θG
In practice,  both optimization tasks are attacked simultaneously by alternating between the steps of the two optimizers
 As noted by [NN], the objective log(N − D(G(z; θG); θD)) often leads to saturated gradients at the initial stages of the training process when the  generator is ineffective, i.e., its samples are easy to  discriminate from the real data
One practical trick to  avoid saturated gradients is to train the generator with  maximizing logD(G(z; θG); θD) instead
Goodfellow et al
[NN] showed that the minimax formulation (N) can be reformulated via minimization of the JensenShannon (JS) divergenceN between the data-generating distribution IPd and the distribution IPG induced by IPz and G
For the architectures of both the generator and the discriminator, we largely reuse a successful version of Radford  et al
[NN] called DCGAN
The generator of DCGAN (see  Figure N, left) is based on up-convolutions [N0] interleaved  with ReLu non-linearity and batch-normalization [NN]
We  refer to [NN] for additional details
 Wasserstein GAN
Recently, Arjovsky et al
[N] have  demonstrated that in some cases the JS divergence behaves  badly and cannot provide any useful direction for training,  e.g., when it is discontinuous
To overcome these degeneracies, they consider the earth mover’s distance (equivalent to  the N-st Wasserstein distance) between the distributions  W (IPd, IPG) = inf IP∈Π(IPd,IPG)  IE(x,x′)∼IP‖x− y‖, (N)  where set Π(IPd, IPG) is a set of all joint distributions IP on x and x′ whose marginals are IPd and IPG, respectively
Intuitively, the distance (N) indicates the cost of the optimal  movement of the probability mass from IPd to IPG
Accord- ing to [N] by using duality, one can rewrite (N) as  NThe Jensen-Shannon divergence is a symmetrized version of  the Kullback-Leibler divergence between the two distributions, i.e.,  JS(IPd, IPG) = KL(IPd‖IPG) +KL(IPG‖IPd)
 upconv, batchnorm  ReLu  upconv, batchnorm  ReLu  upconv, batchnorm  ReLu  concat  Gaussian noise  generated   images  DCGAN generator separable generator  concat  concat  concat  upconv, tanh  upconv, batchnorm  ReLu  Figure N
Architectures of the DCGAN generator (left) and our  separable generator (right)
 W (IPd, IPG)= sup D∈CN  (  IEx∼IPdD(x)−IEx′∼IPGD(x ′) )  , (N)  where CN is the set of all N-Lipschitz functions D : X → R
Optimizing w.r.t
the set CN is complicated
As a practical approximation to the set of all N-Lipschitz functions, Arjovsky et al
[N] suggest to use neural networks D(x; θD) with all parameters θD clipped to a fixed segment
Very recently, Gulrajani et al
[NN] proposed a surrogate objective  to (N), which is based on the LN-distance between the norm  of the discriminator gradient at specific points and one
In  all, we arrive at the minimax game  W (θD, θG) = IEz∼IPzD(G(z; θG); θD)  − IEx∼IPdataD(x; θD) +R(θD), (N)  where R is the regularizer (see [NN] for details)
The objective (N) is very similar to the original game of GANs (N),  but has better convergence properties
In what follows, we  refer to the method of [NN] as WGAN-GP
 N.N
Model Extensions  In this section, we present our modeling contributions
 First, we describe our approach to separate the red and green  channels of the generator
Second, we discuss a way to train  a multi-channel generator using the two-channel data in the  LIN dataset
Finally, we propose a new star-shaped architecture that uses the red-green channel separation to obtain  multiple channels in the output
 Channel separation
The key idea of the channel separation consists in separating the filters of all the upconvolutional layers and the corresponding features into  NNNN    two halves
The first set of filters is responsible for generating the red channel, while the second half generates the  green channel
To make sure the green channel matches the  red one, we use one way connections from the red convolutional filters towards the green ones
Figure N (right) depicts  our modification in comparison to DCGAN (left)
 Multi-channel models
The LIN dataset [N] contains  only two-channel images, the red and one type of the green  at a time
Obtaining up to N channels simultaneously from  a set of N0 proteins (a fixed red and N greens) would require  the creation of nearly N0,000 yeast strains
Scaling even  higher is currently impossible with this imaging technique  due to the limited number of FPs with non-overlapping absorption spectra
Because of these constraints, training the  generator only on a subset of channels is a task of practical importance
The first approach we present consists in  training a multi-channel GAN using an artificial training set  of multi-channel images created from the real two-channel  images
We proceed as follows: for each two-channel image, we search in every other class for its nearest-neighbors  (using LN-distance) in the red channel
Then, we create a  new sample by combining the original image with the green  channels of its nearest neighbors in other classes
 We can then use this dataset to train a multi-output DCGAN
The only difference in the architecture is that the generator outputs c+N channels, where c is the number of green channels used in the experiment, and the discriminator takes  (c+ N)-channel images as input
 Star-shaped model
In our experiments, the multichannel approach did not perform well, because, even using  the nearest neighbors, the extra greens channels were not  exactly consistent with the original red signal, emphasizing  the importance of correlations between channels
 To overcome this effect, we propose a star-shaped architecture for the generator, consisting of a single red tower  (a stack of upconvolutional layers with non-linearities inbetween) that feeds into c green towers (see Figure N, right)
 Unlike the multi-channel model described above, the green  outputs are independent conditioned on the red
Thus, the  model can be trained using the existing two-channel images
 In our experiments, we found it important to use batch  normalization [NN] in the red tower only once, compared to  a more naive way of c times
The latter leads to interference between several normalizations of the same features  and prevents convergence of the training scheme
 After the forward pass, we use c discriminators attached  to different versions of the greens, all paired with the same  generated red
For the WGAN-GP version of this model,  we apply the original procedure of [NN] with the modification that during the discriminator update we simultaneously  update all c discriminators, and the generator receives back  the accumulated gradient
 N
Experiments  Evaluating generative models is in general non-trivial
In  the context of GANs and other likelihood-free approaches,  evaluation is even harder, because the models do not provide a way to compute the log-likelihood on the test set,  which is the most common evaluation technique
Recently,  a number of techniques applicable to evaluating GANs have  been proposed [NN, N0, N0]
Among those, we chose the following two: the neural-network two-sampled test discussed  by [NN] combined with the surrogates of the earth mover’s  distance [N, NN] and an optimization-based approach of [N0]  to check if the test samples can be well reconstructed
We  modify these techniques to match our needs and check their  performance using sensible baselines (Sections N.N and N.N)
 Finally, in Section N.N, we show the cell growth cycle generated with our star-shaped model
 N.N
Neural-network Two-sample Test  Lopez-Paz and Oquab [NN] have recently applied the  classifier two-sample test (CNST) to evaluate the quality of  GAN models
A trained generator is evaluated on a heldout test set
This test test is split again into a test-train and  test-test subsets
The test-train set is then used to train a  fresh discriminator, which tries to distinguish fake images  (from the generator) from the real images
Afterwards, the  final measure of the quality of the generator is computed as  the performance of the new discriminator on the test-test set  and the freshly generated images
 When CNST is applied for images, the discriminator is  usually a ConvNet, but even very small ConvNets can discriminate between fake and real images almost perfectly
To  obtain a useful measure, Lopez-Paz and Oquab [NN] deliberately weaken the ConvNet by fixing some of its parameters  to the values obtained by pre-training on ImageNet
 ImageNet-based features are clearly not suitable for LIN  cell images, so we weaken the discriminator in another way
 We use the negation of the WGAN-GP [NN] discriminator  objective as a surrogate to the earth mover’s distance
Similar to [NN], we train this discriminator on the test-train subset and compute the final estimates on the test-test subset
 For all the runs, we repeat the experiment on N0 different  random splits of the test set and train the discriminator for  N000 steps with the optimizer used by [NN]
For the experiments involving multi-channel generators, we train a separate discriminator for each green channel paired with the  red channel
 In our experiments, the training procedure occasionally  failed and produced large outliers
To be more robust, we  always report a median over N0 random splits together with  the median absolute deviation to represent the variance
In  the Suppl
Mat
[NN], we additionally quantitatively and  qualitatively compare the WGAN-GP [NN], WGAN [N] and  cross-entropy discriminators used in CNST
 NNNN    Steps CNST Samples  se p  
G  A N  Nk NN.0± 0.N  Nk N.N± 0.N  N0k N.N± 0.N  se p  
W  G A  N -G  P Nk N.0± 0.N  Nk N.N± 0.N  N0k N.N± 0.N  R ea  l  - −0.N  ±0.N  Figure N
Scores of the classifier two-sample test (CNST) between  the generators and the hold-out test sets of images
We report the  scores of separable GAN and WGAN-GP at different stages of  training
For each line, we show the samples from the corresponding models to demonstrate that the lower CNST scores correspond  to better-looking (sharper, less artifacts, etc.) images
Best viewed  in color and on a screen
An extended version of this figure is  given in the Suppl
Mat
[NN]
 Sanity checks of the two-sample test
We evaluate CNST  in two baseline settings
First, we compare the separable GAN [NN] and the WGAN-GP [NN] models (based on  the same DCGAN architecture, trained on the same set  of images of N proteins) at different stages of the training  process
For each of these models, we also show qualitative difference between the generated images
Figure N  shows that along the training process, quality of both GAN  and WGAN-GP improves, i.e., generated images become  sharper and contain less artifacts, consistent with the CNST  score
To better visualize the difference between the trained  GAN and WGAN-GP models, in Figure N, we show multiple samples of the green channel corresponding to the same  red channel
We see that the CNST evaluation captures several aspects of the visual quality (such as sharpness, correct  shape, absence of artifacts, diversity of samples) and provides a meaningful score
 From Figures N and N, we also conclude that the quality  of GAN samples is worse than the quality of WGAN-GP  according to visual inspection
CNST (based on WGANGP) confirms this observation, which is not surprising given  that WGAN-GP was trained using the same methodology
 Surprisingly, when evaluated with the cross-entropy CNST,  WGAN-GP also performs better than GAN (see Suppl
Mat
 [NN] for details)
 As the second baseline evaluation, we use CNST to compare real images of different classes
Table N shows that  when evaluated w.r.t
the test set of the same class the estiSamples from separable models  se p  ar ab  le G  A N  se p  ar ab  le W  G A  N -G  P  Figure N
Samples generated by separable GAN (top) and WGANGP (bottom) models trained on the N selected proteins shown in  Figure N
Each row has samples with identical red channel, but  different green ones
We observe that WGAN-GP provides much  larger variability of the green channel conditioned on the red
In  particular, in the three bottom rows, even the type of the protein  changes, which we have never observed for the samples of GAN  (this effect should be present, because the model is trained without  any distinction between the classes, but is surprisingly rare)
This  difference is captured by the CNST evaluation: the GAN model  has the score of N.N± 0.N compared to N.N± 0.N of WGAN-GP
 mates are significantly smaller (but with non-zero variance)  compared to when evaluated w.r.t
different classes
Note  that the CNST score is not a metric
In particular, Table N is  not symmetric reflecting biases between the train/test splits
 Specifically to WGAN-GP, the score can also be negative,  because the quadratic regularization term is the dominant  part of the objective (N) when the two image sources are  very similar
 As an additional test, we include two extra proteins  FimN and TeaN that are known to have similar localization  to ArpN and TeaN, respectively
We observe that CNST reflects this similarity by giving the pairs of similar proteins  a much smaller score compared to most of other pairs (but  still significantly higher than comparing a protein to itself)
 Results
Table N shows the results of CNST applied to several models with multiple output channels (see Section N.N):  the multi-channel model and its separable version, the starshaped model and the two baselines, which do not align  NNNN    test AlpNN ArpN CkiN MkhN SidN TeaN FimN TeaN  AlpNN 0.N ± 0.N NN.N ± 0.N N.N ± 0.N NN.N ± 0.N N.N ± 0.N N0.N ± 0.N NN.N ± 0.N NN.N ± 0.N ArpN NN.N ± 0.N 0.N ± 0.N NN.N ± 0.N NN.N ± 0.N N0.N ± 0.N NN.N ± 0.N N.N ± 0.N NN.N ± 0.N CkiN N.N ± 0.N NN.N ± 0.N -0.N ± 0.N NN.N ± 0.N NN.0 ± 0.N NN.N ± 0.N NN.N ± 0.N NN.0 ± 0.N MkhN NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N -0.N ± 0.N NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N SidN N.0 ± 0.N NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N -0.N ± 0.N NN.N ± 0.N NN.N ± 0.N N.N ± 0.N TeaN NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N -0.N ± 0.N NN.N ± 0.N N.N ± 0.N FimN NN.N ± 0.N N.N ± 0.N NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N -0.N ± 0.N N0.N ± 0.N TeaN N.N ± 0.N NN.N ± 0.N NN.0 ± 0.N NN.N ± 0.N N.N ± 0.N N.N ± 0.N NN.N ± 0.N -0.N ± 0.N  Table N
Results of CNST with WGAN-GP when comparing real images of different proteins
For each run, the training images of one  class are evaluated w.r.t
the test images of another class
The reported values are comparable with Table N, but not with Figure N
 real images one-class  non-separable  one-class  separable  multi-channel  non-separable  multi-channel  separable star-shaped  separable red/green - ✗ ✓ ✗ ✓ ✓  class conditioned - ✗ ✗ ✓ ✓ ✓  AlpNN 0.N ± 0.N 0.N ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N 0.N ± 0.N ArpN 0.N ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N CkiN -0.N ± 0.N 0.N ± 0.N N.0 ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N MkhN -0.N ± 0.N 0.N ± 0.N 0.N ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N SidN -0.N ± 0.N 0.N ± 0.N N.0 ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N TeaN -0.N ± 0.N 0.N ± 0.N 0.N ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N N proteins -0.N ± 0.N 0.N ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N N.N ± 0.N  Table N
Results of CNST with the WGAN-GP objective comparing several multi-channel models w.r.t
the real images
All the models  were trained with WGAN-GP
The values in this table are directly comparable to the ones in Table N
 green channels of different classes with the same red channel: one-class generators trained individually for each class  and their separable versions
All the models were trained  with WGAN-GP with the same ratio of the width of the  generator tower to the number of output channels
 We observe that the individual one-class WGAN-GP  models lead to higher quality compared to all the models outputting synchronized channels for all the classes
 Among the models that synchronize channels, the starshaped model performs best, but for some proteins there is  a significant drop in quality w.r.t
the one-class models
 N.N
Optimization to Reconstruct the Test Set  One of the common failures of GANs is the loss of  modes from the distribution, usually referred to as mode  collapse
There is evidence [NN] that image quality can be  inversely correlated with mode coverage
To test for the  mode collapse, we perform an experiment proposed in [N0],  where for a fixed trained generator G we examine how well  it can reconstruct images from a held out test set
For each  image in the test set, we minimize the LN-distance (normalized by the number of pixels) between the generated and test  images w.r.t
the noise vector z
We call this task regular reconstruction
We use N0 iterations of L-BFGS and run it N  times to select the best reconstruction
We also performed  an additional task, separable reconstruction, which examines the ability of separable networks to reproduce modes  of the green channel conditioned on the red
In this task, we  use a two-step procedure: first, we minimize the LN-error  between the red channels holding the green noise fixed, and  then we minimize the LN-error in the green channel while  keeping the red noise fixed at it’s optimized value
To complete the study, we also report the negative log likelihood  (NLL) w.r.t
the prior IPz of the noise vectors z obtained with a reconstruction procedure
As a baseline for the reconstruction error, we show the nearest neighbor cell (in  both red and green channels) from the training set and the  average LN-distance to the nearest neighbors
As a baseline  for NLL, we show the averaged NLL for the random point  generated from IPz 
 We apply the reconstruction procedure to evaluate four  models: separable one-class and star-shaped models trained  with both GAN and WGAN-GP algorithms
Figure N and  Table N present qualitative and quantitative results, respectively
For all the measurements, we report the median values and the median absolute deviation
In Figure N, we plot  reconstruction errors vs
NLL values for the MkhN, which  was the hardest protein in the separable reconstruction task
 Analyzing the results, we observe that separable reconstruction is a harder task than the single step procedure
 Second, WGAN-GP models can reconstruct better, probably because they suffer less from the mode collapse
And  finally, the star-shaped models do not degrade the performance in terms of reconstruction, except for some hard proteins (see more details in the Suppl
Mat
[NN])
 NNNN    (a) (b) (c) (d) (e) (f)  Figure N
Examples of cell reconstructions
(a) – a test image;  (b) – the LN nearest neighbor; (c) – regular reconstruction by oneclass separable WGAN-GP; (d) – regular reconstruction by starshaped WGAN-GP; (e) – separable reconstruction by star-shaped  WGAN-GP; (e) – separable reconstruction by star-shaped GAN
 An extended version of this figure is given in the Suppl
Mat
[NN]
 LN-error NLL  Nearest neighbors 0.0NN± 0.00N - Gaussian noise - NNN± N  re g  u la  r  GAN-sep 0.0NN± 0.00N NNN± NN WGAN-GP-sep 0.0NN± 0.00N NNN± N GAN-star 0.0NN± 0.00N NNN± NN WGAN-GP-star 0.0NN± 0.00N NN0± N  se p  ar ab  le GAN-sep 0.0NN± 0.0NN NNN± NN WGAN-GP-sep 0.0NN± 0.00N NNN± N GAN-star 0.0NN± 0.0NN NNN± N WGAN-GP-star 0.0NN± 0.0N0 NNN± N  Table N
Reconstruction experiment
For the four trained models (GAN/WGAN-GP and separable one-class/star-shaped), we  report LN-errors of the reconstructions and the negative log likelihoods (NLL) of the latent vectors found by the reconstruction
 N.N
Progression Through the Cell Cycle  As described in Section N.N, the localization of BgsN can  be used to accurately pinpoint the cell cycle stage
However,  not nearly as much as is known about how the localization  of the other proteins changes within the cell cycle [NN]
 Using our separable GAN architecture, we can interpolate between points in the latent space [NN] to move across  the different stages of growth and division
Due to the architecture of our network, the output of the green channel  will always remain consistent with the red output
We show  an example of the reconstructed cell cycle in Figure N and  several animated examples in the Suppl
Mat
[NN]
As a  validation of our approach, ArpN is seen gradually moving  a dot like pattern at the tips of the cell towards the middle  of the cell during mitosis, as has been previously described  in the literature [NN]
 It’s important to highlight that the LIN dataset lacks true  multi-channel (N+) images, and as such, we are unable to  compare how our generated multi-channel images compare  to real fluorescent images
We hope that as more datasets  0.0NN 0.0N0 0.0NN 0.N00 0.NNN LN-error  N00  NN0  N00  NN0  N00  NN0  N00  N L L  GAN-sep  WGAN-GP-sep  GAN-star  WGAN-GP-star  0.0NN 0.0N0 0.0NN 0.N00 0.NNN LN-error  N00  NN0  N00  NN0  N00  NN0  N00  N L L  GAN-sep  WGAN-GP-sep  GAN-star  WGAN-GP-star  (a) regular reconstruction (b) separable reconstruction  Figure N
Reconstruction errors against negative log likelihood  (NLL) of the latent vectors found by reconstruction
We show all  the cells corresponding to protein MkhN, which appears to be the  hardest for the star-shaped models
The vertical gray line shows  the median LN-error of the nearest neighbor
Horizontal gray lines  show mean NLL (± N std) of the noise sampled from the Gaussian  prior
In the separable (red-first) setting, the star-shaped model  trained with GAN provides very bad reconstructions, whereas the  same model trained with WGAN-GP results in high NLL values
 An extended version of this figure is given in the Suppl
Mat
[NN]
 BgsN AlpNN ArpN CkiN MkhN SidN TeaN                                    y  Figure N
Cell cycle of a star-shaped WGAN-GP model
 in biology become open, we will have a better baseline to  compare our model too
 N
Conclusion  Although generative modeling has seen an explosion in  popularity in the last couple of years, so far it has mostly  been applied to the synthesis of real world images
Our results in this paper suggest that modern generative models  can be fruitfully applied to images obtained by fluorescent  microscopy
By leveraging correlation between different  image channels, we were able to simulate the localization  of multiple proteins throughout the cell cycle
This could  enable in the future the exploration of uninvestigated, inaccessible or unaffordable biological/biomedical experiments,  to catalyze new discoveries and potentially enable new diagnostic and prognostic bioimaging applications
 Acknowledgements
A
Osokin was supported by the ERC  grant Activia (no
N0NNNN)
F
Vaggi was supported by a  grant from the chair Havas-Dauphine
 NNN0    References  [N] M
Arjovsky, S
Chintala, and L
Bottou
Wasserstein generative adversarial networks
In ICML, N0NN
N, N, N  [N] Y
Bengio, L
Yao, G
Alain, and P
Vincent
Generalized denoising auto-encoders as generative models
In NIPS, N0NN
 N  [N] M
Chalfie, Y
Tu, G
Euskirchen, W
W
Ward, and D
C
 Prasher
Green fluorescent protein as a marker for gene expression
Science, pages N0N–N0N, NNNN
N  [N] X
Chen, Y
Duan, R
Houthooft, J
Schulman, I
Sutskever,  and P
Abbeel
InfoGAN: Interpretable representation learning by information maximizing generative adversarial nets
 In NIPS, N0NN
N  [N] A
Chessel
An overview of data science uses in bioimage  informatics
Methods, N0NN
N  [N] L
de Oliveira, M
Paganini, and B
Nachman
Learning particle physics by example: Location-aware generative adversarial networks for physics synthesis
arXiv:NN0N.0NNNNvN,  N0NN
N  [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
ImageNet: A large-scale hierarchical image database
 In CVPR, N00N
N  [N] E
L
Denton, S
Chintala, A
Szlam, and R
Fergus
Deep  generative image models using a Laplacian pyramid of adversarial networks
In NIPS, N0NN
N  [N] J
Dodgson, A
Chessel, F
Vaggi, M
Giordan, M
Yamamoto, K
Arai, M
Madrid, M
Geymonat, J
F
Abenza,  J
Cansado, M
Sato, A
Csikasz-Nagy, and R
E
Carazo  Salas
Reconstructing regulatory pathways by systematically mapping protein localization interdependency networks
bioRxiv:NNNNN, N0NN
N, N, N  [N0] A
Dosovitskiy, J
T
Springenberg, M
Tatarchenko, and  T
Brox
Learning to generate chairs, tables and cars with  convolutional networks
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), NN:NNN–N0N, N0NN
 N  [NN] V
Dumoulin, I
Belghazi, B
Poole, O
Mastropietro,  A
Lamb, M
Arjovsky, and A
Courville
Adversarially  learned inference
In ICLR, arXiv:NN0N.00N0NvN, N0NN
N  [NN] Y
Ganin, E
Ustinova, H
Ajakan, P
Germain, H
Larochelle,  F
Laviolette, M
Marchand, and V
Lempitsky
Domainadversarial training of neural networks
Journal of Machine  Learning Research, NN(NN):N–NN, N0NN
N  [NN] I
Goodfellow
NIPS N0NN tutorial: Generative adversarial  networks
arXiv:NN0N.00NN0vN, N0NN
N, N  [NN] I
Goodfellow, J
Pouget-Abadie, M
Mirza, B
Xu,  D
Warde-Farley, S
Ozair, A
Courville, and Y
Bengio
Generative adversarial nets
In NIPS, N0NN
N, N, N, N  [NN] I
Gulrajani, F
Ahmed, M
Arjovsky, V
Dumoulin, and  A
Courville
Improved training of Wasserstein GANs
 arXiv:NN0N.000NNvN, N0NN
N, N, N, N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
In  ICML, N0NN
N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
In  ICML, N0NN
N  [NN] P
Isola, J.-Y
Zhu, T
Zhou, and A
A
Efros
Image-to-image  translation with conditional adversarial networks
In CVPR,  N0NN
N  [NN] G
R
Johnson, J
Li, A
Shariff, G
K
Rohde, and R
F
Murphy
Automated Learning of Subcellular Variation among  Punctate Protein Patterns and a Generative Model of Their  Relation to Microtubules
PLoS computational biology,  NN(NN):eN00NNNN, N0NN
N  [N0] D
P
Kingma and M
Welling
Auto-encoding variational  Bayes
In ICLR, arXiv:NNNN.NNNNvN0, N0NN
N  [NN] Y
LeCun, C
Cortes, and C
J
C
Burges
The MNIST  database of handwritten digits
http://yann.lecun
 com/exdb/mnist/, NNNN
N  [NN] C
Ledig, L
Theis, F
Huszar, J
Caballero, A
Cunningham, A
Acosta, A
Aitken, A
Tejani, J
Totz, Z
Wang, and  W
Shi
Photo-realistic single image super-resolution using a  generative adversarial network
In CVPR, N0NN
N  [NN] C
Li and M
Wand
Precomputed real-time texture synthesis  with markovian generative adversarial networks
In ECCV,  N0NN
N  [NN] Z
Liu, P
Luo, X
Wang, and X
Tang
Deep learning face  attributes in the wild
In ICCV, N0NN
N  [NN] V
Ljosa, K
L
Sokolnicki, and A
E
Carpenter
Annotated  high-throughput microscopy image sets for validation
Nature Methods, N(N):NNN, N0NN
N  [NN] D
Lopez-Paz and M
Oquab
Revisiting classifier twosample tests
In ICLR, arXiv:NNN0.0NNNNvN, N0NN
N, N  [NN] S
G
Martin and R
A
Arkowitz
Cell polarization in  budding and fission yeasts
FEMS microbiology reviews,  NN(N):NNN–NNN, N0NN
N, N  [NN] M
Mathieu, J
Zhao, P
Sprechmann, A
Ramesh, and Y
LeCun
Disentangling factors of variation in deep representation using adversarial training
In NIPS, N0NN
N  [NN] E
Meijering, A
E
Carpenter, H
Peng, F
A
Hamprecht, and  J.-C
Olivo-Marin
Imagining the future of bioimage analysis
Nature Biotechnology, NN(NN):NNN0–NNNN, N0NN
N, N  [N0] L
Metz, B
Poole, D
Pfau, and J
Sohl-Dickstein
 Unrolled generative adversarial networks
In ICLR,  arXiv:NNNN.0NNNNvN, N0NN
N, N, N  [NN] M
Mirza and S
Osindero
Conditional generative adversarial nets
arXiv:NNNN.NNNNvN, N0NN
N  [NN] R
F
Murphy
Building cell models and simulations from  microscope images
Methods, NN:NN–NN, N0NN
N  [NN] S
Nowozin, B
Cseke, and R
Tomiokao
f-GAN: Training  generative neural samplers using variational divergence minimization
In NIPS, N0NN
N  [NN] A
Odena, C
Olah, and J
Shlens
Conditional image synthesis with auxiliary classifier GANs
In ICML, N0NN
N  [NN] A
Osokin, A
Chessel, R
E
Carazo Salas, and F
Vaggi
 GANs for biological image synthesis
Code and supplementary materials
https://github.com/aosokin/  biogans, N0NN
N, N, N, N  [NN] T
D
Pollard and J.-Q
Wu
Understanding cytokinesis:  lessons from fission yeast
Nature reviews Molecular cell  biology, NN(N):NNN–NNN, N0N0
N  [NN] B
Poole, A
A
Alemi, J
Sohl-Dickstein, and A
Angelovam
 Improved generator objectives for GANs
In NIPS Workshop  on Adversarial Training, arXiv:NNNN.0NNN0vN, N0NN
N, N  NNNN  http://yann.lecun.com/exdb/mnist/ http://yann.lecun.com/exdb/mnist/ https://github.com/aosokin/biogans https://github.com/aosokin/biogans   [NN] A
Radford, L
Metz, and S
Chintala
Unsupervised representation learning with deep convolutional generative adversarial networks
In ICLR, arXiv:NNNN.0NNNNvN, N0NN
N, N,  N, N  [NN] S
Reed, Z
Akata, X
Yan, L
Logeswaran, B
Schiele, and  H
Lee
Generative adversarial text to image synthesis
In  ICML, N0NN
N  [N0] T
Salimans, I
Goodfellow, W
Zaremba, V
Cheung, A
Radford, and X
Chen
Improved techniques for training GANs
 In NIPS, N0NN
N  [NN] N
C
Shaner, R
E
Campbell, P
A
Steinbach, B
N
Giepmans, A
E
Palmer, and R
Y
Tsien
Improved monomeric  red, orange and yellow fluorescent proteins derived from  Discosoma sp
red fluorescent protein
Nature biotechnology, NN(NN):NNNN, N00N
N  [NN] I
Tolstikhin, S
Gelly, O
Bousquet, C.-J
Simon-Gabriel,  and B
Schölkopf
AdaGAN: Boosting generative models
 arXiv:NN0N.0NNNNvN, N0NN
N  [NN] R
Y
Tsien
The green fluorescent protein
Annual review of  biochemistry, NN:N0N–NNN, NNNN
N  [NN] A
van den Oord, N
Kalchbrenner, L
Espeholt,  K
Kavukcuoglu, O
Vinyals, and A
Graves
Conditional image generation with PixelCNN decoders
In NIPS,  N0NN
N  [NN] D
A
Van Valen, T
Kudo, K
M
Lane, D
N
Macklin, N
T
 Quach, M
M
DeFelice, I
Maayan, Y
Tanouchi, E
A
Ashley, and M
W
Covert
Deep Learning Automates the Quantitative Analysis of Individual Cells in Live-Cell Imaging Experiments
PLOS Computational Biology, NN(NN):eN00NNNN,  N0NN
N  [NN] X
Wang and A
Gupta
Generative image modeling using  style and structure adversarial networks
In ECCV, N0NN
N  [NN] T
White
Sampling generative networks
 arXiv:NN0N.0NNNNvN, N0NN
N  [NN] E
Williams, J
Moore, S
W
Li, G
Rustici, A
Tarkowska,  A
Chessel, S
Leo, B
Antal, R
K
Ferguson, U
Sarkans,  A
Brazma, R
E
Carazo Salas, and J
Swedlow
The Image Data Resource: A scalable platform for biological image  data access, integration, and dissemination
Nature Methods,  NN:NNN–NNN, N0NN
N  [NN] J
Wu, C
Zhang, T
Xue, B
Freeman, and J
Tenenbaum
 Learning a probabilistic latent space of object shapes via ND  generative-adversarial modeling
In NIPS, N0NN
N  [N0] Y
Wu, Y
Burda, R
Salakhutdinov, and R
Grosse
On the  quantitative analysis of decoder-based generative models
In  ICLR, arXiv:NNNN.0NNNNvN, N0NN
N  [NN] H
Yan and M
K
Balasubramanian
Meiotic actin rings are  essential for proper sporulation in fission yeast
Journal of  Cell Science, NNN(N):NNNN–NNNN, N0NN
N  [NN] F
Yu, A
Seff, Y
Zhang, S
Song, T
Funkhouser, and J
Xiao
 LSUN: Construction of a large-scale image dataset using  deep learning with humans in the loop
arXiv:NN0N.0NNNNvN,  N0NN
N  [NN] J
Zhao, M
Mathieu, and Y
LeCun
Energy-based generative  adversarial networks
In ICLR, arXiv:NN0N.0NNNNvN, N0NN
 N  [NN] J.-Y
Zhu, P
Krähenbühl, E
Shechtman, and A
A
Efros
 Generative visual manipulation on the natural image manifold
In ECCV, N0NN
N  [NN] J.-Y
Zhu, T
Park, P
Isola, and A
A
Efros
Unpaired imageto-image translation using cycle-consistent adversarial networks
In ICCV, N0NN
N  NNNNDeepSetNet: Predicting Sets With Deep Neural Networks   DeepSetNet: Predicting Sets with Deep Neural Networks  S
Hamid Rezatofighi Vijay Kumar B G Anton Milan  Ehsan Abbasnejad Anthony Dick Ian Reid  School of Computer Science, The University of Adelaide, Australia  Abstract  This paper addresses the task of set prediction using  deep learning
This is important because the output of many  computer vision tasks, including image tagging and object  detection, are naturally expressed as sets of entities rather  than vectors
As opposed to a vector, the size of a set is  not fixed in advance, and it is invariant to the ordering of  entities within it
We define a likelihood for a set distribution and learn its parameters using a deep neural network
 We also derive a loss for predicting a discrete distribution  corresponding to set cardinality
Set prediction is demonstrated on the problem of multi-class image classification
 Moreover, we show that the proposed cardinality loss can  also trivially be applied to the tasks of object counting and  pedestrian detection
Our approach outperforms existing  methods in all three cases on standard datasets
 N
Introduction  Deep neural networks have shown state-of-the-art performance on many computer vision problems, including semantic segmentation [NN], visual tracking [NN], image captioning [NN], scene classification [NN], and object detection [NN]
However, traditional convolutional architectures  require a problem to be formulated in a certain way: in particular, they are designed to predict a vector (or a matrix,  or a tensor in a more general sense), that is either of a fixed  length or whose size depends on the input (cf 
fully convolutional architectures)
 For example, consider the task of scene classification  where the goal is to predict the label (or category) of a given  image
Modern approaches typically address this by a series  of convolutional layers, followed by a number of fully connected layers, which are finally mapped to predict a fixedsized vector [NN, NN, NN]
The length of the predicted vector corresponds to the number of candidate categories, e.g
 N,000 for the ImageNet challenge [NN]
Each element is a  score or probability of a particular category, and the final  prediction is a probability distribution over all categories
 This strategy is perfectly admissible if one expects to find  CNN  I  Vector  CNN CNN  Set  s h e e p  b ik e  p e rs o n  b o o k  p iz z a  b ir d  s c is s o rs  c a r  o v e n  b o w l  a p p le  p e rs o n  p iz z a  c a r  Figure N: Left: Traditional CNNs learn parameters θ∗ to  predict a fixed vector Y 
Right: In contrast, we propose to  train a separate CNN to learn a parameter vector w∗, which  is used to predict the set cardinality of a particular output
 exactly one or at least the same number of categories across  all images
However, natural images typically show multiple entities (e.g
table, pizza, person, etc.), and what is  perhaps more important, this number differs from image to  image
During evaluation, this property is not taken into account
The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) only counts an error if the “true” label is  not among the top-N candidates
Another strategy to account for multiple classes is to fix the number to a certain  value for all test instances, and report precision and recall  by counting false positive and false negative predictions, as  was done in [NN, NN]
Arguably, both methods are suboptimal because in a real-world scenario, where the correct  labelling is unknown, the prediction should in fact not only  rank all labels according to their likelihood of being present,  but also to report how many objects (or labels) are actually  present in one particular image
Deciding how many objects  are present in an image is a crucial part of human perception and scene understanding but is missing from our current evaluation of automated image understanding methods
 As a second example, let us consider pedestrian detecNNNNN    tion
The parallel to scene classification that we motivated  above is that, once again, in real scenarios, the number of  people in a particular scene is not known beforehand
The  most common approach is to assign a confidence score to  a number of region candidates [N, NN, NN, NN], which are  typically selected heuristically by thresholding and nonmaxima suppression
We argue that it is important not to  simply discard the information about the actual number of  objects at test time, but to exploit it while selecting the subset of region proposals
 The examples above motivate and underline the importance of set prediction in certain applications
It is important  to note that, in contrast to vectors, a set is a collection of elements which is invariant under permutation and the size of  a set is not fixed in advance
To this end, we use a principled  definition of a set as the union of cardinality distribution and  family of joint distributions over each cardinality value
In  summary, our main contributions are as follows: (i) Starting from the mathematical definition of a set distribution,  we derive a loss that enables us to employ existing machine  learning methodology to learn this distribution from data
 (ii) We integrate our loss into a deep learning framework  to exploit the power of a multi-layer architecture
(iii) We  present state-of-the-art results for multi-label image classification and pedestrian detection on standard datasets and  competitive results on the task of object counting
 N
Related Work  A sudden success in multiple applications including  voice recognition [NN], machine translation [N0] and image classification [NN], has sparked the deployment of  deep learning methods throughout numerous research areas
 Deep convolutional (CNN) and recurrent (RNN) neural networks now outperform traditional approaches in tasks like  semantic segmentation [N, NN], image captioning [NN] or object detection [NN]
Here, we will briefly review some of the  recent approaches to image classification and object detections and point out their limitations
 Image or scene classification is a fundamental task of  understanding photographs
The goal here is to predict  a scene label for a given image
Early datasets, such as  Caltech-N0N [N0], mostly contained one single object and  could easily be described by one category
Consequently,  a large body of literature focused on single-class prediction [NN, NN, NN, N0]
However, real-world photographs typically contain a collection of multiple objects and should  therefore be captioned with multiple tags
Surprisingly,  there exists rather little work on multi-class image classification that makes use of deep architectures
Gong et  al
[NN] combine deep CNNs with a top-k approximate  ranking loss to predict multiple labels
Wei et al
[NN] propose a Hypotheses-Pooling architecture that is specifically  designed to handle multi-label output
While both methods  open a promising direction, their underlying architectures  largely ignore the correlation between multiple labels
To  address this limitation, recently, Wang et al
[NN] combined  CNNs and RNNs to predict a number of classes in a sequential manner
RNNs, however, are not suitable for set prediction mainly for two reasons
First, the output represents a  sequence rather than a set and is thus highly dependent on  the prediction order, as was shown recently by Vinyals et  al
[NN]
Second, the final prediction may not result in a feasible solution (e.g
it may contain the same element multiple  times), such that post-processing or heuristics such as beam  search must be employed [NN, NN]
Here we show that our  approach not only guarantees to always predict a valid set,  but also outperforms previous methods
 Pedestrian detection can also be viewed as a classification problem
Traditional approaches follow the slidingwindow paradigm [NN, N, NN, NN, N], where each possible  (or rather plausible) image region is scored independently  to contain a person or not
More recent methods like Fast RCNN [NN] or the single-shot multi-box detector (SSD) [NN]  learn the relevant image features rather than manually engineering them, but retain the sliding window approach
 All the above approaches require some form of postprocessing to suppress spurious detection responses that  originate from the same person
This is typically addressed by non-maximum suppression (NMS), a greedy  optimisation strategy with a fixed overlap threshold
Recently, several alternatives have been proposed to replace  the greedy NMS procedure, including sequential head detection with LSTMs [NN], a global optimisation approach  to NMS [NN, NN], or even learning NMS end-to-end using  CNNs [NN]
None of the above methods, however, explicitly consider the number of objects while selecting the final  set of boxes
Contrary to existing pedestrian detection approaches, we incorporate the cardinality into the NMS algorithm itself and show an improvement over the state of the  art on two benchmarks
Note that the idea of considering the  number of pedestrians can be applied in combination with  any of the aforementioned detection techniques to further  improve their performances
 It is important to bear in mind that unlike many existing  approaches that learn to count [N, N, NN, N0, NN, NN, N0, NN],  our main goal is not object counting
Rather, we derive  a formulation for set prediction using deep learning
Estimating the cardinality of objects and thereby their count  is a byproduct of our approach
To demonstrate the effectiveness of our formulation, we also conduct experiments  on the task of object counting, outperforming many recent  methods on the widely used USCD dataset
 N
Random Vectors vs
Random Finite Sets  In statistics, a continuous random variable y is a variable that can take an infinite number of possible values
A  NNNN    continuous random vector can be defined by stacking several continuous random variables into a fixed length vector,  Y = (yN, · · · , ym)
The mathematical function describing the possible values of a continuous random vector and their  associated joint probabilities is known as a probability density function (PDF) p(Y ) such that ∫  p(Y )dY = N
 In contrast, a random finite set (RFS) Y is a finite-set valued random variable Y = {yN, · · · , ym}
The main dif- ference between an RFS and a random vector is that for the  former, the number of constituent variables, m, is random  and the variables themselves are random and unordered
 Throughout the paper, we use Y = {yN, · · · , ym} for a set with unknown cardinality, Ym = {yN, · · · , ym}|| for a set  with known cardinality and Y = (yN, · · · , ym) for a vector with known dimension
 A statistical function describing a finite-set variable Y is a combinatorial probability density function p(Y) which consists of a discrete probability distribution, the so-called  cardinality distribution, and a family of joint probability  densities on both the number and the values of the constituent variables
Similar to the definition of a PDF for  a random variable, the PDF of an RFS must sum to unity  over all possible cardinality values and all possible element  values and their permutations [NN]
The PDF of an mdimensional random vector can be defined in terms of an  RFS as  p(yN, · · · , ym) , N  m! p({yN, · · · , ym}||)
(N)  The normalisation factor m! = ∏m  k=N k appears because the  probability density for a set {yN, · · · , ym}|| must be equally distributed among all the m! possible permutations of the vector [NN]
 Conventional machine learning approaches, such as  Bayesian learning and convolutional neural networks, have  been proposed to learn the optimal parameters θ∗ of the distribution p(Y |x,θ∗) which maps the input vector x to the output vector Y 
In this paper, we instead propose an approach that can learn a pair (θ∗,w∗) of parameter vectors for a set distribution that allow one to map the input vector  x into the output set Y , i.e
p(Y|x,θ∗,w∗)
The additional parameters w∗ define a PDF over the set cardinality, as we  will explain in the next section
 N
Deep Set Network  Let us begin by defining a training set D = {Yi,xi}, where each training sample i = N, 


, n is a pair consist- ing of an input feature xi ∈ R  l and an output (or label) set  Yi = {yN, yN, 


, ymi}, yk ∈ R d, mi ∈ N  0
In the following we will drop the instance index i for better readability
 Note that m := |Y| denotes the cardinality of set Y 
The  probability of a set Y is defined as  p(Y|x,θ,w) =p(m|x,w)×  m!× Um × p(yN, yN, · · · , ym|x,θ), (N)  where p(m|·, ·) and p(yN, yN, · · · , ym|·, ·) are respectively a cardinality distribution and a symmetric joint probability  density distribution of the elements
U is the unit of hypervolume in Rd, which makes the joint distribution unitless [NN, NN]
θ denotes the parameters that estimate the  joint distribution of set element values for a fixed cardinality,N while w represents the collection of parameters which  estimate the cardinality distribution of the set elements
 The above formulation represents the probability density  of a set which is very general and completely independent  from the choices of both cardinality and spatial distributions
It is thus straightforward to transfer it to many applications that require the output to be a set
However, to  make the problem amenable to mathematical derivation and  implementation, we adopt two assumptions: i) the outputs  (or labels) in the set are independent and identically distributed (i.i.d.) and ii) their cardinality follows a Poisson  distribution with parameter λ
Thus, we can write the distribution as  p(Y|x,θ,w) =  ∫  p(m|λ)p(λ|x,w)dλ×  m!× Um×  (  m ∏  k=N  p(yk|x,θ)  )  
 (N)  N.N
Posterior distribution  To learn the parameters θ and w, we first define the posterior distribution over them as  p(θ,w|D) ∝ p(D|θ,w)p(θ)p(w)  ∝ n ∏  i=N  [ ∫  p(mi|λ)p(λ|xi,w)dλ×mi!  Umi  (  mi ∏  k=N  p(yk|xi,θ)  )]  p(xi)p(θ)p(w)
 (N)  A closed form solution for the integral in Eq
(N) can be  obtained by using conjugate priors:  m ∼ P(m;λ)  λ ∼ G(λ;α(x,w), β(x,w))  α(x,w), β(x,w) > 0 ∀x,w  θ ∼ N (θ; 0, σNNI)  w ∼ N (w; 0, σNNI),  NThis is also known as spatial distribution of points in point process  statistics
 NNNN    where P(·, λ), G(·;α, β), and N (·; 0, σNI) represent re- spectively a Poisson distribution with parameters λ, a  Gamma distribution with parameters (α, β) and a zero mean normal distribution with covariance equal to σNI
 We assume that the cardinality follows a Poisson distribution whose mean, λ, follows a Gamma distribution, with  parameters which can be estimated from the input data x
 Note that the cardinality distribution in Eq
(N) can be replaced by any other discrete distribution
For example, it is  a valid assumption to model the number of objects in natural images by a Poisson distribution [N]
Thus, we could  directly predict λ to model this distribution by formulating  the cardinality as p(m|x,w) = P(m;λ(x,w))
However, this would limit the model’s expressive power because two  visually entirely different images with the same number of  objects would be mapped to the same λ
Instead, to allow  for uncertainty of the mean, we model it with another distribution, which we choose to be Gamma for mathematical  convenience
Consequently, the integrals in p(θ,w|D) are simplified and form a negative binomial distribution,  NB (m; a, b) = Γ(m+ a)  Γ(m+ N)Γ(a) .(N− b)abm, (N)  where Γ is the Gamma function
Finally, the full posterior distribution can be written as  p(θ,w|D) ∝ n ∏  i=N  [  NB  (  mi;α(xi,w), N  N + β(xi,w)  )  ×mi!× U mi ×  (  mi ∏  k=N  p(yk|xi,θ)  )  ]  p(θ)p(w)
 (N)  N.N
Learning  For simplicity, we use a point estimate for the posterior p(θ,w|D), i.e
p(θ,w|D) = δ(θ = θ∗,w = w∗|D), where (θ∗,w∗) are computed using the following MAP es- timator:  (θ∗,w∗) = argmax θ,w  log (p (θ,w|D)) 
(N)  The optimisation problem in Eq
(N) can be decomposed  w.r.t
the parameters θ and w
Therefore, we can learn them  independently as  θ ∗ = argmax  θ  −γN‖θ‖+ n ∑  i=N  mi ∑  k=N  log (p(yk|xi,θ)) (N)  and  w ∗ =argmax  w  n ∑  i=N  [  log  (  Γ(mi + α(xi,w))  Γ(mi + N)Γ(α(xi,w))  )  +log  (  β(xi,w) α(xi,w)  (N + β(xi,w)α(xi,w)+mi)  )  ]  − γN‖w‖,  (N)  where γN and γN are the regularisation parameters, proportional to the predefined covariance parameters σN and σN
 These parameters are also known as weight decay parameters and commonly used in training neural networks
 The learned parameters θ∗ in Eq
(N) are used to map an  input feature vector x into an output vector Y 
For example,  in image classification, θ∗ is used to predict the distribution  Y over all categories, given the input image x
Note that  θ ∗ can generally be learned using a number of existing machine learning techniques
In this paper we rely on deep  CNNs to perform this task
 To learn the highly complex function between the input feature x and the parameters (α, β), which are used for estimating the output cardinality distribution, we train  a second deep neural network
Using neural networks  to predict a discrete value may seem counterintuitive, because these methods at their core rely on the backpropagation algorithm, which assumes a differentiable loss
 Note that we achieve this by describing the discrete distribution by continuous parameters α, β (Negative binomial  NB(·, α, NN+β )), and can then easily draw discrete samples from that distribution
More formally, to estimate w∗, we  compute the partial derivatives of the objective function in  Eq
(N) w.r.t
α(·, ·) and β(·, ·) and use standard backpropa- gation to learn the parameters of the deep neural network
 We refer the reader to the supplementary material for  the complete derivation of the partial derivatives, a more  detailed derivation of the posterior in Eqs
(N)-(N) and the  proof for decomposition of the MAP estimation in Eq
(N)
 N.N
Inference  Having the learned parameters of the network (w∗,θ∗), for a test feature x+, we use a MAP estimate to generate a  set output as Y∗ = argmaxY p(Y|D,x +), where  p(Y|D,x+) =  ∫  p(Y|x+,θ,w)p(θ,w|D)dθdw  and p(θ,w|D) = δ(θ = θ∗,w = w∗|D)
Since the unit of hypervolume U in most practical application in unknown,  to calculate the mode of the set distribution p(Y|D,x+), we use sequential inference as explained in [NN]
To this end,  we first calculate the mode m∗ of the cardinality distribution  m∗ = argmaxm p(m|x +,w∗), where p(m|x+,w∗) =  NB (  m;α(x+,w∗), NN+β(x+,w∗)  )  
Then, we calculate the  mode of the joint distribution for the given cardinality m∗  as  Y∗ = argmax Ym∗  p({yN, · · · , ym∗}|||x +,θ∗)
(N0)  To estimate the most likely set Y∗ with cardinality m∗, we use the first CNN with the parameters θ∗ which predicts  p(yN, · · · , yM |x +,θ∗), where M is the maximum cardinality of the set, i.e
{yN, · · · , ym∗} ⊆ {yN, · · · , yM}, ∀m ∗
 NNN0    Since the samples are i.i.d., the joint probability maximised  when the probability of each element in the set is maximised
Therefore, the most likely set Y∗ with cardinality m∗ is obtained by ordering the probabilities of the set elements yN, · · · , yM as the output of the first CNN and choos- ing m∗ elements with highest probability values
 Note that the assumptions in Sec
N are necessary to  make both learning and inference computationally tractable  and amenable to an elegant mathematical formulation
A  major advantage of this approach is that we can use any  state-of-the-art classifier/detector as the first CNN (θ∗) to  further improve its performance
Modifying any of the  assumptions, e.g
non-i.i.d
set elements, leads to serious  mathematical complexities [NN], and is left for future work
 N
Experimental Results  Our proposed method is best suited for applications that  expect the solution to be in the form of a set, i.e
permutation invariant and of an unknown cardinality
To that end,  we perform an experiment on multi-label image classification in Sec
N.N
In addition, we explore our cardinality estimation loss on the object counting problem in Sec
N.N and  then show in Sec
N.N how incorporating cardinality into a  state-of-the art pedestrian detector and formulating it as a  set problem can boost up its performance
 N.N
Multi-Label Image Classification  As opposed to the more common and more studied  problem of (single-label) image classification, the task  here is rather to label a photograph with an arbitrary, apriori unknown number of tags
We perform experiments  on two standard benchmarks, the PASCAL VOC N00N  dataset [N] and the Microsoft Common Objects in Context  (MS COCO) dataset [NN]
 Implementation details
In this experiment, similar  to [NN], we build on the NN-layers VGG network [NN], pre- trained on the N0NN ImageNet dataset
We adapt VGG for  our purpose by modifying the last fully connected prediction layer to predict N0 classes for PASCAL VOC, and N0 classes for MS COCO
We then fine-tune the entire network for each of these datasets using two commonly  used losses for multi-label classification, softmax and binary cross-entropy (BCE)N [NN, NN]
To learn both classifiers, we set the weight decay to N ·N0−N, with a momentum of 0.N and a dropout rate of 0.N
The learning rate is ad- justed to gradually decrease after each epoch, starting from  0.0N for softmax and from 0.00N for binary cross-entropy
The learned parameters of these classifiers correspond to θ∗  for our proposed deep set network (cf 
Eq
(N) and Fig
N)
 NWeighted Approximate Ranking (WARP) objective is another commonly used loss for multi-label classification
However, it does not perform as well as softmax and binary cross-entropy for the used datasets [NN]
 To learn the cardinality distribution, we use the same  VGG-NN network as above and modify the final fully connected layer to predict N values followed by two weighted sigmoid activation functions for α and β
It is important  to note, that the outputs must be positive to describe a  valid Gamma distribution
We therefore also append two  weighted sigmoid transfer functions with weights αM , βM to ensure that the values predicted for α and β are in a  valid range
Our model is not sensitive to these parameters and we set their values to be large enough (αM = NN0 and βM = N0) to guarantee that the mode of the distribu- tion can accommodate the largest cardinality existing in the  dataset
We then fine-tune the network on cardinality distribution using the objective loss defined in Eq
(N)
To train  the cardinality CNN, we set a constant learning rate 0.00N, weight decay N·N0−NN, momentum rate 0.N and dropout 0.N
 Evaluation protocol
To evaluate the performance of the  classifiers and our deep set network, we employ the commonly used evaluation metrics for multi-label image classification [NN, NN]: precision and recall of the generated labels per-class (C-P and C-R) and overall (O-P and O-R)
 Precision is defined as the ratio of correctly predicted labels  and total predicted labels, while recall is the ratio of correctly predicted labels and ground-truth labels
In case no  predictions (or ground truth) labels exist, i.e
the denominator becomes zero, precision (or recall) is defined as N00%
To generate the predicted labels for a particular image, we  perform a forward pass of the CNN and choose top-k labels according to their scores similar to [NN, NN]
Since  the classifier always predicts a fixed-sized prediction for  all categories, we sweep k from 0 to the maximum num- ber of classes to generate a precision/recall curve, which is  common practice in multi-label image classification
However, for our proposed DeepSet Network, the number of labels per instance is predicted from the cardinality network
 Therefore, prediction/recall is not dependent on value k and  one single precision/recall value can be computed
 To calculate the per-class and overall precision/recall,  their average values over all classes and all examples are  computed, respectively
In addition, we also report the FN  score (the harmonic mean of precision and recall) averaged  over all classes (C-FN) and all instances and classes (O-FN)
 PASCAL VOC N00N
The Pascal Visual Object Classes  (VOC) [N] benchmark is one of the most widely used  datasets for detection and classification
It consists of NNNN images with a N0/N0 split for training and test, where objects from N0 pre-defined categories have been annotated by bounding boxes
Each image may contain between N and N unique objects
We compare our results with a state- of-the-art classifier as described above
The resulting precision/recall plots are shown in Fig
N(a) together with our  proposed approach using the estimated cardinality
Note  NNNN    GT: motorcycle chair, dining-table, book, tv, couch,  potted-plant, vase  chair, dining-table, book, tv, couch,  potted-plant, vase Prediction:  person, chair, car, dining-table,  cup, knife, fork, pizza, wine-glass  person, chair, car, dining-table,  cup, knife, fork, pizza, wine-glass  motorcycle  ----Figure N: Qualitative results of our multi-class image labelling approach
For each image, the ground truth tags and our  predictions are denoted below
Note that we show the exact output of our network, without any heuristics or post-processing
 GT:  Prediction:  chair, cup, book,   keyboard, mouse  chair, cup, book,   keyboard, mouse, tv  banana  banana, bottle  person, toothbrush  person, toothbrush,  cell-phone  teddy-bear  teddy-bear, bird, car, person  oven  oven, toaster, fridge, bowl,  sink, microvawe  Figure N: Interesting failure cases of our method
The “spurious” TV class predicted on the left is an artifact in annotation  because in many examples, computer monitors are actually labelled as TV
In other cases, our network can correctly reason  about the number of objects or concepts in the scene, but is constrained by a fixed list of categories defined in the dataset
 that by enforcing the correct cardinality for each image, we  are able to clearly outperform the baseline w.r.t
both measures
Note also that our prediction (+) can nearly replicate  the oracle (∗), where the ground truth cardinality is known
The mean absolute cardinality error of our prediction on  PASCAL VOC is 0.NN± 0.NN
 Microsoft COCO
Another popular benchmark for image  captioning, recognition, and segmentation is the recent Microsoft Common Objects in Context (MS-COCO) [NN]
The  dataset consists of NNN thousand images, each labelled with per instance segmentation masks of N0 classes
The num- ber of unique objects for each image can vary between 0 and NN
Around N00 images in the training dataset do not contain any of the N0 classes and there are only a handful of images that have more than N0 tags
The majority of the images contain between one and three labels
We use NNNNN images as training and validation split (N0% - N0%), and the remaining N0N0N images as test data
We predict the cardi- nality of objects in the scene with a mean absolute error of  0.NN and a standard deviation of 0.NN
 Fig
N(b) shows a significant improvement of precision  and recall and consequently the FN score using our deep set  0 0.N N  Recall  0  0.N  0.N  0.N  0.N  N  P re  c is  io n  SoftMax SoftMax DS (ours)  SoftMax DS (GT card.) BCE BCE DS (ours)  BCE DS (GT card.)  k=0 k=N  k=N  k=N0  (a)  PASCAL VOC N00N  0 0.N N  Recall  0  0.N  0.N  0.N  0.N  N  SoftMax SoftMax DS (ours)  SoftMax DS (GT card.) BCE BCE DS (ours)  BCE DS (GT card.)  k=0 k=N  k=N  k=N0  (b)  MS COCO  Figure N: Experimental results on multi-label image classification
The baselines (solid curves) represent state-of-theart classifiers, fine-tuned for each dataset, using two different loss functions
The methods are evaluated by choosing  the top-k predictions across the entire dataset, for different  k
Our approach predicts k and is thus evaluated only on  one single point (+)
It outperforms both classifiers significantly in terms of precision and recall and comes very close  to the performance when the true cardinality is known (∗)
 network compared to the softmax and binary cross-entropy  classifiers for all ranking values k
We also outperform the  state-of-the art multi-label classifier CNN-RNN [NN], for  the reported value of k = N
Our results, listed in Tab
N,  NNNN    Table N: Quantitative results for multi-label image classification on the MS COCO dataset
 Classifier Eval
C-P C-R C-FN O-P O-R O-FN  Softmax k=N NN.N NN.N NN.N N0.N NN.N NN.0  BCE k=N NN.N N0.N NN.N NN.N NN.N NN.N  CNN-RNN [NN] k=N NN.0 NN.N N0.N NN.N NN.N NN.N  Ours (Softmax) k=m∗ NN.N NN.N NN.N NN.N NN.N NN.N  Ours (BCE) k=m∗ NN.N NN.N NN.N N0.N NN.N NN.N  show around N percentage points improvement for the FN score on top of the baseline classifiers and about N percent- age points improvement compared to the state of the art on  this dataset
Examples of perfect label prediction using our  proposed approach are shown in Fig
N
The deep set network can properly recognise images with no labels at all, as  well as images with many tags
We also investigated failure cases where either the cardinality CNN or the classifier  fails to make a correct prediction
We showcase some of  these cases in Fig N
We argue here that some of the failure cases are simply due to a missed ground truth annotation, such as the left-most example, but some are actually  semantically correct w.r.t
the cardinality prediction, but are  penalized during evaluation because a particular object category is not available in the dataset
This is best illustrated  in the second example in Fig
N
Here, our network correctly predicts the number of objects in the scene, which  is two, however, the can does not belong to any of the N0  categories in the dataset and is thus not annotated
Similar  situations also appear in other images further to the right
 N.N
Object Counting  To show the robustness of our cardinality loss, we first  evaluate our cardinality estimation on the common crowd  counting application
To this end, we apply our approach  on the widely used UCSD dataset [N] and compare our results to four state-of-the art approaches [N, NN, NN, N0]
The  USCD dataset includes a N000-frames long video sequence,  captured by a fixed outdoor surveillance camera
In addition  to the video, the region of interest (ROI), the perspective  map of the scene and the location annotations of all pedestrians in each frame are also provided
 Implementation details
We build our cardinality network  structure on top of the well-known AlexNet [NN] architecture
However, we replace the first convolutional layer with  a single channel filter to accept grayscale images as input,  and the last fully connected layer with N layers outputs, sim- ilar to the case above (cf 
Sec
N.N)
To estimate the counts,  we calculate the mode of the negative binomial distribution
 As input, we use a grayscale image constructed by superimposing all region proposals and their scores generated by  an off-the-shelf pedestrian detector (before non-maximum  suppression)
We use the multi-scale deep CNN approach  (MS-CNN) [N] trained on the KITTI dataset [NN] for our  Table N: Count mean absolute error on UCSD dataset
 Method max downscale upscale min overall  C-Forest [NN] N.NN N.N0 N.NN N.NN N.NN  IOC [N] N.NN N.NN N.NN N.NN N.NN  Cs-CCNN [N0] N.N0 N.NN N.NN N.NN N.NN  CCNN [NN] N.NN N.NN N.NN N.N0 N.NN  Hydra Ns [NN] N.NN N.NN N.NN N.NN N.NN  Hydra Ns [NN] N.NN N.NN N.NN N.NN N.NN  Ours N.NN N.N0 0.NN N.NN N.NN  purpose
We found, that this input provides a stronger signal than the raw RGB images, yielding better results
Note  that we process the input images with a pedestrian detector,  however, we do not use any location or perspective information that is available for this dataset
During learning,  we only rely on the object count for each image region
 We follow exactly the same data split used in [NN] by  conducting four different and separate experiments on maximal, downscale, upscale and minimal subsets in UCSD  dataset
In order to train our network, similar to [NN] we  use data augmentation in each experiment by extracting N00 random patches from each training image and their corresponding ground truth counts
We also randomly flip each  patch during training
To ensure that we can count all pedestrians from the entire image at test time, we choose the  patch sizes to be exactly half of the image size (NN × NNN pixels) and then perform inference on the resulting N non- overlapping regions
The weights are initialised randomly  and the network is trained for N00 epochs
All hyperparam- eters are set as in Sec
N.N
 Results
Tab
N shows the mean absolute error between the  predicted and the ground truth counts
We show competitive or superior performance in each experiment except for  the ‘minimal’ subset
The main reason is that the training  set size is too small (only N0 images) in this particular split and even data augmentation cannot generalize the cardinality model for the test images
Moreover, unlike other  methods, we do not utilize any location information but  only provide the object count as ground truth
Considering  the overall performance, our approach outperforms state-ofthe-art counting approaches that do not use the perspective  map (Hydra Ns and Ns) and performs favourably compared  to many existing methods that exploit localisation and perspective information
 Discussion
One obvious alternative for our proposed cardinality loss may seem to directly regress for m
This alternative, however, has two main drawbacks
First, it cannot  be formulated within a Bayesian set framework to model  uncertainty, and second, the regression loss does not yield  a discrete distribution and hence does not fit the underlying  mathematical foundation of this paper
Nevertheless, we  have run the same experiments as above using a standard  regression loss but did not reach the same performance
Using the regression loss we achieve a mean cardinality error  NNNN    (a) Proposals (b) MS-CNN [N] (c) Our result  Figure N: Example pedestrian detection result of our approach
To select relevant detection candidates from an  overcomplete set of proposals (a), state-of-the-art methods  rely on non-maximum suppression (NMS) with a fixed setting (b)
We show that a better result can be achieved by  adjusting the NMS threshold adaptively, depending on the  number of instances in each image (N in this case) (c)
 (MCE) of 0.NN on MS COCO, while our loss yields an MCE of 0.NN
This is also reflected in the O-FN score which drops from NN.N to NN.N when directly regressing for m
 N.N
Pedestrian Detection  In this section, we cast the task of pedestrian detection  as a set prediction problem and demonstrate that incorporating cardinality prediction (person count) can be beneficial to  improve performance
To this end, we perform experiments  on two widely used datasets, Caltech Pedestrians [N] and  MOTNN from the MOTChallenge benchmark [NN]
Recalling Eqs
(N) and (N), we need two networks with parameters  w ∗ and θ∗ for cardinality estimation and detection proposals, respectively
For the cardinality network, we use the  exact same architecture and setup as in Sec
N.N and train it  on the training sets of these datasets
Note that it is not our  intention to engineer a completely novel pedestrian detector  here
Rather, for θ∗, we take an off-the-shelf state-of-theart system (MS-CNN) [N] and show how it can be further  improved by taking the cardinality prediction into account
 To generate the final detection outputs, most detectors  often rely on non-maximum suppression (NMS), which  greedily picks the boxes with highest scores and suppresses  any boxes that overlap more than a pre-defined threshold  TO
This heuristic makes the solution more ad-hoc than  what is expressed in our set formulation in Eq
(N)
However, we are still able to improve the detector performance  by adjusting this threshold for each frame separately
To obtain the final detection output, we use the prediction on the  number of people (m∗) in the scene to choose an adaptive NMS threshold for each image
In particular, we start from  the default value of TO, and increase it gradually until the  number of boxes reaches m∗
In the case if the number of  final boxes is larger than m∗, we pick m∗ boxes with the  highest scores, which corresponds to the MAP set prediction as discussed in Sec
N.N
To ensure a fair comparison,  we also determine the best (global) value for TO = 0.N for  Table N: Pedestrian detection results measured by FN score  (higher is better) and log-average miss rate (lower is better)
 FN-score ↑ MR ↓  Method Caltech MOTNN Calt
MOTNN  MS-CNN [N] NN.NN NN.0N N0.N NN.N  MS-CNN-DS (ours) NN.NN NN.NN N0.N NN.N  MS-CNN-DS (GT card.) NN.NN NN.NN N0.N NN.N  the MS-CNN baseline
Fig
N demonstrates an example of  the adjusted NMS threshold when considering the number  of pedestrians in the image
 To quantify the detection performance, we adapt the  same evaluation metrics and follow the protocols used on  the Caltech detection benchmark [N]
The evaluation metrics used here are log-average miss rate (MR) over false positive per image
Additionally, we compute the FN score (the  harmonic mean of precision and recall)
The FN score is  computed from all detections predicted from our DeepSet  network and is compared with the highest FN score along  the MS-CNN precision-recall curve
To calculate MR, we  concatenate all boxes resulted from our adaptive NMS approach and change the threshold over all scores from our  predicted sets
Quantitative detection results are shown in  Tab
N
Note that we do not retrain the detector, but are still  able to improve its performance by predicting the number  of pedestrians in each frame in these two dataset
 N
Conclusion  We proposed a deep learning approach for predicting  sets
To achieve this goal, we derived a loss for learning a  discrete distribution over the set cardinality
This allowed us  to use standard backpropagation for training a deep network  for set prediction
We have demonstrated the effectiveness  of this approach on crowd counting, pedestrian detection  and multi-class image classification, achieving competitive  or superior results in all three applications
As our network  is trained independently, it can be trivially applied to any existing classifier or detector, to further improve performance
 Note that this decoupling is a direct consequence of our  underlying mathematical derivation due to the i.i.d
assumptions, which renders our approach very general and applicable to a wide range of models
In future, we plan to extend  our method to multi-class cardinality estimation and investigate models that do not make i.i.d
assumptions
Another  potential avenue could be to exploit the Bayesian nature of  the model to include uncertainty as opposed to relying on  the MAP estimation alone
 Acknowledgments
This research was supported by the  Australian Research Council through the Centre of Excellence in Robotic Vision, CENN0N000NN, and through Laureate Fellowship FLNN0N00N0N to IDR
 NNNN    References  [N] C
Arteta, V
Lempitsky, J
A
Noble, and A
Zisserman
Interactive object counting
In ECCV, pages N0N–NNN, N0NN
 N, N  [N] R
Benenson, M
Mathias, R
Timofte, and L
V
Gool
Pedestrian detection at N00 frames per second
In CVPR N0NN
N  [N] Z
Cai, Q
Fan, R
Feris, and N
Vasconcelos
A unified  multi-scale deep convolutional neural network for fast object  detection
In ECCV N0NN
N, N  [N] A
B
Chan, Z.-S
J
Liang, and N
Vasconcelos
Privacy preserving crowd monitoring: Counting people without people  models or tracking
In CVPR, pages N–N, N00N
N  [N] A
B
Chan and N
Vasconcelos
Bayesian poisson regression  for crowd counting
In CVPR, pages NNN–NNN, N00N
N, N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected CRFs
In ICLR, N0NN
 N  [N] N
Dalal and B
Triggs
Histograms of oriented gradients for  human detection
In CVPR N00N, pages NNN–NNN
N  [N] P
Dollár, C
Wojek, B
Schiele, and P
Perona
Pedestrian detection: An evaluation of the state of the art
IEEE T
Pattern  Anal
Mach
Intell., NN, N0NN
N  [N] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn, and  A
Zisserman
The PASCAL Visual Object Classes Challenge  N00N (VOCN00N) Results
N  [N0] L
Fei-Fei, R
Fergus, and P
Perona
One-shot learning  of object categories
IEEE T
Pattern Anal
Mach
Intell.,  NN(N):NNN–NNN, Apr
N00N
N  [NN] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and  D
Ramanan
Object detection with discriminatively trained  part based models
IEEE T
Pattern Anal
Mach
Intell.,  NN(N):NNNN–NNNN, N0N0
N  [NN] L
Fiaschi, U
Koethe, R
Nair, and F
A
Hamprecht
Learning to count with regression forest and structured labels
In  ICPR, pages NNNN–NNNN, N0NN
N  [NN] A
Geiger, P
Lenz, and R
Urtasun
Are we ready for autonomous driving? The KITTI Vision Benchmark Suite
In  CVPR N0NN
N  [NN] R
Girshick
Fast R-CNN
In ICCV N0NN
N  [NN] Y
Gong, Y
Jia, T
Leung, A
Toshev, and S
Ioffe
Deep  convolutional ranking for multilabel image annotation
arXiv  preprint arXiv:NNNN.NNNN, N0NN
N, N  [NN] Y
Gong, Y
Jia, T
Leung, A
Toshev, and S
Ioffe
Deep convolutional ranking for multilabel image annotation
CoRR,  abs/NNNN.NNNN, N0NN
N  [NN] A
Graves, A.-r
Mohamed, and G
E
Hinton
Speech recognition with deep recurrent neural networks
In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP N0NN, Vancouver, BC, Canada, May NN-NN,  N0NN, pages NNNN–NNNN, N0NN
N  [NN] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural Comput., N(N):NNNNNNN0, Nov
NNNN
N  [NN] J
Hosang, R
Benenson, and B
Schiele
Learning nonmaximum suppression
In CVPR N0NN, July N0NN
N  [N0] H
Idrees, I
Saleemi, C
Seibert, and M
Shah
Multi-source  multi-scale counting in extremely dense crowd images
In  CVPR, N0NN
N  [NN] J
Johnson, A
Karpathy, and L
Fei-Fei
Densecap: Fully  convolutional localization networks for dense captioning
In  CVPR N0NN
N, N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
ImageNet  classification with deep convolutional neural networks
In  NIPS*N0NN, pages N0NN–NN0N
N, N, N  [NN] D
Lee, G
Cha, M.-H
Yang, and S
Oh
Individualness  and determinantal point processes for pedestrian detection
 In ECCV N0NN, pages NN0–NNN
DOI: N0.N00N/NNN-N-NNNNNNNN-N N0
N  [NN] V
Lempitsky and A
Zisserman
Learning to count objects  in images
In NIPS, pages NNNN–NNNN, N0N0
N  [NN] G
Lin, A
Milan, C
Shen, and I
Reid
RefineNet: Multipath refinement networks for high-resolution semantic segmentation
In CVPR N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, L
Bourdev, R
Girshick, J
Hays, P
Perona, D
Ramanan, C
L
Zitnick, and  P
Dollár
Microsoft COCO: Common objects in context
 arXiv:NN0N.0NNN [cs], May N0NN
arXiv: NN0N.0NNN
N, N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.Y
Fu, and A
C
Berg
SSD: Single shot multibox detector
 In ECCV N0NN, Lecture Notes in Computer Science, pages  NN–NN, N0NN
DOI: N0.N00N/NNN-N-NNN-NNNNN-0 N
N, N  [NN] R
P
Mahler
Statistical multisource-multitarget information  fusion, volume NNN
Artech House Boston, N00N
N, N, N  [NN] A
Milan, L
Leal-Taixé, I
Reid, S
Roth, and K
Schindler
 MOTNN: A benchmark for multi-object tracking
 arXiv:NN0N.00NNN [cs], Mar
N0NN
N  [N0] V
N
Murthy, V
Singh, T
Chen, R
Manmatha, and D
Comaniciu
Deep decision network for multi-class image classification
In CVPR N0NN, June N0NN
N  [NN] H
Nam and B
Han
Learning multi-domain convolutional  neural networks for visual tracking
In CVPR N0NN
N  [NN] D
Onoro-Rubio and R
J
López-Sastre
Towards  perspective-free object counting with deep learning
In  ECCV, pages NNN–NNN, N0NN
N  [NN] G
Papandreou, L.-C
Chen, K
P
Murphy, and A
L
Yuille
 Weakly- and semi-supervised learning of a deep convolutional network for semantic image segmentation
In ICCV  N0NN, Dec
N0NN
N  [NN] T
T
Pham, S
Hamid Rezatofighi, I
Reid, and T.-J
Chin
 Efficient point process inference for large-scale object detection
In CVPR N0NN
N  [NN] V.-Q
Pham, T
Kozakaya, O
Yamaguchi, and R
Okada
 COUNT forest: Co-voting uncertain number of targets using random forest for crowd density estimation
In ICCV,  pages NNNN–NNNN, N0NN
N, N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS*N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
Imagenet large scale visual recognition challenge
Int
J
Comput
Vision, NNN(N):NNN–NNN,  N0NN
N  NNNN    [NN] P
Sermanet, D
Eigen, X
Zhang, M
Mathieu, R
Fergus,  and Y
LeCun
OverFeat: Integrated recognition, localization and detection using convolutional networks
CoRR,  abs/NNNN.NNNN, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
CoRR,  abs/NN0N.NNNN, N0NN
N, N  [N0] I
Sutskever, O
Vinyals, and Q
V
Le
Sequence to sequence  learning with neural networks
In NIPS*N0NN, pages NN0N–  NNNN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
CoRR, abs/NN0N.NNNN,  N0NN
N  [NN] O
Vinyals, S
Bengio, and M
Kudlur
Order matters: Sequence to sequence for sets
arXiv:NNNN.0NNNN [cs, stat],  Nov
N0NN
arXiv: NNNN.0NNNN
N  [NN] O
Vinyals, M
Fortunato, and N
Jaitly
Pointer networks
In  NIPS*N0NN, pages NNNN–NN00
N  [NN] P
Viola and M
J
Jones
Robust real-time face detection
Int
 J
Comput
Vision, NN(N):NNN–NNN, May N00N
N  [NN] B.-N
Vo et al
Model-based classification and novelty detection for point pattern data
In ICPR, N0NN
N  [NN] S
Walk, N
Majer, K
Schindler, and B
Schiele
New features and insights for pedestrian detection
In CVPR N0N0
 N  [NN] J
Wang, Y
Yang, J
Mao, Z
Huang, C
Huang, and W
Xu
 CNN-RNN: A unified framework for multi-label image classification
In CVPR, June N0NN
N, N, N, N, N  [NN] Y
Wei, W
Xia, J
Huang, B
Ni, J
Dong, Y
Zhao,  and S
Yan
CNN: Single-label to multi-label
CoRR,  abs/NN0N.NNNN, N0NN
N  [NN] M
D
Zeiler and R
Fergus
Visualizing and understanding convolutional networks
In ECCV N0NN, pages NNN–NNN
 DOI: N0.N00N/NNN-N-NNN-N0NN0-N NN
N  [N0] C
Zhang, H
Li, X
Wang, and X
Yang
Cross-scene crowd  counting via deep convolutional neural networks
In CVPR,  pages NNN–NNN, N0NN
N, N  [NN] Y
Zhang, D
Zhou, S
Chen, S
Gao, and Y
Ma
Singleimage crowd counting via multi-column convolutional neural network
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages NNN–NNN, N0NN
 N  NNNNQuantitative Evaluation of Confidence Measures in a Machine Learning World   Quantitative evaluation of confidence measures in a machine learning world  Matteo Poggi, Fabio Tosi, Stefano Mattoccia  University of Bologna  Department of Computer Science and Engineering (DISI)  Viale del Risorgimento N, Bologna, Italy  {matteo.poggiN, fabio.tosiN, stefano.mattoccia}@unibo.it  Abstract  Confidence measures aim at detecting unreliable depth  measurements and play an important role for many purposes and in particular, as recently shown, to improve  stereo accuracy
This topic has been thoroughly investigated by Hu and Mordohai in N0N0 (and N0NN) considering NN confidence measures and two local algorithms on  the two datasets available at that time
However, since then  major breakthroughs happened in this field: the availability  of much larger and challenging datasets, novel and more  effective stereo algorithms including ones based on deep  learning and confidence measures leveraging on machine  learning techniques
Therefore, this paper aims at providing an exhaustive and updated review and quantitative  evaluation of NN (actually, NN considering variants) stateof-the-art confidence measures - focusing on recent ones  mostly based on random-forests and deep learning - with  three algorithms on the challenging datasets available today
Moreover we deal with problems inherently induced by  learning-based confidence measures
How are these methods able to generalize to new data? How a specific training improves their effectiveness? How more effective confidence measures can actually improve the overall stereo accuracy?  N
Introduction  Although depth from stereo still represents an open problem [N, NN, NN], in recent years this field has seen notable improvements concerning the effectiveness of such algorithms  (e.g., [NN, N0]) and confidence measures, aimed at detecting  unreliable disparity assignments, proved to be very effective cues when plugged in stereo vision pipelines as shown  in [NN, NN, NN, N0]
However, shortcomings of stereo algorithms have been emphasized by the availability of very  challenging datasets with ground-truth such as KITTI N0NN  (KNN) [N], KITTI N0NN (KNN) [NN] and Middlebury N0NN  (MNN) [NN]
Thus, the ability to reliably predict failures of a  stereo algorithm by means of a confidence measure is fundamental and many approaches have been proposed for this  purpose
Hu and Mordohai [NN] exhaustively reviewed confidence measures available at that time, with two variants of  a standard local algorithm, and defined a very effective metric to evaluate their effectiveness on the small and mostly  unrealistic dataset [NN] with ground-truth available
However, since then there have been major breakthroughs in this  field:  • Novel and more reliable confidence prediction methods, in particular those based on random-forests [N, NN,  NN, NN] and deep learning [NN, N0]  • Much larger datasets with ground-truth depicting very  challenging and realistic scenes acquired in indoor  [NN] and outdoor environments [N, NN]  • Novel and more effective stereo algorithms, some  leveraging on deep learning techniques [NN, NN], more  and more often coupled with confidence measures  [N0, NN, NN]
Moreover, in recent years, SGM [N] became the preferred disparity optimization method for  most state-of-the-art stereo algorithms (e.g., [NN, N0])  Considering these facts, we believe that this field deserves a further and deeper analysis
Therefore, in this paper  we aim at i) extending and updating the taxonomy provided  in [NN] including novel confidence measure and in particular those based on machine learning techniques, ii) exhaustively assessing their performance on the larger and much  more challenging datasets [NN, NN] available today, iii) understanding the impact of training data on the effectiveness  of confidence measures based on machine learning, iv) assessing their performance when dealing with new data and  state-of-the-art stereo algorithms, v) and evaluating their behavior when plugged into a state-of-the-art stereo pipeline
 Although our focus is mostly on approaches based on  machine learning, for completeness, we include in our taxonomy and evaluation any available confidence measure
 NNNNN    Overall, we assess the performance of NN measures, actually NN considering their variants, providing an exhaustive  evaluation of state-of-the-art in this field with three stereo  algorithms on the three challenging datasets with groundtruth KNN, KNN and MNN available today
 N
Related work  The most recent taxonomy and evaluation of confidence  measures for stereo was proposed by Hu and Mordohai  [NN]
They exhaustively categorized the NN confidence measures available at that time in six categories according to  the cues exploited to infer depth reliability
Moreover, they  proposed an effective metric to clearly assess the effectiveness of confidence prediction based on area under the curve  (AUC) analysis and quantitatively evaluated the considered  measures on former indoor Middlebury [NN, NN] and outdoor Fountain PNN [NN] datasets with a standard local algorithm using sum of absolute differences (SAD) and normalized cross correlation (NCC) as matching costs
 However, since then novel confidence measures were  proposed [NN, NN, NN, N, N, N, NN, NN, NN] and more importantly this field was affected by methodologies inspired  by machine learning
In their seminal work, Hausler et al
 [N] proposed to infer match reliability by feeding a random  forest, trained for classification, with multiple confidence  measures showing that this fusion strategy yields much better performance with respect to any other considered confidence measure
Following this strategy, the reliability of  confidence measures was further improved in [NN] and [NN]  considering more effective features
In this context, [NN]  enables to infer a confidence measure leveraging only features extracted in constant time from the left disparity map
 Differently from [N], in [NN, NN, NN] the random-forests  are trained in regression mode
Concerning methodologies  based on Convolutional Neural Networks (CNN), Seki and  Pollefeys [N0] proposed to infer a confidence measure by  processing features extracted from the left and right disparity maps while Poggi and Mattoccia [NN] learned from  scratch a confidence measure by feeding to a CNN the left  disparity map
Moreover, in [NN] was proposed a method  to combine multiple hand-crafted cues and in [NN] a strategy to improve confidence accuracy exploiting local consistency
Concerning unsupervised training of confidence  measures, Mostegel et al
[NN] proposed to determine training labels exploiting contradictions between multiple depth  maps computed from different viewpoints while Tosi et al
 [NN] leveraging on a pool of confidence measures
 This field has also seen the deployment of confidence  measures plugged into stereo vision pipelines to improve  the overall accuracy as proposed in [NN, NN, NN, N0, NN,  NN, NN, NN, N], to deal with occlusions [N, NN] or to improve accuracy near depth discontinuities [N]
Most of  these approaches are aimed at improving the accuracy of  Semi Global Matching (SGM) [N] algorithm exploiting as  cue an estimated match reliability
Confidence measures  have been effectively deployed for sensor fusion combining  depth maps from multiple sensors [N0, NN]
Finally, confidence measures suited for embedded stereo systems have  been analyzed in [NN]
 Recent years have also witnessed the availability of very  challenging datasets depicting indoor, such as the MNN [NN],  and outdoor environments, such as KNN [N] and KNN [NN]
 Differently from former standard dataset [NN] used to test  algorithms, the novel ones clearly emphasize that stereo is  still an open research problem
This fact also paved the  way to most recent trend in stereo vision aimed at tackling stereo with CNNs
In this context [NN] Zbontar and  Le Cun proposed the first successful attempt to infer an effective matching cost from a stereo pair with a CNN now  deployed by almost any top-performing stereo method on  KNN, KNN and MNN datasets
Following this strategy Chen  et al
[N] and Luo et al
[NN] proposed very efficient architectures enabling real-time stereo matching while [N0]  enables to combine multiple disparity maps with a CNN
 A further step forward, aimed at departing from a conventional stereo pipeline, is represented by Mayer et al
[NN]
 In this case, given a stereo pair, the left-right stereo correspondence is regressed from scratch with a CNN trained  end-to-end
 N
Taxonomy of confidence measures  Despite the large number of confidence measures proposed, all of them process (a subset of) information concerning the cost curve, the relationship between left and  right images or disparity maps
Following [NN], confidence  measures can be grouped into categories according to their  input cues
To better clarify which cues are processed by  each single measure we introduce the following notation
 Given a stereo pair made of left (L) and right (R) images,  a generic stereo algorithm assigns a cost curve c to each  pixel of L
We denote the minimum of such curve as cN and  its corresponding disparity hypothesis as dN
We refer to  the second minimum of the curve as cN (and to its disparity  hypothesis as dN), while cNm denotes the second local minimum (it may coincide with cN)
In our taxonomy we group  the considered NN confidence measures (and their variants)  in the following N categories
 N.N
Minimum cost and local properties of the cost curve  These methods analyze local properties of the cost curve  encoded by cN, cN and cNm
As confidence values for each  point, the matching score measure (MSM) [NN] simply assumes the negation of minimum cost cN
Maximum margin  (MM) computes the difference between cNm and cN while  its variant maximum margin naive (MMN) [NN] replaces  NNNN    cNm with cN
Non linear margin (NLM) [N] computes a  non linear transformation according to the difference between cNm and cN while its variant non linear margin naive  (NLMN) replaces cNm with cN
Curvature (CUR) [NN] and  local curve LC [NN] analyze the behavior of the cost curve  around the minimum cN and its two neighbors at (dN-N) and  (dN+N) according two similar, yet different, strategies
Peak  ratio (PKR) [N0, NN] computes the ratio between cNm and  cN
In one of its variants, peak ratio naive (PKRN) [NN],  cNm is replaced with the second minimum cN
In average  peak ratio (APKR) [NN] the confidence value is computed  averaging PKR values on a patch
We include in our evaluation a further variant, based on the same patch-based average strategy adopted by APKR and referred to as average peak ratio naive (APKRN)
Similarly and respectively,  weighted peak ratio (WPKR) [NN] and weighted peak ratio naive (WPKRN), average on a patch the original confidence measures PKR and PKRN with binary weights computed according to the reference image content
Finally, we  include in this category two confidence measures belonging  to the pool of features proposed in [N]
Disparity ambiguity measure (DAM) computes the distance between dN and  dN, while semi-global energy (SGE) relies on a strategy inspired by the SGM algorithm [N]
It sums, within a patch,  the cN costs of points laying on multiple scanlines penalized,  if their disparity is not the same of the point under examination, by PN when the difference is N and by PN (>PN)  otherwise
 N.N
Analysis of the entire cost curve  Differently from previous confidence measures, those  belonging to this category analyze for each point the overall distribution of matching costs
Perturbation (PER) [N]  measures the deviation of the cost curve to an ideal one
 Maximum likelihood measure (MLM) [NN, NN] and attainable likelihood measure (ALM) [NN, NN] infer from the  matching costs a probability density function (pdf) with respect to an ideal cN, respectively, equal to zero for MLM and  to the actual cN for ALM
Number of inflections (NOI) [NN]  determines the number of local minima in the cost curve  while local minima in neighborhood (LMN) [NN] counts,  on a patch, the number of points with local minimum at the  same disparity dN of the examined point
Winner margin  measure (WMN) [NN] normalizes for each point the difference between cNm and cN by the sum of all costs while its  variant winner margin measure naive (WMNN) [NN] adopts  the same strategy replacing cNm with cN
Finally, negative  entropy measure (NEM) [NN, NN] relates the degree of uncertainty of each point to the negative entropy of its matching costs
 N.N
Left and right consistency  This category evaluates the consistency between corresponding points according to two different cues: one, symmetric, based on left and right maps and one, asymmetric,  based only on the left map
Confidence measures adopting  the first strategy are: left-right consistency (LRC) [N, NN],  that assigns as confidence the negation of the absolute difference between the disparity of a point in L and its homologous point in R, and left-right difference (LRD) [NN] that  computes the difference between cN and cN divided by the  absolute difference between cN and the minimum cost of the  homologous point in R
We include in this category zeromean sum of absolute differences (ZSAD) [N] that evaluates the dissimilarity between patches centered on homologous points in the stereo pair
It is worth pointing out that  for LRC and ZSAD the full cost volume is not required
 On the other hand, confidence measures based only on the  analysis of the reference disparity map exploit the uniqueness constraint
Asymmetric consistency check (ACC) [NN]  and uniqueness constraint (UC) [N] detect the pool of multiple colliding points at the same coordinate in the right image
ACC verifies, according to a binary strategy, whether  the candidate with the largest disparity in the pool has the  smallest cost with respect to any other one while UC simply selects as valid the candidate with the minimum cost
 Moreover, we consider two further non binary variants of  this latter strategy
One referred to as uniqueness constraint  cost (UCC), that assumes as confidence the negative of cN,  and one referred to as uniqueness constraint occurrences  (UCO), that assumes that confidence is inversely proportional to the number of collisions
For the latter four outlined strategies the other candidates in the pool of colliding  points are always set to invalid
 N.N
Disparity map features  Confidence measures belonging to this group are obtained by extracting features from the reference disparity  map
Therefore they are potentially suited to infer confidence for any ND sensing device
Distance to discontinuity  (DTD) [NN, NN] determines for each point the distance to  the supposed closest depth boundary while, for the same  purpose, disparity map variance (DMV) computes the disparity gradient module [N]
Remaining confidence measures  belonging to this category extract features on a patch centered on the examined point
Variance of disparity (VAR)  [NN, NN] computes the disparity variance, disparity agreement (DA) [NN] counts the number of points having the  same disparity of the central one, median deviation of disparity (MDD) [NN, NN, NN] computes the difference between  disparity and its median and disparity scattering (DS) [NN]  encodes the number of different disparity assignments on  the patch
 NNN0    N.N
Reference image features  Confidence measures belonging to this category use as  domain only the reference image
Distance to border (DB)  [NN, NN] aims at detecting invalid disparity assignments often originated in the image border due to the stereo setup
 Assuming the left image as reference a more meaningful  variant of DB, referred to as distance to left border (DLB),  deploys the distance to the left border
Both measures rely  on prior information and not on image content
The last two  confidence measure of this category extract features from  the reference image: horizontal gradient measure (HGM)  [N, NN] analyses the response to horizontal gradients in order to detect image texture while distance to edge (DTE)  attempts to detect depth boundaries, sometimes unreliable  for stereo algorithms, according to the distance to the closest edge
 N.N
Image distinctiveness  The idea behind these confidence measures is to exploit  the notion of distinctiveness of the examined point within  its neighborhoods along the horizontal scanline of the same  image
Distinctiveness (DTS) [NN, NN] exactly leverages  on such definition by assuming as confidence for a given  point the lowest self-matching cost computed within a certain prefixed range excluding the point under examination
 Distinctive similarity measure (DSM) [NN, NN] assigns as  confidence value to a given point the product of two DTSs,  one computed on the reference image and the other one on  the right image in the location of the assumed homologous  point, divided by the square of cN [NN] or cN [NN]
For a given  point the self-aware matching measure (SAMM) [NN, NN]  computes the zero mean normalized correlation between the  left-right cost curve, appropriately translated according to  the assumed disparity, and the left-left cost curve
 N.N
Learning-based approaches  Recently, some authors proposed to infer confidence  measures exploiting machine learning frameworks
A common trend in such approaches consists in feeding a random  forest classifier with multiple confidence measures [N, NN,  NN, NN] or deploying for the same purpose deep learning  architectures [N0, NN]
A notable difference with conventional confidence measures reviewed so far, is that learningbased approaches require a training phase, on datasets with  ground-truth or by means of appropriate methodologies  [NN, NN], to infer the degree of uncertainty of disparity assignments
 N.N.N Random forest approaches  In this category a seminal approach is represented by ensemble learning (ENSc) [N]
This method infers a confidence measure by feeding to a random forest, trained  for classification, a feature vector made of NN confidence measures extracted from the original stereo pair, the  left and right disparity maps and the cost volumes computed on the stereo pair at different scales
Then, the  resulting features are up-sampled to the original resolution
The feature vector consists of the following measures: PKRN,N,N, NEMN,N,N, PERN,N,N, LRCN, HGMN,N,N,  DMVN,N,N, DAMN,N,N, ZSADN,N,N and SGEN
The superscript refers to the scale: N original resolution, N halfresolution and N quarter-resolution
The authors advocate  to train the random-forest with such feature vector for classification ”as confidence measures do not contain matching  error magnitude information”, by extracting the posterior  probability of the predicted class at inference time
However, the average response over all the trees in the forest  can be used as well by training in regression
Therefore,  we also include in our evaluation ensemble learning in regression mode (ENSr) that to the best of our knowledge  has not been considered before
In ground control point  (GCP) [NN] the confidence measure is inferred by feeding to  a random forest, trained in regression mode, a feature vector  containing N measures computed at the original scale
The  features extracted from left image, left and right disparity  maps and the cost volume are: MSM, DB, MMN, AML,  LRC, LRD, DTD and MDD
In leveraging stereo (LEV)  [NN] a feature vector containing NN measures extracted from  the left image, left and right disparity maps and cost volume is fed to a random forest trained for regression
The  feature vector, superscript encodes the patch size, consists  of: PKR, PKRN, MSM, MM, WMN, MLM, PER, NEM,  LRC, LRD, LC, DTD, VARN,N,N,N, MDDN,N,N,N, HGM and  DLB
Differently from previous approaches, O(N) disparity features (ON) [NN] proposes a method entirely based on  features extracted in constant time from the left disparity  map
The feature vector, superscript encodes the patch size,  consists of: DAN,N,N,N, DSN,N,N,N, MEDN,N,N,N, MDDN,N,N,N  and VARN,N,N,N, being MED the median of disparity
As for  ENSr, GCP and LEV the feature vector is fed to a random  forest trained in regression mode
We conclude this section  observing that ENS [N] and LEV [NN] also propose variants  of the original method with a reduced number of features,  respectively N and N
For LEV, the features are selected analyzing the importance of variable once trained the random  forest with the full NN feature vector and then retraining the  network
However, as reported in [N] and [NN], being higher  the effectiveness of full feature vectors, we consider in our  evaluation such versions of ENS, in classification and regression mode, and LEV
 N.N.N CNN approaches  As for many other computer vision fields, convolutional  neural networks have recently proven to be very effective  NNNN    also for confidence estimation
In patch based confidence  prediction (PBCP) [N0] the input of a CNN consists of two  channels pN and pN computed, on a patch basis, from left  and right disparity maps
Being patch values strictly related  to their central pixel, confidence map computation is pretty  demanding
A faster solution, made of patches no longer  related to central pixels, allows for a very efficient confidence map prediction according to common optimization  techniques in deep learning, with a minor reduction of effectiveness
However, being the full-version more effective  we consider this one in our experiments
 A step towards a further abstraction is represented by  confidence CNN (CCNN) [NN]
In fact, in this approach  confidence prediction is regressed by a CNN without extracting any cue from the input data
The deep network,  trained on patches, learns from scratch a confidence measure by processing only the left disparity map
This property, shared with ON, makes these methods potentially  suited to any ND sensor [NN, NN]
 N.N
SGM specific  This category groups two approaches intrinsically related to SGM [N]
The idea behind these approaches is to exploit intermediate results available in such stereo algorithm  to infer a confidence map
Specifically, the local-global  relation (PS) [N0] combines the cues available in the cost  curve before and after semi-global optimization, while sum  of consistent scanlines (SCS) [NN] counts for each pixel the  number of scanlines voting for the same disparity assigned  by the full SGM pipeline
 N
Evaluation protocol and experimental results  In this section, we report exhaustive experimental results  concerning different aspects related to the examined confidence measures on the following datasets KNN (NNN images), KNN (N00 images) and MNN (NN images)
For each  dataset we consider the stereo pairs belonging to the training set being the ground-truth available
We include in the  evaluation all the measures previously reviewed including  any variant
Moreover, for patch-based ones (i.e., APKR,  APKRN, WPKR, WPKRN, DA, DS, MED, VAR) we consider patches of different size (i.e., N × N, N × N, N × N and NN × NN corresponding to superscript N,N,N,N in LEV and ON features) being the scale effective according to [NN, NN]
 Of course, we consider state-of-the-art methods based on  random forests, including variant ENSr, and the two approaches based on CNNs
Overall, we evaluate NN confidence measuresN
In Section N.N we assess with three stereo  algorithms the performance of such measures when dealNSource code available at vision.disi.unibo.it/˜mpoggi/  code.html  ing with the selection of correct matches by means of the  ROC curve analysis proposed in [NN] and widely adopted  in this field [N, NN, NN, NN, NN, N0]
Moreover, since machine learning is the key technology behind most recent approaches, in Section N.N we report how training affects their  effectiveness focusing in particular on the amount of training samples and the capability to generalize across different  data (i.e., datasets)
Finally, being confidence measures often employed to improve stereo accuracy [NN, NN, NN, N0],  in Section N.N we assess the performance of the most effective confidence measures when plugged in one of such  state-of-the-art methods [NN]
 N.N
Detection of correct matches  The ability to distinguish correct disparity assignments  from wrong ones is the most desirable property of a confidence measure
To quantitatively evaluate this, [NN] adopted  ROC curve analysis, measuring the capability of removing  errors from a disparity map according to the confidence values
That is, given a disparity map, a subset p of pixels is  extracted in order of decreasing confidence (e.g., N% of the  total pixels) and the error rate on such sample is computed,  as the percentage of points with an absolute distance from  ground-truth value higher than a threshold τ , varying with  the dataset
Then, the subset is increased by extracting more  pixels (e.g., an additional N%) and the error rate is computed, until all the pixels in the image are considered
Ties  are solved by including all the tying pixels in the subsample
The relation between each sub-sample p and its error  rate draws a ROC curve and its AUC measures the capability of the confidence measure to effectively distinguish  good matches from wrong ones
Considering a disparity  map with a portion ε ∈ [0, N] of erroneous pixels, an opti- mal measure would be able to achieve a 0 error rate when  extracting the first (N − ε) points
Thus, the optimal AUC value [NN] can be obtained as follows  AUCopt =  ∫ ε N−ε  p− (N− ε)  p dp = ε+ (N− ε) ln (N− ε)  (N)  Following this protocol, we evaluate the NN confidence  measures on KNN, KNN and MNN with three popular stereo  algorithms adopting the winner takes all strategy for disparity selection:  • AD-CENSUS: aggregates matching costs, computed  on N × N patches with census transform [NN], with a N× N box-filter
 • MC-CNN [NN]: local method inferring costs from image patches using a CNN
We used the same networks  trained by the authors on KNN, KNN and MNN
 NNNN  vision.disi.unibo.it/~mpoggi/code.html vision.disi.unibo.it/~mpoggi/code.html   (a)  KNN (ε = NN.NN%) KNN (ε = NN.NN%) MNN (ε = NN.NN%)  Category measure rank AUC measure rank AUC measure rank AUC  N.N APKRNN N NN 0.NN0N APKRNN N  NN 0.NNNN APKRNN N N 0.NNNN  N.N WMNN NNN 0.NNNN WMN NNN 0.N0NN WMN NNN 0.NNNN  N.N LRD NN0 0.NNNN LRD NNN 0.NNNN LRD NNN 0.NNNN  N.N DANN N N 0.NNNN DANN N  N 0.NNNN DANN N N 0.NNNN  N.N DB NNN 0.NNNN DB NNN 0.NN0N DLB NNN 0.NNNN  N.N SAMM NNN 0.N0N0 SAMM NN0 0.NNNN DSM NN0 0.NNNN  N.N.N ON NN 0.NN0N ON NN 0.NNNN ON NN 0.NNNN  N.N.N CCNN NN 0.NNNN CCNN NN 0.N0NN CCNN NN 0.NNNN  Optimal 0.N0NN 0.0NNN 0.0NNN  Categories N.N.N and N.N.N  Measure KNN KNN MNN  ENSc N NN NN  ENSr N N NN  GCP N N N  LEV N N N  ON N N N  PBCP N N N  CCNN N N N  (b)  (c)  KNN (ε = NN.N0%) KNN (ε = NN.NN%) MNN (ε = NN.N0%)  Category measure rank AUC measure rank AUC measure rank AUC  N.N APKRNN N NN 0.0NNN APKRNN N  NN 0.0N0N APKRNN N N 0.0NNN  N.N WMN NN0 0.0NNN WMN NNN 0.0NNN WMN NNN 0.0NNN  N.N LRD NNN 0.0NNN LRD NNN 0.0NNN UCC NNN 0.0NNN  N.N DSN N N 0.0NNN DSN N  N 0.0NNN DSNN N NN 0.N0NN  N.N DLB NNN 0.NNNN HGM NNN 0.NNNN DLB NNN 0.NNN0  N.N SAMM NNN 0.0NNN SAMM NNN 0.0NNN DSM NN0 0.NNNN  N.N.N ON NN 0.0NNN ON NN 0.0NNN ON NN 0.0NN0  N.N.N CCNN NN 0.0NNN CCNN NN 0.0NNN CCNN NN 0.0NNN  Optimal 0.0NNN 0.0NNN 0.0NNN  Categories N.N.N and N.N.N  Measure KNN KNN MNN  ENSc N N NN  ENSr N N NN  GCP N N NN  LEV N N N  ON N N N  PBCP N N N  CCNN N N N  (d)  (e)  KNN (ε = NN.NN%) KNN (ε = NN.NN%) MNN (ε = NN.NN%)  Category measure rank AUC measure rank AUC measure rank AUC  N.N APKRNN N N 0.0NNN APKRNN N  N 0.0NNN APKRN N N 0.0NNN  N.N WMN NNN 0.0NNN WMN NNN 0.0N0N WMN NN 0.0.NNN  N.N UCC NNN 0.0NNN UCC NNN 0.0NN0 UCC NNN 0.0NNN  N.N DSNN N NN 0.0NNN DSNN N  NN 0.0N0N DSNN N NN 0.0NNN  N.N DB NNN 0.NNNN DB NNN 0.NNNN DLB NN0 0.NNNN  N.N DSM NNN 0.0NNN DSM NNN 0.0NNN DSM NNN 0.N0NN  N.N.N LEV NN 0.0NNN ON NN 0.0NNN ON NN 0.0NNN  N.N.N CCNN NN 0.0NNN CCNN NN 0.0N0N CCNN NN 0.0NNN  N.N SCS NNN 0.0NNN SCS NNN 0.0NN0 SCS NNN 0.N0N0  Optimal 0.0NNN 0.0NNN 0.0NNN  Categories N.N.N and N.N.N  Measure KNN KNN MNN  ENSc NN NN NN  ENSr N N NN  GCP N N NN  LEV N N NN  ON N N N  PBCP N N N  CCNN N N N  (f)  Table N
Detection of correct matches with three stereo algorithms - top (a,b) AD-CENSUS, middle (c,d) MC-CNN and bottom (e,f) SGM  - and three datasets KNN, KNN and MNN
For each algorithm there are two tables
On the left the best confidence measure for each category  (e.g., N.N refers to measures belonging to the category reviewed in Section N.N), the ranking (within categories and, in superscript, absolute)  and the AUC
On the right, the absolute ranking of learning-based confidence measures
We also report average error rate ε for each  dataset on the top labels
Concerning categories N.N.N and N.N.N we trained each confidence measure on the first N0 images of KNN with the  considered algorithm (i.e., (a,b) with AD-CENSUS, (c,d) with MC-CNN and (e,f) with SGM)
 • SGM [N]: eight scanline implementation with ADCENSUS aggregated costs as data term and PN and PN,  respectively, 0.N and 0.N (being costs normalized)
 Concerning confidence measures based on machine  learning, for each stereo algorithm, we train each one on a  subset of images from the KNN dataset (the first N0 images,  extracting a sample from each pixel with available groundtruth, for a total of N.N million samples) and evaluate it on  all the datasets (for KNN excluding the training images), in  order to assess their performance on very different scenes
 For approaches based on random forests we train on N0 trees  as suggested in [NN] and adopting a fixed number of iteration as termination criteria (e.g., proportional to the number  of trees), while we train CNN based measures for NN epochs  (resulting in about N million iterations), with a batch of size  NN, learning rate of 0.00N and momentum of 0.N, by minimizing the loss functions reported in [NN, N0]
Different  training sets (e.g., datasets, number of samples and so on)  may lead to different performance
This fact will be thoroughly evaluated in Section N.N
For the evaluation reported  in this section we trained only on KNN in order to assess  how much a confidence measure is able to generalize its behavior across different datasets which is an important and  desirable feature in most practical applications
We adopt  as error bound τ = N for KNN and KNN and τ = N for MNNN  as suggested in the corresponding papers
 In Table N we summarize results in terms of AUC averaged on each dataset (KNN, KNN and MNN) for AD-CENSUS  (a,b), MC-CNN (c,d) and SGM (d,e), reporting the averNMiddlebury frames have been processed at quarter resolution to level  out the original disparity range with other datasets (N00 vs NNN for KITTIs)
 NNNN    age error rate ε for each dataset
For each algorithm we  report on the left table the best measure for each category  described in Section N and its absolute ranking and, on the  right table, the absolute ranking for confidence measures  based on machine learning
Observing tables N (a,c,e),  we can notice that these latter measures always yield the  best results, with CCNN systematically the top-performing  one in terms of AUC, and the ones based on random forest following very close (with ON the best in its category  in N out of N experiments)
Focusing on categories N.N.N  and N.N.N, we can notice that in most cases PBCP, ON and  LEV perform very well with the exception of the SGM algorithm and MNN (Table N(f))
In this specific case, excluding CCNN, APKRNN performs better than approaches  based on machine learning
Anyway, in this case too, the  effectiveness of ON and PBCP seems acceptable
This fact  highlights that some confidence measure based on learning  approaches (in particular CCNN but also ON and PBCP)  have excellent performance across different data
Interestingly, such measures use as input cue only the disparity  maps
Tables N (b,d,f) also show that for other measures  such as ENSc, ENSr, GCP and LEV this behavior is not  always verified, in particular with MNN
Finally, we observe that ENSr always (and sometimes significantly) outperforms ENSc
Concerning other categories, we can notice that APKR yields good results in all the experiments  and not only with MNN and SGM as already highlighted
 Other interesting confidence measures are those belonging  to category N.N and in particular DA with AD-CENSUS and  DS with MC-CNN and SGM
Such results confirm that processing cues from the disparity map only, as done by best  learning-based approaches, yields reliable confidence estimation
Other categories do not seem particularly effective,  especially those based only on left image cues have always  the overall worst performance
For measures belonging to  category N.N, though not very effective excluding experiments with SGM, WMN always achieves the best results
 Besides, it’s worth pointing out that naive versions of traditional strategies produce worse AUC values than their original counterparts
Regarding SGM-specific methods, SCS  always outperforms PS but with AUC values quite far from  the top-performing approaches
Finally, concerning categories N.N and N.N, such measures on the three datasets do  not grant reliable confidence prediction
 N.N
Impact of training data  Having assessed the performance of the confidence measure with different algorithms and datasets, this section aims  at analyzing the impact of training data on the effectiveness  of learning-based measures
To quantitatively compare the  results between different training configuration, we define  ∆k as the ratio between the AUC value achieved by the measure k and the AUCopt as,  Figure N
Ratio between the average AUC achieved by learningbased confidence measures trained with different number of samples from KNN and the optimal AUC
Evaluated on the rest of KNN  with AD-CENSUS algorithm
 ∆k = AUCk  AUCopt (N)  The lower the ∆k, the better the training configuration
The first issue we are going to evaluate is the amount  of training samples required and how it affects the overall effectiveness of each confidence measure
We carried  out multiple trainings with a different number of samples  obtained from N, N0, NN, N0 and NN stereo pairs of KNN  dataset starting from the first image
These subsets provide, respectively, about 0.N, N.N, N, N.N and N.N million  samples with available ground-truth for training
By using  more data we can deploy more complex random forests as  well
Nevertheless, we keep the same parameters and termination criteria described in Section N.N to compare the  behavior of the same forest fed with different feature vectors when more samples are available
Figure N reports ∆k, as a function of the number of training samples, for the best  six measures based on machine learning (i.e., ENSr, GCP,  LEV, ON, CCNN and PBCP) trained on AD-CENSUS algorithm
We can notice how the amount of training data  slightly changes the effectiveness of the methods based on  random forest (less than 0.0N ∆k improvement), highlight- ing how the best AUC is obtained starting from N.N million  samples
Conversely, measures based on CNNs improve  their effectiveness by a significant margin only when trained  on a sufficiently larger amount of data, but such improvement almost saturates at N.N million samples
In particular,  we can observe how CCNN achieves the worst results when  trained with the smallest subset of images, resulting to be  the best measure with a larger training set (with a ∆k mar- gin of about 0.NN)
Excluding LEV and ENSr at N.NM, all  the measures show a monotonic improvement in terms of  AUC by increasing the number of samples
 The second issue evaluated concerns how much a conNNNN    Figure N
Experimental results on MNN
Ratio between the average  AUC achieved by each confidence measure, trained on KNN (blue)  and MNN (orange), and the optimal AUC evaluated on the rest of  MNN with AD-CENSUS algorithm
 fidence measure can generalize across different environments/scenes (i.e., datasets)
To quantitatively evaluate  this behavior, we trained with AD-CENSUS the confidence  measures on a subset of MNN, processing an almost equivalent amount of training samples with respect to the training configuration adopted in Section N.N
Then, we compared the results achieved with this configuration to the one  used in Section N.N with AD-CENSUS on the remaining  data from MNN, computing ∆k as defined in Equation N
A confidence measure achieving similar ∆k in the two con- figuration is able to generalize well between the two very  different scenarios
Figure N plots the two values for the six  confidence measures
We can clearly notice how measures  based on CNNs better generalize with respect to random  forest approaches, with CCNN being more effective in this  sense than PBCP
Moreover, ON appears to better adapt to  different data, achieving a lower margin between the two  ∆k with respect to ENSr, GCP and LEV
This experiment highlights once again that confidence measures using as input cue the disparity map(s) (i.e., CCNN, PBCP and ON)  seem less prone to under-fitting
 N.N
Improvements to stereo accuracy  The final issue we investigated is the impact of confidence measures on stereo accuracy, a topic that recently  gained a lot of attention (e.g., [NN, NN, NN, N0])
For this  evaluation we choose the cost modulation proposed by Park  and Yoon [NN]
The reason is that differently from [NN],  which is specific for SGM algorithm, and [NN, N0], based on  parameters potentially different from measure to measure,  [NN] is suited for any stereo algorithm and parameter-free
 SGM was tuned as reported in Section N.N
We plugged in  [NN] the machine learning based measures, as well as three  standalone measures (i.e., APKR, SAMM and DANN)
On  the three datasets KNN, KNN and MNN, from Table N we can  notice that confidence measures based on machine learnKNN KNN MNN  badN avg badN avg badN avg  SGM NN.NN N.N0 NN.NN N.NN NN.NN N.NN  APKRNN NN.NN N0 N.N0N0 N.NNN0 N.NNN0 NN.NNN N.NNN0  SAMM N0.NNN N.NNN N.NNN N.NNN NN.0NN0 N.NNN  DANN NN.NN N N.N0N N.N0N N.NNN NN.NNN N.N0N  ENSc N0.NN N N.NNN N.0NN N.NNN NN.NNN N.00N  ENSr N0.NN N N.NNN N.0NN N.NNN NN.NNN N.NNN  GCP NN.0NN N.NNN N.NNN N.NNN NN.NNN N.NNN  LEV N0.NNN N.NNN N.NNN N.NNN NN.NNN N.NNN  ON N0.NNN N.NNN N.NNN N.NNN NN.NNN N.0NN  PBCP N0.NNN N.N0N N.NNN N.NNN NN.NNN N.NNN  CCNN N0.NNN N.NNN N.NNN N.N0N NN.NNN N.NNN  Table N
Error rate (percentage) and average pixel error on the  three datasets achieved by vanilla SGM (first row) and the confidence modulation proposed in [NN] plugging: APKRNN, SAMM,  DANN, ENSc, ENSr , GCP, LEV (the one proposed in [NN]), ON,  PBCP and CCNN
Learning-based confidence measures trained,  with AD-CENSUS, on the first N0 images of KNN
 ing are overall more effective than other ones
In particular, ON achieves the lowest error rate with KNN and CCNN  and PBCP outperforms other ones in KNN and MNN
This  experiment highlights that there is not a direct relationship  with the effectiveness of the confidence measure in terms of  AUC
However, most effective confidence measures (i.e.,,  CCNN, PBCP and ON) according to this metric achieve  the best results
Finally we point out that in this experiments, ENSc and ENSr, frequently perform better than others confidence measures, conventional and learning-based  ones
Moreover, for their deployment in cost modulation  ENSc outperforms ENSr most of the times, conversely to  what observed in terms of AUC
 N
Conclusions  In this paper we have reviewed and evaluated state-ofthe-art confidence measures focusing our attention on recent  ones based on machine learning techniques
Our exhaustive evaluation, with three stereo algorithms and three large  and challenging datasets, clearly highlights that learningbased ones are much more effective than conventional approaches
In particular, those using as input cue the disparity maps achieve better results in terms of detection of  correct match, capability to adapt to new data and effectiveness to improve stereo accuracy
In such methods training  is certainly an additional issue but, as reported in our evaluation, the overall amount of training data required is limited  and best learning-based confidence measures much better  generalize to new data
 Acknowledgement  We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used  for this research
 NNNN    References  [N] Z
Chen, X
Sun, L
Wang, Y
Yu, and C
Huang
A deep  visual correspondence embedding model for stereo matching  costs
In Proceedings of the IEEE International Conference  on Computer Vision, pages NNN–NN0, N0NN
N  [N] L
Di Stefano, M
Marchionni, and S
Mattoccia
A fast areabased stereo matching algorithm
Image and vision computing, NN(NN):NNN–N00N, N00N
N, N  [N] G
Egnal, M
Mintz, and R
P
Wildes
A stereo confidence  metric using single view imagery
In PROC
VISION INTERFACE, pages NNN–NN0, N00N
N  [N] F
Garcia, B
Mirbach, B
E
Ottersten, F
Grandidier, and  I
Cuesta-Contreras
Pixel weighted average strategy for  depth sensor data fusion
In ICIP, pages NN0N–NN0N
IEEE,  N0N0
N  [N] A
Geiger, P
Lenz, C
Stiller, and R
Urtasun
Vision meets  robotics: The kitti dataset
Int
J
Rob
Res., NN(NN):NNNN–  NNNN, sep N0NN
N, N  [N] R
Gherardi
Confidence-based cost modulation for stereo  matching
In Pattern Recognition, N00N
ICPR N00N
NNth  International Conference on, pages N–N, Dec N00N
N  [N] R
Haeusler and R
Klette
Evaluation of stereo confidence  measures on synthetic and recorded image data
In N0NN International Conference on Informatics, Electronics and Vision, ICIEV N0NN, pages NNN–NNN, N0NN
N, N  [N] R
Haeusler, R
Nair, and D
Kondermann
Ensemble learning for confidence measures in stereo vision
In CVPR
Proceedings, pages N0N–NNN, N0NN
N
N, N, N, N, N  [N] H
Hirschmuller
Stereo processing by semiglobal matching and mutual information
IEEE Transactions on Pattern  Analysis and Machine Intelligence (PAMI), N0(N):NNN–NNN,  feb N00N
N, N, N, N, N  [N0] H
Hirschmüller, P
R
Innocent, and J
Garibaldi
Real-time  correlation-based stereo vision with reduced border errors
 Int
J
Comput
Vision, NN(N-N), apr N00N
N  [NN] H
Hirschmuller, M
Buder, and I
Ernst
Memory efficient  semi-global matching
In ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences,  pages NNN–NNN, N0NN
N  [NN] H
Hirschmller
Evaluation of cost functions for stereo  matching
In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, N00N
N  [NN] X
Hu and P
Mordohai
A quantitative evaluation of confidence measures for stereo vision
IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), pages NNNN–  NNNN, N0NN
N, N, N, N, N  [NN] S
Kim, D
g
Yoo, and Y
H
Kim
Stereo confidence metrics using the costs of surrounding pixels
In N0NN NNth International Conference on Digital Signal Processing, pages  NN–N0N, Aug N0NN
N, N  [NN] S
Kim, C
Y
Jang, and Y
H
Kim
Weighted peak ratio for  estimating stereo confidence level using color similarity
In  N0NN IEEE Asia Pacific Conference on Circuits and Systems  (APCCAS), pages NNN–NNN, Oct N0NN
N, N  [NN] D
Kong and H
Tao
A method for learning matching errors  in stereo computation
In British Machine Vision Conference  (BMVC), N00N N00N
N  [NN] S
Lefebvre, S
Ambellouis, and F
Cabestaing
A  colour correlation-based stereo matching using ND windows
In IEEE, editor, Third International IEEE Conference on Signal-Image Technologies and Internet-Based System, SITIS’0N, pages N0N–NN0, Shanghai, China, Dec N00N
 IEEE
N  [NN] W
Luo, A
G
Schwing, and R
Urtasun
Efficient Deep  Learning for Stereo Matching
In Proc
CVPR, N0NN
N  [NN] R
Manduchi and C
Tomasi
Distinctiveness maps for image  matching
In Image Analysis and Processing, NNNN
Proceedings
International Conference on, pages NN–NN
IEEE, NNNN
 N  [N0] G
Marin, P
Zanuttigh, and S
Mattoccia
Reliable fusion of  tof and stereo depth driven by confidence measures
In NNth  European Conference on Computer Vision (ECCV N0NN),  pages NNN–N0N, N0NN
N, N  [NN] L
Matthies
Stereo vision for planetary rovers: Stochastic  modeling to near real-time implementation
Int
J
Comput
 Vision, N(N), jul NNNN
N  [NN] N
Mayer, E
Ilg, P
Häusser, P
Fischer, D
Cremers,  A
Dosovitskiy, and T
Brox
A large dataset to train convolutional networks for disparity, optical flow, and scene flow  estimation
In The IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), N0NN
N, N  [NN] M
Menze and A
Geiger
Object scene flow for autonomous  vehicles
In Conference on Computer Vision and Pattern  Recognition (CVPR), N0NN
N, N  [NN] P
Merrell, A
Akbarzadeh, L
Wang, P
Mordohai, J
M
 Frahm, R
Yang, D
Nister, and M
Pollefeys
Real-time  visibility-based fusion of depth maps
In N00N IEEE NNth International Conference on Computer Vision, pages N–N, Oct  N00N
N, N  [NN] D
B
Min and K
Sohn
An asymmetric post-processing  for correspondence problem
Sig
Proc.: Image Comm.,  NN(N):NN0–NNN, N0N0
N, N  [NN] P
Mordohai
The self-aware matching measure for stereo
In  The International Conference on Computer Vision (ICCV),  pages NNNN–NNNN
IEEE, N00N
N, N  [NN] C
Mostegel, M
Rumpler, F
Fraundorfer, and H
Bischof
 Using self-contradiction to learn confidence measures in  stereo vision
In The IEEE Conference on Computer Vision  and Pattern Recognition (CVPR), N0NN
N, N  [NN] M
G
Park and K
J
Yoon
Leveraging stereo matching with  learning-based confidence measures
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  June N0NN
N, N, N, N, N, N, N  [NN] D
Pfeiffer, S
Gehrig, and N
Schneider
Exploiting the  power of stereo confidences
In IEEE Computer Vision and  Pattern Recognition, pages NNN–N0N, Portland, OR, USA,  June N0NN
N  [N0] M
Poggi and S
Mattoccia
Deep stereo fusion: combining  multiple disparity hypotheses with deep-learning
In Proceedings of the Nth International Conference on ND Vision,  NDV, N0NN
N  [NN] M
Poggi and S
Mattoccia
Learning a general-purpose confidence measure based on o(N) features and a smarter aggregation strategy for semi global matching
In Proceedings of  NNNN    the Nth International Conference on ND Vision, NDV, N0NN
 N, N, N, N, N, N  [NN] M
Poggi and S
Mattoccia
Learning from scratch a confidence measure
In Proceedings of the NNth British Conference on Machine Vision, BMVC, N0NN
N, N, N, N, N  [NN] M
Poggi and S
Mattoccia
Learning to predict stereo reliability enforcing local consistency of confidence maps
In The  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July N0NN
N  [NN] M
Poggi, F
Tosi, and S
Mattoccia
Efficient confidence  measures for embedded stereo
In NNth International Conference on Image Analysis and Processing (ICIAP N0NN),  September N0NN
N  [NN] M
Poggi, F
Tosi, and S
Mattoccia
Even more confident predictions with deep machine-learning
In NNth IEEE  Embedded Vision Workshop (EVWN0NN) held in conjunction with IEEE Conference on Computer Vision and Pattern  Recognition (CVPR), July N0NN
N  [NN] N
Sabater, A
Almansa, and J
M
Morel
Meaningful  Matches in Stereovision
IEEE Transactions on Pattern  Analysis and Machine Intelligence (PAMI), NN(N):NN0–NN,  dec N0NN
N  [NN] D
Scharstein, H
Hirschmller, Y
Kitajima, G
Krathwohl,  N
Nesic, X
Wang, and P
Westling
High-resolution stereo  datasets with subpixel-accurate ground truth
In GCPR,  pages NN–NN
N, N  [NN] D
Scharstein and R
Szeliski
Stereo matching with nonlinear diffusion
International Journal of Computer Vision,  NN:NNN–NNN, NNNN
N  [NN] D
Scharstein and R
Szeliski
A taxonomy and evaluation  of dense two-frame stereo correspondence algorithms
Int
J
 Comput
Vision, NN(N-N):N–NN, apr N00N
N, N  [N0] A
Seki and M
Pollefeys
Patch based confidence prediction  for dense disparity map
In British Machine Vision Conference (BMVC), N0NN
N, N, N, N, N, N  [NN] A
Spyropoulos, N
Komodakis, and P
Mordohai
Learning  to detect ground control points for improving the accuracy  of stereo matching
In The IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), pages NNNN–NNNN
 IEEE, N0NN
N, N, N, N, N, N  [NN] C
Strecha, W
von Hansen, L
J
V
Gool, P
Fua, and  U
Thoennessen
On benchmarking camera calibration and  multi-view stereo for high resolution imagery
In N00N  IEEE Computer Society Conference on Computer Vision and  Pattern Recognition (CVPR), NN-NN June N00N, Anchorage,  Alaska, USA, N00N
N  [NN] F
Tosi, M
Poggi, A
Tonioni, L
Di Stefano, and S
Mattoccia
Learning confidence measures in the wild
In NNth  British Machine Vision Conference (BMVC N0NN), September N0NN
N, N  [NN] A
Wedel, A
Meiner, C
Rabe, U
Franke, and D
Cremers
 Detection and Segmentation of Independently Moving Objects from Dense Scene Flow
In Proceedings of the Nth  International Conference on Energy Minimization Methods  in Computer Vision and Pattern Recognition, pages NN–NN,  Bonn, Germany, August N00N
Springer
N  [NN] K.-J
Yoon and I.-S
Kweon
Distinctive similarity measure  for stereo matching under point ambiguity
Computer Vision  and Image Understanding, NNN(N):NNN–NNN, N00N
N  [NN] R
Zabih and J
Woodfill
Non-parametric local transforms  for computing visual correspondence
In Proceedings of  the Third European Conference on Computer Vision (Vol
 II), ECCV ’NN, pages NNN–NNN, Secaucus, NJ, USA, NNNN
 Springer-Verlag New York, Inc
N  [NN] J
Zbontar and Y
LeCun
Stereo matching by training a convolutional neural network to compare image patches
Journal of Machine Learning Research, NN:N–NN, N0NN
N, N, N  NNNNEnd-To-End Face Detection and Cast Grouping in Movies Using Erdos-Renyi Clustering   End-to-end Face Detection and Cast Grouping in Movies  Using Erdős-Rényi Clustering  SouYoung JinN, Hang SuN, Chris StaufferN, and Erik Learned-MillerN  NCollege of Information and Computer Sciences, University of Massachusetts, Amherst NVisionary Systems and Research (VSR)  Figure N: Clustering results from Hannah and Her Sisters
Each unique color shows a particular cluster
It can be seen  that most individuals appear with a consistent color, indicating successful clustering
 Abstract We present an end-to-end system for detecting and clustering faces by identity in full-length movies
Unlike works  that start with a predefined set of detected faces, we consider the end-to-end problem of detection and clustering  together
We make three separate contributions
First,  we combine a state-of-the-art face detector with a generic  tracker to extract high quality face tracklets
We then introduce a novel clustering method, motivated by the classic  graph theory results of Erdős and Rényi
It is based on the  observations that large clusters can be fully connected by  joining just a small fraction of their point pairs, while just  a single connection between two different people can lead  to poor clustering results
This suggests clustering using a  verification system with very few false positives but perhaps  moderate recall
We introduce a novel verification method,  rank-N counts verification, that has this property, and use  it in a link-based clustering scheme
Finally, we define a  novel end-to-end detection and clustering evaluation metric  allowing us to assess the accuracy of the entire end-to-end  system
We present state-of-the-art results on multiple video  data sets and also on standard face databases
 Project page: http://souyoungjin.com/erclustering  This research is based in part upon work supported by the Office of the  Director of National Intelligence (ODNI), Intelligence Advanced Research  Projects Activity (IARPA) under contract number N0NN-NN0NNN000N0
 The views and conclusions contained herein are those of the authors and  should not be interpreted as necessarily representing the official policies or  endorsements, either expressed or implied, of ODNI, IARPA, or the U.S
 Government
The U.S
Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright  annotation thereon
 N
Introduction  The problem of identifying face images in video and  clustering them together by identity is a natural precursor to  high impact applications such as video understanding and  analysis
This general problem area was popularized in the  paper “Hello! My name is...Buffy” [N], which used text  captions and face analysis to name people in each frame of  a full-length video
In this work, we use only raw video  (with no captions), and group faces by identity rather than  naming the characters
In addition, unlike face clustering  methods that start with detected faces, we include detection  as part of the problem
This means we must deal with false  positives and false negatives, both algorithmically, and in  our evaluation method
We make three contributions:  • A new approach to combining high-quality face de- tection [NN] and generic tracking [NN] to improve both  precision and recall of our video face detection
 • A new method, Erdős-Rényi clustering, for large-scale clustering of images and video tracklets
We argue  that effective large-scale face clustering requires face  verification with fewer false positives, and we introduce rank-N counts verification, showing that it indeed  achieves better true positive rates in low false positive  regimes
Rank-N counts verification, used with simple  link-based clustering, achieves high quality clustering  results on three separate video data sets
 • A principled evaluation for the end-to-end problem of face detection and clustering in videos; until now there  has been no clear way to evaluate the quality of such an  NNNN    Figure N: Overview of approach
Given a movie, our approach generates tracklets (Sec
N) and then does Erdős-Rényi  Clustering and FAD verification between all tracklet pairs
(Sec
N) Our final output is detections with unique character Ids
 end-to-end system, but only to evaluate its individual  parts (detection and clustering)
 We structure the paper as follows
In Section N we discuss related work
In Section N, we describe the first phase  of our system, in which we use a face detector and generic  tracker to extract face tracklets
In Section N, we introduce  Erdős-Rényi clustering and rank-N counts verification
Sections N and N present experiments and discussions
 N
Related Work  In this section, we first discuss face tracking and then the  problem of naming TV (or movie) characters
We can divide the character-naming work into two categories: fully  unsupervised and with some supervision
We then discuss  prior work using reference images
Related work on clustering is covered in Section N.N
 Recent work on robust face tracking [NN, NN, NN] has  gradually expanded the length of face tracklets, starting  from face detection results
Ozerov et al
[NN] merge results from different detectors by clustering based on spatiotemporal similarity
Clusters are then merged, interpolated,  and smoothed for face tracklet creation
Similarly, Roth et  al
[NN] generate low-level tracklets by merging detection  results, form high-level tracklets by linking low-level tracklets, and apply the Hungarian algorithm to form even longer  tracklets
Tapaswi et al
[NN] improve on this [NN] by removing false positive tracklets
 With the development of multi-face tracking techniques,  the problem of naming TV charactersN has been also widely  studied [NN, NN, N, N, NN, N0, NN]
Given precomputed face  tracklets, the goal is to assign a name or an ID to a group  of face tracklets with the same identity
Wu et al
[NN, N0]  iteratively cluster face tracklets and link clusters into longer  tracks in a bootstrapping manner
Tapaswi et al
[NN] train  classifiers to find thresholds for joining tracklets in two  stages: within a scene and across scenes
Similarly, we aim  to generate face clusters in a fully unsupervised manner
 NAnother related problem is person re-identification [NN, NN, N] in  which the goal is to tell whether a person of interest seen in one camera has been observed by another camera
Re-identification typically uses  the whole body on short time scales while naming TV characters focuses  on faces, but over a longer period of time
 Though solving this problem may yield a better result  for face tracking, some forms of supervision specific to  the video or characters in the test data can improve performance
Tapaswi et al
[NN] perform face recognition, clothing clustering and speaker identification, where face models  and speaker models are first trained on other videos containing the same main characters as in the test set
In [N, N],  subtitles and transcripts are used to obtain weak labels for  face tracks
More recently, Haurilet et al
[NN] solve the  problem without transcripts by resolving name references  only in subtitles
Our approach is more broadly applicable  because it does not use subtitles, transcripts, or any other  supervision related to the identities in the test data, unlike  these other works [NN, NN, N, N]
 As in the proposed verification system, some existing  work [N, NN] uses reference images
For example, index  code methods [NN] map each single image to a code based  upon a set of reference images, and then compare these  codes
On the other hand, our method compares the relative distance of two images with the distance of one of  the images to the reference set, which is different
In addition, we use the newly defined rank-N counts, rather than  traditional Euclidean or Mahalanobis distance measures to  compare images [N, NN] for similarity measures
 N
Detection and tracking  Our goal is to take raw videos, with no captions or annotations, and to detect all faces and cluster them by identity
We start by describing our method for generating face  tracklets, or continuous sequences of the same face across  video frames
We wish to generate clean face tracklets that  contain face detections from just a single identity
Ideally,  exactly one tracklet should be generated for an identity from  the moment his/her face appears in a shot until the moment  it disappears or is completely occluded
 To achieve this, we first detect faces in each video frame  using the Faster R-CNN object detector [NN], but retrained  on the WIDER face data set [NN], as described by Jiang et  al
[NN]
Even with this advanced detector, face detection  sometimes fails under challenging illumination or pose
In  videos, those faces can be detected before or after the chalNNNN    lenging circumstances by using a tracker that tracks both  forward and backward in time
We use the distribution  field tracker [NN], a general object tracker that is not trained  specifically for faces
Unlike face detectors, the tracker’s  goal is to find in the next frame the object most similar to  the target in the current frame
The extra faces found by the  tracker compensate for missed detections (Fig
N, bottom  of block N)
Tracking helps not only to catch false negatives, but also to link faces of equivalent identity in different  frames
 One simple approach to combining a detector and tracker  is to run a tracker forward and backward in time from every single face detection for some fixed number of frames,  producing a large number of “mini-tracks”
A Viterbi-style  algorithm [N0, N] can then be used to combine these minitracks into longer sequences
This approach is computationally expensive since the tracker is run many times on overlapping subsequences, producing heavily redundant minitracks
To improve performance, we developed the following novel method for combining a detector and tracker
 Happily, it also improves precision and recall, since it takes  advantage of the tracker’s ability to form long face tracks of  a single identity
 The method starts by running the face detector in each  frame
When a face is first detected, a tracker is initialized  with that face
In subsequent frames, faces are again detected
In addition, we examine each current tracklet to see  where it might be extended by the tracking algorithm in the  current frame
We then check the agreement between detection and tracking results
We use the intersection over  union (IoU) between detections and tracking results with  threshold 0.N, and apply the Hungarian algorithm[NN] to establish correspondences among multiple matches
If a detection matches a tracking result, the detection is stored in  the current face sequence such that the tracker can search  in the next frame given the detection result
For the detections that have no matched tracking result, a new tracklet  is initiated
If there are tracking results that have no associated detections, it means that either a) the tracker could  not find an appropriate area on the current frame, or b) the  tracking result is correct while the detector failed to find  the face
The algorithm postpones its decision about the  tracked region for the next α consecutive frames (α = N0)
If the face sequence has any matches with detections within  α frames, the algorithm will keep the tracking results
Otherwise, it will remove the tracking-only results
The second block of Fig
N summarizes our proposed face tracklet  generation algorithm and shows examples corrected by our  joint detection-tracking strategy
Next, we describe our approach to clustering based on low false positive verification
 N
Erdős-Rényi Clustering and Rank-N Counts Verification  In this section, we describe our approach to clustering  face images, or, in the case of videos, face tracklets
We  adopt the basic paradigm of linkage clustering, in which  each pair of points (either images or tracklets) is evaluated  for linking, and then clusters are formed among all points  connected by linked face pairs
We name our general approach to clustering Erdős-Rényi clustering since it is inspired by classic results in graph theory due to Erdős and  Rényi [N], as described next
 Consider a graph G with n vertices and probability p of  each possible edge being present
This is the Erdős-Rényi  random graph model [N]
The expected number of edges  is (  n N  )  p
One of the central results of this work is that, for  ǫ > 0 and n sufficiently large, if  p > (N + ǫ) lnn  n , (N)  then the graph will almost surely be connected (there exists a path from each vertex to every other vertex)
Fig
N  shows this effect on different graph sizes, obtained through  simulation
 0 0.N 0.N 0.N 0.N 0.N  Probability p of each edge  0  0.N  N  P ro  b (c  o n n e c te  d ) N=NNN  N=NNN  N=NNN  N=NN  N=NN  N=NN  Figure N: Simulation of cluster connectedness as a function  of cluster size, N , and the probability p of connecting point  pairs
The figure shows that for various N (different colored  lines), the probability that the cluster is fully connected (on  the y-axis) goes up as more pairs are connected
For larger  graphs, a small probability of connected pairs still leads to  high probability that the graph will be fully connected
 Consider a clustering system in which links are made  between tracklets by a verifier (a face verification system),  whose job is to say whether a pair of tracklets is the “same”  person or two “different” people
While graphs obtained  in clustering problems are not uniformly random graphs,  the results of Erdős and Rényi suggest that this verifier can  have a fairly low recall (percentage of same links that are  connected) and still do a good job connecting large clusters
In addition, false matches may connect large clusters  of different identities, dramatically hurting clustering performance
This motivates us to build a verifier that focuses  on low false positives rather than high recall
In the next  section, we present our approach to building a verifier that  is designed to have good recall at low false positive rates,  NNNN    and hence is appropriate for clustering problems with large  clusters, like grouping cast members in movies
 N.N
Rank-N counts for fewer false positives  Our method compares images by comparing their multidimensional feature vectors
More specifically, we count  the number of feature dimensions in which the two images  are closer in value than the first image is to any of a set  of reference images
We call this number the rank-N count  similarity
Intuitively, two images whose feature values are  “very close” for many different dimensions are more likely  to be the same person
Here, an image is considered “very  close” to a second image in one dimension if it is closer to  the second image in that dimension than to any of the reference images
 More formally, to compare two images IA and IB , our  first step is to obtain feature vectors A and B for these images
We extract N0NN-D feature vectors from the fcN layer  of a standard pre-trained face recognition CNN [NN]
In  addition to these two images, we use a fixed reference set  with G images (we typically set G = N0), and compute CNN feature vectors for each of these reference images.N  Let the CNN feature vectors for the reference images be  RN, RN, ..., RG
We sample reference images from the TV  Human Interactions Dataset [NN], since these are likely to  have a similar distribution to the images we want to cluster
 For each feature dimension i (of the N0NN), we ask  whether  |Ai −Bi| < min j  |Ai −R j i |
 That is, is the value in dimension i closer between A and B  than between A and all the reference images? If so, then we  say that the ith feature dimension is rank-N between A and  B
The cumulative rank-N counts feature R is simply the  number of rank-N counts across all N0NN features:  R = N0NN ∑  i=N  I  [  |Ai −Bi| < min j  |Ai −R j i |  ]  ,  where I[·] is an indicator function which is N if the expres- sion is true and 0 otherwise
 Taking inspiration from Barlow’s notion that the brain  takes special note of “suspicious coincidences” [N], each  rank-N feature dimension can be considered a suspicious  coincidence
It provides some weak evidence that A and  B may be two images of the same person
On the other  hand, in comparing all N0NN feature dimensions, we expect  to obtain quite a large number of rank-N feature dimensions  even if A and B are not the same person
 When two images and the reference set are selected randomly from a large distribution of faces (in this case they  NThe reference images may overlap in identity with the clustering set,  but we choose reference images so that there is no more than one occurrence of each person in the reference set
 are usually different people), the probability that A is closer  to B in a particular feature dimension than to any of the  reference images is just  N  G+ N 
 Repeating this process N0NN times means that the expected  number of rank-N counts is simply  E[R] = N0NN  G+ N ,  since expectations are linear (even in the presence of statistical dependencies among the feature dimensions)
Note that  this calculation is a fairly tight upper bound on the expected  number of rank-N features conditioned on the images being  of different identities, since most pairs of images in large  clustering problems are different, and conditioning on ”different” will tend reduce the expected rank-N count
Now if  two images IA and IB have a large rank-N count, it is likely  they represent the same person
The key question is how to  set the threshold on these counts to obtain the best verification performance
 Recall that our goal, as guided by the Erdős-Rényi random graph model, is to find a threshold on the rank-N  counts R so that we obtain very few false positives (declaring two different faces to be “same”) while still achieving  good recall (a large number of same faces declared to be  “same”)
Fig
N shows distributions of rank-N counts for various subsets of image pairs from Labeled Faces in the Wild  (LFW) [NN]
The red curve shows the distribution of rank-N  counts for mismatched pairs from all possible mismatched  pairs in the entire data set (not just the test sets)
Notice  that the mean is exactly where we would expect with a  gallery size of N0, at N0NN NN  ≈ N0
The green curve shows the distribution of rank-N counts for the matched pairs, which  is clearly much higher
The challenge for clustering, of  course, is that we don’t have access to these distributions  since we don’t know which pairs are matched and which are  not
The yellow curve shows the rank-N counts for all pairs  of images in LFW, which is nearly identical to the distribution of mismatched rank-N counts, since the vast majority  of possibe pairs in all of LFW are mismatched
This is the  distribution to which the clustering algorithm has access
 If the N,0NN CNN features were statistically independent (but not identically distributed), then the distribution  of rank-N counts would be a binomial distribution (blue  curve)
In this case, it would be easy to set a threshold on  the rank-N counts to guarantee a small number of false positives, by simply setting the threshold to be near the right end  of the mismatched (red) distribution
However, the dependencies among the CNN features prevent the mismatched  rank-N counts distribution from being binomial, and so this  approach is not possible
 NNNN    Figure N: LFW distribution of rank-N counts
Each distribution is normalized to sum to N
 Table N: Verification performance comparisons on all possible LFW pairs
The proposed rank-N counts gets much  higher recall at fixed FPRs
 FPR R a  n k  N co  u n  t  L N  T em  p la  te  A d  ap ta  ti o  n [N  ]  R an  k -O  rd er  D is  ta n  ce [ N  N ]  NE-N 0.0NNN 0.00NN 0.00NN 0.00NN  NE-N 0.0NNN 0.00NN 0.00NN 0.00NN  NE-N 0.0NNN 0.0NN0 0.00NN 0.00NN  NE-N 0.NNNN 0.NNNN 0.0NNN 0.00NN  NE-N 0.NN00 0.NNNN 0.0NNN 0.0NNN  NE-N 0.N0NN 0.NN00 0.NNNN 0.NNNN  NE-N 0.NNNN 0.NNNN 0.NNNN 0.NNNN  NE-N 0.NNN0 0.NNNN 0.NN0N 0.NNNN  NE-N 0.NNNN 0.NNNN 0.NNNN 0.NNNN  N.N
Automatic determination of rank-N count threshold  Ideally, if we could obtain the rank-N count distribution  of mismatched pairs of a test set, we could set the threshold  such that the number of false positives becomes very low
 However, it is not clear how to get the actual distribution of  rank-N counts for mismatched pairs at test time
 Instead, we can estimate the shape of the mismatched  pair rank-N count distribution using one distribution (LFW),  and use it to estimate the distribution of mismatched rank-N  counts for the test distribution
We do this by fitting the left  half of the LFW distribution to the left half of the clustering  distribution using scale and location parameters
The reason we use the left half to fit the distribution is that this part  of the rank-N counts distribution is almost exclusively influence by mismatched pairs
The right side of this matched  distribution then gives us an approximate way to threshold  the test distribution to obtain a certain false positive rate
It  is this method that we use to report the results in the leftmost column of Table N
 A key property of our rank-N counts verifier is that it  has good recall across a wide range of the low false positive regime
Thus, our method is relatively robust to the  setting of the rank-N counts threshold
In order to show  that our rank-N counts feature has good performance for the  types of verification problems used in clustering, we construct a verification problem using all possible pairs of the  LFW database [NN]
In this case, the number of mismatched  pairs (quadratic in N ) is much greater than the number of  matched pairs
As shown in Table N, we observe that our  verifier has higher recall than three competing methods (all  of which use the same base CNN representation) at low  false positive rates
 Using rank-N counts verification for tracklet clustering
In our face clustering application, we consider every  pair (I, J) of tracklets, calculate a value akin to the rank-N count R, and join the tracklets if the threshold is exceeded
 In order to calculate an R value for tracklets, we sample a  random subset of N0 face images from each tracklet, compute a rank-N count R for each pair of images, and take the  maximum of the resulting R values
 N.N
Averaging over gallery sets  While our basic algorithm uses a fixed (but randomly selected) reference gallery, the method is susceptible to the  case in which one of the gallery images happens to be similar in appearance to a person with a large cluster, resulting  in a large number of false negatives
To mitigate this effect,  we implicitly average the rank-N counts over an exponential  number of random galleries, as follows
 The idea is to sample random galleries of size g from a  larger super-gallery with G images; we used g = N0, G = N000
We are interested rank-N counts, in which image A’s feature is closer to B than to any of the gallery of size g
 Suppose we know that among the N000 super-gallery images, there are K (e.g., K = N) that are closer to A than B is
The probability that a random selection (with replacement) of g images from the super-gallery would contain  none of the K closer images (and hence represent a rank-N  count) is  r(A,B) =  (  N.0− K  G  )g  
 That is, r(A,B) is the probability of having a rank-N count with a random gallery, and using r(A,B) as the count is equivalent to averaging over all possible random galleries
 In our final algorithm, we sum these probabilities rather  than the deterministic rank-N counts
 N.N
Efficient implementation  For simplicity, we discuss the computational complexity  of our fixed gallery algorithm; the complexity of the average  gallery algorithm is similar
With F , G, and N indicating  the feature dimensionality, number of gallery images, and  NNN0    number of face tracklets to be clustered, the time complexity of the naive rank-N count algorithm is O(F ∗G ∗NN)
However, for each feature dimension, we can sort N test  image feature values and G gallery image feature values in  time O((N +G) log(N +G))
Then, for each value in test image A, we find the closest gallery value, and increment  the rank-N count for the test images that are closer to A
Let  Y be the average number of steps to find the closest gallery  value
This is typically much smaller than N 
The time  complexity is then O(F ∗ [(N +G) log(N +G)+N ∗Y ])
 N.N
Clustering with do-not-link constraints  It is common in clustering applications to incorporate  constraints such as do-not-link or must-link, which specify  that certain pairs should be in separate clusters or the same  cluster, respectively [NN, NN, NN, NN, NN]
They are also often  seen in the face clustering literature [N, NN, N0, NN, NN, NN]
 These constraints can be either rigid, implying they must be  enforced [NN, NN, NN, NN], or soft, meaning that violations  cause an increase in the loss function, but those violations  may be tolerated if other considerations are more important  in reducing the loss [NN, NN, NN, N0, NN]
 In this work, we assume that if two faces appear in the  same frame, they must be from different people, and hence  their face images obey a do-not-link constraint
Furthermore, we extend this hard constraint to the tracklets that  contain faces
If two tracklets have any overlap in time,  then the entire tracklets represent a do-not-link constraint
 We enforce these constraints on our clustering procedure
Note that connecting all pairs below a certain dissimilarity threshold followed by transitive closure is equivalent to single-linkage agglomerative clustering with a joining threshold
In agglomerative clustering, a pair of closest  clusters is found and joined at each iteration until there is a  single cluster left or a threshold met
A naı̈ve implementation will simply search and update the dissimilarity matrix  at each iteration, making the whole process O(nN) in time
There are faster algorithms giving the optimal time complexity O(nN) for single-linkage clustering [NN, NN]
Many of these algorithms incur a dissimilarity update at each iteration, i.e
update d(i, k) = min(d(i, k), d(j, k)) after com- bining cluster i and j (and using i as the cluster id of the  resulting cluster)
If the pairs with do-not-link constraints  are initialized with +∞ dissimilarity, the aforementioned update rule can be modified to incorporate the constraints  without affecting the time and space complexity:  d(i, k) =        min(d(i, k), d(j, k)) d(i, k) N= +∞ AND d(j, k) N= +∞  +∞ otherwise  N
Experiments  We evaluate our proposed approach on three video data  sets: the Big Bang Theory (BBT) Season N (s0N), Episodes  (a) Rank-N Count (b) Rank-Order Distance [NN]  Figure N: Visualization of the combined detection and clustering metric for the first few minutes of the Hannah set
 N-N (e0N-e0N) [N], Buffy the Vampire Slayer (Buffy) Season N (s0N), Episodes N-N (e0N-e0N) [N], and Hannah and  Her Sisters (Hannah) [NN]
Each episode of the BBT and  Buffy data set contains N-N and NN-NN characters respectively, while Hannah has annotations for NNN characters.N  Buffy and Hannah have many occlusions which make the  face clustering problem more challenging
In addition to the  video data sets, we also evaluate our clustering algorithm on  LFW [NN] which contains NNN0 subjects.N  An end-to-end evaluation metric
There are many evaluation metrics used to independently evaluate detection,  tracking, and clustering
Previously, it has been difficult to  evaluate the relative performance of two end-to-end systems  because of the complex trade-offs between detection, tracking, and clustering performance
Some researchers have attempted to overcome this problem by providing a reference  set of detections with suggested metrics [N0], but this approach precludes optimizing complete system performance
 To support evaluation of the full video-to-identity pipeline,  in which false positives, false negatives, and clustering errors are handled in a common framework, we introduce unified pairwise precision (UPP) and unified pairwise recall  (UPR) as follows
 Given a set of annotations, {aN, aN, ..., aA} and detec- tions, {dN, dN, ..., dD}, we consider the union of three sets of tuples: false positives resulting from unannotated face  detections {di, ∅}; valid face detections {di, aj}; and false negatives resulting from unmatched annotations {∅, aj}
Fig
N visualizes every possible pair of tuples ordered by  false positives, valid detections, and false negatives for the  first few minutes of the Hannah data set
Further, groups of  tuples have been ordered by identity to show blocks of identity to aid our understanding of the visualization, although  the order is inconsequential for the numerical analysis
 In Fig N, the large blue region (and the regions it contains) represents all pairs of annotated detections, where  we have valid detections corresponding to their best annotation
In this region, white pairs are correctly clustered,  magenta pairs are the same individual but not clustered,  cyan pairs are clustered but not the same individual, and  NWe removed garbage classes such as ‘unknown’ or ‘false positive’
NAll known ground truth errors are removed
 NNNN    Table N: Clustering performance comparisons on various data sets
The leftmost shows our rankNcount by setting a threshold  automatically
For the rest of the columns, we show f-scores using optimal (oracle-supplied) thresholds
For BBT and Buffy,  we show average scores over six episodes
The full table with individual episode results is given in Supp
Mat
Best viewed  in color (Nst place, Nnd place, Nrd place)
 Verification system + Link-based clustering algorithm Other clustering algorithms  Test set R a  n k  -N C  o u  n t  (a u  to m  a ti  c th  re sh  o ld  )  R a  n k  -N C  o u  n t  L N  T em  p la  te  A d  ap ta  ti o  n [N  ]  R an  k -O  rd er  D is  ta n  ce [ N  N ]  R an  k -O  rd er  D is  ta n  ce  b as  ed C  lu st  er in  g [N  N ]  A ffi  n it  y  P ro  p ag  at io  n [N  N ]  D B  S C  A N  [ N ]  S p  ec tr  al  C lu  st er  in g  [ N N ]  B ir  ch [N  N ]  M in  iB at  ch  K M  ea n  s [ N  0 ]  Video  BBT s0N [N] .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN  Buffy s0N [N] .NNNN .NNNN .NNNN .NNNN .NNN0 .NNNN .NN0N .NN0N .NNNN .NNNN .NNNN  Hannah [NN] .NNNN .NNNN .NNNN .NNN0 .NNNN .NNNN .NNNN .NNN0 .NNNN .NNN0 .N0NN  Image LFW [NN] .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .0NNN .NNNN .NNN0 .NNNN  blue pairs are not clustered pairs from different individuals
 The upper left portion of the matrix represents false positives with no corresponding annotation
The green pairs in  this region correspond to any false positive matching with  any valid detection
The lower right portion of the matrix  corresponds to the false negatives
The red pairs in this  region correspond to any missed clustered pairs resulting  from these missed detections
The ideal result would contain blue and white pairs, with no green, red, cyan, or magenta
 The unified pairwise precision (UPP) is the fraction of  pairs, {di, aj} within all clusters with matching identi- ties, i.e., the number of white pairs divided by the number of white, cyan, and green pairs
UPP decreases if: two  matched detections in a cluster do not correspond to the  same individual; if a matched detection is clustered with  a false positive; for each false positive regardless of its clustering; and for false positives clustered with valid detections
Similarly, the unified pairwise recall (UPR) is the  fraction of pairs within all identities that have been properly  clustered, i.e., the number of white pairs divided by number  of white, magenta, and red pairs
UPR decreases if: two  matched detections of the same identity are not clustered;  a matched detection should be matched but there is no corresponding detection; for each false negative; and for false  negative pairs that should be detected and clustered
The  only way to achieve perfect UPP and UPR is to detect every  face with no false positives and cluster all faces correctly
 At a glance, our visualization in Fig
N shows that our detection produces few false negatives, many more false positives, and is less aggressive in clustering
Using this unified  metric, others can tune their own detection, tracking, and  clustering algorithms to optimize the unified performance  metrics
Note that for image matching without any detection failures, the UPP and UPR reduce to standard pairwise  precision and pairwise recall
 The UPP and UPR can be summarized with a single Fmeasure (the weighted harmonic mean) providing a single,  unified performance measure for the entire process
It can  be α-weighted to alter the relative value of precision and  recall performance:  Fα = N  α UPP  + N−α UPR  (N)  where α ∈ [0, N]
α = 0.N denotes a balanced F-measure
 N.N
Threshold for rank-N counts  The leftmost column in Table N shows our clustering results when the threshold is set automatically by the validation set
We used LFW as a validation set for BBT, Buffy  and Hannah while Hannah was used for LFW
Note that the  proposed method is very competitive even when the threshold is automatically set
 N.N
Comparisons  We can divide other clustering algorithms into two broad  categories–link-based clustering algorithms (like ours) that  use a different similarity function and clustering algorithms  that are not link-based (such as spectral clustering [NN])
 Table N shows the comparisons to various distance functions [N, NN, NN] with our link-based clustering algorithm
 LN shows competitive performance in LFW while the performance drops dramatically when a test set has large pose  variations
We also compare against a recent so-called  “template adaptation” method [N] which also requires a reference set
It takes Nnd and Nrd place on Buffy and BBT
In  addition, we compare to the rank-order method [NN] in two  different ways: link-based clustering algorithms using their  rank-order distance, and rank-order distance based clustering
 In addition, we compare against several generic clustering algorithms (Affinity Propagation [NN], DBSCAN [N],  NNNN    (a) (b) (c) (d) (e)  (f) (g) (h) (i) (j)  Figure N: Clustering results from Buffy the Vampire Slayer
A failure example can be seen in frame (e), in which the main  character Buffy (otherwise in a purple box) in shown in a pink box
 (a) (b) (c) (d) (e)  (f) (g) (h) (i) (j)  Figure N: Clustering results from the Big Bang Theory
A failure example can be seen in frame (d), in which the main  character Howard (otherwise in a magenta box) in shown in a gray box
 Spectral Clustering [NN], Birch [NN], KMeans [N0]), where  LN distance is used as pairwise metric
For algorithms that  can take as input the similarity matrix (Affinity Propagation,  DBSCAN, Spectral Clustering), do-not-link constraints are  applied by setting the distance between the corresponding  pairs to ∞
Note that this is just an approximation, and in general does not guarantee the constraints in the final clustering result (e.g
for single-linkage agglomerative clustering, a modified update rule is also needed in Section N.N)
 Note that all other settings (feature encoding, tracklet  generation) are common for all methods
In Table N, except  for the leftmost column, we report the best F0.N scores using  optimal (oracle-supplied) thresholds for (number of clusters, distance)
The link-based clustering algorithm with  rank-N counts outperforms the state-of-the-art on all four  data sets in F0.N score
Figures N and N show some clustering results on Buffy and BBT
 N
Discussion  We have presented a system for doing end-to-end clustering in full length videos and movies
In addition to a  careful combination of detection and tracking, and a new  end-to-end evaluation metric, we have introduced a novel  approach to link-based clustering that we call Erdős-Rényi  clustering
We demonstrated a method for automatically estimating a good decision threshold for a verification method  based on rank-N counts by estimating the underlying portion  of the rank-N counts distribution due to mismatched pairs
 This decision threshold was shown to result in good recall at a low false-positive operating point
Such operating  points are critical for large clustering problems, since the  vast majority of pairs are from different clusters, and false  positive links that incorrectly join clusters can have a large  negative effect on clustering performance
 There are several things that could disrupt our algorithm:  a) if a high percentage of different pairs are highly similar  (e.g
family members), b) if only a small percentage of pairs  are different (e.g., one cluster contains N0% of the images),  and if same pairs lack lots of matching features (e.g., every cluster is a pair of images of the same person under  extremely different conditions)
Nevertheless, we showed  excellent results on N popular video data sets
Not only do  we dominate other methods when thresholds are optimized  for clustering, but we outperform other methods even when  our thresholds are picked automatically
 NNNN    References  [N] H
Barlow
Cerebral cortex as model builder
In Matters of  Intelligence, pages NNN–N0N
Springer, NNNN
N  [N] M
Bauml, M
Tapaswi, and R
Stiefelhagen
Semisupervised learning with constraints for person identification  in multimedia data
In Proc
CVPR, N0NN
N, N, N  [N] R
G
Cinbis, J
Verbeek, and C
Schmid
Unsupervised metric learning for face identification in TV video
In Proc
 ICCV, N0NN
N  [N] N
Crosswhite, J
Byrne, C
Stauffer, O
M
Parkhi, Q
Cao,  and A
Zisserman
Template adaptation for face verification  and identification
In Face and Gesture, N0NN
N, N, N  [N] S
J
Davey, M
G
Rutten, and B
Cheung
A comparison of  detection performance for several track-before-detect algorithms
EURASIP Journal on Advances in Signal Processing,  N00N:NN, N00N
N  [N] B
DeCann and A
Ross
Modeling errors in a biometric reidentification system
IET Biometrics, N(N):N0N–NNN, N0NN
 N  [N] P
Erdős and A
Rényi
On the evolution of random graphs
 Publications of the Mathematical Institute of the Hungarian  Academy of Sciences, N:NN–NN, NNN0
N  [N] M
Ester, H.-P
Kriegel, J
Sander, and X
Xu
A densitybased algorithm for discovering clusters in large spatial  databases with noise
KDD, NN(NN):NNN–NNN, NNNN
N  [N] M
Everingham, J
Sivic, and A
Zisserman
”Hello! My  name is..
Buffy” Automatic naming of characters in TV  video
In Proc
BMVC, N00N
N, N  [N0] G
D
Forney
The Viterbi algorithm
Proceedings of the  IEEE, NN(N):NNN–NNN, NNNN
N  [NN] B
J
Frey and D
Dueck
Clustering by passing messages  between data points
Science, NNN(NNNN):NNN–NNN, N00N
N  [NN] A
Gyaourova and A
Ross
Index codes for multibiometric  pattern retrieval
IEEE Transactions on Information Forensics and Security (TIFS), N(N):NNN–NNN, April N0NN
N  [NN] M.-L
Haurilet, M
Tapaswi, Z
Al-Halah, and R
Stiefelhagen
Naming TV characters by watching and analyzing dialogs
In Proc
CVPR, N0NN
N  [NN] G
B
Huang, M
Mattar, T
Berg, and E
Learned-Miller
Labeled faces in the wild: A database for studying face recognition in unconstrained environments
In The Workshop on  Faces in Real-Life Images at ECCV, N00N
N, N, N, N  [NN] H
Jiang and E
Learned-Miller
Face detection with the  Faster R-CNN
In Face and Gesture, N0NN
N, N  [NN] H
W
Kuhn
The hungarian method for the assignment problem
Naval research logistics quarterly, N(N-N):NN–NN, NNNN
 N  [NN] Z
Li, J
Liu, and X
Tang
Pairwise constraint propagation  by semidefinite programming for semi-supervised classification
In Proc
ICML, N00N
N  [NN] G
Lisanti, I
Masi, A
D
Bagdanov, and A
D
Bimbo
Person re-identification by iterative re-weighted sparse ranking
 TPAMI, NN(N):NNNN–NNNN, August N0NN
N  [NN] Z
Lu and T
K
Leen
Penalized probabilistic clustering
 Neural Computation, NN(N):NNNN–NNNN, N00N
N  [N0] A
Milan, L
Leal-Taixé, I
Reid, S
Roth, and K
Schindler
 MOTNN: A benchmark for multi-object tracking
 arXiv:NN0N.00NNN [cs], Mar
N0NN
arXiv: NN0N.00NNN
N  [NN] S
Miyamoto and A
Terami
Semi-supervised agglomerative  hierarchical clustering algorithms with pairwise constraints
 In Fuzzy Systems (FUZZ), pages N–N
IEEE, N0N0
N  [NN] F
Murtagh and P
Contreras
Algorithms for hierarchical  clustering: an overview
Wiley Interdisciplinary Reviews:  Data Mining and Knowledge Discovery, N(N):NN–NN, N0NN
 N  [NN] C
Otto, D
Wang, and A
K
Jain
Clustering millions of  faces by identity
TPAMI, Mar
N0NN
N  [NN] A
Ozerov, J.-R
Vigouroux, L
Chevallier, and P
Pérez
On  evaluating face tracks in movies
In Proc
ICIP, N0NN
N, N,  N  [NN] A
Ozerov, J.-R
Vigouroux, L
Chevallier, and P
Pérez
On  evaluating face tracks in movies
In Proc
ICIP
IEEE, N0NN
 N  [NN] O
M
Parkhi, A
Vedaldi, and A
Zisserman
Deep face  recognition
In bmvc, N0NN
N  [NN] A
Patron-Perez, M
Marszaek, A
Zisserman, and I
D
Reid
 High five: Recognising human interactions in tv shows
In  Proc
BMVC, N0N0
N  [NN] S
Ren, K
He, R
B
Girshick, and J
Sun
Faster R-CNN:  towards real-time object detection with region proposal networks
In Proc
NIPS, N0NN
N  [NN] M
Roth, M
Bauml, R
Nevatia, and R
Stiefelhagen
Robust  multi-pose face tracking by multi-stage tracklet association
 In Proc
ICPR, N0NN
N  [N0] D
Sculley
Web-scale k-means clustering
In Proc
WWW,  pages NNNN–NNNN
ACM, N0N0
N, N  [NN] L
Sevilla-Lara and E
Learned-Miller
Distribution fields for  tracking
In Proc
CVPR, N0NN
N, N  [NN] N
Shental, A
Bar-Hillel, T
Hertz, and D
Weinshall
Computing Gaussian mixture models with EM using equivalence  constraints
In Proc
NIPS, N00N
N  [NN] J
Shi and J
Malik
Normalized cuts and image segmentation
TPAMI, NN(N):NNN–N0N, N000
N, N  [NN] R
Sibson
SLINK: an optimally efficient algorithm for the  single-link cluster method
The computer journal, NN(N):N0–  NN, NNNN
N  [NN] M
Tapaswi, M
Bauml, and R
Stiefelhagen
”Knock!  Knock! Who is it?” Probabilistic person identification in TV  series
In Proc
CVPR, N0NN
N  [NN] M
Tapaswi, C
C
Corez, M
Bauml, H
K
Ekenel, and  R
Stiefelhagen
Cleaning up after a face tracker: False positive removal
In Proc
ICIP, N0NN
N  [NN] M
Tapaswi, O
M
Parkhi, E
Rahtu, E
Sommerlade,  R
Stiefelhagen, and A
Zisserman
Total cluster: A person  agnostic clustering method for broadcast videos
In ICVGIP,  N0NN
N, N  [NN] K
Wagstaff, C
Cardie, S
Rogers, S
Schrödl, et al
Constrained k-means clustering with background knowledge
In  Proc
ICML, N00N
N  [NN] B
Wu, S
Lyu, B.-G
Hu, and Q
Ji
Simultaneous clustering and tracklet linking for multi-face tracking in videos
In  Proc
ICCV, N0NN
N, N  [N0] B
Wu, Y
Zhang, B.-G
Hu, and Q
Ji
Constrained clustering and its application to face clustering in videos
In Proc
 CVPR, N0NN
N, N  NNNN    [NN] S
Yang, P
Luo, C
C
Loy, and X
Tang
Wider face: A face  detection benchmark
In CVPR, N0NN
N  [NN] T
Zhang, R
Ramakrishnan, and M
Livny
Birch: an efficient data clustering method for very large databases
In  SIGMOD
ACM, NNNN
N, N  [NN] Z
Zhang, P
Luo, C
C
Loy, and X
Tang
Joint face representation adaptation and clustering in videos
In Proc
 ECCV, N0NN
N  [NN] L
Zheng, Y
Yang, and A
G
Hauptman
Person reidentification: Past, present and future
arXiv, Oct
N0NN
 N  [NN] C
Zhu, F
Wen, and J
Sun
A rank-order distance based  clustering algorithm for face tagging
In Proc
CVPR, N0NN
 N, N, N  NNNNLearning From Video and Text via Large-Scale Discriminative Clustering   Learning from Video and Text via Large-Scale Discriminative Clustering  Antoine MiechN,N Jean-Baptiste AlayracN,N Piotr BojanowskiN Ivan Laptev N,N Josef SivicN,N,N  NÉcole Normale Supérieure NInria NCIIRC  Abstract  Discriminative clustering has been successfully applied to a number of weakly supervised learning tasks
 Such applications include person and action recognition,  text-to-video alignment, object co-segmentation and colocalization in videos and images
One drawback of discriminative clustering, however, is its limited scalability
 We address this issue and propose an online optimization  algorithm based on the Block-Coordinate Frank-Wolfe algorithm
We apply the proposed method to the problem  of weakly supervised learning of actions and actors from  movies together with corresponding movie scripts
The  scaling up of the learning problem to NN feature-length  movies enables us to significantly improve weakly supervised action recognition
 N
Introduction  Action recognition has been significantly improved in recent years
Most existing methods [NN, NN, NN, N0] rely on  supervised learning and, therefore, require large-scale, diverse and representative action datasets for training
Collecting such datasets, however, is a difficult task given the  high costs of manual search and annotation of the video
 Notably, the largest action datasets today are still orders  of magnitude smaller (UCFN0N [NN], ActivityNet [N]) compared to large image datasets, they often contain label noise  and target specific domains such as sports (SportsNM [N0])
 Weakly supervised learning aims to bypass the need  of manually-annotated datasets using readily-available, but  possibly noisy and incomplete supervision
Examples of  such methods include learning of person names from image captions or video scripts [N, N0, NN, NN]
Learning actions from movies and movie scripts has been approached  in [N, N, N, NN]
Most of the work on weakly supervised person and action learning, however, has been limited to one or  a few movies
Therefore the power of leveraging large-scale  NDépartement d’informatique de l’ENS, École normale supérieure,  CNRS, PSL Research University, NN00N Paris, France
NCzech Institute of Informatics, Robotics and Cybernetics at the  Czech Technical University in Prague
 Figure N: We automatically recognize actors and their actions in a  of dataset of NN movies with scripts as weak supervision
 weakly annotated video data has not been fully explored
 In this work we aim to scale weakly supervised learning of actions
In particular, we follow the work of [N] and  learn actor names together with their actions from movies  and movie scripts
While actors are learned separately for  each movie, differently from [N], our method simultaneously learns actions from all movies and movie scripts available for training
Such an approach, however, requires solving a large-scale optimization problem
We address this  issue and propose to scale weakly supervised learning by  adapting the Block-Coordinate Frank-Wolfe approach [NN]
 Our optimization procedure enables action learning from  tens of movies and thousands of action samples, readily  available from our subset of movies or other recent datasets  with movie descriptions [NN]
This, in turn, results in large  improvements in action recognition
 Besides the optimization, our work introduces a new  model for background class in the form of a constraint
It  enables better and automatic modeling of the background  class (i.e
unknown actors and actions)
We evaluate our  method on NN movies and demonstrate significant improvements for both actor and action recognition
Example results are illustrated in Figure N
 NNNN    N.N
Related Work  This section reviews related work on discriminative clustering, Frank-Wolfe optimization and its applications to the  weakly supervised learning of people and actions in video
 Discriminative clustering and Frank-Wolfe
The  Frank-Wolfe algorithm [NN, NN] allows to minimize large  convex problems over convex sets by solving a sequence  of linear problems
In computer vision, it has been used in  combination with discriminative clustering [N] for action  localization [N], text-to-video alignment [N, N], object  co-localization in videos and images [NN] or instance-level  segmentation [NN]
A variant of Frank-Wolfe with randomized block coordinate descent was proposed in [NN]
This  extension leads to lower complexity in terms of time and  memory requirements while preserving the convergence  rate
In this work we build on [NN] and adapt it for the  problem of large-scale weakly supervised learning of  actions from movies
 Weakly supervised action recognition
Movie scripts  are used as a source of weak supervision for temporal action localization in [N]
An extension of this work [N] exploits the temporal order of actions as a learning constraint
 Other [NN] target spatio-temporal action localization and  recognition in video using a latent SVM
A weakly supervised extension of this method [NN] localizes actions without location supervision at the training time
Another recent  work [NN] proposes a multi-fold Multiple-Instance Learning (MIL) SVM to localize actions given video-level supervision at training time
Closer to us is the work of [N]  that improves weakly supervised action recognition by joint  action-actor constraints derived from scripts
While the approach in [N] is limited to a few action classes and movies,  we propose here a scalable solution and demonstrate significant improvements in action recognition when applied to  the large-scale weakly supervised learning of actions from  many movies
 Weakly supervised person recognition
Person recognition in TV series has been studied in [N0, NN] where the  authors propose a solution to the problem of associating  speaker names in scripts and faces in videos
Speakers in  videos are identified by detecting face tracks with lip motion
The method in [N] presents an alternative solution by  formulating the association problem using a convex surrogate loss
Parkhi et al
[NN] present a method for person  recognition combining a MIL SVM with a model for the  background class
Most similar to our model is the one presented in [N]
The authors propose a discriminative clustering cost under linear constraints derived from scripts to  recover the identities and actions of people in movies
Apart  from scaling-up the approach of [N] to much larger datasets,  our model extends and improves [N] with a new background  constraint
 Contributions
In this paper we make the following  contributions: (i) We propose an optimization algorithm  based on Block-Coordinate Frank-Wolfe that allows scaling up discriminative clustering models [N] to much larger  datasets
(ii) We extend the joint weakly supervised PersonAction model of [N], with a simple yet efficient model of the  background class
(iii) We apply the proposed optimization  algorithm to scale-up discriminative clustering to an order  of magnitude larger dataset, resulting in significantly improved action recognition performance
 N
Discriminative Clustering for Weak Supervision  We want to assign labels (e.g
names or action classes)  to samples (e.g
person tracks in the video)
As opposed  to the standard supervised learning setup, the exact labels  of samples are not known at training time
Instead, we are  given only partial information that some samples in a subset  (or bag) may belong to some of the labels
This ambiguous  setup, also known as multiple instance learning, is common,  for example, when learning human actions from videos and  associated text descriptions
 To address this challenge of ambiguous and partial  labels, we build on the discriminative clustering criterion based on a linear classifier and a quadratic loss  (DIFFRAC [N])
This framework has shown promising results in weakly supervised and unsupervised computer vision tasks [N, N, N, N, NN, NN, N0, NN]
In particular, we use  this approach to group samples into linearly separable clusters
Suppose we have N samples to group into K classes
 We are given d-dimensional features X ∈ RN×d, one for each of the N samples, and our goal is to find a binary matrix Y ∈ {0, N}N×K assigning each of the N samples to one of the labels, where Ynk = N if and only if the sample n (e.g
a person track in a movie) is assigned to the label k  (e.g
action class running)
 First, suppose the assignment matrix Y is given
In this  case finding a linear classifier W can be formulated as a  ridge regression problem  min W∈Rd×K  N  NN ‖Y −XW‖NF +  λ  N ‖W‖NF, (N)  where X is a matrix of input features, Y is the given labels assignment matrix, ‖.‖F is the matrix norm (or Frobe- nius norm) induced by the matrix inner product 〈., .〉F (or Frobenius inner product) and λ is a regularization hyperparameter set to a fixed constant
The key observation is  that we can resolve the classifier W ∗ in closed form as  W ∗(Y ) = (X⊤X +NλI)−NX⊤Y
(N)  In our weakly supervised setting, however, Y is unknown
Therefore, we treat Y as a latent variable and optimize (N) w.r.t
W and Y 
In details, plugging the optimal  NNNN    solution W ∗ (N) in the cost (N) removes the dependency on  W and the cost can be written as a quadratic function of Y ,  i.e
C(Y ) = 〈Y,A(X,λ)Y 〉F, where A(X,λ) is a matrix that only depends on the data X and a regularization parameter λ
Finding the best assignment matrix Y can then  be written as the minimization of the cost C(Y ):  min Y ∈{0,N}N×K  〈Y,A(X,λ)Y 〉F
(N)  Solving the above problem, however, can lead to degenerate solutions [N] unless additional constraints on Y are  provided
In section N, we incorporate weak supervision in  the form of constraints on the latent assignment matrices  Y 
The constraints on Y used for weak supervision generally decompose into small independent blocks
This block  structure is the key for our optimization approach that we  will present next
 N.N
Large-Scale optimization  The Frank-Wolfe (FW) algorithm has been shown effective for optimizing convex relaxation of (N) in a number of  vision problems [N, N, N, N, NN, NN]
It only requires solving  linear programs on a set of constraints
Therefore, it avoids  costly projections and allows the use of complicated constraints such as temporal ordering [N]
However, the standard FW algorithm is not well suited to solve (N) for a large  number of samples N 
 First, storing the N ×N matrix A(X,λ) in memory be- comes prohibitive (e.g
the size of A becomes ≥ N00GB for N ≥ N00000)
Second, each update of the FW algorithm requires a full pass over the data resulting in a space and  time complexity of order N for each FW step
 Weakly supervised learning is, however, largely motivated by the desire of using large-scale data with “cheap”  and readily-available but incomplete and noisy annotation
 Scaling up weakly supervised learning to a large number of  samples is, therefore, essential for its success
We address  this issue and develop an efficient version of the FW algorithm
Our solution builds on the Block-Coordinate FrankWolfe (BCFW) [NN] algorithm and extends it with a smart  block-dependent update procedure as described next
The  proposed update procedure is one of the key contribution of  this paper
 N.N.N Block-coordinate Frank-Wolfe (BCFW)  The Block-Coordinate version of the Frank-Wolfe algorithm [NN] is useful when the convex feasible set Y can be written as a Cartesian product of n smaller sets of constraints: Y = Y(N) × 


× Y(n)
Inspired by coordinate descent techniques, BCFW consists of updating one variable block Y(i) at a time with a reduced Frank-Wolfe step
This method has potentially n times cheaper iterates both  in space and time
We will see that most of the weakly supervised problems exhibit such a block structure on latent  variables
 N.N.N BCFW for discriminative clustering  To benefit from BCFW, we have to ensure that the time and  space complexity of the block update does not depend on  the total number of samples N (e.g
person tracks in all  movies) but only depends on the size Ni of smaller blocks  of samples i (e.g
person tracks within one movie)
After  a block is sampled, the update consists of two steps
First,  the gradient with respect to the block is computed
Then the  linear oracle is called to obtain the next iterate
As we show  below, the difficult part in our case is to efficiently compute  the gradient with respect to the block
 Block gradient: a naive approach
Let’s denote Ni the  size of block i
The objective function f of problem (N) is  f(Y ) = 〈Y,A(X,λ)Y 〉F, where (see [N])  A(X,λ) = N  NN (IN −X(X  ⊤X +NλId) −NX⊤)
(N)  To avoid storing matrix A(X,λ) of size N×N , one can pre- compute the matrix P = (X⊤X +NλId)  −NX⊤ ∈ Rd×N 
We can write the block gradient with respect to a subset of  samples i as follows:  ∇(i)f(Y ) = N  N (Y (i) −X(i)PY ), (N)  where Y (i) ∈ RNi×K and X(i) ∈ RNi×d are the label as- signment variable and the feature matrix for block i (e.g
 person tracks in movie i), respectively
Because of the PY  matrix multiplication, naively computing this formula has  the complexity O(NdK), where N is the total number of samples, d is the dimensionality of the feature space and K  is the number of classes
As this depends linearly on N , we  aim to find a more efficient way to compute block gradients,  as described next
 Block gradient: a smart update
We now propose an update procedure that avoids re-computation of block gradients and whose time and space complexity at each iteration  depends on Ni instead of N 
The main intuition is that we  need to find a way to store information about all the blocks  in a compact form
A natural way of doing so is to maintain  the weights of the linear regression parameters W ∈ Rd×K 
From (N) we have W = PY 
If we are able to maintain the variable W at each iteration with the desired complexity  O(NidK), then the block gradient computation (N) can be reduced fromO(NdK) toO(NidK)
We now explain how to effectively achieve that
 NNNN    At each iteration t of the algorithm, we only update a  block i of Y while keeping all other blocks fixed
We denote  the direction of the update by Dt ∈ R N×K and the step size  by γt
With this notation the update becomes  Yt+N = Yt + γtDt
(N)  The update rule for the weight variable Wt can now be written as follows:  Wt+N = P (Yt + γtDt)  Wt+N = Wt + γtPDt, (N)  Recall that at iteration t, BCFW only updates block i, therefore Dt has non zero value only at block i
In block notation  we can therefore write the matrix product PDt as:  [  P (N), · · · , P (i), · · · , P (n) ]  ×      0  D (i) t  0      = P (i)D (i) t ,  (N)  where P (i) ∈ Rd×Ni and D (i) t ∈ R  Ni×K are the i-th blocks  of matrices P and Dt, respectively
The outcome is an update of the following form  Wt+N = Wt + γtP (i)D  (i) t , (N)  where the computational complexity for updating W has  been reduced to O(NidK) compared to O(NdK) in the standard update
 We have designed a Block-Coordinate Frank-Wolfe update with time and space complexity depending only on the  size of the blocks and not the entire dataset
This allows  to scale discriminative clustering to problems with a very  large number of samples
The pseudo-code for the algorithm is summarized in Algorithm N
Next, we describe an  application of this large-scale discriminative clustering algorithm to weakly supervised person and action recognition  in movies
 N
Weakly supervised Person-Action model  We now describe an application of our large-scale discriminative clustering algorithm with weak-supervision
 The goal is to assign to each person track a name and an action
Both names and actions are mined from movie scripts
 For a given movie i, we assume to have Ni automatically  extracted person tracks as well as the parsing of a movie  script into person names and action classes
We also assume that scripts and movies have been roughly aligned in  time
In such a setup we can assign labels (e.g
a name or  an action) from a script section to a subset of tracksN from the corresponding time interval of a movie (see Figure N for  example)
In the following, we explain how to convert such  form of weak supervision into a set of constraints on latent  Algorithm N BCFW for Discriminative Clustering [N]  Initiate Y0, P := (X ⊤X + NλId)  −NX⊤, W0 = PY0, gi = +∞, ∀i
for t = N 

.Niter do  i← sample from distribution proportional to g [NN]  ∇(i)f(Yt)← N N (Y  (i) t −X  (i)Wt) # Block gradient Ymin ← argminx∈Y(i)〈∇(i)f(Yt), x〉F # Linear oracle  D ← Ymin − Y (i)  gi ← −〈D,∇(i)f(Yt)〉F # Block gap γ ← min(N, giN  N 〈D,D−X(i)P (i)D〉F  ) # Line-search  Wt+N ←Wt + γP (i)D # W update  Y (i) t+N ← Y  (i) t + γD # Block update  end for  variables corresponding to the names and actions of people
 We will also show how these constraints easily decompose  into blocks
We denote Z the latent variable assignment  matrix for person names and T for actions
 N.N
Weak-supervision as constraints  We use linear constraints to incorporate weak supervision from movie scripts
In detail, we define constraints  on subsets of person tracks that we call “bags”
In the following we explain the procedure for construction of bags  together with the definition of the appropriate constraints
 ‘At least one’ constraint
Suppose a script reveals the  presence of a person p in some time interval of the movie
 We construct a set N with all person tracks in this interval
As first proposed by [N], we model that at least one track in  N is assigned to person p by the following constraint  ∑  n∈N  Znp ≥ N
(N0)  We can apply the same type of constraint when solving for  action assignment T 
 Person-Action constraint
Scripts can also provide information that a person p is performing an action a in a scene
 In such cases we can formulate stricter and more informative constraints as follows
We construct a setN containing all person tracks appearing in this scene
Following [N], we  formulate a joint constraint on presence of a person performing a specific action as  ∑  n∈N  ZnpTna ≥ N
(NN)  Mutual exclusion constraint
We also model that each  person track can only be assigned to exactly one label
This  NNN0    Movie N:      …      Le  st er               ..
     …      V irg  in ia               ..
 …      Lester gets up and   starts after Jane taking   his plate with him…      …      S ta  nd  u  p               …                D  riv e  …   0  N  0  N  N  0  N  0  0  0  0  0  Script N:  Virginia is driving   while Buster intently   studies the terrain…  Script i:  Movie i:  : Person assignment matrix of movie i  : # of known characters in movie i : # of person tracks in movie i  : Subset of tracks with constraints : Action assignment matrix  : # of action classes in the model : # of person tracks in ALL movies  Figure N: Overview of the Person-Action weakly supervised model, see text for detailed explanations
 restriction can be formalized by the mutual exclusion constraint  ZNP = NN , (NN)  for Z (i.e
rows sum up to N)
Same constraint holds for T 
 Background class constraint
One of our contributions  is a novel way of coping with the background class
As  opposed to previous work [N], our approach allows us to  have background model that does not require any external  data
Also it does not require a specific background class  classifier as in [NN]
 Our background class constraint can be seen as a way  to supervise people and actions that are not mentioned in  scripts
We observe that tracks that are not subject to constraints from Eq
(N0) and tracks that belong to crowded  shots are likely to belong to the background class
Let us  denote by B the set of such tracks
We impose that at least a certain fraction α ∈ [0, N] of tracks in B must belong to the background class
Assuming that person label p = N cor- responds to the background, we obtain the following linear  constraint (similar constraint can be defined for actions on  T ):  ∑  n∈B  ZnN ≥ α | B | 
(NN)  N.N
Person-Action model formulation  Here we summarize the complete formulation of the person and action recognition problems
 Solving for names
We formulate the person recognition  problem as discriminative clustering, where XN are face descriptors:  min Z∈{0,N}N×P  〈Z,A(XN, λ)Z〉F, (Discriminative cost) (NN)  such that            ∑  n∈N Znp ≥ N, (At least one) ∑  n∈B ZnN ≥ α | B |, (Background)  ZNP = NN 
(Mutual exclusion)  Solving for actions
After solving the previous problem  for names separately for each movie, we vertically concatenate all person name assignment matrices Z
We also define  a single action assignment variable T in {0, N}M×A, where M is the total number of tracks across all movies and XN are action descriptors (details given later)
We formulate  our action recognition problem as a large QP:  min T∈{0,N}M×A  〈T,A(XN, µ)T 〉F, (Discriminative cost) (NN)  such that                    ∑  n∈N Tna ≥ N, (At least one) ∑  n∈N ZnpTna ≥ N, (Person-Action) ∑  n∈B TnN ≥ β | B |, (Background)  TNA = NM 
(Mutual exclusion)  Block-Separable constraints
The set of linear constraints on the action assignment matrix T is block separable  since each movie has it own set of constraints, i.e
there are  no constraints spanning multiple movies
Therefore, we can  fully demonstrate here the power of our large-scale discriminative clustering optimization (Algorithm N)
 N
Experimental Setup  N.N
Dataset  Our dataset is composed of NN Hollywood feature-length  movies (see the list in Appendix [NN]) that we obtained  from either BluRay or DVD
For all movies, we downloaded their scripts (on www.dailyscript.com) that  we temporally aligned with the videos and movie subtitles  using the method described in [NN]
The total number of  frames in all NN movies is NN,NN0,NNN
The number of body  tracks detected across all movies (see N.N for more details)  is M = N0NNNN
 N.N
Text pre-processing  To provide weak supervision for our method we process  movie scripts to extract occurrences of the NN most frequent  action classes: Stand Up, Eat, Sit Down, Sit Up,  NNNN  www.dailyscript.com   Hand Shake, Fight, Get Out of Car, Kiss, Hug,  Answer Phone, Run, Open Door and Drive
To do  so, we collect a corpus of movie scripts different from the  set of our NN movies and train simple text-based action classifiers using linear SVM and a TF-IDF representation of  words composed of uni-grams and bi-grams
After retrieving actions in our target movie scripts, we also need to identify who is performing the action
We used spaCy [NN] to  parse every sentence classified as describing one of the NN  actions and get every subject for each action verb
 N.N
Person detection and Features  Face tracks
To obtain tracks of faces in the video, we  run the multi-view face detector [NN] based on the DPM  model [NN]
We then extract face tracks using the same  method as in [N0, NN]
For each detected face, we compute facial landmarks [NN] followed by the face alignment  and resizing of face images to NNNxNNN pixel
We use pretrained vgg-face features [NN] to extract descriptors for each  face
We kept the features of dimension N0NN computed by  the network at the last fully-connected layer that we LN normalized
For each face track, we choose the top K (in practice, we choose K=N) faces that have the best facial landmark confidence
Then we represent each track by averaging the features of the top K faces
 Body tracks
To get the person body tracks, we run  the Faster-RCNN network with VGG-NN architecture finetuned on VOC 0N [NN]
Then we track bounding boxes using the same tracker as used to obtain face tracks
To get  person identity for body tracks, we greedily link each body  track to one face track by maximizing a spatio-temporal  bounding box overlap measure
However if a body track  does not have an associated face track as the actor’s face  may look away from the camera, we cannot obtain its identity
Such tracks can be originating from any actor in the  movie
To capture motion features of each body track, we  compute bag-of-visual-words representation of dense trajectory descriptors [NN] inside the bounding boxes defined  by the body track
We use N000 cluster centers for each of  the HOF, MBHx and MBHy channels
In order to capture  appearance of each body track we extract ResNet-N0 [NN]  pre-trained on ImageNet
For each body bounding box, we  compute the average RoI-pooled [NN] feature map of the last  convolutional layer within the bounding box, which yields  a feature vector of dimension N0NN for each box
We extract  a feature vector every N0th frame, average extracted feature  vectors over the duration of the track and LN normalize
Finally, we concatenate the dense trajectory descriptor and the  appearance descriptor resulting in a NN0NN-dimensional descriptor for each body track
 Method Acc
Multi-Class AP Background AP  Cour et al
[N] NN% NN% − Sivic et al
[NN] NN% NN% − Bojanowski et al
[N] NN% NN% NN% Parkhi et al
[NN] NN% NN% NN% Our method NN% NN% NN%  Table N: Comparison on the Casablanca benchmark [N]
 Episode N N N N N  Sivic et al
[NN] N0 NN N0 NN NN  Parkhi et al
[NN] NN N0 NN NN NN  Ours NN NN NN NN NN  Table N: Comparison on the Buffy benchmark [NN] using AP
 α 0 0.N 0.N 0.N 0.N 0.N 0.NN N.0  Accuracy NN NN N0 NN NN NN NN NN  AP NN NN N0 NN NN NN NN NN  Table N: Sensitivity to hyper-parameter α (NN) on Casablanca
 N
Evaluation  N.N
Evaluation of person recognition  We compare our person recognition method to several  other methods on the Casablanca benchmark from [N] and  the Buffy benchmark from [NN]
All methods are evaluated on the same inputs: same face tracks, scripts and characters
Table N shows the Accuracy (Acc.) and Average  Precision (AP) of our approach compared to other methods  on the Casablanca benchmark [N]
In particular we compare to Parkhi et al
[NN] which is a strong baseline using  the same CNN face descriptors as in our method
We also  show the AP of classifying the background character class  (Background AP)
We compare in Table N our approach to  other methods [NN, NN] reporting results on season N of the  TV series “Buffy the Vampire Slayer”
Both of these methods [NN, NN] use speaker detection to mine additional strong  (but possibly incorrect) labels from the script, which we  also incorporate (as additional bags) to make the comparison fair
Our method demonstrates significant improvement  over the previous results
It also outperforms other methods  on the task of classifying background characters
Finally,  Table N shows the sensitivity to hyper-parameter α from  the background constraint (NN) on the Casablanca benchmark
Note that in contrast to other methods, our background model does not require supervision for the background class
This clearly demonstrates the advantage of  our proposed background model
For all experiments the  hyper-parameter α of the background constraint (NN) was  set to N0%
Figure N illustrates our qualitative results for character recognition in different movies
 NNNN    METHOD # movies Joint-Model St.U
E
S.D
Si.U
H.S
F
G.C
K
H
A
R
O.D
D
mAP  (a) Random ∅ No 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N N.N 0.N 0.N 0.N (b) Script only ∅ No N.0 N.N N.N N.N N.N N.N N.N NN.N N.N N.N NN.N N.N N.0 N.N (c) Fully-supervised N No NN.N 0.N NN.N 0.N 0.N N.N N.N N.N N.N N.0 NN.N NN.N N.N N.N  (d) Few training movies N Yes NN.N N.N NN.N N.N N.N N.N N.0 N.0 N.N N.N NN.0 NN.N NN.N NN.N  (e) No Joint Model NN No N0.N N.0 NN.N N.N NN.0 NN.N N.0 NN.N N.N N.N NN.N NN.N NN.N NN.N  (f) Full setup NN Yes NN.0 N.N NN.N N.N N.N N.N N.0 NN.N N.N N.N NN.N NN.N NN.N NN.N  Table N: Average Precision of actions evaluated on N movies
(St.U: Stand Up, E.: Eat, S.D: Sit Down, Si.U.: Sit Up, H.S: Hand  Shake, F.: Fight, G.C.: Get out of Car, K.: Kiss, H.: Hug, A.: Answer Phone, R.: Run, O.D.: Open Door, D.: Drive)  0 0.N 0.N 0.N 0.N 0.N 0.N 0.N  recall  0  0.N  0.N  0.N  0.N  N  p re  c is  io n  Ours (NN movies) AP=0.NN  Ours (N movie) AP=0.NN  Bojanowski et al
AP=0.0N  Figure N: PR curves of action SitDown from Casablanca
 0 N0 N0 N0 N0 N0 N0  Number of movies  NN.N  NN  NN.N  NN  NN.N  NN  NN.N  NN  NN.N  m A  P  Figure N: Action recognition mAP with increasing number of  training movies
 N.N
Evaluation of action recognition  First, we compare our method to Bojanowski et al
 N0NN [N]
Their evaluation uses different body tracks than  ours, we design here an algorithm-independent evaluation  setup
We compare our model using the Casablanca movie  and the Sit Down action
For the purpose of evaluation,  we have manually annotated all person tracks in the movie  and then manually labeled whether or not they contain the  Sit Down action
Given this ground truth, we assess the  two models in a similar way as typically done in object detection
Figure N shows a precision-recall curve evaluating  recognition of the Sit Down action
We show our method  trained on Casablanca only (as done in [N]) and then on all  NN movies
Our method trained on Casablanca is already  better than [N]
The improvement becomes even more evident when training our method on all NN movies
 To evaluate our method on all NN action classes, we use  five movies (American Beauty, Casablanca, Double Indemnity, Forrest Gump and Fight Club)
For each of these  movies we have manually annotated all person tracks produced by our tracker according to NN target action classes  and the background action class
We assume that each track  corresponds to at most one target action
In rare cases where  Method R@N R@N R@N0 Median Rank  Yu et al
[NN] N.N% NN.N% NN.N% N0 Levi et al
[NN] N.N% NN.N% NN.N% NN Our baseline N.N% NN.N% NN.N% NN  Table N: Baseline comparison against winners of the LSMDCN0NN  movie clip retrieval challenge  this assumption is violated, we annotate the track by one of  the correct action classes
 In Table N we compare results of our model to different  baselines
The first baseline (a) corresponds to the random  assignment of action classes
The second baseline (b) Script  only uses information extracted from the scripts: each time  an action appears in a bag, all person tracks in this bag are  then simply annotated with this action
Baseline (c) is using  our action descriptors but trained in a fully supervised setup on a small subset of annotated movies
To demonstrate  the strength of this baseline we have used the same action  descriptors on the LSMDCN0NNN movie clip retrieval challenge
This is the largest public benchmark [NN] related to  our work that considers movie data (but without person localization as we do in our work)
Table N shows our features  employed in simple CCA method as done in [NN] achieving  state-of-the-art on this benchmark
The fourth baseline (d)  is our method train only using the five evaluated movies
 The fifth baseline (e) is our model without the joint personaction constraint (NN), but still trained on all NN movies
Finally, the last result (f) is from our model using all the NN  training movies and person-action constraints (NN)
Results  demonstrate that optimizing our model on more movies  brings the most significant improvement to the final results
 We confirm the idea from [N] that adding the information  of who is performing the action in general helps identifying actions
However we also notice it is not always true  for actions with interacting people such as: Fight, Hand  Shake, Hug or Kiss
Knowing who is doing the action  does not seems to help for these actions
Figure N shows improvements in action recognition when gradually increasing the number of training movies
Figure N shows qualitative results of our model on different movies
The statistics  about the ground truth and constraints together with additional results are provided in Appendix [NN]
Nhttps://sites.google.com/site/describingmovies/lsmdc-N0NN  NNNN    Figure N: Qualitative results for face recognition
Green bounding boxes are face tracks correctly classified as background characters
 Figure N: Qualitative results for action recognition
P stands for for the name of the character and A for the action performed by P
Last  row (in red) shows mislabeled tracks with high confidence (e.g
hugging labeled as kissing, sitting in a car labeled as driving)
 N
Conclusion We have proposed an efficient online optimization  method based on the Block-Coordinate Frank-Wolfe algorithm
We use this new algorithm to scale-up discriminative  clustering model in the context of weakly supervised person  and action recognition in feature-length movies
Moreover,  we have proposed a novel way of handling the background  class, which does not require collecting background class  data as required by the previous approaches, and leads to  better performance for person recognition
In summary, the  proposed model significantly improves action recognition  results on NN feature-length movies
The significance of the  technical contribution goes beyond the problem of personaction recognition as the proposed optimization algorithm  can scale-up other problems recently tackled by discriminative clustering
Examples include: unsupervised learning from narrated instruction videos [N], text-to-video alignment [N], co-segmentation [NN], co-localization in videos  and images [NN] or instance-level segmentation [NN], which  can be now scaled-up to an order of magnitude larger  datasets
 Acknowledgments
This work has been supported by ERC grants  ACTIVIA (no
N0NNNN) and LEAP (no
NNNNNN), CIFAR Learning in Machines & Brains program, ESIF, OP Research, development and education Project IMPACT No
CZ.0N.N.0N/0.0/0.0/NN 00N/0000NNN and a  Google Research Award
 NNNN    References  [N] J.-B
Alayrac, P
Bojanowski, N
Agrawal, I
Laptev, J
Sivic,  and S
Lacoste-Julien
Unsupervised learning from narrated  instruction videos
In CVPR, N0NN
N, N, N  [N] F
Bach and Z
Harchaoui
Diffrac: a discriminative and flexible framework for clustering
In NIPS, N00N
N, N, N  [N] T
L
Berg, A
C
Berg, J
Edwards, M
Maire, R
White, Y.W
Teh, E
Learned-Miller, and D
A
Forsyth
Names and  faces in the news
In CVPR, volume N, pages II–NNN, N00N
 N  [N] P
Bojanowski, F
Bach, I
Laptev, J
Ponce, C
Schmid, and  J
Sivic
Finding Actors and Actions in Movies
In ICCV,  N0NN
N, N, N, N, N, N, N  [N] P
Bojanowski, R
Lajugie, F
Bach, I
Laptev, J
Ponce,  C
Schmid, and J
Sivic
Weakly supervised action labeling in videos under ordering constraints
In ECCV, N0NN
N,  N, N  [N] P
Bojanowski, R
Lajugie, E
Grave, F
Bach, I
Laptev,  J
Ponce, and C
Schmid
Weakly-supervised alignment of  video with text
In ICCV, N0NN
N, N, N  [N] F
Caba Heilbron, V
Escorcia, B
Ghanem, and J
Carlos Niebles
Activitynet: A large-scale video benchmark  for human activity understanding
In CVPR, pages NNN–NN0,  N0NN
N  [N] T
Cour, B
Sapp, C
Jordan, and B
Taskar
Learning from  ambiguously labeled images
In CVPR, N00N
N, N  [N] O
Duchenne, I
Laptev, J
Sivic, F
Bach, and J
Ponce
Automatic annotation of human actions in video
In ICCV, N00N
 N, N  [N0] M
Everingham, J
Sivic, and A
Zisserman
“Hello! My  name is..
Buffy” – Automatic Naming of Characters in TV  Video
In BMVC, N00N
N, N, N  [NN] M
Frank and P
Wolfe
An algorithm for quadratic programming
Naval Research Logistics Quarterly, NNNN
N  [NN] R
B
Girshick, P
F
Felzenszwalb, and D
McAllester
Discriminatively trained deformable part models, release N
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep Residual Learning  for Image Recognition
In CVPR, N0NN
N  [NN] M
Honnibal and M
Johnson
An improved non-monotonic  transition system for dependency parsing
In EMNLP, N0NN
 N  [NN] M
Jaggi
Revisiting Frank-Wolfe: Projection-free sparse  convex optimization
In ICML, N0NN
N  [NN] A
Joulin, F
Bach, and J
Ponce
Discriminative Clustering  for Image Co-segmentation
In CVPR, N0N0
N, N  [NN] A
Joulin, F
Bach, and J
Ponce
Multi-class cosegmentation
 In CVPR, N0NN
N  [NN] A
Joulin, K
Tang, and L
Fei-Fei
Efficient image and video  co-localization with frank-wolfe algorithm
In ECCV, N0NN
 N, N  [NN] A
Joulin, K
Tang, and L
Fei-Fei
Efficient image and video  co-localization with Frank-Wolfe algorithm
In ECCV, N0NN
 N  [N0] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In CVPR, pages NNNN–NNNN, N0NN
 N  [NN] S
Lacoste-Julien, M
Jaggi, M
Schmidt, and P
Pletscher
 Block-coordinate frank-wolfe optimization for structural  SVMs
In ICML, N0NN
N, N, N  [NN] T
Lan, Y
Wang, and G
Mori
Discriminative figure-centric  models for joint action localization and recognition
In  ICCV, N0NN
N  [NN] I
Laptev, M
Marszalek, C
Schmid, and B
Rozenfeld
 Learning realistic human actions from movies
In CVPR,  N00N
N, N  [NN] G
Levi, D
Kaufman, L
Wolf, and T
Hassner
Video Description by Combining Strong Representation and a Simple  Nearest Neighbor Approach
In ECCV LSMDCN0NN Workshop, N0NN
N  [NN] M
Mathias, R
Benenson, M
Pedersoli, and L
Van Gool
 Face detection without bells and whistles
In ECCV, N0NN
N  [NN] A
Miech, J
B
Alayrac, P
Bojanowski, I
Laptev, and  J
Sivic
Supplementary material (appendix)
https:  //hal.inria.fr/hal-0NNNNNN0
N, N  [NN] A
Osokin, J.-B
Alayrac, I
Lukasewitz, P
Dokania, and  S
Lacoste-Julien
Minding the gaps for block frank-wolfe  optimization of structured svms
In ICML, N0NN
N  [NN] O
Parkhi, E
Rahtu, and A
Zisserman
It’s in the bag:  Stronger supervision for automated face labelling
In ICCV  Workshop, N0NN
N, N, N  [NN] O
Parkhi, A
Vedaldi, and A
Zisserman
Deep Face Recognition, British Machine Vision Conference
In BMVC, N0NN
 N  [N0] V
Ramanathan, A
Joulin, P
Liang, and L
Fei-Fei
Linking people in videos with “their” names using coreference  resolution
In ECCV, N0NN
N  [NN] A
Rohrbach, M
Rohrbach, N
Tandon, and B
Schiele
A  dataset for movie description
In CVPR, N0NN
N, N  [NN] G
Seguin, P
Bojanowski, R
Lajugie, and I
Laptev
 Instance-level video segmentation from object tracks
In  CVPR, N0NN
N, N, N  [NN] R
Shaoqing, H
Kaiming, G
Ross, and S
Jian
Faster rcnn: Towards real-time object detection with region proposal  networks
In NIPS, N0NN
N  [NN] N
Shapovalova, A
Vahdat, K
Cannons, T
Lan, and  G
Mori
Similarity constrained latent support vector machine: An application to weakly supervised action classification
In ECCV, N0NN
N  [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In NIPS, pages  NNN–NNN, N0NN
N  [NN] J
Sivic, M
Everingham, and A
Zisserman
“Who are you?”  - Learning person specific classifiers from video
In CVPR,  N00N
N, N, N  [NN] K
Soomro, A
R
Zamir, and M
Shah
UCFN0N: A dataset  of N0N human actions classes from videos in the wild
arXiv  preprint arXiv:NNNN.0N0N, N0NN
N  [NN] M
Tapaswi, M
Bauml, and R
Stiefelhagen
“Knock!  Knock! Who is it?” probabilistic person identification in  tv-series
In CVPR, N0NN
N  [NN] H
Wang and C
Schmid
Action Recognition with Improved  Trajectories
In ICCV, N0NN
N, N  NNNN  https://hal.inria.fr/hal-0NNNNNN0 https://hal.inria.fr/hal-0NNNNNN0   [N0] L
Wang, Y
Qiao, and X
Tang
Action recognition with  trajectory-pooled deep-convolutional descriptors
In CVPR,  pages NN0N–NNNN, N0NN
N  [NN] P
Weinzaepfel, X
Martin, and C
Schmid
Towards  weakly-supervised action localization
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [NN] Y
Yu, H
Ko, J
Choi, and G
Kim
Video captioning  and retrieval models with semantic attention
In ECCV  LSMDCN0NN Workshop, N0NN
N  NNNNConvolutional Dictionary Learning via Local Processing   Convolutional Dictionary Learning via Local Processing  Vardan Papyan  vardanp@campus.technion.ac.il  Yaniv Romano  yromano@tx.technion.ac.il  Jeremias Sulam  jsulam@cs.technion.ac.il  Michael Elad  elad@cs.technion.ac.il  Technion - Israel Institute of Technology  Technion City, Haifa NN000, Israel  Abstract  Convolutional sparse coding is an increasingly popular model in the signal and image processing communities,  tackling some of the limitations of traditional patch-based  sparse representations
Although several works have addressed the dictionary learning problem under this model,  these relied on an ADMM formulation in the Fourier domain, losing the sense of locality and the relation to the  traditional patch-based sparse pursuit
A recent work suggested a novel theoretical analysis of this global model, providing guarantees that rely on a localized sparsity measure
 Herein, we extend this local-global relation by showing how  one can efficiently solve the convolutional sparse pursuit  problem and train the filters involved, while operating locally on image patches
Our approach provides an intuitive  algorithm that can leverage standard techniques from the  sparse representations field
The proposed method is fast to  train, simple to implement, and flexible enough that it can  be easily deployed in a variety of applications
We demonstrate the proposed training scheme for image inpainting  and image separation, achieving state-of-the-art results
 N
Introduction  The celebrated sparse representation model has led to  impressive results in various applications over the last  decade [N0, N, NN, N0, N]
In this context one assumes that a  signal X ∈ RN is a linear combination of a few columns, also called atoms, taken from a matrix D ∈ RN×M termed a dictionary; i.e
X = DΓ where Γ ∈ RM is a sparse vector
Given X, finding its sparsest representation, called  sparse pursuit, amounts to solving the following problem  min Γ  ‖Γ‖0 s.t
‖X−DΓ‖N ≤ ǫ, (N)  where ǫ stands for the model mismatch or an additive noise  strength
The solution for the above can be approximated  using greedy algorithms such as Orthogonal Matching Pursuit (OMP) [N] or convex formulations such as BP [N]
The  task of learning the model, i.e
identifying the dictionary D  that best represents a set of training signals, is called dictionary learning and several methods have been proposed for  tackling it, including K-SVD [N], MOD [NN], online dictionary learning [NN], trainlets [NN], and more
 When dealing with high-dimensional images, traditional  image processing algorithms train a local model for fully  overlapping patches extracted from X, process these independently using the trained local prior and then average the  results to obtain a global estimate
This approach, which  we refer to in what follows as patch averaging, gained  much popularity and success due to its simplicity and highperformance [N0, N0, N, N0]
A different approach is the  Convolutional Sparse Coding (CSC), which works with a  global model by imposing a specific structure on the dictionary involved [NN, N, NN, NN, NN, NN, NN]
In particular, this  assumes that D is a banded convolutional dictionary, implying that the signal is a superposition of a few local atoms,  or filters, shifted to different positions
Unlike patch averaging that restores independently the same information in  the image several times, CSC treats the information jointly  and only once
Several works have presented algorithms  for training convolutional dictionaries [N, NN, NN], circumventing some of the computational burdens of this problem  by relying on ADMM solvers that operate in the Fourier  domain
In doing so, these methods lost the connection to  the patch-based processing paradigm, as widely practiced  in many signal and image processing applications
 In this work, we propose a novel approach for training  the CSC model, called slice-based dictionary learning
Unlike current methods, we leverage a localized strategy enabling the solution of the global problem in terms of only  local computations in the original domain
The main advantages of our method over existing ones are:  N
It operates locally on patches, while solving faithfully  NNNNN    Figure N: Top: Patches extracted from natural images
Bottom: Their corresponding slices
Observe how the slices are far  simpler, and contained by their corresponding patches
 the global CSC problem;  N
It is easy to implement and intuitive to understand;  N
It reveals how one should modify current (and any)  dictionary learning algorithms to solve the CSC problem in a variety of applications;  N
It can leverage standard techniques from the sparse  representations field, such as OMP, LARS, K-SVD,  MOD, online dictionary learning and trainlets;  N
When compared to state-of-the-art methods, it can be  applied to standard-sized images, converges faster and  provides a better model; and  N
It can naturally allow for a different number of nonzeros in each spatial location, according to the local  signal complexity
 The rest of this paper is organized as follows: Section N  reviews the CSC model
The proposed method is presented  in Section N and contrasted with conventional approaches  in Section N
Section N shows how our method can be employed to tackle the tasks of image inpainting and separation, and later in Section N we demonstrate empirically our  algorithms
We conclude this work in Section N
 N
Convolutional sparse coding  The CSC model assumes that a global signal X can be  decomposed as X = ∑m  i=N di ∗ Γi, where di ∈ R n are  local filters that are convolved with their corresponding features maps (or sparse representations) Γi ∈ R N 
Alternatively, following Figure N, the above can be written in matrix  =  �i ∈ ℝ −  �i� ∈ ℝ  �i ∈ ℝ  � ∈ ℝ�×�� ∈ ℝ� � ∈ ℝ�  ⋮�� ∈ ℝ × � ∈ ℝ × −  Figure N: The CSC model and its constituent elements
 form as X = DΓ; where D ∈ RN×Nm is a banded con- volutional dictionary built from shifted versions of a local  matrix DL, containing the atoms {di} m i=N as its columns,  and Γ ∈ RNm is a global sparse representation obtained by interlacing the {Γi}  m i=N
In this setting, a patch RiX taken  from the global signal equals Ωγi, where Ω ∈ R n×(Nn−N)m  is a stripe dictionary and γi ∈ R (Nn−N)m is a stripe vector
 Here we defined Ri ∈ R n×N to be the operator that extracts  the i-th n-dimensional patch from X
 The work in [NN] suggested a theoretical analysis of  this global model, driven by a localized sparsity measure
 Therein, it was shown that if all the stripes γi are sparse,  the solution to the convolutional sparse pursuit problem is  unique and can be recovered by greedy algorithms, such as  the OMP [N], or convex formulations such as the Basis Pursuit (BP) [N]
This analysis was then extended in [NN] to a  noisy regime showing that, under similar sparsity assumptions, the global problem formulation and the pursuit algorithms are also stable
Herein, we leverage this local-global  relation from an algorithmic perspective, showing how one  can efficiently solve the convolutional sparse pursuit problem and train the dictionary (i.e., the filters) involved, while  only operating locally
 Note that the global sparse vector Γ can be broken into a  set of non-overlapping m-dimensional sparse vectors αNi=N,  which we call needles
The essence of the presented algorithm is in the observation that one can express the global  signal as X = ∑N  i=N R T i DLαi, where R  T i ∈ R  N×n is  the operator that puts DLαi in the i-th position and pads  the rest of the entries with zeros
Denoting by si the i-th  slice DLαi, we can write the above as X = ∑N  i=N R T i si
 It is important to stress that the slices do not correspond  to patches extracted from the signal, RiX, but rather to  much simpler entities
They represent only a fraction of  the i-th patch, since RiX = Ri ∑N  j=N R T j sj , i.e
a patch  is constructed from several overlapping slices
Unlike current works in signal and image processing, which train a  local dictionary on the patches {RiX} N i=N, in what follows  we define the learning problem with respect to the slices,  {si} N i=N, instead
In other words, we aim to train DL instead of Ω
As a motivation, we present in Figure N a set of  patches RiX extracted from natural images and their corresponding slices si, obtained from the proposed algorithm,  which will be presented in Section N
Indeed, one can observe that the slices are simpler than the patches, as they  contain less information
 NNNN    N
Proposed method: slice-based dictionary  learning  The convolutional dictionary learning problem refers to  the following optimizationN objective,  min D,Γ  N  N ‖X−DΓ‖NN + λ‖Γ‖N, (N)  for a convolutional dictionary D as in Figure N and a Lagrangian parameter λ that controls the sparsity level
Employing the decomposition of X in terms of its slices, and  the separability of the ℓN norm, the above can be written as  the following constrained minimization problem,  min DL,{αi}  N  i=N ,  {si} N  i=N  N  N ‖X−  N ∑  i=N  RTi si‖ N N + λ  N ∑  i=N  ‖αi‖N  s.t
si = DLαi
 (N)  One could tackle this problem using half-quadratic splitting  [NN] by introducing a penalty term over the violation of the  constraint and gradually increasing its importance
Alternatively, we can employ the ADMM algorithm [N] and solve  the augmented Lagrangian formulation (in its scaled form),  min DL,{αi}  N  i=N ,  {si} N  i=N ,{ui}  N  i=N  N  N ‖X−  N ∑  i=N  RTi si‖ N N  +  N ∑  i=N  (  λ‖αi‖N + ρ  N ‖si −DLαi + ui‖  N N  )  ,  (N)  where {ui} N i=N are the dual variables that enable the constrains to be met
 N.N
Local sparse coding and dictionary update  The minimization of Equation (N) with respect to all the  needles {αi} N i=N is separable, and can be addressed independently for every αi by leveraging standard tools such  as LARS
This also allows for having a different number  of non-zeros per slice, depending on the local complexity
 Similarly, the minimization with respect to DL can be done  using any patch-based dictionary learning algorithm such as  the K-SVD, MOD, online dictionary learning or trainlets
 Note that in the dictionary update stage, while minimizing  for DL, one could refrain from iterating until convergence,  and instead perform only a few iterations before proceeding  with the remaining variables
In addition, when employing  the K-SVD dictionary update, the {αi} are also updated subject to the support found in the sparse pursuit stage
 NHereafter, we assume that the atoms in the dictionary are normalized  to a unit ℓN norm
 N.N
Slice update via local Laplacian  The minimization of Equation (N) with respect to all the  slices {si} N i=N amounts to solving the quadratic problem  min {si}Ni=N  N  N ‖X−  N ∑  i=N  RTi si‖ N N +  ρ  N  N ∑  i=N  ‖si −DLαi + ui‖ N N
 (N)  Taking the derivative with respect to the variables  sN, sN, 


sN and nulling them, we obtain the following system of linear equations  RN(  N ∑  i=N  RTi si −X) + ρ(sN −DLαN + uN) = 0  ..
 RN (  N ∑  i=N  RTi si −X) + ρ(sN −DLαN + uN ) = 0
 (N)  Defining  R̄ =        RN ..
 RN       S̄ =        sN ..
 sN       Z̄ =        DLαN − uN ..
 DLαN − uN       , (N)  the above can be written as  0 = R̄ (  R̄T S̄−X )  + ρ (  S̄− Z̄ )  =⇒ S̄ = (  R̄R̄T + ρI )−N (  R̄X+ ρZ̄ )  
(N)  Using the Woodbury matrix identity and the fact that  R̄T R̄ = ∑N  i=N R T i Ri = nI, where I is the identity matrix, the above is equal to  S̄ =  (  N  ρ I−  N  ρN R̄  (  I+ N  ρ R̄T R̄  )−N  R̄T  )  (  R̄X+ ρZ̄ )  = (  I− R̄ (ρI+ nI) −N  R̄T )  (  N  ρ R̄X+ Z̄  )  
 (N)  Plugging the definitions of R̄, S̄ and Z̄, we obtain  si =  (  N  ρ RiX+DLαi − ui  )  −Ri      N  ρ+ n  N ∑  j=N  RTj  (  N  ρ RjX+DLαj − uj  )     
 (N0)  Although seemingly complicated at first glance, the above  is simple to interpret and implement in practice
This expression indicates that one should (i) compute the estimated  slices pi = N ρ RiX+DLαi − ui, then (ii) aggregate them  to obtain the global estimate X̂ = ∑N  j=N R T j pj , and finally  NNNN    Algorithm N: Slice-based dictionary learning  Input : Signal X, initial dictionary DL Output: Trained dictionary DL, needles {αi}  N i=N and  slices {si} N i=N  Initialization:  si = N  n RiX, ui = 0 (NN)  for iteration = N : T do  Local sparse pursuit (needle):  αi = argmin α̂i  ρ  N ‖si −DLα̂i + ui‖  N N + λ‖α̂i‖N  (NN)Slice reconstruction:  pi = N  ρ RiX+DLαi − ui (NN)  Slice aggregation:  X̂ =  N ∑  j=N  RTj pj (NN)  Slice update via local Laplacian:  si = pi − N  ρ+ n RiX̂ (NN)  Dual variable update:  ui = ui + si −DLαi (NN)  Dictionary update:  DL, {αi} N i=N = argmin  DL,{α̂i}Ni=N  N ∑  i=N  ‖si −DLα̂i + ui‖ N N  s.t
supp(α̂i) = supp(αi) (NN)end  (iii) subtract from pi the corresponding patch from the aggregated signal, i.e
RiX̂
As a remark, since this update  essentially subtracts from pi an averaged version of it, it  can be seen as a patch-based local Laplacian operation
 N.N
Boundary conditions  In the description of the CSC model (see Figure N), we  assumed for simplicity circulant boundary conditions
In  practice, however, natural signals such as images are in general not circulant and special treatment is needed for the  boundaries
One way of handling this issue is by assuming that X = MDΓ, where M ∈ RN×N+N(n−N) is matrix that crops the first and last n − N rows of the dictionary D (see Figure N)
The change needed in Algorithm N to incorporate M is minor
Indeed, one has to simply replace  the patch extraction operator Ri, with RiM T , where the  operator MT ∈ RN+N(n−N)×N pads a global signal with n − N zeros on the boundary and Ri extracts a patch from the result
In addition, one has to replace the patch placement operator RTi with MR T i , which simply puts the input  in the location of the i-th patch and then crops the result
 N.N
From patches to slices  The ADMM variant of the proposed algorithm, named  slice-based dictionary learning, is summarized in Algorithm N
While we have assumed the data corresponds to  one signal X, this can be easily extended to consider several signals
 At this point, a discussion regarding the relation between  this algorithm and standard (patch-based) dictionary learning techniques is in place
Indeed, from a quick glance  the two approaches seem very similar: Both perform local sparse pursuit on local patches extracted from the signal, then update the dictionary to represent these patches  better, and finally apply patch-averaging to obtain a global  estimate of the reconstructed signal
Moreover, both iterate  this process in a block-coordinate descent manner in order  to minimize the overall objective
So, what is the difference  between this algorithm and previous approaches?  The answer lies in the migration from patches to slices
 While originally dictionary learning algorithms aimed to  represent patches RiX taken from the signal, our scheme  suggests to train the dictionary to construct slices, which  do not necessarily reconstruct the patch fully
Instead, only  the summation of these slices results in the reconstructed  patches
To illustrate this relation, we show in Figure N  the decomposition of several patches in terms of their constituent slices
One can observe that although the slices are  simple in nature, they manage to construct the rather complex patches
The difference between this illustration and  that of Figure N is that the latter shows patches RiX and  only the slices that are fully contained in them
 Note that the slices are not mere auxiliary variables, but  rather emerge naturally from the convolutional formulation
After initializing these with patches from the signal,  Figure N: The first column contains patches extracted from  the training data, and second to eleventh columns are the  corresponding slices constructing these patches
Only the  ten slices with the highest energy are presented
 NNNN    si = N n RiX, each iteration progressively “carves” portions  from the patch via the local Laplacian, resulting in simpler  constructions
Eventually, these variables are guaranteed to  converge to DLαi – the slices we have defined
 Having established the similarities and differences between the traditional patch-based approach and the slice  alternative, one might wonder what is the advantage of  working with slices over patches
In the conventional approach, the patches are processed independently, ignoring  their overlap
In the slice-based case, however, the local  Laplacian forces the slices to communicate and reach a consensus on the reconstructed signal
Put differently, the CSC  offers a global model, while earlier patch-based methods  used local models without any holistic fusion of them
 N
Comparison to other methods  In this section we explain further the advantages of our  method, and compare it to standard algorithms for training  the CSC model such as [NN, NN]
Arguably the main difference resides in our localized treatment, as opposed to the  global Fourier domain processing
Our approach enables  the following benefits:  N
The sparse pursuit step can be done separately for each  slice and is therefore trivial to parallelize
 N
The algorithm can work in a complete online regime  where in each iteration it samples a random subset of  slices, solves a pursuit for these and then updates the  dictionary accordingly
Adopting a similar strategy in  the competing algorithms [NN, NN] might be problematic, since these are deployed in the Fourier domain on  global signals and it is therefore unclear how to operate  on a subset of local patches
 N
Our algorithm can be easily modified to allow a different number of non-zeros in each location of the global  signal
Such local complexity adaptation cannot be offered by the Fourier-oriented algorithms
 We now turn to comparing the proposed algorithm to  alternative methods in terms of computational complexity
 Denote by I the number of signals on which the dictionary  is trained, and by k the maximal number of non-zeros in a  needleN αi
At each iteration of our algorithm we employ  LARS that has a complexity of O(kN+mkN+nm) per slice [N0], resulting in O(IN(kN +mkN + nm) + nmN) compu- tations for all N slices in all the I images
The last term,  nmN, corresponds to the precomputation of the Gram of the  dictionary DL (which is in general negligible)
Then, given  the obtained needles, we reconstruct the slices, requiring  O(INnk), aggregate the results to form the global estimate,  NAlthough we solve the Lagrangian formulation of LARS, we also limit  the maximal number of non-zeros per needle to be at most k
 Method Time Complexity  [NN]  I < m mI  N N + (q − N)mIN  ︸ ︷︷ ︸  linear systems  + qImN log (N) ︸ ︷︷ ︸  FFT  + qImN ︸ ︷︷ ︸  thresholding  [NN]  I ≥ m m  N N + (q − N)m  N N  ︸ ︷︷ ︸  linear systems  + qImN log (N) ︸ ︷︷ ︸  FFT  + qImN ︸ ︷︷ ︸  thresholding  Ours INnm + IN(k N + mk  N )  ︸ ︷︷ ︸  LARS / OMP  +nm N  ︸ ︷︷ ︸  Gram  + INk(n + m) + nm N  ︸ ︷︷ ︸  K-SVD  Table N: Complexity analysis
For the convenience of the  reader, the dominant term is highlighted in red color
 incurring O(INn), and update the slices, which requires an additional O(INn)
These steps are negligible compared to the sparse pursuits and are thus omitted in the final expression
Finally, we update the dictionary using the K-SVD,  which is O(nmN + INkn + INkm) [NN]
We summarize the above in Table N
In addition, we present in the same  table the complexity of each iteration of the (Fourier-based)  algorithm in [NN]
In this case, q corresponds to the number of inner iterations in their ADMM solver of the sparse  pursuit and dictionary update
 The most computationally demanding step in our algorithm is the local sparse pursuit, which is O(NI(kN+mkN+ nm))
Assuming that the needles are very sparse, which indeed happens in all of our experiments, this reduces to  O(NImn)
On the other hand, the complexity in the algo- rithm of [NN] is dominated by the computation of the FFT,  which is O(NImq log(N))
We conclude that our algo- rithm scales linearly with the global dimension, while theirs  grows as N log(N)
Note that this also holds for other re- lated methods, such as that of [NN], which also depend on  the global FFT
Moreover, one should remember the fact  that in our scheme one might run the pursuits on a small  percentage of the total number of slices, meaning that in  practice our algorithm can scale as O(µNInm), where µ is a constant smaller than one
 N
Image processing via CSC  In this section, we demonstrate our proposed algorithm  on several image processing tasks
Note that the discussion thus far focused on one dimensional signals, however  it can be easily generalized to images by replacing the convolutional structure in the CSC model with block-circulant  circulant-block (BCCB) matrices
 N.N
Image inpainting  Assume an original image X is multiplied by a diagonal  binary matrix A ∈ RN×N , which masks the entries Xi in which A(i, i) = 0
In the task of image inpainting, given the corrupted image Y = AX, the goal is to restore the original unknown X
One can tackle this problem by solving the following CSC problem  min Γ  N  N ‖Y −ADΓ‖NN + λ‖Γ‖N, (NN)  NN00    where we assume the dictionary D was pretrained
Using  similar steps to those leading to Equation (N), the above can  be written as  min {αi}  N  i=N ,{si}  N  i=N ,  {ui} N  i=N  N  N ‖Y −A  N ∑  i=N  RTi si‖ N N  +  N ∑  i=N  (  λ‖αi‖N + ρ  N ‖si −DLαi + ui‖  N N  )  
 (NN)  This objective can be minimized via the algorithm described  in the previous section
Moreover, the minimization with  respect to the local sparse codes {αi} N i=N remains the same
 The only difference regards the update of the slices {si} N i=N,  in which case one obtains the following expression  si =  (  N  ρ RiY +DLαi − ui  )  −Ri      N  ρ+ n A  N ∑  j=N  RTj  (  N  ρ RjY +DLαj − uj  )     
 (N0)  The steps leading to the above equation are almost identical to those in subsection N.N, and they only differ in the  incorporation of the mask A
 N.N
Texture and cartoon separation  In this task the goal is to decompose an image X into  its texture component XT that contains highly oscillating  or pseudo-random patterns, and a cartoon part XC that is  a piece-wise smooth image
Many image separation algorithms tackle this problem by imposing a prior on both  components
For cartoon, one usually employs the isotropic  (or anisotropic) Total Variation norm, denoted by ‖XC‖TV 
The modeling of texture, on the other hand, is more difficult  and several approaches have been considered over the years  [NN, N, NN, NN]
 In this work, we propose to model the texture component using the CSC model
As such, the task of separation  amounts to solving the following problem  min DT ,ΓT ,  XC  N  N ‖X−DTΓT −XC‖  N N + λ ‖ΓT ‖N + ξ‖XC‖TV ,  (NN)  where DT is a convolutional (texture) dictionary, and ΓT is  its corresponding sparse vector
Using similar derivations  to those presented in Section N.N, the above is equivalent to  min DL,α  i  T ,si  T ,  XC ,ZC  N  N  ∥  ∥  ∥  ∥  ∥  X−  N ∑  i=N  RTi s i T −XC  ∥  ∥  ∥  ∥  ∥  N  N  +λ  N ∑  i=N  ∥  ∥α i T  ∥  ∥  N + ξ‖ZC‖TV  s.t
siT = DLα i T , XC = ZC ,  (NN)  where we split the variable XC into XC = ZC in order to facilitate the minimization over the TV norm
Its corresponding ADMM formulationN is given by  min DL,α  i  T ,si  T ,ui  T ,  XC ,ZC ,VC  N  N  ∥  ∥  ∥  ∥  ∥  X−  N ∑  i=N  RTi s i T −XC  ∥  ∥  ∥  ∥  ∥  N  N  +  N ∑  i=N  (ρ  N  ∥  ∥siT −DLα i T + u  i T  ∥  ∥  N  N + λ  ∥  ∥α i T  ∥  ∥  N  )  + η  N ‖XC − ZC +VC‖  N N + ξ‖ZC‖TV ,  (NN)  where {siT } N i=N, {α  i T }  N i=N and {u  i T }  N i=N are the texture  slices, needles and dual variables, respectively, and VC is  the dual variable of the global cartoon XC 
The above optimization problem can be minimized by slightly modifying  Algorithm N
The update for {αi} N i=N is a sparse pursuit and  the update for the ZC variable is a TV denoising problem
 Then, one can update the {siT } N i=N and XC jointly by  siT = N  ρ piT −  N ρ  N + n N  ρ + N  η  Ri      N  ρ  N ∑  j=N  RTj p j T +  N  η QC      XC = N  η QC −  N η  N + n N  ρ + N  η      N  ρ  N ∑  j=N  RTj p j T +  N  η QC     ,  (NN)  where piT = RiX + ρ (  DLα i T − u  i T  )  and QC = X + η (ZC −VC)
The final step of the algorithm is updat- ing the texture dictionary DL via any dictionary learning  method
 N
Experiments  We turn to demonstrate our proposed slice-based dictionary learning
Throughout the experiments we use the  LARS algorithm [N] to solve the LASSO problem and the  K-SVD [N] for the dictionary learning
The reader should  keep in mind, nevertheless, that one could use any other  pursuit or dictionary learning algorithm for the respective  updates
In all experiments, the number of filters trained are  N00 and they are of size NN× NN
 N.N
Slice-based dictionary learning  Following the test setting presented in [NN], we run our  proposed algorithm to solve Equation (N) with λ = N on the Fruit dataset [NN], which contains ten images
As in  [NN], the images were mean subtracted and contrast normalized
We present in Figure N the dictionary obtained  NDisregarding the training of the dictionary, this is a standard twofunction ADMM problem
The first set of variables are {si T }N i=N  and XC ,  and the second are {αi T }N i=N  and ZC 
 NN0N    after several iterations using our proposed slice-based dictionary learning, and compare it to the result in [NN] and  also to the method AVA-AMS in [NN]
Note that all three  methods handle the boundary conditions, which were discussed in Section N.N
We compare in Figure N the objective  of the three algorithms as function of time, showing that our  algorithm is more stable and also converges faster
In addition, to demonstrate one of the advantages of our scheme,  we train the dictionary on a small subset (N0%) of all slices and present the obtained result in the same figure
 (a) Proposed - Iteration N
(b) Proposed - Iteration N00
 (c) [NN]
(d) [NN]
 Figure N: The dictionary obtained after N and N00 iterations  using the slice-based dictionary learning method
Notice  how the atoms become crisper as the iterations progress
 For comparison, we present also the result of [NN] and [NN]
 Figure N: Our method versus the those in [NN] and [NN]
 N.N
Image inpainting  We turn to test our proposed algorithm on the task of image inpainting, as described in Section N.N
We follow the  experimental setting presented in [NN] and compare to their  state-of-the-art method using their publicly available code
 The dictionaries employed in both approaches are trained  on the Fruit dataset, as described in the previous subsection (see Figure N)
For a fair comparison, in the inference  stage, we tuned the parameter λ for both approaches
Table  N presents the results in terms of peak signal-to-noise ratio  (PSNR) on a set of publicly available standard test images,  showing our method leads to quantitatively better resultsN
 Figure N compares the two visually, showing our method  also leads to better qualitative results
 A common strategy in image restoration is to train the  dictionary on the corrupted image itself, as shown in [NN],  as opposed to employing a dictionary trained on a separate  collection of images
The algorithm presented in Section  N.N can be easily adapted to this framework by updating the  local dictionary on the slices obtained at every iteration
To  exemplify the benefits of this, we include the resultsN obtained by using this approach in Table N and Figure N
 N.N
Texture and cartoon separation  We conclude by applying our proposed slice-based dictionary learning algorithm to the task of texture and cartoon  separation
The TV denoiser used in the following experiments is the publicly available software of [N]
We run  our method on the synthetic image Sakura and a portion  extracted from Barbara, both taken from [NN], and on the  image Cat, originally from [NN]
For each of these, we compare with the corresponding methods
We present the results of all three experiments in Figure N, together with the  NThe PSNR is computed as N0 log( √ N/‖X−X̂‖N), where X and X̂  are the original and restored images
Since the images are normalized, the  range of PSNR values is non-standard
NA comparison with the method of [NN] was not possible in this case, as  their implementation cannot handle training a dictionary on standard-sized  images
 Figure N: Visual comparison on a cropped region extracted  from the image Barbara
Left: [NN] (PSNR = N.NNdB)
Middle: Ours (PSNR = N.NNdB)
Right: Ours with dictionary  trained on the corrupted image (PSNR = NN.NNdB)
 NN0N    Barbara Boat House Lena Peppers C.man Couple Finger Hill Man Montage  Heide et al
NN.00 N0.NN N0.NN NN.NN N.NN N.NN NN.NN NN.NN N0.NN NN.N0 NN.NN  Proposed NN.NN N0.NN N0.NN NN.NN N.NN N.NN NN.NN NN.0N N0.NN NN.NN NN.N0  Image specific NN.N0 NN.N0 NN.NN NN.NN NN.NN N0.NN NN.NN NN.0N N0.N0 NN.NN NN.NN  Table N: Comparison between the slice-based dictionary learning and the algorithm in [NN] on the task of image inpainting
 (a) Original image
(b) Enhanced output
 Figure N: Enhancement of the image Flower via cartoontexture separation
 trained dictionaries
Lastly, as an application for our texture separation algorithm, we enhance the image Flower by  multiplying its texture component by a scalar factor (greater  than one) and combining the result with the original image
 We treat the colored image by transforming it to the Lab  color space, manipulating the L channel, and finally transforming the result back to the original domain
The original  image and the obtained result are depicted in Figure N
One  can observe that our approach does not suffer from halos,  gradient reversals or other common enhancement artifacts
 N
Conclusion  In this work we proposed the slice-based dictionary  learning algorithm
Our method employs standard patchbased tools from the realm of sparsity to solve the global  CSC problem
We have shown the relation between our  method and the patch-averaging paradigm, clarifying the  main differences between the two: (i) the migration from  patches to the simpler entities called slices, and (ii) the application of a local Laplacian that results in a global consensus
Finally, we illustrated the advantages of the proposed  algorithm in a series of applications and compared it to related state-of-the-art methods
 N
Acknowledgments  The research leading to these results has received funding in part from the European Research Council under EU’s  Nth Framework Program, ERC under Grant NN0NNN, and in  part by Israel Science Foundation (ISF) grant no
NNN0/NN
 (a) Original
(b) Dictionary
(c) Original
(d) Dictionary
(e) Original
(f) Dictionary
 (g) Our cartoon
(h) Our texture
(i) Our cartoon
(j) Our texture
(k) Our cartoon
(l) Our texture
 (m) [NN]
(n) [NN]
(o) [NN]
(p) [NN]
(q) [NN]
(r) [NN]
 Figure N: Texture and cartoon separation for the images Sakura, Barbara and Cat
 NN0N    References  [N] M
Aharon, M
Elad, and A
Bruckstein
K-svd: An algorithm for designing overcomplete dictionaries for sparse  representation
IEEE Transactions on Signal Processing,  NN(NN):NNNN–NNNN, N00N
 [N] J.-F
Aujol, G
Gilboa, T
Chan, and S
Osher
Structuretexture image decompositionmodeling, algorithms, and parameter selection
International Journal of Computer Vision,  NN(N):NNN–NNN, N00N
 [N] S
Boyd, N
Parikh, E
Chu, B
Peleato, and J
Eckstein
 Distributed optimization and statistical learning via the alternating direction method of multipliers
Foundations and  Trends R© in Machine Learning, N(N):N–NNN, N0NN
 [N] H
Bristow, A
Eriksson, and S
Lucey
Fast convolutional  sparse coding
In Proceedings of the IEEE Conference on  Computer Vision and Pattern Recognition, pages NNN–NNN,  N0NN
 [N] S
H
Chan, R
Khoshabeh, K
B
Gibson, P
E
Gill, and T
Q
 Nguyen
An augmented lagrangian method for total variation  video restoration
IEEE Transactions on Image Processing,  N0(NN):N0NN–NNNN, N0NN
 [N] S
Chen, S
A
Billings, and W
Luo
Orthogonal least squares  methods and their application to non-linear system identification
International Journal of Control, N0(N):NNNN–NNNN,  NNNN
 [N] S
S
Chen, D
L
Donoho, and M
A
Saunders
Atomic  Decomposition by Basis Pursuit
SIAM Review, NN(N):NNN–  NNN, N00N
 [N] W
Dong, L
Zhang, G
Shi, and X
Wu
Image deblurring  and super-resolution by adaptive sparse domain selection and  adaptive regularization
IEEE Transactions on Image Processing, N0(N):NNNN–NNNN, N0NN
 [N] B
Efron, T
Hastie, I
Johnstone, R
Tibshirani, et al
Least  angle regression
The Annals of Statistics, NN(N):N0N–NNN,  N00N
 [N0] M
Elad and M
Aharon
Image denoising via sparse  and redundant representations over learned dictionaries
 IEEE Transactions on Image Processing, NN(NN):NNNN–NNNN,  N00N
 [NN] M
Elad and M
Aharon
Image denoising via sparse and  redundant representations over learned dictionaries
IEEE  Transactions on Image Processing, NN(NN):NNNN–NNNN, Dec
 N00N
 [NN] M
Elad, J.-L
Starck, P
Querre, and D
L
Donoho
Simultaneous cartoon and texture image inpainting using morphological component analysis (mca)
Applied and Computational Harmonic Analysis, NN(N):NN0–NNN, N00N
 [NN] K
Engan, S
O
Aase, and J
H
Husoy
Method of optimal  directions for frame design
In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),  volume N, pages NNNN–NNNN
IEEE, NNNN
 [NN] D
Geman and C
Yang
Nonlinear image recovery with halfquadratic regularization
IEEE Transactions on Image Processing, N(N):NNN–NNN, NNNN
 [NN] R
Grosse, R
Raina, H
Kwong, and A
Y
Ng
ShiftInvariant Sparse Coding for Audio Classification
In Uncertainty in Artificial Intelligence, N00N
 [NN] S
Gu, W
Zuo, Q
Xie, D
Meng, X
Feng, and L
Zhang
 Convolutional sparse coding for image super-resolution
In  Proceedings of the IEEE International Conference on Computer Vision, pages NNNN–NNNN, N0NN
 [NN] F
Heide, W
Heidrich, and G
Wetzstein
Fast and flexible  convolutional sparse coding
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNNN–  NNNN
IEEE, N0NN
 [NN] F
Huang and A
Anandkumar
Convolutional dictionary  learning through tensor factorization
In Feature Extraction:  Modern Questions and Challenges, pages NNN–NNN, N0NN
 [NN] B
Kong and C
C
Fowlkes
Fast convolutional sparse coding (fcsc)
Department of Computer Science, University of  California, Irvine, Tech
Rep, N0NN
 [N0] J
Mairal, F
Bach, J
Ponce, et al
Sparse modeling for image  and vision processing
Foundations and Trends R© in Computer Graphics and Vision, N(N-N):NN–NNN, N0NN
 [NN] J
Mairal, F
Bach, J
Ponce, and G
Sapiro
Online dictionary  learning for sparse coding
In Proceedings of the NNth Annual  International Conference on Machine Learning, pages NNN–  NNN
ACM, N00N
 [NN] S
Ono, T
Miyata, and I
Yamada
Cartoon-texture image decomposition using blockwise low-rank texture characterization
IEEE Transactions on Image Processing, NN(N):NNNN–  NNNN, N0NN
 [NN] V
Papyan, J
Sulam, and M
Elad
Working locally thinking globally-part I: Theoretical guarantees for convolutional  sparse coding
arXiv preprint arXiv:NN0N.0N00N, N0NN
 [NN] V
Papyan, J
Sulam, and M
Elad
Working locally thinking  globally-part II: Stability and algorithms for convolutional  sparse coding
arXiv preprint arXiv:NN0N.0N00N, N0NN
 [NN] R
Rubinstein, M
Zibulevsky, and M
Elad
Efficient implementation of the k-svd algorithm using batch orthogonal  matching pursuit
Cs Technion, N0(N):N–NN, N00N
 [NN] J
Sulam, B
Ophir, M
Zibulevsky, and M
Elad
Trainlets:  Dictionary learning in high dimensions
IEEE Transactions  on Signal Processing, NN(NN):NNN0–NNNN, N0NN
 [NN] B
Wohlberg
Efficient convolutional sparse coding
In IEEE  International Conference on Acoustics, Speech and Signal  Processing (ICASSP), pages NNNN–NNNN
IEEE, N0NN
 [NN] B
Wohlberg
Boundary handling for convolutional sparse  representations
In Image Processing (ICIP), N0NN IEEE International Conference on, pages NNNN–NNNN
IEEE, N0NN
 [NN] J
Wright, A
Y
Yang, A
Ganesh, S
S
Sastry, and Y
Ma
 Robust face recognition via sparse representation
IEEE  Transactions on Pattern Analysis and Machine Intelligence,  NN(N):NN0–NNN, N00N
 [N0] J
Yang, J
Wright, T
S
Huang, and Y
Ma
Image superresolution via sparse representation
IEEE Transactions on  Image Processing, NN(NN):NNNN–NNNN, N0N0
 [NN] M
D
Zeiler, D
Krishnan, G
W
Taylor, and R
Fergus
Deconvolutional networks
In IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), pages NNNN–NNNN
 IEEE, N0N0
 [NN] H
Zhang and V
M
Patel
Convolutional sparse codingbased image decomposition
In BMVC, N0NN
 NN0NTALL: Temporal Activity Localization via Language Query   TALL: Temporal Activity Localization via Language Query  Jiyang GaoN Chen SunN Zhenheng YangN Ram NevatiaN  NUniversity of Southern California NGoogle Research  {jiyangga, zhenheny, nevatia}@usc.edu, chensun@google.com  Abstract  This paper focuses on temporal localization of actions  in untrimmed videos
Existing methods typically train classifiers for a pre-defined list of actions and apply them in  a sliding window fashion
However, activities in the wild  consist of a wide combination of actors, actions and objects; it is difficult to design a proper activity list that meets  users’ needs
We propose to localize activities by natural  language queries
Temporal Activity Localization via Language (TALL) is challenging as it requires: (N) suitable design of text and video representations to allow cross-modal  matching of actions and language queries; (N) ability to locate actions accurately given features from sliding windows  of limited granularity
We propose a novel Cross-modal  Temporal Regression Localizer (CTRL) to jointly model text  query and video clips, output alignment scores and action  boundary regression results for candidate clips
For evaluation, we adopt TaCoS dataset, and build a new dataset for  this task on top of Charades by adding sentence temporal  annotations, called Charades-STA
We also build complex  sentence queries in Charades-STA for test
Experimental  results show that CTRL outperforms previous methods significantly on both datasets
 N
Introduction  Activities in the wild consist of a diverse combination  of actors, actions and objects over various periods of time
 Earlier work focused on classification of video clips that  contained a single activity, i.e
where the videos were  trimmed
Recently, there has also been significant work in  localizing activities in longer, untrimmed videos [N0, NN]
 One major limitation of existing action localization methods is that they are restricted to pre-defined list of actions
 Although the lists of activities can be relatively large [N],  they still face difficulty in covering complex activity questions, for example, “A person runs to the window and then  look out.” , as shown in Figure N
Hence, it is desirable to  use natural language queries to localize activities
Use of  natural language not only allows for an open set of activiN.N s NN.N s  Language Query: A person runs to the window and then look out  Figure N
Temporal activity localization via language query in an  untrimmed video
 ties but also natural specification of additional constraints,  including objects and their properties as well as relations  between the involved entities
We propose the task of Temporal Activity Localization via Language (TALL): given a  temporally untrimmed video and a natural language query,  the goal is to determine the start and end times for the described activity inside the video
 For traditional temporal action localization, most current  approaches [N0, NN, NN, NN, NN] apply activity classifiers  trained with optical flow-based methods [NN, NN] or Convolutional Neural Networks (CNNs) [NN, NN] in a sliding  window fashion
A direct extension to support natural language query is to map the queries into a discrete label space
 However, it is non-trivial to design a label space which has  enough coverage for such activities without losing useful  details in users’ queries
 To go beyond discrete activity labels, one possible solution is to embed visual features and sentence features into  a common space [N0, NN, NN]
However, for temporal localization of activities, it is unclear what a proper visual  model to extract visual features for retrieval is, and how  to achieve high precision of predicted start/end time
Although one could densely sample sliding windows at different scales, doing so is not only computationally expensive but also makes the alignment task more challenging,  as the search space increases
An alternative to dense sampling is to adjust the temporal boundaries of proposals by  learning regression parameters; such an approach has been  successful for object localization, as in [NN]
However, temNNNN    poral regression has not been attempted in the past work  and is more difficult as the activities are characterized by  a spatio-temporal volume, which may lead to more background noise
 These challenges motivate us to propose a novel Crossmodal Temporal Regression Localizer (CTRL) model to  jointly model text query, video clip candidates and their  temporal context information to solve the TALL task
 CTRL generates alignment scores along with location regression results for candidate clips
It utilizes a CNN model  to extract visual features of the clips and a Long Shortterm Memory (LSTM) network to extract sentence embeddings
A cross-modal processing module is designed to  jointly model the text and visual features, which calculates  element-wise addition, multiplication and direct concatenation
Finally, multilayer networks are trained for visualsemantic alignment and clip location regression
We design  the non-parameterized and parameterized location offsets  for temporal coordinate regression
In parameterized setting, the length and the central coordinate of the clip is first  parameterized by the ground truth length and coordinate
 In non-parameterized setting, the start and end coordinates  are used directly
We show that the non-parameterized one  works better, unlike the case for object boundary regression
 To facilitate research of TALL, we also generate sentence temporal annotations for Charades [NN] dataset
 We name it Charades-STA.We evaluate our methods on  TACoS and Charades-STA datasets by the metric of “R@n,  IoU=m”, which represents the percentage of at least one of  the top-n results ( start and end pairs ) having IoU with the  ground truth larger than m
Experimental results demonstrate the effectiveness of our proposed CTRL framework
 In summary, our contributions are two-fold:  (N) We propose a novel problem formulation of Temporal  Activity Localization via natural Language (TALL) query
 (N) We introduce an effective Cross-modal Temporal  Regression Localizer (CTRL) which estimates alignment  scores and temporal action boundary by jointly modeling  language query and video clips.N  N
Related Work  Action classification and temporal localization
There  have been tremendous explorations about action classification in videos using deep convolutional neural networks  (ConvNets)
Representative methods include two-stream  ConvNets, CND (ND ConvNets) and ND ConvNets with  temporal LSTM or mean pooling
Specifically, Simonyan  and Zisserman [NN] modeled the appearance and motion  information in two separate ConvNets and combined the  scores by late fusion
Tran et al
[NN] used ND convolutional filters to capture motion information in neighboring  NSource codes are available in https://github.com/jiyanggao/TALL 
 frames
[NN] [N0] proposed to use ND ConvNets to extract  deep features for one frame and use temporal mean pooling  or LSTM to model temporal information
 For temporal action localization task, Shou et al
[NN]  trained CND [NN] with localization loss and achieved stateof-the-art performance on THUMOS NN
Ma et al
[NN]  used a temporal LSTM to generate frame-wise prediction  scores and then merged the detection intervals based on  the predictions
Singh et al
[N0] extended two-stream  [NN] framework with person detection and bi-directional  LSTMs and achieved state-of-the-art performance on MPIICooking dataset [NN]
Gao et al
[N] proposed to use temporal coordinate regression to refine action boundary for temporal localization
 Sentence-based image/video retrieval
Given a set of  candidate videos/images and a sentence query, this task requires retrieving the videos/images that match the query
 Karpathy et al
[N] proposed Deep Visual-Semantic Alignment (DVSA) model
DVSA used bidirectional LSTMs to  encode sentence embeddings, and R-CNN object detectors  [N] to extract features from object proposals
Skip-thought  [NN] learned a SentNVec model by applying skip-gram [NN]  on sentence level and achieved top performance in sentencebased image retrieval task
Sun et al
[NN] proposed to discover visual concepts from image-sentence pairs and apply  the concept detectors for image retrieval
Gao et al
[N]  proposed to learn verb-object pairs as action concepts from  image-sentence pairs
Hu et al
[N] and Mao et al
[NN]  formulated the problem of natural language object retrieval
 As for video retrieval, Lin et al
[NN] parsed the sentence descriptions into a semantic graph, which are then  matched to visual concepts in the videos by generalized bipartite matching
Bojanowski et al
[N] tackled the problem  of video-text alignment: given a video and a set of sentences  with temporal ordering, assigning a temporal interval for  each sentence
In our settings, only one sentence query is  input to the system and temporal ordering is not used
 Object detection
Our work is partly inspired by the  success of recent object detection approaches
R-CNN [N]  consists of selective search, CNN feature extraction, SVM  classification and bounding box regression
Fast-RCNN [N]  designs RoI pooling layer and the model could be trained  by end-to-end framework
One of the key element shared  in those successful object detection frameworks [NN, NN, N]  is the bounding box regression layer
We show that, unlike object boundary regression using parameterized offsets,  non-parameterized offsets work better for action boundary  regression
 N
Methods  In this section, we describe our Cross-modal Temporal  Regression Localizer (CTRL) for Temporal Activity Localization via Language (TALL) and training procedure in deNNNN    Sentence  Query  Sentence Embedding  Skip‐Thoughts  LSTM Word  Embedding  OR  Clip­level feature extractor  multi­modal  Processing concatenation  FC  FC FC  alignment  score  location  regressor  FC  Add  Mul  pooling  Visual Encoder  �" #  �"  �$  �$ %&'  �$ ()"& ,%&+  ��-,./  �-,/  Sentence Encoder  �"$  �$ (0N ,%&+  Figure N
Cross-modal Temporal Regression Localizer (CTRL) architecture
CTRL contains four modules: a visual encoder to extract  features for video clips, a sentence encoder to extract embeddings, a multi-modal processing network to generate combined representations  for visual and text domain, and a temporal regression network to produce alignment scores and location offsets
 tail
CTRL contains four modules: a visual encoder to extract features for video clips, a sentence encoder to extract  embeddings, a multi-modal processing network to generate  combined representations for visual and text domain, and  a temporal regression network to produce alignment scores  and location offsets between the input sentence query and  video clips
 N.N
Problem Formulation  We denote a video as V = {ft} T t=N, T is the frame number of the video
Each video is associated with temporal  sentence annotations: A = {(sj , τ s j , τ  e j )}  M j=N, M is the sentence annotation number of the video V , sj is a natural lan- guage sentence of a video clip, which has τ sj and τ  e j as start  and end time in the video
The training data are the sentence and video clip pairs
The task is to predict one or more  (τ sj , τ e j ) for the input natural language sentence query
 N.N
CTRL Architecture  Visual Encoder
For a long untrimmed video V , we gen- erate a set of video clips C = {(ci, t  s i , t  e i )}  H i=N by temporal  sliding windows, where H is the total number of the clips of the video V , tsi and t  e i are the start and end time of video  clip ci
We define visual encoder as a function Fve(ci) that maps a certain clip ci and its context to a feature vector fv , whose dimension is ds
Inside the visual encoder, a fea- ture extractor Ev is used to extract clip-level feature vec- tors, whose input is nf frames and output is a vector with dimension dv 
For one video clip ci, we consider itself (as the central clip) and its surrounding clips (as context clips)  ci,q, q ∈ [−n, n], j is the clip shift, n is the shift boundary
 We uniformly sample nf frames from each clip (central and context clips)
The feature vector of central clip is denoted  as f ctlv 
For the context clips, we use a pooling layer to calculate a pre-context feature fprev = N n  ∑−N q=−n Ev(ci,q) and  post-context feature fpostv = N n  ∑n q=N Ev(ci,q)
Pre-context  feature and post-context feature are pooled separately, as  the end and the start of an activity can be quite different and  both could be critical for temporal localization
fprev , f ctl v  and fpostv are concatenated and then linearly transformed to the feature vector fv with dimension ds, as the visual repre- sentation for clip ci
 Sentence Encoder
A sentence encoder is a function  Fse(sj) that maps a sentence description sj to a embedding space, whose dimension is ds( the same as visual feature space )
Specifically, a sentence embedding extractor Es is used to extract a sentence-level embedding f ′s and is fol- lowed by a linear transformation layer, which maps f ′s to fs with dimension ds, the same as visual representation fv 
We experiment two kinds of sentence embedding extractors,  one is a LSTM network which takes a word as input at each  step, and the hidden state of final step is used as sentencelevel embedding; the other is an off-the-shelf sentence encoder, Skip-thought [NN]
More details would be discussed  in Section N
 Multi-modal Processing Module
The inputs of the  multi-modal processing module are a visual representation  fv and a sentence embedding fs, which have the same di- mension ds
We use vector element-wise addition (+), vec- tor element-wise multiplication (×) and vector concatena- tion (‖) followed by a Fully Connected (FC) layer to com- bine the information from both modalities
Addition and  NNNN    multiplication operation allow additive and multiplicative  interaction between two modalities and don’t change the  feature dimension
The FC layer allows interaction among all elements
The input dimension of the FC layer is N ∗ ds and the output dimension is ds
The outputs from all three operations are concatenated to construct a multi-modal representation fsv = (fs × fv) ‖ (fs + fv) ‖ FC(fs ‖ fv), which is the input for our core module, temporal localization regression networks
 Temporal Localization Regression Networks
Temporal localization regression network takes the multi-modal  representation fsv as input, and has two sibling output lay- ers
The first one outputs an alignment score csi,j between the sentence sj and the video clip ci
The second one out- puts clip location regression offsets
We design two location  offsets, the first one is parameterized offset: t = (tc, tl), where tc and tl are parameterized central point offset and length offset respectively
The parameterization is as follows:  tp = (p− pc)/lc, tl = log(l/lc) (N)  where p and l denote the clip’s center coordinate and clip length respectively
Variables p, pc are for predicted clip and test clip (like wise for l)
The second offset is non- parameterized offset: t = (ts, te), where ts and te are the start and end point offsets
 ts = s− sc, te = e− ec (N)  where s and e denote the clip’s start and end coordinate re- spectively
Temporal coordinate regression can be thought  as clip location regression from a test clip to a nearby  ground-truth clip, as the original clip could be either too  tight or too loose, the regression process tend to find better  locations
 N.N
CTRL Training  Multi-task Loss Function
CTRL contains two sibling  output layers, one for alignment and the other for regression
We design a multi-task loss L on a mini-batch of training samples to jointly train for visual-semantic alignment and clip location regression
 L = Laln + αLreg (N)  where Laln is for visual-semantic alignment and Lreg is for clip location regression, and α is a hyper-parameter, which controls the balance between the two task losses
The alignment loss encourages aligned clip-sentence pairs to have  positive scores and misaligned pairs to have negative scores
 Laln = N  N  N∑  i=0  [αclog(N + exp(−csi,i))+  N∑  j=0,j N=i  αwlog(N + exp(csi,j))] (N)  clip c  �" �#  Intersection  Union  Non Intersection  Length  Figure N
Intersection over Union (IoU) and non-Intersection over  Length (nIoL)
 where N is the batch size, csi,j is the alignment score be- tween sentence sj and video clip ci, αc and αw are the hy- per parameters which control the weights between positive  ( aligned ) and negative ( misaligned ) clip-sentence pairs
 The regression loss Lreg is calculated for the aligned clip-sentence pairs
A sentence sj annotation contains start and end time (τ sj , τ  e j )
The aligned sliding window clip ci  has (tsi , t e i )
The ground truth offsets t  ∗ are calculated from  start and end times
 Lreg = N  N  N∑  i=0  [R(t∗x,i − tx,i) +R(t ∗ y,i − ty,i)] (N)  where x and y indicate p and l for parameterized offsets, or s and e for non-parameterized offsets
R(t) is smooth LN function
 Sampling Training Examples
To collect training samples, we use multi-scale temporal sliding windows with  [NN, NNN, NNN, NNN] frames and N0% overlap
(Note that,  at test time, we only use coarsely sampled clips.) We  use the following strategy to collect training samples T = {[(sh, τ  s h, τ  e h), (ch, t  s h, t  e h)]}  NT h=0
Each training sample contains a sentence description (sh, τ s h, τ  e h) and a video clip  (ch, t s h, t  e h)
For a sliding window clip c from C with temporal annotation (ts, te) and a sentence description s with temporal annotation (τ s, τe), we align them as a pair of training samples if they satisfy (N) Intersection over Union  (IoU) is larger than 0.N; (N) non Intersection over Length  (nIoL) is smaller than 0.N and (N) one sliding window clip  can be aligned with only one sentence description
The reason we use nIoL is that we want the the most part of the  sliding window clip to overlap with the assigned sentence,  and simply increasing IoU threshold would harm regression  layers ( regression aims to move the clip from low IoU to  high IoU)
As shown in Figure N, although the IoU between  c and sN is about 0.N, if we assign c to sN, then it will disturb the model ,because c contains information of sN
 N
Evaluation  In this section, we describe the evaluation settings and  discuss the experiment results  NNN0    N.N
Datasets  TACoS [NN]
This dataset was built on the top of MPIICompositive dataset [NN] and contains NNN videos
Every  video is associated with two type of annotations
The first  one is fine-grained activity labels with temporal location  (start and end time)
The second set of annotations is natural  language descriptions with temporal locations
The natural language descriptions were obtained by crowd-sourcing  annotators, who were asked to describe the content of the  video clips by sentences
In total, there are NNNNN pairs of  sentence and video clips
We split it in N0% for training,  NN% for validation and NN% for test
 Charades-STA
Charades [NN] contains around N0k  videos and each video contains temporal activity annotation (from NNN activity categories) and multiple video-level  descriptions
TALL needs clip-level sentence annotation:  sentence descriptions with start and end time, which are  not provided in the original Charades dataset
We noticed  that the names of activity categories in Charades are parsed  from the video-level descriptions, so many of activity names  appear in descriptions
Another observation we make is  that most descriptions in Charades share a similar syntactic structure: consisting of multiple sub-sentences, which  are connected by comma, period and conjunctions, such as  “then”, “while”, “after”, “and”
For example, “A person is  sitting down by the door
They stand up and start carefully  leaving some dishes in the sink”
 Based on these observations, we designed a semiautomatic way to generate sentence temporal annotation
 The first step is sentence decomposition: a long sentence  is split to sub-sentences by a set of conjunctions (which are  collected by hand ), and for each sub-sentence, the subject (  parsed by Stanford CoreNLP [NN] ) of the original long sentence is added to start
The second step is keyword matching: we extract keywords for each activity categories and  match them to sub-sentences, if they are matched, the temporal annotation (start and end time) are assigned to the subsentences
The third step is a human check: for each pair  of sub-sentence and temporal annotation, we (two of the  co-authors) checked whether the sentence made sense and  whether they matched the activity annotation
An example  is shown in Figure N
 Although TACoS and Charades-STA are challenging,  their lengths of queries are limited to single sentences
 To explore the potential of CTRL framework on handling  longer and more complex sentences, we build a complex  sentence set
Inside each video, we connect consecutive  sub-sentences to make complex query, each complex query  contains at least two sub-sentences, and is checked to make  sure that the time span is less than half of the video length
 We use them for test purpose only
In total, there are NNNNN  clip-sentence pairs in Charades-STA training set, NNNN clipsentence pairs in test set and NNNN complex sentence quires
 Sit Stand Up  Video  Activity Annotation  Sentence  Sub­sentences  decomposition Sub Sub  keyword matching  A person is sitting down by the  door. They stand up and start   carefully leaving some dishes in   the sink
 Sub 0:A person is sitting down by the door.  Sub N: They stand up
 Sub N: They start carefully leaving some   dishes in the sink  [N.N, N.N]: Stand up  [N.N, N.N]: Sit  Sentence  Activity Annotation STA  Sub­Sentences  [N.N, N.N]: A person is sitting   down by the door  [N.N, N.N]: They stand up
 Figure N
Charades-STA construction
 On average, there are N.N words per non-complex sentence,  and NN.N words per complex sentence
 N.N
Experiment Settings  We will introduce evaluation metric, baseline methods  and our system variants in this part
 N.N.N Evaluation Metric  We adopted a similar metric used by [N] to compute “R@n, IoU=m”, which means that the percentage of at least one of the top-n results having Intersection over Union (IoU) larger than m
This metric itself is on sentence level, so the overall performance is the average among all the sentences
R(n,m) = NN ∑N  i=N r(n,m, si), where r(n,m, si) is the recall for a query si, N is total number of queries and R(n,m) is the averaged overall performance
 N.N.N Baseline Methods  We consider two sentence based image/video retrieval  baseline methods: visual-semantic alignment with LSTM  (VSA-RNN ) [N] and visual-semantic alignment with Skipthought vector (VSA-STV) [NN]
For these two baseline  methods, we use the same training samples and test sliding  windows as those for CTRL
 VSA-RNN
This baseline method is similar to the model  in DVSA [N]
We use a regular LSTM instead of BRNN  to encode the input description
The size of hidden state  of LSTM is N0NN and the output size is N000
Video  clips are processed by a CND network that is pre-trained  on SportsNM [N0]
The N0NN dimensional fcN vector is extracted and linearly transformed to N000 dimensional,  which is used as the clip-level feature
Cosine similarity  is used to calculate the confidence score between the clip  and the sentence
Hinge loss is used to train the model
 NNNN    At test time, we compute the alignment score between input sentence query and all the sliding windows in the video
 VSA-STV: Instead of using RNN to extract sentence embedding, we use an off-the-shelf Skip-thought [NN] sentence  embedding extractor
A skip-thought vector is NN00 dimensional, we linearly transform it to N000 dimensional
Visual  encoder is the same as for VSA-RNN
 Verb and Object Classifiers
We also implemented  baseline methods based on annotations of pre-defined actions and objects
TACoS dataset also contains pre-defined  actions and object annotations at clip-level
These objects and actions annotations are from the original MPIICompositive dataset [NN]
NN categories of actions and NN  categories of objects are involved in training set
We use  the same CND feature as above to train action classifiers and  object classifiers
The classifier is based on a N-layer fully  connected network, the size of first layer is N0NN and the  size of second layer is the number of categories
The test  sentences are parsed by Stanford CoreNLP [NN], and verbobject (VO) pairs are extracted using the sentence dependencies
The VO pairs are matched with action and object  annotations based on string matching
The alignment score  between a sentence query and a clip is the score of matched  action and object classifier responses
Verb means that we  only use action classifier; Verb+Obj means that both action  classifiers and object classifiers are used
 N.N.N System Variants  We experimented with variants of our system to test the effectiveness of our method
CTRL(aln): we don’t use regression, train the CTRL with only alignment loss Laln
CTRL(reg-p): train the CTRL with alignment loss Laln and parameterized regression loss Lreg−p
CTRL(reg-np): context information is considered and CTRL is trained with  alignment loss Laln and non-parameterized regression loss Lreg−np
CTRL(loc): SCNN [NN] proposed to use overlap loss to improve activity localization performance
Based on  our pure alignment(without regression), we implemented a  similar loss function considering clip overlap as in SCNN
 Lloc = ∑n  i (0.N∗( N/(N+e−csi,i )N  IoUi −N)), where csi,i and IoUi  are respectively the alignment score and Intersection over  Union (IoU) between the aligned pairs of sentence and clip  in a mini-batch
The major difference is that SCNN solved a  classification problem, so they use Softmax score, however  in our case, we consider an alignment problem
The overall loss function is Lscnn = Laln + Lloc
For this method, we use CND as the visual encoder and Skip-thought as the  sentence encoder
 N.N
Experiments on TACoS  In this part, we discuss the experiment results on TACoS
 First we compare the performance of different visual en0.N 0.N 0.N 0.N 0.N Intersection over Union (IoU)  0.0  0.N  0.N  0.N  0.N  0.N  0.N  R e ca  ll  R@N0  R@N  R@N  Comparison of Visual Features  CND VGG MeanPooling LRCN  Figure N
Performance comparison of different visual encoders
 coders; second we compare two sentence embedding methods; third we compare the performance of CTRL variants  and baseline methods
The length of sliding windows for  test is NNN with overlap 0.N, multi-scale windows are not  used
We empirically set the context clip number n as N and the length of context window as NNN frames
The dimension of fv , fs and fsv are all set to N000
We set batch size as NN, the networks are optimized by Adam [NN] optimizer on  a Nvidia TITAN X GPU
 Comparison of visual features
We consider three cliplevel visual encoders: CND [NN], LRCN [N], VGG+Mean  Pooling [N0]
Each of them takes a clip with NN frames as  input and outputs a N000-dimensional feature vector
For  CND, fcN feature vector is extracted and then linearly trans- formed to N000-dimension
For LRCN and VGG poolng,  we extract fcN of VGG-NN for each frame
The LSTM’s hidden state size is NNN.We use Skip-thought as the sentence embedding extractor and other parts of the model  are the same to CTRL(aln)
There are three groups of  curves, which are Recall@N0, Recall@N and Recall@N respectively, shown in Figure
N
We can see that CND performs generally better than other two methods
LRCN’s  performance is inferior, the reason maybe that the dataset is  relatively small, not enough to train the LSTM well
 Comparison of sentence embedding
For sentence  encoder, we consider two commonly used methods:  wordNvec+LSTM [N] and Skip-thought [NN]
In our implementation of wordNvec, we train skip-gram model on English Dump of Wikipedia
The dimension of the word vector is N00 and the hidden state size of the LSTM is NNN
For  Skip-thought vector, we linearly transform it from NN00dimension to N000-dimension
We use CND as the visual  feature extractor and other parts are the same to CTRL(aln)
 From the results, we can see that the performance of Skipthought is generally better than wordNvec+LSTM
We conjecture the reason is that the scale of TACoS is not large  enough to train the LSTM (comparing with the counterpart  NNNN    0.N 0.N 0.N 0.N 0.N Intersection over Union (IoU)  0.00  0.0N  0.N0  0.NN  0.N0  0.NN  0.N0  R e ca  ll@ N  Comparison of Sentence Embedding  Skip-thought LSTM  Figure N
Performance comparison of different sentence embedding
 datasets in object detection, like ReferIt [NN], FlickrN0k Entities [N0], which contains over N00k sentences)
 Comparison with other methods
We test our system  variants and baseline methods on TACoS and report the result for IoU ∈ {0.N, 0.N, 0.N} and Recall@{N, N}
The results are shown in Table N
“Random” means that we  randomly select n windows from the test sliding windows and evaluate Recall@n with IoU=m
All methods use the same CND features
VSA-RNN uses the end-to-end trained  LSTM as the sentence encoder and all other methods use  pre-trained Skip-thought as sentence embedding extractor
 We can see that visual retrieval baselines (i.e
VSARNN, VSA-STV) lead to inferior performance, even compared with our pure alignment model CTRL(aln)
We believe the major reasons are two-fold: N) the multilayer alignment network learns better alignment than the simple cosine  similarity model, which is trained by hinge loss function; N)  visual retrieval models do not encode temporal context information in a video
Pre-defined classifiers also produce  inferior results
We think it is mainly because the predefined actions and objects are not precise enough to represent sentence queries
By comparing Verb and Verb+Obj,  we can see that additional object (such as “knife”, “egg”)  information helps to represent sentence queries
 Temporal action boundary regression As described  before, we implemented a temporal localization loss function similar to the one in SCNN [NN], which consider clip  overlap
Experiment results show that CTRL(loc) does  not bring much improvement over CTRL(aln), perhaps because CTRL(loc) still relies on clip selection from sliding  windows, which may not overlap with ground truth well
 CTRL(reg-np) outperforms CTRL(aln) and CTRL(loc) significantly, showing the effectiveness of temporal regression  model
By comparing CTRL(reg-p) and CTRL(reg-np) in  Table N, it can be seen that non-parameterized setting helps  the localizer regress the action boundary to a more accurate  location
We think the reason is that unlike objects can be  re-scaled in images due to camera projection, actions’ time  spans can not be easily rescaled in videos (we don’t consider  slow motion and quick motion)
Thus, to do the boundary  regression effectively, the object bounding box coordinates  Table N
Comparison of different methods on TACoS  Method R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  Random 0.NN N.NN N.NN N.NN N.0N NN.0N  Verb N.NN N.NN N.NN N.NN N.NN NN.NN  Verb+Obj N.NN NN.NN NN.NN NN.NN NN.N0 NN.N0  VSA-RNN N.NN N.NN N.NN N.N0 NN.N0 NN.0N  VSA-STV N.NN N0.NN NN.0N NN.N0 NN.NN NN.NN  CTRL (aln) N0.NN NN.NN NN.NN NN.NN NN.0N NN.0N  CTRL (loc) N0.N0 NN.NN NN.NN NN.NN NN.N0 NN.NN  CTRL (reg-p) NN.NN NN.NN NN.NN NN.0N NN.NN NN.NN  CTRL (reg-np) NN.N0 NN.NN NN.NN NN.NN NN.NN NN.NN  Table N
Comparison of different methods on Charades-STA  Method R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  Random N.NN N.0N NN.NN NN.0N  VSA-RNN N0.N0 N.NN NN.NN N0.NN  VSA-STV NN.NN N.NN NN.NN NN.NN  CTRL (aln) NN.NN N.NN NN.NN NN.NN  CTRL (loc) N0.NN N.NN NN.NN NN.NN  CTRL (reg-p) NN.NN N.NN NN.NN NN.NN  CTRL (reg-np) NN.NN N.NN NN.NN NN.NN  Table N
Experiments of complex sentence query
 Method R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  Random NN.NN N.NN NN.NN NN.NN  CTRL NN.0N N.0N NN.NN NN.NN  CTRL+Fusion NN.NN N.NN NN.NN NN.NN  should be first normalized to some standard scale, but for  actions, time itself is the standard scale
 Some prediction and regression results are shown in Figure N
We can see that the alignment prediction gives  a coarse location, which is limited by the fixed window  length; the regression model helps to refine the clip’s bounding box to a higher IoU location
 N.N
Experiments on Charades-STA  In this part, we evaluate CTRL models and baseline  methods on Charades-STA and report the results for IoU ∈ {0.N, 0.N} and Recall@{N, N}, which are shown in Table N
The lengths of sliding windows for test are NNN and NNN,  window’s overlap is 0.N
It can be seen that the results  are consistent with those in TACoS
CTRL(reg-np) shows  a significant improvement over CTRL(aln) and CTRL(loc)
 The non-parameterized settings (CTRL(reg-np)) work consistently better than the parameterized settings (CTRL(regp))
Figure N shows some prediction and regression results
 We also test complex sentence query on Charades-STA
 As shown in Table
N, “CTRL” means that we simply input the whole complex sentence into CTRL model
 “CTRL+fusion” means that we input each sentence of a  complex query separately into CTRL, and then do a late fusion
Specifically, we compute the average alignment score  NNNN    ground truth  alignment prediction  regression refinement  NN.0 s NN.N s  NN.N s N0.N s  NN.0 s NN.N s  Query: He gets a cutting board and knife
 Query: The person sets up two separate glasses on the counter
 NN.0 s NN.N s  NN.N s NN.0 s  NN.N s NN.N s  ground truth  alignment prediction  regression refinement  Figure N
Alignment prediction and regression refinement examples in TACoS
The row with gray background shows the ground truth for  the given query; the row with blue background shows the sliding window alignment results; the row with green background shows the clip  regression results
 ground truth  alignment prediction  regression refinement  Query:A person runs to the window and then look out  Complex Query:A person is walking around the room
She is eating something  ground truth N.0 s NN.N s  alignment prediction N.N s N.N s  regression refinement N.N s NN.N s  N.N s NN.Ns  N.Ns NN.N s  N0.N s NN.N s  regression refinement + fusion N.Ns NN.N s  Figure N
Alignment prediction and regression refinement examples in Charades-STA
 over all sentences, take the minimum of all start times and  maximum of all end times as start and end time of the complex query
Although the random performance in Table
N  (complex) is higher than that in Table N (single), the gain  over random performance remains similar, which indicates  that CTRL is able to handle complex query consisting multiple activities well
Comparing CTRL and CTRL+Fusion,  we can see that CTRL could be an effective first step for  complex query, if combined with other fusion methods
 In general, we observed two types of common hard  cases: (N) long query sentences increase chances of failure,  likely because the sentence embeddings are not discriminative enough; (N) videos that contain similar activities but  with different objects (e.g
in TACOS dataset, put a cucumber on chopping board, and put a knife on chopping board)  are hard to distinguish amongst each other
 N
Conclusion  We addressed the problem of Temporal Activity Localization via Language (TALL) and proposed a novel Crossmodal Temporal Regression Localizer (CTRL) model,  which uses temporal regression for activity location refinement
We showed that non-parameterized offsets works  better than parameterized offsets for temporal boundary regression
Experimental results show the effectiveness of our  method on TACoS and Charades-STA
 NNNN    References  [N] P
Bojanowski, R
Lajugie, E
Grave, F
Bach, I
Laptev,  J
Ponce, and C
Schmid
Weakly-supervised alignment of  video with text
In ICCV, N0NN
 [N] F
Caba Heilbron, V
Escorcia, B
Ghanem, and J
Carlos Niebles
Activitynet: A large-scale video benchmark for  human activity understanding
In CVPR, N0NN
 [N] J
Donahue, L
Anne Hendricks, S
Guadarrama,  M
Rohrbach, S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual  recognition and description
In CVPR, N0NN
 [N] J
Gao, C
Sun, and R
Nevatia
Acd: Action concept discovery from image-sentence corpora
In ICMR
ACM, N0NN
 [N] J
Gao, Z
Yang, C
Sun, K
Chen, and R
Nevatia
Turn  tap: Temporal unit regression network for temporal action  proposals
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [N] R
Girshick
Fast r-cnn
In ICCV, N0NN
 [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
 [N] R
Hu, H
Xu, M
Rohrbach, J
Feng, K
Saenko, and T
Darrell
Natural language object retrieval
In CVPR, N0NN
 [N] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, N0NN
 [N0] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In CVPR, N0NN
 [NN] S
Kazemzadeh, V
Ordonez, M
Matten, and T
L
Berg
 Referitgame: Referring to objects in photographs of natural  scenes
In EMNLP, N0NN
 [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
In ICLR, N0NN
 [NN] R
Kiros, Y
Zhu, R
R
Salakhutdinov, R
Zemel, R
Urtasun,  A
Torralba, and S
Fidler
Skip-thought vectors
In NIPS,  N0NN
 [NN] D
Lin, S
Fidler, C
Kong, and R
Urtasun
Visual semantic  search: Retrieving videos via complex textual queries
In  CVPR, N0NN
 [NN] S
Ma, L
Sigal, and S
Sclaroff
Learning activity progression in lstms for activity detection and early detection
In  CVPR, N0NN
 [NN] C
D
Manning, M
Surdeanu, J
Bauer, J
R
Finkel,  S
Bethard, and D
McClosky
The stanford corenlp natural language processing toolkit
In ACL, N0NN
 [NN] J
Mao, J
Huang, A
Toshev, O
Camburu, A
L
Yuille, and  K
Murphy
Generation and comprehension of unambiguous  object descriptions
In CVPR, N0NN
 [NN] J
Mao, W
Xu, Y
Yang, J
Wang, Z
Huang, and A
Yuille
 Deep captioning with multimodal recurrent neural networks  (m-rnn)
In ICLR, N0NN
 [NN] T
Mikolov, I
Sutskever, K
Chen, G
S
Corrado, and  J
Dean
Distributed representations of words and phrases  and their compositionality
In NIPS, N0NN
 [N0] B
A
Plummer, L
Wang, C
M
Cervantes, J
C
Caicedo,  J
Hockenmaier, and S
Lazebnik
FlickrN0k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models
In ICCV, N0NN
 [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In  CVPR, N0NN
 [NN] M
Regneri, M
Rohrbach, D
Wetzel, S
Thater, B
Schiele,  and M
Pinkal
Grounding action descriptions in videos
 Transactions of the Association for Computational Linguistics, N:NN–NN, N0NN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
 [NN] M
Rohrbach, S
Amin, M
Andriluka, and B
Schiele
A  database for fine grained activity detection of cooking activities
In CVPR, N0NN
 [NN] M
Rohrbach, M
Regneri, M
Andriluka, S
Amin,  M
Pinkal, and B
Schiele
Script data for attribute-based  recognition of composite activities
In ECCV, N0NN
 [NN] Z
Shou, D
Wang, and S.-F
Chang
Temporal action localization in untrimmed videos via multi-stage cnns
In CVPR,  N0NN
 [NN] G
A
Sigurdsson, G
Varol, X
Wang, A
Farhadi, I
Laptev,  and A
Gupta
Hollywood in homes: Crowdsourcing data  collection for activity understanding
In ECCV, N0NN
 [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In NIPS, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 [N0] B
Singh, T
K
Marks, M
Jones, O
Tuzel, and M
Shao
A  multi-stream bi-directional recurrent neural network for finegrained action detection
In CVPR, N0NN
 [NN] C
Sun, C
Gan, and R
Nevatia
Automatic concept discovery from parallel text and visual corpora
In ICCV, N0NN
 [NN] D
Tran, L
Bourdev, R
Fergus, L
Torresani, and M
Paluri
 Learning spatiotemporal features with Nd convolutional networks
In ICCV, N0NN
 [NN] H
Wang, A
Kläser, C
Schmid, and C.-L
Liu
Action recognition by dense trajectories
In CVPR, N0NN
 [NN] S
Yeung, O
Russakovsky, G
Mori, and L
Fei-Fei
Endto-end learning of action detection from frame glimpses in  videos
In CVPR, N0NN
 [NN] J
Yuan, B
Ni, X
Yang, and A
A
Kassim
Temporal action  localization with pyramid of score distribution features
In  CVPR, N0NN
 [NN] J
Yue-Hei Ng, M
Hausknecht, S
Vijayanarasimhan,  O
Vinyals, R
Monga, and G
Toderici
Beyond short snippets: Deep networks for video classification
In ICCV, N0NN
 NNNNActive Decision Boundary Annotation With Deep Generative Models   Active Decision Boundary Annotation with Deep Generative Models  Miriam Huijser  Aiir Innovations  Amsterdam, The Netherlands  https://aiir.nl/  Jan C
van Gemert  Delft University of Technology  Delft, The Netherlands  http://jvgemert.github.io/  Abstract  This paper is on active learning where the goal is to  reduce the data annotation burden by interacting with a  (human) oracle during training
Standard active learning  methods ask the oracle to annotate data samples
Instead,  we take a profoundly different approach: we ask for annotations of the decision boundary
We achieve this using a  deep generative model to create novel instances along a Nd  line
A point on the decision boundary is revealed where the  instances change class
Experimentally we show on three  data sets that our method can be plugged into other active  learning schemes, that human oracles can effectively annotate points on the decision boundary, that our method is  robust to annotation noise, and that decision boundary annotations improve over annotating data samples
 N
Introduction  If data is king, then annotation labels are its crown jewels
Big image data sets are relatively easy to obtain; it’s  the ground truth labels that are expensive [N, NN]
With the  huge success of deep learning methods critically depending  on large annotated datasets, there is a strong demand for  reducing the annotation effort [NN, N0, NN, NN, NN]
 In active learning [NN] the goal is to train a good predictor while minimizing the human annotation effort for large  unlabeled data sets
During training, the model can interact  with a human oracle who provides ground truth annotations  on demand
The challenge is to have the model automatically select a small set of the most informative annotations  so that prediction performance is maximized
 In this paper we exploit the power of deep generative  models for active learning
Active learning methods [NN, NN,  NN] typically ask the oracle to label an existing data sample
 Instead, we take a radically different approach: we ask the  oracle to directly annotate the decision boundary itself
To  achieve this, we first use all unlabeled images to learn a Kdimensional embedding
In this K-dimensional embedding  we select a N-dimensional query line and employ a deep  Figure N: Active decision boundary annotation using a deep  generative model
In a K-dimensional feature space (top), a  N-dimensional query line (blue) is converted to a row of images (bottom) by generating visual samples along the line
 The oracle annotates where the generated samples change  classes (red point)
This point lies close to the decision  boundary and we use that point to improve classification
 generative model to generate visual samples along this line
 We visualize the N-dimensional line as an ordered row of  images and simply ask the oracle to visually annotate the  point where the generated visual samples changes classes
 Since the generated images are ordered, the oracle does not  need to examine and annotate each and every image, merely  identifying the change point is enough
The point between  two samples of different classes is a point that lies close to  the decision boundary and we use that point to improve the  classification model
In figure N we show an illustration of  our approach
 NNNN  https://aiir.nl/ http://jvgemert.github.io/   We make the following contributions
First, we use a  deep generative model to present a N-dimensional query  line to the oracle
Second, we directly annotate the decision boundary instead of labeling data samples
Third, we  learn a decision boundary from both labeled instances and  boundary annotations
Fourth, we evaluate if the generative  model is good enough to construct query lines that a human  oracle can annotate, how much noise the decision boundary  annotations allow, and how our decision boundary annotation version of active learning compares to traditional active  learning where data samples are labeled
 N
Related work  Active learning [NN] is an iterative framework and starts  with the initialization of a prediction model either by training on a small set of labeled samples or by random initialization
Subsequently, a query strategy is used to interactively query a oracle which can be another model or a human annotator
The annotated query is then used to retrain  the model and starts the next iteration
Active learning in  computer vision includes work on selecting the most influential images [NN], refraining from labeling unclear visuals [NN], zero-shot transfer learning [NN], multi-label active  learning [NN]
Similar to these methods, our paper uses active learning in the visual domain to minimize the number  of iterations while maximizing prediction accuracy
 There are several settings of active learning
A poolbased setting [NN, NN] assumes the availability of a large  set of unlabeled instances
Instead, a stream-based setting  [N, N] is favorable for online learning where a query is selectively sampled from an incoming data stream
Alternatively,  in a query synthesis setting [N, N, N0] an input data distribution is used to generate queries for the oracle in the input space
Recently, [N] and [N] proposed methods for efficiently learning halfspaces, i.e
linear classifiers, using synthesized instances
Instead of using an existing setting, our  paper proposes a new active learning setting: active boundary annotation
Other active learners use sample instances  to query the oracle
Instead, we generate a row of instances  and query the point where the instances change class label
 We do not query an annotation of a sample, we query an  annotation of the decision boundary
 Query strategies in active learning are the informativeness measures used to select or generate new queries
Much  work has been done on this topic [NN], here we describe a  few prominent strategies
Uncertainty sampling [NN] is a  query strategy that selects the unlabeled sample of which  the current model is least certain
This could be obtained  by sampling closest to the decision boundary [N, NN] or  based on entropy [NN]
The Uncertainty-dense sampling  method [NN, NN] aims to correct for the problems associated with uncertainty sampling by selecting samples that  are not only uncertain but that also lie in dense areas of  the data distribution and are thus representative of the data
 In Batch methods [NN, NN, NN], not a single sample is  queried at each iteration, but a set of samples
In Queryby-committee [NN] multiple models are used as a committee  and queries the samples where the disagreement is highest
Our active boundary annotation method can plug-in  any sampling method and thus does not depend on a particular query strategy
We will experimentally demonstrate  that our boundary annotation method can readily be applied  to various query strategies
 In our active learning approach we make use of deep  generative models
Probabilistic generative models such as  variational autoencoders [N0, NN], learn an inference model  to map images to a latent space and a decoder to map  from latent space back again to image space
Unfortunately  the generated images are sometimes not very sharp
Nonprobabilistic models such as the generative adversarial nets  (GAN) [NN] produce higher-quality images than variational  autoencoders [N0], but can only map from latent space to  image space
These models randomly sample latent vectors from a predefined distribution and then learn a mapping  from these latent vectors to images; there is no mapping that  can embed real images in the latent space
To perform classification on real images in the embedding we need an inference step to map images to the latent space
Fortunately, recent generative adversarial models can produce high-quality  images and provide efficient inference [NN, NN]
In this paper we use such a GAN model
 N
Active decision boundary annotation  We have N data samples x ∈ RD, each sample is paired with a class label (X,Y) = {(xN, yN), 


, (xN , yN )} where for clarity we focus on the binary case y ∈ {−N, N} and a linear classification model
As often done in active  learning we assume an initial set A which contains a hand- ful of annotated images
Each data sample xi has a corresponding latent variable zi ∈ R K 
For clarity we omit the  index i whenever it is clear that we refer to a single data  point
The tightest hypersphere that contains all latent variables zi is denoted by Ω
Every iteration in active learning estimates a decision boundary θ̂ where the goal is to best  approximate the real decision boundary θ while minimizing  the number of iterations
 In figure N we show an overview of our method
Each  image x is embedded in a manifold as a latent variable  Gz(x) = z
In this embedding we use a standard active learning query strategy to select the most informative query  sample z∗
We then construct a query line q in such a  way that it intersects the query sample z∗ and is perpendicular to the current estimate of the decision boundary θ̂ at  point zp
We uniformly sample latent points z along the Ndimensional query line q and for each point on the line generate an estimate of the corresponding image Gx(z) = x̂
 NNNN    Figure N: Overview of our method
Data labels y are the red  squares and yellow stars; unlabeled data is a gray circle
A  deep generative model is used to map an image sample x to  its corresponding latent variable Gz(x) = z
Vice versa, an image is generated from the latent variable z by Gx(z) = x̂
The hypersphere Ω bounds the latent space
The true de- cision boundary (black line) is θ and the current estimate  of the decision boundary (green line) is θ̂
The query line  q (blue) goes through the query sample z∗ and is perpendicular to the current estimate of the decision boundary θ̂,  intersecting it at point zp
The query line q is uniformly  sampled (blue bars) and bounded by Ω, which gives a row of generated images as illustrated at the right
Note how a  ‘0’ morphes to an ‘N’ after it passes the decision boundary  θ
The latent boundary annotation point is given by zq∩θ
 On this generated row of images we ask the oracle to provide the point where the images change class, this is where  the decision boundary intersects the query line q ∩ θ and the latent variable zq∩θ is a decision boundary annotation
 Using the boundary annotation we can assign a label to the  query sample z∗ which we add to the set of annotated samples A
All annotated decision boundary points are stored in the set B
The estimate of the decision boundary θ̂ is found by optimizing a joint classification/regression loss
 The classification loss is computed on the labeled samples  A while at the same time the regression aims to fit the deci- sion boundary through the annotations in B
 Deep generative embedding
We make use of GANs  (Generative Adversarial Nets) [NN] to obtain a high-quality  embedding
In GANs, a generative model G can create realistic-looking images from a latent random variable  G(z) = x̂
The generative model is trained by making use  of a strong discriminator D that tries to separate synthetic  generated images from real images
Once the discriminator  cannot tell synthetic images from real images, the generator  is well trained
The generator and discriminator are simultaneously trained by playing a two-player minimax game,  for details see [NN]
 Deep generative inference
Because we perform classification in the embedding we need an encoder to map the  image samples to the latent space
Thus, in addition to  a mapping from latent space to images (decoding) as in a  standard GAN, we also need an inference model to map  images to latent space (encoding) [NN, NN]
This is done  by optimizing two joint distributions over (x, z): one for the encoder q(x, z) = q(x)q(z|x) and one for the decoder p(x, z) = p(z)p(x|z)
Since both marginals are known, we can sample from them
The encoder marginal q(x) is the empirical data distribution and the decoder marginal is defined as p(z) = N (0, I)
The encoder and the decoder are trained together to fool the discriminator
This is achieved  by playing an adversarial game to try and match the encoder  and decoder joint distributions where the adversarial game  is played between the encoder and decoder on the one side  and the discriminator on the other side, for details see [NN]
 Query strategy
For compatibility with standard active  learning methods we allow plugging in any query strategy that selects an informative query sample z∗
Such  a plug-in is possible by ensuring that the query sample  z∗ is part of the query line q
An example of a popular query sample strategy is uncertainty sampling that selects the sample whose prediction is the least confident:  z∗ = argmaxz N− Pθ̂(ŷ|z), where ŷ is the class label with the highest posterior probability under the current prediction model θ̂
This can be interpreted as the sample where  the model is least certain about
We experimentally validate  our method’s compatibility with common query strategies
 Constructing the query line
The query line q determines which images will be shown to the oracle
To  make decision boundary annotation possible these images  should undergo a class-change
A reasonable strategy is  then to make sure the query line intersects the current estimate of the decision boundary θ̂
A crisp class change  will make annotating the point of change easier
Thus, we  increase the likelihood of a crisp class-change by ensuring  that q is perpendicular to θ̂
Since the query sample z∗ lies  on the query line and q ⊥ θ̂ this is enough information to construct the query line q
Let the current estimation  of the linear decision boundary θ̂ be a hyperplane which  is parametrized by a vector ŵ perpendicular to the plane  and offset with a bias b̂
Then the query line is given by  q(t) = zp + (z∗ − zp)t, where zp is the projection of the  query point z∗ on the current decision boundary θ̂ and is  defined as zp = z∗ − (ŵ ⊺z∗+b̂) ŵ⊺ŵ  ŵ, for derivation details see  the supplemental material
 NNNN    Constructing the image row
We have to make a choice  about which image samples along the query line q to show  to the oracle
We first restrict the size of q to lie within the  tightest hypersphere Ω that captures all latent data samples
The collection of points H that lie within and on the sur- face of the hypersphere Ω is defined as H = {z ∈ RK : ||z− z̄|| ≤ r} where r = maxNi=0 ||zi − z̄|| and z̄ is the av- erage vector over all latent samples z
As a second step we  uniformly sample s latent samples from q∩H which are de- coded to images with the deep generative model G(z) = x̂
 Annotating the decision boundary
Annotating the decision boundary can be done by a human oracle or by a  given model
A model is often used as a convenience to  do large scale experiments where human experiments are  too time-consuming
For example, large scale experiments  for active learning methods that require query sample annotation are typically performed by replacing the human  annotation by the ground truth annotation
This assumes  that the human will annotate identically to the ground truth,  which may be violated close to the decision boundary
To  do large scale experiments for our decision boundary annotation method we also use a model based on the ground  truth, like commonly done for query sample annotation
We  train an oracle-classifier on ground truth labels and use the  that model as a ground truth decision boundary where the  intersection zq∩θ = q∩θ can be computed
For both oracle types –the oracle-classifier and the human oracle– we store  the decision boundary annotations in B and we also ask the annotators for the label y of the query point z∗, for which  we store the pair (z∗, y) in A
We experimentally evaluate human and model-based annotations
 Model optimization using boundary annotations
At  every iteration of active learning we update θ̂
We use a  classification loss on the labeled samples in A while at the same time we optimize a regression loss to fit the decision  boundary through the annotations in B
In this paper we restrict ourselves to the linear case and parametrize θ̂ with a  linear model ŵ⊺z+ b̂ = 0
For the classification loss we use a standard SVM hinge loss over the labeled samples (z, y) in A as  Lclass = N  |A|  ∑  (z,y)∈A  max (  0, N− y(ŵ⊺z+ b̂) )  
(N)  For the regression, we use a simple squared loss to fit the  model ŵ + b̂ to the annotations in B  Lregress = N  |B|  ∑  z∈B  (  ŵ⊺z+ b̂ )N  
(N)  The final loss L jointly weights the classification and re- gression losses equally and simply becomes  L = N  N Lclass +  N  N Lregress + λ||ŵ||  N, (N)  where the parameter λ controls the influence of the regularization term ||ŵ||N where we use λ = N in all experiments
Note that because Lclass and Lregress are both convex losses, the joint loss L is convex as well
 N
Experiments  We perform active learning experiments on three  datasets
MNIST contains N0,000 binary digit images, N0k  to train and N0k in the test set
The SVHN dataset [NN] contains challenging digit images from Google streetview, it  has NN,NNN train and NN,0NN test images
The SVHN dataset  has NNN,NNN extra images, which we use to train the embedding
In the third dataset we evaluate our method on more  variable images than digits
We create the Shoe-Bag dataset  of N0,000 train and NN,000 test images by taking subsets  from the Handbags dataset [NN] and the Shoes dataset [NN]
 For every dataset we train a deep generative embedding  following [NN]
For MNIST and SVHN we train an embedding with N00 dimensions, for the more varied Shoe-Bag  we train an embedding of NNN dimensions
For the training  of the embeddings we set dropout = 0.N for the layers of  the discriminator
All embeddings are trained on the train  data, except for the SVHN embedding; which is trained on  the larger “extra” dataset following [NN]
We will make our  code available [N]
All learning is done in the embedding  and the experiments that do not use a human oracle use an  SVM trained on all labels as the oracle
 We evaluate active learning with the Area Under the (accuracy) Learning Curve (AULC) measure [NN, NN]
The  Area under the Learning Curve is computed by integrating  over the test accuracy scores of N active learning iterations  using the trapezoidal rule:  AULC = N ∑  i=N  N  N (acci−N + acci), (N)  where acc0 is the test accuracy of the initial classifier before the first query
The AULC score measures the informativeness per annotation and is high for active learning  methods that quickly learn high-performing models with  few queries, i.e
in few iterations
We also report Mean  Average Precision (MAP) results in supplemental material
 We first evaluate essential properties of our method on  two classes: ‘0’ versus ‘N’
Later we evaluate on all classes
 N.N
Exp N: Evaluating various query strategies  Our method can re-use standard active learning query  strategies that select a sample for oracle annotation
We  evaluate four sample-based query strategies
Random sampling is a common baseline for query strategies [N0]
Uncertainty sampling selects the least confident sample point [NN]
 Uncertainty-dense sampling [NN] selects samples that are  not only uncertain but that also lie in dense areas of the data  NNNN    Experiment N: Evaluating various query strategies  MNIST 0 vs
N SVHN 0 vs
N Shoe-Bag  Strategy Sample Boundary (ours) Sample Boundary (ours) Sample Boundary (ours)  Uncertainty NNN.0± 0.N NNN.N ± 0.N NNN.N± N.N NNN.N ± N.0 NNN.N± 0.N NNN.N ± 0.N Uncertainty-dense NNN.N± N0.N NNN.0 ± N0.N NN.N± N.N NNN.N ± N.N NNN.0± N.N NNN.N ± N.0 N Cluster centroid NNN.N± 0.N NNN.0 ± 0.N NN.0± N.N N0N.N ± N.N NNN.0± N.N NNN.N ± 0.N Random NNN.N± N.0 NNN.N ± 0.N NNN.N± N.N NNN.N ± N.N NN0.N± N.N NNN.0 ± 0.N  Table N: AULC results for four active learning query strategies
Results are on MNIST (classifying 0 and N), SVHN (classifying 0 and N) and Shoe-Bag after NN0 queries, where the maximum possible AULC score is NN0
The results are averaged  over NN repetitions
For each row, the significantly best result is shown in bold, where significance is measured with a paired  t-test with p < 0.0N
SVHN is the most difficult dataset
Uncertainty sampling is generally the best query strategy
Boundary  annotation significantly outperforms sample annotations for all datasets for all query strategies
 distribution
The K-cluster centroid [NN] uses batches of K  samples, where we set K = N
We plug in each query sam- ple strategy in our line query construction approach used for  decision boundary annotation
 The results in Table N show that for all three datasets  and for all four query strategies our boundary annotations  outperform sample annotations
For the uncertainty-dense  method the improvement is the largest, which may be due to  this method sampling from dense areas of the distribution,  and boundary annotations add complementary information
 The uncertainty sampling gives best results for both active  learning methods and all three datasets
It is also the strategy where our method improves the least, and is thus the  most challenging to improve upon
We select uncertainty  sampling for the remainder of the experiments
 N.N
Exp N: Evaluating generative model quality  The generative model that we plug into our method  should be able to construct recognizable line queries so that  human oracles can annotate them
In figure N we show some  line queries generated for all three datasets by our active  learning method with uncertainty sampling
Some query  lines are difficult to annotate, as shown in figure N(b) and  others are of good quality as shown in figure N(a)
 We quantitatively evaluate the generation quality per  dataset by letting N0 humans each annotate the same N0 line  queries
Line queries are subsampled to have a fixed sample  resolution of 0.NN, i.e
the distance between each image on the line and thus vary in length depending on their position  in the hypersphere Ω
The human annotators are thus pre- sented with more images for longer query lines and fewer  images for shorter query lines
For all N0 line queries we  evaluate the inter-human annotation consistency
A higher  consistency suggests that the query line is well-recognizable  and thus that the generative model has a good image generation quality
We instructed the human oracles to annotate  the point where they saw a class change; or indicate if they  see no change, this happens for N out of the N0 lines
 Experiment N: Evaluating inter-human annotations  lines without change samples deviation  MNIST 0 vs
N N N SVHN 0 vs
N N N Shoe-Bag N N  Table N: Annotation consistency results averaged over N0  query line annotations from N0 human oracles
We show  the number of lines marked as having no class change and  the average deviation in number of images, rounded up,  from the average annotation per line
Human consistency  is worse for the non-uniform Shoe-Bag dataset
The more  uniform datasets MNIST and SVHN have quite accurate human consistency
 In Table N we show the results for the inter-human annotation consistency
The Shoe-Bag embedding does not seem  to be properly trained because the human annotators see no  change in half of the query lines
In addition, the variance  between the images make the consistency lower
MNIST  has a deviation of N images and N lines were reported with  no change
SVHN provides the highest quality query lines the human annotators agreed collectively on the inadequacy  of only one query line and the human annotators are most  consistent for this dataset
 N.N
Exp N: Evaluating annotation noise  In experiment N we show that there is variation in the annotations between human oracles
Here we aim to answer  the question if that variation matters
We evaluate the effect  of query line annotation noise on the classification performance
We vary the degree of additive line annotation noise  with respect to SVM oracle decision boundary annotations  on the N-dimensional line query
We vary the standard deviation σ of Gaussian noise to σ ∈ {N, 


, N} image samples away from the oracle
 NNN0    (a) Query lines with high human consistency
 (b) Query lines with low human consistency
 Figure N: Examples of line queries for MNIST (top two rows) SVHN (middle two rows) and shoe-bag (bottom two rows)
Red  triangles indicate the mean annotation per line and the gray bar indicates the standard deviation from the mean annotation
 (a) Query lines for which the N0 human annotators were most consistent and (b) query lines for which the human annotators  were most inconsistent
For visibility these query lines are subsampled in NN images; the human annotators were presented  with more or fewer images depending on the length of the line query
The human annotators are more consistent for query  lines with clearer images and a more sudden change of class, such as the third and fourth row from the top
It should be noted,  however, that the class-changes on these lines are not as sudden as is visualized here; the human annotators were presented  with more images, also seeing the images in between the images presented here
 The results in Table N show that adding sampling noise  of up to about σ = N images to the SVM oracle annota- tions has a slight negative effect on the performance of our  boundary annotation method, but it is still significantly better than sample annotation
Comparing these results to the  inter-human annotation consistency results in Table N shows  that Shoe-Bag annotation variation is around N, and thus the quality of the generator will likely degrade accuracy
For  MNIST and SVHN the human consistency is around or below N images which is well-handled
 N.N
Exp N: Evaluating a human oracle  In this experiment we evaluate classification performance with a human oracle
For all three datasets we have a  human oracle annotate the first N0 line queries, selected using uncertainty sampling
We repeat the same experimental  setup for sample-based active learning
The results are averaged over NN repetitions
 The results in Table N show that an oracle-SVM outperforms a human annotator
This is probably because the active learner method that is being trained is also an SVM,  NNNN    Experiment N: Evaluating annotation noise  Sampling noise MNIST 0 vs
N SVHN 0 vs
N Shoe-Bag  (# images) Sample Boundary (ours) Sample Boundary (ours) Sample Boundary (ours)  0 NNN.N± 0.N NNN.0 ± 0.N NNN.N± N.N NNN.0 ± 0.N NNN.N± 0.N NNN.N ± 0.N N NNN.N± 0.N NNN.N ± 0.N NNN.N± N.N NNN.N ± N.N NNN.N± 0.N NNN.N ± 0.N N NNN.N± 0.N NNN.N ± 0.N NNN.N± N.N NNN.N ± N.N NNN.N± 0.N NNN.N ± 0.N N NNN.N± 0.N NNN.0 ± 0.N NNN.N± N.N NNN.N ± N.N NNN.N± 0.N NNN.N ± 0.N N NNN.N± 0.N NNN.N± 0.N NNN.N± N.N NNN.N± 0.N NNN.N± 0.N NNN.N ± 0.N N NNN.N ± 0.N NNN.N± 0.N NNN.N± N.N NNN.N± N0.N NNN.N± 0.N NNN.0± 0.N  Table N: AULC results for noisy boundary active learning with uncertainty sampling for MNIST (classifying 0 and N), SVHN  (classifying 0 and N) and Handbags vs
Shoes after NN0 queries (maximum possible score is NN0)
Each experiment is  repeated NN times
For each row, the significantly best result is shown in bold, where significance is measured with a paired  t-test with p < 0.0N
Noise has been added to the boundary annotation points; not to the image labels
Results worsen with  more added noise, with the turning point of the significant better performance of Boundary around a sampling noise of N  images for MNIST and SVHN, and N images for Shoe-Bag
 Experiment N: Evaluating a human oracle  MNIST 0 vs
N SVHN 0 vs
N Shoe-Bag  Annotation Sample Boundary (ours) Sample Boundary (ours) Sample Boundary (ours)  Human oracle N.N± 0.N N.N ± 0.N N.N± 0.N N.N± 0.N N.N± 0.N N.N± 0.N SVM oracle N.N± 0.N N.N± 0.N N.N± 0.N N.N± 0.N N.N± 0.N N.N± 0.N  Table N: AULC results for a human and a SVM oracle for sample-based active learning and our boundary active learning  for MNIST (classifying 0 and N), SVHN (classifying 0 and N) and Shoe-Bag after N0 queries (maximum possible score is  N0)
The experiments are repeated NN times and significant results per row are shown in bold for p < 0.0N
Results always  improve for boundary annotation, but these improvements are not significant for SVHN and Shoe-Bag
 and since the oracle is also an SVM it will choose the perfect samples
For humans, boundary annotation always improves over sample annotation
Yet, for SVHN and ShoeBag this improvement is not significant
This is probably  due to the small number of queries, where our method after  only N0 iterations has not yet achieved peak performance as  corroborated by the learning curves in figure N
 N.N
Exp N: Generalization over classes  Up to now we have shown that our method outperforms  sample-based active learning on a subset of MNIST and  SVHN
To see whether our method generalizes to the other  classes we evaluate the performance averaged over all the  SVHN and MNIST class pairs using uncertainty sampling  as query strategy
We show results in Table N and plot  the learning curves in figure N
Averaged over all datasets  and class pairs our method is significantly better than the  sample-based approach
 We also evaluate on the CIFAR-N0 dataset using GIST  features following Jain et al
(EH-Hash) [NN] for uncertainty  sampling
Results in figure N show we clearly outperform  EH-Hash in terms of AUROC improvement
 Experiment N: Full dataset evaluation  Sample Boundary (ours)  MNIST NNN.N± 0.0N NNN.N ± 0.0N SVHN NNN.N± 0.N NN0.N ± N.N Shoe-Bag NNN.N± 0.N NNN.N ± 0.N  Table N: AULC results for sample-based active learning and  boundary active learning for all datasets after NN0 queries  (maximum possible score is NN0), averaged over all class  pairs
The experiments are repeated N times and significant  results are shown in bold
Significance is measured with  a paired t-test with p < 0.0N
For all datasets our method  significantly improves over sample-based active learning
 N
Discussion  We extend active learning with a method for direct decision boundary annotation
We use a deep generative model  to synthesize new images along a N-dimensional query line,  and ask an oracle to annotate the point where the images  change class: this point is an annotation of the decision  boundary
Note that this may not lead to a direct accelerNNNN    0 N0 N0 N0 N0 N0 N0 N0 N0 N0 N00 NN0 NN0 NN0 NN0 NN0  Number of queries  0.NN  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  N.00  T e st   a cc  u ra  cy  Boundary (ours)  Sample  Supervised  (a) MNIST averaged over all classes
 0 N0 N0 N0 N0 N0 N0 N0 N0 N0 N00 NN0 NN0 NN0 NN0 NN0  Number of queries  0.NN  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  N.00  T e st   a cc  u ra  cy  Boundary (ours)  Sample  Supervised  (b) SVHN averaged over all classes
 0 N0 N0 N0 N0 N0 N0 N0 N0 N0 N00 NN0 NN0 NN0 NN0 NN0  Number of queries  0.NN  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  N.00  T e st   a cc  u ra  cy  Boundary (ours)  Sample  Supervised  (c) Shoe-Bag both classes
 Figure N: Learning curves over all datasets and all class pairs using uncertainty sampling as query strategy
The experiments  are repeated N times, standard deviations are indicated by line width
The fully supervised oracle-SVM is the upper bound
 Our boundary method outperforms the sample-based method
 0 N0 N00 NN0 N00 NN0 N00  Number of queries  0.00  0.0N  0.N0  0.NN  0.N0  0.NN  0.N0  A U  R O  C  I m  p ro  v e m  e n t  Boundary (ours) Jain et al
 Figure N: Comparison of our method against Jain et al
[NN]  on CIFAR-N0 using NNN-d GIST descriptors
The results are  averaged over N repeats
We outperform Jain et al
[NN]  ation of the annotation speed
If the annotation cost is per  hour, e.g
a radiologist, then ease of annotation and system  speed become key
If costs are per task, e.g
Amazon Mechanical Turk, then the informativeness of each annotation  should be maximally exploited
Our method falls in the latter category: We increase the informativeness per annotation
 One disadvantage of our method is that it is very easy to  annotate changes visually, but this is not so straightforward  in other domains
The core of our method can in principle  also be used on any input data, but actually using a human  oracle to detect a class change for non-visual data would become tedious fast
For example, having a human annotate a  class-change for raw sensor data, speech or text documents  would be quite difficult in practice
Our method could still  be applicable if the non-visual data can be easily visualized
 Another problem is precisely annotating the decision  boundary when the margin between classes is large
With  a large margin the generated samples may all look similar  to each other and it is difficult to annotate the class change  exactly
A solution could be to annotate the margin on each  side instead of the decision boundary
 Our method depends critically on the quality of the generative model
We specifically evaluated this by including the Shoe-Bag dataset where the quality of the generated samples impairs the consistency of human annotation
 If the generative model is of low quality, our method will  fail as well
Deep generative models are an active area  of research, so we are confident that the quality of generative models will improve
One possible direction for future  work could be to exploit the knowledge of the annotated  decision boundary to update the generative model
 In this work we consider linear models only
The decision boundary hyperplane lives in a K-N dimensional space,  and thus K-N independents points span the plane exactly
 The reason why we need more than K-N query annotations  is that boundary annotation points may not be independent
 Future work on a new query strategy that would enforce  boundary point independence may be promising to reduce  the number of annotations required
For non-linear models, a non-linear N-dimensional query line could perhaps  work better
Also, when data sets are not linearly separable  we may require more than one annotation of the decision  boundary for N query line
This is left for future work
 Our paper showed that boundary annotation for visual  data is possible and improves results over only labeling  query samples
We show that our method can plug in existing active learning strategies, that humans can consistently annotate the boundary if the generative model is good  enough, that our method is robust to noise, and that it significantly outperforms sample-based methods for all evaluated  classes and data sets
 NNNN    References  [N] https://github.com/MiriamHu/  ActiveBoundary,
 [N] I
M
Alabdulmohsin, X
Gao, and X
Zhang
Efficient active  learning of halfspaces via query synthesis
In AAAI, pages  NNNN–NNNN, N0NN
 [N] D
Angluin
Queries and concept learning
Machine learning, N(N):NNN–NNN, NNNN
 [N] L
E
Atlas, D
A
Cohn, R
E
Ladner, M
A
El-Sharkawi,  R
J
Marks, M
Aggoune, and D
Park
Training connectionist networks with queries and selective sampling
In NIPS,  pages NNN–NNN, NNNN
 [N] E
B
Baum and K
Lang
Query learning can work poorly  when a human oracle is used
In International Joint Conference on Neural Networks, volume N, page N, NNNN
 [N] C
Campbell, N
Cristianini, A
Smola, et al
Query learning with large margin classifiers
In ICML, pages NNN–NNN,  N000
 [N] L
Chen, H
Hassani, and A
Karbasi
Dimension coupling:  Optimal active learning of halfspaces via query synthesis
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [N] D
Cohn, L
Atlas, and R
Ladner
Improving generalization with active learning
Machine learning, NN(N):N0N–NNN,  NNNN
 [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
In  CVPR
IEEE, N00N
 [N0] E
L
Denton, S
Chintala, a
szlam, and R
Fergus
Deep  generative image models using a laplacian pyramid of adversarial networks
In C
Cortes, N
D
Lawrence, D
D
Lee,  M
Sugiyama, and R
Garnett, editors, Advances in Neural  Information Processing Systems NN, pages NNNN–NNNN
Curran Associates, Inc., N0NN
 [NN] J
Donahue, P
Krähenbühl, and T
Darrell
Adversarial feature learning
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] V
Dumoulin, I
Belghazi, B
Poole, A
Lamb, M
Arjovsky,  O
Mastropietro, and A
Courville
Adversarially learned inference
arXiv preprint arXiv:NN0N.00N0N, N0NN
 [NN] A
Freytag, E
Rodner, and J
Denzler
Selecting influential examples: Active learning with expected model output changes
In European Conference on Computer Vision,  pages NNN–NNN
Springer, N0NN
 [NN] E
Gavves, T
Mensink, T
Tommasi, C
G
M
Snoek, and  T
Tuytelaars
Active transfer learning with zero-shot priors:  Reusing past datasets for future tasks
In ICCV, N0NN
 [NN] I
Goodfellow, J
Pouget-Abadie, M
Mirza, B
Xu,  D
Warde-Farley, S
Ozair, A
Courville, and Y
Bengio
Generative adversarial nets
In Advances in Neural Information  Processing Systems, pages NNNN–NNN0, N0NN
 [NN] Y
Guo and D
Schuurmans
Discriminative batch mode active learning
In Advances in neural information processing  systems, pages NNN–N00, N00N
 [NN] J.-H
Jacobsen, J
van Gemert, Z
Lou, and A
W
Smeulders
 Structured receptive fields in cnns
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNN0–NNNN, N0NN
 [NN] P
Jain, S
Vijayanarasimhan, and K
Grauman
Hashing hyperplane queries to near points with applications to largescale active learning
In Advances in Neural Information  Processing Systems, pages NNN–NNN, N0N0
 [NN] C
Kading, A
Freytag, E
Rodner, P
Bodesheim, and J
Denzler
Active learning and discovery of object categories in the  presence of unnameable instances
In CVPR, N0NN
 [N0] D
P
Kingma, S
Mohamed, D
J
Rezende, and M
Welling
 Semi-supervised learning with deep generative models
In  Advances in Neural Information Processing Systems, pages  NNNN–NNNN, N0NN
 [NN] D
P
Kingma and M
Welling
Auto-encoding variational  bayes
ICLR, N0NN
 [NN] D
Lewis and G
A
William
A sequential algorithm for  training text classifiers
In Proceedings of the NNth annual  international ACM SIGIR conference on Research and development in information retrieval, pages N–NN
SpringerVerlag New York, Inc., NNNN
 [NN] D
D
Lewis and J
Catlett
Heterogeneous uncertainty sampling for supervised learning
In Proceedings of the eleventh  international conference on machine learning, pages NNN–  NNN, NNNN
 [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV, N0NN
 [NN] P
Mettes, J
C
van Gemert, and C
G
Snoek
Spot on:  Action localization from pointly-supervised proposals
In  European Conference on Computer Vision, pages NNN–NNN
 Springer, N0NN
 [NN] Y
Netzer, T
Wang, A
Coates, A
Bissacco, B
Wu, and A
Y
 Ng
Reading digits in natural images with unsupervised feature learning
In NIPS workshop on deep learning and unsupervised feature learning, N0NN
 [NN] A
Owens, J
Wu, J
H
McDermott, W
T
Freeman, and  A
Torralba
Ambient sound provides supervision for visual  learning
In ECCV
Springer, N0NN
 [NN] J
ONeill, S
J
Delany, and B
MacNamee
Model-free and  model-based active learning for regression
In Advances in  Computational Intelligence Systems, pages NNN–NNN, N0NN
 [NN] D
Pathak, P
Krahenbuhl, J
Donahue, T
Darrell, and A
A
 Efros
Context encoders: Feature learning by inpainting
In  CVPR, N0NN
 [N0] M
E
Ramirez-Loaiza, M
Sharma, G
Kumar, and M
Bilgic
Active learning: an empirical study of common baselines
Data Mining and Knowledge Discovery, NN(N):NNN–  NNN, N0NN
 [NN] B
Settles
Active learning literature survey
University of  Wisconsin, Madison, NN(NN-NN):NN, N0N0
 [NN] B
Settles and M
Craven
An analysis of active learning  strategies for sequence labeling tasks
In Proceedings of the  conference on empirical methods in natural language processing, pages N0N0–N0NN
Association for Computational  Linguistics, N00N
 [NN] H
S
Seung, M
Opper, and H
Sompolinsky
Query by committee
In Proceedings of the fifth annual workshop on Computational learning theory, pages NNN–NNN
ACM, NNNN
 NNNN  https://github.com/MiriamHu/ActiveBoundary https://github.com/MiriamHu/ActiveBoundary   [NN] X
Shen and C
Zhai
Active feedback in ad hoc information  retrieval
In Proceedings of the NNth annual international  ACM SIGIR conference on Research and development in information retrieval, pages NN–NN
ACM, N00N
 [NN] S
Tong and D
Koller
Support vector machine active learning with applications to text classification
Journal of machine learning research, N(Nov):NN–NN, N00N
 [NN] Z
Wang, B
Du, L
Zhang, L
Zhang, M
Fang, and D
Tao
 Multi-label active learning based on maximum correntropy  criterion: Towards robust and discriminative labeling
In  ECCV, N0NN
 [NN] Z
Xu, R
Akella, and Y
Zhang
Incorporating diversity and  density in active learning for relevance feedback
In European Conference on Information Retrieval, pages NNN–NNN
 Springer, N00N
 [NN] A
Yu and K
Grauman
Fine-grained visual comparisons  with local learning
In Proceedings of the IEEE Conference  on Computer Vision and Pattern Recognition, pages NNN–  NNN, N0NN
 [NN] J
Zhu, H
Wang, B
K
Tsou, and M
Ma
Active learning  with sampling by uncertainty and density for data annotations
IEEE Transactions on audio, speech, and language  processing, NN(N):NNNN–NNNN, N0N0
 [N0] J.-J
Zhu and J
Bento
Generative adversarial active learning
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] J.-Y
Zhu, P
Krähenbühl, E
Shechtman, and A
A
Efros
 Generative visual manipulation on the natural image manifold
In European Conference on Computer Vision, pages  NNN–NNN
Springer, N0NN
 NNNNWeakly-Supervised Learning of Visual Relations   Weakly-supervised learning of visual relations  Julia PeyreN,N Ivan LaptevN,N Cordelia SchmidN,N Josef SivicN,N,N  Abstract  This paper introduces a novel approach for modeling visual relations between pairs of objects
We call relation a  triplet of the form (subject, predicate, object) where the predicate is typically a preposition (eg
’under’, ’in front  of’) or a verb (’hold’, ’ride’) that links a pair of objects  (subject, object)
Learning such relations is challenging as the objects have different spatial configurations and appearances depending on the relation in which they occur
 Another major challenge comes from the difficulty to get annotations, especially at box-level, for all possible triplets,  which makes both learning and evaluation difficult
The  contributions of this paper are threefold
First, we design  strong yet flexible visual features that encode the appearance and spatial configuration for pairs of objects
Second,  we propose a weakly-supervised discriminative clustering  model to learn relations from image-level labels only
Third  we introduce a new challenging dataset of unusual relations  (UnRel) together with an exhaustive annotation, that enables accurate evaluation of visual relation retrieval
We  show experimentally that our model results in state-of-theart results on the visual relationship dataset [NN] significantly improving performance on previously unseen relations (zero-shot learning), and confirm this observation on  our newly introduced UnRel dataset
 N
Introduction  While a great progress has been made on the detection  and localization of individual objects [NN, NN], it is now time  to move one step forward towards understanding complete  scenes
For example, if we want to localize “a person sitting  on a chair under an umbrella”, we not only need to detect the  objects involved : “person”, “chair”, “umbrella”, but also  need to find the correspondence of the semantic relations  “sitting on” and “under” with the correct pairs of objects  in the image
Thus, an important challenge is automatic  NDépartement d’informatique de l’ENS, Ecole normale supérieure,  CNRS, PSL Research University, NN00N Paris, France
NINRIA NCzech Institute of Informatics, Robotics and Cybernetics at the Czech  Technical University in Prague
NUniv
Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, NN000  Grenoble, France
 car under elephant person in cart  person ride dog person on top of traffic light  Figure N: Examples of top retrieved pairs of boxes in UnRel  dataset for unusual queries (indicated below each image)  with our weakly-supervised model described in N.N
 understanding of how entities in an image interact with each  other
 This task presents two main challenges
First, the appearance of objects can change significantly due to interactions with other objects (person cycling, person driving)
 This visual complexity can be tackled by learning “visual  phrases” [NN] capturing the pair of objects in a relation as  one entity, as opposed to first detecting individual entities in  an image and then modeling their relations
This approach,  however, does not scale to the large number of relations  as the number of such visual phrases grows combinatorially, requiring large amounts of training data
To address  this challenge, we need a method that can share knowledge  among similar relations
Intuitively, it seems possible to  generalize frequent relations to unseen triplets like those depicted in Figure N : for example having seen “person ride  horse” at training could help recognizing “person ride dog”  at test time
 The second main challenge comes from the difficulty to  provide exhaustive annotations on the object level for relations that are by their nature non mutually-exclusive (i.e
 “on the left of” is also “next to”)
A complete labeling of R relations for all pairs of N objects in an image would indeed  NNNN    require O(NNR) annotations for each image
Such diffi- culty makes both learning and evaluation very challenging
 For learning, it would be desirable to learn relations from  image-level annotations only
For evaluation, current largescale datasets [NN, NN] do not allow retrieval evaluation due  to large amount of missing annotations
 Contributions
The contributions of this work are threefold
First, to address the combinatorial challenge, we develop a method that can handle a large number of relations  by sharing parameters among them
For example, we learn  a single “on” classifier that can recognize both “person on  bike” and “dog on bike”, even when “dog on bike” has not  been seen in training
The main innovation is a new model  of an object relation that represents a pair of boxes by explicitly incorporating their spatial configuration as well as  the appearance of individual objects
Our model relies on a  multimodal representation of object configurations for each  relation to handle the variability of relations
Second, to  address the challenge of missing training data, we develop  a model that, given pre-trained object detectors, is able to  learn classifiers for object relations from image-level supervision only
It is, thus, sufficient to provide an image-level  annotation, such as “person on bike”, without annotating  the objects involved in the relation
Finally, to address the  issue of missing annotations in test data, we introduce a new  dataset of unusual relations (UnRel), with exhaustive annotation for a set of unusual triplet queries, that enables to  evaluate retrieval on rare triplets and validate the generalization capabilities the learned model
 N
Related Work  Alignment of images with language
Learning correspondences between fragments of sentences and image regions has been addressed by the visual-semantic alignment  which has been used for applications in image retrieval and  caption generation [N, NN, NN]
With the appearance of new  datasets providing box-level natural language annotations  [NN, NN, NN, NN], recent works have also investigated caption generation at the level of image regions for the tasks  of natural language object retrieval [N0, NN, NN] or dense  captioning [NN]
Our approach is similar in the sense that  we aim at aligning a language triplet with a pair of boxes  in the image
Typically, existing approaches do not explicitly represent relations between noun phrases in a sentence  to improve visual-semantic alignment
We believe that understanding these relations is the next step towards image  understanding with potential applications in tasks such as  Visual Question Answering [N]
 Learning triplets
Triplet learning has been addressed in  various tasks such as mining typical relations (knowledge  extraction) [N, NN, NN, NN], reasoning [NN, NN, NN], object detection [NN, NN], image retrieval [NN] or fact retrieval [NN]
 In this work, we address the task of relationship detection in  images
This task was studied for the special case of humanobject interactions [N, N0, NN, NN, N0, NN, N0, NN], where  the triplet is in the form (person, action, object)
Contrary to these approaches, we do not restrict the subject to be a person and we cover a broader class of predicates that includes prepositions and comparatives
Moreover, most of  the previous work in human-object interaction was tested  on small datasets only and does not explicitly address the  combinatorial challenge in modeling relations [NN]
Recently, [NN] tried to generalize this setup to non-human subjects and scale to a larger vocabulary of objects and relations by developing a language model sharing knowledge  among relations for visual relation detection
In our work  we address this combinatorial challenge by developing a  new visual representation that generalizes better to unseen  triplets without the need for a strong language model
This  visual representation shares the spirit of [NN, NN, N0] and  we show it can handle multimodal relations and generalizes  well to unseen triplets
Our model also handles a weaklysupervised set-up when only image-level annotations for  object relations are available
It can thus learn from complex scenes with many objects participating in different relations, whereas previous work either uses full supervision  or typically assumes only one object relation per image, for  example, in images returned by a web search engine
Finally, we also address the problem to evaluate accurately  due to missing annotations also pointed out in [NN, NN]
We  introduce a new dataset of unusual relations exhaustively  labeled for a set of triplet queries, the UnRel dataset
This  dataset enables the evaluation of relation retrieval and localization
Our dataset is related to the “Out of context” dataset  of [N] which also presents objects in unusual configurations
 However, the dataset of [N] is not annotated with relations  and does not match the vocabulary of objects in [NN], which  prevents direct comparisons with existing methods that use  data from [NN] for training
 Weak supervision
Most of the work on weaklysupervised learning for visual recognition has focused on  learning objects [N, NN, NN]
Here, we want to tackle the  task of weakly-supervised detection of relations
This task  is more complex as we need to detect the individual objects  that satisfy the specific relation
We assume that pre-trained  detectors for individual objects are available and learn relations among objects with image-level labels
Our work uses  a discriminative clustering objective [N], that has been successful in several computer vision tasks [N, NN], but has not  been so far, to the best of our knowledge, used for modeling  relations
 Zero-shot learning
Zero-shot learning has been mostly  explored for object classification [NN, NN, NN, NN]  and recently for the task of describing images with  novel objects [NN, NN]
In our work, we address  zero-shot learning of relations in the form of triplets  (subject, predicate, object), where each term has already  NNN0    Figure N: Our visual representation is the composition of  appearance features for each object [a(os),a(oo)] and their spatial configuration r(os,oo) represented by the green ar- row
 been seen independently during training, but not in that specific combination
We develop a model to detect and localize such zero-shot relations
 N
Representing and learning visual relations  We want to represent triplets t = (s, r, o) where s is the subject, o the object and r is the predicate
s and o are nouns and can be objects like “person”, “horse”, “car” or regions  such as “sky”, “street”, “mountain”
The predicate r is a term that links the subject and the object in a sentence and  can be a preposition (“in front of”, “under”), a verb (“ride”,  “hold”) or a comparative adjective (“taller than”)
To detect  and localize such triplets in test images, we assume that the  candidate object detections for s and o are given by a de- tector trained with full supervision
Here we use the object  detector [NN] trained on the Visual Relationship Detection  training set [NN]
In N.N, we will explain our representation  of a triplet t = (s, r, o) and show in N.N how we can learn to detect triplets in images given weak image-level supervision for relations
 N.N
Visual representation of relations  A triplet t = (s, r, o) such as “person next to surfboard” in Figure N visually corresponds to a pair of objects (s, o) in a certain configuration
We represent such pairs by the spatial configuration between object bounding boxes (os,oo) and the individual appearance of each object
 Representing spatial configurations of objects
Given  two boxes os = [xs, ys, ws, hs], oo = [xo, yo, wo, ho], where (x, y) are the coordinates of the center of the box, and (w, h) are the width and height of the box, we encode the spatial configuration with a N-dimensional vector:  r(os,oo) = [ xo − xs√ wshs  ︸ ︷︷ ︸  rN  , yo − ys√ wshs  ︸ ︷︷ ︸  rN  ,  √  woho wshs  ︸ ︷︷ ︸  rN  ,  os ∩ oo os ∪ oo ︸ ︷︷ ︸  rN  , ws hs ︸︷︷︸  rN  , wo ho ︸︷︷︸  rN  ]  (N)  where rN and rN represent the renormalized translation be- tween the two boxes, rN is the ratio of box sizes, rN is the overlap between boxes, and rN, rN encode the aspect ratio of each box respectively
Directly adopting this feature as  our representation might not be well suited for some spatial relations like “next to” which are multimodal
Indeed,  “s next to o” can either correspond to the spatial configura- tion “s left of o” or “s right of o”
Instead, we propose to discretize the feature vector (N) into k bins
For this, we as- sume that the spatial configurations r(os,oo) are generated by a mixture of k Gaussians and we fit the parameters of the Gaussian Mixture Model to the training pairs of boxes
We  take the scores representing the probability of assignment  to each of the k clusters as our spatial representation
In our experiments, we use k = N00, thus the spatial repre- sentation is a N00-dimensional vector
In Figure N, we show  examples of pairs of boxes for the most populated components of the trained GMM
We can observe that our spatial  representation can capture subtle differences between configurations of boxes, see row N and row N of Figure N, where  “person on board” and “person carry board” are in different  clusters
 Representing appearance of objects
Our appearance  features are given by the fcN output of a Fast-RCNN [NN]  trained to detect individual objects
In our experiments, we  use Fast-RCNN with VGGNN pre-trained on ImageNet
As  the extracted features have high dimensionality, we perform  PCA on the LN-normalized features to reduce the number  of dimensions from N0NN to N00
We concatenate the appearance features of the subject and object and apply LNnormalization again
 Our final visual feature is a concatenation of the spatial configuration r(os,oo) and the appearance of objects [a(os),a(oo)]
In our experiments, it has a dimensionality of d = N000
In the fully supervised setup, where each re- lation annotation is associated with a pair of object boxes in  the image, we use ridge regression to train a multi-way relation classifier to assign a relation to a given visual feature
 Training is performed jointly on all relation examples of the  training set
 In the next section, we describe how we learn relation  classifiers with only weak, image-level, annotations
 N.N
Weakly-supervised learning of relations  Equipped with pre-trained detectors for individual objects, our goal here is to learn to detect and localize relations between objects, given image-level supervision only
 For example, for a relation “person falling off horse” we are  given (multiple) object detections for “person” and “horse”,  but do not know which objects participate in the relation,  as illustrated in Figure N
Our model is based on a weaklysupervised discriminative clustering objective [N], where we  introduce latent variables to model which pairs of objects  NNNN    Figure N: Examples for different GMM components of our spatial configuration model (one per row)
In the first column we  show the spatial configuration corresponding to the mean of the pairs of boxes per component
Note that our representation  can capture subtle differences between spatial configurations, see e.g., row N and N
 participate in the relation
We train a classifier for each  predicate r and incorporate weak annotations in the form of constraints on latent variables
Note that the relation classifiers are shared across object categories (eg
the relations  “person on horse” and “cat on table” share the same classifier “on”) and can thus be used to predict unseen triplets
 Discriminative clustering of relations
Our goal is to  learn a set of classifiers W = [wN, ...,wR] ∈ Rd×R where each classifier wr predicts the likelihood of a pair of objects  (s, o) to belong to the rth predicate in a vocabulary of R predicates
If strong supervision was provided for each pair  of objects, we could learn W by solving a ridge regression :  min W∈Rd×R  N  N ‖Z −XW‖NF + λ‖W‖NF (N)  where Z ∈ {0, N}N×R are the ground truth labels for each of the N pairs of objects across all training images, and X = [xN, ...,xN]  T is a N × d matrix where each row xk is the visual feature corresponding to the kth pair of objects
However, in a weakly-supervised setup the entire matrix Z is unknown
Building on DIFFRAC [N], our approach is to  optimize the cost :  min Z∈Z  min W∈Rd×R  N  N ‖Z −XW‖NF + λ‖W‖NF (N)  which treats Z as a latent assignment matrix to be learnt together with the classifiers W ∈ Rd×R
Minimizing the first term encourages the predictions made by W to match the latent assignments Z, while the second term is a LN- regularization on the classifiers W 
This framework enables to incorporate weak annotations by constraining the space  of valid assignment matrices Z ∈ Z 
The valid matrices Z ∈ {0, N}N×R satisfy the multiclass constraint ZNR = NN which assigns a pair of objects to one and only one predicate  r
We describe in the next paragraph how to incorporate the weak annotations as constraints
 Weak annotations as constraints
For an image, we are  given weak annotations in the form of triplets t = (s, r, o) ∈ T 
Having such triplet (s, r, o) indicates that at least one of the pairs of objects with object categories (s, o) is in relation r
Let us call Nt the subset of pairs of objects in the image that correspond to object categories (s, o)
The rows of Z that are in subset Nt should satisfy the constraint :  ∑  n∈Nt  Znr ≥ N (N)  This constraint ensures that at least one of the pair of  objects in the subset Nt is assigned to predicate r
For in- stance, in case of the image in Figure N that contains NN  candidate pairs of objects that potentially match the triplet  t = (person, falling off, horse), the constraint (N) im- poses that at least one of them is in relation falling off 
 Triplet score
At test time, we can compute a score for a  pair of boxes (os,oo) relative to a triplet t = (s, r, o) as  S((os,oo) | t) = vrel((os,oo) | r) + αsubvsub(os | s) +αobjvobj(oo | o) + αlangℓ((s, o) | r),  (N)  where vrel((os,oo)|r) = x(os,oo)wr is the score returned by the classifier wr for predicate r (learnt by optimizing (N)) for the visual representation x(os,oo) of the pair of boxes
 vsub(os|s) and vobj(oo|o) are the object class likelihoods returned by the object detector
ℓ((s, o)|r) is a score of a language model that we can optionally combine with our  visual model
 Optimization
We optimize the cost in (N) on pairs of objects in the training set using a variant of the Frank-Wolfe algorithm [NN, NN]
The hyperparameters (αsub, αobj , αlang) are optimized on an held-out fully-annotated validation set  which has no overlap with our training and test sets
In our  experiments we use the validation split of [NN] of the Visual  NNNN    Figure N: Image from the COCO dataset [NN] associated  with caption : “A person falling off the side of a horse as it  rides”
The boxes correspond to the possible candidates for  object category person (blue) and horse (red)
Our model has to disambiguate the right pair for the relation “falling  off” among NN candidate pairs
 Genome dataset [NN]
Unless otherwise specified, the candidate pairs, both at training and test time, are the outputs of  the object detector [NN] that we fine-tuned on the Visual Relationship Detection dataset [NN]
For each image, we keep  the object candidates whose confidence scores is above 0.N  among the top N00 detections
Non-maximum suppression  with threshold 0.N is applied to handle multiple detections
 This results in an average of NN object detections per image,  i.e
around N00 pairs of boxes
 N
Experiments  In this section, we evaluate the performance of our model  on two datasets for different evaluation setups
First, we  evaluate our new visual representation for relations on the  Visual Relationship Detection dataset [NN]
We show results  with our weakly-supervised model learned from imagelevel supervision and present large improvements over the  state of the art for detecting unseen triplets (zero-shot detection)
Second, we evaluate our model for the task of unusual  triplets retrieval and localization on our new UnRel dataset
 N.N
Recall on Visual Relationship Detection dataset  Dataset
We evaluate our method on the Visual Relationship Detection dataset [NN] following the original experimental setup
This dataset contains N000 training and N000  test images with ground truth annotations for relations between pairs of objects
Due to the specific train/test split  provided by [NN], N0% of test triplets are not seen at training and allow for evaluation of zero-shot learning
Some  of these triplets are rare in the linguistic and visual world  (e.g
“laptop on stove”), but most of them are only infrequent in the training set or have not been annotated (e.g
 “van on the left of car”)
Around N0K triplets are annotated  in the training set, with an average of N.N relations per image
The dataset contains N00 objects and N0 predicates, i.e
 N00 × N00 × N0 possible triplets
However there are only NNNN different annotated triplets
 Evaluation set-up
Following [NN], we compute  recall@x which corresponds to the proportion of ground  truth pairs among the x top scored candidate pairs in each  image
We use the scores returned by (N) to sort the candidate pairs of boxes
The evaluation is reported for three  setups
In predicate detection, candidate pairs of boxes  are ground truth boxes, and the evaluation only focuses on  the quality of the predicate classifier
In the other two more  realistic setups, the subject and object confidence scores are  provided by an object detector and we also check whether  the candidate boxes intersect the ground truth boxes : either  both subject and object boxes should match (relationship  detection), or the union of them should match (phrase  detection)
For a fair comparison with [NN], we report  results using the same set of R-CNN [NN] object detections  as them
The localization is evaluated with IoU = 0.N
 Benefits of our visual representation
We first evaluate  the quality of our visual representation in a fully supervised  setup where the ground truth spatial localization for each  relation is known, i.e
we know which objects in the image are involved in each relation at training time
For this,  we solve the multi-label ridge regression in (N)
Training  with one-vs-rest SVMs gives similar results
We compare  three types of features described in Section N.N in Table N:  [S] the spatial representation (f.), [A] the appearance representation (g.) and [S+A] the concatenation of the two (h.)
 We compare with the Visual Phrases model [NN] and several  variants of [NN] N : Visual model alone (b.), Language (likelihood of a relationship) (c.), combined Visual+Language  model (d.)
In row (e.) we also report the performance of the  full language model of [NN], that scores the candidate pairs  of boxes based on their predicted object categories, that we  computed using the model and word embeddings provided  by the authors
Because their language model is orthogonal to our visual model, we can combine them together (i.)
 The results are presented on the complete test set (column  All) and on the zero-shot learning split (column Unseen)
 Table N shows that our combined visual features [S+A] improve over the visual features of [NN] by N0% on the task of predicate detection and more than N0% on the hardest task of relationship detection
Furthermore, our purely visual  features without any use of language (h.) reach comparable performance to the combined Visual+Language features  of [NN] and reach state-of-the-art performance (i.) when  combined with the language scores of [NN]
The good performance of our spatial features [S] alone (f.) confirms the  observation we made in Figure N that our spatial clusters  group pairs of objects in similar relations
That could partly  explain why the visual model of [NN] has low performance
 NWhen running the evaluation code of [NN], we found slighlty better  performance than what is reported in their paper
See appendix [N] for  more details
 NNNN    Predicate Det
Phrase Det
Relationship Det
 All Unseen All Unseen All Unseen  Full sup
 a
Visual Phrases [NN] 0.N - 0.0N - - b
Visual [NN] N.N N.N N.N N.0 N.N 0.N  c
Language (likelihood) [NN] NN.N N.N 0.0N 0.00 0.0N 0.00  d
Visual + Language [NN] NN.N N.N NN.N N.N NN.N N.N  e
Language (full) [NN] NN.N NN.N NN.N N.N NN.N N.N  f
Ours [S] NN.N NN.N NN.N N.N NN.N N.0  g
Ours [A] NN.N NN.N NN.N N.N NN.N N.0  h
Ours [S+A] N0.N NN.N NN.N N.N NN.N N.N  i
Ours [S+A] + Language [NN] NN.N NN.N NN.N N.N NN.N N.N  Weak sup
 j
Ours [S+A] NN.N NN.0 NN.0 N.N NN.N N.N  k
Ours [S+A] - Noisy NN.N NN.N NN.N N.0 NN.N N.N  Table N: Results on Visual Relationship Detection dataset [NN] for R@N0
See appendix [N] for results with R@N00
 Their model learns a classifier only based on the appearance  of the union of the two object boxes and lacks information  about their spatial configuration
 Weak supervision
We evaluate our weakly-supervised  classifiers W learned on image-level labels as described in Section N.N
We use the ground truth annotations of the Visual Relationship Detection dataset as image-level labels
 We report the results using our combined spatial and appearance features (j.) in Table N
We see that when switching to weak supervision the recall@N0 only drops from  N0.N% to NN.N% for predicate detection and has limited in- fluence on the other tasks
This is an interesting result as  it suggests that, given pre-trained object detectors, weak  image-level annotations are enough to learn good classifiers  for relations
To investigate this further we have also tried  to learn relation classifiers directly from noisy image-level  labels without inferring at training time which objects participate in which relation
For each triplet t = (s, r, o) in an image containing candidate pairs of boxes (os,oo) we ran- domly select one of the pairs as being in relation r and dis- card the other object pairs
This is equivalent to training in  a fully-supervised setup but with noisy labels
The performance obtained by this classifier (k.) is below our weaklysupervised learning set-up but is surprisingly high
We believe that this is related to a particular bias present in the  Visual Relationship Detection dataset [NN], which contains  many images with only two prominent objects involved in a  specific relation (more than half of the triplets fall into this  category)
To underline the ability of the weakly-supervised  model to disambiguate the correct bounding boxes, we evaluate in a more difficult setup where we replace the candidate  test pairs of [NN] by all candidate pairs formed by objects  of confidence scores above 0.N
This multiplies by N the  number of candidate pairs, resulting in an increased level  of ambiguity
In this more challenging setup, our approach  obtains a recall@N0 for Phrase Detection (resp
Relationship Detection) of NN.N% (resp
NN.0%) compared to the  ”Ours [S+A] Noisy” baseline which drops to NN.N% (resp
 N0.N%)
 Unseen triplets
Following [NN] we report results on the  “zero-shot split” of the test set containing only the test  triplets not seen in training
Results for both of our fullysupervised and weakly-supervised methods are shown in  Table N (column Unseen)
Interestingly, our fully supervised model almost triples the performance on the unseen  triplets compared to the Visual+Language model of [NN]
 Even using weak supervision, our recall of NN.0% is signifi- cantly better than their fully supervised method
We believe  that this improvement is due to the strength of our visual  features that generalize well to unseen triplets
 Figure N shows examples of predictions of both seen and  unseen triplets (last row) by our model [S+A] trained with  weak-supervision
We note that many of the misclassified  relations are in fact due to missing annotations in the dataset  (yellow column)
First, not all pairs of objects in the image are labeled; second, the pairs that are labeled are not  labelled exhaustively, i.e
“person riding horse” can be labelled as “person on horse” and predicting “riding” for this  pair of objects is considered as an error
Not having exhaustive annotation per object pair is therefore an issue as  predicates are not necessary mutually exclusive
We tackle  this problem in the next section by introducing a new exhaustively labeled dataset that enables retrieval evaluation
 Our real errors (red column) are mostly due to two reasons:  either the spatial configuration is challenging (e.g.“person  on table”), or the spatial configuration is roughly correct  but the output predicate is incorrect (e.g
“van has car” has  similar configuration to ”person has bag”)
 N.N
Retrieval of rare relations on UnRel Dataset  Dataset
To address the problem of missing annotations,  we introduce a new challenging dataset of unusual relations,  UnRel, that contains images collected from the web with  NNNN    correctly recognized relations missing ground truth incorrectly recognized  se en  tr ip  le ts  GT: on, has  [NN]: ride  GT: under  [NN]: on  GT: above  [NN]: cover  GT: none  [NN]: behind  GT: next to, behind  [NN]: on  se en  tr ip  le ts  GT: has  [NN]: hold  GT: behind  [NN]: behind  GT: in the front of  [NN]: behind  GT: on the right of  [NN]: next to  GT: none  [NN]: has  u n  se en  tr ip  le ts  GT: above  [NN]: in the front of  GT: on  [NN]: above  GT: has, wear  [NN]: wear  GT: against  [NN]: next to  GT: behind, left of  [NN]: behind  Figure N: Relationship detections on the test set of [NN]
We show examples among the top scored triplets detected for each  relation by our weakly-supervised model described in N.N
The triplet is correctly recognized if both the object detections and  the relation match ground truth (in green), else the triplet is incorrect (in red)
We also show examples of correctly predicted  relations where the ground truth is erroneous : either missing or incomplete (in yellow)
The last row shows zero-shot triplets  that are not in the training set
See the appendix [N] for additional qualitative results
 unusual language triplet queries such as “person ride giraffe”
We exhaustively annotate these images at box-level  for the given triplet queries
UnRel dataset has three main  advantages
First, it is now possible to evaluate retrieval and  localization of triplet queries in a clean setup without problems posed by missing annotations
Second, as the triplet  queries of UnRel are rare (and thus likely not seen at training), it enables evaluating the generalization performance of  the algorithm
Third, other datasets can be easily added to  act as confusers to further increase the difficulty of the retrieval set-up
Currently, UnRel dataset contains more than  N000 images queried with NN triplet queries
 Setup
We use our UnRel dataset as a set of positive pairs  to be retrieved among all the test pairs of the Visual Relationship Dataset
We evaluate retrieval and localization  with mean average precision (mAP) over triplet queries t = (s, r, o) of UnRel in two different setups
In the first setup (with GT) we rank the manually provided ground truth  pairs of boxes (os,oo) according to their predicate scores vrel((os,oo) | r) to evaluate relation prediction without the difficulty of object detection
In the second setup (with  candidates) we rank candidate pairs of boxes (os,oo) pro- vided by the object detector according to predicate scores  vrel((os,oo) | r)
For this second setup we also evaluate the accuracy of localization : a candidate pair of boxes is  positive if its IoU with one ground truth pair is above 0.N
 We compute different localization metrics : mAP–subj  computes the overlap of the predicted subject box with the  ground truth subject box, mAP–union computes the over- lap of the predicted union of subject and object box with  the union of ground truth boxes and mAP–subj/obj com- putes the overlap of both the subject and object boxes with  their respective ground truth
Like in the previous section,  we form candidate pairs of boxes by taking the top-scored  object detections given by [NN]
We keep at most N00 candidate objects per image, and retain at most N00 candidate  pairs per image
For this retrieval task where it is important  to discriminate the positive from negative pairs, we found  it is important to learn an additional “no relation” class by  adding an extra column to W in (N)
The negative pairs are sampled at random among the candidates that do not match  the image-level annotations
 Results
Retrieval results are shown in Table N
Our classifiers are trained on the training subset of the Visual Relationship Dataset
We compare with two strong baselines
The first baseline is our implementation of [NN] (their  trained models are not available online)
For this, we trained  a classifier [NN] to output predicates given visual features  extracted from the union of subject and object bounding  boxes
We do not use the language model as its score does  not affect the retrieval results (only adding a constant offset to all retrieved images)
We verified our implementation  on the Visual Relationship Dataset where results of [NN] are  available
As the second baseline, we use the DenseCap  NNNN    bike above person building has wheel cat wear tie person above bed cone on top of person  to p  N to  p N  to p  N  Figure N: Top N retrieved pairs of boxes for a set of UnRel triplet queries (first line is best) with our weakly-supervised  model
The pair is marked as positive (green) if the candidate subject and object boxes coincide with a ground truth subject  and object boxes with IoU ≥ 0.N
We provide more qualitative results in appendix [N]
 [NN] model to generate region candidates for each image  and sort them according to the score of the given triplet  query
Note that this is a strong baseline as we use the  pre-trained model released by the authors which has been  trained on NNK images of [NN] in a fully supervised manner using localized language descriptions, compared to our  model trained on only NK training images of [NN]
DenseCap outputs only a single bounding box (not a pair of boxes)  but we interpret its output as either a subject box or a union  of boxes
We cannot compare with the Visual Phrases [NN]  approach as it requires training data for each triplet, which  is not available for these rare queries
We report the chance  as the performance given by random ordering of the proposals
Results in Table N show significant improvements of our  method over the baselines
Note that our weakly-supervised  method outperforms these strong baselines that are fully supervised
This confirms our results from the previous section that (i) our visual features are well suited to model relations, (ii) they generalize well to unseen triplets, and (iii)  training from weak image-level supervision is possible and  results only in a small loss of accuracy compared to training  from fully supervised data
Examples of retrieved unusual  relations are shown in Figure N
 N
Conclusion  We have developed a new powerful visual descriptor for  representing object relations in images achieving state-ofthe-art performance on the Visual Relationship Detection  dataset [NN], and in particular significantly improving the  current results on unseen object relations
We have also deWith GT With candidates  - union subj subj/obj  Chance NN.N N.N N.N N.N  Full sup
 DenseCap [NN] - N.N N.N Reproduce [NN] N0.N NN.0 N0.0 N.N  Ours [S+A] NN.N NN.N NN.N N.N  Weak sup
 Ours [S+A] NN.N NN.N NN.0 N.N  Ours [S+A] - Noisy NN.0 NN.0 N0.N N.N  Table N: Retrieval on UnRel (mAP) with IoU=0.N  veloped a weakly-supervised model for learning object relations and have demonstrated that, given pre-trained object  detectors, object relations can be learnt from weak imagelevel annotations without a significant loss of recognition  performance
Finally, we introduced, UnRel, a new evaluation dataset for visual relation detection that enables to  evaluate retrieval without missing annotations and assess  generalization to unseen triplets
Our work opens-up the  possibility of learning a large vocabulary of visual relations  directly from large-scale Internet collections annotated with  image-level natural language captions
 Acknowledgements
This work was partly supported by ERC grants Activia (no
N0NNNN), LEAP (no
NNNNNN) and Allegro  (no
NN0NNN), CIFAR Learning in Machines & Brains program  and ESIF, OP Research, development and education Project IMPACT No
CZ.0N.N.0N/0.0/0.0/NN 00N/0000NNN
 NNNN    References  [N] Supplementary material (appendix) for the paper
http:  //arxiv.org/abs/NN0N.0NNNN
N, N, N, N  [N] J
Andreas, M
Rohrbach, T
Darrell, and D
Klein
Neural  module networks
In CVPR, N0NN
N  [N] F
R
Bach and Z
Harchaoui
Diffrac: a discriminative and  flexible framework for clustering
In NIPS, N00N
N, N, N  [N] H
Bilen and A
Vedaldi
Weakly supervised deep detection  networks
In CVPR, N0NN
N  [N] P
Bojanowski, R
Lajugie, F
Bach, I
Laptev, J
Ponce,  C
Schmid, and J
Sivic
Weakly supervised action labeling  in videos under ordering constraints
In ECCV, N0NN
N  [N] A
Chang, W
Monroe, M
Savva, C
Potts, and C
D
Manning
Text to Nd scene generation with rich lexical grounding
 ACL, N0NN
N  [N] X
Chen, A
Shrivastava, and A
Gupta
Neil: Extracting  visual knowledge from web data
In ICCV, N0NN
N  [N] M
J
Choi, A
Torralba, and A
S
Willsky
Context models and out-of-context objects
Pattern Recognition Letters,  N0NN
N  [N] V
Delaitre, J
Sivic, and I
Laptev
Learning person-object  interactions for action recognition in still images
In NIPS,  N0NN
N  [N0] C
Desai, D
Ramanan, and C
Fowlkes
Discriminative models for static human-object interactions
In CVPR Workshops,  N0N0
N  [NN] M
Elhoseiny, S
Cohen, W
Chang, B
Price, and A
Elgammal
Sherlock: Scalable fact learning in images
AAAI, N0NN
 N  [NN] H
Fang, S
Gupta, F
N
Iandola, R
Srivastava, L
Deng,  P
Dollár, J
Gao, X
He, M
Mitchell, J
C
Platt, C
L
Zitnick, and G
Zweig
From captions to visual concepts and  back
In CVPR, N0NN
N  [NN] A
Frome, G
S
Corrado, J
Shlens, S
Bengio, J
Dean, M
A
 Ranzato, and T
Mikolov
Devise: A deep visual-semantic  embedding model
In NIPS
N0NN
N  [NN] C
Galleguillos, A
Rabinovich, and S
Belongie
Object categorization using co-occurrence, location and appearance
In  CVPR, N00N
N  [NN] R
Girshick
Fast R-CNN
In ICCV, N0NN
N, N, N  [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
N  [NN] A
Gupta and L
S
Davis
Beyond nouns: Exploiting prepositions and comparative adjectives for learning visual classifiers
In ECCV, N00N
N  [NN] A
Gupta, A
Kembhavi, and L
S
Davis
Observing humanobject interactions: Using spatial and functional compatibility for recognition
PAMI, N00N
N  [NN] L
A
Hendricks, S
Venugopalan, M
Rohrbach, R
Mooney,  K
Saenko, and T
Darrell
Deep compositional captioning:  Describing novel object categories without paired training  data
In CVPR, N0NN
N  [N0] R
Hu, H
Xu, M
Rohrbach, J
Feng, K
Saenko, and T
Darrell
Natural language object retrieval
CVPR, N0NN
N  [NN] R
Jenatton, N
L
Roux, A
Bordes, and G
R
Obozinski
A  latent factor model for highly multi-relational data
In NIPS,  N0NN
N  [NN] J
Johnson, A
Karpathy, and L
Fei-Fei
Densecap: Fully  convolutional localization networks for dense captioning
In  CVPR, N0NN
N, N, N  [NN] J
Johnson, R
Krishna, M
Stark, L.-J
Li, D
A
Shamma,  M
S
Bernstein, and L
Fei-Fei
Image retrieval using scene  graphs
In CVPR, N0NN
N  [NN] A
Joulin, K
Tang, and L
Fei-Fei
Efficient image and video  co-localization with frank-wolfe algorithm
In ECCV, N0NN
 N  [NN] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, N0NN
 N  [NN] A
Karpathy, A
Joulin, and L
Fei-Fei
Deep fragment embeddings for bidirectional image sentence mapping
In NIPS,  N0NN
N  [NN] S
Kazemzadeh, V
Ordonez, M
Matten, and T
L
Berg
 Referitgame: Referring to objects in photographs of natural  scenes
In EMNLP, N0NN
N  [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, M
Bernstein, and L
Fei-Fei
Visual genome: Connecting language  and vision using crowdsourced dense image annotations
In  IJCV, N0NN
N, N, N  [NN] A
Lazaridou, E
Bruni, and M
Baroni
Is this a wampimuk?  cross-modal mapping between distributional semantics and  the visual world
In ACL, N0NN
N  [N0] C
Li, D
Parikh, and T
Chen
Automatic discovery of groups  of objects for scene understanding
In CVPR, N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft COCO: Common objects in context
In ECCV, N0NN
N  [NN] C
Lu, R
Krishna, M
Bernstein, and L
Fei-Fei
Visual relationship detection with language priors
In ECCV, N0NN
N,  N, N, N, N, N, N  [NN] J
Mao, J
Huang, A
Toshev, O
Camburu, A
Yuille, and  K
Murphy
Generation and comprehension of unambiguous  object descriptions
CVPR, N0NN
N  [NN] A
Miech, J.-B
Alayrac, P
Bojanowski, I
Laptev, and  J
Sivic
Learning from video and text via large-scale discriminative clustering
ICCV, N0NN
N  [NN] D
Movshovitz-Attias and W
W
Cohen
Kb-lda: Jointly  learning a knowledge base of hierarchy, relations, and facts
 In ACL, N0NN
N  [NN] M
Oquab, L
Bottou, I
Laptev, and J
Sivic
Is object localization for free? weakly-supervised learning with convolutional neural networks
In CVPR, N0NN
N  [NN] A
Osokin, J.-B
Alayrac, I
Lukasewitz, P
K
Dokania, and  S
Lacoste-Julien
Minding the gaps for block Frank-Wolfe  optimization of structured SVMs
In ICML, N0NN
N  [NN] B
A
Plummer, L
Wang, C
M
Cervantes, J
C
Caicedo,  J
Hockenmaier, and S
Lazebnik
FlickrN0k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models
In ICCV, N0NN
N  [NN] A
Prest, C
Schmid, and V
Ferrari
Weakly supervised learning of interactions between humans and objects
PAMI, N0NN
 N  [N0] V
Ramanathan, C
Li, J
Deng, W
Han, Z
Li, K
Gu,  Y
Song, S
Bengio, C
Rossenberg, and L
Fei-Fei
Learning  NNNN  http://arxiv.org/abs/NN0N.0NNNN http://arxiv.org/abs/NN0N.0NNNN   semantic relationships for better action retrieval in images
 In CVPR, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN: Towards real-time object detection with region proposal networks
In NIPS
N0NN
N, N  [NN] A
Rohrbach, M
Rohrbach, R
Hu, T
Darrell, and  B
Schiele
Grounding of textual phrases in images by reconstruction
ECCV, N0NN
N  [NN] F
Sadeghi, S
K
Divvala, and A
Farhadi
Viske: Visual  knowledge extraction and question answering by visual verification of relation phrases
In CVPR, N0NN
N  [NN] M
A
Sadeghi and A
Farhadi
Recognition using visual  phrases
In CVPR, N0NN
N, N, N, N, N  [NN] R
Socher, D
Chen, C
D
Manning, and A
Ng
Reasoning  with neural tensor networks for knowledge base completion
 In NIPS, N0NN
N  [NN] R
Socher, M
Ganjoo, C
D
Manning, and A
Ng
Zero-shot  learning through cross-modal transfer
In NIPS, N0NN
N  [NN] S
Venugopalan, L
A
Hendricks, M
Rohrbach, R
Mooney,  T
Darrell, and K
Saenko
Captioning images with diverse  objects
arXiv preprint arXiv:NN0N.0NNN0, N0NN
N  [NN] Y
Xian, Z
Akata, G
Sharma, Q
Nguyen, M
Hein, and  B
Schiele
Latent embeddings for zero-shot classification
 CVPR, N0NN
N  [NN] B
Yao and L
Fei-Fei
Grouplet: A structured image representation for recognizing human and object interactions
In  CVPR, N0N0
N  [N0] B
Yao and L
Fei-Fei
Modeling mutual context of object  and human pose in human-object interaction activities
In  CVPR, N0N0
N  [NN] B
Yao, X
Jiang, A
Khosla, A
L
Lin, L
Guibas, and L
FeiFei
Human action recognition by learning bases of action  attributes and parts
In ICCV, N0NN
N  [NN] M
Yatskar, V
Ordonez, and A
Farhadi
Stating the obvious: Extracting visual common sense knowledge
In NAACL,  N0NN
N  [NN] S
Zagoruyko, A
Lerer, T.-Y
Lin, P
O
Pinheiro, S
Gross,  S
Chintala, and P
Dollár
A multipath network for object  detection
BMVC, N0NN
N  [NN] Y
Zhu, A
Fathi, and L
Fei-Fei
Reasoning about object  affordances in a knowledge base representation
In ECCV,  N0NN
N  NNNNLearning ND Object Categories by Looking Around Them   Learning ND Object Categories by Looking Around Them  David NovotnyN,N Diane LarlusN Andrea VedaldiN  N Visual Geometry Group  Dept
of Engineering Science, University of Oxford  {david,vedaldi}@robots.ox.ac.uk  N Computer Vision Group  Naver Labs Europe  diane.larlus@naverlabs.com  Abstract  Traditional approaches for learning ND object categories use either synthetic data or manual supervision
In  this paper, we propose a method which does not require  manual annotations and is instead cued by observing objects from a moving vantage point
Our system builds on  two innovations: a Siamese viewpoint factorization network  that robustly aligns different videos together without explicitly comparing ND shapes; and a ND shape completion network that can extract the full shape of an object from partial  observations
We also demonstrate the benefits of configuring networks to perform probabilistic predictions as well as  of geometry-aware data augmentation schemes
We obtain  state-of-the-art results on publicly-available benchmarks
 N
Introduction  Despite their tremendous effectiveness in tasks such as  object category detection, most deep neural networks do not  understand the ND nature of object categories
Reasoning  about objects in ND is necessary in many applications, for  physical reasoning, or to understand the geometric relationships between different objects or scene elements
 The typical approach to learn ND objects is to make use  of large collections of high quality CAD models such as [N]  or [NN], which can be used to fully supervise models to recognize the objects’ viewpoint and ND shape
Alternatively,  one can start from standard image datasets such as PASCAL VOC [N], augmented with other types of supervision,  such as object segmentations and keypoint annotations [N]
 Whether synthetically generated or manually collected, annotations have so far been required in order to overcome  the significant challenges of learning ND object categories,  where both viewpoint and geometry are variable
 In this paper, we develop an alternative approach that  can learn ND object categories in an unsupervised manner  (fig
N), replacing synthetic or manual supervision with motion
Humans learn about the visual word by experiencing it  continuously, through a variable viewpoint, which provides  Figure N
We propose a convolutional neural network architecture  to learn the ND geometry of object categories from videos only,  without manual supervision
Once learned, the network can predict i) viewpoint, ii) depth, and iii) a point cloud, all from a single  image of a new object instance
 very strong cues on its ND structure
Our goal is to build  on such cues in order to learn the ND geometry of object  categories, using videos rather than images of objects
We  are motivated by the fact that videos are almost as cheap as  images to capture, and do not require annotations
 We build on mature structure-from-motion (SFM) technology to extract ND information from individual video sequences
However, these cues are specific to each object  instance as contained in different videos
The challenge is  to integrate this information in a global ND model of the object category, as well as to work with noisy and incomplete  reconstructions from SFM
 We propose a new deep architecture composed of three  modules (fig
N)
The first module estimates the absolute viewpoint of objects in all video sequences (sec
N.N)
 This aligns different object instances to a common reference  frame where geometric relationships can be modeled more  NNNNN    Figure N
Overview of our architecture
As a preprocessing, structure from motion (SFM) extracts egomotion and a depth map for every  frame
For training, our architecture takes pairs of frames ft, ft′ and produces a viewpoint estimate, a depth estimate, and a ND geometry  estimate
At test time, viewpoint, depth, and ND geometry are predicted from single images
 easily
The second estimates the ND shape of an object from  a given viewpoint, producing a depth map (sec
N.N)
The  third completes the depth map to a full ND reconstruction in  the globally-aligned reference frame (sec
N.N)
Combined  and trained end-to-end without supervision, from videos  alone, these components constitute VpDR-Net, a network  for viewpoint, depth and reconstruction, capable of extracting viewpoint and shape of a new object instance from a  single image
 One of our main contributions is thus to demonstrate the  utility of using motion cues in learning ND categories
We  also introduce two significant technical innovations in the  viewpoint and shape estimation modules as well as design  guidelines and training strategies for ND estimation tasks
 The first innovation (sec
N.N) is a new approach to  align video sequences of different ND objects based on a  Siamese viewpoint factorization network
While existing  methods [NN, NN] align shapes by looking at ND features, we  propose to train VpDR-Net to directly estimate the absolute  viewpoint of an object
We train our network to reconstruct  relative camera motions and we show that this implicitly  aligns different objects instances together
By avoiding explicit shape comparisons in ND space, this method is simpler  and more robust than alternatives
 The second innovation (sec
N.N) is a new network architecture that can generate a complete point cloud for the  object from a partial reconstruction obtained from monocular depth estimation
This is based on a shape representation  that predicts the support of a point probability distribution  in ND space, akin to a flexible voxelization, and a corresponding space occupancy map
 As a general design guideline, we demonstrate throughout the paper the utility of allowing deep networks to express uncertainty in their estimate by predicting probability distributions over outputs (sec
N), yielding more robust training and useful cues (such as separating foreground  and background in a depth map)
We also demonstrate the  significant power of geometry-aware data augmentation,  where a deep network is used to predict the geometry of an  image and the latter is used to generate new realistic views  to train other components of the system (sec
N)
Each component and design choice is thoroughly evaluated in sec
N,  with significant improvements over the state-of-the-art
 N
Related work  Viewpoint estimation
The vast majority of methods for  learning the viewpoint of object categories use manual supervision [NN, NN, NN, NN, NN, NN, NN] or synthetic [NN] data
 In [N0], a deep architecture predicts a relative camera pose  and depth for a pair of images
Only a few works have  used videos [NN, NN]
[NN] solves the shape alignment problem using a global search strategy based on the pairwise  alignment of point clouds, a step we avoid by means of our  Siamese viewpoint factorization network
 ND shape prediction
A traditional approach to ND reconstruction is to use handcrafted ND models [NN, NN], and more  recently ND CAD models [N, NN]
Often the idea is to search  for the ND model in a CAD library that best fits the image [NN, N, NN, N]
Alternatively, CAD models can be used  to train a network to directly predict the ND shape of an  object [N0, NN, NN, N]
Morphable models have sometimes  been used [NN, NN], particularly for modeling faces [N, N0]
 All these methods require ND models at train time
 Data-driven approaches for geometry
Structure from  motion (SFM) generally assumes fixed geometry between  views and is difficult to apply directly to object categories  due to intra-class variations
Starting from datasets of unordered images, methods such as [NN] and [NN] use SFM  and manual annotations, such as keypoints in [N, NN], to estimate a rough ND geometry of objects
Here, we leverage  motion cues and do not need extra annotations
 NNNN    N
Method  We propose a single Convolutional Neural Network  (CNN), VpDR-Net, that learns a ND object category by observing it from a variable viewpoint in videos and no supervision (fig
N)
Videos do not solve the problem of modeling  intra-class shape variations, but they provide powerful yet  noisy cues about the ND shape of individual objects
 VpDR-Net takes as an input a set of K video sequences SN, ..., SK of an object category (such as cars or chairs), where a video Si = (f i  N , ..., f i  Ni ) contains N i RGB or  RGBD frames f it ∈ RH×W×C (where C = N for RGB and C = N for RGBD data) and learns a model of the ND category
This model has three components: i) a predictor Φvp(f t i ) of the absolute viewpoint of the object (implicitly aligning the different object instances to a common  reference frame; sec
N.N), ii) a monocular depth predictor  Φdepth(f t i ) (sec
N.N) and iii) and a shape predictor Φpcl(f  t i )  that extends the depth map to a point cloud capturing the  complete shape of the object (sec
N.N)
Learning starts by  preprocessing videos to extract instance-specific egomotion  and shape information (sec
N.N)
 N.N
Sequence-specific structure and pose  Video sequences are pre-processed to extract from each  frame f it a tuple (K i t , g  i t, D  i t) consisting of: (i) the camera  calibration parameters Kit , (ii) its pose g i t ∈ SE(N), and  (iii) a depth map Dit ∈ RH×W associating a depth value to each pixel of f it 
The camera pose g  i t = (R  i t, T  i t ) consists of a rotation matrix Rit ∈ SO(N) and a translation vector T it ∈ RN.N We extract this information using off-the- shelf methods: the structure-from-motion (SFM) algorithm  COLMAP for RGB sequences [NN, NN], and an open-source  implementation [NN] of KinectFusion (KF) [NN] for RGBD  sequences
The information extracted from RGB or RGBD  data is qualitatively similar, except that the scale of SFM  reconstructions is arbitrary
 N.N
Intra-sequence alignment  Methods such as SFM or KF can reliably estimate camera pose and depth information for single objects and individual video sequences, but are not applicable to different  instances and sequences
In fact, their underlying assumption is that geometry is fixed, which is true for single (rigid)  objects, but false when the geometry and appearance differ  due to intra-class variations
 Learning ND object categories requires to relate their  variable ND shapes by identifying and putting in correspondence analogous geometric features, such as the object front  and rear
For rigid objects, such correspondences can be expressed by rigid transformations that align occurrences of  NWe use the convention that git transforms world-relative coordinates  pworld to camera-relative coordinates pcamera = g i tpworld
 analogous geometric features
 The most common approach for aligning ND shapes, also  adopted by [NN] for video sequences, is to extract and match  ND feature descriptors
Once objects in images or videos are  aligned, the data can be used to supervise other tasks, such  as learning a monocular predictor of the absolute viewpoint  of an object [NN]
 One of our main contributions, described below, is to reverse this process by learning a viewpoint predictor without explicitly matching ND shapes
Empirically (sec
N), we  show that, by skipping the intermediate ND analysis, our  method is often more effective and robust than alternatives
 Siamese network for viewpoint factorization
Geometric analogies between ND shapes can often be detected in  image space directly, based on visual similarity
Thus, we  propose to train a CNN Φvp that maps a single frame f i t to  its absolute viewpoint ĝit = Φvp(f i t ) in the globally-aligned  reference frame
We wish to learn this CNN from the viewpoints estimated by the algorithms of sec
N.N for each video  sequence
However, these estimated viewpoints are not absolute, but valid only within each sequence; formally, there  are unknown sequence-specific motions hi = (Ri, T i) ∈ SE(N) that map the sequence-specific camera poses git to global poses ĝit = g  i th  i.N  To address this issue, we propose to supervise the network using relative pose changes within each sequence,  which are invariant to the alignment transformation hi
For- mally, the transformation hi is eliminated by computing the relative pose change of the camera from frame t to frame t′:  ĝit′(ĝ i t)  −N = git′h i(hi)−N(git)  −N = git′(g i t)  −N
(N)  Expanding the expression with ĝit = (R̂ i t, T̂  i t ), we find equations expressing the relative rotation and translation  R̂it′(R̂ i t)  ⊤ = Rit′(R i t)  ⊤, (N)  T̂ it′ − R̂it′(R̂it)⊤T̂ it = T it′ −Rit′(Rit)⊤T it 
(N) Eqs
(N) and (N) are used to constrain the training of a  Siamese architecture, which, given two frames t and t′, evaluates the CNN twice to obtain estimates (R̂it, T̂  i t ) =  Φvp(f i t ) and (R̂  i t′ , T̂  i t′) = Φvp(f  i t′)
The estimated poses  are then compared to the ground truth ones, (Rit, T i t ) and  (Rit′ , T i t′), in a relative manner by using losses that enforce  the estimated poses to satisfy eqs
(N) and (N):  ℓR(R̂ i t, T̂  i t , R̂  i t′ , T̂  i t′)  · = ‖ ln R̂itt′(Ritt′)⊤‖F (N)  ℓT (R̂ i t, T̂  i t , R̂  i t′ , T̂  i t′)  · = ‖T̂ itt′ − T itt′‖N (N)  where ln is the principal matrix logarithm and  Rit′t · = Rit′(R  i t)  ⊤, R̂it′t · = R̂it′(R̂  i t)  ⊤,  T it′t · = T it′ −Rit′tT it , T̂ it′t  · = T̂ it′ − R̂it′tT̂ it 
 Nhi composes to the right: it transforms the world reference frame and  then moves it to the camera reference frame
 NNN0    While this CNN is only required to correctly predict relative  viewpoint changes within each sequence, since the same  CNN is used for all videos, the most plausible/regular solution for the network is to assign similar viewpoint predictions (R̂it, T̂ i t ) to images viewed from the same viewpoint,  leading to a globally consistent alignment of the input sequences
Furthermore, in a large family of ND objects, different ones (e.g
SUVs and sedans) tend to be mediated by  intermediate cases
This is shown empirically in sec
N
 Scale ambiguity in SFM
For methods such as SFM, there  is an additional ambiguity: reconstructions are known only  up to sequence-specific scaling factors λi > 0, so that the camera pose is parametrized as git(λ  i) = (Rit, λ iT it )
This  ambiguity leaves eq
(N) unchanged, but eq
(N) becomes:  T̂ it′ − R̂it′tT̂ it = λi(T it′ −Rit′tT it ) ⇒ T̂ it′t = λiT it′t During training, the ambiguity can be removed from loss (N)  by dividing vectors T it′t and T̂ i t′t by their Euclidean norm
 Note that for KF sequences λi = N
As the viewpoints are learned, an estimate of λ̂i is computed using a moving av- erage over training iterations for the other network modules  to use (see supplementary material for details)
 Probabilistic predictions
Due to intrinsic ambiguities in  the images or to errors in the SFM supervision (caused  for example by reflective or textureless surfaces), Φvp is occasionally unable to predict the ground truth viewpoint  accurately
We found beneficial to allow the network to  explicitly learn these cases and express uncertainty as an  additional input-dependent prediction
For the translation  component, we modify the network to predict the absolute  pose T̂ it as well as its confidence score σT̂ it (predicted as  the output of a soft ReLU units to ensure positivity)
We  then model the relative translation as a Gaussian distribution  with standard deviation σT = σT̂ i t′ + σT̂ it  and our model is  now learned by minimizing the negative log-likelihood LT which replaces the loss ℓT :  LT = − ln N  (NπσNT ) N  N  exp  (  −N N  ℓNT σNT  )  
(N)  The rotation component is more complex due to the nonEuclidean geometry of SO(N), but it was found sufficient to assume that the error term (N) has Laplace distribution  and optimize LR = − ln NCR exp (  − √ NℓR σR  )  , σR = σR̂i t′ +  σR̂it , where CR is a normalization term ensuring that the  probability distribution integrates to one
During training,  by optimizing the losses LR and LT instead of ℓR and ℓT , the network can discount gross errors by dividing the losses  by a large predicted variance
 Architecture
The architecture of Φvp is a variant of ResNet-N0 [NN] with some modifications to improve its performance as viewpoint predictor
The lower layers of Φvp  Figure N
Data augmentation
Training samples generated leveraging monocular depth estimation (ours, top) and using depth from  KF (baseline, bottom)
Missing pixels due to missing depth in red
 are used to extract a multiscale intermediate representation  (denoted HC for hypercolumn [NN] in fig
N)
The upper  layers consist of N × N downsampling residual blocks that predict the viewpoint (see supp
material for details)
 N.N
Depth prediction  The depth predictor module Φdepth of VpDR-Net takes individual frames f it and outputs a corresponding depth map D̂t = Φdepth(f  i t ), performing monocular depth estimation
 Estimating depth from a single image is inherently ambiguous and requires comparing the image to internal priors  of the object shape
Similar to pose, we allow the network  to explicitly learn and express uncertainty about depth estimates by predicting a posterior distribution over possible  pixel depths
For robustness to outliers from COLMAP and  KF, we assume a Laplace distribution with negative loglikelihood loss  LD = WH ∑  j=N  − ln √ N  Nσ̂dj exp  (  − √ N |dj − λ̂i  −N d̂j |  σ̂dj  )  , (N)  where dj is the noisy ground truth depth output by SFM  or KF for a given pixel j, and d̂j and σ̂dj are respectively the corresponding predicted depth mean and standard deviation
The relative scale λ̂i is N for KF and is estimated as explained in sec
N.N for SFM
 N.N
Point-cloud completion  Given any image f of an object instance, its aligned ND shape can be reconstructed by estimating and aligning its  depth map using the output of the viewpoint and depth predictors of sec
N.N and N.N
However, since a depth map  cannot represent the occluded portions of the object, such  a reconstruction can only be partial
In this section, we describe the third and last component of VpDR-Net, whose  goal is to generate a full reconstruction of the object, beyond what is visible in the given view
 Partial point cloud
The first step is to convert the predicted depth map D̂f = Φdepth(f) into a partial point cloud  P̂f · = {p̂j : j = N, 


, HW}, p̂j ·= K−N  [  uj vj d̂i ]⊤  , where (uj , vj) are the coordinates of a pixel j in the depth  map D̂f and K is the camera calibration matrix
Empir- ically, we have found that the reconstruction problem is  NNNN    Figure N
Viewpoint prediction
Most confident viewpoint predictions (sorted by predicted confidence from left to right) where the  viewpoint predicted by VpDR-Net is used to align the PascalND ground truth CAD models with each image
 much easier if the data is aligned in the global reference  frame established by VpDR-Net
Thus, we transform P̂f into a globally-aligned point cloud as P̂Gf = ĝ  −NP̂f , where ĝ = Φvp(f) is the camera pose estimated by the viewpoint- prediction network
 Point cloud completion network
Next, our goal is to learn  the point cloud completion part of our network Φpcl that  takes the aligned but incomplete point could P̂Gf and produces a complete object reconstruction Ĉ
We do so by pre- dicting a ND occupancy probability field
However, rather  than using a volumetric method that may require a discrete  and fixed voxelization of space, we propose a simple and  efficient alternative
First, the network Φpcl predicts a set  of M ND points Ŝ = (ŝN, 


, ŝM ) ∈ RN×M that, during training, closely fit the ground truth ND point cloud C
This step minimizes the fitting error:  ℓpcl(Ŝ) = N  |C| ∑  c∈C min  m=N,...,M ‖c− ŝm‖N 
(N)  The ND point cloud Ŝ provides a good coverage of the ground truth object shape
However, this point cloud is conservative and distributed in the vicinity of the ground truth  object
Thus, while this is not a precise representation of  the object shape, it works well as a support of a probability  distribution of space occupancy
In order to estimate the occupancy probability values, the network Φpcl(P̂ G f ) predicts  additional scalar outputs  δm = |{c ∈ C : ∀m′ : ‖ŝm − c‖N ≤ ‖ŝm′ − c‖N}|/|C|  proportional to the number of ground truth surface points  c ∈ C for which the support point ŝm is the nearest neigh- bor
The network is trained to compute a prediction δ̂m of the occupancy masses δm by minimizing the squared error loss ℓδ(δ̂, δ) =  ∑M m=N(δ̂m − δm)N
 Given the network prediction (Ŝ, δ̂) = Φpcl(P̂ G f ), the  completed point cloud is then defined as the subset of points  Ĉ that have sufficiently high occupancy, defined as: Ĉτ = {ŝm ∈ Ŝ : δm ≥ τ} where τ is a confidence parameter
The  set Ĉτ can be further refined by using e.g
a ND Laplacian filter to smooth out noise
 Architecture
The point cloud completion network Φpcl is modeled after PointNet [NN], originally proposed to semantically segment a point clouds
Here we adapt it to perform  a completely different task, namely ND shape reconstruction
This is made possible by our model where shape is  represented as a cloud of ND support points Ŝ and their oc- cupancy masses δ̂
Differently from Φvp and Φdepth, Φpcl is not convolutional but uses a sequence of fully connected  layers to process the ND points in P̂Gf , after appending an appearance descriptor to each of them
A key step is to add  an intermediate orderless pooling operator to remove the  dependency on the order and number of input points (see  the supplementary material for details)
The architecture is  configured to predict M = N0N points Ŝ
 Leave out
During training the incomplete point cloud P̂Gf is downsampled by randomly selecting between M = N0N  and N0N points based on their depth prediction confidence as estimated by Φdepth
Similar to dropout, dropping points allows the network to overfit less, to become less sensitive  to the size of the input point cloud, and to implicitly discard  background points (as these are assigned low confidence by  depth prediction)
For the latter reason, leave out is maintained at test time too with M = N0N
 N
Geometry-aware data augmentation  As viewpoint prediction with deep networks benefits significantly from large training sets [NN], we increase the effective size of the training videos by data augmentation
 This is trivial for tasks such as classification, where one can  translate or scale an image without changing its identity
 The same is true for viewpoint recognition if the task is to  only estimate the viewpoint orientation as in [NN, NN], as  images can be scaled and translated without changing the  equivalent viewpoint orientation
However, this assumption  is not satisfied if, as in our case, the goal is to estimate all N  DoF of the camera pose
 Inspired by the approach of [NN], we propose to solve  NNNN    object class test set level of supervision method ↓ eR ↓ eC ↓ erelR ↓ e rel T ↑ APeR ↑ APeC  car PascalND  unsupervised VPNet + aligned FrC [NN] NN.NN NN.NN NN.NN 0.NN 0.NN 0.0N  unsupervised VpDR-Net + FrC (ours) NN.NN N.NN NN.N0 0.NN 0.NN 0.NN  fully supervised VPNet + PascalND NN.NN N.NN N0.NN 0.NN 0.NN 0.NN  chair  PascalND  unsupervised VPNet + aligned LDOS [NN] NN.NN NN.NN NN.0N 0.NN 0.0N 0.00  unsupervised VpDR-Net + LDOS (ours) NN.NN NN.NN NN.NN 0.NN 0.NN 0.NN  fully supervised VPNet + PascalND NN.NN N.NN NN.NN 0.NN 0.NN 0.NN  LDOS  unsupervised VPNet + aligned LDOS [NN] N0.NN 0.NN NN.N0 0.NN 0.N0 0.NN  unsupervised VpDR-Net + LDOS (ours) NN.NN 0.NN N0.N0 0.N0 0.N0 0.NN  fully supervised VPNet + PascalND NN.NN N.NN NN.NN 0.NN 0.NN 0.00  Table N
Viewpoint prediction
Angular error er and camera-center distance ec for absolute pose evaluation, and relative camera rotation  error erelR and translation error e rel T for relative pose evaluation
APeR and APeC evaluate absolute angular error and camera-center  distance of the pose predictions taking into account the associated estimate confidence values
VpDR-Net trained on video sequences, is  compared to VPNet trained on aligned video sequences and a fully-supervised VPNet
↑ (resp
↓) means larger (resp
lower) is better
 ↓ eR ↓ eC ↓ erelR ↓ e rel T ↑ APeR ↑ APeC  Test set: LDOS  VpDR-Net (ours) NN.NN 0.NN N0.N0 0.N0 0.N0 0.NN  VpDR-Net-NoProb NN.NN 0.NN NN.NN 0.NN 0.NN 0.0N  VpDR-Net-NoDepth NN.NN 0.NN NN.NN N.0N 0.0N 0.0N  VpDR-Net-NoAug NN.NN 0.NN NN.NN 0.NN 0.NN 0.NN  Test set: PascalND  VpDR-Net (ours) NN.NN NN.NN NN.NN 0.NN 0.NN 0.NN  VpDR-Net-NoProb NN.NN NN.0N NN.NN N.0N 0.0N 0.NN  VpDR-Net-NoDepth N0.NN NN.NN NN.NN N.NN 0.0N 0.NN  VpDR-Net-NoAug NN.NN NN.N0 NN.NN 0.NN 0.N0 0.NN  Table N
Viewpoint prediction
Different flavors of VpDR-Net  with removed components to evaluate their respective impact
 this problem by using the estimated scene geometry to generate new realistic viewpoints (fig
N)
Given a sample  (f it , g i t, D  i t), we apply a random perturbation to the viewpoint (with a forward bias to avoid unoccluding too many  pixels) and use depth-image-based rendering (DIBR) [NN]  to generate a new sample (f i∗, g i ∗, D  i ∗), warping both the image and the depth map
 Sometimes the depth map Dit from KF contains too many holes to yield satisfactory DIBR results (fig
N, bottom); we found preferable to use the depth D̂it = Φdepth(ft) estimated by the network which is less accurate but more  robust, containing almost no missing pixels (fig
N, top)
 N
Experiments  We assess viewpoint estimation in sec
N.N, depth prediction in sec
N.N, and point cloud prediction in sec
N.N
 Datasets
Throughout the experimental section, we consider three datasets for training and benchmarking our network: (N) FreiburgCars (FrC) [NN] which consists of RGB  video sequences with the camera circling around various  types of cars; (N) the Large Dataset of Object Scans  (LDOS) [N] containing RGBD sequences of man-made objects; and (N) PascalND [NN], a standard benchmark for pose  estimation [NN, NN]
 For viewpoint estimation, PascalND already contains  viewpoint annotations
For LDOS, experiments focus on  the chair class
In order to generate ground truth pose annotations for evaluation, we manually aligned ND reconstructions of N0 randomly-selected chair videos and used  N0 randomly-selected frames for each video as a test set
 For depth estimation, we evaluate on LDOS as it provides high quality depth maps one can use as ground truth
 For point cloud reconstruction, we use FrC and LDOS
 Ground truth point clouds for evaluation are obtained by  merging the SFM or RGBD depth maps from all frames of  a given test video sequence, sampling N·N0N points and post- processing those using a ND Laplacian filter
For FrC, five  videos were randomly selected and removed from the train  set, picking N0 random frames per video for evaluation
For  LDOS the pose estimation test frames are used
 Learning details
VpDR-Net is trained with stochastic gradient descent with a momentum of 0.000N and an initial  learning rate of N0−N
The weights of the losses were em- pirically set to achieve convergence on the training set
Better convergence was observed by training VpDR-Net in two  stages
First, Φdepth and Φvp were optimized jointly, lower- ing the learning rate tenfold when no further improvement  in the training losses was observed
Then, Φpcl is optimized after initializing the bias of its last layer, which corresponds  to an average point cloud of the object category, by randomly sampling points from the ground truth models
 N.N
Pose estimation  PascalND
First, we evaluate the VpDR-Net viewpoint predictor on the PascalND benchmark [NN]
Unlike previous  works [NN, NN] that focus on estimating the object/camera  viewpoint represented by a N DoF rotation matrix, we evaluate the full N DoF camera pose represented by the rotation  matrix R together with the translation vector T 
 In PascalND, the camera poses are expressed relatively  to the whole scenes instead of the objects themselves, so we  adjust the dataset annotations
We crop every object using  bounding box annotations after reshaping the box to a fixed  aspect ratio, and resize the crop to NN0 × NN0 pixels
The  NNNN    0 0.NN 0.N0 0.NN N  Ground truth depth rank  0  0.N  0.N  0.N  R M  S  0 0.NN 0.N0 0.NN N  Confidence rank  0  0.N  0.N  0.N  R M  S  VpDR-Net (ours) VpDR-Net-Rand BerHu-Net BerHu-Net-Rand  Figure N
Monocular depth prediction
Cumulative RMS depth  reconstruction error for the LDOS data, when pixels are ranked by  ground truth depth (left) and by confidence (right)
 Figure N
Monocular depth prediction
Top: input image; middle: predicted depth; bottom: predicted depth confidence
Depth  maps are filtered by removing low confidence pixels
 camera pose is adjusted to the cropped object using the PNP  algorithm to minimize the reprojection error between the  camera-projected vertices of the ground truth CAD model  and the original projection after cropping and resizing
 Absolute pose evaluation
We first evaluate absolute camera pose estimation using two standard measures: the angular error eR = N − N  N ‖ lnR∗R̂⊤‖F between the ground truth camera pose R∗ and the prediction R̂ [NN, NN], as well as the camera-center distance eC = ‖Ĉ − C∗‖N between the pre- dicted camera center Ĉ and the ground truth C∗
Following the common practice [NN, NN] we report median eR and eC over all pose predictions on each test set
 Note that, while object viewpoints in PascalND and our  method are internally consistent for a whole category, they  may still differ between them by an arbitrary global ND similarity transformation
Thus, as detailed in the supplementary material, the two sets of annotations are aligned by a  single global similarity TG before assessment
Relative pose evaluation
To assess methods with measures independent of TG we also evaluate: (N) the relative rotation error between pairs of ground truth relative camera  motions R∗tt′ and the corresponding predicted relative motions R̂tt′ given by e rel R = N  − N N ‖ lnR∗tt′R̂⊤tt′‖F and (N) the  normalized relative translation error erelT = ‖T̂tt′ − T ∗tt′‖N, where both T̂tt′ and T  ∗ tt′ are ℓN-normalized so the measure  is invariant to the scaling component of TG
We report the median errors over all possible image pairs in each test set
 Pose prediction confidence evaluation
A feature of our  model is to produce confidence scores with its viewpoint estimates
We evaluate the reliability of these scores by correlating them with viewpoint prediction accuracy
In order to  do so, predictions are divided into “accurate” and “inaccurate” by comparing their errors eR and eC to thresholds (set to eR =  π N  following [NN, NN] and eC = NN and 0.N for Pas- calND or LDOS respectively)
Predictions are then ranked  by decreasing confidence scores and the average precisions  APeR and APeC of the two ranked lists are computed
 Baselines
We compare our viewpoint predictor to a strong  baseline, called VPNet, trained using absolute viewpoint labels
VPNet is a ResNetN0 architecture [NN] with the final  softmax classifier replaced by a viewpoint estimation layer  that predicts the N DoF pose ĝit
Following [NN], rotation matrices are decomposed in Euler angles, each discretized  in NN equal bins
This network is trained to predict a softmax distribution over the angular bins and to regress a ND  vector corresponding to the camera translation T 
The aver- age softmax value across the three max-scoring Euler angles  is used as a prediction confidence score
 We test both an unsupervised and a fully-supervised variant of VPNet
VPNet-unsupervised is comparable to our  setting and is trained on the output of the global camera poses estimated from the videos by the state-of-the-art  sequence-alignment method of [NN]
In the fully-supervised  setting, VPNet is trained instead by using ground-truth  global camera poses provided by the PascalND training set
 Results
Table N compares VpDR-Net to the VPNet  baselines
First, we observe that our baseline VPNetunsupervised is very strong, as we report eR = NN.N er- ror for the full rotation matrix, while the original method  of [NN] reports an error of NN.N just for the azimuth component
Nevertheless, VpDR-Net outperforms VPNet in all  performance metrics except for a single case (eR for LDOS chairs)
Furthermore, the advantage is generally substantial,  and the unsupervised VpDR-Net reduces the gap with fullysupervised VPNet by N0 % or better in the vast majority of  the cases
This shows the advantage of the proposed viewpoint factorization method compared to aligning ND shapes  as in [NN]
Second, we observe that the confidence scores  estimated by VpDR-Net are significantly more correlated  with the accuracy of the predictions than the softmax scores  in VPNet, providing a reliable self-assessment mechanism
 The most confident viewpoint predictions of VpDR-Net are  shown in fig
N
 Ablation study
We evaluate the importance of the different components of VpDR-Net by turning them off and measuring performance on the chair class
In table N, VpDRNet-NoProb replaces the robust probabilistic losses LR and LT with their non-probabilistic counterparts ℓR and ℓT , and confidence predictions are replaced with random scores for  AP evaluation
VpDR-Net-NoDepth removes the depth  prediction and point cloud prediction branches during training, retaining only the Φvp subnetwork
VpDR-Net-NoAug does not use the data augmentation mechanism of sec
N
 We observe a significant performance drop when each of  NNNN    Figure N
Point cloud prediction
From a single input image of an unseen object instance (top row), VpDR-Net predicts the ND geometry  of that instance in the form of a ND point cloud (seen from two different angles, middle and bottom rows)
 Test set LDOS FrC  Metric ↑ mVIoU ↓ mDpcl ↑ mVIoU ↓ mDpcl  Aubry [N] 0.0N N.N0 0.NN 0.NN  VpDR-Net (ours) 0.NN 0.N0 0.NN 0.NN  VpDR-Net-Fuse (ours) 0.NN 0.NN 0.NN 0.NN  Table N
Point cloud prediction
Comparison between VpDRNet and the method of Aubry et al
[N]
 the components is removed
This confirms the importance  of all contributions in the network design
Interestingly, we  observe that the depth prediction branch Φdepth is crucial for pose estimation (e.g
-NN.NN eR on LDOS)
 N.N
Depth prediction  The monocular depth prediction module of VpDR-Net is  compared against three baselines: VpDR-Net-Rand uses  VpDR-Net to estimate depth but predicts random confidence scores
BerHu-Net is a variant of the state-of-theart depth prediction network from [NN] based on the same  Φdepth subnetwork as VpDR-Net (but dropping Φpcl and Φvp)
Following [NN], for training it uses the BerHu depth loss and a dropout layer, which allows it to produce a confidence score of the depth measurements at test time using the  sampling technique of [NN, N]
Finally, BerHu-Net-Rand is  the same network, but predicting random confidence scores
 Results
Fig
N (right) shows the cumulative root-meansquared (RMS) depth reconstruction error for LDOS after  sorting pixels by their confidence as estimated by the network
By fitting better to inlier pixels and giving up on  outliers, VpDR-Net produces a much better estimate than  alternatives for the vast majority of pixels
Furthermore, accuracy is well predicted by the confidence scores
Fig
N  (left) shows the cumulative RMS by depth, demonstrating  that accuracy is better for pixels closer to the camera, which  are more likely to be labeled with correct depth
Qualitative  results are shown in fig
N
 N.N
Point cloud prediction  We evaluate the point cloud completion module of  VpDR-Net by comparing ground truth point clouds C to the point clouds Ĉ predicted by Φpcl using: (N) the voxel intersection-over-union (VIoU) measure that computes the  Jaccard similarity between the volumetric representations  of Ĉ and C, and (N) the normalized point cloud distance of [N0]
We average these measures over the test set leading  to mVIoU and mDpcl (see supp
material for details)
VpDR-Net is compared against the approach of Aubry et  al
[N] using their code
[N] is a ND CAD model retrieval method which first trains a large number of exemplar models which, in our case, are represented by individual video frames with their corresponding ground truth  ND point clouds
Then, given a testing image, [N] detects  the object instance and retrieves the best matching model  from the database
We align the retrieved point cloud to  the object location in the testing image using the PNP algorithm
For VpDR-Net, we evaluate two flavors
The original VpDR-Net that predicts the point cloud Ĉ and VpDR- Net-Fuse which further merges Ĉ with the predicted partial depth map point cloud P̂ 
 Table N shows that our reconstructions are significantly  better on both metrics for both LDOS chairs and FrC cars
 Fusing the results with the original depth map produces a  denser point cloud estimate and marginally improves the results
Qualitative results are shown in fig
N
 N
Conclusion  We have demonstrated the power of motion cues in replacing manual annotations and synthetic data in learning  ND object categories
We have done so by proposing a single neural network that simultaneously performs monocular viewpoint estimation, depth estimation, and shape reconstruction
This network is based on two innovations, a  new image-based viewpoint factorization method and a new  probabilistic shape representation
The contribution of each  component was assessed against suitable baselines
 NNNN    References  [N] M
Aubry, D
Maturana, A
Efros, B
Russell, and J
Sivic
 Seeing Nd chairs: exemplar part-based Nd-Nd alignment using a large dataset of cad models
In Proc
CVPR, N0NN
N,  N  [N] A
Bansal, B
Russell, and A
Gupta
Marr Revisited: NDND model alignment via surface normal prediction
In Proc
 CVPR, N0NN
N  [N] V
Blanz and T
Vetter
Face recognition based on fitting a Nd  morphable model
PAMI, NN(N):N0NN–N0NN, N00N
N  [N] J
Carreira, S
Vicente, L
Agapito, and J
Batista
Lifting  object detection datasets into Nd
PAMI, NN(N):NNNN–NNNN,  N0NN
N, N  [N] A
X
Chang, T
A
Funkhouser, L
J
Guibas, P
Hanrahan,  Q
Huang, Z
Li, S
Savarese, M
Savva, S
Song, H
Su,  J
Xiao, L
Yi, and F
Yu
Shapenet: An information-rich Nd  model repository
CoRR, abs/NNNN.0N0NN, N0NN
N, N  [N] S
Choi, Q
Zhou, S
Miller, and V
Koltun
A large dataset  of object scans
CoRR, abs/NN0N.0NNNN, N0NN
N  [N] C
B
Choy, D
Xu, J
Gwak, K
Chen, and S
Savarese
NdrNnN: A unified approach for single and multi-view Nd object  reconstruction
In Proc
ECCV, N0NN
N  [N] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn,  and A
Zisserman
The pascal visual object classes (voc)  challenge
IJCV, NN(N):N0N–NNN, N0N0
N  [N] Y
Gal and Z
Ghahramani
Bayesian convolutional neural  networks with Bernoulli approximate variational inference
 In Proc
ICLR, N0NN
N  [N0] R
Girdhar, D
F
Fouhey, M
Rodriguez, and A
Gupta
 Learning a predictable and generative vector representation  for objects
In Proc
ECCV, N0NN
N  [NN] D
Glasner, M
Galun, S
Alpert, R
Basri, and  G
Shakhnarovich
Viewpoint-aware object detection and  pose estimation
In Proc
ICCV, N0NN
N  [NN] A
Gupta, A
Vedaldi, and A
Zisserman
Synthetic data for  text localisation in natural images
In Proc
CVPR, N0NN
N  [NN] S
Gupta, P
A
Arbeláez, R
B
Girshick, and J
Malik
Aligning ND models to RGB-D images of cluttered scenes
In  Proc
CVPR, N0NN
N  [NN] B
Hariharan, P
Arbeláez, R
Girshick, and J
Malik
Hypercolumns for object segmentation and fine-grained localization
In Proc
CVPR, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In Proc
CVPR, N0NN
N, N  [NN] A
Kar, S
Tulsiani, J
Carreira, and J
Malik
Categoryspecific object reconstruction from a single image
In Proc
 CVPR, N0NN
N  [NN] A
Kendall, V
Badrinarayanan, and R
Cipolla
Bayesian  segnet: Model uncertainty in deep convolutional encoderdecoder architectures for scene understanding
CoRR,  abs/NNNN.0NNN0, N0NN
N  [NN] I
Laina, C
Rupprecht, V
Belagiannis, F
Tombari, and  N
Navab
Deeper depth prediction with fully convolutional  residual networks
In NDV, N0NN
N  [NN] J
J
Lim, H
Pirsiavash, and A
Torralba
Parsing ikea objects: Fine pose estimation
In Proc
ICCV, N0NN
N  [N0] F
Liu, D
Zeng, Q
Zhao, and X
Liu
Joint face alignment  and Nd face reconstruction
In Proc
ECCV, N0NN
N  [NN] D
G
Lowe
Three-dimensional object recognition from single two-dimensional images
Artif
Intell., NN(N):NNN–NNN,  NNNN
N  [NN] Y
Y
Morvan
Acquisition, compression and rendering of  depth and texture for multi-view video
PhD thesis, Technische Universiteit Eindhoven, N00N
N  [NN] R
Mottaghi, Y
Xiang, and S
Savarese
A coarse-to-fine  model for Nd pose estimation and sub-category recognition
 In Proc
CVPR, N0NN
N  [NN] R
A
Newcombe, S
Izadi, O
Hilliges, D
Molyneaux,  D
Kim, A
J
Davison, P
Kohi, J
Shotton, S
Hodges, and  A
Fitzgibbon
Kinectfusion: Real-time dense surface mapping and tracking
In Proc
ISMAR, N0NN
N  [NN] M
Ozuysal, V
Lepetit, and P.Fua
Pose estimation for category specific multiview object localization
In Proc
CVPR,  N00N
N  [NN] B
Pepik, M
Stark, P
Gehler, and B
Schiele
Multi-view  priors for learning detectors from sparse viewpoint data
In  Proc
ICLR, N0NN
N  [NN] M
Prasad, A
Fitzgibbon, A
Zisserman, and L
V
Gool
 Finding nemo: Deformable object class modelling using  curve matching
In Proc
CVPR, N0N0
N  [NN] C
R
Qi, H
Su, K
Mo, and L
J
Guibas
Pointnet: Deep  learning on point sets for Nd classification and segmentation
 CoRR, abs/NNNN.00NNN, N0NN
N  [NN] L
G
Roberts
Machine perception of three-dimensional  solids
PhD thesis, Massachusetts Institute of Technology
 Dept
of Electrical Engineering, NNNN
N  [N0] J
Rock, T
Gupta, J
Thorsen, J
Gwak, D
Shin, and  D
Hoiem
Completing Nd object shape from one depth image
In Proc
CVPR, N0NN
N  [NN] R
B
Rusu and S
Cousins
ND is here: Point Cloud Library  (PCL)
In Proc
ICRA, N0NN
N  [NN] S
Savarese and L
Fei-Fei
Nd generic object categorization,  localization and pose estimation
In Proc
ICCV, N00N
N  [NN] J
L
Schönberger and J.-M
Frahm
Structure-from-motion  revisited
In Proc
CVPR, N0NN
N  [NN] J
L
Schönberger, E
Zheng, M
Pollefeys, and J.-M
Frahm
 Pixelwise view selection for unstructured multi-view stereo
 In Proc
ECCV, N0NN
N  [NN] N
Sedaghat and T
Brox
Unsupervised generation of a viewpoint annotated car dataset from videos
In Proc
ICCV,  N0NN
N, N, N, N  [NN] H
Su, C
R
Qi, Y
Li, and L
J
Guibas
Render for cnn:  Viewpoint estimation in images using cnns trained with rendered Nd model views
In Proc
ICCV, N0NN
N, N, N, N  [NN] i
Sun, H
Su, S
Savarese, and L
Fei-Fei
A multi-view  probabilistic model for Nd object classes
In Proc
CVPR,  N00N
N  [NN] M
Tatarchenko, A
Dosovitskiy, and T
Brox
Multi-view Nd  models from single images with a convolutional network
In  Proc
ECCV, N0NN
N  [NN] S
Tulsiani and J
Malik
Viewpoints and keypoints
In Proc
 CVPR, N0NN
N, N, N, N  NNNN    [N0] B
Ummenhofer, H
Zhou, J
Uhrig, N
Mayer, E
Ilg,  A
Dosovitskiy, and T
Brox
Demon: Depth and motion network for learning monocular stereo
CoRR, abs/NNNN.0NN0N,  N0NN
N  [NN] J
Wu, C
Zhang, T
Xue, W
T
Freeman, and J
B
Tenenbaum
Learning a probabilistic latent space of object shapes  via Nd generative-adversarial modeling
In Proc
NIPS, N0NN
 N  [NN] Y
Xiang, W
Kim, W
Chen, J
Ji, C
Choy, H
Su, R
Mottaghi, L
Guibas, and S
Savarese
ObjectnetNd: A large scale  database for Nd object recognition
In Proc
ECCV, N0NN
N  [NN] Y
Xiang, R
Mottaghi, and S
Savarese
Beyond pascal: A  benchmark for Nd object detection in the wild
In WACV,  N0NN
N, N  [NN] S
Zhu, L
Zhang, and B
M
Smith
Model evolution: An  incremental approach to non-rigid structure from motion
In  Proc
CVPR, N0N0
N  [NN] Z
Zia, M
Stark, B
Schiele, and K
Schindler
Detailed Nd  representations for object recognition and modeling
PAMI,  NN(NN):NN0N–NNNN, N0NN
N  NNNN