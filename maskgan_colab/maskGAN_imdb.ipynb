{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maskGAN_imdb.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZyejo0JiKGZ",
        "colab_type": "code",
        "outputId": "befbfe59-5766-457f-ceca-62294c5b3237",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "!git clone -b fixing_sample_gen https://github.com/Midroni/yesweGAN"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yesweGAN'...\n",
            "remote: Enumerating objects: 4, done.\u001b[K\n",
            "remote: Counting objects: 100% (4/4), done.\u001b[K\n",
            "remote: Compressing objects: 100% (4/4), done.\u001b[K\n",
            "remote: Total 574 (delta 0), reused 1 (delta 0), pack-reused 570\u001b[K\n",
            "Receiving objects: 100% (574/574), 388.56 MiB | 14.23 MiB/s, done.\n",
            "Resolving deltas: 100% (282/282), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOguCX-SiNFt",
        "colab_type": "code",
        "outputId": "f363a94f-9d23-47f7-c807-32293d454bf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 843
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.6.0\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/a2/38929ec9677cb0009837b77674388ab4a35ad81573f3289b21963eda0f9a/tensorflow_gpu-1.6.0-cp27-cp27mu-manylinux1_x86_64.whl (209.2MB)\n",
            "\u001b[K     |████████████████████████████████| 209.2MB 61kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (2.0.0)\n",
            "Collecting tensorboard<1.7.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/b8/7f64efd6aea9e21b836dc9acac60634ce9c41fe153ffd4df2acedc9a86e6/tensorboard-1.6.0-py2-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 34.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.0.post1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.1.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6.0) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==1.6.0) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.6.0) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.6.0) (5.4.0)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 50.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6.0) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.4.0->tensorflow-gpu==1.6.0) (44.0.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp27-none-any.whl size=107221 sha256=17e9d101f6557ae68b0bb1169112800893a8e0de8450371e6deb79f91294a756\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-2.1.0 tensorflow-gpu-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7f7G5dghg-r",
        "colab_type": "code",
        "outputId": "20668635-2037-4166-823b-b56f4173d08a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda=9.0.176-1"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-21 02:43:11--  https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 152.195.57.194\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|152.195.57.194|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?dVhdfdPND4A11XQY7u5s3AGWhdin2DAJS995sMsWs1ey6P4saJ_U-uo3fWblP4K9I-BKYSOeDJsiqmnwl3Gz_8wlgvv2hgSTLGumRqQJwfFPXBKLn8xgCnXKUfswL_YU0PmS9Wva3xCCjkCARErKI4iwOmwgts9BlqaZbhyvANnCPTDywxic6CDdDCT-y4EI0hsMwZvbivpNpzD4ckMq [following]\n",
            "--2020-03-21 02:43:12--  https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?dVhdfdPND4A11XQY7u5s3AGWhdin2DAJS995sMsWs1ey6P4saJ_U-uo3fWblP4K9I-BKYSOeDJsiqmnwl3Gz_8wlgvv2hgSTLGumRqQJwfFPXBKLn8xgCnXKUfswL_YU0PmS9Wva3xCCjkCARErKI4iwOmwgts9BlqaZbhyvANnCPTDywxic6CDdDCT-y4EI0hsMwZvbivpNpzD4ckMq\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 152.199.39.144\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|152.199.39.144|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1212738714 (1.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.13G   216MB/s    in 5.4s    \n",
            "\n",
            "2020-03-21 02:43:18 (216 MB/s) - ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’ saved [1212738714/1212738714]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-9-0-local.\n",
            "(Reading database ... 144542 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:4 file:/var/cuda-repo-9-0-local  Packages [15.4 kB]\n",
            "Ign:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:11 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Hit:13 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:14 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:15 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [832 kB]\n",
            "Get:17 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,784 kB]\n",
            "Get:18 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,151 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [857 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,361 kB]\n",
            "Get:23 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [861 kB]\n",
            "Fetched 7,117 kB in 7s (1,077 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "0 upgraded, 34 newly installed, 0 to remove and 25 not upgraded.\n",
            "Need to get 0 B/1,097 MB of archives.\n",
            "After this operation, 2,315 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-9-0-local  cuda-license-9-0 9.0.176-1 [22.0 kB]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  cuda-misc-headers-9-0 9.0.176-1 [684 kB]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  cuda-core-9-0 9.0.176-1 [16.9 MB]\n",
            "Get:4 file:/var/cuda-repo-9-0-local  cuda-cudart-9-0 9.0.176-1 [106 kB]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  cuda-driver-dev-9-0 9.0.176-1 [10.9 kB]\n",
            "Get:6 file:/var/cuda-repo-9-0-local  cuda-cudart-dev-9-0 9.0.176-1 [767 kB]\n",
            "Get:7 file:/var/cuda-repo-9-0-local  cuda-command-line-tools-9-0 9.0.176-1 [25.4 MB]\n",
            "Get:8 file:/var/cuda-repo-9-0-local  cuda-nvrtc-9-0 9.0.176-1 [6,348 kB]\n",
            "Get:9 file:/var/cuda-repo-9-0-local  cuda-nvrtc-dev-9-0 9.0.176-1 [9,334 B]\n",
            "Get:10 file:/var/cuda-repo-9-0-local  cuda-cusolver-9-0 9.0.176-1 [26.2 MB]\n",
            "Get:11 file:/var/cuda-repo-9-0-local  cuda-cusolver-dev-9-0 9.0.176-1 [5,317 kB]\n",
            "Get:12 file:/var/cuda-repo-9-0-local  cuda-cublas-9-0 9.0.176-1 [25.0 MB]\n",
            "Get:13 file:/var/cuda-repo-9-0-local  cuda-cublas-dev-9-0 9.0.176-1 [49.4 MB]\n",
            "Get:14 file:/var/cuda-repo-9-0-local  cuda-cufft-9-0 9.0.176-1 [84.1 MB]\n",
            "Get:15 file:/var/cuda-repo-9-0-local  cuda-cufft-dev-9-0 9.0.176-1 [73.7 MB]\n",
            "Get:16 file:/var/cuda-repo-9-0-local  cuda-curand-9-0 9.0.176-1 [38.8 MB]\n",
            "Get:17 file:/var/cuda-repo-9-0-local  cuda-curand-dev-9-0 9.0.176-1 [57.9 MB]\n",
            "Get:18 file:/var/cuda-repo-9-0-local  cuda-cusparse-9-0 9.0.176-1 [25.2 MB]\n",
            "Get:19 file:/var/cuda-repo-9-0-local  cuda-cusparse-dev-9-0 9.0.176-1 [25.3 MB]\n",
            "Get:20 file:/var/cuda-repo-9-0-local  cuda-npp-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:21 file:/var/cuda-repo-9-0-local  cuda-npp-dev-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:22 file:/var/cuda-repo-9-0-local  cuda-nvgraph-9-0 9.0.176-1 [6,081 kB]\n",
            "Get:23 file:/var/cuda-repo-9-0-local  cuda-nvgraph-dev-9-0 9.0.176-1 [5,658 kB]\n",
            "Get:24 file:/var/cuda-repo-9-0-local  cuda-samples-9-0 9.0.176-1 [75.9 MB]\n",
            "Get:25 file:/var/cuda-repo-9-0-local  cuda-documentation-9-0 9.0.176-1 [53.1 MB]\n",
            "Get:26 file:/var/cuda-repo-9-0-local  cuda-libraries-dev-9-0 9.0.176-1 [2,596 B]\n",
            "Get:27 file:/var/cuda-repo-9-0-local  cuda-nvml-dev-9-0 9.0.176-1 [47.6 kB]\n",
            "Get:28 file:/var/cuda-repo-9-0-local  cuda-visual-tools-9-0 9.0.176-1 [398 MB]\n",
            "Get:29 file:/var/cuda-repo-9-0-local  cuda-toolkit-9-0 9.0.176-1 [2,836 B]\n",
            "Get:30 file:/var/cuda-repo-9-0-local  cuda-libraries-9-0 9.0.176-1 [2,566 B]\n",
            "Get:31 file:/var/cuda-repo-9-0-local  cuda-runtime-9-0 9.0.176-1 [2,526 B]\n",
            "Get:32 file:/var/cuda-repo-9-0-local  cuda-demo-suite-9-0 9.0.176-1 [3,880 kB]\n",
            "Get:33 file:/var/cuda-repo-9-0-local  cuda-9-0 9.0.176-1 [2,552 B]\n",
            "Get:34 file:/var/cuda-repo-9-0-local  cuda 9.0.176-1 [2,504 B]\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-license-9-0.\n",
            "(Reading database ... 144601 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-license-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-9-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-core-9-0.\n",
            "Preparing to unpack .../02-cuda-core-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-core-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-9-0.\n",
            "Preparing to unpack .../03-cuda-cudart-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-9-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-9-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-9-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-9-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-9-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-9-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-9-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-9-0.\n",
            "Preparing to unpack .../11-cuda-cublas-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-9-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-9-0.\n",
            "Preparing to unpack .../13-cuda-cufft-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-9-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-9-0.\n",
            "Preparing to unpack .../15-cuda-curand-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-9-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-9-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-9-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-9-0.\n",
            "Preparing to unpack .../19-cuda-npp-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-9-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-9-0.\n",
            "Preparing to unpack .../21-cuda-nvgraph-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-9-0.\n",
            "Preparing to unpack .../22-cuda-nvgraph-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-samples-9-0.\n",
            "Preparing to unpack .../23-cuda-samples-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-samples-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-documentation-9-0.\n",
            "Preparing to unpack .../24-cuda-documentation-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-9-0.\n",
            "Preparing to unpack .../25-cuda-libraries-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-9-0.\n",
            "Preparing to unpack .../26-cuda-nvml-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-9-0.\n",
            "Preparing to unpack .../27-cuda-visual-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-9-0.\n",
            "Preparing to unpack .../28-cuda-toolkit-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-9-0.\n",
            "Preparing to unpack .../29-cuda-libraries-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-runtime-9-0.\n",
            "Preparing to unpack .../30-cuda-runtime-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-9-0.\n",
            "Preparing to unpack .../31-cuda-demo-suite-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-9-0.\n",
            "Preparing to unpack .../32-cuda-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda.\n",
            "Preparing to unpack .../33-cuda_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda (9.0.176-1) ...\n",
            "Setting up cuda-license-9-0 (9.0.176-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-9.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-core-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-samples-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-9-0 (9.0.176-1) ...\n",
            "Setting up cuda (9.0.176-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ejWdzNrl5IPe",
        "colab_type": "code",
        "outputId": "13b6f835-ba93-4c37-eea7-dd7c5c64ebcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myXuKPN25x_t",
        "colab_type": "code",
        "outputId": "aba8d7d3-b88d-4e7a-a472-8045cff9ffba",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# copy dataset into working directory\n",
        "%cd /content\n",
        "!cp -r '/content/drive/My Drive/imdb' '/content/yesweGAN/maskgan_colab/dataset'"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nFfWXj1wuYdY",
        "colab_type": "code",
        "outputId": "21905d47-79ef-425a-f355-e8c3e16f2eec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# copy most recent checkpoint into working directory to continue training\n",
        "%cd /content\n",
        "!cp -r '/content/drive/My Drive/imdb mle checkpoint/train' '/content/yesweGAN/maskgan_colab/maskGAN'"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6_G3THLoUoi",
        "colab_type": "text"
      },
      "source": [
        "# Must be using python 2 runtime w GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMz5ZNbql-EW",
        "colab_type": "text"
      },
      "source": [
        "## Run MaskGAN in MLE pretraining mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a4ec72a5-bd96-4ec7-f4c9-1ead5e4a68a2",
        "id": "3mV31eblvixR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/yesweGAN/maskgan_colab\n",
        "! python train_mask_gan.py \\\n",
        " --data_dir='dataset/imdb' \\\n",
        " --data_set='imdb' \\\n",
        " --batch_size=128 \\\n",
        " --sequence_length=20 \\\n",
        " --base_directory='maskGAN' \\\n",
        " --hparams=\"gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,dis_num_layers=2,gen_learning_rate=0.00074876,dis_learning_rate=5e-4,baseline_decay=0.99,dis_train_iterations=1,gen_learning_rate_decay=0.95\" \\\n",
        " --mode='TRAIN' \\\n",
        " --max_steps=10000 \\\n",
        " --perplexity_threshold=1000000 \\\n",
        " --generator_model='seq2seq_vd' \\\n",
        " --discriminator_model='seq2seq_vd' \\\n",
        " --is_present_rate=0.5 \\\n",
        " --summaries_every=50 \\\n",
        " --print_every=250 \\\n",
        " --max_num_to_print=3 \\\n",
        " --gen_training_strategy=cross_entropy \\\n",
        " --seq2seq_share_embedding=true \\\n",
        " --baseline_method=critic \\\n",
        " --attention_option=luong"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yesweGAN/maskgan_colab\n",
            "Unique 2-grams: 1300789\n",
            "Unique 3-grams: 3512054\n",
            "Unique 4-grams: 4986879\n",
            "Vocab size: 69849\n",
            "Training model.\n",
            "Optimizing Generator vars.\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69849, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69849,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "\n",
            "Optimizing Discriminator vars:\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(69849, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Optimizing Critic vars:\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Trainable Variables in Graph:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69849, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69849,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(69849, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "WARNING:tensorflow:From train_mask_gan.py:548: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2020-03-19 00:25:34.258261: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-03-19 00:25:34.444358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-19 00:25:34.444709: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \n",
            "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 7.43GiB freeMemory: 7.32GiB\n",
            "2020-03-19 00:25:34.444743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n",
            "2020-03-19 00:25:39.386725: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7072 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFD9rWo42ezv",
        "colab_type": "code",
        "outputId": "e37e9037-d28d-43da-d2da-ea21c8020616",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# make copy of mle training checkpoints\n",
        "%cd /content\n",
        "!cp -r '/content/yesweGAN/maskgan_colab/maskGAN/train' '/content/drive/My Drive/imdb mle checkpoint'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3pcQ4Gpl2wp",
        "colab_type": "text"
      },
      "source": [
        "## Run MaskGAN in GAN mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a06ae42e-4dd5-41a3-f86d-99babab135f7",
        "id": "dLyPlm0cGEZu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/yesweGAN/maskgan_colab\n",
        "!python train_mask_gan.py \\\n",
        " --data_dir='dataset/imdb' \\\n",
        " --data_set='imdb' \\\n",
        " --batch_size=128 \\\n",
        " --sequence_length=20 \\\n",
        " --base_directory='/content/yesweGAN/maskgan_colab/maskGAN' \\\n",
        " --hparams=\"gen_rnn_size=550,dis_rnn_size=550,gen_num_layers=2,dis_num_layers=2,gen_learning_rate=0.000038877,gen_learning_rate_decay=1.0,gen_full_learning_rate_steps=2000000,gen_vd_keep_prob=0.33971,rl_discount_rate=0.89072,dis_learning_rate=5e-4,baseline_decay=0.99,dis_train_iterations=2,dis_pretrain_learning_rate=0.005,critic_learning_rate=5.1761e-7,dis_vd_keep_prob=0.71940\" \\\n",
        " --mode='TRAIN' \\\n",
        " --max_steps=10000 \\\n",
        " --perplexity_threshold=1000000 \\\n",
        " --generator_model='seq2seq_vd' \\\n",
        " --discriminator_model='seq2seq_vd' \\\n",
        " --is_present_rate=0.5 \\\n",
        " --summaries_every=250 \\\n",
        " --print_every=250 \\\n",
        " --max_num_to_print=3 \\\n",
        " --gen_training_strategy='reinforce' \\\n",
        " --seq2seq_share_embedding=true \\\n",
        " --baseline_method=critic \\\n",
        " --attention_option=luong"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yesweGAN/maskgan_colab\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0310 00:27:21.489969 139858729412480 lazy_loader.py:50] \n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "W0310 00:27:23.502741 139858729412480 module_wrapper.py:139] From train_mask_gan.py:1161: The name tf.app.run is deprecated. Please use tf.compat.v1.app.run instead.\n",
            "\n",
            "W0310 00:27:23.504081 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/data/imdb_loader.py:40: tf_record_iterator (from tensorflow.python.lib.io.tf_record) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use eager execution and: \n",
            "`tf.data.TFRecordDataset(path)`\n",
            "W0310 00:29:08.413126 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/data/imdb_loader.py:59: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
            "\n",
            "Unique 2-grams: 1300789\n",
            "Unique 3-grams: 3512054\n",
            "Unique 4-grams: 4986879\n",
            "Vocab size: 69849\n",
            "W0310 00:29:20.685357 139858729412480 module_wrapper.py:139] From train_mask_gan.py:1126: The name tf.gfile.MakeDirs is deprecated. Please use tf.io.gfile.makedirs instead.\n",
            "\n",
            "Training model.\n",
            "W0310 00:29:20.685695 139858729412480 module_wrapper.py:139] From train_mask_gan.py:502: The name tf.logging.info is deprecated. Please use tf.compat.v1.logging.info instead.\n",
            "\n",
            "I0310 00:29:20.685774 139858729412480 train_mask_gan.py:502] Training model.\n",
            "W0310 00:29:20.686714 139858729412480 module_wrapper.py:139] From train_mask_gan.py:515: The name tf.train.replica_device_setter is deprecated. Please use tf.compat.v1.train.replica_device_setter instead.\n",
            "\n",
            "W0310 00:29:20.686961 139858729412480 module_wrapper.py:139] From train_mask_gan.py:517: The name tf.container is deprecated. Please use tf.compat.v1.container instead.\n",
            "\n",
            "W0310 00:29:20.690521 139858729412480 module_wrapper.py:139] From train_mask_gan.py:247: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0310 00:29:20.693761 139858729412480 module_wrapper.py:139] From train_mask_gan.py:249: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "W0310 00:29:20.698447 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:596: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
            "\n",
            "W0310 00:29:20.698750 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:94: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
            "\n",
            "W0310 00:29:20.704561 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:104: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "W0310 00:29:20.705522 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/regularization/variational_dropout.py:37: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0310 00:29:20.715392 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:116: __init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "W0310 00:29:20.734066 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:71: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0310 00:29:20.748658 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:155: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "W0310 00:29:20.790999 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "W0310 00:29:20.796634 139858729412480 deprecation.py:506] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling __init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "W0310 00:29:20.920504 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:253: Print (from tensorflow.python.ops.logging_ops) is deprecated and will be removed after 2018-08-20.\n",
            "Instructions for updating:\n",
            "Use tf.print instead of tf.Print. Note that tf.print returns a no-output operator that directly prints the output. Outside of defuns or eager mode, this operator will not be executed unless it is directly specified in session.run or used as a control dependency for other operators. This is only a concern in graph mode. Below is an example of how to ensure tf.print executes in graph mode:\n",
            "\n",
            "W0310 00:29:20.931468 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/contrib/layers/python/layers/layers.py:1866: apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "W0310 00:29:20.951458 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:310: The name tf.matrix_transpose is deprecated. Please use tf.linalg.matrix_transpose instead.\n",
            "\n",
            "W0310 00:29:21.026355 139858729412480 deprecation.py:323] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:362: __init__ (from tensorflow.python.ops.distributions.categorical) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "W0310 00:29:21.028361 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/distributions/categorical.py:242: __init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
            "Instructions for updating:\n",
            "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
            "W0310 00:29:21.029144 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/ops/distributions/categorical.py:278: multinomial (from tensorflow.python.ops.random_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use `tf.random.categorical` instead.\n",
            "W0310 00:29:21.041198 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/models/seq2seq_vd.py:322: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
            "\n",
            "W0310 00:29:26.624068 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_losses.py:41: The name tf.losses.sigmoid_cross_entropy is deprecated. Please use tf.compat.v1.losses.sigmoid_cross_entropy instead.\n",
            "\n",
            "W0310 00:29:26.672734 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_losses.py:126: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0310 00:29:27.007671 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_losses.py:57: The name tf.losses.mean_squared_error is deprecated. Please use tf.compat.v1.losses.mean_squared_error instead.\n",
            "\n",
            "W0310 00:29:28.286993 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_optimization.py:114: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
            "\n",
            "W0310 00:29:28.287225 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/model_optimization.py:118: The name tf.trainable_variables is deprecated. Please use tf.compat.v1.trainable_variables instead.\n",
            "\n",
            "\n",
            "Optimizing Generator vars:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69849, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(550, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69849,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1100, 550) dtype=float32_ref>\n",
            "\n",
            "Optimizing Discriminator vars:\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(69849, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(550, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1100, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(550, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Optimizing Critic vars:\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(550, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "W0310 00:29:35.844297 139858729412480 module_wrapper.py:139] From train_mask_gan.py:377: The name tf.summary.scalar is deprecated. Please use tf.compat.v1.summary.scalar instead.\n",
            "\n",
            "W0310 00:29:35.858165 139858729412480 module_wrapper.py:139] From /content/yesweGAN/maskgan_colab/model_utils/helper.py:38: The name tf.summary.histogram is deprecated. Please use tf.compat.v1.summary.histogram instead.\n",
            "\n",
            "W0310 00:29:36.524502 139858729412480 module_wrapper.py:139] From train_mask_gan.py:423: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0310 00:29:36.532145 139858729412480 module_wrapper.py:139] From train_mask_gan.py:425: The name tf.summary.text is deprecated. Please use tf.compat.v1.summary.text instead.\n",
            "\n",
            "W0310 00:29:36.533526 139858729412480 module_wrapper.py:139] From train_mask_gan.py:428: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
            "\n",
            "\n",
            "Trainable Variables in Graph:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69849, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(550, 550) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69849,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1100, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(69849, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(550, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1100, 2200) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2200,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1100, 550) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(550, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(550, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "W0310 00:29:36.597843 139858729412480 deprecation.py:323] From train_mask_gan.py:546: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2020-03-10 00:29:38.478163: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcuda.so.1\n",
            "2020-03-10 00:29:38.519224: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.519790: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-10 00:29:38.522501: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-03-10 00:29:38.534383: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-03-10 00:29:38.537804: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-03-10 00:29:38.544079: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-03-10 00:29:38.552803: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-03-10 00:29:38.558878: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-03-10 00:29:38.571123: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-10 00:29:38.571253: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.571840: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.572324: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-10 00:29:38.572715: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-03-10 00:29:38.586189: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2000175000 Hz\n",
            "2020-03-10 00:29:38.588355: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562f16d119c0 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-10 00:29:38.588384: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Host, Default Version\n",
            "2020-03-10 00:29:38.743741: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.744475: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x562f16d11800 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
            "2020-03-10 00:29:38.744516: I tensorflow/compiler/xla/service/service.cc:176]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\n",
            "2020-03-10 00:29:38.744695: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.745229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1618] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "2020-03-10 00:29:38.745294: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-03-10 00:29:38.745318: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "2020-03-10 00:29:38.745338: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcufft.so.10.0\n",
            "2020-03-10 00:29:38.745357: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcurand.so.10.0\n",
            "2020-03-10 00:29:38.745376: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusolver.so.10.0\n",
            "2020-03-10 00:29:38.745394: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcusparse.so.10.0\n",
            "2020-03-10 00:29:38.745413: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudnn.so.7\n",
            "2020-03-10 00:29:38.745500: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.746054: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.746575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1746] Adding visible gpu devices: 0\n",
            "2020-03-10 00:29:38.750602: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.0\n",
            "2020-03-10 00:29:38.751849: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1159] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2020-03-10 00:29:38.751879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1165]      0 \n",
            "2020-03-10 00:29:38.751890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1178] 0:   N \n",
            "2020-03-10 00:29:38.754809: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.755509: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:983] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-10 00:29:38.756064: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2020-03-10 00:29:38.756125: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14221 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "I0310 00:29:38.759409 139858729412480 saver.py:1284] Restoring parameters from /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt-268\n",
            "W0310 00:30:03.681040 139858729412480 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/saver.py:1069: get_checkpoint_mtimes (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file utilities to get mtimes.\n",
            "I0310 00:30:03.683342 139858729412480 session_manager.py:500] Running local_init_op.\n",
            "I0310 00:30:04.021142 139858729412480 session_manager.py:502] Done running local_init_op.\n",
            "I0310 00:30:12.559847 139858729412480 supervisor.py:737] Starting standard services.\n",
            "I0310 00:30:12.858433 139858729412480 supervisor.py:743] Starting queue runners.\n",
            "I0310 00:30:12.858711 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "I0310 00:30:13.155273 139852867667712 supervisor.py:1099] global_step/sec: 0\n",
            "W0310 00:30:17.906004 139852876060416 deprecation.py:323] From /usr/local/lib/python2.7/dist-packages/tensorflow_core/python/training/saver.py:963: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to delete files with this prefix.\n",
            "2020-03-10 00:30:19.279675: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.289919: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.299557: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.310697: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.320860: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.331114: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.338795: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.343931: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.344554: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.349462: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.355707: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.361273: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.363668: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.366602: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.371070: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.376041: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.380194: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.380993: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.385354: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.390051: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.394991: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.396537: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.400071: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.404401: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.408975: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.412921: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.414001: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.418385: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.423006: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.427727: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.428692: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.431413: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.448513: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.478311: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.504829: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.523465: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.538105: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.552130: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.565963: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.579754: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.713277: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.718199: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.722775: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.727297: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.731603: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.736085: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.740445: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.744304: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.748120: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.752016: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.755932: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.759738: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.763527: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.767332: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.771096: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.774812: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.778650: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.782514: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.786315: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:19.789077: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:24.401118: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcublas.so.10.0\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 0 1 1 1 1 1 1 0 1...]...][[8 23354 69849 8689 58348 120 26 5755 1110 69849...]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 0 1 1 1 1 1 1 0 1...]...][[8 23354 69849 8689 58348 120 26 5755 1110 69849...]...][[23354 1255 8689 58348 120 26 5755 1110 198 2685...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 0 0 0 1 1 1 0 0...]...][[577 5337 12 69849 69849 69849 81 354 39 69849...]...][[5337 12 361 5 2 81 354 39 12 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 0 0 0 1 1 1 0 0...]...][[577 5337 12 69849 69849 69849 81 354 39 69849...]...][[5337 12 53229 5 0 81 354 39 0 4...]...]\n",
            "2020-03-10 00:30:27.853293: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.854308: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.856206: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.858064: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.859805: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.861565: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.861847: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.864262: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.866332: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.868211: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.869955: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.870502: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.872469: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.874347: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.876084: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.877288: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.878118: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.880099: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.881831: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.883731: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.884750: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.886078: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.888076: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.889879: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.891190: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen_1/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.891544: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.894154: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.896440: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.898582: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.900674: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.902753: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.905118: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.907375: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.909456: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.911560: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.913713: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.915798: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.917881: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.920001: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.922068: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.964572: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.966083: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.967554: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.969013: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.970738: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.972174: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.973607: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.975057: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.976582: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.978075: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.979550: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.980961: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.982413: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.983989: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.985508: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.986924: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.988340: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.989791: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.991254: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:27.992197: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[137 13 590 379 11 1057 8 55 225 2911...]...]\n",
            "2020-03-10 00:30:35.151716: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.161725: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.170964: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.180760: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.190259: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.200046: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.206936: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.211323: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.214767: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.216006: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.220594: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.224618: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.228588: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.229821: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.232958: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.236881: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.240517: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.244048: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.244426: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.248513: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.252325: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.256263: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.258408: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.260343: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.264735: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.268611: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.272325: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.272932: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.276802: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.280804: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.283686: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.285639: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.315128: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.326541: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.337763: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.349044: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.360900: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.372071: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.383353: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.394924: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.505060: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.508098: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.511305: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.514421: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.517572: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.520714: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.523883: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.526929: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.530071: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.533017: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.535920: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.539054: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.542062: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.545002: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.547900: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.550864: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.553877: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.556839: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.559660: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:35.562041: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[17 13 37 379 11 9 307 55 4 2911...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[50 13 0 379 11 10 1 55 1 2911...]...]\n",
            "2020-03-10 00:30:52.349923: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.350886: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.351583: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.352191: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.352845: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.353443: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.354054: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.354660: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.355261: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.355884: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.356499: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.357095: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.357704: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.358312: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.358930: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.359542: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.360146: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.360777: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.361434: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:52.362039: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "W0310 00:30:52.892658 139858729412480 module_wrapper.py:139] From train_mask_gan.py:756: The name tf.Summary is deprecated. Please use tf.compat.v1.Summary instead.\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "global_step: 274\n",
            " perplexity: 968.750\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            " percent of 3-grams captured: 0.131.\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            " percent of 2-grams captured: 0.546.\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.114.\n",
            " arithmetic_avg: 0.233.\n",
            "global_step: 274\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66360\n",
            " G train loss: 147.60037\n",
            "2020-03-10 00:30:55.191697: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.195860: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.200898: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.205640: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.210044: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.214197: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.216390: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.217996: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.219434: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.219861: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.221264: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.222795: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.224348: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.225143: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.226026: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.227629: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.229244: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.230816: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.231080: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.232447: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.234039: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.235718: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.236979: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.237614: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.239058: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.240618: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.242092: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.242617: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.243760: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.245263: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.246389: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis_1/decoder/rnn/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.247114: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.248554: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.249899: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.251248: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.252620: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.254002: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.255429: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.256951: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.258394: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node gen/decoder/rnn_2/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.290995: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.292294: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_1/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.293500: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_2/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.294795: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_3/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.296050: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_4/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.297256: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_5/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.298514: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_6/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.300014: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_7/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.301269: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_8/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.302853: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_9/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.304083: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_10/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.305276: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_11/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.306529: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_12/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.307795: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_13/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.309017: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_14/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.310251: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_15/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.312104: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_16/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.313466: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_17/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.314879: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_18/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "2020-03-10 00:30:55.315831: W tensorflow/core/grappler/utils/graph_view.cc:830] No registered 'attn_mul_fun_f32f32' OpKernel for GPU devices compatible with node {{node dis/decoder/rnn_1/attention_construct_19/attn_mul_fun_f32f32}}\n",
            "\t.  Registered:  <no registered kernels>\n",
            "\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[17 13 37 379 11 9 307 55 4 2911...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 37 379 11 9 307 55 4...]...][[0 1 0 1 1 0 0 1 0 1...]...][[10 69849 13 69849 379 11 69849 69849 55 69849...]...][[110 13 7511 379 11 346 949 55 294 2911...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  seen                0.577        3.706        -6.379       -0.549       -2.798       -0.304       -2.494       \n",
            "   [1]  was                 0.566        0.000        0.000        0.000        -2.525       -0.379       -0.000       \n",
            "   [0]  2009                0.473        5.275        -8.489       -0.749       -2.835       -0.396       -2.439       \n",
            "   [1]  awful               0.433        0.000        0.000        0.000        -2.342       -0.366       -0.000       \n",
            "   [1]  that                0.467        0.000        0.000        0.000        -2.630       -0.352       -0.000       \n",
            "   [0]  home                0.495        3.563        -7.490       -0.702       -2.952       -0.365       -2.588       \n",
            "   [0]  write               0.440        6.522        -7.781       -0.820       -2.526       -0.386       -2.140       \n",
            "   [1]  up                  0.429        0.000        0.000        0.000        -1.915       -0.374       -0.000       \n",
            "   [0]  during              0.393        4.099        -7.952       -0.933       -2.150       -0.367       -1.783       \n",
            "   [1]  halfway             0.401        0.000        0.000        0.000        -1.366       -0.353       -0.000       \n",
            "   [0]  never               0.445        7.438        -6.896       -0.810       -1.534       -0.356       -1.177       \n",
            "   [0]  very                0.444        8.406        -5.831       -0.813       -0.813       -0.377       -0.436       \n",
            "   [1]  for                 0.478        0.000        0.000        0.000        0.000        -0.378       0.000        \n",
            "   [1]  it                  0.477        0.000        0.000        0.000        0.000        -0.394       0.000        \n",
            "   [1]  to                  0.509        0.000        0.000        0.000        0.000        -0.391       0.000        \n",
            "   [1]  start               0.493        0.000        0.000        0.000        0.000        -0.403       0.000        \n",
            "   [1]  for                 0.519        0.000        0.000        0.000        0.000        -0.399       0.000        \n",
            "   [1]  the                 0.567        0.000        0.000        0.000        0.000        -0.415       0.000        \n",
            "   [1]  plot                0.497        0.000        0.000        0.000        0.000        -0.444       0.000        \n",
            "   [1]  characters          0.449        0.000        0.000        0.000        0.000        -0.409       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  are                 0.581        3.895        -5.248       -0.543       -2.304       -0.309       -1.995       \n",
            "   [1]  easy                0.530        0.000        0.000        0.000        -1.977       -0.359       -0.000       \n",
            "   [1]  to                  0.547        0.000        0.000        0.000        -2.219       -0.367       -0.000       \n",
            "   [0]  i                   0.560        5.470        -2.948       -0.580       -2.491       -0.378       -2.113       \n",
            "   [1]  that                0.611        0.000        0.000        0.000        -2.146       -0.391       -0.000       \n",
            "   [1]  risa                0.586        0.000        0.000        0.000        -2.409       -0.406       -0.000       \n",
            "   [1]  garcia              0.558        0.000        0.000        0.000        -2.705       -0.405       -0.000       \n",
            "   [0]  had                 0.512        4.309        -5.663       -0.669       -3.037       -0.401       -2.635       \n",
            "   [0]  and                 0.541        3.051        -3.010       -0.615       -2.658       -0.387       -2.271       \n",
            "   [0]  a                   0.578        12.625       -3.075       -0.548       -2.294       -0.392       -1.901       \n",
            "   [0]  great               0.591        10.067       -6.569       -0.527       -1.960       -0.406       -1.554       \n",
            "   [1]  director            0.572        0.000        0.000        0.000        -1.609       -0.413       -0.000       \n",
            "   [0]  fantasy             0.490        2.985        -8.143       -0.714       -1.806       -0.409       -1.397       \n",
            "   [0]  2                   0.519        4.857        -7.097       -0.656       -1.226       -0.387       -0.839       \n",
            "   [1]  a                   0.558        0.000        0.000        0.000        -0.641       -0.392       -0.000       \n",
            "   [1]  screenwriter        0.598        0.000        0.000        0.000        -0.719       -0.408       -0.000       \n",
            "   [0]  have                0.611        5.160        -5.241       -0.493       -0.808       -0.424       -0.384       \n",
            "   [1]  dozens              0.627        0.000        0.000        0.000        -0.354       -0.432       0.000        \n",
            "   [1]  of                  0.671        0.000        0.000        0.000        -0.397       -0.440       0.000        \n",
            "   [0]  like                0.640        8.059        -5.355       -0.446       -0.446       -0.453       0.007        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  been                0.564        2.915        -6.245       -0.573       -3.072       -0.304       -2.768       \n",
            "   [1]  is                  0.596        0.000        0.000        0.000        -2.805       -0.330       -0.000       \n",
            "   [0]  this                0.614        8.964        -3.151       -0.488       -3.150       -0.359       -2.791       \n",
            "   [0]  be                  0.660        11.951       -5.448       -0.415       -2.988       -0.378       -2.610       \n",
            "   [1]  knows               0.638        0.000        0.000        0.000        -2.888       -0.417       -0.000       \n",
            "   [1]  how                 0.569        0.000        0.000        0.000        -3.242       -0.414       -0.000       \n",
            "   [0]  every               0.526        3.841        -7.313       -0.643       -3.640       -0.377       -3.263       \n",
            "   [0]  so                  0.459        8.684        -4.864       -0.779       -3.365       -0.353       -3.012       \n",
            "   [1]  a                   0.467        0.000        0.000        0.000        -2.903       -0.316       -0.000       \n",
            "   [0]  of                  0.516        8.465        -3.883       -0.661       -3.260       -0.316       -2.943       \n",
            "   [0]  well                0.500        7.945        -6.170       -0.693       -2.918       -0.342       -2.576       \n",
            "   [0]  tv                  0.487        5.821        -6.887       -0.719       -2.498       -0.340       -2.157       \n",
            "   [1]  best                0.478        0.000        0.000        0.000        -1.996       -0.337       -0.000       \n",
            "   [0]  just                0.421        7.786        -5.960       -0.866       -2.241       -0.338       -1.903       \n",
            "   [1]  was                 0.374        0.000        0.000        0.000        -1.544       -0.310       -0.000       \n",
            "   [1]  noises              0.353        0.000        0.000        0.000        -1.733       -0.284       -0.000       \n",
            "   [1]  off                 0.321        0.000        0.000        0.000        -1.946       -0.276       -0.000       \n",
            "   [0]  a                   0.346        9.003        -3.602       -1.062       -2.185       -0.267       -1.918       \n",
            "   [0]  heart               0.283        8.537        -7.568       -1.261       -1.261       -0.284       -0.977       \n",
            "   [1]  is                  0.286        0.000        0.000        0.000        0.000        -0.251       0.000        \n",
            "Samples\n",
            "Sample 0 .  seen was 2009 awful that home write up during halfway never very for it to start for the plot characters\n",
            "Sample 1 .  are easy to i that risa garcia had and a great director fantasy 2 a screenwriter have dozens of like\n",
            "Sample 2 .  been is this be knows how every so a of well tv best just was noises off a heart is\n",
            "\n",
            "\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 0 1 0 0 0 1...]...][[16 1803 69849 45 110 69849 703 69849 69849 69849...]...][[1803 34 45 110 0 703 206 15 5359 751...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 0 1 0 0 0 1...]...][[16 1803 69849 45 110 69849 703 69849 69849 69849...]...][[1803 24 45 110 46 703 402 1 492 751...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[0 0 1 0 1 0 0 0 0 1...]...][[7 69849 69849 174 69849 12 69849 69849 69849 69849...]...][[5 23 174 19 12 301 4 22800 20 15795...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[0 0 1 0 1 0 0 0 0 1...]...][[7 69849 69849 174 69849 12 69849 69849 69849 69849...]...][[147 12 174 2182 12 17874 1 981 165 15795...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 31 607 120 26 207 1110 3 47331...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 8 2 120 26 1 1110 3 169...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "global_step: 279\n",
            " perplexity: 962.597\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            " percent of 3-grams captured: 0.138.\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            " percent of 2-grams captured: 0.539.\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.115.\n",
            " arithmetic_avg: 0.232.\n",
            "global_step: 279\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66140\n",
            " G train loss: 158.66170\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 8689 58348 120 26 5755 1110 3 2685...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 23354 1922 8689 58348 120 26 5755 1110 3...]...][[1 1 0 0 1 1 0 1 1 0...]...][[8 23354 1922 69849 69849 120 26 69849 1110 3...]...][[23354 1922 64063 16274 120 26 467 1110 3 18...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  pushover            0.504        0.000        0.000        0.000        -2.723       -0.306       -0.000       \n",
            "   [1]  fred                0.525        0.000        0.000        0.000        -3.057       -0.352       -0.000       \n",
            "   [0]  shana               0.516        11.697       -13.213      -0.661       -3.432       -0.400       -3.031       \n",
            "   [0]  har                 0.502        11.908       -11.601      -0.690       -3.111       -0.424       -2.687       \n",
            "   [1]  off                 0.450        0.000        0.000        0.000        -2.718       -0.433       -0.000       \n",
            "   [1]  his                 0.387        0.000        0.000        0.000        -3.051       -0.411       -0.000       \n",
            "   [0]  loved               0.381        11.243       -7.323       -0.966       -3.425       -0.357       -3.068       \n",
            "   [1]  portrayal           0.353        0.000        0.000        0.000        -2.761       -0.333       -0.000       \n",
            "   [1]  of                  0.407        0.000        0.000        0.000        -3.100       -0.304       -0.000       \n",
            "   [0]  but                 0.466        8.271        -5.832       -0.764       -3.480       -0.330       -3.150       \n",
            "   [0]  his                 0.433        11.746       -6.165       -0.837       -3.050       -0.383       -2.667       \n",
            "   [1]  the                 0.466        0.000        0.000        0.000        -2.484       -0.377       -0.000       \n",
            "   [0]  this                0.483        11.735       -5.122       -0.728       -2.788       -0.409       -2.379       \n",
            "   [0]  one                 0.493        11.356       -6.230       -0.708       -2.314       -0.439       -1.875       \n",
            "   [0]  another             0.484        8.796        -7.066       -0.726       -1.802       -0.464       -1.339       \n",
            "   [1]  from                0.504        0.000        0.000        0.000        -1.208       -0.469       -0.000       \n",
            "   [0]  eddie               0.463        10.258       -8.965       -0.769       -1.356       -0.490       -0.866       \n",
            "   [1]  indemnity           0.409        0.000        0.000        0.000        -0.659       -0.470       -0.000       \n",
            "   [1]  and                 0.440        0.000        0.000        0.000        -0.740       -0.425       -0.000       \n",
            "   [0]  garbage             0.436        8.624        -8.460       -0.831       -0.831       -0.434       -0.396       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  movie               0.535        3.034        -3.332       -0.626       -3.217       -0.306       -2.911       \n",
            "   [1]  nutshell            0.549        0.000        0.000        0.000        -2.909       -0.370       -0.000       \n",
            "   [0]  at                  0.526        5.072        -6.011       -0.643       -3.266       -0.413       -2.853       \n",
            "   [0]  is                  0.534        4.427        -3.000       -0.628       -2.945       -0.422       -2.523       \n",
            "   [0]  christmas           0.489        5.293        -8.267       -0.715       -2.601       -0.430       -2.171       \n",
            "   [1]  don                 0.458        0.000        0.000        0.000        -2.117       -0.418       -0.000       \n",
            "   [1]  t                   0.457        0.000        0.000        0.000        -2.377       -0.408       -0.000       \n",
            "   [0]  movie               0.503        4.870        -4.278       -0.687       -2.669       -0.409       -2.260       \n",
            "   [0]  very                0.491        3.650        -5.796       -0.711       -2.225       -0.428       -1.797       \n",
            "   [0]  to                  0.523        7.002        -3.647       -0.648       -1.700       -0.426       -1.273       \n",
            "   [1]  the                 0.573        0.000        0.000        0.000        -1.181       -0.438       -0.000       \n",
            "   [0]  was                 0.550        8.455        -4.313       -0.597       -1.326       -0.461       -0.865       \n",
            "   [1]  of                  0.593        0.000        0.000        0.000        -0.818       -0.461       -0.000       \n",
            "   [0]  new                 0.559        5.573        -6.731       -0.582       -0.919       -0.473       -0.445       \n",
            "   [1]  crap                0.538        0.000        0.000        0.000        -0.378       -0.462       0.000        \n",
            "   [1]  if                  0.512        0.000        0.000        0.000        -0.424       -0.454       0.000        \n",
            "   [1]  you                 0.543        0.000        0.000        0.000        -0.476       -0.439       -0.000       \n",
            "   [1]  don                 0.534        0.000        0.000        0.000        -0.535       -0.447       -0.000       \n",
            "   [1]  t                   0.535        0.000        0.000        0.000        -0.601       -0.448       -0.000       \n",
            "   [0]  blood               0.510        7.819        -8.254       -0.674       -0.674       -0.453       -0.222       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  i                   0.507        12.780       -4.688       -0.679       -4.747       -0.306       -4.441       \n",
            "   [0]  elements            0.507        6.521        -7.634       -0.679       -4.568       -0.360       -4.208       \n",
            "   [1]  however             0.492        0.000        0.000        0.000        -4.366       -0.387       -0.000       \n",
            "   [0]  here                0.490        6.125        -6.897       -0.713       -4.902       -0.402       -4.500       \n",
            "   [0]  missing             0.433        12.862       -8.649       -0.837       -4.703       -0.408       -4.295       \n",
            "   [0]  run                 0.451        6.121        -8.332       -0.797       -4.340       -0.394       -3.946       \n",
            "   [1]  name                0.381        0.000        0.000        0.000        -3.978       -0.399       -0.000       \n",
            "   [0]  although            0.342        4.733        -7.572       -1.073       -4.466       -0.384       -4.082       \n",
            "   [0]  rambling            0.327        4.400        -11.380      -1.118       -3.810       -0.368       -3.442       \n",
            "   [0]  as                  0.353        12.439       -5.514       -1.042       -3.022       -0.356       -2.666       \n",
            "   [1]  because             0.337        0.000        0.000        0.000        -2.223       -0.358       -0.000       \n",
            "   [0]  questions           0.371        6.108        -8.839       -0.992       -2.496       -0.355       -2.141       \n",
            "   [0]  silent              0.367        7.304        -7.675       -1.004       -1.688       -0.363       -1.325       \n",
            "   [1]  sends               0.386        0.000        0.000        0.000        -0.768       -0.370       -0.000       \n",
            "   [1]  opposing            0.407        0.000        0.000        0.000        -0.862       -0.382       -0.000       \n",
            "   [1]  messages            0.431        0.000        0.000        0.000        -0.968       -0.394       -0.000       \n",
            "   [1]  to                  0.473        0.000        0.000        0.000        -1.086       -0.405       -0.000       \n",
            "   [0]  eugenie             0.477        7.787        -12.733      -0.740       -1.220       -0.425       -0.795       \n",
            "   [1]  on                  0.502        0.000        0.000        0.000        -0.539       -0.434       -0.000       \n",
            "   [0]  for                 0.546        5.878        -5.865       -0.605       -0.605       -0.451       -0.154       \n",
            "Samples\n",
            "Sample 0 .  pushover fred shana har off his loved portrayal of but his the this one another from eddie indemnity and garbage\n",
            "Sample 1 .  movie nutshell at is christmas don t movie very to the was of new crap if you don t blood\n",
            "Sample 2 .  i elements however here missing run name although rambling as because questions silent sends opposing messages to eugenie on for\n",
            "\n",
            "\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 1 1 0 0 1 0 1 1...]...][[10 141 27 77 126 69849 69849 2 69849 2431...]...][[141 27 77 126 73 126 2 1023 2431 2347...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 1 1 0 0 1 0 1 1...]...][[10 141 27 77 126 69849 69849 2 69849 2431...]...][[141 27 77 126 91 2 2 9665 2431 2347...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[0 0 0 0 0 0 0 1 0 1...]...][[10 69849 69849 69849 69849 69849 69849 69849 91 69849...]...][[17 5 634 2 181 17 31 91 2075 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[0 0 0 0 0 0 0 1 0 1...]...][[10 69849 69849 69849 69849 69849 69849 69849 91 69849...]...][[28499 417 699 46 8 38 4 91 1365 7...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 27 81 2251 3 12 5...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 2 81 354 39 12 5...]...]\n",
            "I0310 00:31:12.858674 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 11 81 20963 0 12 5...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "global_step: 284\n",
            " perplexity: 958.371\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            " percent of 3-grams captured: 0.148.\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            " percent of 2-grams captured: 0.550.\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            " percent of 4-grams captured: 0.035.\n",
            " geometric_avg: 0.142.\n",
            " arithmetic_avg: 0.244.\n",
            "global_step: 284\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66193\n",
            " G train loss: 161.67604\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 2 81 354 39 12 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[577 5337 12 361 5 2 81 354 39 12...]...][[1 1 1 1 0 1 0 0 1 1...]...][[577 5337 12 361 5 69849 81 69849 69849 12...]...][[5337 12 361 5 30 81 3943 50 12 5...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  cronenberg          0.517        0.000        0.000        0.000        -1.870       -0.309       -0.000       \n",
            "   [1]  s                   0.525        0.000        0.000        0.000        -2.099       -0.357       -0.000       \n",
            "   [1]  camera              0.459        0.000        0.000        0.000        -2.356       -0.393       -0.000       \n",
            "   [1]  is                  0.458        0.000        0.000        0.000        -2.645       -0.401       -0.000       \n",
            "   [0]  all                 0.544        3.576        -5.808       -0.610       -2.970       -0.403       -2.568       \n",
            "   [1]  great               0.570        0.000        0.000        0.000        -2.650       -0.421       -0.000       \n",
            "   [0]  intrigued           0.574        7.961        -11.489      -0.555       -2.975       -0.437       -2.538       \n",
            "   [0]  can                 0.560        5.912        -6.329       -0.580       -2.717       -0.443       -2.274       \n",
            "   [1]  s                   0.564        0.000        0.000        0.000        -2.399       -0.443       -0.000       \n",
            "   [1]  is                  0.581        0.000        0.000        0.000        -2.694       -0.448       -0.000       \n",
            "   [1]  only                0.574        0.000        0.000        0.000        -3.024       -0.453       -0.000       \n",
            "   [0]  detailed            0.509        7.339        -9.716       -0.676       -3.395       -0.446       -2.949       \n",
            "   [0]  present             0.378        6.668        -8.713       -0.973       -3.053       -0.429       -2.624       \n",
            "   [0]  i                   0.361        4.451        -3.886       -1.018       -2.335       -0.402       -1.933       \n",
            "   [1]  the                 0.411        0.000        0.000        0.000        -1.479       -0.383       -0.000       \n",
            "   [0]  ve                  0.421        4.491        -6.755       -0.866       -1.661       -0.378       -1.282       \n",
            "   [0]  son                 0.410        9.906        -8.591       -0.892       -0.892       -0.383       -0.510       \n",
            "   [1]  s                   0.419        0.000        0.000        0.000        0.000        -0.382       0.000        \n",
            "   [1]  makes               0.419        0.000        0.000        0.000        0.000        -0.389       0.000        \n",
            "   [1]  it                  0.403        0.000        0.000        0.000        0.000        -0.393       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  have                0.496        0.000        0.000        0.000        -2.610       -0.301       -0.000       \n",
            "   [1]  watched             0.442        0.000        0.000        0.000        -2.931       -0.339       -0.000       \n",
            "   [0]  it                  0.393        3.587        -4.360       -0.933       -3.290       -0.354       -2.937       \n",
            "   [0]  is                  0.390        4.743        -3.703       -0.943       -2.646       -0.351       -2.295       \n",
            "   [1]  several             0.356        0.000        0.000        0.000        -1.913       -0.349       -0.000       \n",
            "   [0]  character           0.286        8.160        -7.101       -1.252       -2.147       -0.351       -1.796       \n",
            "   [1]  and                 0.315        0.000        0.000        0.000        -1.005       -0.342       -0.000       \n",
            "   [1]  aided               0.310        0.000        0.000        0.000        -1.129       -0.331       -0.000       \n",
            "   [1]  by                  0.321        0.000        0.000        0.000        -1.267       -0.329       -0.000       \n",
            "   [1]  a                   0.363        0.000        0.000        0.000        -1.423       -0.334       -0.000       \n",
            "   [1]  gap                 0.387        0.000        0.000        0.000        -1.597       -0.338       -0.000       \n",
            "   [1]  of                  0.471        0.000        0.000        0.000        -1.793       -0.347       -0.000       \n",
            "   [0]  a                   0.532        3.677        -3.677       -0.631       -2.013       -0.355       -1.658       \n",
            "   [1]  few                 0.561        0.000        0.000        0.000        -1.552       -0.373       -0.000       \n",
            "   [0]  but                 0.608        6.391        -5.293       -0.498       -1.742       -0.387       -1.355       \n",
            "   [0]  director            0.601        7.552        -7.384       -0.509       -1.396       -0.408       -0.988       \n",
            "   [1]  viewings            0.628        0.000        0.000        0.000        -0.996       -0.424       -0.000       \n",
            "   [0]  his                 0.590        4.006        -5.422       -0.527       -1.118       -0.439       -0.679       \n",
            "   [0]  office              0.515        3.689        -8.859       -0.663       -0.663       -0.439       -0.224       \n",
            "   [1]  bad                 0.554        0.000        0.000        0.000        0.000        -0.425       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  who                 0.525        3.861        -6.667       -0.644       -3.493       -0.301       -3.192       \n",
            "   [0]  peevishness         0.514        5.825        -12.236      -0.665       -3.199       -0.350       -2.849       \n",
            "   [1]  sucked              0.486        0.000        0.000        0.000        -2.845       -0.371       -0.000       \n",
            "   [1]  from                0.503        0.000        0.000        0.000        -3.194       -0.379       -0.000       \n",
            "   [0]  efx                 0.470        4.203        -12.742      -0.754       -3.586       -0.392       -3.194       \n",
            "   [1]  police              0.368        0.000        0.000        0.000        -3.179       -0.388       -0.000       \n",
            "   [1]  response            0.366        0.000        0.000        0.000        -3.569       -0.359       -0.000       \n",
            "   [1]  to                  0.405        0.000        0.000        0.000        -4.007       -0.352       -0.000       \n",
            "   [0]  well                0.387        4.058        -6.751       -0.948       -4.498       -0.363       -4.136       \n",
            "   [1]  bitter              0.358        0.000        0.000        0.000        -3.985       -0.359       -0.000       \n",
            "   [0]  demos               0.344        7.255        -12.763      -1.067       -4.474       -0.349       -4.125       \n",
            "   [0]  if                  0.317        4.264        -6.503       -1.148       -3.826       -0.346       -3.480       \n",
            "   [1]  was                 0.297        0.000        0.000        0.000        -3.006       -0.338       -0.000       \n",
            "   [1]  really              0.410        0.000        0.000        0.000        -3.375       -0.332       -0.000       \n",
            "   [0]  happy               0.303        6.311        -8.029       -1.193       -3.789       -0.365       -3.424       \n",
            "   [0]  what                0.343        3.863        -6.512       -1.070       -2.915       -0.339       -2.576       \n",
            "   [0]  it                  0.342        5.959        -4.268       -1.072       -2.071       -0.348       -1.723       \n",
            "   [0]  truth               0.326        5.595        -8.507       -1.121       -1.121       -0.343       -0.778       \n",
            "   [1]  believe             0.353        0.000        0.000        0.000        0.000        -0.341       0.000        \n",
            "   [1]  i                   0.378        0.000        0.000        0.000        0.000        -0.350       0.000        \n",
            "Samples\n",
            "Sample 0 .  cronenberg s camera is all great intrigued can s is only detailed present i the ve son s makes it\n",
            "Sample 1 .  have watched it is several character and aided by a gap of a few but director viewings his office bad\n",
            "Sample 2 .  who peevishness sucked from efx police response to well bitter demos if was really happy what it truth believe i\n",
            "\n",
            "\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[0 1 0 0 1 1 1 1 1 1...]...][[10 69849 2 69849 69849 3 478 2124 338 98...]...][[5 2 1563 120 3 478 2124 338 98 38...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[0 1 0 0 1 1 1 1 1 1...]...][[10 69849 2 69849 69849 3 478 2124 338 98...]...][[250 2 5 2826 3 478 2124 338 98 38...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[0 1 1 0 1 1 1 1 0 0...]...][[84 69849 30 287 69849 42 136 11 99 69849...]...][[3 30 287 71 42 136 11 99 816 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[0 1 1 0 1 1 1 1 0 0...]...][[84 69849 30 287 69849 42 136 11 99 69849...]...][[19 30 287 163 42 136 11 99 160 25...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 0 45 110 0 12 0 874 5359 751...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 34 45 110 0 703 206 15 5359 751...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 5 45 110 0 859 1101 504 5359 751...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "global_step: 289\n",
            " perplexity: 981.123\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            " percent of 3-grams captured: 0.146.\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            " percent of 2-grams captured: 0.555.\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            " percent of 4-grams captured: 0.020.\n",
            " geometric_avg: 0.118.\n",
            " arithmetic_avg: 0.241.\n",
            "global_step: 289\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66787\n",
            " G train loss: 158.88863\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 34 45 110 0 703 206 15 5359 751...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 1803 34 45 110 0 703 206 15 5359...]...][[1 0 1 1 1 0 0 0 1 1...]...][[16 1803 69849 45 110 0 69849 69849 69849 5359...]...][[1803 2881 45 110 0 1095 2319 5447 5359 751...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  somebody            0.471        0.000        0.000        0.000        -2.266       -0.294       -0.000       \n",
            "   [0]  sinister            0.464        6.584        -9.573       -0.769       -2.543       -0.324       -2.220       \n",
            "   [1]  has                 0.428        0.000        0.000        0.000        -1.993       -0.341       -0.000       \n",
            "   [1]  seen                0.501        0.000        0.000        0.000        -2.237       -0.340       -0.000       \n",
            "   [1]  the                 0.582        0.000        0.000        0.000        -2.512       -0.363       -0.000       \n",
            "   [0]  credit              0.548        8.626        -9.187       -0.601       -2.820       -0.387       -2.432       \n",
            "   [0]  pleasant            0.422        6.367        -8.478       -0.863       -2.491       -0.375       -2.115       \n",
            "   [0]  reflects            0.333        5.469        -8.773       -1.101       -1.827       -0.333       -1.495       \n",
            "   [1]  pierre              0.361        0.000        0.000        0.000        -0.816       -0.298       -0.000       \n",
            "   [1]  richard             0.363        0.000        0.000        0.000        -0.916       -0.304       -0.000       \n",
            "   [0]  get                 0.358        3.479        -6.618       -1.028       -1.028       -0.313       -0.715       \n",
            "   [1]  gerard              0.342        0.000        0.000        0.000        0.000        -0.324       0.000        \n",
            "   [1]  depardieu           0.364        0.000        0.000        0.000        0.000        -0.331       0.000        \n",
            "   [1]  this                0.401        0.000        0.000        0.000        0.000        -0.347       0.000        \n",
            "   [1]  is                  0.431        0.000        0.000        0.000        0.000        -0.364       0.000        \n",
            "   [1]  the                 0.503        0.000        0.000        0.000        0.000        -0.379       0.000        \n",
            "   [1]  worst               0.454        0.000        0.000        0.000        0.000        -0.395       0.000        \n",
            "   [1]  remake              0.468        0.000        0.000        0.000        0.000        -0.379       0.000        \n",
            "   [1]  of                  0.565        0.000        0.000        0.000        0.000        -0.378       0.000        \n",
            "   [1]  a                   0.629        0.000        0.000        0.000        0.000        -0.401       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  has                 0.467        0.000        0.000        0.000        -2.186       -0.290       -0.000       \n",
            "   [0]  cowboys             0.436        4.939        -12.818      -0.830       -2.454       -0.305       -2.149       \n",
            "   [1]  be                  0.490        0.000        0.000        0.000        -1.823       -0.311       -0.000       \n",
            "   [1]  one                 0.511        0.000        0.000        0.000        -2.046       -0.358       -0.000       \n",
            "   [1]  of                  0.597        0.000        0.000        0.000        -2.298       -0.382       -0.000       \n",
            "   [0]  to                  0.652        6.058        -4.604       -0.428       -2.579       -0.433       -2.147       \n",
            "   [1]  all                 0.721        0.000        0.000        0.000        -2.415       -0.473       -0.000       \n",
            "   [0]  extreme             0.635        6.411        -8.262       -0.454       -2.712       -0.538       -2.173       \n",
            "   [0]  such                0.611        10.481       -6.822       -0.493       -2.534       -0.482       -2.052       \n",
            "   [1]  despite             0.588        0.000        0.000        0.000        -2.292       -0.453       -0.000       \n",
            "   [1]  its                 0.509        0.000        0.000        0.000        -2.573       -0.432       -0.000       \n",
            "   [1]  cheekiness          0.493        0.000        0.000        0.000        -2.888       -0.373       -0.000       \n",
            "   [0]  collection          0.391        4.491        -8.374       -0.939       -3.243       -0.354       -2.888       \n",
            "   [0]  ladylove            0.364        7.987        -12.416      -1.011       -2.586       -0.292       -2.293       \n",
            "   [1]  model               0.376        0.000        0.000        0.000        -1.768       -0.268       -0.000       \n",
            "   [1]  reproductions       0.387        0.000        0.000        0.000        -1.984       -0.276       -0.000       \n",
            "   [0]  hope                0.313        4.627        -7.610       -1.163       -2.228       -0.289       -1.939       \n",
            "   [1]  oil                 0.308        0.000        0.000        0.000        -1.196       -0.256       -0.000       \n",
            "   [0]  worst               0.261        12.490       -6.671       -1.343       -1.343       -0.251       -1.092       \n",
            "   [1]  its                 0.198        0.000        0.000        0.000        0.000        -0.229       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  japanese            0.586        5.612        -9.167       -0.535       -2.665       -0.297       -2.368       \n",
            "   [1]  anyone              0.586        0.000        0.000        0.000        -2.392       -0.338       -0.000       \n",
            "   [1]  else                0.477        0.000        0.000        0.000        -2.686       -0.355       -0.000       \n",
            "   [0]  boredom             0.461        6.181        -12.436      -0.774       -3.015       -0.354       -2.661       \n",
            "   [0]  and                 0.520        5.703        -3.825       -0.654       -2.516       -0.359       -2.157       \n",
            "   [0]  just                0.471        5.517        -5.452       -0.753       -2.091       -0.368       -1.723       \n",
            "   [0]  movie               0.517        10.122       -4.350       -0.660       -1.502       -0.364       -1.138       \n",
            "   [1]  that                0.589        0.000        0.000        0.000        -0.945       -0.376       -0.000       \n",
            "   [1]  all                 0.682        0.000        0.000        0.000        -1.061       -0.378       -0.000       \n",
            "   [1]  the                 0.732        0.000        0.000        0.000        -1.192       -0.387       -0.000       \n",
            "   [0]  the                 0.765        7.858        -3.477       -0.267       -1.338       -0.389       -0.949       \n",
            "   [1]  characters          0.739        0.000        0.000        0.000        -1.202       -0.386       -0.000       \n",
            "   [1]  ended               0.758        0.000        0.000        0.000        -1.349       -0.369       -0.000       \n",
            "   [0]  slowmotion          0.744        6.398        -13.106      -0.296       -1.515       -0.369       -1.146       \n",
            "   [0]  scott               0.710        4.545        -8.531       -0.343       -1.368       -0.361       -1.007       \n",
            "   [1]  the                 0.739        0.000        0.000        0.000        -1.151       -0.348       -0.000       \n",
            "   [0]  above               0.741        7.843        -8.703       -0.300       -1.292       -0.350       -0.941       \n",
            "   [1]  spots               0.680        0.000        0.000        0.000        -1.114       -0.347       -0.000       \n",
            "   [0]  century             0.534        5.275        -7.695       -0.627       -1.251       -0.329       -0.922       \n",
            "   [0]  circumvented        0.497        5.278        -13.683      -0.700       -0.700       -0.306       -0.394       \n",
            "Samples\n",
            "Sample 0 .  somebody sinister has seen the credit pleasant reflects pierre richard get gerard depardieu this is the worst remake of a\n",
            "Sample 1 .  has cowboys be one of to all extreme such despite its cheekiness collection ladylove model reproductions hope oil worst its\n",
            "Sample 2 .  japanese anyone else boredom and just movie that all the the characters ended slowmotion scott the above spots century circumvented\n",
            "\n",
            "\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 0 0 1 0 1 1 0 1 0...]...][[99 42 69849 69849 78 69849 19 9 69849 4...]...][[42 1125 228 78 10 19 9 1739 4 663...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 0 0 1 0 1 1 0 1 0...]...][[99 42 69849 69849 78 69849 19 9 69849 4...]...][[42 13 8 78 296 19 9 92 4 9...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 0 0 0 0 0 0 1 1...]...][[25473 69849 45 69849 69849 69849 69849 69849 69849 55...]...][[185 45 329 40 894 564 3 1902 55 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 0 0 0 0 0 0 1 1...]...][[25473 69849 45 69849 69849 69849 69849 69849 69849 55...]...][[953 45 11 5 630 2 13 12 55 15...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 7 174 2 1 301 4 22800 3 15795...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 23 174 19 12 301 4 22800 20 15795...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 37 174 1591 1077 301 4 22800 3176 15795...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "global_step: 294\n",
            " perplexity: 959.387\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            " percent of 3-grams captured: 0.154.\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            " percent of 2-grams captured: 0.578.\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            " percent of 4-grams captured: 0.024.\n",
            " geometric_avg: 0.129.\n",
            " arithmetic_avg: 0.252.\n",
            "global_step: 294\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67172\n",
            " G train loss: 155.09949\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 23 174 19 12 301 4 22800 20 15795...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7 5 23 174 19 12 301 4 22800 20...]...][[1 0 1 0 0 1 1 1 0 1...]...][[7 5 69849 174 69849 69849 301 4 22800 69849...]...][[5 21 174 5 2 301 4 22800 69518 15795...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  is                  0.489        0.000        0.000        0.000        -2.289       -0.299       -0.000       \n",
            "   [0]  t                   0.491        5.964        -5.596       -0.711       -2.569       -0.339       -2.230       \n",
            "   [1]  every               0.459        0.000        0.000        0.000        -2.087       -0.364       -0.000       \n",
            "   [0]  is                  0.469        4.980        -3.890       -0.757       -2.343       -0.363       -1.980       \n",
            "   [0]  a                   0.508        5.113        -3.988       -0.677       -1.780       -0.366       -1.413       \n",
            "   [1]  job                 0.576        0.000        0.000        0.000        -1.238       -0.379       -0.000       \n",
            "   [1]  to                  0.606        0.000        0.000        0.000        -1.390       -0.402       -0.000       \n",
            "   [1]  stimulate           0.597        0.000        0.000        0.000        -1.561       -0.419       -0.000       \n",
            "   [0]  nbb                 0.571        5.730        -12.585      -0.560       -1.752       -0.417       -1.335       \n",
            "   [1]  superficially       0.557        0.000        0.000        0.000        -1.338       -0.407       -0.000       \n",
            "   [1]  i                   0.565        0.000        0.000        0.000        -1.502       -0.401       -0.000       \n",
            "   [0]  i                   0.576        6.754        -4.285       -0.552       -1.686       -0.407       -1.280       \n",
            "   [1]  take                0.574        0.000        0.000        0.000        -1.273       -0.414       -0.000       \n",
            "   [1]  an                  0.605        0.000        0.000        0.000        -1.429       -0.411       -0.000       \n",
            "   [1]  ambitious           0.606        0.000        0.000        0.000        -1.605       -0.425       -0.000       \n",
            "   [0]  guy                 0.524        8.801        -7.565       -0.645       -1.802       -0.430       -1.372       \n",
            "   [1]  over                0.455        0.000        0.000        0.000        -1.298       -0.401       -0.000       \n",
            "   [0]  premchand           0.457        4.373        -12.504      -0.783       -1.457       -0.370       -1.088       \n",
            "   [0]  squeak              0.469        11.999       -12.635      -0.757       -0.757       -0.364       -0.393       \n",
            "   [1]  market              0.416        0.000        0.000        0.000        0.000        -0.366       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  getting             0.447        3.668        -8.000       -0.806       -4.678       -0.300       -4.378       \n",
            "   [0]  17                  0.424        6.258        -8.677       -0.857       -4.347       -0.297       -4.050       \n",
            "   [0]  this                0.453        5.438        -3.687       -0.792       -3.918       -0.300       -3.618       \n",
            "   [1]  the                 0.512        0.000        0.000        0.000        -3.509       -0.319       -0.000       \n",
            "   [0]  a                   0.560        6.465        -3.268       -0.580       -3.940       -0.356       -3.584       \n",
            "   [0]  takashi             0.505        4.440        -9.204       -0.682       -3.771       -0.387       -3.385       \n",
            "   [0]  phishing            0.488        4.033        -13.751      -0.718       -3.468       -0.371       -3.097       \n",
            "   [1]  ve                  0.497        0.000        0.000        0.000        -3.088       -0.359       -0.000       \n",
            "   [0]  in                  0.545        6.356        -4.486       -0.607       -3.467       -0.364       -3.103       \n",
            "   [0]  tan                 0.520        6.510        -12.785      -0.653       -3.211       -0.385       -2.826       \n",
            "   [0]  i                   0.538        5.741        -4.093       -0.620       -2.872       -0.379       -2.493       \n",
            "   [1]  i                   0.562        0.000        0.000        0.000        -2.528       -0.383       -0.000       \n",
            "   [0]  day                 0.551        6.288        -7.985       -0.596       -2.838       -0.391       -2.447       \n",
            "   [0]  time                0.556        7.844        -6.487       -0.587       -2.517       -0.385       -2.132       \n",
            "   [1]  lots                0.532        0.000        0.000        0.000        -2.167       -0.384       -0.000       \n",
            "   [0]  suffocates          0.495        4.286        -13.097      -0.702       -2.433       -0.373       -2.060       \n",
            "   [0]  and                 0.537        5.536        -4.489       -0.622       -1.942       -0.354       -1.588       \n",
            "   [0]  on                  0.580        7.340        -5.914       -0.545       -1.482       -0.369       -1.113       \n",
            "   [0]  disengagement       0.570        5.849        -12.845      -0.561       -1.052       -0.394       -0.658       \n",
            "   [0]  one                 0.576        3.942        -5.317       -0.551       -0.551       -0.399       -0.151       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  movie               0.510        0.000        0.000        0.000        -2.517       -0.308       -0.000       \n",
            "   [1]  was                 0.501        0.000        0.000        0.000        -2.826       -0.359       -0.000       \n",
            "   [1]  lacking             0.483        0.000        0.000        0.000        -3.173       -0.390       -0.000       \n",
            "   [0]  where               0.494        3.327        -6.635       -0.706       -3.562       -0.403       -3.159       \n",
            "   [0]  thrive              0.456        5.898        -10.996      -0.786       -3.207       -0.409       -2.798       \n",
            "   [0]  oddly               0.385        8.328        -8.872       -0.955       -2.718       -0.403       -2.315       \n",
            "   [1]  and                 0.430        0.000        0.000        0.000        -1.979       -0.376       -0.000       \n",
            "   [0]  hard                0.477        3.264        -7.914       -0.740       -2.222       -0.360       -1.862       \n",
            "   [0]  an                  0.530        11.120       -5.594       -0.634       -1.664       -0.371       -1.293       \n",
            "   [0]  movie               0.564        3.765        -4.277       -0.572       -1.157       -0.401       -0.756       \n",
            "   [1]  realism             0.588        0.000        0.000        0.000        -0.656       -0.433       -0.000       \n",
            "   [1]  the                 0.621        0.000        0.000        0.000        -0.737       -0.463       -0.000       \n",
            "   [1]  dialogue            0.557        0.000        0.000        0.000        -0.827       -0.490       -0.000       \n",
            "   [1]  was                 0.537        0.000        0.000        0.000        -0.928       -0.481       -0.000       \n",
            "   [1]  muddled             0.530        0.000        0.000        0.000        -1.042       -0.464       -0.000       \n",
            "   [0]  m                   0.486        9.725        -6.185       -0.721       -1.170       -0.451       -0.719       \n",
            "   [1]  and                 0.529        0.000        0.000        0.000        -0.505       -0.434       -0.000       \n",
            "   [1]  trying              0.496        0.000        0.000        0.000        -0.566       -0.432       -0.000       \n",
            "   [0]  a                   0.529        6.172        -3.545       -0.636       -0.636       -0.425       -0.211       \n",
            "   [1]  hard                0.561        0.000        0.000        0.000        0.000        -0.427       0.000        \n",
            "Samples\n",
            "Sample 0 .  is t every is a job to stimulate nbb superficially i i take an ambitious guy over premchand squeak market\n",
            "Sample 1 .  getting 17 this the a takashi phishing ve in tan i i day time lots suffocates and on disengagement one\n",
            "Sample 2 .  movie was lacking where thrive oddly and hard an movie realism the dialogue was muddled m and trying a hard\n",
            "\n",
            "\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 0 1 1 1 1 1 0 0 0...]...][[761 9365 69849 451 8 1844 15 26 69849 69849...]...][[9365 21650 451 8 1844 15 26 183 518 5809...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 0 1 1 1 1 1 0 0 0...]...][[761 9365 69849 451 8 1844 15 26 69849 69849...]...][[9365 35987 451 8 1844 15 26 188 33 5...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[1 1 0 0 1 1 0 0 1 1...]...][[31 84 9 69849 69849 2277 2 69849 69849 75...]...][[84 9 67 0 2277 2 171 3 75 319...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[1 1 0 0 1 1 0 0 1 1...]...][[31 84 9 69849 69849 2277 2 69849 69849 75...]...][[84 9 236 39326 2277 2 1 378 75 319...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 10 19 1 0 2 1023 31 664...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 77 126 73 126 2 1023 2431 2347...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 9 17 4 71 2 1023 13 44...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "global_step: 299\n",
            " perplexity: 935.495\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            " percent of 3-grams captured: 0.116.\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            " percent of 2-grams captured: 0.557.\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            " percent of 4-grams captured: 0.022.\n",
            " geometric_avg: 0.112.\n",
            " arithmetic_avg: 0.232.\n",
            "global_step: 299\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67236\n",
            " G train loss: 154.68829\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 77 126 73 126 2 1023 2431 2347...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 141 27 77 126 73 126 2 1023 2431...]...][[1 1 0 0 0 0 1 1 0 0...]...][[10 141 27 69849 69849 69849 69849 2 1023 69849...]...][[141 27 2152 74 30 71 2 1023 689 3...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  should              0.470        0.000        0.000        0.000        -2.845       -0.297       -0.000       \n",
            "   [1]  have                0.454        0.000        0.000        0.000        -3.194       -0.325       -0.000       \n",
            "   [0]  delivered           0.493        6.159        -10.063      -0.708       -3.586       -0.343       -3.243       \n",
            "   [0]  bad                 0.525        8.279        -6.468       -0.645       -3.230       -0.373       -2.858       \n",
            "   [0]  all                 0.576        6.308        -5.675       -0.552       -2.903       -0.398       -2.505       \n",
            "   [0]  me                  0.538        8.267        -6.061       -0.620       -2.640       -0.436       -2.203       \n",
            "   [1]  a                   0.549        0.000        0.000        0.000        -2.268       -0.427       -0.000       \n",
            "   [1]  former              0.558        0.000        0.000        0.000        -2.546       -0.419       -0.000       \n",
            "   [0]  problems            0.564        11.092       -8.606       -0.572       -2.858       -0.414       -2.444       \n",
            "   [0]  of                  0.619        12.053       -3.815       -0.480       -2.567       -0.414       -2.153       \n",
            "   [0]  karloff             0.559        12.150       -10.915      -0.581       -2.342       -0.443       -1.899       \n",
            "   [0]  already             0.540        13.425       -7.979       -0.616       -1.977       -0.418       -1.558       \n",
            "   [1]  the                 0.572        0.000        0.000        0.000        -1.528       -0.395       -0.000       \n",
            "   [1]  murder              0.553        0.000        0.000        0.000        -1.715       -0.400       -0.000       \n",
            "   [0]  the                 0.578        3.832        -3.213       -0.547       -1.926       -0.395       -1.531       \n",
            "   [0]  through             0.545        3.707        -7.755       -0.606       -1.548       -0.406       -1.142       \n",
            "   [0]  to                  0.567        11.002       -4.129       -0.567       -1.057       -0.387       -0.669       \n",
            "   [1]  dentist             0.557        0.000        0.000        0.000        -0.549       -0.392       -0.000       \n",
            "   [0]  have                0.540        3.942        -5.213       -0.617       -0.617       -0.387       -0.229       \n",
            "   [1]  the                 0.570        0.000        0.000        0.000        0.000        -0.377       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  is                  0.494        7.603        -2.572       -0.706       -3.508       -0.298       -3.210       \n",
            "   [0]  a                   0.515        11.092       -3.083       -0.663       -3.146       -0.334       -2.812       \n",
            "   [1]  is                  0.516        0.000        0.000        0.000        -2.787       -0.359       -0.000       \n",
            "   [0]  my                  0.540        11.951       -6.195       -0.616       -3.129       -0.374       -2.755       \n",
            "   [0]  film                0.556        11.712       -4.572       -0.588       -2.822       -0.388       -2.434       \n",
            "   [1]  to                  0.566        0.000        0.000        0.000        -2.508       -0.401       -0.000       \n",
            "   [1]  some                0.535        0.000        0.000        0.000        -2.816       -0.411       -0.000       \n",
            "   [0]  rated               0.446        4.027        -7.718       -0.807       -3.161       -0.405       -2.756       \n",
            "   [0]  gore                0.450        3.389        -9.041       -0.797       -2.642       -0.367       -2.275       \n",
            "   [0]  dance               0.445        12.322       -9.129       -0.810       -2.071       -0.344       -1.728       \n",
            "   [1]  in                  0.487        0.000        0.000        0.000        -1.416       -0.335       -0.000       \n",
            "   [0]  girlfriend          0.475        6.652        -9.216       -0.744       -1.590       -0.345       -1.244       \n",
            "   [1]  yuzna               0.492        0.000        0.000        0.000        -0.950       -0.356       -0.000       \n",
            "   [1]  has                 0.474        0.000        0.000        0.000        -1.066       -0.363       -0.000       \n",
            "   [1]  been                0.466        0.000        0.000        0.000        -1.197       -0.360       -0.000       \n",
            "   [0]  was                 0.473        8.693        -4.574       -0.748       -1.344       -0.350       -0.994       \n",
            "   [1]  as                  0.492        0.000        0.000        0.000        -0.669       -0.345       -0.000       \n",
            "   [0]  hate                0.472        3.532        -7.693       -0.751       -0.751       -0.349       -0.402       \n",
            "   [1]  director            0.474        0.000        0.000        0.000        0.000        -0.349       0.000        \n",
            "   [1]  society             0.416        0.000        0.000        0.000        0.000        -0.350       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  someone             0.447        0.000        0.000        0.000        -3.606       -0.300       -0.000       \n",
            "   [0]  brochures           0.433        6.688        -13.140      -0.836       -4.049       -0.325       -3.724       \n",
            "   [0]  movies              0.432        6.473        -6.023       -0.840       -3.606       -0.337       -3.269       \n",
            "   [1]  been                0.428        0.000        0.000        0.000        -3.106       -0.347       -0.000       \n",
            "   [0]  little              0.456        12.363       -7.199       -0.785       -3.487       -0.351       -3.136       \n",
            "   [0]  premiere            0.400        5.831        -9.143       -0.915       -3.033       -0.363       -2.670       \n",
            "   [1]  her                 0.377        0.000        0.000        0.000        -2.378       -0.351       -0.000       \n",
            "   [1]  life                0.387        0.000        0.000        0.000        -2.670       -0.342       -0.000       \n",
            "   [0]  what                0.461        3.778        -6.384       -0.775       -2.997       -0.342       -2.655       \n",
            "   [1]  is                  0.483        0.000        0.000        0.000        -2.495       -0.369       -0.000       \n",
            "   [0]  10                  0.461        7.126        -6.841       -0.775       -2.801       -0.390       -2.410       \n",
            "   [1]  a                   0.487        0.000        0.000        0.000        -2.274       -0.387       -0.000       \n",
            "   [0]  and                 0.515        8.849        -3.775       -0.663       -2.553       -0.384       -2.169       \n",
            "   [1]  teacher             0.515        0.000        0.000        0.000        -2.122       -0.388       -0.000       \n",
            "   [0]  waste               0.483        3.785        -8.174       -0.728       -2.382       -0.384       -1.999       \n",
            "   [0]  no                  0.492        3.635        -6.198       -0.709       -1.857       -0.370       -1.487       \n",
            "   [0]  much                0.475        8.407        -6.116       -0.745       -1.289       -0.363       -0.926       \n",
            "   [1]  for                 0.510        0.000        0.000        0.000        -0.611       -0.356       -0.000       \n",
            "   [0]  70mm                0.504        3.633        -13.663      -0.686       -0.686       -0.363       -0.323       \n",
            "   [1]  dance               0.477        0.000        0.000        0.000        0.000        -0.365       0.000        \n",
            "Samples\n",
            "Sample 0 .  should have delivered bad all me a former problems of karloff already the murder the through to dentist have the\n",
            "Sample 1 .  is a is my film to some rated gore dance in girlfriend yuzna has been was as hate director society\n",
            "Sample 2 .  someone brochures movies been little premiere her life what is 10 a and teacher waste no much for 70mm dance\n",
            "\n",
            "\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 1 0 1 1 1 1 1 0 0...]...][[47 2 1349 69849 19 10 5 92 8 69849...]...][[2 1349 115 19 10 5 92 8 5290 22...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 1 0 1 1 1 1 1 0 0...]...][[47 2 1349 69849 19 10 5 92 8 69849...]...][[2 1349 164 19 10 5 92 8 318 366...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 1 1 1 0 0 1 0...]...][[157 69849 4 69849 6756 439 78 69849 69849 33...]...][[4140 4 2666 6756 439 78 0 797 33 575...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 1 1 1 0 0 1 0...]...][[157 69849 4 69849 6756 439 78 69849 69849 33...]...][[3463 4 97 6756 439 78 5313 5 33 8183...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 85 7...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 2075 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 143 7...]...]\n",
            "I0310 00:32:12.858704 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "I0310 00:32:15.366347 139852867667712 supervisor.py:1099] global_step/sec: 0.28639\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "global_step: 304\n",
            " perplexity: 931.961\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            " percent of 3-grams captured: 0.139.\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            " percent of 2-grams captured: 0.547.\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            " percent of 4-grams captured: 0.023.\n",
            " geometric_avg: 0.120.\n",
            " arithmetic_avg: 0.236.\n",
            "global_step: 304\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67418\n",
            " G train loss: 154.74989\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 2075 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 634 2 181 17 31 91 2075...]...][[1 1 1 1 1 1 1 1 0 1...]...][[10 17 5 634 2 181 17 31 91 69849...]...][[17 5 634 2 181 17 31 91 8 7...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  movie               0.506        0.000        0.000        0.000        -1.113       -0.303       -0.000       \n",
            "   [1]  is                  0.507        0.000        0.000        0.000        -1.250       -0.343       -0.000       \n",
            "   [1]  basically           0.542        0.000        0.000        0.000        -1.403       -0.361       -0.000       \n",
            "   [1]  a                   0.556        0.000        0.000        0.000        -1.575       -0.374       -0.000       \n",
            "   [1]  action              0.590        0.000        0.000        0.000        -1.769       -0.380       -0.000       \n",
            "   [1]  movie               0.583        0.000        0.000        0.000        -1.985       -0.391       -0.000       \n",
            "   [1]  at                  0.565        0.000        0.000        0.000        -2.229       -0.396       -0.000       \n",
            "   [1]  its                 0.511        0.000        0.000        0.000        -2.503       -0.395       -0.000       \n",
            "   [0]  in                  0.537        14.286       -3.796       -0.621       -2.810       -0.373       -2.437       \n",
            "   [1]  it                  0.530        0.000        0.000        0.000        -2.457       -0.372       -0.000       \n",
            "   [0]  this                0.546        13.844       -4.003       -0.605       -2.758       -0.367       -2.391       \n",
            "   [0]  i                   0.549        8.995        -3.233       -0.599       -2.417       -0.370       -2.048       \n",
            "   [1]  depth               0.549        0.000        0.000        0.000        -2.042       -0.375       -0.000       \n",
            "   [0]  ed                  0.489        3.231        -8.504       -0.716       -2.292       -0.380       -1.912       \n",
            "   [0]  movie               0.504        6.274        -4.645       -0.684       -1.769       -0.369       -1.400       \n",
            "   [1]  but                 0.534        0.000        0.000        0.000        -1.218       -0.372       -0.000       \n",
            "   [0]  show                0.553        14.108       -6.652       -0.592       -1.367       -0.380       -0.987       \n",
            "   [1]  that                0.567        0.000        0.000        0.000        -0.870       -0.386       -0.000       \n",
            "   [0]  of                  0.611        4.928        -3.779       -0.493       -0.977       -0.386       -0.591       \n",
            "   [0]  noir                0.581        6.645        -7.773       -0.544       -0.544       -0.398       -0.146       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  guess               0.498        0.000        0.000        0.000        -2.146       -0.300       -0.000       \n",
            "   [1]  i                   0.495        0.000        0.000        0.000        -2.409       -0.357       -0.000       \n",
            "   [1]  can                 0.472        0.000        0.000        0.000        -2.705       -0.394       -0.000       \n",
            "   [0]  skepticism          0.446        6.278        -9.522       -0.808       -3.037       -0.395       -2.642       \n",
            "   [0]  cheesing            0.463        3.551        -13.221      -0.770       -2.502       -0.376       -2.126       \n",
            "   [1]  point               0.419        0.000        0.000        0.000        -1.945       -0.373       -0.000       \n",
            "   [1]  this                0.451        0.000        0.000        0.000        -2.183       -0.342       -0.000       \n",
            "   [1]  guy                 0.401        0.000        0.000        0.000        -2.451       -0.345       -0.000       \n",
            "   [0]  start               0.394        5.030        -7.789       -0.932       -2.752       -0.324       -2.428       \n",
            "   [1]  trying              0.377        0.000        0.000        0.000        -2.043       -0.312       -0.000       \n",
            "   [1]  to                  0.426        0.000        0.000        0.000        -2.294       -0.298       -0.000       \n",
            "   [1]  make                0.419        0.000        0.000        0.000        -2.575       -0.323       -0.000       \n",
            "   [0]  actors              0.426        5.674        -7.147       -0.852       -2.891       -0.332       -2.558       \n",
            "   [0]  suspense            0.376        6.066        -7.602       -0.979       -2.289       -0.342       -1.946       \n",
            "   [1]  why                 0.418        0.000        0.000        0.000        -1.471       -0.319       -0.000       \n",
            "   [1]  did                 0.418        0.000        0.000        0.000        -1.651       -0.336       -0.000       \n",
            "   [1]  it                  0.412        0.000        0.000        0.000        -1.854       -0.342       -0.000       \n",
            "   [0]  for                 0.455        5.335        -5.345       -0.788       -2.081       -0.338       -1.743       \n",
            "   [0]  d                   0.478        4.014        -7.408       -0.739       -1.451       -0.364       -1.087       \n",
            "   [0]  childhood           0.450        5.784        -8.887       -0.799       -0.799       -0.393       -0.407       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  am                  0.481        0.000        0.000        0.000        -2.965       -0.303       -0.000       \n",
            "   [0]  worst               0.423        9.052        -6.452       -0.859       -3.329       -0.343       -2.986       \n",
            "   [0]  somewhat            0.459        4.269        -8.281       -0.780       -2.772       -0.357       -2.415       \n",
            "   [1]  what                0.520        0.000        0.000        0.000        -2.237       -0.365       -0.000       \n",
            "   [0]  the                 0.547        10.158       -3.799       -0.604       -2.511       -0.388       -2.123       \n",
            "   [0]  cooney              0.526        9.632        -12.620      -0.643       -2.141       -0.407       -1.734       \n",
            "   [1]  was                 0.514        0.000        0.000        0.000        -1.682       -0.409       -0.000       \n",
            "   [1]  using               0.482        0.000        0.000        0.000        -1.889       -0.402       -0.000       \n",
            "   [0]  all                 0.537        4.586        -6.156       -0.621       -2.121       -0.389       -1.731       \n",
            "   [0]  new                 0.517        3.749        -6.747       -0.660       -1.684       -0.400       -1.284       \n",
            "   [1]  movie               0.527        0.000        0.000        0.000        -1.149       -0.407       -0.000       \n",
            "   [1]  and                 0.545        0.000        0.000        0.000        -1.290       -0.409       -0.000       \n",
            "   [1]  also                0.549        0.000        0.000        0.000        -1.448       -0.415       -0.000       \n",
            "   [1]  the                 0.568        0.000        0.000        0.000        -1.626       -0.418       -0.000       \n",
            "   [1]  caliber             0.578        0.000        0.000        0.000        -1.826       -0.422       -0.000       \n",
            "   [0]  doltish             0.554        4.325        -12.859      -0.591       -2.050       -0.432       -1.618       \n",
            "   [1]  the                 0.571        0.000        0.000        0.000        -1.637       -0.431       -0.000       \n",
            "   [0]  mustachioed         0.554        11.643       -12.785      -0.590       -1.838       -0.428       -1.410       \n",
            "   [0]  both                0.504        4.996        -8.102       -0.685       -1.401       -0.423       -0.978       \n",
            "   [0]  chinese             0.447        6.325        -8.598       -0.804       -0.804       -0.402       -0.402       \n",
            "Samples\n",
            "Sample 0 .  movie is basically a action movie at its in it this i depth ed movie but show that of noir\n",
            "Sample 1 .  guess i can skepticism cheesing point this guy start trying to make actors suspense why did it for d childhood\n",
            "Sample 2 .  am worst somewhat what the cooney was using all new movie and also the caliber doltish the mustachioed both chinese\n",
            "\n",
            "\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 1 1 0 0 1 1 0 0...]...][[264 69849 69849 10 17 69849 69849 16 804 69849...]...][[9 1281 10 17 2 223 16 804 2 605...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 1 1 0 0 1 1 0 0...]...][[264 69849 69849 10 17 69849 69849 16 804 69849...]...][[94 27 10 17 619 949 16 804 13221 484...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 1 1 0 1 1 1 0 1 1...]...][[9 139 110 30 69849 3 0 98 69849 10...]...][[139 110 30 668 3 0 98 8 10 202...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 1 1 0 1 1 1 0 1 1...]...][[9 139 110 30 69849 3 0 98 69849 10...]...][[139 110 30 123 3 0 98 153 10 202...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 153 1563 642 3 478 85 88 47357 38...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 2 1563 120 3 478 2124 338 98 38...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 480 1563 17552 3 478 817 838 7532 38...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "global_step: 309\n",
            " perplexity: 931.322\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            " percent of 3-grams captured: 0.158.\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            " percent of 2-grams captured: 0.540.\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            " percent of 4-grams captured: 0.023.\n",
            " geometric_avg: 0.126.\n",
            " arithmetic_avg: 0.240.\n",
            "global_step: 309\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67473\n",
            " G train loss: 154.00789\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 2 1563 120 3 478 2124 338 98 38...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 2 1563 120 3 478 2124 338 98...]...][[1 0 1 0 1 1 0 0 0 1...]...][[10 5 69849 1563 69849 3 478 69849 69849 69849...]...][[5 430 1563 526 3 478 17 1 23 38...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  is                  0.503        0.000        0.000        0.000        -3.021       -0.292       -0.000       \n",
            "   [0]  problem             0.510        3.807        -9.277       -0.674       -3.391       -0.335       -3.056       \n",
            "   [1]  rip                 0.419        0.000        0.000        0.000        -3.051       -0.370       -0.000       \n",
            "   [0]  self                0.369        6.917        -8.235       -0.997       -3.425       -0.292       -3.134       \n",
            "   [1]  of                  0.470        0.000        0.000        0.000        -2.726       -0.232       -0.000       \n",
            "   [1]  already             0.481        0.000        0.000        0.000        -3.060       -0.328       -0.000       \n",
            "   [0]  movie               0.490        10.877       -4.120       -0.714       -3.436       -0.361       -3.074       \n",
            "   [0]  and                 0.513        8.196        -3.820       -0.668       -3.056       -0.384       -2.672       \n",
            "   [0]  not                 0.503        5.661        -5.055       -0.687       -2.681       -0.408       -2.273       \n",
            "   [1]  like                0.497        0.000        0.000        0.000        -2.239       -0.397       -0.000       \n",
            "   [0]  format              0.447        8.300        -9.052       -0.806       -2.513       -0.379       -2.134       \n",
            "   [1]  and                 0.483        0.000        0.000        0.000        -1.917       -0.319       -0.000       \n",
            "   [1]  i                   0.499        0.000        0.000        0.000        -2.152       -0.341       -0.000       \n",
            "   [0]  version             0.496        6.725        -7.660       -0.702       -2.416       -0.367       -2.049       \n",
            "   [0]  about               0.513        6.425        -5.584       -0.667       -1.924       -0.368       -1.556       \n",
            "   [0]  overly              0.441        5.439        -9.091       -0.819       -1.412       -0.393       -1.019       \n",
            "   [1]  did                 0.436        0.000        0.000        0.000        -0.666       -0.311       -0.000       \n",
            "   [1]  last                0.406        0.000        0.000        0.000        -0.748       -0.285       -0.000       \n",
            "   [0]  locations           0.432        8.800        -10.326      -0.840       -0.840       -0.256       -0.584       \n",
            "   [1]  the                 0.484        0.000        0.000        0.000        0.000        -0.286       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  bland               0.493        11.335       -11.153      -0.708       -2.669       -0.291       -2.378       \n",
            "   [1]  is                  0.494        0.000        0.000        0.000        -2.202       -0.327       -0.000       \n",
            "   [1]  a                   0.516        0.000        0.000        0.000        -2.472       -0.345       -0.000       \n",
            "   [1]  jewel               0.479        0.000        0.000        0.000        -2.776       -0.368       -0.000       \n",
            "   [0]  reviews             0.461        10.422       -7.713       -0.774       -3.116       -0.346       -2.770       \n",
            "   [1]  trying              0.427        0.000        0.000        0.000        -2.630       -0.338       -0.000       \n",
            "   [0]  strange             0.408        4.939        -7.861       -0.896       -2.953       -0.318       -2.635       \n",
            "   [1]  break               0.415        0.000        0.000        0.000        -2.310       -0.304       -0.000       \n",
            "   [0]  b                   0.417        6.633        -7.445       -0.875       -2.593       -0.308       -2.285       \n",
            "   [1]  society             0.370        0.000        0.000        0.000        -1.929       -0.316       -0.000       \n",
            "   [0]  hot                 0.367        4.924        -8.179       -1.003       -2.165       -0.294       -1.871       \n",
            "   [1]  the                 0.427        0.000        0.000        0.000        -1.304       -0.300       -0.000       \n",
            "   [1]  last                0.393        0.000        0.000        0.000        -1.464       -0.342       -0.000       \n",
            "   [1]  of                  0.483        0.000        0.000        0.000        -1.644       -0.326       -0.000       \n",
            "   [0]  for                 0.524        11.498       -5.480       -0.647       -1.846       -0.368       -1.478       \n",
            "   [0]  scream              0.477        11.556       -8.517       -0.740       -1.346       -0.386       -0.960       \n",
            "   [0]  times               0.506        4.358        -7.807       -0.680       -0.680       -0.341       -0.339       \n",
            "   [1]  1937                0.504        0.000        0.000        0.000        0.000        -0.352       0.000        \n",
            "   [1]  film                0.522        0.000        0.000        0.000        0.000        -0.350       0.000        \n",
            "   [1]  from                0.534        0.000        0.000        0.000        0.000        -0.361       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  br                  0.548        0.000        0.000        0.000        -3.304       -0.293       -0.000       \n",
            "   [1]  br                  0.571        0.000        0.000        0.000        -3.709       -0.333       -0.000       \n",
            "   [0]  is                  0.560        6.265        -2.995       -0.580       -4.164       -0.360       -3.804       \n",
            "   [0]  the                 0.578        6.315        -2.774       -0.549       -4.024       -0.362       -3.662       \n",
            "   [0]  feel                0.560        7.659        -8.548       -0.580       -3.902       -0.363       -3.539       \n",
            "   [0]  around              0.534        3.641        -8.658       -0.628       -3.729       -0.354       -3.375       \n",
            "   [1]  save                0.483        0.000        0.000        0.000        -3.482       -0.339       -0.000       \n",
            "   [0]  i                   0.503        5.784        -2.763       -0.688       -3.909       -0.322       -3.587       \n",
            "   [0]  point               0.458        3.458        -7.958       -0.780       -3.616       -0.327       -3.289       \n",
            "   [0]  pretty              0.504        4.696        -7.254       -0.684       -3.184       -0.326       -2.858       \n",
            "   [0]  i                   0.523        5.437        -2.753       -0.648       -2.806       -0.339       -2.467       \n",
            "   [0]  i                   0.530        5.435        -2.752       -0.634       -2.423       -0.356       -2.067       \n",
            "   [0]  as                  0.540        6.060        -4.694       -0.616       -2.009       -0.366       -1.642       \n",
            "   [1]  needs               0.540        0.000        0.000        0.000        -1.564       -0.367       -0.000       \n",
            "   [0]  last                0.490        3.612        -6.601       -0.713       -1.756       -0.364       -1.392       \n",
            "   [0]  should              0.482        5.534        -7.303       -0.729       -1.171       -0.348       -0.823       \n",
            "   [1]  many                0.484        0.000        0.000        0.000        -0.495       -0.339       -0.000       \n",
            "   [1]  more                0.486        0.000        0.000        0.000        -0.556       -0.342       -0.000       \n",
            "   [0]  for                 0.536        15.002       -5.183       -0.625       -0.625       -0.346       -0.279       \n",
            "   [1]  between             0.567        0.000        0.000        0.000        0.000        -0.358       0.000        \n",
            "Samples\n",
            "Sample 0 .  is problem rip self of already movie and not like format and i version about overly did last locations the\n",
            "Sample 1 .  bland is a jewel reviews trying strange break b society hot the last of for scream times 1937 film from\n",
            "Sample 2 .  br br is the feel around save i point pretty i i as needs last should many more for between\n",
            "\n",
            "\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[0 0 1 1 1 0 0 0 1 1...]...][[29 69849 69849 250 98 123 69849 69849 69849 8...]...][[3 0 250 98 123 92 23 64 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[0 0 1 1 1 0 0 0 1 1...]...][[29 69849 69849 250 98 123 69849 69849 69849 8...]...][[5 123 250 98 123 0 28963 0 8 0...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 0 1 0 1 1 0 0 0...]...][[10 2789 343 69849 451 69849 4 91 69849 69849...]...][[2789 343 201 451 55 4 91 6380 7745 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 0 1 0 1 1 0 0 0...]...][[10 2789 343 69849 451 69849 4 91 69849 69849...]...][[2789 343 16 451 2707 4 91 811 1255 6...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 5 11 99 147 1090...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 136 11 99 816 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 2040 11 99 49722 44...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "global_step: 314\n",
            " perplexity: 923.597\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            " percent of 3-grams captured: 0.139.\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            " percent of 2-grams captured: 0.579.\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            " percent of 4-grams captured: 0.027.\n",
            " geometric_avg: 0.130.\n",
            " arithmetic_avg: 0.249.\n",
            "global_step: 314\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67461\n",
            " G train loss: 151.97369\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 136 11 99 816 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[84 3 30 287 71 42 136 11 99 816...]...][[1 1 1 1 1 0 1 1 0 0...]...][[84 3 30 287 71 42 69849 11 99 69849...]...][[3 30 287 71 42 0 11 99 58313 1...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  of                  0.574        0.000        0.000        0.000        -1.013       -0.300       -0.000       \n",
            "   [1]  all                 0.618        0.000        0.000        0.000        -1.137       -0.379       -0.000       \n",
            "   [1]  let                 0.615        0.000        0.000        0.000        -1.277       -0.420       -0.000       \n",
            "   [1]  me                  0.582        0.000        0.000        0.000        -1.434       -0.410       -0.000       \n",
            "   [1]  just                0.533        0.000        0.000        0.000        -1.609       -0.368       -0.000       \n",
            "   [0]  the                 0.571        6.453        -3.033       -0.560       -1.807       -0.322       -1.485       \n",
            "   [1]  that                0.589        0.000        0.000        0.000        -1.400       -0.338       -0.000       \n",
            "   [1]  after               0.574        0.000        0.000        0.000        -1.572       -0.345       -0.000       \n",
            "   [0]  digressive          0.560        8.290        -14.036      -0.580       -1.764       -0.332       -1.432       \n",
            "   [0]  and                 0.577        4.420        -3.427       -0.550       -1.330       -0.320       -1.011       \n",
            "   [0]  old                 0.530        4.829        -6.489       -0.634       -0.876       -0.333       -0.543       \n",
            "   [1]  i                   0.540        0.000        0.000        0.000        -0.271       -0.303       0.000        \n",
            "   [1]  seriously           0.505        0.000        0.000        0.000        -0.304       -0.314       0.000        \n",
            "   [1]  thought             0.489        0.000        0.000        0.000        -0.342       -0.303       -0.000       \n",
            "   [1]  about               0.517        0.000        0.000        0.000        -0.384       -0.311       -0.000       \n",
            "   [1]  writing             0.484        0.000        0.000        0.000        -0.431       -0.345       -0.000       \n",
            "   [1]  my                  0.532        0.000        0.000        0.000        -0.484       -0.337       -0.000       \n",
            "   [1]  own                 0.546        0.000        0.000        0.000        -0.543       -0.366       -0.000       \n",
            "   [1]  screenplay          0.499        0.000        0.000        0.000        -0.610       -0.380       -0.000       \n",
            "   [0]  poorly              0.504        5.913        -8.697       -0.684       -0.684       -0.353       -0.332       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  is                  0.557        6.965        -2.945       -0.586       -3.331       -0.301       -3.030       \n",
            "   [1]  seeing              0.579        0.000        0.000        0.000        -3.082       -0.337       -0.000       \n",
            "   [0]  from                0.588        4.007        -5.657       -0.531       -3.460       -0.353       -3.106       \n",
            "   [0]  sj                  0.569        3.853        -12.887      -0.563       -3.289       -0.350       -2.938       \n",
            "   [0]  if                  0.553        5.050        -6.510       -0.592       -3.060       -0.336       -2.724       \n",
            "   [0]  top                 0.514        10.374       -7.854       -0.665       -2.771       -0.319       -2.452       \n",
            "   [1]  when                0.541        0.000        0.000        0.000        -2.364       -0.316       -0.000       \n",
            "   [0]  who                 0.562        6.820        -6.257       -0.577       -2.654       -0.324       -2.330       \n",
            "   [1]  32                  0.566        0.000        0.000        0.000        -2.332       -0.335       -0.000       \n",
            "   [0]  ms                  0.601        4.964        -11.232      -0.510       -2.618       -0.336       -2.282       \n",
            "   [1]  chicago             0.525        0.000        0.000        0.000        -2.367       -0.330       -0.000       \n",
            "   [0]  act                 0.518        4.957        -8.592       -0.658       -2.658       -0.312       -2.346       \n",
            "   [1]  broadcasting        0.484        0.000        0.000        0.000        -2.245       -0.315       -0.000       \n",
            "   [0]  compared            0.486        4.023        -8.161       -0.721       -2.521       -0.328       -2.193       \n",
            "   [1]  final               0.489        0.000        0.000        0.000        -2.021       -0.346       -0.000       \n",
            "   [0]  experience          0.521        7.880        -7.929       -0.652       -2.269       -0.370       -1.899       \n",
            "   [0]  cite                0.514        4.396        -13.381      -0.665       -1.815       -0.390       -1.425       \n",
            "   [0]  vhs                 0.489        6.675        -8.824       -0.715       -1.291       -0.395       -0.896       \n",
            "   [0]  film                0.524        6.840        -5.093       -0.647       -0.647       -0.396       -0.251       \n",
            "   [1]  school              0.568        0.000        0.000        0.000        0.000        -0.392       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  douglas             0.451        2.946        -8.774       -0.797       -3.447       -0.297       -3.151       \n",
            "   [1]  a                   0.491        0.000        0.000        0.000        -2.975       -0.315       -0.000       \n",
            "   [1]  horrible            0.437        0.000        0.000        0.000        -3.340       -0.351       -0.000       \n",
            "   [0]  tv                  0.423        7.815        -6.796       -0.860       -3.750       -0.348       -3.402       \n",
            "   [0]  atmosphere          0.438        8.391        -9.871       -0.825       -3.245       -0.356       -2.889       \n",
            "   [1]  but                 0.482        0.000        0.000        0.000        -2.717       -0.372       -0.000       \n",
            "   [1]  somehow             0.457        0.000        0.000        0.000        -3.050       -0.390       -0.000       \n",
            "   [0]  series              0.435        4.086        -7.208       -0.832       -3.424       -0.384       -3.040       \n",
            "   [0]  and                 0.473        4.259        -3.960       -0.748       -2.910       -0.376       -2.534       \n",
            "   [1]  also                0.497        0.000        0.000        0.000        -2.427       -0.388       -0.000       \n",
            "   [0]  all                 0.560        7.470        -5.894       -0.579       -2.725       -0.390       -2.334       \n",
            "   [0]  keeps               0.472        8.172        -8.678       -0.751       -2.409       -0.406       -2.003       \n",
            "   [0]  cinematography      0.423        6.015        -7.966       -0.860       -1.862       -0.367       -1.495       \n",
            "   [1]  are                 0.459        0.000        0.000        0.000        -1.125       -0.352       -0.000       \n",
            "   [0]  and                 0.505        6.940        -3.974       -0.683       -1.263       -0.371       -0.892       \n",
            "   [1]  kinds               0.496        0.000        0.000        0.000        -0.651       -0.390       -0.000       \n",
            "   [0]  fact                0.482        4.223        -7.301       -0.731       -0.731       -0.385       -0.346       \n",
            "   [1]  sci                 0.482        0.000        0.000        0.000        0.000        -0.374       0.000        \n",
            "   [1]  fi                  0.476        0.000        0.000        0.000        0.000        -0.368       0.000        \n",
            "   [1]  good                0.420        0.000        0.000        0.000        0.000        -0.364       0.000        \n",
            "Samples\n",
            "Sample 0 .  of all let me just the that after digressive and old i seriously thought about writing my own screenplay poorly\n",
            "Sample 1 .  is seeing from sj if top when who 32 ms chicago act broadcasting compared final experience cite vhs film school\n",
            "Sample 2 .  douglas a horrible tv atmosphere but somehow series and also all keeps cinematography are and kinds fact sci fi good\n",
            "\n",
            "\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[1 0 0 0 0 1 1 0 1 1...]...][[10 17 69849 69849 69849 69849 436 1 69849 868...]...][[17 32756 199 3895 5998 436 1 4242 868 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[1 0 0 0 0 1 1 0 1 1...]...][[10 17 69849 69849 69849 69849 436 1 69849 868...]...][[17 980 545 583 31 436 1 58 868 6...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 1 0 1 1 0 1 0 0 1...]...][[9 140 2 69849 3 193 69849 5920 69849 69849...]...][[140 2 340 3 193 156 5920 257 14340 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 1 0 1 1 0 1 0 0 1...]...][[9 140 2 69849 3 193 69849 5920 69849 69849...]...][[140 2 810 3 193 16 5920 598 959 1...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 10 333 10 160 9 91 4 663...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 228 78 10 19 9 1739 4 663...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 45 11 10 20 9 31269 4 663...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "global_step: 319\n",
            " perplexity: 914.941\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            " percent of 3-grams captured: 0.147.\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            " percent of 2-grams captured: 0.574.\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.121.\n",
            " arithmetic_avg: 0.247.\n",
            "global_step: 319\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67553\n",
            " G train loss: 151.54874\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 228 78 10 19 9 1739 4 663...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 42 1125 228 78 10 19 9 1739 4...]...][[1 1 0 0 1 0 1 0 1 1...]...][[99 42 1125 69849 69849 10 69849 9 69849 4...]...][[42 1125 0 27 10 0 9 3 4 663...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  just                0.466        0.000        0.000        0.000        -3.300       -0.298       -0.000       \n",
            "   [1]  15                  0.353        0.000        0.000        0.000        -3.705       -0.322       -0.000       \n",
            "   [0]  the                 0.390        8.635        -2.694       -0.942       -4.160       -0.339       -3.821       \n",
            "   [0]  have                0.382        6.764        -5.255       -0.962       -3.613       -0.380       -3.233       \n",
            "   [1]  this                0.411        0.000        0.000        0.000        -2.976       -0.402       -0.000       \n",
            "   [0]  the                 0.456        4.694        -2.466       -0.786       -3.341       -0.412       -2.929       \n",
            "   [1]  i                   0.464        0.000        0.000        0.000        -2.869       -0.407       -0.000       \n",
            "   [0]  of                  0.560        8.813        -3.502       -0.579       -3.221       -0.388       -2.834       \n",
            "   [1]  to                  0.593        0.000        0.000        0.000        -2.966       -0.374       -0.000       \n",
            "   [1]  miss                0.574        0.000        0.000        0.000        -3.330       -0.338       -0.000       \n",
            "   [0]  remarkably          0.485        11.893       -11.390      -0.723       -3.739       -0.294       -3.445       \n",
            "   [0]  me                  0.477        13.317       -6.261       -0.740       -3.385       -0.252       -3.134       \n",
            "   [0]  has                 0.469        4.441        -5.775       -0.757       -2.970       -0.255       -2.714       \n",
            "   [0]  not                 0.485        8.147        -4.877       -0.724       -2.484       -0.276       -2.208       \n",
            "   [1]  more                0.489        0.000        0.000        0.000        -1.976       -0.307       -0.000       \n",
            "   [0]  director            0.504        15.442       -6.930       -0.686       -2.218       -0.324       -1.894       \n",
            "   [0]  not                 0.512        5.975        -4.877       -0.669       -1.721       -0.337       -1.385       \n",
            "   [0]  is                  0.523        4.229        -3.992       -0.648       -1.181       -0.343       -0.838       \n",
            "   [0]  wish                0.550        8.159        -9.598       -0.599       -0.599       -0.341       -0.258       \n",
            "   [1]  at                  0.564        0.000        0.000        0.000        0.000        -0.339       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  taking              0.446        0.000        0.000        0.000        -3.087       -0.296       -0.000       \n",
            "   [0]  seen                0.513        7.017        -6.392       -0.667       -3.466       -0.308       -3.158       \n",
            "   [1]  for                 0.566        0.000        0.000        0.000        -3.142       -0.361       -0.000       \n",
            "   [0]  it                  0.542        3.257        -4.457       -0.613       -3.528       -0.390       -3.138       \n",
            "   [1]  santa               0.456        0.000        0.000        0.000        -3.273       -0.363       -0.000       \n",
            "   [0]  1981                0.470        11.911       -10.900      -0.755       -3.674       -0.314       -3.360       \n",
            "   [0]  again               0.448        4.731        -7.078       -0.804       -3.277       -0.315       -2.962       \n",
            "   [0]  to                  0.503        8.002        -4.253       -0.686       -2.776       -0.310       -2.466       \n",
            "   [1]  i                   0.522        0.000        0.000        0.000        -2.346       -0.346       -0.000       \n",
            "   [0]  version             0.511        6.516        -7.318       -0.672       -2.634       -0.359       -2.275       \n",
            "   [0]  in                  0.558        7.725        -4.625       -0.583       -2.203       -0.347       -1.856       \n",
            "   [1]  of                  0.658        0.000        0.000        0.000        -1.818       -0.358       -0.000       \n",
            "   [0]  tradition           0.560        4.164        -8.553       -0.579       -2.041       -0.393       -1.649       \n",
            "   [1]  movie               0.561        0.000        0.000        0.000        -1.642       -0.321       -0.000       \n",
            "   [0]  i                   0.580        7.880        -3.971       -0.545       -1.843       -0.311       -1.533       \n",
            "   [0]  occupation          0.575        5.651        -13.035      -0.553       -1.457       -0.322       -1.136       \n",
            "   [0]  schwentke           0.551        7.759        -10.529      -0.596       -1.015       -0.319       -0.696       \n",
            "   [1]  i                   0.567        0.000        0.000        0.000        -0.471       -0.307       -0.000       \n",
            "   [1]  read                0.557        0.000        0.000        0.000        -0.528       -0.320       -0.000       \n",
            "   [0]  aircrew             0.553        4.295        -13.120      -0.593       -0.593       -0.314       -0.279       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  characters          0.477        4.746        -7.458       -0.740       -3.251       -0.297       -2.954       \n",
            "   [0]  minute              0.426        3.766        -7.885       -0.852       -2.818       -0.335       -2.483       \n",
            "   [1]  more                0.417        0.000        0.000        0.000        -2.207       -0.366       -0.000       \n",
            "   [0]  hollywood           0.466        8.336        -7.665       -0.764       -2.478       -0.385       -2.093       \n",
            "   [0]  lord                0.506        7.073        -8.659       -0.681       -1.924       -0.383       -1.541       \n",
            "   [0]  would               0.527        7.854        -6.585       -0.641       -1.395       -0.378       -1.018       \n",
            "   [1]  movies              0.517        0.000        0.000        0.000        -0.846       -0.359       -0.000       \n",
            "   [1]  is                  0.525        0.000        0.000        0.000        -0.950       -0.346       -0.000       \n",
            "   [1]  definitely          0.518        0.000        0.000        0.000        -1.067       -0.328       -0.000       \n",
            "   [1]  smart               0.402        0.000        0.000        0.000        -1.198       -0.321       -0.000       \n",
            "   [1]  house               0.407        0.000        0.000        0.000        -1.345       -0.355       -0.000       \n",
            "   [1]  this                0.457        0.000        0.000        0.000        -1.510       -0.366       -0.000       \n",
            "   [0]  when                0.493        8.542        -6.236       -0.706       -1.695       -0.368       -1.327       \n",
            "   [1]  a                   0.535        0.000        0.000        0.000        -1.110       -0.376       -0.000       \n",
            "   [1]  family              0.519        0.000        0.000        0.000        -1.246       -0.367       -0.000       \n",
            "   [0]  collection          0.411        8.720        -7.905       -0.888       -1.399       -0.360       -1.039       \n",
            "   [1]  a                   0.458        0.000        0.000        0.000        -0.574       -0.381       -0.000       \n",
            "   [1]  contest             0.486        0.000        0.000        0.000        -0.644       -0.369       -0.000       \n",
            "   [0]  tyson               0.485        4.048        -11.524      -0.723       -0.723       -0.372       -0.351       \n",
            "   [1]  live                0.423        0.000        0.000        0.000        0.000        -0.376       0.000        \n",
            "Samples\n",
            "Sample 0 .  just 15 the have this the i of to miss remarkably me has not more director not is wish at\n",
            "Sample 1 .  taking seen for it santa 1981 again to i version in of tradition movie i occupation schwentke i read aircrew\n",
            "Sample 2 .  characters minute more hollywood lord would movies is definitely smart house this when a family collection a contest tyson live\n",
            "\n",
            "\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[1 1 0 0 0 0 1 0 1 1...]...][[16 2 366 69849 69849 69849 69849 149 69849 17...]...][[2 366 9 80 23 395 149 10 17 8386...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[1 1 0 0 0 0 1 0 1 1...]...][[16 2 366 69849 69849 69849 69849 149 69849 17...]...][[2 366 5 112 5 8 149 173 17 8386...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[0 0 0 1 0 0 0 1 1 0...]...][[8 69849 69849 69849 36433 69849 69849 69849 1309 0...]...][[47 5 1640 36433 117 166 4 1309 0 4853...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[0 0 0 1 0 0 0 1 1 0...]...][[8 69849 69849 69849 36433 69849 69849 69849 1309 0...]...][[3041 3 76 36433 924 22530 0 1309 0 7...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[529 45 329 40 717 36 3 48 714 15...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[185 45 329 40 894 564 3 1902 55 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[13890 45 329 40 9 26 3 4 9 15...]...]\n",
            "I0310 00:33:12.858769 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "global_step: 324\n",
            " perplexity: 907.234\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            " percent of 3-grams captured: 0.143.\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            " percent of 2-grams captured: 0.564.\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.119.\n",
            " arithmetic_avg: 0.243.\n",
            "global_step: 324\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67459\n",
            " G train loss: 154.13297\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[185 45 329 40 894 564 3 1902 55 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[25473 185 45 329 40 894 564 3 1902 55...]...][[0 1 1 1 0 0 1 0 0 1...]...][[25473 69849 45 329 40 69849 69849 3 69849 69849...]...][[6 45 329 40 2645 19 3 1156 67 15...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  br                  0.542        7.584        -5.646       -0.612       -2.750       -0.304       -2.446       \n",
            "   [1]  has                 0.503        0.000        0.000        0.000        -2.400       -0.364       -0.000       \n",
            "   [1]  used                0.472        0.000        0.000        0.000        -2.695       -0.365       -0.000       \n",
            "   [1]  her                 0.437        0.000        0.000        0.000        -3.025       -0.365       -0.000       \n",
            "   [0]  25                  0.361        9.599        -9.186       -1.020       -3.396       -0.365       -3.031       \n",
            "   [0]  film                0.417        7.678        -5.095       -0.874       -2.668       -0.365       -2.303       \n",
            "   [1]  of                  0.551        0.000        0.000        0.000        -2.015       -0.401       -0.000       \n",
            "   [0]  starring            0.519        8.102        -7.681       -0.656       -2.262       -0.437       -1.825       \n",
            "   [0]  had                 0.495        6.391        -5.805       -0.703       -1.804       -0.409       -1.394       \n",
            "   [1]  with                0.597        0.000        0.000        0.000        -1.235       -0.374       -0.000       \n",
            "   [1]  two                 0.565        0.000        0.000        0.000        -1.387       -0.384       -0.000       \n",
            "   [1]  autistic            0.582        0.000        0.000        0.000        -1.557       -0.351       -0.000       \n",
            "   [0]  show                0.607        7.795        -6.861       -0.499       -1.748       -0.338       -1.410       \n",
            "   [0]  real                0.576        5.192        -7.357       -0.551       -1.402       -0.335       -1.067       \n",
            "   [0]  or                  0.613        3.995        -5.587       -0.489       -0.955       -0.307       -0.648       \n",
            "   [0]  think               0.593        11.136       -6.704       -0.523       -0.523       -0.309       -0.214       \n",
            "   [1]  she                 0.511        0.000        0.000        0.000        0.000        -0.298       0.000        \n",
            "   [1]  describes           0.477        0.000        0.000        0.000        0.000        -0.273       0.000        \n",
            "   [1]  as                  0.497        0.000        0.000        0.000        0.000        -0.280       0.000        \n",
            "   [1]  a                   0.540        0.000        0.000        0.000        0.000        -0.317       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  in                  0.535        0.000        0.000        0.000        -1.856       -0.301       -0.000       \n",
            "   [1]  mind                0.505        0.000        0.000        0.000        -2.084       -0.327       -0.000       \n",
            "   [0]  of                  0.596        4.887        -4.066       -0.517       -2.339       -0.339       -2.001       \n",
            "   [0]  seeing              0.621        6.419        -7.063       -0.476       -2.046       -0.313       -1.732       \n",
            "   [0]  bastardizes         0.603        7.961        -13.648      -0.506       -1.762       -0.294       -1.468       \n",
            "   [0]  romance             0.524        5.566        -8.028       -0.646       -1.410       -0.270       -1.140       \n",
            "   [1]  always              0.554        0.000        0.000        0.000        -0.858       -0.263       -0.000       \n",
            "   [1]  serious             0.584        0.000        0.000        0.000        -0.964       -0.259       -0.000       \n",
            "   [1]  ha                  0.585        0.000        0.000        0.000        -1.082       -0.256       -0.000       \n",
            "   [1]  ha                  0.583        0.000        0.000        0.000        -1.214       -0.253       -0.000       \n",
            "   [0]  steven              0.618        9.170        -9.122       -0.481       -1.363       -0.251       -1.113       \n",
            "   [1]  ha                  0.615        0.000        0.000        0.000        -0.991       -0.250       -0.000       \n",
            "   [0]  to                  0.641        9.164        -4.094       -0.444       -1.113       -0.248       -0.865       \n",
            "   [0]  makes               0.628        9.163        -7.805       -0.465       -0.751       -0.240       -0.511       \n",
            "   [1]  ha                  0.621        0.000        0.000        0.000        -0.320       -0.235       -0.000       \n",
            "   [1]  ha                  0.616        0.000        0.000        0.000        -0.359       -0.229       -0.000       \n",
            "   [1]  ha                  0.613        0.000        0.000        0.000        -0.404       -0.226       -0.000       \n",
            "   [1]  ha                  0.610        0.000        0.000        0.000        -0.453       -0.228       -0.000       \n",
            "   [0]  s                   0.601        4.510        -4.809       -0.509       -0.509       -0.233       -0.276       \n",
            "   [1]  was                 0.598        0.000        0.000        0.000        0.000        -0.245       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  making              0.484        4.446        -8.293       -0.726       -2.719       -0.290       -2.429       \n",
            "   [0]  1997                0.454        4.743        -10.547      -0.790       -2.237       -0.318       -1.919       \n",
            "   [0]  minutes             0.461        6.628        -7.587       -0.774       -1.625       -0.331       -1.294       \n",
            "   [1]  to                  0.512        0.000        0.000        0.000        -0.956       -0.352       -0.000       \n",
            "   [1]  be                  0.554        0.000        0.000        0.000        -1.073       -0.388       -0.000       \n",
            "   [1]  said                0.614        0.000        0.000        0.000        -1.205       -0.414       -0.000       \n",
            "   [1]  about               0.623        0.000        0.000        0.000        -1.353       -0.435       -0.000       \n",
            "   [1]  high                0.584        0.000        0.000        0.000        -1.519       -0.416       -0.000       \n",
            "   [0]  city                0.561        8.386        -7.996       -0.578       -1.705       -0.359       -1.346       \n",
            "   [1]  confidential        0.552        0.000        0.000        0.000        -1.266       -0.320       -0.000       \n",
            "   [0]  of                  0.660        3.452        -3.897       -0.416       -1.421       -0.304       -1.117       \n",
            "   [1]  teen                0.573        0.000        0.000        0.000        -1.129       -0.372       -0.000       \n",
            "   [1]  exploitation        0.560        0.000        0.000        0.000        -1.267       -0.301       -0.000       \n",
            "   [0]  not                 0.565        4.683        -4.839       -0.571       -1.422       -0.288       -1.134       \n",
            "   [1]  from                0.595        0.000        0.000        0.000        -0.956       -0.298       -0.000       \n",
            "   [0]  makes               0.593        2.940        -7.710       -0.522       -1.074       -0.323       -0.751       \n",
            "   [1]  end                 0.543        0.000        0.000        0.000        -0.619       -0.325       -0.000       \n",
            "   [1]  of                  0.648        0.000        0.000        0.000        -0.695       -0.294       -0.000       \n",
            "   [0]  i                   0.649        2.941        -3.321       -0.432       -0.780       -0.364       -0.416       \n",
            "   [0]  the                 0.676        11.574       -2.938       -0.391       -0.391       -0.366       -0.025       \n",
            "Samples\n",
            "Sample 0 .  br has used her 25 film of starring had with two autistic show real or think she describes as a\n",
            "Sample 1 .  in mind of seeing bastardizes romance always serious ha ha steven ha to makes ha ha ha ha s was\n",
            "Sample 2 .  making 1997 minutes to be said about high city confidential of teen exploitation not from makes end of i the\n",
            "\n",
            "\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[0 0 1 0 1 1 1 0 0 1...]...][[10 69849 69849 677 69849 1753 4924 4924 69849 69849...]...][[17 5 677 6798 1753 4924 4924 15 1753 1761...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[0 0 1 0 1 1 1 0 0 1...]...][[10 69849 69849 677 69849 1753 4924 4924 69849 69849...]...][[10 40 677 64 1753 4924 4924 9 95 1761...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 1 1 0 1 1 0 1 1 0...]...][[9 509 127 738 69849 884 127 69849 34 25...]...][[509 127 738 255 884 127 738 34 25 23...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 1 1 0 1 1 0 1 1 0...]...][[9 509 127 738 69849 884 127 69849 34 25...]...][[509 127 738 23 884 127 645 34 25 232...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 1102 20 1844 9 26 183 518 5809...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 451 8 1844 15 26 183 518 5809...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 167 813 1844 1382 26 183 518 5809...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "global_step: 329\n",
            " perplexity: 906.757\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            " percent of 3-grams captured: 0.153.\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            " percent of 2-grams captured: 0.569.\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            " percent of 4-grams captured: 0.035.\n",
            " geometric_avg: 0.145.\n",
            " arithmetic_avg: 0.252.\n",
            "global_step: 329\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67495\n",
            " G train loss: 154.34732\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 451 8 1844 15 26 183 518 5809...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[761 9365 21650 451 8 1844 15 26 183 518...]...][[1 1 0 0 1 0 1 1 1 1...]...][[761 9365 21650 69849 69849 1844 69849 26 183 518...]...][[9365 21650 34 2 1844 33 26 183 518 5809...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  maxwell             0.532        0.000        0.000        0.000        -2.481       -0.296       -0.000       \n",
            "   [1]  caulfield           0.494        0.000        0.000        0.000        -2.785       -0.330       -0.000       \n",
            "   [0]  who                 0.511        8.791        -5.925       -0.672       -3.127       -0.341       -2.786       \n",
            "   [0]  a                   0.545        4.106        -3.028       -0.608       -2.756       -0.349       -2.407       \n",
            "   [1]  england             0.361        0.000        0.000        0.000        -2.412       -0.349       -0.000       \n",
            "   [0]  by                  0.342        5.060        -5.571       -1.072       -2.708       -0.330       -2.378       \n",
            "   [1]  his                 0.347        0.000        0.000        0.000        -1.836       -0.355       -0.000       \n",
            "   [1]  young               0.373        0.000        0.000        0.000        -2.062       -0.392       -0.000       \n",
            "   [1]  daughter            0.309        0.000        0.000        0.000        -2.315       -0.423       -0.000       \n",
            "   [1]  melissa             0.309        0.000        0.000        0.000        -2.599       -0.436       -0.000       \n",
            "   [0]  the                 0.368        9.415        -2.840       -1.000       -2.917       -0.452       -2.466       \n",
            "   [1]  savage              0.385        0.000        0.000        0.000        -2.153       -0.468       -0.000       \n",
            "   [1]  a                   0.426        0.000        0.000        0.000        -2.417       -0.464       -0.000       \n",
            "   [1]  recent              0.305        0.000        0.000        0.000        -2.714       -0.450       -0.000       \n",
            "   [0]  it                  0.289        12.641       -3.526       -1.242       -3.047       -0.425       -2.622       \n",
            "   [0]  world               0.338        3.871        -7.269       -1.084       -2.026       -0.427       -1.599       \n",
            "   [1]  16                  0.343        0.000        0.000        0.000        -1.058       -0.442       -0.000       \n",
            "   [0]  truly               0.305        8.942        -6.811       -1.187       -1.187       -0.452       -0.735       \n",
            "   [1]  tom                 0.141        0.000        0.000        0.000        0.000        -0.457       0.000        \n",
            "   [1]  is                  0.145        0.000        0.000        0.000        0.000        -0.487       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  were                0.537        3.564        -6.449       -0.622       -3.101       -0.306       -2.794       \n",
            "   [1]  am                  0.512        0.000        0.000        0.000        -2.783       -0.355       -0.000       \n",
            "   [0]  david               0.548        3.304        -8.661       -0.602       -3.124       -0.374       -2.750       \n",
            "   [1]  fan                 0.549        0.000        0.000        0.000        -2.832       -0.387       -0.000       \n",
            "   [0]  at                  0.559        3.759        -5.573       -0.582       -3.179       -0.384       -2.795       \n",
            "   [0]  i                   0.556        2.850        -3.252       -0.586       -2.916       -0.382       -2.534       \n",
            "   [0]  film                0.586        13.910       -4.659       -0.535       -2.615       -0.379       -2.236       \n",
            "   [0]  she                 0.501        13.647       -6.827       -0.692       -2.335       -0.381       -1.954       \n",
            "   [1]  ben                 0.535        0.000        0.000        0.000        -1.846       -0.366       -0.000       \n",
            "   [1]  stiller             0.571        0.000        0.000        0.000        -2.072       -0.368       -0.000       \n",
            "   [0]  detectives          0.559        6.956        -14.167      -0.581       -2.326       -0.379       -1.947       \n",
            "   [0]  i                   0.555        12.550       -3.301       -0.589       -1.959       -0.376       -1.583       \n",
            "   [0]  i                   0.553        9.603        -3.302       -0.592       -1.538       -0.375       -1.163       \n",
            "   [1]  movies              0.543        0.000        0.000        0.000        -1.061       -0.376       -0.000       \n",
            "   [0]  of                  0.645        5.053        -3.911       -0.438       -1.192       -0.376       -0.816       \n",
            "   [0]  kids                0.636        2.856        -8.469       -0.453       -0.846       -0.392       -0.454       \n",
            "   [0]  about               0.643        12.637       -5.688       -0.441       -0.441       -0.386       -0.056       \n",
            "   [1]  was                 0.625        0.000        0.000        0.000        0.000        -0.385       0.000        \n",
            "   [1]  awful               0.629        0.000        0.000        0.000        0.000        -0.374       0.000        \n",
            "   [1]  br                  0.673        0.000        0.000        0.000        0.000        -0.371       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  burke               0.522        0.000        0.000        0.000        -1.904       -0.298       -0.000       \n",
            "   [0]  s                   0.516        3.122        -4.336       -0.662       -2.137       -0.337       -1.801       \n",
            "   [1]  an                  0.560        0.000        0.000        0.000        -1.656       -0.355       -0.000       \n",
            "   [0]  of                  0.660        13.715       -3.749       -0.415       -1.860       -0.358       -1.502       \n",
            "   [0]  slow                0.548        8.806        -8.399       -0.601       -1.622       -0.337       -1.284       \n",
            "   [1]  sent                0.626        0.000        0.000        0.000        -1.145       -0.302       -0.000       \n",
            "   [0]  was                 0.627        3.736        -4.527       -0.466       -1.286       -0.291       -0.995       \n",
            "   [1]  investigate         0.677        0.000        0.000        0.000        -0.920       -0.282       -0.000       \n",
            "   [1]  a                   0.698        0.000        0.000        0.000        -1.033       -0.280       -0.000       \n",
            "   [1]  prison              0.553        0.000        0.000        0.000        -1.160       -0.268       -0.000       \n",
            "   [0]  at                  0.581        7.333        -5.712       -0.543       -1.302       -0.256       -1.047       \n",
            "   [1]  inmates             0.568        0.000        0.000        0.000        -0.853       -0.269       -0.000       \n",
            "   [0]  i                   0.580        5.894        -3.417       -0.545       -0.958       -0.290       -0.667       \n",
            "   [1]  inexplicably        0.615        0.000        0.000        0.000        -0.464       -0.311       -0.000       \n",
            "   [1]  dying               0.670        0.000        0.000        0.000        -0.521       -0.324       -0.000       \n",
            "   [0]  with                0.737        7.508        -5.090       -0.305       -0.584       -0.323       -0.261       \n",
            "   [1]  t                   0.728        0.000        0.000        0.000        -0.314       -0.303       -0.000       \n",
            "   [0]  iffy                0.703        7.892        -11.331      -0.352       -0.352       -0.268       -0.085       \n",
            "   [1]  too                 0.653        0.000        0.000        0.000        0.000        -0.238       0.000        \n",
            "   [1]  shabby              0.607        0.000        0.000        0.000        0.000        -0.225       0.000        \n",
            "Samples\n",
            "Sample 0 .  maxwell caulfield who a england by his young daughter melissa the savage a recent it world 16 truly tom is\n",
            "Sample 1 .  were am david fan at i film she ben stiller detectives i i movies of kids about was awful br\n",
            "Sample 2 .  burke s an of slow sent was investigate a prison at inmates i inexplicably dying with t iffy too shabby\n",
            "\n",
            "\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 1 1 0 0 0 0 0 0 1...]...][[1840 1943 12 368 69849 69849 69849 69849 69849 69849...]...][[1943 12 368 755 654 78 2 368 1115 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 1 1 0 0 0 0 0 0 1...]...][[1840 1943 12 368 69849 69849 69849 69849 69849 69849...]...][[1943 12 368 43 17 970 2198 0 2 19...]...]\n",
            "targets[[7 12 2069 0 3572 1 33 0 984 5 37 3056 1306 5 143 18 24 12 3056 9][2907 12 84 81 19 3687 6405 13 2 19 11 13 5536 4 28 3047 120 17790 1 12295][353 0 298 8 60 646 713 1 99 70...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[69 7 12 2069 0 3572 1 33 0 984...]...][[0 1 0 1 1 0 0 1 0 1...]...][[69 69849 12 69849 0 3572 69849 69849 0 69849...]...][[7 12 2069 0 3572 1 33 0 984 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[69 7 12 2069 0 3572 1 33 0 984...]...][[0 1 0 1 1 0 0 1 0 1...]...][[69 69849 12 69849 0 3572 69849 69849 0 69849...]...][[112 12 0 0 3572 87 62 0 82 5...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[4 9 19 1 2277 3381 171 3 75 319...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[84 9 67 0 2277 2 171 3 75 319...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[12 9 16 2 2277 120 171 3 75 319...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "global_step: 334\n",
            " perplexity: 898.917\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            " percent of 3-grams captured: 0.148.\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            " percent of 2-grams captured: 0.582.\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            " percent of 4-grams captured: 0.021.\n",
            " geometric_avg: 0.121.\n",
            " arithmetic_avg: 0.250.\n",
            "global_step: 334\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67411\n",
            " G train loss: 154.51425\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[84 9 67 0 2277 2 171 3 75 319...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 84 9 67 0 2277 2 171 3 75...]...][[0 1 0 0 1 0 1 1 1 1...]...][[31 69849 9 69849 69849 2277 69849 171 3 75...]...][[74 9 98 801 2277 1224 171 3 75 319...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  bad                 0.523        5.849        -6.683       -0.648       -2.662       -0.300       -2.362       \n",
            "   [1]  i                   0.501        0.000        0.000        0.000        -2.262       -0.346       -0.000       \n",
            "   [0]  movies              0.486        5.726        -6.117       -0.722       -2.539       -0.356       -2.183       \n",
            "   [0]  means               0.518        3.568        -8.856       -0.658       -2.041       -0.350       -1.691       \n",
            "   [1]  reaction            0.558        0.000        0.000        0.000        -1.552       -0.357       -0.000       \n",
            "   [0]  slasher             0.471        3.703        -8.171       -0.753       -1.742       -0.366       -1.376       \n",
            "   [1]  lot                 0.415        0.000        0.000        0.000        -1.110       -0.327       -0.000       \n",
            "   [1]  of                  0.545        0.000        0.000        0.000        -1.246       -0.313       -0.000       \n",
            "   [1]  people              0.566        0.000        0.000        0.000        -1.399       -0.358       -0.000       \n",
            "   [1]  left                0.580        0.000        0.000        0.000        -1.571       -0.359       -0.000       \n",
            "   [1]  with                0.666        0.000        0.000        0.000        -1.764       -0.349       -0.000       \n",
            "   [1]  after               0.639        0.000        0.000        0.000        -1.980       -0.362       -0.000       \n",
            "   [1]  seeing              0.662        0.000        0.000        0.000        -2.223       -0.317       -0.000       \n",
            "   [0]  together            0.606        4.473        -8.371       -0.501       -2.496       -0.307       -2.189       \n",
            "   [0]  good                0.525        4.807        -5.794       -0.645       -2.240       -0.270       -1.969       \n",
            "   [0]  in                  0.570        10.010       -4.234       -0.562       -1.790       -0.238       -1.552       \n",
            "   [0]  up                  0.585        4.118        -6.228       -0.536       -1.378       -0.270       -1.108       \n",
            "   [1]  fat                 0.563        0.000        0.000        0.000        -0.945       -0.287       -0.000       \n",
            "   [0]  movie               0.559        6.688        -4.521       -0.582       -1.061       -0.289       -0.772       \n",
            "   [0]  best                0.584        13.328       -6.349       -0.538       -0.538       -0.299       -0.239       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  movie               0.504        0.000        0.000        0.000        -7.035       -0.304       -0.000       \n",
            "   [0]  biased              0.280        4.454        -8.238       -1.273       -7.898       -0.346       -5.000       \n",
            "   [1]  so                  0.221        0.000        0.000        0.000        -7.438       -0.339       -0.000       \n",
            "   [0]  yet                 0.240        7.313        -7.834       -1.427       -8.350       -0.374       -5.000       \n",
            "   [1]  it                  0.225        0.000        0.000        0.000        -7.772       -0.436       -0.000       \n",
            "   [0]  kyu                 0.211        7.449        -12.920      -1.554       -8.726       -0.469       -5.000       \n",
            "   [1]  me                  0.194        0.000        0.000        0.000        -8.052       -0.483       -0.000       \n",
            "   [1]  wish                0.216        0.000        0.000        0.000        -9.039       -0.489       -0.000       \n",
            "   [0]  particular          0.159        4.443        -8.771       -1.842       -10.148      -0.501       -5.000       \n",
            "   [1]  had                 0.143        0.000        0.000        0.000        -9.326       -0.485       -0.000       \n",
            "   [0]  credits             0.107        6.368        -8.040       -2.236       -10.470      -0.489       -5.000       \n",
            "   [0]  mvp                 0.077        13.267       -11.236      -2.568       -9.244       -0.488       -5.000       \n",
            "   [0]  computer            0.066        9.124        -9.580       -2.717       -7.494       -0.503       -5.000       \n",
            "   [0]  of                  0.124        4.469        -4.380       -2.086       -5.363       -0.530       -4.834       \n",
            "   [1]  deaf                0.083        0.000        0.000        0.000        -3.680       -0.563       -0.000       \n",
            "   [1]  as                  0.086        0.000        0.000        0.000        -4.131       -0.538       -0.000       \n",
            "   [0]  of                  0.160        3.924        -4.375       -1.835       -4.638       -0.531       -4.107       \n",
            "   [0]  into                0.194        7.766        -6.728       -1.641       -3.148       -0.537       -2.610       \n",
            "   [0]  blossomed           0.184        4.473        -12.846      -1.692       -1.692       -0.509       -1.183       \n",
            "   [1]  watched             0.177        0.000        0.000        0.000        0.000        -0.468       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  in                  0.539        7.249        -4.899       -0.617       -4.732       -0.296       -4.436       \n",
            "   [0]  hopefully           0.415        11.304       -8.591       -0.879       -4.619       -0.334       -4.285       \n",
            "   [0]  movie               0.417        7.072        -3.236       -0.874       -4.199       -0.341       -3.858       \n",
            "   [0]  is                  0.444        3.081        -3.382       -0.813       -3.732       -0.355       -3.377       \n",
            "   [1]  screenplay          0.361        0.000        0.000        0.000        -3.278       -0.370       -0.000       \n",
            "   [0]  to                  0.421        3.694        -3.995       -0.866       -3.680       -0.379       -3.301       \n",
            "   [0]  dallas              0.464        3.351        -11.473      -0.767       -3.160       -0.388       -2.772       \n",
            "   [1]  horror              0.511        0.000        0.000        0.000        -2.686       -0.392       -0.000       \n",
            "   [0]  ways                0.430        7.453        -8.527       -0.845       -3.016       -0.390       -2.626       \n",
            "   [0]  i                   0.429        3.862        -3.793       -0.847       -2.437       -0.371       -2.066       \n",
            "   [0]  is                  0.456        9.594        -4.174       -0.785       -1.786       -0.366       -1.419       \n",
            "   [1]  at                  0.496        0.000        0.000        0.000        -1.123       -0.370       -0.000       \n",
            "   [1]  tiffany             0.480        0.000        0.000        0.000        -1.261       -0.375       -0.000       \n",
            "   [1]  s                   0.484        0.000        0.000        0.000        -1.416       -0.373       -0.000       \n",
            "   [1]  you                 0.508        0.000        0.000        0.000        -1.589       -0.372       -0.000       \n",
            "   [0]  style               0.384        6.875        -8.000       -0.957       -1.784       -0.372       -1.412       \n",
            "   [0]  s                   0.395        6.680        -4.413       -0.929       -0.929       -0.365       -0.564       \n",
            "   [1]  is                  0.429        0.000        0.000        0.000        0.000        -0.375       0.000        \n",
            "   [1]  going               0.381        0.000        0.000        0.000        0.000        -0.389       0.000        \n",
            "   [1]  to                  0.445        0.000        0.000        0.000        0.000        -0.392       0.000        \n",
            "Samples\n",
            "Sample 0 .  bad i movies means reaction slasher lot of people left with after seeing together good in up fat movie best\n",
            "Sample 1 .  movie biased so yet it kyu me wish particular had credits mvp computer of deaf as of into blossomed watched\n",
            "Sample 2 .  in hopefully movie is screenplay to dallas horror ways i is at tiffany s you style s is going to\n",
            "\n",
            "\n",
            "targets[[50418 12 330 8991 8 0 328 45 4 28 29 3 4895 12 117 39 5 983 3 600][17 13 1136 9 59 395 7 4 255 73 126 72 47 9 67 478 7231 7 13 401][9546 288 21 4095 1669 12 872 17 18 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[601 50418 12 330 8991 8 0 328 45 4...]...][[0 1 1 0 1 0 0 0 1 1...]...][[601 69849 12 330 69849 8 69849 69849 69849 4...]...][[50418 12 330 8991 8 0 328 45 4 28...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[601 50418 12 330 8991 8 0 328 45 4...]...][[0 1 1 0 1 0 0 0 1 1...]...][[601 69849 12 330 69849 8 69849 69849 69849 4...]...][[582 12 330 54583 8 1 55377 165 4 28...]...]\n",
            "targets[[17 13 42 39307 33 3566 1 15 49 293 84 1 7096 85 7 5 2 814 17 1156][26862 6675 2698 188 4 28 2 737 3840 34679 5638 7071 469 75 25 4437 38 4573 187 40][42 215 10 19 16 0 13340 57 9 367...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 42 39307 33 3566 1 15 49...]...][[0 0 1 1 1 0 1 0 1 1...]...][[10 69849 69849 42 39307 33 69849 1 69849 49...]...][[17 13 42 39307 33 3566 1 15 49 293...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 13 42 39307 33 3566 1 15 49...]...][[0 0 1 1 1 0 1 0 1 1...]...][[10 69849 69849 42 39307 33 69849 1 69849 49...]...][[504 2529 42 39307 33 9995 1 38126 49 293...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 1001 13 10 307 5 92 0 5290 739...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 1349 115 19 10 5 92 8 5290 22...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 33 8 24391 19 5 92 4 5290 344...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "global_step: 339\n",
            " perplexity: 894.665\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            " percent of 3-grams captured: 0.147.\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            " percent of 2-grams captured: 0.553.\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            " percent of 4-grams captured: 0.018.\n",
            " geometric_avg: 0.114.\n",
            " arithmetic_avg: 0.240.\n",
            "global_step: 339\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67260\n",
            " G train loss: 156.59065\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 1349 115 19 10 5 92 8 5290 22...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[47 2 1349 115 19 10 5 92 8 5290...]...][[1 0 0 0 0 1 1 0 1 0...]...][[47 2 69849 69849 69849 69849 5 92 69849 5290...]...][[2 3896 19 43 140 5 92 3415 5290 12...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  a                   0.501        0.000        0.000        0.000        -3.350       -0.298       -0.000       \n",
            "   [0]  tonight             0.436        8.645        -8.824       -0.830       -3.761       -0.340       -3.422       \n",
            "   [0]  film                0.472        6.951        -4.326       -0.751       -3.291       -0.342       -2.949       \n",
            "   [0]  about               0.516        4.376        -5.940       -0.661       -2.852       -0.362       -2.490       \n",
            "   [0]  m                   0.419        3.779        -5.854       -0.869       -2.459       -0.378       -2.081       \n",
            "   [1]  is                  0.436        0.000        0.000        0.000        -1.785       -0.352       -0.000       \n",
            "   [1]  made                0.512        0.000        0.000        0.000        -2.004       -0.358       -0.000       \n",
            "   [0]  beating             0.495        4.393        -13.860      -0.702       -2.250       -0.376       -1.874       \n",
            "   [1]  1973                0.549        0.000        0.000        0.000        -1.738       -0.362       -0.000       \n",
            "   [0]  s                   0.550        5.390        -4.780       -0.598       -1.951       -0.364       -1.587       \n",
            "   [1]  what                0.638        0.000        0.000        0.000        -1.519       -0.351       -0.000       \n",
            "   [0]  because             0.559        4.597        -6.287       -0.582       -1.705       -0.360       -1.345       \n",
            "   [1]  presumably          0.524        0.000        0.000        0.000        -1.261       -0.319       -0.000       \n",
            "   [0]  any                 0.475        3.364        -7.016       -0.744       -1.416       -0.299       -1.117       \n",
            "   [0]  after               0.470        5.954        -6.752       -0.754       -0.754       -0.294       -0.461       \n",
            "   [1]  low                 0.496        0.000        0.000        0.000        0.000        -0.309       0.000        \n",
            "   [1]  budget              0.528        0.000        0.000        0.000        0.000        -0.336       0.000        \n",
            "   [1]  this                0.558        0.000        0.000        0.000        0.000        -0.351       0.000        \n",
            "   [1]  two                 0.498        0.000        0.000        0.000        0.000        -0.353       0.000        \n",
            "   [1]  hander              0.516        0.000        0.000        0.000        0.000        -0.333       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  you                 0.537        0.000        0.000        0.000        -6.521       -0.297       -0.000       \n",
            "   [0]  friday              0.406        7.969        -7.874       -0.901       -7.321       -0.344       -5.000       \n",
            "   [0]  at                  0.438        12.710       -5.331       -0.825       -7.208       -0.306       -5.000       \n",
            "   [1]  excess              0.454        0.000        0.000        0.000        -7.165       -0.345       -0.000       \n",
            "   [0]  force               0.219        3.623        -7.919       -1.518       -8.045       -0.369       -5.000       \n",
            "   [1]  movie               0.220        0.000        0.000        0.000        -7.327       -0.264       -0.000       \n",
            "   [0]  competition         0.219        8.800        -11.792      -1.516       -8.226       -0.300       -5.000       \n",
            "   [0]  into                0.259        4.952        -6.648       -1.350       -7.533       -0.350       -5.000       \n",
            "   [0]  for                 0.339        6.323        -5.033       -1.080       -6.942       -0.403       -5.000       \n",
            "   [1]  mean                0.178        0.000        0.000        0.000        -6.580       -0.449       -0.000       \n",
            "   [0]  stars               0.249        6.264        -7.852       -1.390       -7.388       -0.353       -5.000       \n",
            "   [0]  non                 0.175        4.066        -8.211       -1.742       -6.733       -0.388       -5.000       \n",
            "   [0]  on                  0.240        5.462        -5.209       -1.428       -5.604       -0.352       -5.000       \n",
            "   [0]  that                0.271        5.536        -4.461       -1.304       -4.689       -0.402       -4.287       \n",
            "   [1]  attempt             0.261        0.000        0.000        0.000        -3.800       -0.411       -0.000       \n",
            "   [0]  name                0.177        4.450        -7.710       -1.734       -4.266       -0.392       -3.874       \n",
            "   [1]  made                0.246        0.000        0.000        0.000        -2.843       -0.340       -0.000       \n",
            "   [0]  if                  0.255        4.081        -6.025       -1.368       -3.192       -0.390       -2.802       \n",
            "   [0]  matthau             0.310        12.190       -10.884      -1.173       -2.047       -0.399       -1.649       \n",
            "   [0]  somewhat            0.374        3.431        -9.127       -0.982       -0.982       -0.416       -0.566       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  of                  0.601        9.138        -3.848       -0.509       -2.298       -0.312       -1.985       \n",
            "   [0]  of                  0.685        11.709       -3.647       -0.379       -2.008       -0.380       -1.628       \n",
            "   [1]  obviously           0.681        0.000        0.000        0.000        -1.829       -0.412       -0.000       \n",
            "   [0]  the                 0.685        9.062        -2.900       -0.379       -2.054       -0.394       -1.660       \n",
            "   [0]  after               0.650        5.316        -6.948       -0.431       -1.881       -0.367       -1.514       \n",
            "   [1]  the                 0.669        0.000        0.000        0.000        -1.627       -0.326       -0.000       \n",
            "   [0]  interesting         0.647        3.424        -7.044       -0.435       -1.827       -0.314       -1.513       \n",
            "   [1]  it                  0.612        0.000        0.000        0.000        -1.562       -0.295       -0.000       \n",
            "   [0]  own                 0.631        4.326        -7.509       -0.460       -1.754       -0.276       -1.478       \n",
            "   [0]  i                   0.605        3.308        -3.456       -0.502       -1.452       -0.286       -1.166       \n",
            "   [1]  joke                0.662        0.000        0.000        0.000        -1.067       -0.290       -0.000       \n",
            "   [0]  setting             0.576        6.322        -8.812       -0.551       -1.198       -0.318       -0.880       \n",
            "   [1]  bad                 0.614        0.000        0.000        0.000        -0.726       -0.295       -0.000       \n",
            "   [0]  way                 0.605        3.510        -7.884       -0.502       -0.815       -0.309       -0.506       \n",
            "   [1]  is                  0.604        0.000        0.000        0.000        -0.352       -0.313       -0.000       \n",
            "   [1]  and                 0.610        0.000        0.000        0.000        -0.395       -0.311       -0.000       \n",
            "   [0]  bad                 0.642        6.854        -5.898       -0.443       -0.443       -0.310       -0.133       \n",
            "   [1]  one                 0.628        0.000        0.000        0.000        0.000        -0.319       0.000        \n",
            "   [1]  would               0.632        0.000        0.000        0.000        0.000        -0.309       0.000        \n",
            "   [1]  review              0.591        0.000        0.000        0.000        0.000        -0.306       0.000        \n",
            "Samples\n",
            "Sample 0 .  a tonight film about m is made beating 1973 s what because presumably any after low budget this two hander\n",
            "Sample 1 .  you friday at excess force movie competition into for mean stars non on that attempt name made if matthau somewhat\n",
            "Sample 2 .  of of obviously the after the interesting it own i joke setting bad way is and bad one would review\n",
            "\n",
            "\n",
            "targets[[10 5015 231 570 3 0 599 2707 8323 33 0 2497 269 35 2816 1152 3 2281 405 1909][4994 7264 578 8 0 200 6183 15 1133 2730 2769 451 1 436 4 4248 10 122 968 1262][27 307 10 19 440 154 143 1 51 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 10 5015 231 570 3 0 599 2707 8323...]...][[0 1 1 1 0 0 0 1 1 0...]...][[15 69849 5015 231 570 69849 69849 69849 2707 8323...]...][[10 5015 231 570 3 0 599 2707 8323 33...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 10 5015 231 570 3 0 599 2707 8323...]...][[0 1 1 1 0 0 0 1 1 0...]...][[15 69849 5015 231 570 69849 69849 69849 2707 8323...]...][[29 5015 231 570 0 295 100 2707 8323 708...]...]\n",
            "targets[[5 0 244 3 3140 3788 3495 439 50 80 208 9 169 7 3214 11 286 15 142 115][1018 2 222 3 10 2889 22 1008 729 1 693 9 67 4 27 7 0 921 198 295][7 12 2 222 3 2 658 19 37 46...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 0 244 3 3140 3788 3495 439 50...]...][[1 1 0 0 0 1 0 0 1 0...]...][[10 5 0 69849 69849 69849 3788 69849 69849 50...]...][[5 0 244 3 3140 3788 3495 439 50 80...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 0 244 3 3140 3788 3495 439 50...]...][[1 1 0 0 0 1 0 0 1 0...]...][[10 5 0 69849 69849 69849 3788 69849 69849 50...]...][[5 0 799 5 853 3788 20 123 50 243...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[0 4 2521 38 17 2 0 797 26211 575...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[4140 4 2666 6756 439 78 0 797 33 575...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[23 4 694 2117 7 9995 0 797 2 575...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "global_step: 344\n",
            " perplexity: 892.171\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            " percent of 3-grams captured: 0.169.\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            " percent of 2-grams captured: 0.590.\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            " percent of 4-grams captured: 0.029.\n",
            " geometric_avg: 0.142.\n",
            " arithmetic_avg: 0.263.\n",
            "global_step: 344\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67138\n",
            " G train loss: 158.14488\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[4140 4 2666 6756 439 78 0 797 33 575...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[157 4140 4 2666 6756 439 78 0 797 33...]...][[0 1 0 0 0 0 1 1 0 1...]...][[157 69849 4 69849 69849 69849 69849 0 797 69849...]...][[139 4 270 0 9 81 0 797 14 575...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  ve                  0.483        13.815       -6.214       -0.728       -3.233       -0.303       -2.930       \n",
            "   [1]  to                  0.538        0.000        0.000        0.000        -2.812       -0.344       -0.000       \n",
            "   [0]  set                 0.521        13.490       -7.111       -0.652       -3.157       -0.363       -2.794       \n",
            "   [0]  the                 0.559        10.190       -2.529       -0.582       -2.812       -0.361       -2.452       \n",
            "   [0]  i                   0.517        9.304        -3.238       -0.660       -2.504       -0.357       -2.147       \n",
            "   [0]  great               0.512        6.688        -5.853       -0.669       -2.070       -0.352       -1.718       \n",
            "   [1]  the                 0.554        0.000        0.000        0.000        -1.573       -0.353       -0.000       \n",
            "   [1]  theater             0.517        0.000        0.000        0.000        -1.766       -0.355       -0.000       \n",
            "   [0]  as                  0.538        5.689        -4.920       -0.619       -1.982       -0.348       -1.635       \n",
            "   [1]  including           0.528        0.000        0.000        0.000        -1.530       -0.348       -0.000       \n",
            "   [0]  the                 0.566        8.199        -2.449       -0.569       -1.718       -0.348       -1.369       \n",
            "   [1]  actors              0.521        0.000        0.000        0.000        -1.289       -0.347       -0.000       \n",
            "   [1]  from                0.541        0.000        0.000        0.000        -1.447       -0.336       -0.000       \n",
            "   [0]  the                 0.581        3.805        -2.474       -0.543       -1.625       -0.339       -1.286       \n",
            "   [0]  say                 0.625        4.716        -6.551       -0.470       -1.214       -0.340       -0.875       \n",
            "   [0]  are                 0.628        3.976        -5.483       -0.466       -0.836       -0.337       -0.499       \n",
            "   [1]  a                   0.629        0.000        0.000        0.000        -0.415       -0.319       -0.000       \n",
            "   [1]  war                 0.623        0.000        0.000        0.000        -0.466       -0.302       -0.000       \n",
            "   [1]  story               0.560        0.000        0.000        0.000        -0.524       -0.291       -0.000       \n",
            "   [0]  was                 0.556        5.058        -4.194       -0.588       -0.588       -0.282       -0.306       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  is                  0.572        4.046        -2.858       -0.558       -3.279       -0.304       -2.975       \n",
            "   [1]  begin               0.546        0.000        0.000        0.000        -3.055       -0.338       -0.000       \n",
            "   [1]  the                 0.570        0.000        0.000        0.000        -3.429       -0.356       -0.000       \n",
            "   [1]  special             0.438        0.000        0.000        0.000        -3.850       -0.354       -0.000       \n",
            "   [1]  effects             0.410        0.000        0.000        0.000        -4.323       -0.375       -0.000       \n",
            "   [0]  and                 0.440        7.397        -2.980       -0.821       -4.853       -0.405       -4.448       \n",
            "   [1]  be                  0.490        0.000        0.000        0.000        -4.526       -0.421       -0.000       \n",
            "   [0]  already             0.437        8.686        -8.867       -0.827       -5.081       -0.417       -4.665       \n",
            "   [1]  special             0.318        0.000        0.000        0.000        -4.776       -0.415       -0.000       \n",
            "   [0]  it                  0.291        13.446       -2.812       -1.234       -5.362       -0.445       -4.917       \n",
            "   [0]  that                0.320        6.044        -3.979       -1.138       -4.635       -0.483       -4.152       \n",
            "   [0]  movies              0.322        2.369        -6.117       -1.133       -3.926       -0.502       -3.425       \n",
            "   [1]  director            0.319        0.000        0.000        0.000        -3.136       -0.514       -0.000       \n",
            "   [1]  shouted             0.331        0.000        0.000        0.000        -3.521       -0.519       -0.000       \n",
            "   [1]  action              0.442        0.000        0.000        0.000        -3.953       -0.513       -0.000       \n",
            "   [0]  local               0.289        3.224        -8.268       -1.242       -4.438       -0.474       -3.964       \n",
            "   [0]  very                0.266        9.541        -5.690       -1.325       -3.588       -0.476       -3.113       \n",
            "   [0]  elizabeth           0.247        6.150        -9.390       -1.397       -2.541       -0.489       -2.053       \n",
            "   [0]  and                 0.277        8.747        -2.813       -1.284       -1.284       -0.520       -0.764       \n",
            "   [1]  indicated           0.233        0.000        0.000        0.000        0.000        -0.533       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  when                0.511        5.951        -6.239       -0.671       -7.569       -0.303       -5.000       \n",
            "   [1]  many                0.490        0.000        0.000        0.000        -7.745       -0.351       -0.000       \n",
            "   [1]  words               0.412        0.000        0.000        0.000        -8.695       -0.359       -0.000       \n",
            "   [1]  describe            0.384        0.000        0.000        0.000        -9.762       -0.351       -0.000       \n",
            "   [1]  this                0.417        0.000        0.000        0.000        -10.959      -0.366       -0.000       \n",
            "   [0]  around              0.394        4.621        -8.258       -0.932       -12.304      -0.387       -5.000       \n",
            "   [0]  anymore             0.227        5.156        -8.345       -1.481       -12.767      -0.386       -5.000       \n",
            "   [0]  cool                0.144        6.220        -8.533       -1.939       -12.670      -0.376       -5.000       \n",
            "   [0]  i                   0.135        4.820        -3.423       -2.002       -12.048      -0.442       -5.000       \n",
            "   [0]  a                   0.154        6.838        -2.880       -1.872       -11.278      -0.549       -5.000       \n",
            "   [0]  that                0.171        13.392       -4.366       -1.765       -10.561      -0.632       -5.000       \n",
            "   [0]  actually            0.139        3.174        -7.005       -1.976       -9.875       -0.664       -5.000       \n",
            "   [0]  t                   0.144        6.442        -4.866       -1.935       -8.868       -0.668       -5.000       \n",
            "   [0]  the                 0.171        4.568        -2.559       -1.766       -7.784       -0.681       -5.000       \n",
            "   [0]  has                 0.160        4.568        -5.371       -1.832       -6.756       -0.680       -5.000       \n",
            "   [1]  crap                0.186        0.000        0.000        0.000        -5.528       -0.652       -0.000       \n",
            "   [0]  i                   0.170        4.568        -3.426       -1.774       -6.207       -0.636       -5.000       \n",
            "   [0]  into                0.195        4.569        -6.392       -1.634       -4.976       -0.606       -4.369       \n",
            "   [0]  employed            0.183        6.018        -10.967      -1.701       -3.752       -0.593       -3.159       \n",
            "   [0]  science             0.100        6.959        -7.352       -2.303       -2.303       -0.569       -1.734       \n",
            "Samples\n",
            "Sample 0 .  ve to set the i great the theater as including the actors from the say are a war story was\n",
            "Sample 1 .  is begin the special effects and be already special it that movies director shouted action local very elizabeth and indicated\n",
            "Sample 2 .  when many words describe this around anymore cool i a that actually t the has crap i into employed science\n",
            "\n",
            "\n",
            "targets[[1022 37 10 5 47 30 0 50896 13 43 3543 69 9 140 251 11 0 673 16 91][0 2195 3902 2130 22 2 1717 2841 11 5 42 43 4 1382 3504 55 112 50 28 448][439 39 327 25 53 173 741 4 367 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[48 1022 37 10 5 47 30 0 50896 13...]...][[1 1 1 1 1 0 1 0 1 0...]...][[48 1022 37 10 5 47 69849 0 69849 13...]...][[1022 37 10 5 47 30 0 50896 13 43...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[48 1022 37 10 5 47 30 0 50896 13...]...][[1 1 1 1 1 0 1 0 1 0...]...][[48 1022 37 10 5 47 69849 0 69849 13...]...][[1022 37 10 5 47 360 0 1216 13 863...]...]\n",
            "targets[[196 0 17 257 0 109 784 2 171 3 166 0 756 3 0 17 1386 31548 1 11405][17 4 28 509 33 30 0 231 42 106 44 16 48 9619 1 2 115 537 39 68][0 834 3 3341 338 5 52 3 2 3225...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 196 0 17 257 0 109 784 2 171...]...][[1 1 0 0 0 1 1 0 0 0...]...][[9 196 0 69849 69849 69849 109 784 69849 69849...]...][[196 0 17 257 0 109 784 2 171 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 196 0 17 257 0 109 784 2 171...]...][[1 1 0 0 0 1 1 0 0 0...]...][[9 196 0 69849 69849 69849 109 784 69849 69849...]...][[196 0 442 2 2 109 784 8 2 91...]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[5 1326 5 0 2 362 16 804 1326 605...]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[9 1281 10 17 2 223 16 804 2 605...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[17 0 13 4857 2 1 16 804 1644 605...]...]\n",
            "I0310 00:34:12.858808 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "I0310 00:34:12.885633 139852867667712 supervisor.py:1099] global_step/sec: 0.382916\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "global_step: 349\n",
            " perplexity: 891.778\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            " percent of 3-grams captured: 0.170.\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            " percent of 2-grams captured: 0.602.\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            " percent of 4-grams captured: 0.031.\n",
            " geometric_avg: 0.147.\n",
            " arithmetic_avg: 0.268.\n",
            "global_step: 349\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.67020\n",
            " G train loss: 159.19322\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[9 1281 10 17 2 223 16 804 2 605...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[264 9 1281 10 17 2 223 16 804 2...]...][[0 0 0 0 1 0 1 1 0 1...]...][[264 69849 69849 69849 69849 2 69849 16 804 69849...]...][[10 43 13 67 2 146 16 804 1197 605...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  this                0.540        4.357        -3.582       -0.616       -3.918       -0.302       -3.615       \n",
            "   [0]  about               0.594        7.659        -6.163       -0.521       -3.707       -0.351       -3.356       \n",
            "   [0]  was                 0.579        3.715        -3.967       -0.546       -3.577       -0.388       -3.189       \n",
            "   [0]  had                 0.533        3.825        -5.574       -0.629       -3.403       -0.379       -3.024       \n",
            "   [1]  a                   0.545        0.000        0.000        0.000        -3.114       -0.358       -0.000       \n",
            "   [0]  those               0.476        7.817        -6.835       -0.742       -3.496       -0.357       -3.139       \n",
            "   [1]  for                 0.581        0.000        0.000        0.000        -3.092       -0.341       -0.000       \n",
            "   [1]  showing             0.447        0.000        0.000        0.000        -3.471       -0.373       -0.000       \n",
            "   [0]  showed              0.385        3.811        -8.364       -0.954       -3.897       -0.345       -3.552       \n",
            "   [1]  complete            0.354        0.000        0.000        0.000        -3.304       -0.337       -0.000       \n",
            "   [0]  road                0.282        8.680        -8.858       -1.264       -3.709       -0.344       -3.365       \n",
            "   [1]  of                  0.423        0.000        0.000        0.000        -2.745       -0.352       -0.000       \n",
            "   [1]  effort              0.297        0.000        0.000        0.000        -3.081       -0.404       -0.000       \n",
            "   [1]  in                  0.325        0.000        0.000        0.000        -3.459       -0.388       -0.000       \n",
            "   [1]  trying              0.249        0.000        0.000        0.000        -3.884       -0.403       -0.000       \n",
            "   [0]  meets               0.262        4.189        -9.406       -1.339       -4.360       -0.393       -3.967       \n",
            "   [0]  the                 0.306        11.452       -3.432       -1.185       -3.392       -0.411       -2.981       \n",
            "   [0]  years               0.276        3.860        -6.557       -1.287       -2.477       -0.434       -2.044       \n",
            "   [1]  quality             0.190        0.000        0.000        0.000        -1.337       -0.428       -0.000       \n",
            "   [0]  the                 0.223        7.442        -3.435       -1.501       -1.501       -0.411       -1.090       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  had                 0.477        7.889        -5.645       -0.739       -6.270       -0.303       -5.000       \n",
            "   [0]  it                  0.430        5.897        -3.940       -0.844       -6.209       -0.343       -5.000       \n",
            "   [0]  extreme             0.265        2.333        -8.103       -1.330       -6.024       -0.372       -5.000       \n",
            "   [1]  movie               0.263        0.000        0.000        0.000        -5.269       -0.424       -0.000       \n",
            "   [0]  the                 0.312        7.160        -2.183       -1.166       -5.916       -0.498       -5.000       \n",
            "   [1]  t                   0.317        0.000        0.000        0.000        -5.333       -0.549       -0.000       \n",
            "   [1]  cause               0.142        0.000        0.000        0.000        -5.987       -0.567       -0.000       \n",
            "   [0]  very                0.123        6.486        -5.775       -2.093       -6.722       -0.612       -5.000       \n",
            "   [0]  in                  0.149        8.698        -3.790       -1.906       -5.197       -0.683       -4.514       \n",
            "   [1]  but                 0.199        0.000        0.000        0.000        -3.694       -0.741       -0.000       \n",
            "   [0]  ripoff              0.220        3.216        -13.563      -1.513       -4.148       -0.756       -3.391       \n",
            "   [1]  did                 0.192        0.000        0.000        0.000        -2.958       -0.737       -0.000       \n",
            "   [1]  cause               0.077        0.000        0.000        0.000        -3.320       -0.707       -0.000       \n",
            "   [0]  after               0.073        2.265        -6.710       -2.616       -3.728       -0.737       -2.991       \n",
            "   [1]  director            0.079        0.000        0.000        0.000        -1.248       -0.799       -0.000       \n",
            "   [1]  a                   0.087        0.000        0.000        0.000        -1.401       -0.859       -0.000       \n",
            "   [1]  lot                 0.058        0.000        0.000        0.000        -1.573       -0.885       -0.000       \n",
            "   [1]  of                  0.102        0.000        0.000        0.000        -1.766       -0.917       -0.000       \n",
            "   [0]  to                  0.138        7.945        -3.690       -1.982       -1.982       -0.903       -1.080       \n",
            "   [1]  to                  0.175        0.000        0.000        0.000        0.000        -0.850       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  combination         0.505        0.000        0.000        0.000        -1.771       -0.304       -0.000       \n",
            "   [1]  of                  0.619        0.000        0.000        0.000        -1.988       -0.342       -0.000       \n",
            "   [1]  reading             0.540        0.000        0.000        0.000        -2.232       -0.334       -0.000       \n",
            "   [0]  the                 0.564        2.329        -2.329       -0.573       -2.506       -0.315       -2.192       \n",
            "   [0]  he                  0.572        15.578       -6.325       -0.558       -2.171       -0.297       -1.873       \n",
            "   [0]  was                 0.558        3.145        -3.680       -0.583       -1.810       -0.280       -1.530       \n",
            "   [1]  viewing             0.509        0.000        0.000        0.000        -1.378       -0.262       -0.000       \n",
            "   [1]  this                0.541        0.000        0.000        0.000        -1.547       -0.260       -0.000       \n",
            "   [1]  film                0.562        0.000        0.000        0.000        -1.736       -0.263       -0.000       \n",
            "   [1]  has                 0.545        0.000        0.000        0.000        -1.950       -0.261       -0.000       \n",
            "   [1]  inspired            0.588        0.000        0.000        0.000        -2.189       -0.257       -0.000       \n",
            "   [0]  i                   0.546        5.586        -3.407       -0.605       -2.457       -0.251       -2.207       \n",
            "   [0]  dvd                 0.512        8.001        -7.786       -0.670       -2.080       -0.242       -1.837       \n",
            "   [1]  and                 0.535        0.000        0.000        0.000        -1.582       -0.241       -0.000       \n",
            "   [1]  i                   0.498        0.000        0.000        0.000        -1.777       -0.246       -0.000       \n",
            "   [0]  use                 0.396        3.649        -8.590       -0.927       -1.995       -0.253       -1.742       \n",
            "   [1]  new                 0.364        0.000        0.000        0.000        -1.198       -0.278       -0.000       \n",
            "   [0]  toronto             0.261        13.003       -8.721       -1.345       -1.345       -0.324       -1.021       \n",
            "   [1]  recently            0.223        0.000        0.000        0.000        0.000        -0.397       0.000        \n",
            "   [1]  i                   0.213        0.000        0.000        0.000        0.000        -0.480       0.000        \n",
            "Samples\n",
            "Sample 0 .  this about was had a those for showing showed complete road of effort in trying meets the years quality the\n",
            "Sample 1 .  had it extreme movie the t cause very in but ripoff did cause after director a lot of to to\n",
            "Sample 2 .  combination of reading the he was viewing this film has inspired i dvd and i use new toronto recently i\n",
            "\n",
            "\n",
            "targets[[5 23 127 737 184 600 19 8 189 2 171 3 0 184 5 319 4 127 1629 11][552 7231 10 17 256 509 0 298 13 53 671 14 0 17 352 319 44 88 3 0][20 159 21 367 10 17 352 127 330 41...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 23 127 737 184 600 19 8 189...]...][[1 0 0 0 1 0 1 1 0 1...]...][[10 5 69849 69849 69849 184 69849 19 8 69849...]...][[5 23 127 737 184 600 19 8 189 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 23 127 737 184 600 19 8 189...]...][[1 0 0 0 1 0 1 1 0 1...]...][[10 5 69849 69849 69849 184 69849 19 8 69849...]...][[5 109 3 38 184 4 19 8 688 2...]...]\n",
            "targets[[5 29 3 60 519 231 98 467 7 51 9 13 115 1 7 133 1765 55 15 71][55687 9149 7617 554 360 341 19 11 45 14399 3 10822 245 4 865 144 9 76 0 754][67 110 10 19 95 143 8 0 1001 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 29 3 60 519 231 98 467 7...]...][[1 1 1 0 1 0 1 0 0 0...]...][[10 5 29 3 69849 519 69849 98 69849 69849...]...][[5 29 3 60 519 231 98 467 7 51...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 5 29 3 60 519 231 98 467 7...]...][[1 1 1 0 1 0 1 0 0 0...]...][[10 5 29 3 69849 519 69849 98 69849 69849...]...][[5 29 3 59 519 17 98 8 3 7...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 2343 30 723 3 14223 98 8 10 202...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 110 30 668 3 0 98 8 10 202...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 58 30 1197 3 108 98 8 10 202...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "global_step: 354\n",
            " perplexity: 892.873\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            " percent of 3-grams captured: 0.148.\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            " percent of 2-grams captured: 0.603.\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            " percent of 4-grams captured: 0.028.\n",
            " geometric_avg: 0.136.\n",
            " arithmetic_avg: 0.260.\n",
            "global_step: 354\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66893\n",
            " G train loss: 159.21686\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 110 30 668 3 0 98 8 10 202...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 139 110 30 668 3 0 98 8 10...]...][[1 0 1 0 1 0 1 1 1 1...]...][[9 139 69849 30 69849 3 69849 98 8 10...]...][[139 2867 30 834 3 1568 98 8 10 202...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  ve                  0.539        0.000        0.000        0.000        -2.881       -0.295       -0.000       \n",
            "   [0]  disbelief           0.572        6.856        -11.000      -0.559       -3.234       -0.331       -2.903       \n",
            "   [1]  all                 0.617        0.000        0.000        0.000        -3.004       -0.356       -0.000       \n",
            "   [0]  premise             0.506        7.515        -7.926       -0.682       -3.372       -0.363       -3.009       \n",
            "   [1]  of                  0.611        0.000        0.000        0.000        -3.020       -0.319       -0.000       \n",
            "   [0]  collection          0.420        3.212        -8.660       -0.868       -3.391       -0.338       -3.053       \n",
            "   [1]  movies              0.439        0.000        0.000        0.000        -2.833       -0.285       -0.000       \n",
            "   [1]  in                  0.473        0.000        0.000        0.000        -3.181       -0.327       -0.000       \n",
            "   [1]  this                0.503        0.000        0.000        0.000        -3.571       -0.363       -0.000       \n",
            "   [1]  series              0.431        0.000        0.000        0.000        -4.009       -0.371       -0.000       \n",
            "   [0]  it                  0.381        8.543        -4.005       -0.964       -4.501       -0.348       -4.152       \n",
            "   [1]  one                 0.391        0.000        0.000        0.000        -3.971       -0.352       -0.000       \n",
            "   [0]  can                 0.332        11.696       -6.467       -1.104       -4.458       -0.384       -4.074       \n",
            "   [0]  famous              0.383        10.658       -7.537       -0.959       -3.766       -0.403       -3.363       \n",
            "   [1]  and                 0.405        0.000        0.000        0.000        -3.151       -0.447       -0.000       \n",
            "   [0]  buffered            0.392        10.655       -12.907      -0.936       -3.538       -0.458       -3.080       \n",
            "   [0]  with                0.511        5.655        -5.403       -0.672       -2.922       -0.443       -2.478       \n",
            "   [0]  1970s               0.427        3.413        -8.690       -0.850       -2.525       -0.446       -2.079       \n",
            "   [0]  tells               0.353        9.289        -8.006       -1.041       -1.881       -0.389       -1.492       \n",
            "   [0]  into                0.390        4.406        -6.766       -0.943       -0.943       -0.382       -0.561       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  just                0.481        2.872        -5.811       -0.731       -5.205       -0.310       -4.895       \n",
            "   [1]  a                   0.492        0.000        0.000        0.000        -5.023       -0.363       -0.000       \n",
            "   [0]  in                  0.512        6.390        -3.892       -0.669       -5.639       -0.397       -5.000       \n",
            "   [1]  cool                0.354        0.000        0.000        0.000        -5.580       -0.408       -0.000       \n",
            "   [0]  in                  0.381        3.426        -3.492       -0.966       -6.264       -0.466       -5.000       \n",
            "   [0]  extreme             0.235        2.904        -8.639       -1.449       -5.948       -0.510       -5.000       \n",
            "   [1]  enjoyed             0.270        0.000        0.000        0.000        -5.052       -0.610       -0.000       \n",
            "   [1]  it                  0.228        0.000        0.000        0.000        -5.671       -0.673       -0.000       \n",
            "   [1]  immensely           0.281        0.000        0.000        0.000        -6.367       -0.732       -0.000       \n",
            "   [1]  and                 0.300        0.000        0.000        0.000        -7.148       -0.746       -0.000       \n",
            "   [0]  seriously           0.237        6.838        -8.058       -1.439       -8.025       -0.734       -5.000       \n",
            "   [0]  cinematography      0.176        3.280        -8.445       -1.738       -7.394       -0.746       -5.000       \n",
            "   [1]  every               0.141        0.000        0.000        0.000        -6.350       -0.802       -0.000       \n",
            "   [1]  time                0.136        0.000        0.000        0.000        -7.129       -0.879       -0.000       \n",
            "   [0]  used                0.106        3.289        -8.097       -2.245       -8.003       -0.943       -5.000       \n",
            "   [1]  is                  0.118        0.000        0.000        0.000        -6.465       -1.019       -0.000       \n",
            "   [0]  now                 0.118        5.004        -7.533       -2.137       -7.258       -1.057       -5.000       \n",
            "   [1]  tv                  0.097        0.000        0.000        0.000        -5.750       -1.071       -0.000       \n",
            "   [0]  fast                0.041        4.723        -8.230       -3.204       -6.455       -1.094       -5.000       \n",
            "   [0]  together            0.026        4.723        -8.310       -3.651       -3.651       -1.214       -2.436       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  intriguing          0.595        0.000        0.000        0.000        -5.362       -0.310       -0.000       \n",
            "   [0]  worth               0.398        7.608        -7.664       -0.922       -6.019       -0.354       -5.000       \n",
            "   [1]  of                  0.550        0.000        0.000        0.000        -5.723       -0.357       -0.000       \n",
            "   [0]  cents               0.442        10.544       -9.172       -0.816       -6.425       -0.411       -5.000       \n",
            "   [1]  drawn               0.411        0.000        0.000        0.000        -6.297       -0.412       -0.000       \n",
            "   [1]  fantasy             0.248        0.000        0.000        0.000        -7.069       -0.440       -0.000       \n",
            "   [1]  come                0.235        0.000        0.000        0.000        -7.936       -0.484       -0.000       \n",
            "   [0]  a                   0.250        3.902        -3.650       -1.387       -8.910       -0.580       -5.000       \n",
            "   [0]  one                 0.262        7.197        -5.497       -1.338       -8.447       -0.663       -5.000       \n",
            "   [0]  the                 0.285        4.178        -3.176       -1.256       -7.981       -0.709       -5.000       \n",
            "   [0]  fire                0.203        3.699        -10.463      -1.592       -7.550       -0.725       -5.000       \n",
            "   [1]  child               0.123        0.000        0.000        0.000        -6.688       -0.728       -0.000       \n",
            "   [1]  s                   0.129        0.000        0.000        0.000        -7.509       -0.774       -0.000       \n",
            "   [0]  because             0.095        11.631       -6.312       -2.357       -8.430       -0.862       -5.000       \n",
            "   [0]  cassi               0.111        9.474        -10.974      -2.194       -6.818       -0.938       -5.000       \n",
            "   [0]  young               0.122        8.238        -7.595       -2.101       -5.191       -1.006       -4.186       \n",
            "   [0]  for                 0.186        3.627        -5.146       -1.681       -3.469       -1.034       -2.435       \n",
            "   [1]  imagine             0.179        0.000        0.000        0.000        -2.008       -1.020       -0.000       \n",
            "   [1]  the                 0.198        0.000        0.000        0.000        -2.254       -0.957       -0.000       \n",
            "   [0]  alien               0.080        8.678        -8.265       -2.530       -2.530       -0.899       -1.631       \n",
            "Samples\n",
            "Sample 0 .  ve disbelief all premise of collection movies in this series it one can famous and buffered with 1970s tells into\n",
            "Sample 1 .  just a in cool in extreme enjoyed it immensely and seriously cinematography every time used is now tv fast together\n",
            "Sample 2 .  intriguing worth of cents drawn fantasy come a one the fire child s because cassi young for imagine the alien\n",
            "\n",
            "\n",
            "targets[[414 5 35 486 3 1817 5680 2 19 111 174 102 30378 613 3206 4 2695 1442 10597 1861][17 5 37 1149 2 297 12998 368 6 6 9 402 1453 78 10 17 14 2 11244 8][3157 41 15548 14 7 12 11184 542 4 646...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16638 414 5 35 486 3 1817 5680 2 19...]...][[1 0 0 1 1 1 0 1 0 1...]...][[16638 414 69849 69849 486 3 1817 69849 2 69849...]...][[414 5 35 486 3 1817 5680 2 19 111...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16638 414 5 35 486 3 1817 5680 2 19...]...][[1 0 0 1 1 1 0 1 0 1...]...][[16638 414 69849 69849 486 3 1817 69849 2 69849...]...][[414 5 473 486 3 1817 25 2 1 111...]...]\n",
            "targets[[24205 5 2 81 1 1031 153 9 723 21 110 238 332 33 86 239 72 10 0 6754][3006 8799 3 417 652 510 4133 8 537 25 254 8 952 1265 8 0 484 3 21845 8][1550 5 0 117 3 0 733 1541 1012 98...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1882 24205 5 2 81 1 1031 153 9 723...]...][[0 0 1 0 1 0 1 1 0 0...]...][[1882 69849 69849 2 69849 1 69849 153 9 69849...]...][[24205 5 2 81 1 1031 153 9 723 21...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1882 24205 5 2 81 1 1031 153 9 723...]...][[0 0 1 0 1 0 1 1 0 0...]...][[1882 69849 69849 2 69849 1 69849 153 9 69849...]...][[13 348 2 369 1 413 153 9 12 1036...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 51 26 1071...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 64 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 1 3 48...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "global_step: 359\n",
            " perplexity: 891.595\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            " percent of 3-grams captured: 0.153.\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            " percent of 2-grams captured: 0.560.\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            " percent of 4-grams captured: 0.020.\n",
            " geometric_avg: 0.119.\n",
            " arithmetic_avg: 0.244.\n",
            "global_step: 359\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66811\n",
            " G train loss: 161.81311\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 64 8 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[29 3 0 250 98 123 92 23 64 8...]...][[1 1 1 1 1 1 1 0 0 0...]...][[29 3 0 250 98 123 92 23 69849 69849...]...][[3 0 250 98 123 92 23 0 42 242...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  of                  0.613        0.000        0.000        0.000        -1.283       -0.291       -0.000       \n",
            "   [1]  the                 0.617        0.000        0.000        0.000        -1.440       -0.344       -0.000       \n",
            "   [1]  worst               0.513        0.000        0.000        0.000        -1.617       -0.332       -0.000       \n",
            "   [1]  movies              0.534        0.000        0.000        0.000        -1.815       -0.283       -0.000       \n",
            "   [1]  ever                0.524        0.000        0.000        0.000        -2.038       -0.297       -0.000       \n",
            "   [1]  made                0.619        0.000        0.000        0.000        -2.288       -0.311       -0.000       \n",
            "   [1]  not                 0.632        0.000        0.000        0.000        -2.568       -0.345       -0.000       \n",
            "   [0]  the                 0.636        6.107        -2.743       -0.452       -2.884       -0.332       -2.551       \n",
            "   [0]  just                0.593        4.055        -5.551       -0.522       -2.730       -0.306       -2.423       \n",
            "   [0]  am                  0.589        2.752        -6.874       -0.529       -2.478       -0.263       -2.216       \n",
            "   [0]  best                0.622        12.989       -6.436       -0.474       -2.189       -0.263       -1.925       \n",
            "   [0]  real                0.550        7.780        -7.743       -0.598       -1.925       -0.276       -1.649       \n",
            "   [1]  which               0.582        0.000        0.000        0.000        -1.490       -0.252       -0.000       \n",
            "   [0]  one                 0.588        9.730        -5.385       -0.531       -1.673       -0.273       -1.399       \n",
            "   [1]  an                  0.630        0.000        0.000        0.000        -1.282       -0.278       -0.000       \n",
            "   [0]  now                 0.619        7.732        -7.365       -0.479       -1.439       -0.293       -1.146       \n",
            "   [0]  certainly           0.535        13.179       -7.670       -0.625       -1.078       -0.280       -0.798       \n",
            "   [0]  or                  0.601        3.712        -5.775       -0.509       -0.509       -0.246       -0.262       \n",
            "   [1]  cinematic           0.580        0.000        0.000        0.000        0.000        -0.270       0.000        \n",
            "   [1]  dung                0.617        0.000        0.000        0.000        0.000        -0.259       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  rhonda              0.493        2.845        -14.078      -0.708       -6.108       -0.312       -5.000       \n",
            "   [1]  name                0.343        0.000        0.000        0.000        -6.062       -0.358       -0.000       \n",
            "   [0]  it                  0.293        5.718        -4.260       -1.228       -6.806       -0.420       -5.000       \n",
            "   [1]  karate              0.214        0.000        0.000        0.000        -6.263       -0.483       -0.000       \n",
            "   [0]  so                  0.189        8.960        -5.700       -1.668       -7.031       -0.570       -5.000       \n",
            "   [0]  a                   0.200        6.573        -2.808       -1.608       -6.021       -0.646       -5.000       \n",
            "   [0]  the                 0.227        5.759        -2.744       -1.483       -4.955       -0.702       -4.253       \n",
            "   [0]  with                0.342        5.414        -4.859       -1.074       -3.897       -0.728       -3.169       \n",
            "   [0]  said                0.459        6.537        -8.156       -0.779       -3.170       -0.696       -2.474       \n",
            "   [0]  time                0.409        8.398        -5.810       -0.893       -2.684       -0.622       -2.062       \n",
            "   [1]  this                0.447        0.000        0.000        0.000        -2.010       -0.560       -0.000       \n",
            "   [1]  movie               0.430        0.000        0.000        0.000        -2.256       -0.496       -0.000       \n",
            "   [0]  dovetails           0.422        4.641        -14.411      -0.864       -2.533       -0.461       -2.072       \n",
            "   [0]  is                  0.464        2.981        -4.644       -0.768       -1.874       -0.444       -1.430       \n",
            "   [1]  perfect             0.507        0.000        0.000        0.000        -1.242       -0.436       -0.000       \n",
            "   [0]  cathartic           0.496        8.496        -14.037      -0.701       -1.394       -0.422       -0.972       \n",
            "   [1]  of                  0.628        0.000        0.000        0.000        -0.779       -0.413       -0.000       \n",
            "   [0]  s                   0.617        2.993        -4.613       -0.483       -0.874       -0.376       -0.498       \n",
            "   [1]  brainless           0.577        0.000        0.000        0.000        -0.440       -0.346       -0.000       \n",
            "   [0]  the                 0.610        13.130       -2.810       -0.494       -0.494       -0.327       -0.167       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  spoiler             0.320        9.215        -6.922       -1.138       -4.677       -0.304       -4.373       \n",
            "   [1]  as                  0.364        0.000        0.000        0.000        -3.974       -0.342       -0.000       \n",
            "   [0]  your                0.382        14.055       -7.701       -0.962       -4.461       -0.399       -4.062       \n",
            "   [1]  the                 0.404        0.000        0.000        0.000        -3.929       -0.444       -0.000       \n",
            "   [0]  have                0.365        13.121       -4.874       -1.007       -4.411       -0.468       -3.943       \n",
            "   [0]  disbelief           0.413        4.062        -11.690      -0.884       -3.822       -0.472       -3.350       \n",
            "   [1]  a                   0.410        0.000        0.000        0.000        -3.298       -0.476       -0.000       \n",
            "   [0]  bugs                0.336        11.142       -9.779       -1.092       -3.703       -0.468       -3.235       \n",
            "   [0]  between             0.390        13.760       -7.343       -0.942       -2.931       -0.459       -2.472       \n",
            "   [0]  going               0.295        5.209        -7.352       -1.220       -2.233       -0.473       -1.760       \n",
            "   [1]  an                  0.367        0.000        0.000        0.000        -1.137       -0.484       -0.000       \n",
            "   [1]  armless             0.402        0.000        0.000        0.000        -1.276       -0.511       -0.000       \n",
            "   [1]  circus              0.355        0.000        0.000        0.000        -1.433       -0.521       -0.000       \n",
            "   [0]  horror              0.447        12.483       -6.903       -0.805       -1.609       -0.520       -1.089       \n",
            "   [1]  wielder             0.492        0.000        0.000        0.000        -0.902       -0.522       -0.000       \n",
            "   [1]  he                  0.506        0.000        0.000        0.000        -1.013       -0.507       -0.000       \n",
            "   [0]  was                 0.513        10.046       -4.263       -0.668       -1.137       -0.485       -0.652       \n",
            "   [1]  by                  0.468        0.000        0.000        0.000        -0.527       -0.455       -0.000       \n",
            "   [1]  throwing            0.553        0.000        0.000        0.000        -0.591       -0.438       -0.000       \n",
            "   [0]  i                   0.515        12.927       -3.587       -0.664       -0.664       -0.439       -0.225       \n",
            "Samples\n",
            "Sample 0 .  of the worst movies ever made not the just am best real which one an now certainly or cinematic dung\n",
            "Sample 1 .  rhonda name it karate so a the with said time this movie dovetails is perfect cathartic of s brainless the\n",
            "Sample 2 .  spoiler as your the have disbelief a bugs between going an armless circus horror wielder he was by throwing i\n",
            "\n",
            "\n",
            "targets[[84 151 9 196 99 318 10 17 13 87 5 10 614 11 0 8561 165 386 2 1686][81 11360 3 584 24951 5 74 19 8 174 95 0 225 0 5054 1069 0 608 3 1132][263 214 8 26 17196 590 284 9695 45 8084...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 84 151 9 196 99 318 10 17 13...]...][[0 0 1 0 0 1 1 1 0 0...]...][[0 69849 69849 9 69849 69849 318 10 17 69849...]...][[84 151 9 196 99 318 10 17 13 87...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 84 151 9 196 99 318 10 17 13...]...][[0 0 1 0 0 1 1 1 0 0...]...][[0 69849 69849 9 69849 69849 318 10 17 69849...]...][[711 95 9 3 58308 318 10 17 628 13...]...]\n",
            "targets[[12 56 6072 7 10140 978 2243 317 5 394 0 116 5 37 74 11 20 232 3454 1][5 23 64 0 4144 3 30 1224 98 18 0 872 184 17 123 284 3467 1 9402 2000][194 227 2041 26370 35 554 1031 279 34 1193...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[39 12 56 6072 7 10140 978 2243 317 5...]...][[1 1 0 0 0 1 1 1 1 1...]...][[39 12 56 69849 69849 69849 978 2243 317 5...]...][[12 56 6072 7 10140 978 2243 317 5 394...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[39 12 56 6072 7 10140 978 2243 317 5...]...][[1 1 0 0 0 1 1 1 1 1...]...][[39 12 56 69849 69849 69849 978 2243 317 5...]...][[12 56 12 1515 126 978 2243 317 5 394...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 201 67518 10 657 6380 458 23266...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 451 55 4 91 6380 7745 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 128 1027 4 4 6380 71 95...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "global_step: 364\n",
            " perplexity: 894.439\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            " percent of 3-grams captured: 0.166.\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            " percent of 2-grams captured: 0.580.\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            " percent of 4-grams captured: 0.030.\n",
            " geometric_avg: 0.143.\n",
            " arithmetic_avg: 0.259.\n",
            "global_step: 364\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66622\n",
            " G train loss: 162.32452\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 451 55 4 91 6380 7745 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 2789 343 201 451 55 4 91 6380 7745...]...][[1 1 1 0 0 0 0 1 0 0...]...][[10 2789 343 201 69849 69849 69849 69849 6380 69849...]...][[2789 343 201 1027 459 150 162 6380 15 17...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  indie               0.594        0.000        0.000        0.000        -3.443       -0.293       -0.000       \n",
            "   [1]  sex                 0.516        0.000        0.000        0.000        -3.865       -0.361       -0.000       \n",
            "   [1]  comedy              0.509        0.000        0.000        0.000        -4.339       -0.333       -0.000       \n",
            "   [0]  expecting           0.549        10.884       -8.336       -0.600       -4.872       -0.344       -4.528       \n",
            "   [0]  4                   0.444        6.503        -8.170       -0.812       -4.796       -0.364       -4.431       \n",
            "   [0]  doesn               0.342        3.682        -7.321       -1.073       -4.472       -0.309       -4.164       \n",
            "   [0]  makes               0.378        6.604        -7.702       -0.972       -3.816       -0.288       -3.528       \n",
            "   [1]  billing             0.398        0.000        0.000        0.000        -3.193       -0.357       -0.000       \n",
            "   [0]  with                0.519        13.681       -5.266       -0.657       -3.584       -0.407       -3.177       \n",
            "   [0]  movie               0.464        3.053        -3.861       -0.767       -3.287       -0.481       -2.806       \n",
            "   [1]  caroms              0.414        0.000        0.000        0.000        -2.828       -0.431       -0.000       \n",
            "   [1]  builds              0.546        0.000        0.000        0.000        -3.175       -0.380       -0.000       \n",
            "   [1]  up                  0.561        0.000        0.000        0.000        -3.565       -0.432       -0.000       \n",
            "   [0]  word                0.421        3.694        -7.874       -0.864       -4.002       -0.406       -3.596       \n",
            "   [0]  been                0.391        2.758        -6.417       -0.938       -3.522       -0.306       -3.216       \n",
            "   [1]  humorous            0.335        0.000        0.000        0.000        -2.901       -0.305       -0.000       \n",
            "   [0]  i                   0.330        3.091        -3.469       -1.110       -3.257       -0.308       -2.949       \n",
            "   [0]  be                  0.433        13.270       -5.330       -0.838       -2.411       -0.346       -2.065       \n",
            "   [0]  only                0.429        12.906       -6.265       -0.846       -1.766       -0.432       -1.334       \n",
            "   [0]  some                0.356        5.985        -5.589       -1.034       -1.034       -0.432       -0.602       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  s                   0.571        0.000        0.000        0.000        -4.367       -0.306       -0.000       \n",
            "   [1]  probably            0.522        0.000        0.000        0.000        -4.903       -0.349       -0.000       \n",
            "   [0]  movie               0.498        6.729        -3.184       -0.697       -5.504       -0.367       -5.000       \n",
            "   [1]  point               0.395        0.000        0.000        0.000        -5.397       -0.382       -0.000       \n",
            "   [0]  living              0.431        4.236        -9.609       -0.842       -6.060       -0.397       -5.000       \n",
            "   [1]  trying              0.343        0.000        0.000        0.000        -5.858       -0.423       -0.000       \n",
            "   [1]  to                  0.397        0.000        0.000        0.000        -6.577       -0.452       -0.000       \n",
            "   [1]  describe            0.406        0.000        0.000        0.000        -7.384       -0.476       -0.000       \n",
            "   [0]  it                  0.332        14.520       -2.907       -1.102       -8.290       -0.483       -5.000       \n",
            "   [1]  teen                0.209        0.000        0.000        0.000        -8.070       -0.480       -0.000       \n",
            "   [0]  i                   0.196        14.295       -2.918       -1.632       -9.060       -0.497       -5.000       \n",
            "   [0]  trouble             0.164        8.502        -9.919       -1.810       -8.340       -0.540       -5.000       \n",
            "   [0]  the                 0.178        15.379       -2.824       -1.725       -7.331       -0.588       -5.000       \n",
            "   [0]  this                0.201        4.028        -4.314       -1.604       -6.294       -0.628       -5.000       \n",
            "   [0]  that                0.213        3.654        -3.932       -1.546       -5.264       -0.644       -4.620       \n",
            "   [0]  but                 0.280        7.726        -5.045       -1.273       -4.175       -0.629       -3.546       \n",
            "   [0]  this                0.301        5.397        -4.314       -1.200       -3.258       -0.602       -2.656       \n",
            "   [0]  horrid              0.346        9.312        -11.464      -1.062       -2.311       -0.560       -1.751       \n",
            "   [0]  quality             0.246        5.127        -7.955       -1.402       -1.402       -0.512       -0.889       \n",
            "   [1]  already             0.207        0.000        0.000        0.000        0.000        -0.476       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  wont                0.595        0.000        0.000        0.000        -0.900       -0.294       -0.000       \n",
            "   [1]  believe             0.663        0.000        0.000        0.000        -1.010       -0.336       -0.000       \n",
            "   [0]  be                  0.709        3.653        -5.792       -0.344       -1.134       -0.353       -0.781       \n",
            "   [1]  i                   0.661        0.000        0.000        0.000        -0.887       -0.340       -0.000       \n",
            "   [1]  was                 0.670        0.000        0.000        0.000        -0.996       -0.286       -0.000       \n",
            "   [1]  16                  0.665        0.000        0.000        0.000        -1.119       -0.264       -0.000       \n",
            "   [1]  when                0.705        0.000        0.000        0.000        -1.256       -0.251       -0.000       \n",
            "   [0]  bring               0.692        3.467        -9.254       -0.368       -1.410       -0.260       -1.150       \n",
            "   [0]  bat                 0.716        6.731        -10.780      -0.335       -1.170       -0.243       -0.927       \n",
            "   [0]  s                   0.712        5.902        -4.575       -0.340       -0.938       -0.241       -0.697       \n",
            "   [1]  proudly             0.754        0.000        0.000        0.000        -0.671       -0.230       -0.000       \n",
            "   [1]  we                  0.780        0.000        0.000        0.000        -0.753       -0.237       -0.000       \n",
            "   [1]  hail                0.748        0.000        0.000        0.000        -0.845       -0.233       -0.000       \n",
            "   [1]  thirty              0.722        0.000        0.000        0.000        -0.949       -0.198       -0.000       \n",
            "   [0]  one                 0.726        6.538        -5.350       -0.320       -1.066       -0.171       -0.895       \n",
            "   [1]  later               0.734        0.000        0.000        0.000        -0.837       -0.180       -0.000       \n",
            "   [0]  real                0.676        3.566        -6.836       -0.392       -0.940       -0.190       -0.750       \n",
            "   [0]  we                  0.732        3.761        -7.208       -0.312       -0.615       -0.176       -0.439       \n",
            "   [0]  unlocking           0.712        3.391        -13.670      -0.339       -0.339       -0.218       -0.122       \n",
            "   [1]  remembered          0.777        0.000        0.000        0.000        0.000        -0.219       0.000        \n",
            "Samples\n",
            "Sample 0 .  indie sex comedy expecting 4 doesn makes billing with movie caroms builds up word been humorous i be only some\n",
            "Sample 1 .  s probably movie point living trying to describe it teen i trouble the this that but this horrid quality already\n",
            "Sample 2 .  wont believe be i was 16 when bring bat s proudly we hail thirty one later real we unlocking remembered\n",
            "\n",
            "\n",
            "targets[[619 4398 282 1065 11 56 534 96 123 2228 2460 13989 4 0 6392 3 2 243 507 16][89 21 121 87 7 195 49 2921 9 89 21 456 46 7 13 2 297 63 9 856][70 651 8 1806 2591 374 4784 1 681 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 619 4398 282 1065 11 56 534 96 123...]...][[0 1 1 0 0 0 1 1 1 1...]...][[2 69849 4398 282 69849 69849 69849 534 96 123...]...][[619 4398 282 1065 11 56 534 96 123 2228...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 619 4398 282 1065 11 56 534 96 123...]...][[0 1 1 0 0 0 1 1 1 1...]...][[2 69849 4398 282 69849 69849 69849 534 96 123...]...][[19 4398 282 18 341 49 534 96 123 2228...]...]\n",
            "targets[[1116 19 36 5051 8 61 0 1500 3 6845 14867 303 377 25 6039 15 2 160 30388 4357][139 77 2754 2 394 19 6 6 815 9 232 987 11 233 9 140 460 1 27 67][140 23 2 8791 147 882 123 27 77 18...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2952 1116 19 36 5051 8 61 0 1500 3...]...][[1 1 1 1 1 0 0 0 1 1...]...][[2952 1116 19 36 5051 8 69849 69849 69849 3...]...][[1116 19 36 5051 8 61 0 1500 3 6845...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2952 1116 19 36 5051 8 61 0 1500 3...]...][[1 1 1 1 1 0 0 0 1 1...]...][[2952 1116 19 36 5051 8 69849 69849 69849 3...]...][[1116 19 36 5051 8 539 7 4 3 6845...]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[353 32756 199 3895 62324 436 1 4242 0 6...]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[17 32756 199 3895 5998 436 1 4242 868 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[32 32756 199 3895 35 436 1 4242 29 6...]...]\n",
            "I0310 00:35:12.858817 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "global_step: 369\n",
            " perplexity: 900.280\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            " percent of 3-grams captured: 0.162.\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            " percent of 2-grams captured: 0.583.\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            " percent of 4-grams captured: 0.026.\n",
            " geometric_avg: 0.135.\n",
            " arithmetic_avg: 0.257.\n",
            "global_step: 369\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66527\n",
            " G train loss: 161.99614\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[17 32756 199 3895 5998 436 1 4242 868 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 32756 199 3895 5998 436 1 4242 868...]...][[0 1 1 1 0 1 1 1 0 1...]...][[10 69849 32756 199 3895 69849 436 1 4242 69849...]...][[1100 32756 199 3895 21 436 1 4242 422 6...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  u                   0.450        2.911        -8.144       -0.798       -1.597       -0.303       -1.294       \n",
            "   [1]  alternated          0.497        0.000        0.000        0.000        -0.898       -0.365       -0.000       \n",
            "   [1]  between             0.567        0.000        0.000        0.000        -1.008       -0.394       -0.000       \n",
            "   [1]  captivating         0.609        0.000        0.000        0.000        -1.131       -0.384       -0.000       \n",
            "   [0]  t                   0.604        14.627       -4.634       -0.504       -1.270       -0.350       -0.920       \n",
            "   [1]  drama               0.547        0.000        0.000        0.000        -0.860       -0.310       -0.000       \n",
            "   [1]  and                 0.576        0.000        0.000        0.000        -0.965       -0.284       -0.000       \n",
            "   [1]  formulaic           0.707        0.000        0.000        0.000        -1.084       -0.271       -0.000       \n",
            "   [0]  episode             0.723        8.940        -8.156       -0.325       -1.217       -0.233       -0.984       \n",
            "   [1]  br                  0.753        0.000        0.000        0.000        -1.001       -0.179       -0.000       \n",
            "   [1]  br                  0.780        0.000        0.000        0.000        -1.124       -0.120       -0.000       \n",
            "   [0]  nobody              0.749        2.629        -10.245      -0.289       -1.262       -0.063       -1.199       \n",
            "   [0]  and                 0.758        14.451       -3.007       -0.277       -1.092       -0.017       -1.075       \n",
            "   [1]  drama               0.723        0.000        0.000        0.000        -0.915       0.010        -0.000       \n",
            "   [1]  part                0.687        0.000        0.000        0.000        -1.027       0.017        -0.000       \n",
            "   [0]  i                   0.678        3.930        -3.196       -0.389       -1.153       -0.004       -1.150       \n",
            "   [0]  shall               0.593        13.278       -8.729       -0.523       -0.858       -0.046       -0.813       \n",
            "   [0]  about               0.686        8.769        -5.581       -0.376       -0.376       -0.110       -0.267       \n",
            "   [1]  by                  0.648        0.000        0.000        0.000        0.000        -0.157       0.000        \n",
            "   [1]  the                 0.652        0.000        0.000        0.000        0.000        -0.195       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  philmore            0.497        5.701        -13.267      -0.700       -3.743       -0.296       -3.447       \n",
            "   [0]  reviews             0.473        3.437        -7.318       -0.749       -3.417       -0.327       -3.090       \n",
            "   [1]  gun                 0.433        0.000        0.000        0.000        -2.995       -0.348       -0.000       \n",
            "   [0]  in                  0.431        3.947        -4.571       -0.842       -3.362       -0.361       -3.001       \n",
            "   [0]  captain             0.527        7.417        -11.088      -0.641       -2.829       -0.370       -2.459       \n",
            "   [0]  making              0.506        6.640        -7.633       -0.682       -2.457       -0.367       -2.090       \n",
            "   [1]  forgotten           0.315        0.000        0.000        0.000        -1.993       -0.348       -0.000       \n",
            "   [0]  a                   0.319        6.947        -3.506       -1.143       -2.237       -0.361       -1.876       \n",
            "   [1]  but                 0.426        0.000        0.000        0.000        -1.229       -0.395       -0.000       \n",
            "   [1]  caused              0.410        0.000        0.000        0.000        -1.379       -0.409       -0.000       \n",
            "   [0]  first               0.465        3.567        -6.094       -0.767       -1.548       -0.406       -1.143       \n",
            "   [1]  minor               0.446        0.000        0.000        0.000        -0.878       -0.390       -0.000       \n",
            "   [1]  storm               0.508        0.000        0.000        0.000        -0.985       -0.369       -0.000       \n",
            "   [1]  of                  0.583        0.000        0.000        0.000        -1.106       -0.351       -0.000       \n",
            "   [0]  misfortune          0.614        10.645       -10.571      -0.487       -1.242       -0.319       -0.923       \n",
            "   [1]  interest            0.542        0.000        0.000        0.000        -0.847       -0.283       -0.000       \n",
            "   [0]  the                 0.553        7.220        -3.222       -0.592       -0.951       -0.249       -0.703       \n",
            "   [1]  in                  0.545        0.000        0.000        0.000        -0.404       -0.242       -0.000       \n",
            "   [1]  1955                0.490        0.000        0.000        0.000        -0.453       -0.243       -0.000       \n",
            "   [0]  directed            0.601        6.207        -8.062       -0.509       -0.509       -0.255       -0.255       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  bodyguard           0.579        0.000        0.000        0.000        -2.557       -0.285       -0.000       \n",
            "   [1]  is                  0.555        0.000        0.000        0.000        -2.871       -0.317       -0.000       \n",
            "   [0]  been                0.494        12.960       -6.128       -0.705       -3.223       -0.292       -2.931       \n",
            "   [0]  the                 0.509        2.951        -3.207       -0.675       -2.827       -0.264       -2.563       \n",
            "   [1]  love                0.504        0.000        0.000        0.000        -2.416       -0.260       -0.000       \n",
            "   [0]  own                 0.579        6.138        -6.904       -0.547       -2.712       -0.260       -2.452       \n",
            "   [0]  on                  0.641        7.942        -5.556       -0.445       -2.431       -0.265       -2.166       \n",
            "   [0]  and                 0.629        7.336        -3.455       -0.464       -2.229       -0.249       -1.980       \n",
            "   [0]  and                 0.625        13.362       -3.465       -0.471       -1.982       -0.205       -1.776       \n",
            "   [1]  characters          0.561        0.000        0.000        0.000        -1.696       -0.173       -0.000       \n",
            "   [1]  with                0.659        0.000        0.000        0.000        -1.904       -0.150       -0.000       \n",
            "   [0]  done                0.516        7.068        -7.770       -0.661       -2.138       -0.166       -1.972       \n",
            "   [0]  a                   0.512        6.844        -3.263       -0.670       -1.658       -0.130       -1.528       \n",
            "   [1]  idiosyncrasies      0.525        0.000        0.000        0.000        -1.110       -0.152       -0.000       \n",
            "   [0]  boring              0.539        3.303        -7.819       -0.619       -1.246       -0.179       -1.067       \n",
            "   [0]  of                  0.631        5.815        -3.491       -0.461       -0.704       -0.195       -0.509       \n",
            "   [1]  is                  0.602        0.000        0.000        0.000        -0.273       -0.208       -0.000       \n",
            "   [1]  intriguing          0.662        0.000        0.000        0.000        -0.306       -0.179       -0.000       \n",
            "   [1]  enough              0.681        0.000        0.000        0.000        -0.344       -0.170       -0.000       \n",
            "   [0]  this                0.680        4.097        -3.741       -0.386       -0.386       -0.141       -0.245       \n",
            "Samples\n",
            "Sample 0 .  u alternated between captivating t drama and formulaic episode br br nobody and drama part i shall about by the\n",
            "Sample 1 .  philmore reviews gun in captain making forgotten a but caused first minor storm of misfortune interest the in 1955 directed\n",
            "Sample 2 .  bodyguard is been the love own on and and characters with done a idiosyncrasies boring of is intriguing enough this\n",
            "\n",
            "\n",
            "targets[[17 5 2 997 36 0 457 4 0 129 0 101 80 1 136 47 0 543 491 90][74 19 53 53 53 74 19 7 12 2 10027 18 7 5 23 273 3299 185 10 1071][203 66 16 74 17 4603 247 528 2 461...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 2 997 36 0 457 4 0...]...][[0 1 0 1 0 0 0 1 0 1...]...][[10 69849 5 69849 997 69849 69849 69849 4 69849...]...][[17 5 2 997 36 0 457 4 0 129...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 2 997 36 0 457 4 0...]...][[0 1 0 1 0 0 0 1 0 1...]...][[10 69849 5 69849 997 69849 69849 69849 4 69849...]...][[60 5 41 997 633 184 16 4 42463 129...]...]\n",
            "targets[[5095 1070 4 479 966 12 3115 1667 1056 201 78 2 3631 917 725 15 0 914 107 193][39 20 27 7 157 24170 22 60 2619 105 165 84 3 30 58 152 9 38 4 103][6625 5 437 379 0 109 5 53 348 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5256 5095 1070 4 479 966 12 3115 1667 1056...]...][[0 1 0 1 1 1 0 0 0 1...]...][[5256 69849 1070 69849 479 966 12 69849 69849 69849...]...][[5095 1070 4 479 966 12 3115 1667 1056 201...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5256 5095 1070 4 479 966 12 3115 1667 1056...]...][[0 1 0 1 1 1 0 0 0 1...]...][[5256 69849 1070 69849 479 966 12 69849 69849 69849...]...][[37 1070 3752 479 966 12 74 23 782 201...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 60 1288 3 193 156 5920 257 656 1...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 2 340 3 193 156 5920 257 14340 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 4 12 3 193 156 5920 257 7 1...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "global_step: 374\n",
            " perplexity: 900.888\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            " percent of 3-grams captured: 0.163.\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            " percent of 2-grams captured: 0.598.\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            " percent of 4-grams captured: 0.032.\n",
            " geometric_avg: 0.146.\n",
            " arithmetic_avg: 0.264.\n",
            "global_step: 374\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66555\n",
            " G train loss: 161.04105\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 2 340 3 193 156 5920 257 14340 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 140 2 340 3 193 156 5920 257 14340...]...][[1 0 0 1 1 1 1 1 0 1...]...][[9 140 69849 69849 3 193 156 5920 257 69849...]...][[140 10 13 3 193 156 5920 257 2181 1...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  m                   0.457        0.000        0.000        0.000        -3.166       -0.299       -0.000       \n",
            "   [0]  this                0.506        2.975        -3.006       -0.682       -3.554       -0.313       -3.242       \n",
            "   [0]  was                 0.504        7.266        -4.098       -0.684       -3.225       -0.350       -2.875       \n",
            "   [1]  of                  0.586        0.000        0.000        0.000        -2.853       -0.360       -0.000       \n",
            "   [1]  both                0.455        0.000        0.000        0.000        -3.203       -0.389       -0.000       \n",
            "   [1]  actors              0.413        0.000        0.000        0.000        -3.596       -0.321       -0.000       \n",
            "   [1]  singers             0.445        0.000        0.000        0.000        -4.037       -0.298       -0.000       \n",
            "   [1]  especially          0.386        0.000        0.000        0.000        -4.532       -0.329       -0.000       \n",
            "   [0]  vehicle             0.191        15.217       -8.166       -1.655       -5.088       -0.330       -4.758       \n",
            "   [1]  and                 0.237        0.000        0.000        0.000        -3.854       -0.292       -0.000       \n",
            "   [1]  when                0.313        0.000        0.000        0.000        -4.327       -0.372       -0.000       \n",
            "   [0]  and                 0.314        3.216        -2.862       -1.158       -4.858       -0.455       -4.403       \n",
            "   [0]  i                   0.278        6.152        -3.215       -1.279       -4.154       -0.467       -3.687       \n",
            "   [0]  or                  0.348        9.452        -6.048       -1.056       -3.227       -0.444       -2.784       \n",
            "   [0]  if                  0.352        3.913        -6.629       -1.045       -2.438       -0.452       -1.986       \n",
            "   [0]  group               0.373        4.096        -8.518       -0.985       -1.563       -0.430       -1.133       \n",
            "   [1]  and                 0.373        0.000        0.000        0.000        -0.649       -0.416       -0.000       \n",
            "   [1]  watch               0.393        0.000        0.000        0.000        -0.729       -0.392       -0.000       \n",
            "   [0]  are                 0.441        2.397        -5.143       -0.818       -0.818       -0.382       -0.436       \n",
            "   [1]  trailer             0.273        0.000        0.000        0.000        0.000        -0.386       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  elements            0.505        7.588        -9.221       -0.683       -4.274       -0.300       -3.974       \n",
            "   [0]  is                  0.508        3.459        -2.365       -0.677       -4.032       -0.332       -3.700       \n",
            "   [0]  a                   0.485        6.862        -2.215       -0.724       -3.767       -0.348       -3.419       \n",
            "   [0]  the                 0.491        9.010        -2.351       -0.712       -3.415       -0.351       -3.065       \n",
            "   [1]  the                 0.492        0.000        0.000        0.000        -3.035       -0.355       -0.000       \n",
            "   [0]  the                 0.489        6.427        -2.280       -0.715       -3.407       -0.357       -3.051       \n",
            "   [1]  to                  0.522        0.000        0.000        0.000        -3.023       -0.356       -0.000       \n",
            "   [0]  take                0.494        7.628        -8.566       -0.705       -3.394       -0.359       -3.034       \n",
            "   [0]  the                 0.493        8.308        -2.319       -0.707       -3.019       -0.348       -2.671       \n",
            "   [0]  the                 0.492        2.550        -2.329       -0.709       -2.595       -0.346       -2.249       \n",
            "   [0]  premise             0.411        8.220        -8.480       -0.889       -2.117       -0.347       -1.771       \n",
            "   [1]  of                  0.516        0.000        0.000        0.000        -1.379       -0.345       -0.000       \n",
            "   [1]  any                 0.442        0.000        0.000        0.000        -1.548       -0.379       -0.000       \n",
            "   [0]  i                   0.419        2.824        -3.265       -0.871       -1.738       -0.370       -1.367       \n",
            "   [1]  the                 0.435        0.000        0.000        0.000        -0.974       -0.381       -0.000       \n",
            "   [1]  movies              0.474        0.000        0.000        0.000        -1.093       -0.396       -0.000       \n",
            "   [1]  i                   0.437        0.000        0.000        0.000        -1.227       -0.407       -0.000       \n",
            "   [0]  i                   0.413        5.040        -3.266       -0.885       -1.378       -0.397       -0.981       \n",
            "   [1]  seen                0.565        0.000        0.000        0.000        -0.554       -0.396       -0.000       \n",
            "   [0]  the                 0.537        4.906        -2.356       -0.621       -0.621       -0.413       -0.209       \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  movie               0.500        12.428       -2.564       -0.693       -2.412       -0.296       -2.116       \n",
            "   [1]  in                  0.486        0.000        0.000        0.000        -1.930       -0.335       -0.000       \n",
            "   [1]  my                  0.575        0.000        0.000        0.000        -2.166       -0.361       -0.000       \n",
            "   [1]  opinion             0.479        0.000        0.000        0.000        -2.432       -0.366       -0.000       \n",
            "   [1]  is                  0.494        0.000        0.000        0.000        -2.731       -0.356       -0.000       \n",
            "   [1]  a                   0.480        0.000        0.000        0.000        -3.066       -0.360       -0.000       \n",
            "   [0]  it                  0.413        5.818        -3.706       -0.884       -3.442       -0.366       -3.076       \n",
            "   [0]  mujhse              0.419        14.015       -13.854      -0.870       -2.872       -0.383       -2.489       \n",
            "   [0]  directing           0.420        11.877       -9.327       -0.868       -2.248       -0.409       -1.838       \n",
            "   [1]  movie               0.398        0.000        0.000        0.000        -1.549       -0.435       -0.000       \n",
            "   [1]  one                 0.407        0.000        0.000        0.000        -1.739       -0.462       -0.000       \n",
            "   [1]  of                  0.490        0.000        0.000        0.000        -1.952       -0.478       -0.000       \n",
            "   [1]  the                 0.467        0.000        0.000        0.000        -2.191       -0.469       -0.000       \n",
            "   [1]  greatest            0.464        0.000        0.000        0.000        -2.460       -0.447       -0.000       \n",
            "   [0]  if                  0.471        7.257        -6.141       -0.752       -2.762       -0.425       -2.337       \n",
            "   [0]  lightest            0.452        5.600        -12.676      -0.793       -2.256       -0.403       -1.853       \n",
            "   [0]  episode             0.511        4.260        -7.755       -0.671       -1.643       -0.388       -1.255       \n",
            "   [0]  of                  0.575        4.256        -3.868       -0.553       -1.091       -0.377       -0.715       \n",
            "   [0]  the                 0.546        4.333        -2.986       -0.605       -0.605       -0.350       -0.255       \n",
            "   [1]  my                  0.614        0.000        0.000        0.000        0.000        -0.318       0.000        \n",
            "Samples\n",
            "Sample 0 .  m this was of both actors singers especially vehicle and when and i or if group and watch are trailer\n",
            "Sample 1 .  elements is a the the the to take the the premise of any i the movies i i seen the\n",
            "Sample 2 .  movie in my opinion is a it mujhse directing movie one of the greatest if lightest episode of the my\n",
            "\n",
            "\n",
            "targets[[29 3 0 88 901 1211 1 4316 1146 98 22 0 3246 123 92 10 5 229 1674 4][317 2502 36 35 1914 3505 4 168 31 0 3615 20 59 103 7 12 43 2 461 2217][312 118 23 942 47 2 1594 10 17 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[437 29 3 0 88 901 1211 1 4316 1146...]...][[0 0 0 0 0 0 1 0 1 1...]...][[437 69849 69849 69849 69849 69849 69849 1 69849 1146...]...][[29 3 0 88 901 1211 1 4316 1146 98...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[437 29 3 0 88 901 1211 1 4316 1146...]...][[0 0 0 0 0 0 1 0 1 1...]...][[437 69849 69849 69849 69849 69849 69849 1 69849 1146...]...][[167 21 53234 43 379 53 1 19 1146 98...]...]\n",
            "targets[[793 4 93 71 262 11 0 834 3 10 29385 2333 184 680 13 1403 33 0 851 227][371 856 137 397 36 10 17 35 3309 177 3 1031 1 155 3154 141 27 68216 69 6][3320 1704 8 112 15 4197 15829 10 122 982...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[75 793 4 93 71 262 11 0 834 3...]...][[0 0 0 0 0 1 0 0 0 1...]...][[75 69849 69849 69849 69849 69849 11 69849 69849 69849...]...][[793 4 93 71 262 11 0 834 3 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[75 793 4 93 71 262 11 0 834 3...]...][[0 0 0 0 0 1 0 0 0 1...]...][[75 69849 69849 69849 69849 69849 11 69849 69849 69849...]...][[543 16616 33 11 139 11 2977 58431 121 10...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[17 307 467 80 23 395 149 312 17 0...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[2 366 9 80 23 395 149 10 17 8386...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[1543 343 18 80 23 395 149 104 17 19...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "global_step: 379\n",
            " perplexity: 901.623\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            " percent of 3-grams captured: 0.159.\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            " percent of 2-grams captured: 0.590.\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            " percent of 4-grams captured: 0.028.\n",
            " geometric_avg: 0.139.\n",
            " arithmetic_avg: 0.259.\n",
            "global_step: 379\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66490\n",
            " G train loss: 160.79492\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[2 366 9 80 23 395 149 10 17 8386...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 2 366 9 80 23 395 149 10 17...]...][[0 0 0 1 1 1 1 0 1 0...]...][[16 69849 69849 69849 80 23 395 149 69849 17...]...][[724 563 1100 80 23 395 149 21 17 48...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  falls               0.308        3.656        -8.599       -1.178       -4.594       -0.294       -4.300       \n",
            "   [0]  voice               0.317        7.240        -7.861       -1.148       -3.835       -0.339       -3.496       \n",
            "   [0]  u                   0.304        3.347        -8.176       -1.190       -3.017       -0.374       -2.644       \n",
            "   [1]  do                  0.411        0.000        0.000        0.000        -2.051       -0.426       -0.000       \n",
            "   [1]  not                 0.465        0.000        0.000        0.000        -2.303       -0.452       -0.000       \n",
            "   [1]  recommend           0.300        0.000        0.000        0.000        -2.585       -0.456       -0.000       \n",
            "   [1]  watching            0.458        0.000        0.000        0.000        -2.902       -0.443       -0.000       \n",
            "   [0]  t                   0.482        3.843        -5.367       -0.729       -3.259       -0.426       -2.832       \n",
            "   [1]  movie               0.436        0.000        0.000        0.000        -2.840       -0.417       -0.000       \n",
            "   [0]  some                0.376        13.770       -5.990       -0.979       -3.188       -0.398       -2.790       \n",
            "   [1]  the                 0.395        0.000        0.000        0.000        -2.481       -0.384       -0.000       \n",
            "   [1]  movie               0.387        0.000        0.000        0.000        -2.785       -0.387       -0.000       \n",
            "   [1]  follows             0.475        0.000        0.000        0.000        -3.127       -0.397       -0.000       \n",
            "   [0]  let                 0.538        2.724        -8.344       -0.620       -3.510       -0.399       -3.112       \n",
            "   [0]  monday              0.387        9.298        -8.694       -0.949       -3.245       -0.390       -2.855       \n",
            "   [0]  has                 0.432        4.789        -5.645       -0.840       -2.578       -0.368       -2.210       \n",
            "   [0]  very                0.473        2.739        -5.658       -0.749       -1.951       -0.355       -1.595       \n",
            "   [0]  this                0.502        11.515       -3.929       -0.690       -1.349       -0.356       -0.993       \n",
            "   [0]  in                  0.477        9.785        -4.154       -0.740       -0.740       -0.353       -0.387       \n",
            "   [1]  warrior             0.489        0.000        0.000        0.000        0.000        -0.351       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  non                 0.372        8.451        -7.854       -0.989       -5.657       -0.294       -5.000       \n",
            "   [0]  tim                 0.272        5.021        -8.442       -1.304       -5.240       -0.314       -4.926       \n",
            "   [1]  rachel              0.337        0.000        0.000        0.000        -4.420       -0.347       -0.000       \n",
            "   [0]  by                  0.370        14.692       -5.342       -0.993       -4.962       -0.401       -4.561       \n",
            "   [1]  are                 0.434        0.000        0.000        0.000        -4.456       -0.433       -0.000       \n",
            "   [1]  among               0.311        0.000        0.000        0.000        -5.002       -0.448       -0.000       \n",
            "   [1]  the                 0.332        0.000        0.000        0.000        -5.616       -0.425       -0.000       \n",
            "   [0]  very                0.370        12.217       -5.434       -0.994       -6.305       -0.426       -5.000       \n",
            "   [0]  lee                 0.315        8.822        -8.230       -1.155       -5.963       -0.426       -5.000       \n",
            "   [0]  i                   0.320        3.174        -2.927       -1.141       -5.398       -0.411       -4.987       \n",
            "   [1]  have                0.349        0.000        0.000        0.000        -4.780       -0.412       -0.000       \n",
            "   [0]  dutched             0.308        6.218        -14.758      -1.179       -5.366       -0.418       -4.948       \n",
            "   [0]  a                   0.309        5.420        -3.107       -1.174       -4.701       -0.413       -4.288       \n",
            "   [0]  the                 0.324        9.642        -2.738       -1.127       -3.960       -0.414       -3.547       \n",
            "   [0]  why                 0.383        7.438        -6.795       -0.959       -3.181       -0.415       -2.766       \n",
            "   [1]  live                0.257        0.000        0.000        0.000        -2.495       -0.419       -0.000       \n",
            "   [0]  that                0.328        3.331        -4.010       -1.113       -2.801       -0.395       -2.406       \n",
            "   [0]  autopsies           0.332        4.742        -14.522      -1.102       -1.895       -0.406       -1.489       \n",
            "   [0]  of                  0.410        14.804       -3.226       -0.890       -0.890       -0.410       -0.480       \n",
            "   [1]  that                0.435        0.000        0.000        0.000        0.000        -0.412       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  agree               0.545        2.915        -8.801       -0.608       -1.788       -0.299       -1.489       \n",
            "   [1]  was                 0.530        0.000        0.000        0.000        -1.325       -0.343       -0.000       \n",
            "   [1]  such                0.550        0.000        0.000        0.000        -1.488       -0.355       -0.000       \n",
            "   [0]  to                  0.570        2.799        -3.628       -0.562       -1.670       -0.375       -1.295       \n",
            "   [1]  blast               0.640        0.000        0.000        0.000        -1.244       -0.382       -0.000       \n",
            "   [1]  it                  0.537        0.000        0.000        0.000        -1.397       -0.414       -0.000       \n",
            "   [1]  has                 0.564        0.000        0.000        0.000        -1.568       -0.357       -0.000       \n",
            "   [1]  that                0.603        0.000        0.000        0.000        -1.761       -0.356       -0.000       \n",
            "   [1]  feel                0.646        0.000        0.000        0.000        -1.977       -0.365       -0.000       \n",
            "   [0]  is                  0.624        5.510        -3.877       -0.472       -2.219       -0.388       -1.832       \n",
            "   [0]  but                 0.684        8.089        -4.986       -0.380       -1.962       -0.372       -1.589       \n",
            "   [0]  dead                0.589        9.274        -8.043       -0.530       -1.776       -0.394       -1.382       \n",
            "   [0]  that                0.634        4.168        -3.815       -0.455       -1.399       -0.338       -1.061       \n",
            "   [1]  your                0.663        0.000        0.000        0.000        -1.060       -0.343       -0.000       \n",
            "   [0]  sucks               0.594        8.853        -8.585       -0.521       -1.190       -0.369       -0.821       \n",
            "   [1]  attitude            0.686        0.000        0.000        0.000        -0.751       -0.341       -0.000       \n",
            "   [0]  movie               0.638        3.836        -4.090       -0.449       -0.843       -0.396       -0.446       \n",
            "   [0]  against             0.643        8.664        -9.008       -0.441       -0.441       -0.380       -0.062       \n",
            "   [1]  me                  0.621        0.000        0.000        0.000        0.000        -0.378       0.000        \n",
            "   [1]  to                  0.647        0.000        0.000        0.000        0.000        -0.360       0.000        \n",
            "Samples\n",
            "Sample 0 .  falls voice u do not recommend watching t movie some the movie follows let monday has very this in warrior\n",
            "Sample 1 .  non tim rachel by are among the very lee i have dutched a the why live that autopsies of that\n",
            "Sample 2 .  agree was such to blast it has that feel is but dead that your sucks attitude movie against me to\n",
            "\n",
            "\n",
            "targets[[51 9 215 10 17 31 2 2166 9 3580 42 87 3091 7 13 11 15830 27086 2663 8][24192 1 888 27 3428 137 11 5 53 1247 131 483 2 734 201 15 3989 7745 1 21208][291 60 563 3040 5 634 43 275 183 362...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[58 51 9 215 10 17 31 2 2166 9...]...][[0 1 0 0 0 1 0 0 0 0...]...][[58 69849 9 69849 69849 69849 31 69849 69849 69849...]...][[51 9 215 10 17 31 2 2166 9 3580...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[58 51 9 215 10 17 31 2 2166 9...]...][[0 1 0 0 0 1 0 0 0 0...]...][[58 69849 9 69849 69849 69849 31 69849 69849 69849...]...][[38 9 60 15049 169 31 43 3 4 12...]...]\n",
            "targets[[5 51 8309 269 55 36 0 3614 6 6 9 118 23 512 73 36 8309 10 501 1606][3778 7250 17994 732 8 2 209 3642 40 1966 6 6 2 323 2573 7314 34 45 115 116][19 5 1009 4 0 220 3 107 11612 1058...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2994 5 51 8309 269 55 36 0 3614 6...]...][[1 0 0 1 1 0 0 0 0 1...]...][[2994 5 69849 69849 269 55 69849 69849 69849 69849...]...][[5 51 8309 269 55 36 0 3614 6 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2994 5 51 8309 269 55 36 0 3614 6...]...][[1 0 0 1 1 0 0 0 0 1...]...][[2994 5 69849 69849 269 55 69849 69849 69849 69849...]...][[5 1338 205 269 55 307 882 9 318 6...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 752 37 36433 117 166 0 1309 0 23...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 5 1640 36433 117 166 4 1309 0 4853...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 29742 43938 36433 117 166 6648 1309 0 1973...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "global_step: 384\n",
            " perplexity: 903.853\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            " percent of 3-grams captured: 0.165.\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            " percent of 2-grams captured: 0.597.\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            " percent of 4-grams captured: 0.030.\n",
            " geometric_avg: 0.143.\n",
            " arithmetic_avg: 0.264.\n",
            "global_step: 384\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66407\n",
            " G train loss: 160.13162\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 5 1640 36433 117 166 4 1309 0 4853...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 47 5 1640 36433 117 166 4 1309 0...]...][[1 0 0 1 1 1 0 1 1 0...]...][[8 47 69849 69849 36433 117 166 69849 1309 0...]...][[47 76 1712 36433 117 166 33 1309 0 118...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  what                0.624        0.000        0.000        0.000        -2.800       -0.293       -0.000       \n",
            "   [0]  get                 0.575        3.266        -6.440       -0.554       -3.143       -0.367       -2.776       \n",
            "   [0]  asked               0.459        8.451        -8.780       -0.778       -2.907       -0.360       -2.547       \n",
            "   [1]  sandlers            0.484        0.000        0.000        0.000        -2.390       -0.304       -0.000       \n",
            "   [1]  best                0.550        0.000        0.000        0.000        -2.683       -0.293       -0.000       \n",
            "   [1]  work                0.546        0.000        0.000        0.000        -3.013       -0.325       -0.000       \n",
            "   [0]  by                  0.523        4.163        -5.753       -0.649       -3.382       -0.330       -3.052       \n",
            "   [1]  date                0.369        0.000        0.000        0.000        -3.069       -0.325       -0.000       \n",
            "   [1]  the                 0.410        0.000        0.000        0.000        -3.445       -0.282       -0.000       \n",
            "   [0]  did                 0.442        13.715       -7.314       -0.817       -3.868       -0.304       -3.564       \n",
            "   [0]  mirjana             0.409        12.406       -13.473      -0.893       -3.425       -0.336       -3.089       \n",
            "   [0]  many                0.384        8.629        -6.391       -0.957       -2.842       -0.348       -2.494       \n",
            "   [1]  happy               0.144        0.000        0.000        0.000        -2.117       -0.354       -0.000       \n",
            "   [1]  gilmore             0.253        0.000        0.000        0.000        -2.377       -0.320       -0.000       \n",
            "   [0]  a                   0.267        6.981        -3.689       -1.321       -2.668       -0.385       -2.283       \n",
            "   [1]  join                0.305        0.000        0.000        0.000        -1.513       -0.468       -0.000       \n",
            "   [0]  petite              0.341        3.087        -12.087      -1.076       -1.698       -0.522       -1.177       \n",
            "   [1]  professional        0.399        0.000        0.000        0.000        -0.699       -0.553       -0.000       \n",
            "   [1]  golfers             0.369        0.000        0.000        0.000        -0.785       -0.567       -0.000       \n",
            "   [0]  sexy                0.414        8.917        -9.618       -0.881       -0.881       -0.547       -0.334       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  is                  0.497        0.000        0.000        0.000        -3.632       -0.300       -0.000       \n",
            "   [0]  did                 0.496        6.186        -7.041       -0.702       -4.078       -0.336       -3.742       \n",
            "   [0]  well                0.462        7.889        -7.118       -0.771       -3.791       -0.351       -3.440       \n",
            "   [0]  movie               0.436        2.285        -2.881       -0.831       -3.390       -0.341       -3.049       \n",
            "   [0]  br                  0.526        8.030        -5.349       -0.643       -2.873       -0.339       -2.534       \n",
            "   [1]  surprise            0.484        0.000        0.000        0.000        -2.503       -0.368       -0.000       \n",
            "   [0]  the                 0.475        4.173        -2.270       -0.744       -2.811       -0.358       -2.453       \n",
            "   [1]  german              0.457        0.000        0.000        0.000        -2.320       -0.345       -0.000       \n",
            "   [1]  postwar             0.553        0.000        0.000        0.000        -2.605       -0.335       -0.000       \n",
            "   [0]  saw                 0.567        4.373        -5.791       -0.567       -2.924       -0.358       -2.566       \n",
            "   [0]  film                0.562        7.773        -4.384       -0.575       -2.646       -0.366       -2.281       \n",
            "   [0]  but                 0.639        7.717        -4.803       -0.449       -2.325       -0.352       -1.973       \n",
            "   [0]  this                0.629        5.756        -3.910       -0.463       -2.107       -0.363       -1.743       \n",
            "   [0]  the                 0.603        2.382        -2.382       -0.506       -1.845       -0.351       -1.494       \n",
            "   [0]  movie               0.560        14.303       -3.677       -0.579       -1.503       -0.325       -1.178       \n",
            "   [1]  mr                  0.523        0.000        0.000        0.000        -1.037       -0.304       -0.000       \n",
            "   [0]  set                 0.554        9.751        -7.129       -0.591       -1.165       -0.285       -0.880       \n",
            "   [0]  i                   0.525        15.741       -3.130       -0.644       -0.644       -0.302       -0.342       \n",
            "   [1]  who                 0.530        0.000        0.000        0.000        0.000        -0.312       0.000        \n",
            "   [1]  usually             0.408        0.000        0.000        0.000        0.000        -0.328       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  mentally            0.347        3.290        -9.635       -1.057       -3.864       -0.286       -3.578       \n",
            "   [0]  was                 0.386        5.163        -4.031       -0.953       -3.151       -0.283       -2.869       \n",
            "   [1]  jess                0.467        0.000        0.000        0.000        -2.468       -0.311       -0.000       \n",
            "   [1]  franco              0.538        0.000        0.000        0.000        -2.771       -0.356       -0.000       \n",
            "   [0]  went                0.505        4.679        -7.509       -0.683       -3.111       -0.383       -2.728       \n",
            "   [1]  best                0.549        0.000        0.000        0.000        -2.726       -0.366       -0.000       \n",
            "   [0]  it                  0.449        4.770        -3.460       -0.800       -3.061       -0.355       -2.706       \n",
            "   [0]  probably            0.408        5.587        -7.319       -0.897       -2.537       -0.311       -2.227       \n",
            "   [1]  it                  0.356        0.000        0.000        0.000        -1.842       -0.290       -0.000       \n",
            "   [1]  s                   0.393        0.000        0.000        0.000        -2.068       -0.285       -0.000       \n",
            "   [1]  certainly           0.360        0.000        0.000        0.000        -2.322       -0.319       -0.000       \n",
            "   [1]  not                 0.439        0.000        0.000        0.000        -2.606       -0.341       -0.000       \n",
            "   [1]  the                 0.421        0.000        0.000        0.000        -2.926       -0.375       -0.000       \n",
            "   [0]  me                  0.389        6.651        -6.013       -0.943       -3.285       -0.380       -2.905       \n",
            "   [0]  the                 0.387        8.285        -3.034       -0.951       -2.629       -0.372       -2.257       \n",
            "   [0]  short               0.439        6.214        -8.708       -0.823       -1.884       -0.366       -1.518       \n",
            "   [1]  doesn               0.309        0.000        0.000        0.000        -1.192       -0.376       -0.000       \n",
            "   [1]  t                   0.380        0.000        0.000        0.000        -1.338       -0.350       -0.000       \n",
            "   [0]  seeing              0.482        6.402        -7.076       -0.729       -1.502       -0.367       -1.136       \n",
            "   [0]  rock                0.420        6.607        -8.538       -0.868       -0.868       -0.399       -0.470       \n",
            "Samples\n",
            "Sample 0 .  what get asked sandlers best work by date the did mirjana many happy gilmore a join petite professional golfers sexy\n",
            "Sample 1 .  is did well movie br surprise the german postwar saw film but this the movie mr set i who usually\n",
            "Sample 2 .  mentally was jess franco went best it probably it s certainly not the me the short doesn t seeing rock\n",
            "\n",
            "\n",
            "targets[[7983 5 2 153 622 104 25 245 4 3919 24 12 92 275 3 60 30 57 519 104][17 5 60 519 29 9 27 307 7 52 72 163 214 9 1207 121 30 0 410 174][6724 2570 5262 16 3345 18 11750 3 12713 505...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4086 7983 5 2 153 622 104 25 245 4...]...][[0 0 1 0 0 0 0 1 1 0...]...][[4086 69849 69849 2 69849 69849 69849 69849 245 4...]...][[7983 5 2 153 622 104 25 245 4 3919...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4086 7983 5 2 153 622 104 25 245 4...]...][[0 0 1 0 0 0 0 1 1 0...]...][[4086 69849 69849 2 69849 69849 69849 69849 245 4...]...][[2 5 2 964 228 33 1425 245 4 2181...]...]\n",
            "targets[[125 9 470 4 38 10 17 7 45 2 81 177 1 7 5 2 385 1633 19 1][21 76 71 356 9 424 9142 1512 18 10 29 13 73 126 0 63 13 126 58 0][3 0 2983 5067 6198 45 7961 73 690 43...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 125 9 470 4 38 10 17 7 45...]...][[1 0 0 1 0 0 0 1 0 1...]...][[431 125 69849 69849 4 69849 69849 69849 7 69849...]...][[125 9 470 4 38 10 17 7 45 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 125 9 470 4 38 10 17 7 45...]...][[1 0 0 1 0 0 0 1 0 1...]...][[431 125 69849 69849 4 69849 69849 69849 7 69849...]...][[125 418 363 4 3 22 1250 7 811 2...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 0 14590 1753 4924 2042 704 1753 3...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 677 6798 1753 4924 4924 15 1753 1761...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 3830 913 1753 4924 815 7003 1753 1652...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "global_step: 389\n",
            " perplexity: 903.220\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            " percent of 3-grams captured: 0.149.\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            " percent of 2-grams captured: 0.592.\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            " percent of 4-grams captured: 0.023.\n",
            " geometric_avg: 0.126.\n",
            " arithmetic_avg: 0.254.\n",
            "global_step: 389\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66350\n",
            " G train loss: 161.36528\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 677 6798 1753 4924 4924 15 1753 1761...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[10 17 5 677 6798 1753 4924 4924 15 1753...]...][[1 1 0 0 1 1 0 0 1 0...]...][[10 17 5 69849 69849 1753 4924 69849 69849 1753...]...][[17 5 0 193 1753 4924 49 1731 1753 190...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  movie               0.435        0.000        0.000        0.000        -3.734       -0.292       -0.000       \n",
            "   [1]  is                  0.438        0.000        0.000        0.000        -4.192       -0.331       -0.000       \n",
            "   [0]  the                 0.420        9.776        -2.783       -0.867       -4.706       -0.355       -4.351       \n",
            "   [0]  both                0.334        15.012       -8.563       -1.098       -4.310       -0.374       -3.936       \n",
            "   [1]  guns                0.414        0.000        0.000        0.000        -3.607       -0.405       -0.000       \n",
            "   [1]  chicks              0.402        0.000        0.000        0.000        -4.049       -0.432       -0.000       \n",
            "   [0]  good                0.312        13.428       -5.400       -1.165       -4.546       -0.449       -4.097       \n",
            "   [0]  redeeming           0.330        4.631        -8.592       -1.110       -3.795       -0.473       -3.322       \n",
            "   [1]  guns                0.400        0.000        0.000        0.000        -3.015       -0.500       -0.000       \n",
            "   [0]  however             0.461        12.319       -8.127       -0.775       -3.385       -0.513       -2.871       \n",
            "   [0]  bad                 0.531        9.194        -6.517       -0.634       -2.930       -0.494       -2.436       \n",
            "   [0]  it                  0.409        10.744       -3.389       -0.893       -2.578       -0.438       -2.141       \n",
            "   [1]  getting             0.249        0.000        0.000        0.000        -1.892       -0.391       -0.000       \n",
            "   [0]  be                  0.372        14.440       -5.785       -0.989       -2.124       -0.409       -1.715       \n",
            "   [0]  isn                 0.443        5.495        -7.848       -0.814       -1.274       -0.446       -0.827       \n",
            "   [1]  black               0.380        0.000        0.000        0.000        -0.516       -0.465       -0.000       \n",
            "   [1]  chicks              0.374        0.000        0.000        0.000        -0.579       -0.464       -0.000       \n",
            "   [1]  who                 0.377        0.000        0.000        0.000        -0.651       -0.464       -0.000       \n",
            "   [0]  but                 0.482        13.468       -5.326       -0.730       -0.730       -0.469       -0.261       \n",
            "   [1]  guns                0.523        0.000        0.000        0.000        0.000        -0.454       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  or                  0.530        3.405        -6.136       -0.634       -6.884       -0.296       -5.000       \n",
            "   [1]  was                 0.487        0.000        0.000        0.000        -7.017       -0.303       -0.000       \n",
            "   [0]  pleasant            0.266        7.233        -8.645       -1.324       -7.878       -0.324       -5.000       \n",
            "   [0]  than                0.302        7.888        -5.870       -1.197       -7.357       -0.442       -5.000       \n",
            "   [0]  s                   0.324        3.530        -4.760       -1.127       -6.916       -0.490       -5.000       \n",
            "   [0]  their               0.400        5.577        -7.164       -0.917       -6.499       -0.543       -5.000       \n",
            "   [0]  a                   0.356        6.739        -3.598       -1.033       -6.267       -0.536       -5.000       \n",
            "   [1]  some                0.295        0.000        0.000        0.000        -5.876       -0.560       -0.000       \n",
            "   [0]  brought             0.143        8.707        -8.213       -1.945       -6.597       -0.596       -5.000       \n",
            "   [0]  hilarious           0.181        6.044        -8.317       -1.709       -5.223       -0.760       -4.463       \n",
            "   [1]  before              0.243        0.000        0.000        0.000        -3.946       -0.807       -0.000       \n",
            "   [1]  the                 0.229        0.000        0.000        0.000        -4.430       -0.824       -0.000       \n",
            "   [0]  am                  0.273        13.511       -7.309       -1.299       -4.974       -0.857       -4.116       \n",
            "   [1]  is                  0.272        0.000        0.000        0.000        -4.125       -0.834       -0.000       \n",
            "   [0]  death               0.296        3.277        -8.487       -1.216       -4.631       -0.831       -3.800       \n",
            "   [0]  into                0.341        5.887        -6.315       -1.075       -3.835       -0.806       -3.029       \n",
            "   [0]  in                  0.317        9.676        -4.388       -1.150       -3.098       -0.768       -2.330       \n",
            "   [0]  antoinette          0.317        7.032        -13.403      -1.149       -2.187       -0.774       -1.413       \n",
            "   [0]  and                 0.312        6.196        -3.565       -1.166       -1.166       -0.779       -0.387       \n",
            "   [1]  though              0.230        0.000        0.000        0.000        0.000        -0.801       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  guy                 0.380        8.232        -7.419       -0.968       -5.386       -0.297       -5.000       \n",
            "   [1]  direct              0.241        0.000        0.000        0.000        -4.959       -0.338       -0.000       \n",
            "   [0]  their               0.349        3.982        -7.820       -1.052       -5.568       -0.378       -5.000       \n",
            "   [0]  if                  0.377        8.019        -6.230       -0.975       -5.070       -0.396       -4.674       \n",
            "   [0]  as                  0.392        8.048        -4.841       -0.937       -4.597       -0.416       -4.181       \n",
            "   [1]  movie               0.343        0.000        0.000        0.000        -4.110       -0.419       -0.000       \n",
            "   [0]  based               0.423        8.076        -7.680       -0.860       -4.614       -0.424       -4.190       \n",
            "   [1]  the                 0.392        0.000        0.000        0.000        -4.214       -0.411       -0.000       \n",
            "   [0]  direction           0.324        7.429        -9.103       -1.127       -4.731       -0.405       -4.326       \n",
            "   [0]  did                 0.354        4.341        -6.677       -1.039       -4.047       -0.400       -3.647       \n",
            "   [0]  s                   0.346        7.569        -4.545       -1.061       -3.376       -0.393       -2.983       \n",
            "   [1]  good                0.270        0.000        0.000        0.000        -2.599       -0.402       -0.000       \n",
            "   [1]  there               0.244        0.000        0.000        0.000        -2.918       -0.420       -0.000       \n",
            "   [1]  were                0.343        0.000        0.000        0.000        -3.276       -0.435       -0.000       \n",
            "   [1]  a                   0.298        0.000        0.000        0.000        -3.678       -0.442       -0.000       \n",
            "   [0]  1970s               0.205        6.902        -8.615       -1.586       -4.129       -0.458       -3.672       \n",
            "   [0]  movie               0.205        3.492        -4.108       -1.586       -2.855       -0.479       -2.376       \n",
            "   [1]  first               0.313        0.000        0.000        0.000        -1.425       -0.503       -0.000       \n",
            "   [1]  rate                0.227        0.000        0.000        0.000        -1.599       -0.512       -0.000       \n",
            "   [0]  it                  0.166        7.465        -3.319       -1.796       -1.796       -0.532       -1.263       \n",
            "Samples\n",
            "Sample 0 .  movie is the both guns chicks good redeeming guns however bad it getting be isn black chicks who but guns\n",
            "Sample 1 .  or was pleasant than s their a some brought hilarious before the am is death into in antoinette and though\n",
            "Sample 2 .  guy direct their if as movie based the direction did s good there were a 1970s movie first rate it\n",
            "\n",
            "\n",
            "targets[[656 898 67 2 2190 3 163 678 10 13 2 394 432 3 443 437 56 4096 56 2615][12 161 160 477 12931 48 3 0 126 2601 236 28 98 38 0 960 12001 3 7946 908][5 29 3 60 519 98 295 8 7 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 656 898 67 2 2190 3 163 678 10...]...][[1 0 0 1 0 1 0 0 0 0...]...][[9 656 69849 69849 2 69849 3 69849 69849 69849...]...][[656 898 67 2 2190 3 163 678 10 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 656 898 67 2 2190 3 163 678 10...]...][[1 0 0 1 0 1 0 0 0 0...]...][[9 656 69849 69849 2 69849 3 69849 69849 69849...]...][[656 65 354 2 176 3 11 125 23 7...]...]\n",
            "targets[[2185 22 0 17 796 6 6 7 13 23 49 23 49 31 30 2240 7 13 81 9][42 389 143 36 2 1689 768 816 3 10 322 832 827 19 22802 7 12 393 5 23632][343 1 868 201 9137 0 868 5 7543 33...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[60 2185 22 0 17 796 6 6 7 13...]...][[1 1 1 1 1 1 0 1 1 0...]...][[60 2185 22 0 17 796 6 69849 7 13...]...][[2185 22 0 17 796 6 6 7 13 23...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[60 2185 22 0 17 796 6 6 7 13...]...][[1 1 1 1 1 1 0 1 1 0...]...][[60 2185 22 0 17 796 6 69849 7 13...]...][[2185 22 0 17 796 6 1693 7 13 21...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 177 738 255 884 9 5 246 2 22...]...]\n",
            "I0310 00:36:12.858728 139852876060416 supervisor.py:1117] Saving checkpoint to path /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt\n",
            "I0310 00:36:12.862085 139852867667712 supervisor.py:1099] global_step/sec: 0.375074\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 127 738 255 884 127 738 34 25 23...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 3 738 255 884 15 8 35 250 3...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "global_step: 394\n",
            " perplexity: 908.134\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            " percent of 3-grams captured: 0.180.\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            " percent of 2-grams captured: 0.603.\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            " percent of 4-grams captured: 0.030.\n",
            " geometric_avg: 0.148.\n",
            " arithmetic_avg: 0.271.\n",
            "global_step: 394\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66263\n",
            " G train loss: 162.69452\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 127 738 255 884 127 738 34 25 23...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 509 127 738 255 884 127 738 34 25...]...][[1 0 1 1 1 0 0 0 0 0...]...][[9 509 69849 738 255 884 69849 69849 69849 69849...]...][[509 144 738 255 884 60 29 0 11 196...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  enjoyed             0.528        0.000        0.000        0.000        -1.711       -0.301       -0.000       \n",
            "   [0]  through             0.612        7.921        -8.410       -0.491       -1.921       -0.344       -1.576       \n",
            "   [1]  review              0.577        0.000        0.000        0.000        -1.605       -0.363       -0.000       \n",
            "   [1]  anyone              0.584        0.000        0.000        0.000        -1.801       -0.329       -0.000       \n",
            "   [1]  reading             0.548        0.000        0.000        0.000        -2.022       -0.292       -0.000       \n",
            "   [0]  my                  0.681        7.617        -5.471       -0.384       -2.271       -0.262       -2.009       \n",
            "   [0]  one                 0.643        8.394        -5.184       -0.442       -2.118       -0.269       -1.849       \n",
            "   [0]  the                 0.623        5.535        -2.173       -0.474       -1.882       -0.236       -1.646       \n",
            "   [0]  that                0.643        5.313        -4.090       -0.441       -1.581       -0.207       -1.374       \n",
            "   [0]  thought             0.598        5.562        -6.634       -0.514       -1.280       -0.191       -1.089       \n",
            "   [1]  consumers           0.715        0.000        0.000        0.000        -0.860       -0.168       -0.000       \n",
            "   [0]  that                0.701        3.298        -4.094       -0.355       -0.966       -0.193       -0.772       \n",
            "   [1]  metal               0.664        0.000        0.000        0.000        -0.686       -0.178       -0.000       \n",
            "   [1]  will                0.735        0.000        0.000        0.000        -0.770       -0.148       -0.000       \n",
            "   [1]  learn               0.491        0.000        0.000        0.000        -0.864       -0.154       -0.000       \n",
            "   [1]  that                0.580        0.000        0.000        0.000        -0.970       -0.100       -0.000       \n",
            "   [0]  is                  0.588        4.025        -3.591       -0.531       -1.089       -0.136       -0.953       \n",
            "   [1]  topic               0.451        0.000        0.000        0.000        -0.626       -0.189       -0.000       \n",
            "   [1]  could               0.529        0.000        0.000        0.000        -0.703       -0.210       -0.000       \n",
            "   [0]  movie               0.454        8.705        -4.016       -0.790       -0.790       -0.270       -0.520       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  saw                 0.516        0.000        0.000        0.000        -3.165       -0.295       -0.000       \n",
            "   [1]  a                   0.440        0.000        0.000        0.000        -3.553       -0.333       -0.000       \n",
            "   [0]  simply              0.274        8.151        -7.711       -1.294       -3.989       -0.318       -3.671       \n",
            "   [1]  of                  0.347        0.000        0.000        0.000        -3.026       -0.314       -0.000       \n",
            "   [1]  this                0.346        0.000        0.000        0.000        -3.397       -0.375       -0.000       \n",
            "   [1]  last                0.189        0.000        0.000        0.000        -3.814       -0.416       -0.000       \n",
            "   [1]  year                0.374        0.000        0.000        0.000        -4.282       -0.434       -0.000       \n",
            "   [0]  thriller            0.311        3.251        -7.727       -1.169       -4.807       -0.509       -4.298       \n",
            "   [1]  all                 0.343        0.000        0.000        0.000        -4.085       -0.498       -0.000       \n",
            "   [1]  i                   0.273        0.000        0.000        0.000        -4.586       -0.490       -0.000       \n",
            "   [1]  can                 0.242        0.000        0.000        0.000        -5.148       -0.463       -0.000       \n",
            "   [0]  of                  0.281        7.144        -3.719       -1.270       -5.780       -0.457       -5.000       \n",
            "   [1]  is                  0.262        0.000        0.000        0.000        -5.063       -0.471       -0.000       \n",
            "   [0]  women               0.174        4.250        -7.587       -1.749       -5.684       -0.474       -5.000       \n",
            "   [0]  has                 0.219        5.019        -5.811       -1.518       -4.417       -0.476       -3.942       \n",
            "   [1]  br                  0.276        0.000        0.000        0.000        -3.255       -0.515       -0.000       \n",
            "   [0]  tv                  0.178        3.546        -6.925       -1.723       -3.655       -0.537       -3.118       \n",
            "   [0]  old                 0.114        8.410        -6.883       -2.168       -2.168       -0.514       -1.654       \n",
            "   [1]  they                0.124        0.000        0.000        0.000        0.000        -0.525       0.000        \n",
            "   [1]  can                 0.106        0.000        0.000        0.000        0.000        -0.587       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  film                0.500        8.273        -3.897       -0.693       -2.852       -0.296       -2.557       \n",
            "   [1]  this                0.492        0.000        0.000        0.000        -2.424       -0.324       -0.000       \n",
            "   [0]  this                0.487        2.932        -3.298       -0.719       -2.722       -0.334       -2.388       \n",
            "   [0]  is                  0.491        3.032        -3.018       -0.712       -2.249       -0.337       -1.912       \n",
            "   [0]  just                0.498        7.898        -5.779       -0.698       -1.725       -0.338       -1.388       \n",
            "   [1]  for                 0.587        0.000        0.000        0.000        -1.153       -0.332       -0.000       \n",
            "   [1]  its                 0.485        0.000        0.000        0.000        -1.295       -0.310       -0.000       \n",
            "   [1]  sheer               0.700        0.000        0.000        0.000        -1.454       -0.276       -0.000       \n",
            "   [0]  and                 0.626        12.219       -2.971       -0.468       -1.632       -0.247       -1.385       \n",
            "   [0]  rox                 0.600        3.348        -14.733      -0.510       -1.307       -0.175       -1.131       \n",
            "   [1]  must                0.590        0.000        0.000        0.000        -0.894       -0.122       -0.000       \n",
            "   [0]  of                  0.624        12.841       -3.691       -0.471       -1.004       -0.098       -0.906       \n",
            "   [1]  my                  0.700        0.000        0.000        0.000        -0.598       -0.090       -0.000       \n",
            "   [0]  before              0.707        7.751        -7.630       -0.347       -0.671       -0.076       -0.596       \n",
            "   [1]  by                  0.669        0.000        0.000        0.000        -0.364       -0.040       -0.000       \n",
            "   [1]  way                 0.741        0.000        0.000        0.000        -0.409       0.002        -0.000       \n",
            "   [0]  on                  0.769        3.699        -5.346       -0.263       -0.459       0.031        -0.490       \n",
            "   [0]  through             0.802        13.008       -7.639       -0.220       -0.220       0.070        -0.291       \n",
            "   [1]  that                0.787        0.000        0.000        0.000        0.000        0.125        -0.000       \n",
            "   [1]  i                   0.742        0.000        0.000        0.000        0.000        0.191        -0.000       \n",
            "Samples\n",
            "Sample 0 .  enjoyed through review anyone reading my one the that thought consumers that metal will learn that is topic could movie\n",
            "Sample 1 .  saw a simply of this last year thriller all i can of is women has br tv old they can\n",
            "Sample 2 .  film this this is just for its sheer and rox must of my before by way on through that i\n",
            "\n",
            "\n",
            "targets[[11663 13 335 22 351 3 26 458 51 24 326 10 19 7 5 35 6548 5358 19 368][2 57 51 0 176 5 37 2890 22 0 1849 8044 8 3854 0 151 11 5 88 731][307 10 3529 22 6 6 9 140 23 251...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1644 11663 13 335 22 351 3 26 458 51...]...][[0 0 0 0 1 0 1 1 1 0...]...][[1644 69849 69849 69849 69849 351 69849 26 458 51...]...][[11663 13 335 22 351 3 26 458 51 24...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1644 11663 13 335 22 351 3 26 458 51...]...][[0 0 0 0 1 0 1 1 1 0...]...][[1644 69849 69849 69849 69849 351 69849 26 458 51...]...][[2 17 381 14 351 586 26 458 51 109...]...]\n",
            "targets[[318 10 609 9 42 96 23 1017 545 36 13049 60 333 420 21 1643 4 1775 1 9696][111 80 9 875 10 17 1168 60 114 0 109 13 11000 601 20260 45 44 226 313 2537][5 23 29 3 146 104 7 5 29 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 318 10 609 9 42 96 23 1017 545...]...][[1 1 1 1 0 1 1 0 0 1...]...][[99 318 10 609 9 69849 96 23 69849 69849...]...][[318 10 609 9 42 96 23 1017 545 36...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[99 318 10 609 9 42 96 23 1017 545...]...][[1 1 1 1 0 1 1 0 0 1...]...][[99 318 10 609 9 69849 96 23 69849 69849...]...][[318 10 609 9 34 96 23 123 46 36...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 1057 368 755 654 78 2 376 3 51935...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 12 368 755 654 78 2 368 1115 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 12 368 755 654 78 2 125 9 27...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "global_step: 399\n",
            " perplexity: 916.458\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            " percent of 3-grams captured: 0.158.\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            " percent of 2-grams captured: 0.597.\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            " percent of 4-grams captured: 0.014.\n",
            " geometric_avg: 0.110.\n",
            " arithmetic_avg: 0.256.\n",
            "global_step: 399\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.66241\n",
            " G train loss: 161.55048\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 12 368 755 654 78 2 368 1115 19...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1840 1943 12 368 755 654 78 2 368 1115...]...][[1 0 1 1 1 1 1 0 0 0...]...][[1840 1943 69849 368 755 654 78 2 69849 69849...]...][[1943 242 368 755 654 78 2 176 10 17...]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  hopes               0.425        0.000        0.000        0.000        -3.134       -0.293       -0.000       \n",
            "   [0]  am                  0.533        4.296        -6.309       -0.629       -3.518       -0.300       -3.218       \n",
            "   [1]  classic             0.568        0.000        0.000        0.000        -3.245       -0.357       -0.000       \n",
            "   [1]  tale                0.441        0.000        0.000        0.000        -3.643       -0.377       -0.000       \n",
            "   [1]  turned              0.371        0.000        0.000        0.000        -4.089       -0.329       -0.000       \n",
            "   [1]  into                0.486        0.000        0.000        0.000        -4.591       -0.294       -0.000       \n",
            "   [1]  a                   0.426        0.000        0.000        0.000        -5.155       -0.340       -0.000       \n",
            "   [0]  world               0.435        7.926        -7.086       -0.833       -5.787       -0.334       -5.000       \n",
            "   [0]  this                0.443        9.960        -3.815       -0.814       -5.561       -0.339       -5.000       \n",
            "   [0]  movie               0.386        4.507        -4.387       -0.952       -5.330       -0.340       -4.990       \n",
            "   [1]  with                0.483        0.000        0.000        0.000        -4.915       -0.328       -0.000       \n",
            "   [0]  in                  0.449        14.234       -3.865       -0.802       -5.518       -0.362       -5.000       \n",
            "   [0]  his                 0.441        13.912       -5.949       -0.818       -5.295       -0.346       -4.949       \n",
            "   [0]  i                   0.394        5.029        -3.262       -0.931       -5.026       -0.333       -4.693       \n",
            "   [0]  anyway              0.196        3.102        -8.757       -1.629       -4.598       -0.315       -4.283       \n",
            "   [1]  king                0.379        0.000        0.000        0.000        -3.333       -0.254       -0.000       \n",
            "   [1]  and                 0.324        0.000        0.000        0.000        -3.742       -0.336       -0.000       \n",
            "   [0]  country             0.184        5.955        -7.977       -1.691       -4.202       -0.346       -3.856       \n",
            "   [0]  there               0.185        12.514       -6.266       -1.690       -2.819       -0.305       -2.514       \n",
            "   [0]  always              0.282        11.992       -7.852       -1.267       -1.267       -0.320       -0.947       \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [0]  i                   0.494        2.990        -4.329       -0.706       -3.451       -0.294       -3.157       \n",
            "   [0]  go                  0.535        5.118        -7.159       -0.625       -3.081       -0.329       -2.753       \n",
            "   [0]  before              0.592        13.102       -7.746       -0.524       -2.758       -0.344       -2.414       \n",
            "   [1]  of                  0.585        0.000        0.000        0.000        -2.508       -0.347       -0.000       \n",
            "   [0]  like                0.542        5.190        -5.690       -0.612       -2.816       -0.332       -2.484       \n",
            "   [0]  suspense            0.372        8.346        -7.840       -0.990       -2.474       -0.309       -2.165       \n",
            "   [0]  few                 0.487        7.881        -6.577       -0.720       -1.666       -0.301       -1.365       \n",
            "   [1]  one                 0.486        0.000        0.000        0.000        -1.063       -0.318       -0.000       \n",
            "   [1]  way                 0.590        0.000        0.000        0.000        -1.193       -0.341       -0.000       \n",
            "   [1]  passage             0.628        0.000        0.000        0.000        -1.339       -0.350       -0.000       \n",
            "   [1]  is                  0.577        0.000        0.000        0.000        -1.504       -0.341       -0.000       \n",
            "   [1]  likely              0.689        0.000        0.000        0.000        -1.688       -0.314       -0.000       \n",
            "   [1]  the                 0.642        0.000        0.000        0.000        -1.895       -0.294       -0.000       \n",
            "   [0]  if                  0.653        6.355        -6.668       -0.426       -2.128       -0.260       -1.867       \n",
            "   [1]  underrated          0.400        0.000        0.000        0.000        -1.911       -0.225       -0.000       \n",
            "   [0]  antidote            0.448        8.083        -12.934      -0.803       -2.145       -0.206       -1.939       \n",
            "   [0]  the                 0.477        8.322        -3.588       -0.741       -1.506       -0.235       -1.271       \n",
            "   [0]  year                0.628        4.305        -7.570       -0.466       -0.859       -0.286       -0.573       \n",
            "   [0]  are                 0.643        6.033        -5.755       -0.441       -0.441       -0.326       -0.115       \n",
            "   [1]  time                0.639        0.000        0.000        0.000        0.000        -0.322       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  s                   0.492        0.000        0.000        0.000        -2.738       -0.292       -0.000       \n",
            "   [0]  same                0.413        7.276        -8.217       -0.884       -3.073       -0.317       -2.757       \n",
            "   [1]  bad                 0.551        0.000        0.000        0.000        -2.458       -0.311       -0.000       \n",
            "   [0]  that                0.543        3.767        -4.739       -0.611       -2.759       -0.351       -2.408       \n",
            "   [1]  film                0.558        0.000        0.000        0.000        -2.412       -0.352       -0.000       \n",
            "   [0]  not                 0.591        7.319        -5.431       -0.526       -2.708       -0.349       -2.359       \n",
            "   [0]  o                   0.465        5.419        -8.041       -0.766       -2.449       -0.350       -2.100       \n",
            "   [0]  soul                0.532        13.057       -9.048       -0.631       -1.890       -0.307       -1.583       \n",
            "   [0]  seem                0.554        3.669        -8.211       -0.591       -1.414       -0.314       -1.100       \n",
            "   [0]  solar               0.573        12.876       -12.074      -0.556       -0.925       -0.329       -0.596       \n",
            "   [1]  time                0.620        0.000        0.000        0.000        -0.413       -0.337       -0.000       \n",
            "   [1]  the                 0.594        0.000        0.000        0.000        -0.464       -0.343       -0.000       \n",
            "   [1]  idea                0.694        0.000        0.000        0.000        -0.521       -0.334       -0.000       \n",
            "   [0]  my                  0.758        7.648        -6.184       -0.277       -0.585       -0.355       -0.230       \n",
            "   [0]  really              0.822        5.625        -7.117       -0.195       -0.346       -0.374       0.028        \n",
            "   [1]  been                0.780        0.000        0.000        0.000        -0.169       -0.402       0.000        \n",
            "   [1]  good                0.711        0.000        0.000        0.000        -0.190       -0.350       0.000        \n",
            "   [1]  but                 0.805        0.000        0.000        0.000        -0.213       -0.281       0.000        \n",
            "   [0]  in                  0.787        4.573        -4.463       -0.240       -0.240       -0.301       0.061        \n",
            "   [1]  wasn                0.687        0.000        0.000        0.000        0.000        -0.290       0.000        \n",
            "Samples\n",
            "Sample 0 .  hopes am classic tale turned into a world this movie with in his i anyway king and country there always\n",
            "Sample 1 .  i go before of like suspense few one way passage is likely the if underrated antidote the year are time\n",
            "Sample 2 .  s same bad that film not o soul seem solar time the idea my really been good but in wasn\n",
            "\n",
            "\n",
            "targets[[8 23319 11477 30354 12 1054 1081 673 64862 4572 1437 13 35 2004 20904 2783 4989 0 2486 1][11 12 30 11 50 28 300 43 10 19 10 1525 21 58 28 1267 2 662 4 0][13 2 1049 10323 249 8 407 8600 608 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6159 8 23319 11477 30354 12 1054 1081 673 64862...]...][[0 0 1 0 1 0 0 1 1 1...]...][[6159 69849 69849 11477 69849 12 69849 69849 673 64862...]...][[8 23319 11477 30354 12 1054 1081 673 64862 4572...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6159 8 23319 11477 30354 12 1054 1081 673 64862...]...][[0 0 1 0 1 0 0 1 1 1...]...][[6159 69849 69849 11477 69849 12 69849 69849 673 64862...]...][[25446 17 11477 22 12 27 2494 673 64862 4572...]...]\n",
            "targets[[293 0 277 5198 3 10 19 25 8 316 1 460 5 85 1276 50 76 62 933 22][206 5140 699 246 5453 23406 36 2170 13 142 35 1373 369 11 7 465 1099 4 93 2][45 0 3509 10259 3 43801 1050 15 15164 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 293 0 277 5198 3 10 19 25 8...]...][[0 0 1 1 1 0 1 1 1 0...]...][[0 69849 69849 277 5198 3 69849 19 25 8...]...][[293 0 277 5198 3 10 19 25 8 316...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 293 0 277 5198 3 10 19 25 8...]...][[0 0 1 1 1 0 1 1 1 0...]...][[0 69849 69849 277 5198 3 69849 19 25 8...]...][[108 4 277 5198 3 2 19 25 8 140...]...]\n",
            "targets[[7 12 2069 0 3572 1 33 0 984 5 37 3056 1306 5 143 18 24 12 3056 9][2907 12 84 81 19 3687 6405 13 2 19 11 13 5536 4 28 3047 120 17790 1 12295][353 0 298 8 60 646 713 1 99 70...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[69 7 12 2069 0 3572 1 33 0 984...]...][[1 1 1 1 1 1 1 0 0 1...]...][[69 7 12 2069 0 3572 1 33 69849 69849...]...][[7 12 2069 0 3572 1 33 977 0 5...]...]\n",
            "Training raising FloatingPoinError.\n",
            "I0310 00:36:42.084112 139858729412480 coordinator.py:224] Error reported to Coordinator: <type 'exceptions.FloatingPointError'>, Training infinite perplexity: nan\n",
            "Traceback (most recent call last):\n",
            "  File \"train_mask_gan.py\", line 1161, in <module>\n",
            "    tf.app.run()\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/tensorflow_core/python/platform/app.py\", line 40, in run\n",
            "    _run(main=main, argv=argv, flags_parser=_parse_flags_tolerate_undef)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 300, in run\n",
            "    _run_main(main, args)\n",
            "  File \"/usr/local/lib/python2.7/dist-packages/absl/app.py\", line 251, in _run_main\n",
            "    sys.exit(main(argv))\n",
            "  File \"train_mask_gan.py\", line 1143, in main\n",
            "    data_ngram_counts)\n",
            "  File \"train_mask_gan.py\", line 743, in train_model\n",
            "    'Training infinite perplexity: %.3f' % perplexity)\n",
            "FloatingPointError: Training infinite perplexity: nan\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8hYuMZmmkcm",
        "colab_type": "text"
      },
      "source": [
        "## Generate samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw-SWqvTmpsi",
        "colab_type": "code",
        "outputId": "b172296a-1c6a-4388-8a1b-407c09254a96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%cd /content/yesweGAN/maskgan_colab\n",
        "!python generate_samples.py \\\n",
        "--data_dir='/content/yesweGAN/maskgan_colab/dataset/imdb' \\\n",
        "--data_set='imdb' \\\n",
        "--batch_size=128 \\\n",
        "--sequence_length=20 \\\n",
        "--base_directory='/content/yesweGAN/maskgan_colab/maskGAN' \\\n",
        "--hparams=\"gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,gen_vd_keep_prob=0.33971\" \\\n",
        "--generator_model='seq2seq_vd' \\\n",
        "--discriminator_model='seq2seq_vd' \\\n",
        "--is_present_rate=0.5 \\\n",
        "--maskgan_ckpt='/content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt-1542' \\\n",
        "--seq2seq_share_embedding=true \\\n",
        "--dis_share_embedding=true \\\n",
        "--attention_option=luong \\\n",
        "--output_path='/content/yesweGAN/maskgan_colab' \\\n",
        "--baseline_method=critic \\\n",
        "--output_masked_logs=True \\\n",
        "--number_epochs=1\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yesweGAN/maskgan_colab\n",
            "Vocab size: 69848\n",
            "\n",
            "Optimizing Generator vars:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(69848, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(69848,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "\n",
            "Optimizing Discriminator vars:\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Optimizing Critic vars:\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "WARNING:tensorflow:From generate_samples.py:181: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2020-03-21 03:08:45.279063: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-03-21 03:08:45.424320: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-03-21 03:08:45.424794: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \n",
            "name: Tesla T4 major: 7 minor: 5 memoryClockRate(GHz): 1.59\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 14.73GiB freeMemory: 14.62GiB\n",
            "2020-03-21 03:08:45.424816: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n",
            "2020-03-21 03:08:45.781027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 14152 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Restoring Generator from /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt-1542.\n",
            "Asserting Generator is a seq2seq-variant.\n",
            "Restoring Discriminator from /content/yesweGAN/maskgan_colab/maskGAN/train/model.ckpt-1542.\n",
            "Epoch number: 0\n",
            "targets[[17 13 37 379 11 9 307 55 4 2911 144 1073 16 7 4 366 16 0 109 101][12 727 4 66 11 51052 7026 5 2 1133 1032 153 1 23 2 2825 16 4282 3 338][17 5 591 13633 698 87 4 1501 2 680]...]\n",
            "targets[[23354 1922 8689 58348 120 26 5755 1110 3 2685 50134 0 21901 5936 1376 36 1363 17941 1 406][2 8597 37 11 20 89 21 27 4 353 0 372 3 60 609 46 20 89 21 182][20968 41 190 20 3923 26 390 5 2 15310]...]\n",
            "targets[[5337 12 361 5 2 81 354 39 12 5 64 223 75 8 0 19 1868 12 162 7][27 307 10 19 440 214 1 6667 33 2 6589 3 2 173 154 199 4712 1 2 74][17 65 2144 36 0 557 3581 4 0 2812]...]\n",
            "targets[[1803 34 45 110 0 703 206 15 5359 751 1 6840 9433 10 5 0 250 984 3 2][45 4 28 29 3 60 30 57 7625 464 91 40726 1 336 2159 57117 3 2703 18638 91][39 255 332 44 39 34 1198 11 30 0]...]\n",
            "targets[[5 23 174 19 12 301 4 22800 20 15795 9 83 191 35 3332 2102 119 2 2935 2402][10 288 21 0 250 17 9 139 123 110 18 9 67 555 745 3 49 179 43 7][17 13 1796 2 81 225 1 0 1135 3]...]\n",
            "targets[[141 27 77 126 73 126 2 1023 2431 2347 6807 9746 0 572 3 2 17680 5196 1 0][133 3233 5 8485 6949 4 48 3 0 5085 8 61 9914 45 77 571 14 2 153 863][286 34 45 77 1292 30 40 114 1 5]...]\n",
            "targets[[17 5 634 2 181 17 31 91 2075 7 53279 1449 1132 1 63 18 18673 11 15 91][481 9 50 66 0 220 10 216 13 259 4 93 18 84 134 118 7 27 4 28][242 2229 3 47 6506 16377 13 787 8 0]...]\n",
            "targets[[5 2 1563 120 3 478 2124 338 98 38 2113 1 9 121 47 20 118 227 1606 0][1869 5 2 5597 3084 259 4 956 78 863 8 0 227 3 1809 35729 2 7389 19 36][6 6 68 32 259 4 586 57 1 19]...]\n",
            "targets[[3 30 287 71 42 136 11 99 816 10 19 9 618 196 43 503 60 197 877 85][402 318 10 17 8 5582 51 246 11935 8 3345 13 13133 0 466 422 3 91 158 377][5 2 497 497 778 18 805 7 5 79]...]\n",
            "targets[[42 1125 228 78 10 19 9 1739 4 663 8434 13264 12 881 52 20139 104 11 581 31][633 120 16 0 2647 10045 19 1322 9 67 1422 3 10 17 478 36 283 9 353 1][3 0 52 1010 959 729 98 5 401 1228]...]\n",
            "targets[[185 45 329 40 894 564 3 1902 55 15 105 9319 887 29 3 916 54 4471 14 2][8 333 11 60 798 25 213 613 2579 2579 2579 2579 2579 2579 2579 2579 2579 2579 10 13][12 23 73 4 28 300 43 303 377 10211]...]\n",
            "targets[[9365 21650 451 8 1844 15 26 183 518 5809 4213 4464 2 1187 10130 3 2873 1816 761 5][9 242 2 340 3 0 6845 6565 1209 5040 83 20495 511 98 18 0 13294 13 379 6][4774 5 35 5279 814 1448 4 3649 2 1061]...]\n",
            "targets[[84 9 67 0 2277 2 171 3 75 319 15 99 318 10 11 676 3 1721 75 32505][17 13 37 379 7 162 71 656 9 67 77 3085 2053 1 4700 14 2 493 9 307][105 907 93 2 877 3 2 184 339 3]...]\n",
            "targets[[2 1349 115 19 10 5 92 8 5290 22 47 13 3551 2 53 360 341 10 105 32005][20 387 5270 6503 10 17 386 21 65 382 73 4 20 35 570 13 92 4 39176 2][897 3615 538 912 22 0 17 7 12 2]...]\n",
            "targets[[4140 4 2666 6756 439 78 0 797 33 575 519 156 36 11 19 8 2 299 63 15][4 875 0 305 290 141 28 748 305 16384 51 0 153 15745 181 9 481 24 79 11402][37 108 683 1652 10 19 18 64 29 83]...]\n",
            "targets[[9 1281 10 17 2 223 16 804 2 605 608 3 778 8 259 4 1006 2 492 184][280 38 0 17 159 21 1090 73 281 18 7 118 1090 0 153 2 171 3 532 4][2182 3 884 0 15378 1 816 10 19 45]...]\n",
            "targets[[139 110 30 668 3 0 98 8 10 202 253 29 16063 1037 1 1037 36 0 1177 10][5 2 65 594 17 9 509 7 4623 1 106 7 174 57 7 5 22 246 6 6][1680 834 3 495 1435 994 211 4 114 8]...]\n",
            "targets[[3 0 250 98 123 92 23 64 8 0 6511 477 61 1745 35 504 7047 3 1356 10609][2 390 38 5837 814 47 25 20 65 1027 10 17 5 2 414 502 3 2 6161 1185][3325 14 7997 0 16564 5 2 1714 6683 14]...]\n",
            "targets[[2789 343 201 451 55 4 91 6380 7745 1 21208 3751 55 4 2 2013 1 8212 1345 39][12 241 56 220 8 259 4 1652 21100 1278 9197 1028 25783 17 4 255 34 1505 21 478][4001 262 10 9 13 2873 51 9 215 37]...]\n",
            "targets[[17 32756 199 3895 5998 436 1 4242 868 6 6 0 5998 436 172 13 1390 292 33 0][15 0 883 5 186 73 1507 147 18 2325 2 1260 3153 3 1893 573 143 8 8575 51][8920 5 13971 2 112 63 199 105 10708 101]...]\n",
            "targets[[140 2 340 3 193 156 5920 257 14340 1 51 9 84 1757 10 17 1 106 0 1561][120 9 113 191 0 57 4 165 949 2 738 3 100 3 0 98 9 27 110 18][8920 8 60 660 5 2 81 11209 7669 17]...]\n",
            "targets[[2 366 9 80 23 395 149 10 17 8386 0 17 1109 0 900 16 0 2192 2864 3958][24616 1 2743 32340 25 775 0 2646 342 4 27 77 22 2361 311 409 7 12 2335 11][17 13 142 2 4564 7 45 11 230 49]...]\n",
            "targets[[47 5 1640 36433 117 166 4 1309 0 4853 6191 2102 628 10283 203 2680 0 1694 41402 2697][5 33 229 0 872 842 8 1054 15224 19 369 513 33 0 9587 468 2057 27727 34 627][5 23 6531 3134 12 117 19 18 7 12]...]\n",
            "targets[[17 5 677 6798 1753 4924 4924 15 1753 1761 460 1711 381 39776 33 316 4924 34 1664 1753][17 13 379 264 9 27 110 48 447 98 167 0 26910 5 0 88 612 102 58 152][911 1501 4 378 471 17 264 0 116 13]...]\n",
            "targets[[509 127 738 255 884 127 738 34 25 23 19451 3 2054 83 854 11 10 3127 96 710][215 2 222 3 10 227 291 1 30 9 50 136 5 10 6 6 9 419 32 50][198 10 17 2 459 16 91 2011 8391 9]...]\n",
            "targets[[1943 12 368 755 654 78 2 368 1115 19 15 5908 9680 14 0 691 1 26 21915 3097][5 35 6841 3 35 881 940 29 95 5766 5 1336 0 88 2139 868 434 3 30 57][12 97 74 10 19 288 21 20324 2 2422]...]\n",
            "targets[[7 12 2069 0 3572 1 33 0 984 5 37 3056 1306 5 143 18 24 12 3056 9][2907 12 84 81 19 3687 6405 13 2 19 11 13 5536 4 28 3047 120 17790 1 12295][353 0 298 8 60 646 713 1 99 70]...]\n",
            "targets[[50418 12 330 8991 8 0 328 45 4 28 29 3 4895 12 117 39 5 983 3 600][17 13 1136 9 59 395 7 4 255 73 126 72 47 9 67 478 7231 7 13 401][9546 288 21 4095 1669 12 872 17 18 7]...]\n",
            "targets[[17 13 42 39307 33 3566 1 15 49 293 84 1 7096 85 7 5 2 814 17 1156][26862 6675 2698 188 4 28 2 737 3840 34679 5638 7071 469 75 25 4437 38 4573 187 40][42 215 10 19 16 0 13340 57 9 367]...]\n",
            "targets[[10 5015 231 570 3 0 599 2707 8323 33 0 2497 269 35 2816 1152 3 2281 405 1909][4994 7264 578 8 0 200 6183 15 1133 2730 2769 451 1 436 4 4248 10 122 968 1262][27 307 10 19 440 154 143 1 51 9]...]\n",
            "targets[[5 0 244 3 3140 3788 3495 439 50 80 208 9 169 7 3214 11 286 15 142 115][1018 2 222 3 10 2889 22 1008 729 1 693 9 67 4 27 7 0 921 198 295][7 12 2 222 3 2 658 19 37 46]...]\n",
            "targets[[1022 37 10 5 47 30 0 50896 13 43 3543 69 9 140 251 11 0 673 16 91][0 2195 3902 2130 22 2 1717 2841 11 5 42 43 4 1382 3504 55 112 50 28 448][439 39 327 25 53 173 741 4 367 10]...]\n",
            "targets[[196 0 17 257 0 109 784 2 171 3 166 0 756 3 0 17 1386 31548 1 11405][17 4 28 509 33 30 0 231 42 106 44 16 48 9619 1 2 115 537 39 68][0 834 3 3341 338 5 52 3 2 3225]...]\n",
            "targets[[5 23 127 737 184 600 19 8 189 2 171 3 0 184 5 319 4 127 1629 11][552 7231 10 17 256 509 0 298 13 53 671 14 0 17 352 319 44 88 3 0][20 159 21 367 10 17 352 127 330 41]...]\n",
            "targets[[5 29 3 60 519 231 98 467 7 51 9 13 115 1 7 133 1765 55 15 71][55687 9149 7617 554 360 341 19 11 45 14399 3 10822 245 4 865 144 9 76 0 754][67 110 10 19 95 143 8 0 1001 12]...]\n",
            "targets[[414 5 35 486 3 1817 5680 2 19 111 174 102 30378 613 3206 4 2695 1442 10597 1861][17 5 37 1149 2 297 12998 368 6 6 9 402 1453 78 10 17 14 2 11244 8][3157 41 15548 14 7 12 11184 542 4 646]...]\n",
            "targets[[24205 5 2 81 1 1031 153 9 723 21 110 238 332 33 86 239 72 10 0 6754][3006 8799 3 417 652 510 4133 8 537 25 254 8 952 1265 8 0 484 3 21845 8][1550 5 0 117 3 0 733 1541 1012 98]...]\n",
            "targets[[84 151 9 196 99 318 10 17 13 87 5 10 614 11 0 8561 165 386 2 1686][81 11360 3 584 24951 5 74 19 8 174 95 0 225 0 5054 1069 0 608 3 1132][263 214 8 26 17196 590 284 9695 45 8084]...]\n",
            "targets[[12 56 6072 7 10140 978 2243 317 5 394 0 116 5 37 74 11 20 232 3454 1][5 23 64 0 4144 3 30 1224 98 18 0 872 184 17 123 284 3467 1 9402 2000][194 227 2041 26370 35 554 1031 279 34 1193]...]\n",
            "targets[[619 4398 282 1065 11 56 534 96 123 2228 2460 13989 4 0 6392 3 2 243 507 16][89 21 121 87 7 195 49 2921 9 89 21 456 46 7 13 2 297 63 9 856][70 651 8 1806 2591 374 4784 1 681 0]...]\n",
            "targets[[1116 19 36 5051 8 61 0 1500 3 6845 14867 303 377 25 6039 15 2 160 30388 4357][139 77 2754 2 394 19 6 6 815 9 232 987 11 233 9 140 460 1 27 67][140 23 2 8791 147 882 123 27 77 18]...]\n",
            "targets[[17 5 2 997 36 0 457 4 0 129 0 101 80 1 136 47 0 543 491 90][74 19 53 53 53 74 19 7 12 2 10027 18 7 5 23 273 3299 185 10 1071][203 66 16 74 17 4603 247 528 2 461]...]\n",
            "targets[[5095 1070 4 479 966 12 3115 1667 1056 201 78 2 3631 917 725 15 0 914 107 193][39 20 27 7 157 24170 22 60 2619 105 165 84 3 30 58 152 9 38 4 103][6625 5 437 379 0 109 5 53 348 1]...]\n",
            "targets[[29 3 0 88 901 1211 1 4316 1146 98 22 0 3246 123 92 10 5 229 1674 4][317 2502 36 35 1914 3505 4 168 31 0 3615 20 59 103 7 12 43 2 461 2217][312 118 23 942 47 2 1594 10 17 13]...]\n",
            "targets[[793 4 93 71 262 11 0 834 3 10 29385 2333 184 680 13 1403 33 0 851 227][371 856 137 397 36 10 17 35 3309 177 3 1031 1 155 3154 141 27 68216 69 6][3320 1704 8 112 15 4197 15829 10 122 982]...]\n",
            "targets[[51 9 215 10 17 31 2 2166 9 3580 42 87 3091 7 13 11 15830 27086 2663 8][24192 1 888 27 3428 137 11 5 53 1247 131 483 2 734 201 15 3989 7745 1 21208][291 60 563 3040 5 634 43 275 183 362]...]\n",
            "targets[[5 51 8309 269 55 36 0 3614 6 6 9 118 23 512 73 36 8309 10 501 1606][3778 7250 17994 732 8 2 209 3642 40 1966 6 6 2 323 2573 7314 34 45 115 116][19 5 1009 4 0 220 3 107 11612 1058]...]\n",
            "targets[[7983 5 2 153 622 104 25 245 4 3919 24 12 92 275 3 60 30 57 519 104][17 5 60 519 29 9 27 307 7 52 72 163 214 9 1207 121 30 0 410 174][6724 2570 5262 16 3345 18 11750 3 12713 505]...]\n",
            "targets[[125 9 470 4 38 10 17 7 45 2 81 177 1 7 5 2 385 1633 19 1][21 76 71 356 9 424 9142 1512 18 10 29 13 73 126 0 63 13 126 58 0][3 0 2983 5067 6198 45 7961 73 690 43]...]\n",
            "targets[[656 898 67 2 2190 3 163 678 10 13 2 394 432 3 443 437 56 4096 56 2615][12 161 160 477 12931 48 3 0 126 2601 236 28 98 38 0 960 12001 3 7946 908][5 29 3 60 519 98 295 8 7 5]...]\n",
            "targets[[2185 22 0 17 796 6 6 7 13 23 49 23 49 31 30 2240 7 13 81 9][42 389 143 36 2 1689 768 816 3 10 322 832 827 19 22802 7 12 393 5 23632][343 1 868 201 9137 0 868 5 7543 33]...]\n",
            "targets[[11663 13 335 22 351 3 26 458 51 24 326 10 19 7 5 35 6548 5358 19 368][2 57 51 0 176 5 37 2890 22 0 1849 8044 8 3854 0 151 11 5 88 731][307 10 3529 22 6 6 9 140 23 251]...]\n",
            "targets[[318 10 609 9 42 96 23 1017 545 36 13049 60 333 420 21 1643 4 1775 1 9696][111 80 9 875 10 17 1168 60 114 0 109 13 11000 601 20260 45 44 226 313 2537][5 23 29 3 146 104 7 5 29 3]...]\n",
            "targets[[8 23319 11477 30354 12 1054 1081 673 64862 4572 1437 13 35 2004 20904 2783 4989 0 2486 1][11 12 30 11 50 28 300 43 10 19 10 1525 21 58 28 1267 2 662 4 0][13 2 1049 10323 249 8 407 8600 608 3]...]\n",
            "targets[[293 0 277 5198 3 10 19 25 8 316 1 460 5 85 1276 50 76 62 933 22][206 5140 699 246 5453 23406 36 2170 13 142 35 1373 369 11 7 465 1099 4 93 2][45 0 3509 10259 3 43801 1050 15 15164 1]...]\n",
            "targets[[12 60 660 11 51 20 1134 4 148 93 2 53 49 19 20 141 12432 4 80 126][2 19112 665 1 178 1249 1768 284 1188 30313 1543 7867 1 26 312 3753 12 30313 36272 33641][63 3 4022 1756 45 77 41802 8 4849 1]...]\n",
            "targets[[9017 2324 267 143 78 0 2494 4 586 276 15859 36 1540 17535 9834 1321 159 21 6100 5985][13 433 19 7902 1 19 2921 334 72 1719 357 9 215 0 9356 748 733 5991 3 574][3044 8 5 23 155 23 734 23 1009 7]...]\n",
            "targets[[2990 6088 4860 9381 5 133 2301 33 0 311 24 1963 0 3337 1479 461 2095 27212 601 34][439 25 37 6966 37425 32 27 1291 0 217 5273 3432 344 1 25661 39 12 437 56 813][353 30 131 829 22 128 43 87 10 5]...]\n",
            "targets[[13 475 10837 8 10 19 36 0 84 4 227 782 7 5 2130 326 15 745 3 212][19 45 13830 142 3983 2761 30 3 20 42 353 0 798 11 9 140 180 1520 146 3][84 307 10 19 8 35 407 687 1860 472]...]\n",
            "targets[[12 212 4 66 47 2974 5293 5204 12 590 13 8 167 1420 4356 22 0 130 8 10][1315 3 1144 992 10 17 5 1373 9 67 56 321 11 4955 389 44 37 407 264 9][1508 6451 30939 13223 11838 3901 107 2 1508 24]...]\n",
            "targets[[0 1191 317 5 2 1157 1223 0 116 5 3704 0 225 5 10508 1 0 224 5 379][481 9 139 421 3063 33 149 1432 49 98 8 1187 154 18 10 13 33 229 0 250][1267 545 4 28 180 7572 257 51 9 106]...]\n",
            "targets[[4 2 74 1212 63 1 286 12 1969 10 184 755 635 2 4757 985 568 284 3479 1][140 11652 11 56 29 188 4 402 14331 8362 31 219 8 0 2348 32 89 21 4831 0][45 77 60 894 519 298 123 233 9 353]...]\n",
            "targets[[2591 328 36 5912 5 29 3 0 10103 928 41 873 104 9 139 123 2982 531 22 188][7457 6371 12 237 5 7 42 71 41 124 10 216 213 982 4 13909 17839 8 62 98][6288 3623 543 6840 22458 21200 22452 11 5 4143]...]\n",
            "targets[[88 3 60 355 1180 10 3578 4 71 2318 32 1667 3383 2 246 122 4 142 303 2846][5 65 23 2 53 49 201 264 9 27 509 149 14 2 3254 1 22 142 201 1380][817 725 45 77 2 1201 477 233 0 2845]...]\n",
            "targets[[604 66 134 1035 5249 10 17 6 6 0 5010 17 15 5283 1 15060 5 221 2 368][84 9 96 23 262 601 12999 13 391 2543 4722 18 24 5 330 22 15 26 1110 24][1 556 314 6945 30774 19436 34225 34 1065 513]...]\n",
            "targets[[9 121 126 72 4 512 2 1051 1470 3 2 673 51 338 218 1017 3 7 9 13][0 544 1916 703 153 1452 5359 12570 1 5352 3948 17456 1825 1 254 32 4983 2 171 3][242 1957 8962 4 66 2 938 3 463 798]...]\n",
            "targets[[124 69 128 257 1078 10 5 64 40 3987 19 10 29 5 404 8886 85 7 724 199][5 56 813 11 10693 12525 5 2 3256 8 1680 1 10564 2185 13019 8 0 650 4515 547][5 2 681 5324 4927 869 11 113 982 4]...]\n",
            "targets[[17 43 105 1071 887 34 389 4 2563 15 62 231 5 42 81 6 6 7 210 21][20 25 475 78 2293 343 968 39 25 173 2168 128 783 240 36 10 2227 92 432 3][0 63 5 2088 2594 1 0 842 271 5]...]\n",
            "targets[[5616 12313 5582 0 3556 2388 5377 40319 25972 13 15058 8 20264 0 4243 45 113 77 254 10][321 839 2097 6 6 128 25 0 9335 9 50 136 43 10 17 0 321 3 2 705][103 91 57 16 3069 4 138 4960 78 0]...]\n",
            "targets[[139 110 10 17 51 9 13 3650 8 3207 9 254 7 871 4 65 387 7740 1087 1][27 56 321 47 0 1104 3 0 7728 68 259 4 80 18 0 914 2400 16 392 0][3 0 117 98 16 30 2220 20 83 113]...]\n",
            "targets[[10190 2247 1606 39 5 56 145 95 4 1823 0 105 190 233 2247 1606 5 2 637 11][17 5 12 1286 629 1846 1066 88 3 0 17 165 1073 16 7 4 875 6 6 0][4 0 3401 16 10 122 9 329 4 2655]...]\n",
            "targets[[155 87 50 255 3968 10 4 6257 6629 11 5 437 630 39 25 56 915 10 5 23][644 389 14 2 938 842 23 256 2 2374 14 4 47 7 13 30 43 7 1499 120][307 10 19 108 214 1 367 7 42 14]...]\n",
            "targets[[54 2164 55 8 47 108 59 1652 14 2 2914 755 176 10 17 274 11 23 30 338][5 2 65 81 247 19 11 2107 15 48 3 0 1429 3 57 1899 78 0 501 46][18 9 89 21 1046 15 0 75 128 0]...]\n",
            "targets[[5 0 84 44 3 0 9509 3982 202 1 5 29 3 0 52 3202 104 44 3 0][10328 10328 36 17322 57550 682 10 5 2 19 43 1700 43 11 305 244 3 1700 3 0][13 770 31 0 1152 3 1370 14113 1264 181]...]\n",
            "targets[[19 529 2 3927 667 3 0 472 3 0 759 1328 1708 8 15379 7 406 0 507 193][734 1 641 2013 436 1015 4 18639 73 573 9 65 509 0 2468 135 873 7 188 0][10 5 247 3268 20 27 4 28 8 0]...]\n",
            "targets[[57 9 66 1406 5679 7 12 14 46 2 6547 316 2184 45 3257 1 9 140 1983 21677][418 4 10 17 31 2 177 1 888 122 1090 60 425 916 5 2 1252 22 0 17][108 3 131 82 829 27 2963 11 2 171]...]\n",
            "targets[[27 23 353 0 673 10 19 13 427 22 18 9 118 1059 106 0 2514 339 132 0][19 5 44060 3 4550 98 0 705 4550 3912 12 6726 0 1258 4550 0 1787 6568 1620 0][21 28 7584 43 3995 10 17 9 112 7]...]\n",
            "targets[[657 25 576 0 2051 25 2281 1 0 101 25 1344 54109 18 9 420 21 535 36 1014][11893 20 25 1213 7442 34 150 21 121 2 151 36 0 81 20094 41736 10 125 45 77][2260 55 8 9777 1 94 307 91 954 78]...]\n",
            "targets[[499 8 5582 8 4569 111 316 1 8948 2252 7127 1017 3989 11 12 111 32 2384 55 190][13 10403 31 47 35 379 369 10 13 1078 1769 12 6028 2535 376 3611 1 116 5 2][215 10 42 43 414 19 51 7 7980 31]...]\n",
            "targets[[46 20 148 2 243 34 12 195 13241 1205 20 236 38 10 17 722 127 2824 82 10][4585 3079 45 3521 2 765 6339 43 7 73 38 314 2520 11 12 85 39 25 108 4034][64 2065 10 17 43 3106 1816 602 51 9]...]\n",
            "targets[[9 353 10 298 1 13 2229 4 66 0 17 4 66 47 32 59 6 6 9 67][9 27 110 10 17 1 59 38 4 198 3823 1808 29 39 5 161 43 10 17 4][42251 5 2 81 19 16 0 5438 7 971]...]\n",
            "targets[[1430 1022 10 17 210 21 497 7 12 165 2 977 49 17 30513 20 139 113 110 555][37 32 350 4 93 10 38 35 851 145 176 122 1 94 7 267 17 22 178 1][7497 12 6441 4 0 276 443 5 4064 26]...]\n",
            "targets[[42 110 22538 10 17 389 4 333 4376 0 105 9 103 10 29 13 52 1097 0 4032][181 2702 201 11 12 1034 53 3186 882 155 10 10362 4798 1109 18998 947 49869 144 2 5803][215 11 17 173 483 602 10 17 5 37]...]\n",
            "targets[[9 970 970 629 5 0 8714 19 123 92 9 50 21 262 60 324 1291 10 609 10][183 10132 342 2384 14 921 1 350 4 76 78 2 176 4165 28200 1162 199 7156 1 27559][0 20230 3 0 1278 12505 1 3 0 11674]...]\n",
            "targets[[5 0 447 19 9 139 581 31 8 2 194 57 0 116 5149 36 379 4 2975 18][49 6 6 2 1043 641 7681 7407 261 125 1455 22 0 1350 3 0 4794 3 0 89][78 2 3061 10600 624 31 311 1 136 1785]...]\n",
            "targets[[1996 1 3566 319 44 5 0 117 172 1 64 913 130 3 10 873 2062 17 39 5][36 22764 1 1192 2095 10 19 5 2 1792 4 0 531 15 323 5501 5883 36 12 2059][300 2 940 11 92 71 53 671 8 40]...]\n",
            "targets[[3 0 881 1996 27 446 24579 774 1099 1 27 1712 134 7 13 92 60 864 16 131][19 702 2 1571 63 0 503 1 0 101 25 37 145 1 1011 37 69 20 50 21][140 2 669 340 3 0 9196 3 13691 246]...]\n",
            "targets[[6 815 37 9 139 555 3 0 21756 3 12410 10 17 45 2034 9 139 555 87 10][84 215 970 11406 42091 10761 12 786 31358 29341 125 6611 821 43 720 154 602 1 9 13][188 4 28 1 145 6115 5 0 1406 9]...]\n",
            "targets[[373 1 2 518 409 8 2 1040 346 391 1409 3 855 11842 1 148 24540 0 1497 3][50 106 2 49 2019 19 147 1 94 9 139 110 48 186 1213 528 190 10 5 29][84 1018 0 17 22 91 84 488 22 3685]...]\n",
            "targets[[13 1262 671 33 10 17 9 27 1059 1291 193 202 22 277 14 9 67 110 0 84][9 84 215 8434 13264 12 397 2835 0 773 20577 9 1147 30 18 0 227 969 228 10][389 596 10 19 445 0 412 885 7417 31]...]\n",
            "targets[[5 0 117 2499 19 123 92 7 1886 52 94 47 7 195 31 0 4076 2555 16 179][4326 12 673 3342 1 5576 5 208 2 813 29 3 60 519 1177 1 147 429 7 45][53 413 807 17 36 2563 46 20 112 0]...]\n",
            "targets[[65 470 4 38 10 17 18 7 13 42 54404 0 116 13 3104 5646 0 109 13 612][196 7 13 2 186 49 17 0 116 13 49 1 0 63 13 69 585 7 12 29][13 8 2 394 95 194 167 0 276 389]...]\n",
            "targets[[5611 52 72 537 0 372 25 42 4160 2400 7977 12 324 31 0 457 3 0 19 0][24799 14083 3828 23 58 49 16 0 2922 12307 55 0 346 965 37 32 810 9511 7797 7][7 13 29 3 0 117 98 9 27 110]...]\n",
            "targets[[0 457 70 50 66 1102 3 4895 739 1077 1 6059 4925 550 10 5 74 7 12 2][1473 285 2 12231 44 18 2780 1141 22 0 2504 16 2 1479 461 34 45 2 13938 433][140 2 200 340 3 3585 98 1 4733 9]...]\n",
            "targets[[5826 9716 1721 550 6884 2 941 12 531 1621 1281 181 24 12 23 356 245 6388 5 2][83 198 20 105 996 16 149 10 832 827 13416 43 2 10337 5593 11 5199 4 8071 44][422 141 27 77 3308 196 44 440 214 1]...]\n",
            "targets[[203 27 77 440 154 99 7 13 645 37 89 21 121 134 7 13 31 0 98 18][17 5 37 948 7 5 221 2863 126 542 14 2724 1947 0 24994 817 10 487 494 4][429 195 545 270 55 22 5074 643 277 2255]...]\n",
            "targets[[4881 3905 12 88 1571 237 1 1675 698 0 243 45 654 8 2 173 294 40 590 9][0 245 6388 1141 547 3 20634 15617 1 4172 7852 27 13139 4 443 38 2 1670 8 2][1426 291 158 396 33927 5 1448 36 2292 4]...]\n",
            "targets[[221 283 9 1818 89 21 940 22 104 9 723 21 110 30 0 95 144 18 233 9][5 42 29 3 146 3256 2446 28 540 328 2684 6540 92 8 1916 9 80 23 121 134][1 338 67 77 22 0 5257 14 0 6498]...]\n",
            "targets[[10 5 29 3 200 12 117 685 12 666 209 8 7321 3182 78 105 16 0 1248 66453][204 23 28 0 29 4 738 10 17 85 99 3595 228 3 1051 3203 1 3057 9 654][1959 2087 47 1117 2166 491 4 106 41 58]...]\n",
            "targets[[276 4743 491 4 1738 2 1437 22 2 2727 958 37 24 45 286 13287 86 1 26 939][483 99 318 29307 14030 9 242 133 8 4313 11 100 522 3 75 59 1143 37 73 57][112 2 19 11 150 21 0 109 1 0]...]\n",
            "targets[[46 20 27 2 115 119 35 541 4 496 1 169 2856 4 28 97 1094 9 207 1478][467 10 17 7 12 371 1185 554 155 6288 1799 7 162 56 266 4 380 43 0 10047][17 50 2222 4 28 2 360 360 341 600]...]\n",
            "targets[[371 337 63 15 2 1534 43 20239 112 4471 10 1055 577 2146 19 10 13 257 1055 85][113 188 4 11642 71 87 74 104 50 165 28 18 10 5 35 2449 4 1648 87 124][194 602 9 1065 60 990 14 4 47 40013]...]\n",
            "targets[[5 0 64 17 11 60 312 1 9 27 123 2518 44 22 475 2144 70 215 7 8][215 7 31 0 33993 15 2 425 8 0 122 497 6356 13 884 31 0 143 3 0][10 5 2 994 7 5 2 4588 19 6]...]\n",
            "targets[[389 85 9 555 7 13 2 1484 199 6963 0 1341 9687 0 1 9379 9 2464 85 7][5 35 212 297 63 3 10475 3541 7721 34 8793 3 107 35 51 24 13 2 493 357][103 10 17 5 117 7870 55 14 1109 852]...]\n",
            "targets[[84 202 3 400 4211 120 15 2 3729 1207 1 1366 29460 8 1069 10 204 27 278 48][17 5 730 4 0 304 5343 27776 1325 408 33 10670 7117 0 109 3 2 1212 312 1][69 69 32773 42944 5 29 3 0 1536 156]...]\n",
            "targets[[5 0 88 1211 19 9 27 110 8 2 194 57 46 20 38 10 17 138 818 7442][3 2 3063 945 527 34 747 4 2132 36748 1 2025 4 0 1395 652 3 26 160 303][27121 65 4231 4 28 11120 16 10 371 19940]...]\n",
            "targets[[471 5 23 74 1101 4 3862 2124 184 7 274 0 457 1 2450 3 258 519 184 314][89 21 103 100 1837 8 338 472 4939 14 194 14 577 6375 118 358 88 3 0 849][17 13 53 777 7 13 4610 1 155 31]...]\n",
            "targets[[242 715 10 202 2 163 23 85 3 0 109 34 12 8789 18 16 0 1136 12319 1][89 21 182 4 198 10 308 314 9 470 4 38 7 18 7 1201 22 37 108 5611][5 241 29 3 0 250 98 9 27 123]...]\n",
            "targets[[210 21 2 414 19 165 1225 740 107 414 18 1321 10 17 210 21 2 1223 352 9][3895 755 3 2 363 3536 4 0 4131 3473 1 1410 44 0 1282 13 23 606 14 6364][21 28 4374 33 238 11 20 27 123 353]...]\n",
            "targets[[0 3988 1 407 3821 8286 5421 13 5363 1267 14 29 3 0 872 184 964 123 4 1656][1 1588 8433 198 49 347 14 105 14897 556 292 4 8071 44 0 2507 1238 0 366 3][22 12 160 7553 31 459 11205 22 38298 205]...]\n",
            "targets[[20 38 832 827 1890 1 2291 4749 94 20 83 112 10 17 6 6 0 305 290 25][267 4 0 69 29 97 108 214 14 1859 34 45 110 0 206 115 7267 83 230 5721][36607 46 20 159 21 58 7155 31 11 390]...]\n",
            "targets[[548 10 122 19754 5745 36 0 705 298 1 17 8 189 9 424 10 10 246 122 0][5 2 53674 19 4 106 2 6469 311 15 127 355 62 158 20264 5439 690 5 65 137][30 276 13364 11668 5 161 18 2 10754 8081]...]\n",
            "targets[[29 3 0 250 9 139 123 110 51 60 3097 446 71 55 4 287 71 121 11 19242][13 2 130 8 10 17 1049 5 621 55 0 1437 5944 1 960 74 454 349 0 95][13 861 4 138 66 10 17 15 2 742]...]\n",
            "targets[[17 13 2 1498 432 3 1243 20 113 693 47 13 164 22 0 101 68 839 408 1][78 10 17 20 121 11 10 5 17 45 1408 3990 18843 8 2 8860 3990 15 35 3355][215 7528 16 0 84 57 227 1606 85 9]...]\n",
            "targets[[9 27 4 4938 0 543 3 0 19 34 118 1997 4 76 2 765 1152 3 937 1120][10 122 389 22 143 8 0 1752 4128 7 13 2 1737 16 193 362 1 1532 3139 15][1222 3 3060 14505 8 4407 113 1439 1 94]...]\n",
            "targets[[84 555 43 460 3694 51 9 215 0 246 19313 167 94 9 159 21 58 121 7 4354][7 12 64 29 541 194 1278 4841 5 2 53 1413 102 745 3 22534 17989 455 534 14285][19 5 427 22 2 1749 3239 673 6 6]...]\n",
            "targets[[188 180 31129 4 1401 2 12137 153 12 84 806 1576 5663 257 51 20 27 23 239 92][4095 1388 12 2515 941 384 15 2314 3567 3522 14 29 3 0 52 3337 3 0 1689 2550][9 555 47 12 0 757 3 1105 1526 9]...]\n",
            "targets[[215 2215 56 1731 2346 8 10 17 0 64 151 9 118 66 13 5319 3195 12 5909 2028][145 13 30 326 8 29 191 7 92 0 16767 298 3 5508 147 8 60 660 100 166][108 82 104 61 25 1211 352 33 28440 3]...]\n",
            "targets[[44 839 441 3 38 2 1808 823 246 17 43 2 1666 1988 3705 22 16 43 1706 228][12 23 0 117 883 17 7 12 401 23 0 117 960 17 7 150 21 27 0 117][19 162 56 11555 43 107 238 546 2 755]...]\n",
            "targets[[5 29 151 9 50 136 43 10 17 1317 9 140 29 3 146 75 34 1285 4111 37][39 12 238 22 246 11 96 93 814 847 168 38 972 1864 2668 3719 2004 5 7 23756][288 21 251 47 4 512 51 9 418 4]...]\n",
            "targets[[2838 9 50 6208 136 5 241 60 160 1628 122 7 12 37 1616 1 607 1 58 0][84 9 13 2 115 11329 43 633 57 44 3 60 114 4 66 1426 9 382 9 242][139 77 259 4 1434 185 10 19 42 33]...]\n",
            "targets[[9 140 23 8014 4 949 829 18 10 17 13 37 7692 1343 9 230 9 203 9 12344][22 0 206 0 379 803 10 6841 5 37 335 4567 29 45 4 589 47 0 3799 13][2190 1199 2190 225 308 444 463 1394 17238 116]...]\n",
            "targets[[958 13 142 2 1133 1670 725 11 1426 154 309 7 13 654 78 157 1156 2181 16 16123][1 57 175 7 188 11 0 1619 156 3 338 25 1608 71 15 62 1966 14 909 3154][5 1872 393 463 29 3 0 117 104 9]...]\n",
            "targets[[2 543 9 169 104 10 74 235 7 78 369 2 605 4127 8 0 384 690 43 3214][3 0 88 322 98 123 1056 8 4800 1 426 0 117 29 92 294 0 6983 3 0][412 550 7 30 2110 15786 210 21 0 64]...]\n",
            "targets[[177 1024 22 3851 225 720 41 37 1532 9125 31 0 1606 1244 32 6417 14 37912 96 10][10 738 5 1216 1517 9 182 4 93 2 1153 220 84 157 3615 128 3125 11 12881 4089][1556 43 2 385 474 985 243 4708 3744 34]...]\n",
            "targets[[0 176 8859 31 0 6181 19 1322 6 6 20 25 1435 78 10 462 17 1 604 479][74 46 20 353 0 1177 80 661 2 2200 1 89 21 278 661 144 0 7261 3 1353][19 1020 4041 33 7585 3 91 655 1 16]...]\n",
            "targets[[144 0 4278 544 31 311 9 389 596 10 17 1 2700 9 207 830 7 44 9 1147][1 52863 45 213 77 60 519 298 9 27 353 2 171 3 1177 1 587 3 90 67][368 1079 221 3481 1 1507 241 85 3 7]...]\n",
            "targets[[848 1 48 21413 102 954 383 10 1264 1744 9036 36 12291 31 219 1678 190 7 1386 641][20 123 169 661 149 2 246 122 1 532 9 656 11 67 77 226 5103 80 20 123][89 21 333 11 0 17 45 56 109 41]...]\n",
            "targets[[312 1591 10 349 15 30596 481 61 29 5 126 9 140 2 6293 16 2 49 5097 17][5 29 3 0 117 98 9 27 123 110 0 684 535 181 5601 15 0 2028 20940 3][20 27 0 598 4 106 0 17 41 353]...]\n",
            "targets[[19 5 2 13420 21967 8 61 0 276 1790 571 8 0 5353 15390 38892 25 30 21248 1][2 13100 206 202 9 207 136 91 539 873 9 207 136 91 186 1587 49 0 547 25][736 109 264 0 466 173 6448 68 1764 226]...]\n",
            "targets[[3 0 117 699 2299 202 123 9 139 77 259 4 810 10 202 22 1912 41 277 16][984 3 0 5388 19 33 0 170 390 4069 576 5724 4 114 5 323 18 7 5 2][9 13 2 183 125 3 2873 154 3 567]...]\n",
            "targets[[144 0 829 3 10 19 7 12 731 4 71 11 0 746 816 1008 0 1475 34 89][5 35 322 556 3 567 5240 19 14261 9 139 1018 7 22 1901 119 0 154 9 139][8 3207 0 68578 3030 25 355 3 0 1]...]\n",
            "targets[[5 2 2193 74 8894 3 0 14695 1720 0 74 454 359 16785 13472 1 2350 316 5497 7502][139 42 353 48 3 0 82 798 22 10 19 1 242 147 3830 8 2 7604 15 907][30141 20916 10 19 1470 3 26 20904 43 2524]...]\n",
            "targets[[2164 55 294 0 57 11 0 224 8 10 17 13 1076 47 2 397 57 16 224 1][43948 1465 1724 422 5 29 3 0 3353 3 0 3400 905 111 2 27983 25185 7516 1459 13093][13 2 397 246 869 202 4742 22 2 1248]...]\n",
            "targets[[140 23 2 200 340 3 1224 1412 14 2 477 18 58 33 0 1645 3 360 360 341][13 2 49 17 7 288 21 127 737 299 487 18 137 2 222 263 10 17 1197 178][10 569 21 27 77 35 1373 17 46 10]...]\n",
            "targets[[12 727 4 1226 22 19242 1 651 7 2 74 17 7 12 23 206 7 1449 2 222][7 12 4914 12979 1 780 14418 8 2 17 43 2 330 939 2404 0 160 939 7 12][2117 8715 3247 3861 4943 45 2 679 430 22]...]\n",
            "targets[[13 1435 78 483 8 27114 19832 51 15132 13 8979 1722 2227 42 167 24 654 78 0 2630][481 9 50 21 1652 7 8 2 126 41 5785 11942 95 348 1 374 116 47 116 134][2617 5 2 53 49 7816 339 3 0 368]...]\n",
            "targets[[75 210 21 43 22368 7 210 21 43 75 7 210 21 65 43 238 7 5 180 853][408 17689 12 84 8 26 2622 10 695 0 1194 16 0 6280 6296 1 405 3 0 105][17 8088 747 1 623 31 7443 1654 8088 12]...]\n",
            "targets[[6 37 595 37 49 10 19 5 2 7514 8 4982 443 20 83 23 66 0 37 446][121 39 25 108 44 39 34 103 11 8302 23295 59 27 77 126 177 14 2106 72 11603][12 213 2 2319 842 4 2505 35 25909 17]...]\n",
            "targets[[192 10 19 45 955 1425 2 81 109 1 523 156 190 9 604 345 18 4 28 671][471 6512 65 693 87 4 1006 2 368 1 99 700 2688 26 869 269 596 14 0 145][3285 431 3285 6 6 16 44298 439 10 13]...]\n",
            "targets[[84 422 1240 529 2 49 1461 47 4 512 36 0 202 4644 1073 4 28 6971 1 2][455 619 12 2913 435 67 31 48 220 8 0 501 77 2227 1827 1978 22 0 95 346][0 88 172 10 19 5 7046 33 48 397]...]\n",
            "targets[[196 9 13 164 4 27 4 366 7536 60 531 44 294 10 17 4 93 0 57 1372][371 777 63 397 7744 3 676 1 2 583 757 93 10 5 2 1571 2789 17 23 29][43 10 17 49 177 1 523 116 18 7]...]\n",
            "targets[[20 38 98 15 56 11343 109 15 3094 20 89 21 456 43 11 560 20 1716 4 12938][2 194 57 0 22066 13 60 519 314 2520 422 152 8 1187 154 7 45 77 19878 33][17 5 4339 8 181 7 45 756 3 0]...]\n",
            "targets[[17 4655 10363 30 3 131 37 446 156 8 10 19 365 4 113 123 166 8 0 19][480 17 5417 43 0 5352 3 0 176 299 1331 18285 583 347 25 358 33 468 1644 1][215 10 8 2 5862 105 483 167 0 3955]...]\n",
            "targets[[5 2 81 1641 11 193 751 2322 1 284 3412 25 56 1123 15 178 18 11 1525 21][3 2804 5 29 3 2 202 3 322 2651 92 33 4954 1042 8 0 3239 10 29 45][19 5 35 322 502 3 0 12278 3 303]...]\n",
            "targets[[3 60 30 57 519 98 43 2922 70 843 11 5215 5 5850 22 108 12519 1 23 30][210 21 39 2 378 3 10 607 19 61 5 5145 642 22 729 29 3 108 4453 22752][37 446 201 5 3704 22 30 2056 1 250]...]\n",
            "targets[[466 19 16 11819 7497 5941 33 6725 7929 99 7497 12 11445 320 294 369 5 2 17331 486][1373 1966 3 153 10018 13583 1 986 279 13464 1642 27 2770 4 1006 35 1373 17 144 7304][81 19 1 180 631 5148 16 27254 12 667]...]\n",
            "targets[[39 13 520 4035 94 389 1306 1597 33 2675 9569 6 6 1 147 70 892 4494 31044 2][19 13 2 453 3 1774 57 1 281 8 2 266 10 17 5 212 18 649 1614 0][229 0 250 7602 3 2684 8 61 8178 9459]...]\n",
            "targets[[473 6 6 9 693 9 207 112 3272 14659 1 1059 67 2 598 4 66 7 9 424][9 467 0 6720 9 874 4 350 10 29 132 7 13 2 53 2565 19 15 745 3][0 17 13 186 838 15 30 3974 7 13]...]\n",
            "targets[[3 127 2073 181 2019 1229 1 885 524 960 452 5 39 238 356 15 10 17 69 416][215 10 2645 154 602 22 6996 7 13 53 871 4 106 37 145 4 106 10 385 231][3345 2576 11083 4451 5960 1 40 425 21707 4978]...]\n",
            "targets[[8 0 311 5 157 502 3 0 611 3 2922 2363 4 7079 0 3964 3 5435 3364 36][2464 99 0 19 16 2 3713 2 15 0 153 8 0 1943 11 24 96 1283 0 19][1591 10 17 532 7 13 164 4 28 2]...]\n",
            "targets[[1046 6596 16655 5 2 1930 4 106 58 15 0 1387 2174 743 54 12 358 54 515 3609][1585 45 161 4 80 15 0 991 9837 254 8 0 19 8 189 32 148 180 43580 15][84 10 465 38 35 744 19 15 2 49]...]\n",
            "targets[[103 295 13 180 671 15 10 832 827 487 16 29 151 7 13 513 33 1956 3718 157][3 0 250 98 9 27 123 110 70 785 22 532 11 7 203 76 126 696 4 7][0 117 3 0 1676 249 959 2183 992 1]...]\n",
            "targets[[159 21 121 47 4 546 37 9 103 7 13 2 171 126 23 256 24976 73 89 21][5 284 9695 12 5010 184 680 17 43 2 522 3 668 15470 13932 3080 4498 3926 9600 4022][728 9247 520 3406 14 3937 26404 5 18072 154]...]\n",
            "targets[[10 17 45 108 1513 7 5 8 189 2 247 1820 17 1574 1932 8704 294 26 1001 12][113 215 320 476 40 51 7 84 211 44 85 3 2 738 9 67 353 1295 0 738][47 2 17 9 140 23 31 30 2 340]...]\n",
            "targets[[134 59 20 65 106 10 17 46 20 709 21 2 340 3 30581 34934 10 17 83 93][68 4592 22 0 1029 674 528 38 0 117 975 17 123 92 39 5 56 95 8 574][65 509 10 422 318 0 2726 7924 1388 6794]...]\n",
            "targets[[19 1470 3 3343 2395 12 298 5 2 480 910 39 12 56 145 3026 41 1269 18 0][17 45 2 742 3 74 156 1 0 109 27 2 742 3 1463 0 129 5 1157 1][5 2 145 4161 9 50 103 3 161 4]...]\n",
            "targets[[227 2504 5 0 1507 338 368 869 0 757 3 9783 2781 6454 4936 5 930 8 82 104][2 3054 3643 661 49 596 0 26522 2 173 2497 214 11 59 28 334 1343 11 149 10][140 23 2 669 340 3 3661 18 10 2879]...]\n",
            "targets[[55 15 2 11782 22 47 45 551 37 229 4184 5515 8836 747 15 0 271 3 4184 3471][242 2245 4 66 30 0 1153 798 22 10 17 9 242 2 679 1932 340 1 9 50][17 13 497 32 67 108 12899 5563 8 0]...]\n",
            "targets[[612 3417 1 21451 65 195 4 71 1 65 67 161 4 80 15 0 63 7 12 137][122 5 3920 7 926 1387 2174 848 14 69 14 1387 2174 503 147 9 121 10 122 241][43446 22 246 16 5083 8 0 3392 1 94]...]\n",
            "targets[[38 88 3 7730 12 98 0 2229 411 3 4412 3573 5 2 589 0 2229 4789 3 2][27 329 898 16 154 4 168 55 98 9 27 113 77 5330 4 949 43 29 18 9][0 1416 3 26 115 3159 120 184 201 0]...]\n",
            "targets[[156 74 1468 5888 743 1909 101 18 94 175 7 13 2 74 834 8 0 84 265 634][49 546 16 0 271 61 13 2 669 1446 6 6 0 225 13 53 49 14 13 0][6034 202 3 2299 762 5 1232 2814 4 0]...]\n",
            "targets[[1 60 812 215 0 5722 227 311 7 13 37 49 70 68 5931 16 0 217 151 17029][13 48 523 1394 1 361 166 584 1975 141 27 252 12763 31 219 24 159 21 27 35][19 5 33 29 3 0 250 98 9 27]...]\n",
            "targets[[5 2 903 81 177 134 709 21 131 75 22 258 5254 30 0 57 335 1024 0 444][1 6052 2466 4 28 2 847 725 36 0 3189 184 377 3 360 341 1116 3661 58 439][1985 1410 10 19 4 58 106 7 12 23]...]\n",
            "targets[[2909 1 520 5960 1357 10 5 241 2 65 1094 69 92 680 18 7 210 21 31 30][9 1018 2 3147 3 0 412 9 196 25 70 164 4 76 157 350 245 2752 1224 18][89 21 121 3 100 82 1802 19 167 0]...]\n",
            "targets[[9 89 21 121 47 12 2646 10 2799 984 41 0 798 22 10 1904 972 12 738 67][225 19 5 42 1043 74 1 2268 16 30 1856 0 153 6603 30314 2898 16 1481 44 2][6310 304 0 172 3 35 1171 1773 11 5]...]\n",
            "targets[[5 2 2271 17 0 1080 6613 1319 5 630 3 268 7 5 6 6 37 259 4 22479][17 12 3615 274 2 81 460 2421 15 91 1637 1958 867 1323 51 9 6258 0 797 9][89 21 387 587 3 10 162 100 266 17202]...]\n",
            "targets[[5 29 3 146 115 104 11 29 635 22 0 5392 24245 9 254 7 11 95 0 277][2037 25 254 8 160 728 484 5 39 2 5259 69 32 30 1103 6621 588 99 29052 78][141 28 21541 16 2853 4 12212 29 3 0]...]\n",
            "targets[[5 2 81 17 18 39 96 28 52 43 11279 1388 39 141 28 52 135 3 47 32][96 53292 3 3598 3 6847 24193 2054 1064 42 535 2 4352 7 6 6 46 7 67 2305][80 23 353 100 1037 46 20 723 21 110]...]\n",
            "targets[[0 189 11 7 13 29 3 0 173 98 11 9 123 5567 2 3292 119 7594 7594 13634][35 6345 340 3 1140 1 407 664 104 9 215 56 5753 8 1716 4 106 10 19 9][5 0 441 3 609 19 11 529 1871 104]...]\n",
            "targets[[5 35 221 4093 734 1855 203 66 16 30 2220 18 0 646 2405 339 5 23 97 49][10428 4709 304 62993 476 2 336 1068 2181 16 2917 1609 1 1925 13302 42 99 62 7290 1002][2 3879 23028 181 1352 9 254 10 19 4]...]\n",
            "targets[[5 213 2246 51 2 1141 11656 55 2 411 1 0 1714 5 802 4 298 8 10 411][872 17 3 30 57 69 241 23 18 426 0 88 6984 22 60 114 143 8 0 1752][0 1022 8 10 738 4385 0 109 32 83]...]\n",
            "targets[[9 198 10 2 81 678 3875 85 7 65 1784 2 171 3 3867 11 9 1317 367 0][3069 45 211 44 15 35 7047 3 13142 2519 233 3961 12 0 10974 1 587 233 94 27][27 110 440 246 19 4957 3 10 63 1]...]\n",
            "targets[[88 104 92 8 338 25 10 19 67 2 765 2513 3 1088 3703 4 7 58 37 9][66 104 38 10 51 0 1893 58804 2 403 16 6517 1 1160 4 1722 17359 70 66 0][1217 5 35 2475 11805 16 1976 112 10282 6]...]\n",
            "targets[[17 1072 71 36 0 1940 194 6207 587 3 0 342 8 0 378 25 4698 1604 1 18975][5 446 1452 44053 8 82 6056 3 0 19 9 103 11 24 203 28 4625 44053 0 1137][215 15383 12 1969 23 4 194 602 16 0]...]\n",
            "targets[[69 408 17 3 0 1582 12 4 10 249 10 17 83 1017 127 659 58 152 20 83][1022 6 6 8 160 728 1287 27001 31241 6024 5 6407 11 40 568 0 38145 1841 27001 284][366 10 5 607 264 48 3 0 657 204]...]\n",
            "targets[[69 9 139 110 2 171 447 72 10 7 12 6161 91 64 3093 220 25 0 275 1549][17777 429 9 50 138 143 4 6813 82 98 233 10 11656 55 60 2412 20299 399 14 2][0 7935 159 21 80 685 58 8 0 129]...]\n",
            "targets[[51 20 196 10 19 202 420 21 93 334 266 128 269 0 19665 1294 4 122 178 87][5 2 6349 28884 17 11 515 15 48 81 181 375 180 49 1032 1 48 201 97 0][7 4 6669 11900 4 599 0 2988 15655 844]...]\n",
            "targets[[5 2 822 19 16 30 2220 7 45 1204 1 79 45 0 504 1246 4 93 20 1535][1337 33 11540 415 34 4584 90 22 7556 2005 31 40 2075 6 6 0 7835 25 53 49][6 287 71 380 20 8475 5 2 640 11]...]\n",
            "targets[[4559 5352 752 8 0 4261 5 4442 2 29052 2147 15 26 231 51 26 1426 291 158 435][29 5758 303 540 36 2 2276 6573 19 18 10 29 5 29 3 26 12533 12868 0 1069][105 454 350 4 2692 0 1249 11 32 25]...]\n",
            "targets[[304 9 50 66 7 14 2 858 304 14 69 14 35 6494 2967 19 1527 15 16380 10357][69043 4294 22 260 5 38 149 2 1178 3467 64218 6518 1362 78 2 432 3 30509 4294 12][3949 45 2 95 3 951 547 43 536 208]...]\n",
            "targets[[3340 3015 3 0 10446 0 1465 1724 637 43 0 13144 451 3 4821 10446 8 17170 13 645][35 1857 1 608 33331 5521 3 0 368 673 9 80 656 75 59 535 8280 43 15 1935][1178 6676 6586 10168 480 4679 8 0 1068 3]...]\n",
            "targets[[2736 4 106 10 19 31 21041 427 22 8093 20946 1 1042 6403 1 254 7 4 28 29][1394 5 323 0 156 25 1604 1 62 101 27 375 3 573 9 509 0 84 315 541][17 9 856 2 49 436 18 7 13 73]...]\n",
            "targets[[242 37 1298 3782 672 391 131 14 9 67 64 110 0 178 339 1 2993 22 6721 357][3 0 9702 5 33 229 29 3 0 88 10081 1 2678 633 1409 123 1016 7 210 21][131 98 1 6653 42579 0 82 806 22 0]...]\n",
            "targets[[42 195 226 149 10 34616 17 46 11 12 47 20 50 651 7 7 562 4 28 29][0 501 173 154 9 27 35305 3 0 184 477 30 3 0 1187 104 27 77 1213 119][1591 10 19 4120 649 696 4 0 49 261]...]\n",
            "targets[[0 13 194 1 2212 2 173 2887 2565 135 1 23 73 332 1697 36 2340 2 115 705][1535 6779 79 92 8 8165 18 15 2 385 177 1 2 6043 270 10 5 2 19 43][17 5 42 1043 658 221 174 130 45 48]...]\n",
            "targets[[15 0 474 11498 2 125 1109 26 582 12 461 714 11735 4 2491 111 2881 1 9 382][3 0 742 37 229 5845 4 0 459 6603 2909 4957 2732 729 5 804 9400 4 12313 4302][45 4 28 29 3 0 88 43937 1000 98]...]\n",
            "targets[[38 0 321 514 0 109 18 0 2368 5 394 4 198 35 502 0 289 102 4693 26][4829 20878 18434 3447 5433 1 31982 11141 314 8 0 4116 3 831 1621 2 7627 201 427 22][204 4141 14 29 3 0 360 753 8 0]...]\n",
            "targets[[673 5 0 14633 29303 8124 18 0 12038 1 21717 3 9179 20296 5 229 52 72 11 7][5468 280 286 195 62 355 292 1 874 4 93 2 17 0 442 19 5 2538 1374 2][140 2 340 3 7649 98 18 21946 9 27]...]\n",
            "targets[[323 63 3 12678 5 408 33 33 2959 17228 543 3 50919 1 7 12 65 2 49 63][103 114 3216 5 2 81 17 7 12 97 74 11 7 5 42 2 246 17 1 83][109 3 1504 37215 17084 58 11 212 0 17]...]\n",
            "targets[[140 213 2 115 222 4135 3 92 16 246 98 18 10 5 2 49 29 7 12 35][125 12 6070 1150 1894 526 3183 5 0 15749 109 8 10 19 18 0 3113 2018 3 417][29952 3 2 114 1070 4 1624 3067 4 0]...]\n",
            "targets[[497 9 92 0 1414 3 3657 55 10 17 31 60 711 2255 1045 427 22 91 1029 624][481 9 13 3814 4 10 19 193 85 3 0 500 3 0 63 1 0 986 279 37][16 1408 1984 2411 575 117 434 1 117 877]...]\n",
            "targets[[139 226 48 166 8 12983 37 9 67 35 2906 1754 11 12983 25 2 176 16688 506 15][0 412 283 8 1138 741 4 560 127 1352 5 35468 41 39748 36 157 2586 2275 6 6][72 318 2983 11203 304 292 241 16 0 53]...]\n",
            "targets[[2 574 161 50 28 250 72 149 10 17 9 113 693 11 131 2282 36 574 34 92][1544 43 30 11 5 10 151 5 565 0 156 27 1246 32 42 89 21 306 8014 4][140 23 251 47 3326 71 78 0 443 4]...]\n",
            "targets[[5 2 871 17 4 690 43 9 89 21 27 100 5242 3 60 1440 43 0 434 18][139 110 2 171 3 104 18 10 5 710 0 29 15 0 30 57 250 405 79 3187][143 8 377 51 20 59 76 2 2212 10390]...]\n",
            "targets[[109 3 10 394 19 5 37 4220 9 139 278 0 1250 1831 55 85 9 140 7140 46][17 13 352 97 862 1 686 60 595 1648 696 4 1081 1 6549 646 2283 41 0 75][65 470 4 38 10 19 7 67 894 5819]...]\n",
            "targets[[233 9 254 44 43 773 885 2813 2455 9 67 77 1716 4 66 7 18 9 113 118][467 0 1734 246 202 1 13 65 261 931 4 10 18 32 793 4 80 97 73 6][7 41 23 70 139 110 104 38 10 29]...]\n",
            "targets[[242 2 200 340 3 11686 9565 12 98 257 0 2622 10 17 5 2 172 3 60 355][0 125 1 26 989 1564 0 8607 1911 5482 237 132 23 14 9227 14 26 11370 479 8][13 0 250 17 9 215 31 56191 1 7]...]\n",
            "targets[[5 2 81 637 19 100 340 3 508 4486 141 197 2 1005 3 10 1373 19 153 1917][8470 5 8403 15 2 13860 3 21923 1 1497 8621 187 385 423 161 188 4 3919 0 2958][20 38 7 41 23 10 5 2 7514 8]...]\n",
            "targets[[5 554 15815 520 2964 5 2 394 279 767 18 7 12 297 9 159 21 58 169 86][15201 132 3163 2 20221 8 35 158 1280 1838 5 2776 33 0 1474 1 18319 51 24 5373][215 10 19 227 311 8 2 4285 2683 9]...]\n",
            "targets[[37 11 13 2 838 721 2843 18 7 150 21 517 85 10 17 1784 0 9957 186 245][9 159 21 27 303 6443 18 196 9 207 31 219 367 12188 3193 9 38 42 43 238][322 17 111 283 5 323 36 0 13366 5690]...]\n",
            "targets[[12 2 49 1080 1024 7 12 2 4162 3 1592 1939 12 1185 503 1966 1 2 3354 298][2127 223 162 0 5325 276 2127 168 38 35 1984 1465 1724 19 9 1653 10 17 21111 22937][2 3503 3 2119 7052 1847 10 487 5 6]...]\n",
            "targets[[139 64 545 4 1658 6 6 51 2 19 6152 11 7 45 2 522 3 8508 381 303][23 1863 4 238 11 286 34 45 358 10 17 2 1153 738 45 4 136 32 25 3235][397 239 157 17 43 764 33 286 34 45]...]\n",
            "targets[[368 36 0 125 34 529 178 0 21285 6384 8 9734 30644 0 63 1109 10670 39111 520 3406][17 5 2 737 502 3 137 9 66 2 171 8 1855 2 9548 3 1705 7481 292 33][17587 2030 199 10 19 1 0 84 7528 5]...]\n",
            "targets[[74 225 74 156 1 2 1795 8 6661 25 52 2058 11 0 178 1341 98 3 0 3392][31293 13 6905 4 314 8 105 104 11 68 14 1679 14 0 20593 1744 0 6505 1 10][239 157 354 26649 19 11 150 21 749 55]...]\n",
            "targets[[5162 5 2 1000 19 9 50 21 136 7 100 44035 94 11 7 302 265 99 2 2623][3 0 74 829 22 10 4183 1658 2455 3 0 578 330 16 29 41 52 3 0 1057][246 3586 2208 0 109 3 7237 4333 14 1323]...]\n",
            "targets[[9 242 142 2 706 245 8373 340 9 140 2009 5334 10 0 117 1855 17 123 190 9][21 121 47 30 0 3474 13 43 30 8 30 9 4105 43 3119 60 57 149 10 119][1115 416 157 29 6 6 132 22 35 11553]...]\n",
            "targets[[1773 0 981 5 23 0 250 17 123 92 56 11 2823 59 27 4 138 4 2 19][0 460 8509 1 13026 1586 0 105 82 104 33 8462 15788 11 9 139 110 0 153 10721][3 50924 4608 732 4 191 265 8 2 21045]...]\n",
            "targets[[425 1197 71 10 378 1 9 203 136 7 5 775 0 12847 179 9 27 1059 110 9][353 0 298 2 1307 167 9 307 10 92 16 246 17 0 5924 141 27 2815 71 47][17 42 968 97 1043 379 9 50 21 58]...]\n",
            "targets[[1530 366 15 7342 10 5 23 145 7 5 2 246 17 15 156 427 22 47 13 1003][50 21 262 9 453 60 57 149 10 1243 9 118 85 4648 10730 529 7 35 21127 678][452 45 213 77 29 3 60 519 458 202]...]\n",
            "targets[[72 29 153 45 1715 11 0 414 4300 95 3 37872 127 101 4 1624 807 5 4 278][12 23 2 666 631 561 8 10 348 2716 69 92 3054 2908 1543 790 5 35 41316 2356][4137 5634 282 5146 4 31516 12 40914 14 11]...]\n",
            "targets[[13 65 671 15 10 17 7 13 2 14217 18481 225 354 22 102 954 1 194 22 1346][10 19 13 38 19780 0 2730 9 67 8 60 844 12 1073 16 463 629 4175 261 3281][0 314 3 0 19 8 1315 3 84 5002]...]\n",
            "targets[[25 658 104 65 658 104 1 39 5 2419 531 7 5 95 97 194 39 5 56 237][120 0 3336 9 2021 0 170 151 157 6438 5286 0 43632 10025 1289 12 9628 0 455 1141][10 298 4779 2814 4 0 298 94 70 50]...]\n",
            "targets[[67 0 1718 3 2 53401 3082 68473 549 3 2 246 2299 202 3 60 1700 34 67 400][10406 1 245 3357 0 492 3 0 116 8567 1 444 1482 292 4 1006 2 2536 2511 436][20 1632 16 1328 5 192 4 93 255 182]...]\n",
            "targets[[321 5 2 53 1228 412 0 19 45 2 613 3049 8 3761 230 4 7 18 7 5][763 339 3 54572 27459 12 1556 7 12 43 2 112 1620 199 323 24905 39983 8804 25226 1][215 10 17 22 735 1048 797 51 7 13]...]\n",
            "targets[[71 15347 13 2 1446 9 856 52 3 2 1543 3985 19 264 30 81 156 27 92 2947][109 12 4368 65 7 2525 0 1269 61 5 4 1624 2 11358 16 0 769 1785 6 6][3 30 10 17 210 21 2 605 1521 46]...]\n",
            "targets[[5 437 0 250 19 9 27 123 110 8 60 114 14 229 14 9 242 1856 7 274][233 0 84 57 9 215 10 17 9 27 467 7 58 1078 7 13 2 360 341 17][5 35 7692 774 565 17 2483 311 30 0]...]\n",
            "targets[[1397 16 0 44577 10 17 141 28 2618 816 16 701 1 21811 3 100 4677 31 100 616][1066 88 3 2 2040 578 8 0 75 12 7642 3 2562 9 230 11 10 17 406 35][33 147 70 30 121 11 0 217 16485 694]...]\n",
            "targets[[250 1 8984 370 3 75 34 65 4231 4 121 126 0 177 27 1408 4076 199 90 7][285 2 243 34 1366 747 4 138 1192 51 1063 8050 1970 55 15 40 54 5266 945 4172][70 65 365 100 52 10604 1243 22 0 941]...]\n",
            "targets[[42 307 60 3097 8629 175 22 3685 9 50 23 1572 87 108 214 9 27 110 7 9][36 160 728 5 2 17 11 113 2012 7 12 1438 56 517 87 108 214 20 66 0][12 212 4 857 256 353 48 3 0 798]...]\n",
            "targets[[5 241 60 1628 869 848 9 84 307 7 85 7 13 36 0 5000 3 0 5916 1][20 367 2917 6385 98 10 29 5 2 203 40 1215 5 81 14 69 14 40 116 6][112 461 9124 98 32 25 81 247 4 106]...]\n",
            "targets[[27 64 110 422 105 61 13 1933 14 0 354 22 0 28500 422 111 32 307 0 13931][182 4 366 33 7342 9 242 2 9237 58 152 9 89 21 1046 15 2 171 3 0][13 53 671 15 2804 2126 9 27 77 35]...]\n",
            "targets[[646 5 23 53 49 1 9 140 767 16 11 18 10 17 5 29 3 0 8714 9][27 2 619 425 34 5 3999 107 1435 78 2 620 15 35 14721 34 45 2 194 2609][27 307 10 17 1 8 0 84 163 228]...]\n",
            "targets[[1156 9964 19679 12754 1 7190 13498 1 513 33 4976 20663 5 2 81 184 19 15 745 3][215 10 19 440 154 602 1704 2487 1 9 1147 0 271 3 0 17 47 551 99 54][178 1253 12 2554 963 3025 5 2375 1 0]...]\n",
            "targets[[14362 17 650 246 768 8 2348 499 120 30 18355 393 8 0 653 904 61 5 81 23][215 10 22 3857 1 67 353 8 440 738 1177 11 10 13 2 1931 984 3 35 881][27 2260 55 884 12314 23329 193 0 3707 1]...]\n",
            "targets[[21 138 66 10 19 31 219 89 21 46 20 148 2 527 41 46 20 424 0 84][19 5 2 114 46934 2571 3 29 125 12 1068 3 26 466 9043 4 0 75 1265 1][7593 203 66 46 20 25 35 2362 340 38]...]\n",
            "targets[[75 122 9 140 1125 1 27 77 149 0 122 233 9 13 1750 7 1646 60 2407 889][19 45 49 101 15 322 347 36 0 177 577 14902 5 23250 4640 14 0 493 21262 7338][9 387 0 1359 3 11188 729 9 113 2755]...]\n",
            "targets[[25 105 179 11 749 44 8 10 19 0 2810 125 252 33 5019 14570 1 0 31213 11930][8 30 10 5 241 8 60 351 463 1177 654 17 9 437 467 0 298 51 9 353][27 110 10 119 163 214 8 463 483 17]...]\n",
            "targets[[7184 285 13293 17005 2233 3699 1 52 72 2 115 8340 294 0 370 2032 416 17005 5 2][60 568 1 9 418 4 66 10 17 70 2736 7 3002 85 2 7 288 21 29 3][1293 63 2982 22 0 935 3 14622 3 49206]...]\n",
            "targets[[187 0 2511 1720 3 1047 30862 0 1535 2107 15 5100 34 10662 62 423 85 2 1325 702][1264 17 123 7 13 2 49 321 15 49 35600 839 2097 35600 67 437 161 4 80 0][47 0 1104 3 10 3920 1470 68 259 4]...]\n",
            "targets[[12 0 250 17 20 139 123 110 8 127 114 42 2682 143 1 103 43 7 80 20][17 5 8366 37 360 7 6462 0 1648 3 30 4415 87 374 80 0 1155 3 10 17][5 2 53 49 458 61 971 48 160 756]...]\n",
            "targets[[242 2 245 1960 3591 18 9 96 23 749 4576 933 15 0 1727 2 69 1202 18 1089][3823 16 0 1558 3 10 19 14 2 8233 9 13 23 1824 11 39 13 20191 8 0][6 650 959 45 0 2157 1 0 281 4]...]\n",
            "targets[[45 65 226 7 0 24423 5 35 181 17 31 7 117 7 45 283 35 181 487 784][215 21600 983 22 1901 29 311 9 196 0 17 13 49 0 101 68 212 1 790 13][9 67 303 1943 16 10 17 538 7 5]...]\n",
            "targets[[2 1157 4601 10 5 4 1922 3229 2080 2312 1838 8208 1 1832 1882 0 1120 68 335 22230][10 17 0 2099 13290 8 45 2 81 3880 2 105 45759 36 0 3239 32 89 21 93][17 406 30 7 45 4 198 8 0 84]...]\n",
            "targets[[188 4 27 2 6622 3 259 3856 4 5313 26 779 10 19 1905 15 7471 1 32313 3773][112 0 5045 3 39727 3926 646 2289 39700 1 3412 26586 25 30 822 14 62 5572 101 60][42512 13 35 7388 102 4558 14 69 14 8]...]\n",
            "targets[[2908 8 0 2888 184 2621 11 6706 0 8158 3 127 711 378 1045 0 1250 11222 7407 1278][9 42 2065 11 39 5 2 122 52 2338 1 1618 72 115 2838 1 9 38 7 0][397 19 33 2264 1 16969 622 166 9 147]...]\n",
            "targets[[9 50 387 53 69 134 108 75 59 722 10 17 30 3 0 101 352 1704 384 84][242 147 2441 14462 13012 5 29 3 764 12 88 13722 3278 1035 58687 13 49 0 117 61951][2 81 17 33 15515 20850 15 4831 0 117]...]\n",
            "targets[[2 81 279 4 27 8 142 35 379 63 6 6 0 19 1 91 369 190 5 180][425 3 1867 529 71 10 17 2 425 3 1867 5 147 8 2 1511 68 2 739 3][1900 307 10 17 85 9 467 149 20905 17605]...]\n",
            "targets[[618 604 103 3 134 255 59 198 10 17 2 163 163 251 70 30 27 263 1355 8][5284 1832 1882 36180 13 969 154 158 97 158 16 0 172 2 17248 5240 1175 34 59 93][1022 1404 6 6 9 140 503 60 84 738]...]\n",
            "targets[[17 65 3796 1911 17415 5 1349 8 0 19 26 960 1136 24 5 142 2 688 1630 1743][76 2 1992 44 3 0 108 101 22 0 122 469 2750 25587 604 345 18 712 26 894][12 23 42 11 10 5 2 74 17 7]...]\n",
            "targets[[179 28 58826 323 208 107 81 11 5 0 1565 61 9 242 2650 128 144 2 17 9589][64 82 19 1374 11279 1388 11 45 142 35 922 3 12393 5 22 0 1934 193 104 845][721 823 2908 8 0 202 2 858 7911 0]...]\n",
            "targets[[48 3 0 116 5 2014 0 17 1203 702 3 0 1735 3 2 81 5203 4 28 555][50 350 4 28 10308 9 50 350 4 28 1799 9 50 350 4 28 155 30 3 131][580 459 2190 44 3 163 6 6 0 17]...]\n",
            "targets[[215 160 728 9 112 20 490 1 467 7 9 13 65 261 931 4 318 10 99 149][288 21 2 340 3 318 10 17 31 30 18 51 60 19653 446 71 1 300 54 67][16810 16 0 12189 16810 16 0 17 7 12]...]\n",
            "targets[[103 11 862 185 8 0 9678 64955 172 3 62 485 295 1200 1625 7778 42 2 115 222][96 28 11 60 112 3 1116 928 104 8 746 406 71 2 7221 18 9 993 10 19][5 270 9 121 43 35355 1 4918 18 56]...]\n",
            "targets[[137 1749 1 297 5 4247 1237 87 4 1176 7 1 1481 7 292 208 235 7 6784 5][17 5 427 22 7740 117 10934 33 45442 38311 1 13 513 33 24172 193 322 241 83 28][141 28 542 167 9 366 11 9 140 23]...]\n",
            "targets[[5 37 504 9 42 467 174 670 54 45 123 226 3386 1 6970 5320 12 6123 460 4924][12 37 73 356 15 10 17 9 1184 121 111 4 366 84 431 56 870 9 80 121][267 1990 356 8 10 7978 2510 3 1243 9]...]\n",
            "targets[[50 23 262 32 446 10 8438 223 118 0 153 58 1392 4 106 0 84 8438 10 19][32806 99 39897 389 143 15 15317 8 39897 61 13 3526 99 39897 6 6 0 19 5 2][591 10 204 28 3856 911 18 7 12 30]...]\n",
            "targets[[5 404 64 99 154 1372 11 70 50 168 143 1 66 146 399 34 25 371 399 14][140 23 1085 251 47 4 93 3 10 19 9 1725 9 365 4 667 7 31 219 282][307 3571 691 52 214 11 30 60 355 278]...]\n",
            "targets[[7 16 30 0 996 1067 8 0 82 11566 0 116 13 74 0 109 934 3169 292 87][47 9 67 555 1 353 3 10 17 9 13 248 1027 4 169 2 3074 63 3 1668][5582 246 17 3 0 1325 13 2 1659 16]...]\n",
            "targets[[653 717 5 2 2102 2 2382 2925 199 1306 1 0 1378 96 27 1435 178 78 0 17][3199 1348 115 17 4 28 251 18 2 4083 2967 29 9088 6426 3198 8 10 407 4848 16][13 133 2 1178 1 67 401 23 400 26]...]\n",
            "targets[[0 7343 1707 1602 550 105 10368 3 0 95 78 0 17 9 389 16 0 1048 10 186][120 7243 5 2 69 326 19 11 188 2 222 1182 22 0 39607 31 214 0 17 747][9 215 5924 3 10 17 9 196 11 7]...]\n",
            "targets[[50 3213 2 363 3 454 259 4 2131 8 1 1165 22 0 27025 3 2 155 975 17][20 148 2 80 106 0 17 82 72 11 9 59 113 1478 4 255 11 32 141 106][17 5 53 155 38 51 11 37909 1773 3334]...]\n",
            "targets[[1 2471 4 60 84 738 287 71 366 33 674 11 0 17 5 23 273 149 925 20][99 745 3 245 166 2916 207 2 1166 13844 45 2 49 411 433 572 9584 200 5709 2689][75 136 32 112 286 11 32 112 62 231]...]\n",
            "targets[[255 332 1327 11 1882 1769 13 23 165 22 1599 8 6634 15 0 372 3 0 177 30][37 24 12 23 2 414 2985 33 0 129 3 0 17 70 80 854 42 2 115 43][5 53 1247 16 2 19 4 1359 4 779]...]\n",
            "targets[[9 215 10 22 1901 9 10431 7 13 36 0 4383 0 848 5 2 222 7258 1 0][46 0 19 68 23 3 1053 8 392 10 5 35 322 95 4 76 35 15316 3 0][5 42 29 670 16 10 19 3920 0 153]...]\n",
            "targets[[5 2 641 734 201 43 2 703 276 363 3138 105 13002 483 578 15 40 701 8 1547][286 83 191 105 3384 156 1 93 2 447 434 0 297 1641 3 10 19 5 11 7][5 3979 427 22 0 990 3 0 206 1001]...]\n",
            "targets[[217 738 5 2 1250 18 46 39 123 13 2 17 11 932 7363 10 13 7 9 96][76 65 1520 3 98 38 739 764 18 10 969 291 158 17 150 21 58 198 791 16][103 11 580 12 1270 39 13 137 13762 16]...]\n",
            "targets[[329 4 28 294 0 19891 3 4275 11 7 1003 2 2855 16 1516 183 1967 20 865 143][0 28137 13 2 81 122 9 13 1408 154 158 51 7 3319 60 1231 59 106 7 15][1386 5 2 360 341 276 184 17 11 805]...]\n",
            "targets[[20 25 2 200 340 3 11631 10764 20 236 58 367 10 17 873 89 21 1143 127 57][243 113 2767 664 473 0 17 54 17039 174 344 1 2152 30 208 2 222 3 1242 1487][50 21 262 87 1153 0 829 16 10 17]...]\n",
            "targets[[17 5 35 1121 1 1262 6204 667 3 1560 1087 1 0 1104 9351 9 13 1520 15 0][3 60 30 57 1628 104 123 42 323 370 3 417 1487 2457 1204 1648 0 63 3124 14][12647 2591 5371 8 0 2304 5 2 1224 1056]...]\n",
            "targets[[2 17 11 499 15 0 19430 3 105 4073 2187 50 21 28 30 74 6 6 0 834][122 123 0 101 25 1149 60 2753 25 11318 0 1868 1 8611 0 2293 1212 18 46325 5][11 562 4 10 231 32 1810 0 217 189]...]\n",
            "targets[[653 16695 319 71 43173 15 2807 31 62 18414 0 5999 339 3 0 31025 9653 0 18101 15052][84 1826 10 17 150 21 168 38 2 880 81 29 99 30 2 2917 1609 98 15 64][31505 13 165 0 229 52 3428 22675 3 268]...]\n",
            "targets[[84 3 0 5715 895 1987 5 2 222 263 72 0 82 104 3 0 202 18 7 12][10 17 5 226 15 2 56 390 177 0 116 5 351 2490 0 156 141 28 358 2411][13 29 3 0 19303 55 16 0 310 1465]...]\n",
            "targets[[2 453 3 57 1 0 29 314 9 529 13 16 0 1561 7 13 2 53 49 1561][5 29 3 1669 12 117 104 3 7366 1 397 14 2 2939 1019 124 0 19 1443 18911][70 865 144 720 228 3 379 675 207 743]...]\n",
            "targets[[3246 104 930 0 11401 3 259 4 783 1181 31 0 2287 3 3315 388 4 706 41 58][13 73 73 126 72 9 196 7 13 164 4 28 4204 155 1 23 736 8 0 95][3457 15 10 17 141 28 5349 4 29 3]...]\n",
            "targets[[307 10 17 227 311 22 29 3 0 957 3136 667 4278 1 132 149 7 9 927 3580][243 20748 3906 2 3043 29 2015 31 4728 4900 11 2 2931 45 3000 0 75 8 300 3043][45 12684 60 1356 4627 9 282 196 11 31425]...]\n",
            "targets[[3 0 117 201 202 4 123 211 44 3 2838 947 38868 31141 58005 1 1255 34097 25 1349][1046 15 30 0 12410 9 418 144 2 889 3 17508 149 10 19 7 67 2 2478 5438][8864 1 3173 1484 199 15252 1 2 7880 8]...]\n",
            "targets[[103 11 50 2731 55 10 122 43 14 69 14 238 1734 19906 204 28 0 250 151 4][165 67 303 1943 16 10 17 51 149 7 8 0 443 31 2 19 1322 8 20395 0][215 10 19 34907 22 15615 1 418 4 66]...]\n",
            "targets[[2661 218 4 304 29 3 26 1247 49 216 558 8 2 1479 427 718 0 194 621 1771][140 2009 5875 0 18804 128 2 222 1 136 9 509 10 190 0 1084 5 65 64 164][3 30 287 71 42 136 9 50 21 749]...]\n",
            "targets[[122 1597 0 5995 3 0 14638 2 1238 3 417 1161 75 622 1294 13 2481 37 32 68][2 394 17 0 116 13 74 0 1821 13 74 0 655 13 74 0 946 13 74 0][1018 10 3896 22 3685 1 9 207 38 4]...]\n",
            "targets[[203 28 6101 146 483 16 259 137 160 6 6 29 142 1115 61 13 1676 793 33 388][9 84 215 755 3 2 821 143 8 0 1289 12 22 729 9 67 56 321 10 13][7643 16 2 363 3 154 41 37 9 27]...]\n",
            "targets[[38 4 103 3 545 14 2 125 3 4651 2 21275 1 14345 11787 22 2757 5282 1 0][17 188 4 27 2 171 3 75 674 7 5 29 3 0 88 1758 3 30 57 99][729 274 38 4 9962 44 421 8981 1 80]...]\n",
            "targets[[5 37 73 11 50 28 300 43 10 19 7 5 23 127 737 16500 3 268 39 5][780 60270 6 6 0 153 1 543 3 9523 902 0 153 3 331 89 21 560 402 6][5 0 244 3 609 11 7286 4 0 10268]...]\n",
            "targets[[19 5 0 13574 3411 3 174 74 17 997 123 585 29 3 0 250 1 4463 3 30][13 53 671 8 0 63 344 3 10 19 0 177 118 35 5398 301 15 35 119 2674][12682 12682 12682 42 51 20 196 20 68 2295]...]\n",
            "targets[[1401 16 2 19 5 23 1316 3 0 390 3544 5381 758 10 31 30 2326 255 1716 4][17 13 437 379 0 17 724 78 18269 221 1240 1 42 418 4314 36 39 10 17 5][64 82 738 3 10 17 14 3 10 1309]...]\n",
            "targets[[67 4 28 29 3 0 250 104 123 51 2141 274 55 1 8058 5 15 2 742 3][17 719 0 17 2 1080 226 108 214 8 0 472 3 443 7 5 3428 128 14 69][9 59 38 4 563 60 12422 31 0 2007]...]\n",
            "targets[[1022 6 6 2 15449 1125 291 158 396 7745 2 53 1009 1125 291 158 234 1 852 24][23 110 10 19 8 43 844 154 9 242 133 1520 15 7 12 245 3357 1550 1 4321][27 7688 67 7688 4 37149 16 105 2688 0]...]\n",
            "targets[[5 606 47 9 118 132 149 10 17 9 307 0 4175 1375 7 59 129 525 69847 69847][30 10 528 164 185 31 0 561 15 13832 9 139 672 2938 4 26 224 149 0 1055][54 886 764 12 351 889 1025 314 33 391]...]\n",
            "targets[[1012 10424 5 143 15 74 46190 1 56 266 16 1847 99 89 21534 12 972 1541 1012 3][67 60 5242 43 157 112 63 9137 6992 3516 169 1202 1 3300 144 1146 6583 1 7 12][229 14 705 298 1470 138 10 29 5 44]...]\n",
            "targets[[488 5 2 81 181 17 15 81 156 81 109 1 2 450 11 65 515 35 181 19][7085 7983 17069 55 180 2 222 3 6926 51 10 19 13 84 107 645 88 1336 85 3][4 875 6 6 10 17 141 28 0 989]...]\n",
            "targets[[12 1746 1406 5 35 1623 441 3 184 8 1315 3 834 967 1 919 18 2770 15 35][3 0 88 348 98 9 139 110 0 684 535 181 5 630 1 1407 109 47 109 0][13 0 250 17 9 139 123 110 9 382]...]\n",
            "targets[[5 0 666 88 913 202 9 27 123 307 22 729 7578 1887 4247 1571 7 45 2464 15][10705 5 2 2319 842 7 45 193 2 583 63 1 48 1623 1356 2479 4 383 0 507][182 2 1212 17 41 58 2 4593 8937 1570]...]\n",
            "targets[[592 4 367 734 1256 1 144 0 154 27 110 48 81 692 1541 1292 186 243 4 390][1090 8674 1832 3269 14 2 10326 1130 11417 2141 20150 124 54 133 76 166 14 26 312 3199][139 793 4 38 10 19 65 8 149 7]...]\n",
            "targets[[371 230 74 16 0 177 888 11 13 571 8 235 10 434 47 96 27 77 2 1875][12 245 4 262 99 1073 2779 154 70 1968 55 15 10 432 3 1356 1243 0 206 13][11 0 105 897 798 16 10 19 25 36]...]\n",
            "targets[[29 3 146 98 111 30 3 0 2013 510 68 8 0 1561 475 736 1 65 23 53][25 37 108 683 9 182 4 359 4 1652 10 17 18 50 21 65 80 11 50 9][307 10 17 99 149 6662 1279 1 0 920]...]\n",
            "targets[[254 10 2 53 2278 19 8 11 0 361 2635 25 1616 208 1510 612 1 0 1127 5][188 4 28 29 3 146 202 11 27 77 400 8 0 34808 3 57 99 7 12 18023][3 0 117 3344 9 27 123 110 10 5]...]\n",
            "targets[[59 27 358 10 17 2 308 44 3 163 46 7 1189 21 16 1575 23528 12 237 1][206 17 13 23 0 117 17 123 18 7 13 2 1131 2707 18 0 662 5 2 37][1354 15294 26 158 1112 8 2 19 11 15294]...]\n",
            "targets[[184 19 11 13 92 36 2 4550 19 11 13 593 976 1 67 160 135 1296 7 45][2409 160 314 2153 1112 4 2 160 474 16 2 1424 366 99 2 1578 5292 1162 866 26][3 2823 45 108 81 1429 4 7 49 181]...]\n",
            "targets[[254 43 0 17 1463 33 2206 36 75 11 7 288 21 737 959 11 193 362 1 1532][27 37 108 355 34 112 10 17 9 139 555 1878 75 2222 7 14 29 3 62 2753][69 2034 143 8 4818 10 19 1886 2 73]...]\n",
            "targets[[2 4699 19 33 6962 55041 31 7 12 57 8 4818 51 142 2 3127 13 1587 44 3][268 20 3413 28 2 14400 4 367 48 75 12 1175 20 121 11 46 20 2045 15 90][529 10 2 678 3 796 399 6 6 9]...]\n",
            "targets[[8368 317 5 2 53 46422 17 8 189 29 3 0 250 9 27 123 110 9 509 0][109 2 3240 1278 45 0 6622 3 7731 26 8085 26 57575 1411 86 445 14253 1 24 525][171 3 75 465 4 27 424 0 19 37]...]\n",
            "targets[[3 0 14308 1536 1 1094 2272 1314 123 92 7 45 113 2034 0 397 659 7 18513 1020][42 215 38319 4120 22 7814 9 13 1986 278 120 33 0 412 18 0 17 5 323 1][42 1779 149 10 122 1 60 1637 5 133]...]\n",
            "targets[[84 555 2 1771 1470 36 0 3148 3 0 18123 24610 1 53972 11 353 0 298 9 196][1865 509 10 17 85 39 13 2 1749 7041 8 0 116 0 503 13 351 2490 601 35006][389 128 1 353 0 829 167 9 307 10]...]\n",
            "targets[[5 35 554 69 4409 69 408 69 896 1 69 92 19 0 405 8 193 91 2842 5977][604 262 30 9 140 884 43 10 17 269 36 75 34 2222 7 13 323 1 69 226][5 142 2 13121 839 896 408 1 513 15]...]\n",
            "targets[[215 10 19 31 0 13021 2004 19 1322 9 13 1184 482 4 783 4344 144 0 194 2212][2271 637 513 33 1882 41153 5 2 989 211 297 16 75 11 367 4 3143 11636 7791 7][140 770 11 255 571 15 0 369 3 10]...]\n",
            "targets[[246 17 339 3 2559 6953 12 368 276 63 33250 36 0 298 8 108 741 190 10 5][2 825 246 17 11 124 23 38113 35005 0 613 1708 3 1278 14237 16704 5 2873 1 0][47 5 10 5421 45 92 142 368 104 38]...]\n",
            "targets[[8424 33890 47 5 873 2 53 1562 17 17688 10131 12 407 2535 14 2 74 534 13 30819][227 4317 5 2 17 43 0 5420 3 1389 16 928 1087 52 3978 928 9489 18 4742 22][141 28 60 244 3 17 58 46 7 2144]...]\n",
            "targets[[84 353 4793 12 3102 12 4203 673 8 60 13951 1208 472 713 1 9 509 174 3186 1942][5 1003 4 28 2 680 18 0 310 1514 8 95 97 108 1265 31 0 12865 3 0][3 0 1606 5998 150 21 211 22 357 1783]...]\n",
            "targets[[324 3 0 34 12 4476 13023 4441 2032 13 748 1841 9 254 37 108 4032 199 7384 1518][19 5 43 2 4034 125 4 548 1 26 2284 8 2 95 3 10504 863 0 19 2544][175 153 1252 279 543 3152 4340 1528 33644 45]...]\n",
            "targets[[307 10 1593 10 249 1 94 65731 16 60 568 4 66 10 24 165 1514 12124 294 135][6813 0 8938 3 2145 1389 22 19 8 13107 643 16 316 472 3423 70 148 147 31 3780][249 311 8 0 451 3 35 12331 522 3]...]\n",
            "targets[[52 741 72 29 51 9 215 1579 223 3899 9 196 7 59 28 0 276 32 59 922][44 38 440 104 167 7 2666 24688 48055 76 127 528 1 0 1806 17 22860 2190 494 4][17 13 497 327 278 7 13 37 74 9]...]\n",
            "targets[[589 7672 1518 1505 21 92 2 19 8 2 291 1 2 315 85 9 300 37 1 6487][5 2 19 11 45 30 0 756 3 2 713 2 869 546 29 0 63 11 65 3032][41894 6065 0 289 102 3 0 41894 252 33]...]\n",
            "targets[[5 2 337 636 0 63 13 615 1 67 2 337 271 2 203 8 60 660 0 176][17 5 29 3 0 173 46 23 0 64 13380 369 11 9 50 230 2378 3 7 702][203 27 509 7 9 2464 55 357 223 242]...]\n",
            "targets[[5 35 379 19 9814 12780 1099 7 12 2088 2 1214 19 0 1419 621 1384 36 35 9657][10 487 22 227 17771 16 0 795 41 2919 57 509 7 37 73 9 5555 0 277 10][3 60 8080 1910 5 149 10 880 339 3]...]\n",
            "targets[[50392 12048 11697 5 0 84 6835 17 3 1178 9112 4522 1 1232 5 23 43 26343 25179 41][0 400 3170 13 283 0 5924 11402 7 59 28 7 5 23 404 20 169 11 88 3][20 182 4 66 2 17 43 179 11 83]...]\n",
            "targets[[50 21 387 134 10 17 13 113 278 22 277 41 31 219 378 9 723 21 58 110][171 3 75 89 21 103 5095 12 5143 19 5 30 11 49 18 9 203 987 9 103][20 121 51 2 19 45 20 1 5 413]...]\n",
            "targets[[105 5 248 3616 16 10 1 7 64 218 11 73 85 3 5637 1 2386 0 109 5][17 5 43 14 2139 14 557 65653 1693 4 13556 10 17 5 113 155 7 12 272 0][82 390 16 5858 15025 5 15380 4 1521 287]...]\n",
            "targets[[275 200 341 2598 427 794 3661 8 2 3431 9897 5700 538 92 2 4427 590 2101 4 3141][3233 45 92 0 88 53910 24124 5695 462 894 115 19 9 103 9 139 123 110 69 272][5921 1 966 15206 1810 73 126 819 72 47]...]\n",
            "targets[[1059 1918 0 84 7059 3935 2192 1568 36 60 3297 9 418 144 30 0 2354 84 546 0][402 4090 55 544 4 106 10 17 22 246 51 9 13 2 527 7 1788 0 574 44][242 2 81 340 3 0 1734 3707 1 9]...]\n",
            "targets[[1022 6 6 0 797 13 1050 15 75 1027 47 10 434 96 27 4039 77 46 358 52][597 75 597 46 20 1203 1239 10 2 49 17 137 5 6416 356 15 20 0 116 0][195 2 277 366 149 31 41619 1 1454 31]...]\n",
            "targets[[9 42 195 144 149 10 29 9 27 4 136 11 10 17 45 4 28 29 3 0][28450 611 5879 389 44 8 5247 1480 427 22 0 928 38194 729 122 11 672 143 8 0][10 17 13 23 606 708 5451 7 13 2]...]\n",
            "targets[[50 9 136 43 1426 7222 69 9 307 22 2 2766 36 8802 4 6015 1 14 11 2766][10 17 5 161 305 9 449 31 255 34 550 7 12 0 872 766 436 123 7 12][260 5674 5 2 557 1141 15 2 1539 8]...]\n",
            "targets[[1736 1 26 115 812 4027 25 3666 4 138 4 3784 12 265 16 9245 2218 18 17860 7976][12 237 221 92 1259 0 260 7 288 21 30 74 9 42 103 0 9865 209 13 356][723 21 239 353 0 3433 13599 298 10 13]...]\n",
            "targets[[12 871 4 15546 3 2 95 8 61 10 19 150 21 1981 28 7 0 503 0 116][60 412 968 2 115 11679 58 152 10 19 150 21 13900 15 2 669 341 1 1258 552][17 5 4203 7 702 0 63 3 451 634]...]\n",
            "targets[[18 10 17 5 394 7 6462 127 1648 31 174 479 1449 100 145 4328 1 9 6887 20][14454 0 2664 16 897 5238 12610 9 2191 596 131 798 134 25 32 30 37 613 7 12][435 34 4231 4 121 126 1591 10 36 7051]...]\n",
            "targets[[11 60 640 5 2 3512 3 0 1023 24551 12959 9 481 9 121 2 171 52 43 11][2731 55 0 109 3 13458 451 8 2 363 3 7351 13458 5 2 963 1376 15 2 1693][1845 15 959 12 69 542 4153 3 3062 9]...]\n",
            "targets[[1170 1430 1022 37 89 21 15622 185 97 229 97 927 46 20 89 21 182 4 121 47][5 33 229 29 3 0 250 98 9 27 123 110 0 336 305 290 349 15 0 336][27 307 4020 1236 16 52 72 2645 154 1]...]\n",
            "targets[[96 1184 383 545 36 352 15504 120 41 42 1559 120 10 7408 18 9 874 4 1335 7][1262 348 42 85 10 19 5 263 36 0 648 184 1412 9 89 21 121 87 255 96][371 81 92 16 246 98 8 60 660 30]...]\n",
            "targets[[67 0 1421 4 106 10 17 33 801 3 2 1912 11 9 27 1918 8 3815 1 9][203 5751 4 3995 1370 817 98 18 10 19 5 65 42 1043 379 74 305 290 58 447][5 2 81 17 16 30 2220 91 0 63]...]\n",
            "targets[[35 6330 842 7 13 2 2069 2269 1 99 2 194 752 1307 9 4098 545 22 0 9763][200 599 8 13702 317 2779 13702 30447 55116 50809 10209 1846 0 81 39624 3562 773 13401 12 227][763 10 17 44 22 194 958 111 9 2164]...]\n",
            "targets[[1022 1135 3 11092 6 6 10 17 3776 2 4673 11 88 2484 2230 439 608 490 9 1779][5 29 3 0 117 104 11 9 27 123 307 7 45 2 53 583 716 15 135 3][13 642 22 1234 463 8 1844 22 463 796]...]\n",
            "targets[[5 35 4462 1282 185 1718 2847 8642 5996 1 2106 20641 166 120 3 29 157 1 25 924][17 5 1338 0 109 5 2202 1764 0 95 70 138 143 4 66 47 551 15 253 3][224 5 33 11944 15190 36 0 2598 304 47]...]\n",
            "targets[[2 397 4336 17 8 0 368 1839 46729 8030 629 3975 3092 6 6 9 386 21 136 73][19 21259 16692 83 1017 127 573 473 45 23 77 642 22 276 246 16 2 2040 29 130][30807 12 1605 1437 5 2 414 502 3 87]...]\n",
            "targets[[425 3 1867 1448 10 4 71 16 12762 85 9 106 2 171 3 681 184 104 38 24][50 9 169 10 22 246 175 70 2021 32 559 1234 700 120 8 258 344 55 3 6168][13 2 394 17 0 547 25 119 13064 1]...]\n",
            "targets[[17 672 44 14 137 315 1094 18 0 82 1001 228 3 0 19 13 2 605 453 39][19 624 13 273 0 1686 3 0 277 7 5034 15 1302 796 36 3270 791 66 0 1908][98 25 42 1003 4 28 2 81 845 3]...]\n",
            "targets[[80 20 76 51 20 1451 2 11287 3 675 12 15 2 22109 1821 1 2 522 3 101][10 210 21 0 7719 3 1305 23702 1528 1388 18 2 49 201 15 8282 558 16 3406 1518][9 112 10 19 123 233 9 215 7 31]...]\n",
            "targets[[1470 38 8251 12 0 9567 5 427 22 0 1188 601 673 0 9567 1156 4843 786 9747 1386][678 18058 53 49 815 20 96 138 44 16 2 6514 296 758 31 30 2326 6 6 29][172 3 0 6287 3 0 768 3 7335 11730]...]\n",
            "targets[[2119 742 3 609 80 661 2 2200 1 758 7 31 30 2287 586 0 281 16 137 52][44 0 12248 969 30 9406 258 1107 768 7553 2 1151 3850 926 105 16648 84 44107 1 1083][7390 23 16 0 1909 41 146 34 512 62]...]\n",
            "targets[[987 11 9 13 1435 4 10 19 33 10730 12 1517 738 1570 2019 869 1545 46 7 96][139 353 0 829 1 9 121 60 660 5 14834 37 8 60 1146 660 0 160 2726 2142][9 50 21 262 10 17 195 92 4572 2]...]\n",
            "targets[[20 89 21 333 256 127 1399 26847 15 94 20 386 21 333 10 17 22 0 82 495][10 5 0 6221 1128 37 272 7 12 57 16 58 0 1420 477 4 76 2 3122 55][322 17 29 3 0 117 9 139 110 1338]...]\n",
            "targets[[140 133 1014 23 9 140 133 2224 60 545 47 0 220 13 9 1184 195 2 5299 10][736 370 3 675 12 23 58 29 1010 344 4 23243 0 14765 3700 162 157 173 4546 5743][40987 5 29 3 0 88 3078 55956 3534 9177]...]\n",
            "targets[[50 21 1652 42 87 74 10 17 5 9 89 21 456 11 32 241 1066 43 2 3102][45 4 28 29 3 0 117 3 0 4638 104 11 9 139 123 110 0 109 5 2][65 467 0 122 9 419 32 148 103 7]...]\n",
            "targets[[149 0 84 3575 228 3 10 21010 7161 3 821 15037 9 31991 4769 11 9 207 77 599][64 1059 254 44 11 7569 1286 25329 12 673 67 77 654 78 2 246 17 33 959 1][5 29836 4 73 37 4653 4 106 9 13]...]\n",
            "targets[[0 2413 10829 499 8 38874 8 1210 764 111 105 5452 1922 1644 26505 2272 761 11333 25 43][17 5 730 4 2720 724 1 0 8438 3340 18 7 12 79 73 52 2063 72 193 1][165 424 10 17 251 0 116 13 1060 39]...]\n",
            "targets[[565 348 1796 109 2 1343 564 1 35 2199 453 3 57 66 7 31 0 681 1864 1557][39088 5853 5 2 243 8 40 544 6560 2 4058 34 451 15 40 701 8 2 35397 1542][584 7829 2408 24603 1209 8747 3937 12430 2307 7281]...]\n",
            "targets[[71 366 44 33 674 10 17 45 308 155 220 31 0 53 457 15 0 4405 199 0][5 142 2 81 17 4 106 15 183 423 9 140 213 261 16 35 1401 4 106 7][19 494 4 28 19 1471 18 42 150 21]...]\n",
            "targets[[71 9 103 9 13 23 53 628 51 9 215 10 662 47 50 9 136 8384 264 0][2369 98 25 2 245 38806 4 169 14 5 100 2369 29 18 10 29 5 641 3996 4][9 213 196 10 13 28574 12 84 19 7]...]\n",
            "targets[[242 2 19 1395 1 3896 1252 2570 28948 389 4 258 6166 4 122 0 84 5044 2683 3][208 8596 3033 50049 14631 46 20 38 45216 6389 1 10144 20 83 112 10 17 543 153 29002][3 30 9 89 21 180 387 134 30 131]...]\n",
            "targets[[653 130 5 8 11763 14107 0 46822 25 3062 0 12198 16468 1 67166 90 6 6 8 26260][2 2324 3 108 108 2119 703 104 9 196 9 207 579 0 250 0 1447 67 4 1494][13069 21 16447 3220 5 2 19 92 55 3]...]\n",
            "targets[[12 186 576 7414 13714 1 16633 25 1149 156 46 20 89 21 76 7 36 10 17 106][87 5 7 11 295 50 387 253 82 924 208 4528 38 2236 38652 41 17469 42970 118 0][604 1652 87 394 10 19 5 9 59 1203]...]\n",
            "targets[[5 2 3851 662 4 2 81 181 17 681 261 1 250 3 30 348 181 135 0 64][53 84 57 9 84 555 3 10 599 3101 436 128 8 60 640 13 51 7 13 7916][140 23 251 46 9 50 949 2 738 3]...]\n",
            "targets[[2972 3 0 9634 974 15 42 43 174 675 20 50 853 1491 4 93 0 88 737 1979][6362 6741 1493 629 5728 1 3559 33073 4048 33417 1899 15 62 373 7110 25175 5433 36 8802 4][8195 869 12138 0 755 3 0 836 6506 3]...]\n",
            "targets[[246 5 2 81 1234 1 690 7019 5 37 155 8 2 2726 20 50 667 0 762 636][140 2 340 3 184 98 1622 9 13 1298 4 169 0 2069 544 311 184 17 22 29][0 4144 4789 13 0 667 3 0 3248 36]...]\n",
            "targets[[968 212 18 89 21 1392 1452 4773 285 2 703 7485 3 1623 20 50 136 4087 4931 34][0 166 3 1210 3101 1692 2573 7576 12412 0 507 45 4 1775 48 53 7790 1513 11 10722][4267 6 6 303 377 5 119 127 217 114]...]\n",
            "targets[[9 1065 11 9 467 3523 4105 31 91 271 108 214 119 1 9 140 2 216 8 26][0 457 3 176 299 223 0 1054 3302 20189 0 6244 14088 11004 13370 1632 1 1073 4 4353][467 37 73 43 10 17 0 57 579 4]...]\n",
            "targets[[78 10 19 0 64 151 11 9 13 65 3693 43 13 11 7 236 28 348 7 12][6940 12 4739 9891 3349 12298 1613 13 371 2762 0 1613 83 64 28 2077 16 29 151 12272][196 7 13 35 554 1010 19 9 13 53]...]\n",
            "targets[[19 19245 13 2 84 1 2 227 16 8686 5123 7 13 40 84 276 92 19 31 2783][20 50 387 23680 1 18987 20 139 195 4 387 0 472 1 75 3 1210 1380 2961 3306][4957 3 73 467 1177 50 1667 4658 0 439]...]\n",
            "targets[[17 13 1900 92 16 160 7673 246 0 321 514 10 17 13 634 4 2639 0 217 2968][1913 218 2 74 1126 249 267 44 22 2 2000 1 724 78 111 24 50 21 1013 94][2574 71 87 73 9 112 10 17 464 0]...]\n",
            "targets[[84 57 9 215 10 17 9 1147 0 84 969 228 41 37 9 215 7 1382 227 311][207 38 4 198 10 163 399 16 2011 18414 1 13379 23 4 750 2 539 769 18 16][242 52 72 671 15 10 17 4 136 0]...]\n",
            "targets[[125 3790 6 6 1156 1464 1469 1039 5947 5455 26979 22760 21959 1 2596 15626 513 33 1493 10401][17 13 43 14 1060 14 35 3257 223 66624 4517 3 6947 15903 9 382 9 159 21 512][20 254 2820 3 0 14037 2033 42 870 2057]...]\n",
            "targets[[80 367 149 48 3 0 98 11 27 77 5786 250 123 1302 2784 36 3270 791 1259 3][261 16 46277 141 168 3088 10 5 2 2018 3 2 53 1836 239 4189 183 243 34 45][232 211 2163 0 64 293 9 58 254 44]...]\n",
            "targets[[17 5 853 0 14474 12981 1 11905 662 123 92 6 6 239 7 5 0 1536 1 88][5 2 52 212 72 648 1436 17 85 7 5 2 994 1115 0 369 1220 25 303 1][0 921 5 0 63 3 105 75 0 24164]...]\n",
            "targets[[17 400 71 15 0 16785 14109 4594 15659 12373 7 13 38 286 593 1 11990 2 130 36][196 11 0 101 68 65 3259 1 30 67 81 3052 0 848 8 60 660 13 5559 2163][2661 16899 8 26 209 14 1860 64172 15570 846]...]\n",
            "targets[[548 597 586 0 75 34 45 110 10 2863 116 22953 444 10645 63 2028 3741 511 61 529][1339 4 5862 240 29 311 1 138 4 0 17 797 4 66 10 29 532 9 13 8][19 92 679 23045 4 991 8398 8 91 1110]...]\n",
            "targets[[64 293 9 1591 10 17 13 85 3 0 1009 261 234 22 0 1029 46 9 67 542][9 28246 27 195 4 712 4725 28581 4 0 1057 3354 18 884 4548 7 20 232 66 9][67 113 110 751 1787 304 2 74 216 9]...]\n",
            "targets[[52 9 667 10 19 0 52 9 5299 7 5 69 408 1010 2271 1 2677 247 2937 192][1475 9 5708 22 10 19 22 2269 132 9 13 24218 8558 60 9476 15131 5 12963 7780 4917][2 57 8 61 98 68 73 52 1342 87]...]\n",
            "targets[[166 31 2 2707 1045 1 174 1307 70 27 98 11 211 8 15 42 2 173 4416 131][6 1023 3993 43683 27173 1 82 2559 8647 28008 2731 55 47 32 121 43 0 3854 299 3854][55 39 14 29 3 0 437 250 92 16]...]\n",
            "targets[[1935 25 799 1154 4 6372 34 96 481 11 1834 2852 521 1418 1 3033 2 791 48219 98][9 67 48 1943 16 10 19 603 233 9 367 0 116 3 1306 43880 3386 6970 29755 37][196 11 16 2 84 422 3 2 84 202]...]\n",
            "targets[[2 115 222 1498 31 214 10 17 5 283 9 856 36 2 5307 5212 0 64 293 11][159 21 121 180 47 4 512 43 10 19 18 51 9 215 7 22 729 9 13 3382][17 5 2 12169 1421 6 6 6059 4925 650]...]\n",
            "targets[[493 1156 928 21393 6052 1 14340 13 2 126 17 94 9 856 8 189 9 13 53 1520][0 1210 3 31287 8 2 1244 3 15779 0 12688 15234 714 7500 789 2 8959 3 1 5245][7 12 0 19 41 42 60 4887 18 352]...]\n",
            "targets[[540 3 299 2818 45 1056 223 49 247 104 5793 6310 45 77 2270 1777 16 87 49 32][16 7 12 1271 3014 1 1149 500 7 12 2 81 346 797 122 120 10803 18 10 5][12571 285 0 172 3 780 35 1171 2786 4686]...]\n",
            "targets[[1154 1331 5 3770 29 3 0 720 250 98 123 92 15 0 1416 3 0 6744 717 31][3 544 311 2829 12 1048 1088 7535 12395 12 122 128 402 10 23 14 1474 15 0 19068][140 23 251 47 7 5 18 39 188 4]...]\n",
            "targets[[0 457 3 0 17 0 323 1394 1 0 135 3 0 1670 68 504 190 0 63 13][3220 13 1016 119 2 794 3 1408 154 1038 9396 33 2213 28852 1 0 2214 4513 31 2][5 29 3 0 250 184 104 9 27 123]...]\n",
            "targets[[19 15 2 171 3 3049 8 3761 1204 6021 16767 5 322 14 35 12321 3 2 231 370][215 10 22 277 16 223 1 9 196 9 59 830 7 44 1 66 46 7 5 100][230 11 131 104 25 81 105 887 34 93]...]\n",
            "targets[[80 23 182 4 198 97 73 240 43 10 19 14 7 5 273 149 23 1237 0 717][2457 1 1069 1 275 122 5454 12554 17428 1580 278 10 1404 3 0 119 1281 15971 895 10][896 513 1 408 0 8092 921 5 1184 1678]...]\n",
            "targets[[206 5 2 4015 1 3502 959 368 15 323 848 1 760 152 9 83 987 7 5 23][26 8 4375 25 13217 1827 33 2 1107 3 4209 2 183 6307 5 5555 4 10811 26 5414][57698 158 12909 3 0 4754 2711 2686 4981 0]...]\n",
            "targets[[45 4 28 33 229 0 1504 250 17 9 27 110 8 0 227 844 154 51 9 215][1455 8 60 333 5 87 2373 4 720 228 167 0 129 3 799 174 422 0 863 22][9 50 1203 136 11 10 5 0 84 57]...]\n",
            "targets[[5 2 637 43 0 75 3 26524 958 8 775 47 5 677 637 39 25 2 173 135][47 2 1080 4110 15003 13 2 1178 15 10 19 24 67 0 3647 4 2249 2 19 15][14864 11059 5 2 49 19 15 480 1380 347]...]\n",
            "targets[[634 10 5 2 3879 832 827 17 18 36 0 9239 91 576 11 7 45 77 513 15][307 7 2361 349 15 30 0 305 926 7 13 2 1151 17 46 20 89 21 38 2789][70 30 27 9578 1375 11 7 83 113 25454]...]\n",
            "targets[[3269 285 35 158 18 6345 1178 3084 1 3670 12418 1217 2 23 4 28 9045 5936 1376 32][19399 12 1616 27287 203 27 77 5578 51 24 389 15 0 321 3 10 19 2 74 984][19 5 35 2449 4 0 442 472 3 699]...]\n",
            "targets[[26865 2591 5842 5 15174 3 189 1 1088 8 2 2540 637 393 14 2 183 243 8940 4835][626 13 1149 15167 22557 626 5 368 9 232 198 48 1095 4 10 17 7 12 2 73][27 4 136 11 3533 21106 1331 13 0 1164]...]\n",
            "targets[[1236 16 536 1 1081 14437 678 1800 2369 346 378 678 41840 141 28 33951 6 6 9 215][103 10 17 141 93 200 57 338 19 1155 2 222 4217 9 59 138 22 43 10 220][196 10 17 118 2 185 205 49 301 7]...]\n",
            "targets[[47 27 20 226 20 27 599 35 30 160 360 7 5 948 233 2041 12 227 19 2828][30 0 826 1 1915 301 226 33 869 75 45 65 1699 10 17 8 235 7 2 1737][3 30 51 9 215 0 15235 1561 16 5455]...]\n",
            "targets[[2000 45 226 48 81 98 245 214 26 84 5 133 26 117 0 2120 6621 12205 33238 5][1019 747 4 1652 10 29 3 2 244 1749 2697 786 314 1515 1028 2934 87 583 7620 0][38 10 17 467 7 14 2 527 1 59]...]\n",
            "targets[[19 5 37 74 11 7 45 4 28 110 4 28 2439 634 7 2103 35 980 612 493][681 115 2789 2479 0 2540 637 12170 31 2 57 51 338 13 29864 0 1522 3 7159 13577][1269 3 155 1409 5 4 11034 1 10881 0]...]\n",
            "targets[[169 10 19 1 47 9 1239 91 4187 432 1771 483 4 28 105 327 6459 104 9 329][2 173 683 4117 4 60 333 51 9 103 3 0 11894 683 38 7736 707 3276 72 114][354 19 36 17363 27995 5 554 1186 22 2]...]\n",
            "targets[[248 3273 18 5970 192 17 11 267 187 0 176 1 143 175 4 5979 91 2423 834 58][5 2 539 832 827 17 11 5 53 647 8 87 331 1 342 193 667 0 170 19][9895 31 26 7750 44 4748 8 0 1752 1916]...]\n",
            "targets[[64 307 10 17 85 3 9120 6832 1 18292 5428 3 4659 2024 9120 6832 1207 3593 10 17][60 1187 12114 4 4337 16107 203 27 77 498 4 0 10569 57 9 139 110 7 9 140][3 0 20248 3 2 49 1860 19 5 11]...]\n",
            "targets[[1 0 1370 15456 5 2 535 1330 848 725 427 22 2 63 33 28411 15797 1183 1 0][128 7 429 5 0 181 17 208 181 8 2 145 360 341 919 89 21 663 0 607][0 1791 3 26 312 0 4139 846 577 23947]...]\n",
            "targets[[27 110 0 1561 16 10 17 440 214 119 1 9 27 4 136 11 4022 1756 280 38][509 10 17 53 73 6 6 10 5 0 64 8563 1873 9465 17947 7719 9 139 110 9][17 5 669 37281 3 472 84 3 30 3803]...]\n",
            "targets[[5 0 666 250 19 11 9 27 123 110 8 60 114 42 4 14087 9 80 23 6261][153 780 35886 34 5 542 16 437 161 269 10 737 4361 680 1159 2 679 19165 169 14][17 210 21 273 73 3 60 57 7 5]...]\n",
            "targets[[627 353 0 829 3 0 6313 128 22 898 167 9 106 2 17 9 27 211 4 4847][5 2 65 647 19 1 11 5 23 2 74 151 7 5 2 2182 3 2 5087 825][23 28 16211 9 232 366 15 0 360 753]...]\n",
            "targets[[36029 12 8356 2502 36 2 6801 834 61 113 1564 73 3 35 1421 16 915 2739 4696 8][122 5 122 607 99 29 422 9 307 7 22 1631 282 99 11 1289 12 122 61 5][17 13 4321 14 194 14 20 148 261 16]...]\n",
            "targets[[50 21 262 10 17 1339 4 76 142 2 2364 303 678 3 1108 7 5 1184 1678 1][0 1002 3 1541 1292 1882 9014 30486 22 2 3456 3 1151 216 104 30 3 61 68 74][16 729 14 2 15685 755 43 0 7548 4674]...]\n",
            "targets[[23 74 18 79 23 37 81 4550 19 2970 1654 5 2 1059 645 36 1061 2295 11649 34][10 17 93 71 230 145 184 51 9 1715 11 9 1499 16 7 1 1066 52 72 308][0 1029 3 0 277 7 550 520 3406 6078]...]\n",
            "targets[[1 71 5 2 436 43 1902 55 8 3249 5816 2838 248 38 7872 7 38 10555 41 52][96 27 77 2 81 17 153 141 27 287 9161 10354 42 486 51 54 5 1531 4 54][44 3 720 399 6 6 7 12 245 4]...]\n",
            "targets[[335 3133 15 0 5372 738 9 627 198 75 0 4132 3 0 813 18 10 17 5 180][95 5766 5 397 5 30 6859 190 161 5 1067 8 131 798 43 7 107 5249 64 694][384 5 2 37508 2515 1156 2 53 183 2314]...]\n",
            "targets[[140 213 261 16 49 492 707 771 7 28 1177 98 41 246 202 16 0 183 41 0][984 3 0 955 5010 17 3 0 858 304 1764 1032 3406 14 0 5787 3 26 102 36][12403 146 2799 4707 9 67 3 4073 17949 1]...]\n",
            "targets[[9 207 38 4 136 43 2236 2004 12 0 1313 1611 5 11 275 3302 331 1 2 243][227 10 1594 3 1594 12 83 28 22 277 15 0 1971 3 0 1287 1869 1568 14072 223][5 35 2483 17 326 22 378 23 35 12178]...]\n",
            "targets[[20 27 23 4660 7 239 0 19 1104 27 2 1770 530 1116 1057 62 4781 13 4 3383][306 4 28 8 0 5150 128 9 50 21 387 134 10 6 6 17 210 21 381 0][139 4652 4 66 10 17 16 2 194 57]...]\n",
            "targets[[15383 12 1969 5 0 63 3 2 125 12 15568 78 4887 256 16181 7899 685 5 8403 33][229 126 94 20 59 512 766 436 1159 2 160 728 3550 543 5492 1543 7020 34 12 301][89 21 121 47 0 859 10805 215 8 10]...]\n",
            "targets[[13 2 394 453 3 281 1 57 39 13 64 29 1413 102 3412 12 939 9182 295 332][2008 577 2100 35540 45 77 259 4 1178 2 2136 11 59 129 30 3561 1 43312 18 784][233 318 40 8 163 179 9 722 43 20]...]\n",
            "targets[[0 4308 1116 1057 16 10 610 637 9 203 987 4 256 56591 18746 0 19 61 3085 258][29 670 4480 9 198 7 29 314 16 0 4837 343 135 1 769 1785 342 873 843 7][5 29 3 0 250 104 9 139 110 0]...]\n",
            "targets[[64 151 447 72 2 348 17 5 2 2119 348 17 2382 5 42 11 6 6 7 188][17 5 2 8297 502 3 0 2116 1447 70 27 37 108 81 98 9 50 21 387 87][1948 246 5 1938 9442 16 9505 3524 1 1907]...]\n",
            "targets[[2 903 11 10 3578 1594 13 400 4 178 696 4 1766 51446 9 139 113 196 0 678][5 65 29 3 0 126 4030 98 9 139 110 8 2 132 6 6 193 0 2642 1][685 761 18305 2433 668 154 8 1061 16 26]...]\n",
            "targets[[477 2 680 61 6804 22 0 410 3 2 184 17 6 6 716 163 263 19478 76 1423][0 665 3 5252 2 21483 11958 2735 33 1644 26505 5 2308 20142 22 246 12 84 16839 2480][180 2 1343 1 8890 2515 270 8 5213 0]...]\n",
            "targets[[108 388 9 67 53 360 1422 3 10 17 9 42 103 0 834 3 2 460 125 116][738 1745 1022 6 6 10 462 539 19 45 77 73 3496 43 233 91 768 8 5470 1][17 5 43 2 183 234 34 5 0 1378]...]\n",
            "targets[[734 17 15 0 629 15176 12 648 450 46 20 424 29948 20 232 112 10 17 504 1290][1018 10 487 22 1901 29 311 1 420 21 262 0 2011 8391 3 7 58 46 0 5488][0 19 672 0 84 459 228 465 38 2]...]\n",
            "targets[[243 12 373 162 2 845 15 0 1727 4 586 0 518 36 1636 844 154 309 0 1727][937 1 734 17 145 112 63 23 2 3435 2914 755 6 6 5 43 385 474 75 0][1779 149 10 17 1 13 53 4232 9 139]...]\n",
            "targets[[5057 577 2386 6828 26 1299 4 0 1727 1 1497 0 13987 3 947 12958 25258 31 0 457][118 23 27 0 3372 3 107 482 4 667 3172 99 884 8 0 1864 18 256 307 7][5 1242 4 512 11 2 2138 3 6940 104]...]\n",
            "targets[[0 670 392 2607 88 3 0 442 17 6 6 14 7 551 60 939 1291 0 277 768][8 411 20 159 21 854 36 0 84 29250 3246 104 11 0 3246 13 65 74 33332 267][42 215 6111 1 9 27 4 136 11 7]...]\n",
            "targets[[10 13 42 14 9 67 6994 53 379 9 13 861 4 106 7 8 26840 2 173 2196][17 65 9 382 65 1762 91 195 109 1463 37 200 1 969 2427 2507 50 1142 144 90][719 6 6 9 42 307 10 175 1 7]...]\n",
            "targets[[37 14 2 340 3 0 3101 443 168 9 802 2392 11849 1 9 27 4 136 11 9][5 2 2845 1 13668 817 1 7 3658 69 97 7 672 120 187 2829 14 2 1548 20804][8 160 728 2654 223 163 250 17 3 2654]...]\n",
            "targets[[5 137 51 7 269 4 19 1 1054 294 62 19 472 32 27 198 178 440 3 0][69 92 1614 18 241 23 16 0 823 276 374 13646 3729 17 1352 9 59 112 4 66][24743 168 31 10 9 76 28478 940 6 6]...]\n",
            "targets[[856 4 66 2 49 184 17 51 9 1291 0 578 330 234 18 9 13 671 0 3741][19 45 2 81 809 30 0 95 144 88 3 0 2722 25 23 69 542 18 1810 2][6 6 1357 129 3 0 176 5 2 15239]...]\n",
            "targets[[509 10 17 0 19 69 2501 20129 1489 8 114 30 3 0 454 68 263 239 19905 29][19 13 3813 8 0 1598 1050 15 74 116 74 503 1 2 839 7958 109 410 142 14][5 2 81 360 341 487 45 35 177 745]...]\n",
            "targets[[11381 33 48 14 0 117 699 19 3 0 1128 200 1708 1 699 443 31 91 117 0][2127 223 42 389 22 3685 37 9 242 3999 503 10 14 9 106 7 0 64 864 9][4 211 5 35 5177 19 349 15 11357 11388]...]\n",
            "targets[[21505 19 472 5 4 103 11 10 88 1614 766 680 45 77 475 3278 9 569 21 136][159 21 27 73 109 1 13 2572 634 20 1143 2 171 3 57 149 75 2856 79 7][27 56 321 14 4 61 310 153 714 32677]...]\n",
            "targets[[20 112 0 123 19186 988 3 1241 1264 1412 7 12 273 318 3486 42 16 0 955 1144][5554 42 195 6211 1 499 318 2 1137 125 20584 622 373 3905 5 40 9688 10 5 23][307 7 22 277 9 50 21 837 10 2566]...]\n",
            "targets[[6385 1564 2 237 11 5 193 901 16 0 534 1 12097 16 0 102 54 285 1083 1897][5 0 777 755 3 5625 12 2401 549 2570 3234 1 26 3396 15 0 17854 54465 38 9116][10 17 5 437 394 0 964 559 2 29539]...]\n",
            "targets[[17 13 649 326 8 0 25211 1084 393 15 0 1500 36 0 1230 10098 391 0 172 3][17 702 0 63 43 35 234 20600 16659 62 2260 55 8 64536 8 0 3907 12 40 324][13 694 154 158 0 57 10 1613 559 265]...]\n",
            "targets[[5 2 630 7719 43 0 145 980 323 5432 3564 1178 2711 31369 2392 511 16661 15773 2 428][105 179 11 25 88 3107 43 10 19 25 91 26551 1853 1 0 992 3 91 5583 1394][1190 5 35 576 184 340 1 10 5 92]...]\n",
            "targets[[139 241 307 272 2645 1140 104 1 10 29 426 1455 44 14 29 3 0 117 5656 3325][109 954 5 2 4260 5190 134 80 70 27 4 27 258 485 5923 26248 8 643 4 230][0 1782 3 1490 0 2630 19889 13046 2896 26775]...]\n",
            "targets[[531 8 0 390 3 548 1 30 11 5 4065 93 71 68804 10 17 47 1748 25 20][12 97 74 4616 786 17763 159 21 5984 52 4 0 1471 6040 85 22 0 2058 24 13][779 261 16 100 266 3 5893 9364 8 2]...]\n",
            "targets[[242 335 8962 4 66 11 0 823 678 16 10 17 5 463 223 163 16 47 7240 71][215 10 19 31 10705 12 2004 637 19 1322 1 13 10041 4 892 193 0 964 1 18399][20 957 4 19711 4 1901 729 1 20 50]...]\n",
            "targets[[2290 10 19 1 1239 7 2 81 368 15 81 2324 156 8 0 794 11 10 19 13][124 23 365 108 683 4 122 20 47 2066 5 42 106 10 17 89 21 1647 16 342][19 13 642 128 15 0 412 2054 1028 8]...]\n",
            "targets[[9562 151 43 10 3140 5 11 221 30 0 5920 575 0 873 980 1031 1722 3688 306 4][109 1022 6 6 9 5915 1681 4554 9 864 134 24 3348 0 209 3 2 557 1141 8][672 4 106 10 17 18 529 55 85 7]...]\n",
            "targets[[13 1427 4 28 2 613 17 61 389 44 31 0 170 57 14 0 200 341 984 3][9 50 21 262 75 25 1245 330 192 4 165 38 10 432 3 821 10797 9 215 7][12 194 13210 56 65 811 55 4 0 6198]...]\n",
            "targets[[17 13 53 49 7 13 155 1263 594 1 212 7 288 21 14 49 14 639 308 18][1742 155 1258 1 2167 2226 5 213 1349 6217 20 15 40 2457 1 1774 7 12 576 11][12 53 53 155 20 121 42 38 2 201]...]\n",
            "targets[[987 23 107 11 4444 3 2596 14 2 183 493 7 12 194 1 0 63 5 2 115][215 10 17 22 1901 7 13 65 155 36 0 3398 557 2424 4 0 3398 200 74 454][1274 183 125 5 427 22 2 1133 2598 304]...]\n",
            "targets[[23178 45 92 31 219 105 81 104 970 62552 66578 41917 1 22421 44044 47423 18 10 57 24][13 30103 31 0 378 1045 67 634 38690 0 277 1647 185 4 223 11930 1167 9 121 1][9 13 261 144 0 1568 31 338 378 261]...]\n",
            "targets[[83 367 10 1330 22753 1165 2 171 52 46 20 27 110 41 25 2 340 3 0 314][5 23 2 737 5817 17 882 5 7 2 737 3115 19 9 382 7 12 3115 18 23][0 687 176 3 1509 10 1 3409 1363 11]...]\n",
            "targets[[722 884 829 11 136 137 38 89 21 453 127 57 10 19 4250 22 1746 7 124 4][30 34 27 110 0 17 1 353 0 298 94 9 242 767 4 146 34 159 21 353][17850 220 8 0 412 5 2267 2716 35 7252]...]\n",
            "targets[[46 7 1189 21 427 22 2 298 10 17 59 27 77 497 1 91 447 85 7 5][246 3586 1861 10 17 2 735 47 5 2 735 4 71 5 87 5 7 614 11 2][746 114 50 28 2 88 259 15816 58 8]...]\n",
            "targets[[8 2 16918 5 29 3 0 117 5927 4962 104 16 317 996 29 995 88 3 62 104][29 3 0 81 284 1781 12 5085 183 468 5057 418 2254 11257 31 0 57 3 91 2583][1665 477 5 147 2 4591 878 29 11 5]...]\n",
            "targets[[12 1247 51 9 242 10 44 3 8817 15 2 19 1610 3 0 1851 45 2 53 5723][140 35 6345 821 1352 61 5 29 293 9 67 4 66 10 17 149 101 1 3052 9684][64 215 10 17 282 969 154 602 51 7]...]\n",
            "targets[[662 5 13446 3 959 1720 6 6 84 0 28144 25 2 903 56 5171 848 1 64 0][2871 418 137 38 29 0 1675 54 9749 55 4 136 9 230 38 9 182 4 28 1616][3 0 368 104 3 0 544 3037 723 21]...]\n",
            "targets[[32 83 27 2 2380 265 4 9571 10 609 767 23 2 735 14 4443 7 5 1770 12991][17 11464 12 1068 5 376 97 498 4 346 16 71 10 17 11067 71 4 80 47 54][6 454 10 17 1762 9 1024 60 308 541]...]\n",
            "targets[[1666 3129 7657 3 2 1040 64873 4958 575 0 13697 12 518 267 185 8 4026 2242 8 0][956 55 3220 6 6 513 33 10656 4387 6 6 1156 6845 6565 1976 10233 5060 5838 4713 3926][15717 12 325 19 408 33 5022 9052 3728 778]...]\n",
            "targets[[20 38 74 360 341 3265 155 104 2637 56 52 10 5 0 17 16 20 6 6 7][202 3 1472 1483 8343 33280 31 40 812 12 7547 328 45 663 13669 180 21513 2 1579 615][9 242 1632 4 4801 2 19 11 12 77]...]\n",
            "targets[[5 0 1536 17 9 139 123 110 9 140 1234 5138 1 9 211 8 42 14 0 21][7 12 275 454 8 62 56641 46982 19031 4 2 2787 2891 8 47897 41 2 234 674 47][82 423 3 60 1842 68 9027 33 959 98]...]\n",
            "targets[[17 5 2 453 3 57 0 225 45 7189 0 135 25 23 2050 0 305 290 25 3453][2 186 49 17 18 7 932 2 160 271 0 271 13 2225 1 448 37 466 118 23][31322 9878 17761 1 2 609 3503 3 362 11]...]\n",
            "targets[[62 10855 5195 21385 1 3247 4962 68 37 404 7248 99 16 894 3389 33 15853 439 1139 8197][49506 12 6414 1745 946 762 22 343 1 0 484 1702 11710 8592 14 4104 200 112 0 1025][19 13 92 8 796 483 1 7 274 8]...]\n",
            "targets[[4 106 10 17 5 2 821 1 9 89 21 382 11 8 2 337 95 9 13 3624][202 5 0 2017 3804 3 699 201 1 360 341 832 827 3782 11086 2 752 713 9283 1729][67 0 1004 4 166 4839 7 15 35 279]...]\n",
            "targets[[5 2 81 17 47 162 10 17 37 81 5 0 98 197 4669 285 4 7 12 3034][212 19 36 1806 153 786 1047 17417 5 2 1179 1282 11 5 247 1 181 2702 14 7][50 21 833 1697 3656 44 9 106 86 8]...]\n",
            "targets[[307 10 17 23 37 73 16 9915 12492 18 16 16885 19327 0 200 1615 1 454 1 4366][1756 1 1922 4529 5304 52 397 1580 36 2783 3661 10 57 436 1 201 2993 27 77 1296][59 27 1715 11 0 0 1497 68 30 8]...]\n",
            "targets[[4 987 10 339 35542 2590 22698 587 50 486 546 0 3260 34 13 60 18498 6050 67 49][307 223 3144 4302 459 44 3 163 10181 1809 3808 8156 16569 34002 212 239 565 777 17 7456][9 2337 10 19 13 5941 8 5284 9 793]...]\n",
            "targets[[12453 3 98 124 2 403 138 4 66 131 483 11 560 90 1545 47 551 4 62 2373][17 5 0 84 3 0 1408 3202 9509 3982 98 1 5 29 3 0 117 31 0 170][838 19 2775 1592 1939 150 21 27 238 22]...]\n",
            "targets[[5 2 53 155 1 610 17 43 105 524 7148 1 5234 42 5243 0 145 176 259 4][27 79 77 261 16 0 17 4319 16 154 1415 20 37 73 16 0 3968 4 0 2147][84 3 0 3955 19257 104 13750 5 88 730]...]\n",
            "targets[[6592 201 5 29 3 4668 1 3222 12 84 98 16 10569 1128 1670 37 7 210 21 799][215 10 19 1 196 7 13 475 594 429 2 12117 487 208 2 742 3 10482 342 10][215 10 432 22 711 1 9 380 20 47]...]\n",
            "targets[[491 4 28 2883 1354 9 182 4 28 2883 1354 6 6 10475 26453 6 6 9 813 11][0 393 3 5182 1275 12563 12 14438 2454 29328 5 2 681 115 19 1471 1 29 11 9285][98 27 2260 55 2 171 1 25 1510 52]...]\n",
            "targets[[65 112 0 1626 4678 9 139 307 7 51 9 13 2 493 1 147 14 2 183 1174][7 18 133 27 4395 119 0 1437 3439 0 17 13 1393 69 15 0 1068 3 156 3129][97 73 15457 1081 5 0 64 2043 3 10]...]\n",
            "targets[[19 5 2 297 63 43 2 183 2435 125 1318 15 2 1247 1245 21970 11 406 26 2][423 106 0 122 2842 11 91 22 91 2 81 2327 16 1137 423 190 32 365 4 535][104 3 0 3631 5001 8 10 411 25 504]...]\n",
            "targets[[19 389 1180 14 2 49 181 19 61 9 89 21 65 103 7 5 9 254 0 63][22 0 858 304 33 584 772 17657 0 12800 2242 5 2 102 2018 8621 187 0 321 3][14749 5857 9250 1807 13 1318 4 55734 8 0]...]\n",
            "targets[[3 2866 28 2972 3 0 9634 18 15 1728 341 2 385 177 1 35 3920 225 6 6][4543 11 9 50 138 22 10 678 2341 5 29 6 6 3465 4 136 8600 10344 3220 83][71 366 44 33 674 11 9 367 2 49]...]\n",
            "targets[[1022 1404 360 341 81 19 537 600 1 343 47 332 96 20 999 16 0 4290 883 5][467 10 17 30 3 146 74 829 20 139 42 353 25 335 356 7 5 37 1058 1][3754 5 29 3 46 23 0 1536 122 22]...]\n",
            "targets[[9 121 88 75 103 959 50 21 93 2 49 662 61 5 1216 297 56 223 959 18][1591 10 17 85 9 13 14454 144 0 184 17 2539 16 146 98 11 56 29 12 555][19 5 172 3 0 6356 8133 184 1568 645]...]\n",
            "targets[[1406 666 8858 5866 0 247 8 2 10110 328 9573 4550 6 6 0 666 1164 430 15 10][3541 3079 5 2 4324 1 4750 3246 436 427 22 0 297 63 3 4238 34 118 0 1483][19 5 2 2004 1568 3 87 70 14 1804]...]\n",
            "targets[[2833 5 0 9724 3 2 63 6774 720 154 881 15 7971 12 1188 2639 9 723 21 110][1609 274 178 128 134 54 5 29 3 3500 12 88 4687 1 467 156 40 1110 3 2][23 110 10 7768 19 8 2 53 194 57]...]\n",
            "targets[[1066 0 57 4 106 10 835 61 5 226 8 336 1355 1 6462 29 3 0 872 276][3489 1 751 5023 25 2 2151 3 51893 34 1134 4 560 160 728 16 2 126 114 44][254 161 160 19818 8 10 19 18 11 5]...]\n",
            "targets[[19 5 186 49 16 1487 1 436 9 139 77 4 2896 37213 1 112 0 5135 7 12][1309 3 503 10 19 45 239 4 5301 28202 6037 8 0 898 1202 7 724 354 3 0][136 10 17 5 16584 5 73 97 81 35]...]\n",
            "targets[[5 64 641 1604 16 439 3 74 17 707 7 5 52 2611 16 1500 3 2654 12 1560][5787 5 29 65 2372 663 6808 9 1046 11 7 12 12642 8 0 1468 1 0 116 5][0 84 172 3 10 19 9 15840 256 1147]...]\n",
            "targets[[3 0 117 98 3 0 291 1 0 1504 2707 8 11224 546 16 0 129 10 19 5][6 10 5 29 3 60 519 98 0 63 5 69 585 0 101 166 53 69 8 0][5101 11 27 16997 55 37 229 25 43 10]...]\n",
            "targets[[12 77 2 132 233 318 10 0 84 57 37 9 307 7 175 15 0 325 17 8][96 387 134 48 75 569 21 169 10 155 46 20 159 21 437 3919 15 0 1204 11][219 7 12 23 370 3 536 41 5994 7]...]\n",
            "targets[[89 21 121 47 17 131 82 10805 215 18 7 682 288 21 0 170 29 9 118 10][559 0 281 1 2191 970 1787 14420 7650 4662 1166 23349 1 2920 11588 278 2422 2 782 4277][1 21248 2359 178 15 2 53 10714 2926 3]...]\n",
            "targets[[5 29 3 0 30 57 7433 98 9 139 123 110 7 204 28 180 28208 18 0 5160][196 10 17 389 596 53 69 1078 11 7 96 139 710 6574 8 0 2903 7161 9 96][59 139 4325 5078 7971 2298 2 12046 44768 4]...]\n",
            "targets[[1082 15 0 105 897 4937 104 186 73 121 47 32 50 512 2 385 522 3 75 2377][20 182 4 66 2 17 11 1990 7425 55 29 4602 640 15 100 82 4602 640 0 28471][12 35 212 17 4 28 92 43 2 243]...]\n",
            "targets[[3 0 250 98 9 139 123 110 116 13 394 193 16 0 362 1 0 1532 88 4][3 320 5 23 65 2 49 17 33 100 1194 18 7 5 2 2229 29 837 46 1242][1430 1022 6 6 39 25 3388 14582 8 47]...]\n",
            "targets[[321 3 235 2 19 43 0 3179 1 3609 3 998 0 708 968 38 49 321 16 2][2541 543 153 23057 2569 124 0 14684 15 26 811 55 4 3340 1116 599 7537 1156 2638 2142][0 154 75 27 77 1073 4 66 0 414]...]\n",
            "targets[[5 53 799 2 414 19 0 990 59 28 2734 33 10946 18 113 585 37 19964 10 5][9 84 307 10 17 71 2 3097 1 35 1780 34 13 258 567 68 2 441 3 3797][2628 1059 642 22 3857 13 65 74 3 30]...]\n",
            "targets[[0 543 153 3 0 2986 1868 1086 2266 27613 34 12138 2 755 4129 33 2 15814 680 31608][139 67 0 1164 1062 678 10 17 6 6 84 3 30 0 101 2501 8 10 17 25][215 10 17 119 223 154 602 1 7 133]...]\n",
            "targets[[109 1 105 1906 225 92 101 168 38 3391 593 5626 3465 4 136 10 92 7 871 4][17 29 3 0 117 9 139 123 110 2180 43 7 124 7 20301 3459 8 2 45054 41028][5 29 3 0 250 104 9 139 123 110]...]\n",
            "targets[[17 124 29 3 0 117 2730 3 1057 2 298 9 27 123 353 0 117 102 8 39][739 5 38 0 872 1792 8 0 217 176 214 1297 1063 3890 45 35 322 1926 14 685][414 435 5 2 63 43 105 969 137 887]...]\n",
            "targets[[3 0 28317 1 88 807 1050 1177 9 139 123 353 9 42 67 4 66 46 39 13][396 3033 6 6 1379 1 6159 33 6541 1314 6 6 7204 6 6 706 245 439 3 30328][3538 1 12733 8522 25 0 11696 2151 34 76]...]\n",
            "targets[[27 67 2 598 4 66 1286 12630 22 0 1971 19 1322 8 48246 111 9 409 9 13][278 23 58 1464 1469 96 93 10 17 155 46 20 80 1134 4 138 44 1 66 10][9 215 0 2533 2027 0 20643 343 135 0]...]\n",
            "targets[[2 19984 3 10 167 227 291 12 1465 905 159 21 65 121 134 32 529 90 44 99][21 12 2335 11 0 3183 3 708 5 0 64 151 11 50 76 3748 3 8120 26459 1193][1308 673 2756 8 4917 5 2 1076 95 4]...]\n",
            "targets[[307 10 17 163 154 602 1 27 307 7 22 378 35 823 3 282 2 291 233 7][81 151 43 10 17 5 87 1146 7 5 58 152 48 3 0 179 20 66 204 28][27 77 126 277 12 36 82 4896 18 10]...]\n",
            "targets[[232 383 0 738 3 10 2327 14 354 14 614 1812 7 360 341 23 155 2372 225 116][112 184 98 9 65 80 18 10 19 13 1157 9 50 21 262 7 195 78 184 3089][9 118 23 27 0 1792 3 106 10 17]...]\n",
            "targets[[30 40353 144 10 471 17 574 5769 22 381 48 4132 36 258 2255 10332 838 109 3264 116][139 64 110 0 3566 339 14 9 140 251 221 295 332 45 37 3 268 20 50 380][42 307 10 17 227 311 1 9 13 53]...]\n",
            "targets[[109 102 954 1 2051 8 10 17 25 30 554 849 180 2 453 3 57 0 1169 3][402 318 10 422 14 2 493 3 272 317 7 13 29 3 0 173 762 9 402 318][307 459 223 7511 700 44 3 163 10181 22652]...]\n",
            "targets[[318 10 19 1816 602 7 923 3394 143 78 60 6678 1 9 230 9 203 810 7 41][128 5 4 146 34 159 21 1124 0 17 8 643 4 65 1124 0 17 1100 365 4][45 195 4 28 0 250 17 9 139 123]...]\n",
            "targets[[262 11 4403 5458 9419 1619 9080 68 15 0 53 1031 792 4598 9 262 11 26 88 744][0 1473 5 2 6067 19 7 5 6067 11 238 10 2762 13 165 645 1646 277 0 19][17234 1609 5 2 81 534 49 31 391 603]...]\n",
            "targets[[1079 348 5670 3 2 6749 19 58 152 221 30 0 369 1220 25 3 2 303 492 546][5 8592 3423 16 13406 1 32 4211 7 120 15 10 7210 305 637 22 0 472 3 759][5 29 3 0 2646 762 3 1210 1332 10]...]\n",
            "targets[[0 14704 9 141 220 44 11 9 242 23 2 340 3 0 104 3 8321 471 6512 9][27 77 440 260 4957 3 0 846 11117 1 468 6052 63 48 427 3252 22 0 584 2581][5 42 157 1278 17 11 12 23 65 273]...]\n",
            "targets[[2 10144 12 63 5 2 49 16736 1 88 1336 13 1016 3839 16 359 8 0 303 377][2320 49 2316 594 5371 49 261 524 18 106 44 56 109 1 115 2729 335 2323 7 12][49 1131 432 3 699 17 235 3 91 794]...]\n",
            "targets[[325 3 0 19 5 1549 1 11 12 134 7 9032 2 700 85 0 109 5 23 257][139 113 77 1520 33 34433 566 1 466 1443 61 9 1982 21 110 2723 4 91 3566 9762][1046 15 0 82 898 4222 10 12016 13 0]...]\n",
            "targets[[3 30 597 359 10 3968 5528 5444 15672 9451 36123 1490 2474 6716 7 83 455 20 4 2][10 2 184 41 2 1979 680 4 71 10 5 2 2229 2018 3 3576 4995 1 320 0][139 353 0 298 1 464 1237 11 0 7]...]\n",
            "targets[[280 52 38 2 202 3 375 1334 8 208 100 266 3 2642 47 5 0 461 12 620][27 37 108 77 37 11482 16 37 115 284 1802 14 21856 2733 5 686 2 813 0 88][36 2 304 33 3 170 390 472 921 5]...]\n",
            "targets[[196 10 17 13 3749 155 7 12 2 247 23 4 28 579 618 17 43 29 125 12][5 2 647 7676 2434 14621 19 46 39 5 142 2 151 14 2906 184 443 10 19 5][5 23 65 2 2380 738 233 9 118 23]...]\n",
            "targets[[562 4 2 8948 1107 1 2 316 1107 51 2 19192 2817 0 330 4 2284 32 89 21][5 2 1211 7082 734 201 43 105 75 34 12 1125 291 1328 5 18037 165 7 5 52][5 142 2 397 17 9 215 7 16 0]...]\n",
            "targets[[2470 898 1 10436 16 9 27 77 259 4 402 0 390 3 10 1173 7689 1149 17 16][17 5 437 607 9 84 215 7 2 363 3 154 602 1 67 60 5432 16665 36 1014][230 10 5 29 3 0 117 98 9 139]...]\n",
            "targets[[71 366 44 33 674 11 9 242 2 460 893 9 196 7 13 2 523 778 31 47][175 29 3 0 88 6455 30360 3 60 640 45 77 2381 33 16040 34 604 387 0 14621][17 5 945 15 181 1 600 0 63 344]...]\n",
            "targets[[5 14 4692 35 279 14 26 1231 7 12 2 166 3 1175 4 2111 0 3370 3 9965][227 145 725 123 92 2 4015 19 11 4682 83 112 16 123 56 82 725 92 233 67][47 3677 70 80 15 258 158 70 50 66]...]\n",
            "targets[[1125 154 99 0 129 3 0 2557 299 25660 389 23 97 544 41 13 58 9417 7 12][196 2743 728 13 822 14 2476 9 27 110 40 8 2778 71 2141 1 2517 3676 14 69][13 0 250 17 9 27 123 110 1818 9]...]\n",
            "targets[[9 215 11 898 6313 1281 10 17 0 1371 7845 98 9 196 7 13 97 2669 18 115][9033 185 339 3 14836 12 171 5 2 229 1535 36 0 3111 3173 474 70 2164 4 722][1166 285 2 2499 216 34 5 2624 78 0]...]\n",
            "targets[[2 527 8 0 407 246 917 3 0 3239 9 67 0 1421 4 667 440 8070 36 0][17 65 3257 60 531 4 0 176 3 4290 883 4936 9 65 67 56 321 11 20 96][25 23 192 98 11 20 50 560 553 2940]...]\n",
            "targets[[5 2 397 17 1 39891 66282 5 322 37 5 57974 67834 34 5 627 322 8 88 3][5 119 35 541 3 3057 1 2848 7 12 14 46 0 543 42 300 70 27 2 834][5 2 595 755 18 7 788 53 5190 7]...]\n",
            "targets[[0 117 95 4 8168 71 8 2 19 5 4 1491 48 2708 4 5285 3789 18 128 5][0 53 219 20 27 4 198 2041 1 60257 69771 3698 16 107 0 53 84 362 22 0][5 29 3 60 519 1177 16 180 2 132]...]\n",
            "targets[[2480 1314 1963 0 297 63 3 11084 0 32472 10 17 13 14 4289 33 60 2690 2 2404][5054 1 1099 222 3 5994 1570 5994 18 5994 934 3501 15 4210 1 1679 405 20 80 23][140 340 3 540 9 38 238 43 540 9]...]\n",
            "targets[[12 4005 95 3 2695 44 5943 798 404 15 19609 4 1652 179 11 24 5 52 13731 43][2270 1611 1071 184 17 22950 0 117 3 1071 184 70 139 195 3585 756 2333 756 2434 8443][125 5 64 14 2814 14 26 9537 7714 41]...]\n",
            "targets[[115 222 3 2237 22 0 2598 16703 4183 11105 11 0 304 1459 31128 2191 16 42931 347 8][98 15 1004 350 245 1 776 42 354 3 107 49 8 48 741 0 9613 724 78 11][0 3981 4 0 403 3899 721 71 9 448]...]\n",
            "targets[[236 28 0 6940 12 3987 117 12016 3 0 291 99 15819 7 13 2 49 27163 284 7403][56 15002 10 5 2 3885 63 9932 5 0 3240 1171 6454 1927 5741 143 4 0 15279 29][10 637 70 892 2154 0 945 3439 3 2]...]\n",
            "targets[[140 2 1452 7591 340 85 54 67 314 492 9 89 21 103 40 98 25 49 1 9][141 27 542 9 13 8 1062 15 8452 1063 23431 14 0 455 102 683 604 1652 882 80][10 360 341 19 45 164 16 7 5 2]...]\n",
            "targets[[67 84 252 10 458 8 0 291 2634 1 9 196 94 47 2 81 458 147 221 700][1591 10 17 85 9 5915 15517 25945 11802 9 103 24 12 2 397 279 469 0 819 24][2164 55 28206 1 9 121 0 63 10 17]...]\n",
            "targets[[424 0 84 17 10 662 13 29 3 0 250 98 9 27 123 110 0 84 29 165][4404 1208 3680 14017 36 0 8440 2689 13410 1956 20404 12 1800 6842 3886 3230 38 814 36 3270][139 1059 418 143 1 307 10 17 175 36]...]\n",
            "targets[[17 141 28 2 749 55 2889 19 18 16 48 293 91 2 1179 17 296 0 4688 25][215 10 17 22 735 1048 797 6389 7 2144 37 73 46 9 1982 21 77 149 7 22][5420 15 2 1218 3640 10 17 866 29 2]...]\n",
            "targets[[17 5 322 16 0 18404 75 34 118 23 38 10 17 138 818 0 5569 12 202 1][9 140 23 2 53 200 340 3 0 2863 393 3 0 10644 887 1 9 113 581 31][29 3 0 2075 181 98 9 139 123 110]...]\n",
            "targets[[19 3318 2 368 757 8 189 7 3318 0 757 6292 33 1734 36 457 4 129 18 8][8 2731 18 174 172 188 1082 7 285 38 2 4466 3 897 11870 21150 6 6 39 12][16 29 27 8955 509 174 422 3 4197 15829]...]\n",
            "targets[[5 0 13477 17 8735 36 0 887 1592 1 8027 27053 15 37 73 564 9 59 27 856][12 245 4 136 15 48 2092 1412 771 32 68 92 4 28 613 41 46 32 68 92][10 5 2 394 19 370 3 5969 705 2060]...]\n",
            "targets[[1121 462 201 46 20 148 646 43 2 7984 814 787 2686 135 36 81 1 836 98 142][5 29 3 0 117 2922 16768 1409 9 27 123 252 9 3978 395 7 4 439 3 84][83 28 53 4883 9 112 10 202 1 2254]...]\n",
            "targets[[10 15 2 522 3 355 22 277 1 196 7 13 81 9 13 180 767 9 159 21][8 2243 702 0 63 3 2 1479 461 34 2306 187 2 1150 8 31 311 15 2 194][36 2 1485 935 39 12 2 171 52 356]...]\n",
            "targets[[14305 3260 6405 13 2 1071 855 201 11 2663 0 29 57 1420 234 9771 24495 125 13 54][399 1 9 140 107 3616 1260 1022 168 10 5 2 360 341 975 17 270 8 17497 26533][112 4355 399 9055 0 8314 22299 4355 8 26]...]\n",
            "targets[[117 3 0 668 2153 7194 104 1881 0 88 413 109 1 0 872 639 3 323 342 1838][103 0 19 45 81 876 15 91 10708 35087 3 0 4038 1 0 18849 3 0 23063 1][7 28 300 2 201 50 21 488 22 2]...]\n",
            "targets[[3 50924 4608 5 2 737 2829 12 2087 8 7779 17 7 5 23 2 184 41 1048 1088][7 50 28 226 284 786 24953 1 16821 10371 11690 44 0 1243 3 62 2602 1 211 55][12 226 7 175 0 1077 11 92 468 27331]...]\n",
            "targets[[36 2930 9 604 136 771 10 19 5 206 8 0 1955 3 5537 443 469 70 128 80][10 5339 8 1351 51 9 13 4911 2561 8 13875 7 13 0 250 244 3 1356 1789 29][18855 92 680 12327 15 2 658 225 17351 55]...]\n",
            "targets[[467 0 8108 34027 2977 0 4291 3 299 1 299 1 15341 32 25 539 1177 1 25 775][467 28251 36 0 53 457 0 735 11 3738 0 122 13 504 9 59 404 169 545 4915][5 2 81 17 2003 15 440 1044 1205 642]...]\n",
            "targets[[16072 702 2 63 43 2 928 125 34 26798 15 26 3283 99 8113 2 766 9 386 21][194 57 4802 340 9 1591 0 378 3 9204 9078 42 8 57 16 0 466 483 3 0][103 11 12 10 5 379 1056 1 513 17]...]\n",
            "targets[[154 41 37 602 35 158 125 3000 2 1426 291 158 234 1 559 40 4 1682 22 26][509 10 17 7 45 35 55 4 1309 2511 936 22 35 158 63 18 9 59 870 16][878 9903 10 5 35 2483 27403 17 41 35]...]\n",
            "targets[[28 1146 9 159 21 58 93 7 30 0 95 144 10 432 3 609 17 7 13 497][3 30 45 8940 4835 123 67 2 49 1126 249 256 40 304 0 415 3 0 1639 12][23 28 4374 33 30 3 0 22965 44 39]...]\n",
            "targets[[949 670 6 6 47 20 66 5 47 20 76 23 65 47 23650 20638 12 2845 1 539][548 9 3994 10 19 51 6996 35875 8 5431 9 118 10 16 60 105 524 94 463 1][509 10 19 53 73 108 3101 75 83 230]...]\n",
            "targets[[8026 635 2020 33 0 3673 21414 2460 17428 22 0 82 489 3 0 8507 1 17197 0 659][0 246 5 56855 15 528 20 50 169 221 283 1 46 766 5 47 20 148 261 16][315 0 739 11 529 178 1935 3 0 4837]...]\n",
            "targets[[201 1881 0 10644 887 34 25 0 10644 887 9 89 21 121 9 139 113 555 3 90][6596 183 2181 54 285 27880 2 666 373 15 35 61632 435 3027 2197 27880 45 48 441 3][7 5 617 4 136 11 14 2 2582 9]...]\n",
            "targets[[5 2 368 3239 832 827 19 6 6 9 1900 215 7 51 9 13 2784 1 67 4395][0 206 19 67 0 181 338 230 4 7 7 159 21 306 73 38 207 207 8 0][368 30 316 1470 3 3948 9029 12 304 3224]...]\n",
            "targets[[84 18375 9 196 6240 67 0 1222 3 2 19 273 149 15 6549 638 22 91 16188 4][9 139 64 110 0 12679 339 3 10 19 1 0 19 12 206 1561 61 67 2 173][17 210 21 60 4165 3 3853 9 420 21]...]\n",
            "targets[[80 20 999 91 195 52 29 2493 126 14 8 52 1121 405 126 181 7488 476 0 336][16 154 1 154 9 196 11 6610 4406 13 0 872 4256 17 6 6 0 11621 27418 4736][5267 51 27976 1601 22 0 130 0 4927 869]...]\n",
            "targets[[288 21 30 11 899 8 149 10 17 18 9 874 4 566 233 7 13 29 3 0][418 4 0 378 1045 1027 81 179 36 47 9 139 555 9 89 21 27 4 27 2][5 29 3 146 98 11 13 31 48 220]...]\n",
            "targets[[27 2518 44 3 53 173 104 8 60 2732 9 59 27 9880 1296 10 19 4 0 988][5 29 3 0 117 1 2139 2651 123 92 7 13 2 53 1614 212 102 2018 3 2][1591 10 1 9 196 431 10 280 38 2]...]\n",
            "targets[[249 208 2 2116 5 2 81 502 3 87 360 341 104 8 1633 443 50 3806 2 754][1473 12460 7 55 1329 14 0 814 15 2 145 74 2067 1 1206 11876 1488 2 337 1135][2175 5 29 3 60 30 57 519 156 26]...]\n",
            "targets[[1609 5 2 1720 9 207 213 555 11 1902 55 18 448 48 23193 36 7 51 9 886][69 92 1 3569 369 8551 850 36 675 10 641 24859 1620 15556 2 755 3 2 498 2066][10925 5 2 1054 17 36 5622 1156 69 542]...]\n",
            "targets[[1065 2 738 3 3615 396 22 60 4183 5444 2253 1 20 232 28 770 11 71 200 1205][5 64 29 49 293 134 10 17 218 796 163 18741 1333 54 2749 0 442 17 1 971][959 26 796 158 331 278 62 197 2829 3251]...]\n",
            "targets[[5428 285 2 323 26556 985 4 2 1133 23085 34 732 52 4609 72 54 8 189 5 54][288 21 180 1632 16 0 19 4 366 51 7 118 14 9 13 21513 15 82 3318 18][2582 8 184 308 213 2207 641 7840 1936 20]...]\n",
            "targets[[113 693 0 158 7945 167 0 299 15 91 18453 224 1905 3137 4387 12 0 795 125 1][8 105 510 1733 3 24635 5 0 117 3 0 13167 8372 202 0 1032 5 322 257 17960][282 8 2 132 6345 17 17946 25 1928 4]...]\n",
            "targets[[13 5070 14 4 87 371 74 10 17 13 13 0 507 428 4 854 137 41 4903 22][3 0 435 3 6501 5 2 799 475 2389 2303 24346 19 58 33 0 477 12 1645 7][59 112 4 66 2 1275 1281 339 3 10]...]\n",
            "targets[[19 5 2088 2 20595 43 33 48 454 36 160 7673 1 65 11 5 30 7 5 9][462 1 42 885 276 439 182 905 317 7 5 5073 4 704 0 1100 12 10 5597 4][19 5 2 938 1245 6492 9 103 9 195]...]\n",
            "targets[[2 11505 4 0 57 3 284 1802 2651 111 75 118 47 32 118 16 0 205 996 1][3 30 9 27 4 136 9 215 10 17 22 378 233 7 159 21 211 44 128 8][4 28 22726 1 3616 15 0 227 166 3]...]\n",
            "targets[[53 1009 17 15 2 81 935 2340 33 0 484 3 6057 1 34105 1332 14 2 2836 340][4 153 1 177 16 142 2 825 19 0 23416 608 3 14260 1 4560 2588 71 78 0][994 11113 5 33 229 0 117 458 9 27]...]\n",
            "targets[[0 2844 328 10 5 0 17 9 139 793 4 843 9 195 3326 4 0 17 797 15][9413 432 3 17 235 11 165 3124 447 15 567 2471 4 245 214 5 2 3551 1146 2939][141 27 77 8 0 898 1297 250 11 5]...]\n",
            "targets[[56 48647 57398 20120 499 120 8 2 1717 6418 115 1782 11 280 180 1974 1 10652 4 255][13 23 0 2074 128 24 13 861 33 0 1104 1 0 19 1077 4 768 0 6488 11][153 1188 36500 45 226 2 397 301 15 26]...]\n",
            "targets[[604 262 11 9 418 4 0 443 4 106 10 3503 3 3 158 3828 0 3352 3 0][0 291 5001 20 59 27 4 2070 0 17 797 16 43 4334 2196 8 2 3431 8 643][13 2 186 2814 1470 3 4086 2008 2312 12]...]\n",
            "targets[[159 21 1046 15 100 3 0 19563 8 0 319 514 202 18 2800 9 254 0 1177 3074][1362 16 10 729 202 7 550 11 0 1599 13 19548 22 17670 190 202 308 13 1432 763][793 60 117 4 383 35 867 333 11219 6804]...]\n",
            "targets[[19 45 77 92 33 908 207 8214 2591 31018 25873 15 244 3 1860 266 3 1044 114 1][42 1779 149 10 19 334 72 2 782 602 1 9 242 23 1085 251 47 7 13 30][207 64 110 10 17 282 167 51 9 13]...]\n",
            "targets[[12 871 4 949 2 109 2939 16 2 17 38 0 3445 10752 22 91 2353 7 59 306][27 113 2518 44 3 2 17 8 60 114 67 9 110 10 8 0 797 9 59 27][17 5 2 81 1110 3 0 5927 298 0]...]\n",
            "targets[[139 77 3943 33 10 19 16 2 132 8 172 85 3 0 554 303 580 128 22 898][13 261 931 4 10 107 2 200 340 3 535 1330 848 125 47 68 32 532 3119 30][19 464 576 1070 33 0 153 4 1006 2]...]\n",
            "targets[[20 367 184 98 10 17 288 21 631 31 30 37 1372 22 7 46 631 375 25 0][26 368 19 1471 3165 10543 4053 6415 3480 4 0 1450 11 67 358 86 26 872 1784 8][551 4 106 10 17 22 246 1059 9400 14916]...]\n",
            "targets[[27 110 10 17 463 41 1108 214 119 0 501 163 154 41 37 1936 4930 1234 285 7][424 8562 391 2 243 641 185 22 40 1985 54 491 53 73 4 126 800 18 179 89][84 9 182 4 136 58 46 9 140 2]...]\n",
            "targets[[242 2 669 340 3 0 2192 567 3 338 18 58 9 67 2 245 57 5561 15 10][2919 3969 67 108 4 103 0 202 13 119 15 0 320 3 1306 18 42 870 357 0][109 7415 16 10 19 4898 37 73 2 522]...]\n",
            "targets[[1633 360 341 436 218 49 2921 16 0 3532 1 109 58 152 7 5 2 737 4256 17][10305 5 1990 55680 8 40 379 209 2006 0 213 480 1548 7480 284 1802 1583 26 648 594][1653 0 84 422 3 10 122 18257 6402 37]...]\n",
            "targets[[5 29 3 2992 7874 12 250 98 123 58 152 24 12 23 65 0 289 102 9 1514][1570 4450 15 11758 10231 9224 2 243 24 12 77 326 33 105 9188 1 94 24 5 19426][466 994 202 13 113 29 3 60 519 179]...]\n",
            "targets[[0 13982 4898 2 201 1 75 173 418 4 106 7 107 0 84 768 3 3220 5 23][1591 10 17 36 0 3297 7 12 245 4 169 16 49 293 3002 44 3 3212 9 140][1602 284 4776 1079 635 44 87 4 704 0]...]\n",
            "targets[[2 687 8235 7 12 533 245 4 106 920 104 7 12 612 4 27 4 106 0 2451][1497 8 2 1860 1782 306 0 914 3 0 1341 3336 16014 31181 0 6121 1776 6 6 10][0 2333 9521 44 39 70 121 87 2952 10]...]\n",
            "targets[[37 20 241 555 2 171 3 74 528 43 10 17 167 58 149 7 37 27 9 1][139 213 196 11 3435 1331 13 0 250 17 9 139 123 110 1597 33 792 3625 223 1][227 57 7647 15600 1601 167 2 17 361 13]...]\n",
            "targets[[45 77 579 36 0 673 5257 3 0 2462 48 3 0 8266 25 11406 0 458 122 8][1269 3 3563 5 627 4 8519 18 32 25 79 0 117 102 6537 1050 15 145 75 8][451 1 3 1108 160 12136 33 543 153 2273]...]\n",
            "targets[[5 2 81 17 4 1013 78 1468 7 8 316 1 460 13 2 81 1068 1 0 177][45 195 4 28 0 88 394 19 123 2496 4 3928 1 9 106 735 1048 797 5354 9][89 21 103 9 27 123 110 2 447 3309]...]\n",
            "targets[[80 20 76 51 20 1484 2 360 341 975 17 15 2 63 43 2 739 3 6368 14018][175 256 110 1 509 2 480 19 9 479 4 131 4791 1 169 7 5953 11 56 29][2789 1594 4213 533 1688 11 4008 1 7618 3]...]\n",
            "targets[[1167 10166 13996 118 7 175 1167 941 10 17 3796 7 13 29 3 0 3400 98 9 215][373 5 29 3 146 104 11 20 121 5 49 272 58 81 18 7 5 38 1879 18216][5 84 1 7096 2 2329 487 7 5 2]...]\n",
            "targets[[307 10 22 35 694 541 2766 1 3551 85 3 0 4279 1 0 21601 9 165 254 7][2 291 602 9 2564 1291 249 3 0 2140 120 7969 15 56 17053 429 99 2 194 870][20 106 0 206 15639 104 7 12 213 504]...]\n",
            "targets[[793 4 5807 238 11 236 28 1267 2 1250 9 79 2265 11 20 139 110 0 84 17][204 2924 1022 9 1018 0 738 3 10 19 8 2 711 895 12220 1 9 242 65 1298][203 136 10 17 5 371 504 1 6193 9420]...]\n",
            "targets[[204 28 0 250 422 3 2896 0 225 188 5318 0 1048 188 4300 2537 5667 20038 3 517][401 5 23 0 2906 19 15 3446 1693 37 9 65 89 21 103 39 5 97 73 23][84 1969 22 5715 895 5 29 3 60 519]...]\n",
            "targets[[2238 4 48 3 0 829 9 139 353 9 196 10 19 13 2 336 778 22 174 616][81 63 45 91 197 3370 1 51 7 5 7784 120 11 7 476 2 63 11 710 20][3 2 316 2798 5 283 0 73 6921 22277]...]\n",
            "targets[[203 987 11 9 1591 10 17 22 0 834 11 9 196 7 13 35 1633 19 7 204][47 10 122 96 27 226 46 7 68 1690 55 33 0 3660 11 488 0 13868 6 6][150 21 180 27354 0 6142 3 14950 317 18]...]\n",
            "targets[[418 4 0 797 532 11 9 59 28 2445 296 9 13 671 0 224 1011 8 10 19][19 2107 15 105 1171 2552 1761 34 25 1922 8349 9514 43695 1 1925 7702 4176 34 166 14][120 9 203 5167 87 1247 7 5 11 9]...]\n",
            "targets[[9193 785 71 4795 36 457 4 129 1681 3244 252 0 209 4 3177 0 115 396 11 285][2137 4301 308 9969 463 399 10 17 6152 48 186 594 653 904 35 18529 3410 1831 11 17][2335 151 43 12159 5 11 0 1139 3588 653]...]\n",
            "targets[[140 2 669 93 11 669 340 3 0 246 122 37 9 67 213 470 4 66 0 17][41 23 20 38 10 17 83 241 8951 22 771 20 38 318 98 111 0 986 101 709][19 45 283 20 207 182 7 4 27 257]...]\n",
            "targets[[337 595 1 5121 14 10 17 5 39 5 29 200 430 1 9 242 23 664 43 0][4 107 2 340 3 0 84 2113 1 256 424 601 316 1 460 9 13 5635 22 318][0 64 293 9 140 715 10 17 2 223]...]\n",
            "targets[[5 2 19 43 48 19 1500 235 2 19 7 12 53 73 8 0 2113 8523 35 3091][7297 5 35 1504 1930 4 5415 8 0 209 3 18796 6651 24 252 6651 8 275 2239 104][27 4 198 10 17 53 303 3823 85 7]...]\n",
            "Closing output_file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOwDbEL3c3B4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K9aZeEuOc3kK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "huMuu2c6c4OQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHl8sFSttCHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mRPevP8JsZ3J",
        "colab_type": "text"
      },
      "source": [
        "## Save and Output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiPsRppGw1CK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FB7DYX0w17O",
        "colab_type": "code",
        "outputId": "7f24f459-61d6-4e1e-a74c-9d9e241829db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# #copy checkpoints to drive\n",
        "%cd /content\n",
        "%cp -av maskgan /content/gdrive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "'maskgan' -> '/content/gdrive/My Drive/maskgan'\n",
            "'maskgan/checkpoint_convert.py' -> '/content/gdrive/My Drive/maskgan/checkpoint_convert.py'\n",
            "'maskgan/sample_shuffler.py' -> '/content/gdrive/My Drive/maskgan/sample_shuffler.py'\n",
            "'maskgan/pretrain_mask_gan.py' -> '/content/gdrive/My Drive/maskgan/pretrain_mask_gan.py'\n",
            "'maskgan/README.md' -> '/content/gdrive/My Drive/maskgan/README.md'\n",
            "'maskgan/dataset' -> '/content/gdrive/My Drive/maskgan/dataset'\n",
            "'maskgan/dataset/iccv' -> '/content/gdrive/My Drive/maskgan/dataset/iccv'\n",
            "'maskgan/dataset/iccv/ptb.valid.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv/ptb.valid.txt'\n",
            "'maskgan/dataset/iccv/ptb.train.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv/ptb.train.txt'\n",
            "'maskgan/dataset/iccv/ptb.test.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv/ptb.test.txt'\n",
            "'maskgan/dataset/iccv2017' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017'\n",
            "'maskgan/dataset/iccv2017/ptb.train.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017/ptb.train.txt'\n",
            "'maskgan/dataset/iccv2017/ptb.valid.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017/ptb.valid.txt'\n",
            "'maskgan/dataset/iccv2017/ptb.test.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017/ptb.test.txt'\n",
            "'maskgan/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/.ipynb_checkpoints'\n",
            "'maskgan/pretrain_mask_gan.pyc' -> '/content/gdrive/My Drive/maskgan/pretrain_mask_gan.pyc'\n",
            "'maskgan/data' -> '/content/gdrive/My Drive/maskgan/data'\n",
            "'maskgan/data/imdb_loader.py' -> '/content/gdrive/My Drive/maskgan/data/imdb_loader.py'\n",
            "'maskgan/data/ptb_loader.py' -> '/content/gdrive/My Drive/maskgan/data/ptb_loader.py'\n",
            "'maskgan/data/__init__.py' -> '/content/gdrive/My Drive/maskgan/data/__init__.py'\n",
            "'maskgan/data/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/data/__init__.pyc'\n",
            "'maskgan/data/ptb_loader.pyc' -> '/content/gdrive/My Drive/maskgan/data/ptb_loader.pyc'\n",
            "'maskgan/data/imdb_loader.pyc' -> '/content/gdrive/My Drive/maskgan/data/imdb_loader.pyc'\n",
            "'maskgan/nas_utils' -> '/content/gdrive/My Drive/maskgan/nas_utils'\n",
            "'maskgan/nas_utils/configs.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/configs.py'\n",
            "'maskgan/nas_utils/variational_dropout.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/variational_dropout.py'\n",
            "'maskgan/nas_utils/__init__.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/__init__.py'\n",
            "'maskgan/nas_utils/custom_cell.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/custom_cell.py'\n",
            "'maskgan/nas_utils/custom_cell.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/custom_cell.pyc'\n",
            "'maskgan/nas_utils/variational_dropout.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/variational_dropout.pyc'\n",
            "'maskgan/nas_utils/configs.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/configs.pyc'\n",
            "'maskgan/nas_utils/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/__init__.pyc'\n",
            "'maskgan/models' -> '/content/gdrive/My Drive/maskgan/models'\n",
            "'maskgan/models/cnn.py' -> '/content/gdrive/My Drive/maskgan/models/cnn.py'\n",
            "'maskgan/models/attention_utils.py' -> '/content/gdrive/My Drive/maskgan/models/attention_utils.py'\n",
            "'maskgan/models/rollout.py' -> '/content/gdrive/My Drive/maskgan/models/rollout.py'\n",
            "'maskgan/models/bidirectional_zaremba.py' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_zaremba.py'\n",
            "'maskgan/models/rnn_nas.py' -> '/content/gdrive/My Drive/maskgan/models/rnn_nas.py'\n",
            "'maskgan/models/seq2seq_vd.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_vd.py'\n",
            "'maskgan/models/feedforward.py' -> '/content/gdrive/My Drive/maskgan/models/feedforward.py'\n",
            "'maskgan/models/critic_vd.py' -> '/content/gdrive/My Drive/maskgan/models/critic_vd.py'\n",
            "'maskgan/models/__init__.py' -> '/content/gdrive/My Drive/maskgan/models/__init__.py'\n",
            "'maskgan/models/bidirectional.py' -> '/content/gdrive/My Drive/maskgan/models/bidirectional.py'\n",
            "'maskgan/models/seq2seq_zaremba.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_zaremba.py'\n",
            "'maskgan/models/seq2seq.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq.py'\n",
            "'maskgan/models/rnn_vd.py' -> '/content/gdrive/My Drive/maskgan/models/rnn_vd.py'\n",
            "'maskgan/models/bidirectional_vd.py' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_vd.py'\n",
            "'maskgan/models/seq2seq_nas.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_nas.py'\n",
            "'maskgan/models/rnn.py' -> '/content/gdrive/My Drive/maskgan/models/rnn.py'\n",
            "'maskgan/models/rnn_zaremba.py' -> '/content/gdrive/My Drive/maskgan/models/rnn_zaremba.py'\n",
            "'maskgan/models/evaluation_utils.py' -> '/content/gdrive/My Drive/maskgan/models/evaluation_utils.py'\n",
            "'maskgan/models/bidirectional.pyc' -> '/content/gdrive/My Drive/maskgan/models/bidirectional.pyc'\n",
            "'maskgan/models/evaluation_utils.pyc' -> '/content/gdrive/My Drive/maskgan/models/evaluation_utils.pyc'\n",
            "'maskgan/models/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/models/__init__.pyc'\n",
            "'maskgan/models/rnn_zaremba.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn_zaremba.pyc'\n",
            "'maskgan/models/bidirectional_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_vd.pyc'\n",
            "'maskgan/models/rnn.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn.pyc'\n",
            "'maskgan/models/bidirectional_zaremba.pyc' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_zaremba.pyc'\n",
            "'maskgan/models/seq2seq_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_vd.pyc'\n",
            "'maskgan/models/attention_utils.pyc' -> '/content/gdrive/My Drive/maskgan/models/attention_utils.pyc'\n",
            "'maskgan/models/cnn.pyc' -> '/content/gdrive/My Drive/maskgan/models/cnn.pyc'\n",
            "'maskgan/models/seq2seq.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq.pyc'\n",
            "'maskgan/models/seq2seq_nas.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_nas.pyc'\n",
            "'maskgan/models/rnn_nas.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn_nas.pyc'\n",
            "'maskgan/models/seq2seq_zaremba.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_zaremba.pyc'\n",
            "'maskgan/models/rollout.pyc' -> '/content/gdrive/My Drive/maskgan/models/rollout.pyc'\n",
            "'maskgan/models/rnn_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn_vd.pyc'\n",
            "'maskgan/models/critic_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/critic_vd.pyc'\n",
            "'maskgan/models/feedforward.pyc' -> '/content/gdrive/My Drive/maskgan/models/feedforward.pyc'\n",
            "'maskgan/regularization' -> '/content/gdrive/My Drive/maskgan/regularization'\n",
            "'maskgan/regularization/variational_dropout.py' -> '/content/gdrive/My Drive/maskgan/regularization/variational_dropout.py'\n",
            "'maskgan/regularization/zoneout.py' -> '/content/gdrive/My Drive/maskgan/regularization/zoneout.py'\n",
            "'maskgan/regularization/__init__.py' -> '/content/gdrive/My Drive/maskgan/regularization/__init__.py'\n",
            "'maskgan/regularization/zoneout.pyc' -> '/content/gdrive/My Drive/maskgan/regularization/zoneout.pyc'\n",
            "'maskgan/regularization/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/regularization/__init__.pyc'\n",
            "'maskgan/regularization/variational_dropout.pyc' -> '/content/gdrive/My Drive/maskgan/regularization/variational_dropout.pyc'\n",
            "'maskgan/losses' -> '/content/gdrive/My Drive/maskgan/losses'\n",
            "'maskgan/losses/losses.py' -> '/content/gdrive/My Drive/maskgan/losses/losses.py'\n",
            "'maskgan/losses/__init__.py' -> '/content/gdrive/My Drive/maskgan/losses/__init__.py'\n",
            "'maskgan/losses/losses.pyc' -> '/content/gdrive/My Drive/maskgan/losses/losses.pyc'\n",
            "'maskgan/losses/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/losses/__init__.pyc'\n",
            "'maskgan/train_mask_gan.py' -> '/content/gdrive/My Drive/maskgan/train_mask_gan.py'\n",
            "'maskgan/model_utils' -> '/content/gdrive/My Drive/maskgan/model_utils'\n",
            "'maskgan/model_utils/model_losses.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_losses.py'\n",
            "'maskgan/model_utils/variable_mapping.py' -> '/content/gdrive/My Drive/maskgan/model_utils/variable_mapping.py'\n",
            "'maskgan/model_utils/model_optimization.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_optimization.py'\n",
            "'maskgan/model_utils/model_utils.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_utils.py'\n",
            "'maskgan/model_utils/helper.py' -> '/content/gdrive/My Drive/maskgan/model_utils/helper.py'\n",
            "'maskgan/model_utils/__init__.py' -> '/content/gdrive/My Drive/maskgan/model_utils/__init__.py'\n",
            "'maskgan/model_utils/n_gram.py' -> '/content/gdrive/My Drive/maskgan/model_utils/n_gram.py'\n",
            "'maskgan/model_utils/variable_mapping.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/variable_mapping.pyc'\n",
            "'maskgan/model_utils/n_gram.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/n_gram.pyc'\n",
            "'maskgan/model_utils/model_utils.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_utils.pyc'\n",
            "'maskgan/model_utils/helper.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/helper.pyc'\n",
            "'maskgan/model_utils/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/__init__.pyc'\n",
            "'maskgan/model_utils/model_losses.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_losses.pyc'\n",
            "'maskgan/model_utils/model_optimization.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_optimization.pyc'\n",
            "'maskgan/model_utils/model_construction.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_construction.py'\n",
            "'maskgan/model_utils/model_construction.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_construction.pyc'\n",
            "'maskgan/train_mask_gan.pyc' -> '/content/gdrive/My Drive/maskgan/train_mask_gan.pyc'\n",
            "'maskgan/generate_samples.py' -> '/content/gdrive/My Drive/maskgan/generate_samples.py'\n",
            "'maskgan/maskGAN' -> '/content/gdrive/My Drive/maskgan/maskGAN'\n",
            "'maskgan/maskGAN/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/maskGAN/.ipynb_checkpoints'\n",
            "'maskgan/maskGAN/train-log.txt' -> '/content/gdrive/My Drive/maskgan/maskGAN/train-log.txt'\n",
            "'maskgan/maskGAN/maskGAN_mle' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle'\n",
            "'maskgan/maskGAN/maskGAN_mle/train-log.txt' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train-log.txt'\n",
            "'maskgan/maskGAN/maskGAN_mle/train' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/.ipynb_checkpoints'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/checkpoint' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/checkpoint'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.index' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.index'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.data-00000-of-00001' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.data-00000-of-00001'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.meta' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.meta'\n",
            "'maskgan/maskGAN/train' -> '/content/gdrive/My Drive/maskgan/maskGAN/train'\n",
            "'maskgan/maskGAN/train/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/.ipynb_checkpoints'\n",
            "'maskgan/maskGAN/train/graph.pbtxt' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/graph.pbtxt'\n",
            "'maskgan/maskGAN/train/model.ckpt-1059.index' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/model.ckpt-1059.index'\n",
            "'maskgan/maskGAN/train/model.ckpt-1059.data-00000-of-00001' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/model.ckpt-1059.data-00000-of-00001'\n",
            "'maskgan/maskGAN/train/checkpoint' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/checkpoint'\n",
            "'maskgan/maskGAN/train/model.ckpt-1059.meta' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/model.ckpt-1059.meta'\n",
            "'maskgan/maskGANj_working.ipynb' -> '/content/gdrive/My Drive/maskgan/maskGANj_working.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fV7rJwxZ0Xa",
        "colab_type": "code",
        "outputId": "6b8d2052-fc97-4b59-b923-1b5f40e55c1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# copy from drive into runtime\n",
        "%cd /content/gdrive/My\\ Drive\n",
        "%cp -r maskgan /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXXGgoq0aZo8",
        "colab_type": "code",
        "outputId": "a89c677f-572e-46da-9978-e07b0e2b7edb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/maskgan/maskGAN/train\n",
        "from google.colab import files\n",
        "files.download('model.ckpt-1542.data-00000-of-00001') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/maskgan/maskGAN/train\n",
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 35792, 0, 0)\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 293, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 321, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 334, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 657, in __init__\n",
            "    self.finish()\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 716, in finish\n",
            "    self.wfile.close()\n",
            "  File \"/usr/lib/python2.7/socket.py\", line 283, in close\n",
            "    self.flush()\n",
            "  File \"/usr/lib/python2.7/socket.py\", line 307, in flush\n",
            "    self._sock.sendall(view[write_offset:write_offset+buffer_size])\n",
            "error: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AcystW6oRMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJLAY-ot-rAt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcqrMZ7W-uEh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dMi-HZO_Leb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}