Practical Projective Structure From Motion (PNSfM)   Practical Projective Structure from Motion (PNSfM)  Ludovic Magerand, Alessio Del Bue  Visual Geometry and Modelling (VGM) Lab, Istituto Italiano di Tecnologia (IIT)  Via Morego N0, NNNNN Genova, Italy  ludovic@magerand.fr, Alessio.DelBue@iit.it  Abstract  This paper presents a solution to the Projective Structure from Motion (PSfM) problem able to deal efficiently  with missing data, outliers and, for the first time, large  scale ND reconstruction scenarios
By embedding the projective depths into the projective parameters of the points  and views, we decrease the number of unknowns to estimate  and improve computational speed by optimizing standard  linear Least Squares systems instead of homogeneous ones
 In order to do so, we show that an extension of the linear  constraints from the Generalized Projective Reconstruction  Theorem can be transferred to the projective parameters,  ensuring also a valid projective reconstruction in the process
We use an incremental approach that, starting from a  solvable sub-problem, incrementally adds views and points  until completion with a robust, outliers free, procedure
Experiments with simulated data shows that our approach is  performing well, both in term of the quality of the reconstruction and the capacity to handle missing data and outliers with a reduced computational time
Finally, results on  real datasets shows the ability of the method to be used in  medium and large scale ND reconstruction scenarios with  high ratios of missing data (up to NN%)
 Notation
Homogeneous coordinates of a vector v are  written as ṽ = [v⊤N] ⊤  and Im×n is the m × n identity matrix
A m × n matrix of 0 (or N) is denoted 0m×n (or Nm×n) and 0n (or Nn) is a n-vector of 0 (or N)
Symbols ⊙ and ⊗ are used for the element-wise and tensor product respectively
The Moore-Penrose pseudo-inverse of a real  vector v is written v+ = v⊤/‖v‖N and its associated skew symmetric matrix is noted [v]  × 
 N
Introduction  Robust factorization methods have been highly successful in delivering a solution to affine Structure from Motion  (SfM) even in the presence of large amounts of missing data  and outliers [NN, NN]
However, the Projective Structure  from Motion (PSfM) [NN] problem still entails difficulties  and despite considerable efforts, there are clear limitations  in current approaches [NN, N, NN, NN, N, NN, NN, NN, N, N, NN]
 These problems span from the non-linearity given by the  perspective camera model to the relevant presence of missing data, noise and outliers in the measurement matrix  containing the ND observations
These nuisances have restricted the applicability of PSfM to relatively small ND reconstruction scenarios with few points and small percentages of missing data
Differently, this paper shows how  PSfM can be solved for challenging real datasets by lessening the non-linearities of previous approaches
 In detail, given f images of a scene and correspondences between a set of n image points in multiple-views, SfM es- timates the ND position of each point and the camera poses
 The simplest instance of SfM adopts affine cameras for ND  projection that leads to a bilinear model in the form of:  M = PS
The measurement matrix M (of size Nf × n) contains the homogeneous image projections m̃i,j while P  (of size Nf×N) represents the vertical concatenations of the N × N camera matrices Pi and S (of size N × n) is the hori- zontal concatenation of the homogeneous ND points s̃j 
As  M is resulting from a product of fixed size matrices, a rankN constraint exists and it has been used in [NN] to factorize such matrix into (P, S) up to an ambiguity using standard computational tools (e.g
Singular Value Decomposition –  SVD)
This factorization approach to the SfM problem has  been successfully applied to obtain a global solution, meaning that all the data is used at once, and usually providing  closed-form solution without the need of an initialisation
 However the affine model restricts applicability to specific scenarios while current challenges in computer vision  go towards reconstructing large scenes where the assumptions of affine cameras are no longer satisfied
Upgrading  the camera model to perspective leads image projections  that also depend on the ND points depths with respect to  the camera, resulting in a different problem defined as:  M⊙ (D⊗ NN) = P S, (N)  N NN    where D is a f × n matrix containing the projective depths for all projections
 Moreover, when dealing with images having wide baselines, it is rather common to have ND points occluded either  by the scene itself or because being out of the camera field
 As a consequence, the matrix M is often incomplete with  some of its entries missing
Completing these entries leads  to an NP-hard problem [NN, N0] that can be defined as:  (Z⊗ NN)⊙M⊙ (D⊗ NN) = (Z⊗ NN)⊙ (P S), (N)  where the f×n binary matrix Z indicates the known entries
These missing correspondences can also result from failures in matching the image projections of the ND points
 Mismatches or extremely noisy correspondences can also  be present and are usually referred to as outliers
Once detected, they can be removed by nullifying the corresponding  entries in Z
The presence of the projective depths D, missing data and outliers are all aspects that have to be dealt with  in order to provide a successful method for PSfM
 N.N
Related Work  Tomasi and Kanade [NN] proposed the first factorization  based approach to SfM using orthographic cameras without missing data
A first estimate of the low-rank bilinear  components was obtained through SVD and afterwards a  metric correction based on constraints raising from the orthographic camera model was used to recover the ND structure solely from image trajectories
Considering multi-view  geometry relations, Sturm and Triggs [NN, N0] proposed the  first extension to perspective cameras by finding a projective depths matrix D which allows the SVD to factorize  M ⊙ (D ⊗ NN) as a product of two rank N matrices
To compute D, pairwise fundamental matrix estimations were  linked together, which can result into accumulation of errors
Moreover, [NN] showed that this method can sometime  converge to useless results
 There have been several attempts to improve Sturm and  Triggs solution [NN, N, NN, NN, N, NN, NN, NN, N, N] providing, in most of the cases, iterative methods given the nonlinearity of the problem
Instead of using pairwise relations  to compute D, such local iterative approaches usually start  initializing D = Nf×n and then adjusting D using the rank constraint while optimizing the reprojection error
These  approaches differ mainly by the constraints used to prevent  the convergence to trivial and ill-conditioned solutions, except for [N] which proposes a SDP formulation based on a  trace norm minimization making it suitable for global optimization
Only few of the mentioned methods [NN, NN, N, N]  try to tackle projective reconstruction with missing data
 Recently, Hong et al
[NN] presented a projective bundle adjustment method based on a Variable Projection approach  from an arbitrary initialization
Convergence to trivial solutions is prevented by a penalty term discouraging update  along the column space of the initial P
 Given previous attempts to solve the problem, it was becoming clearer that more attention needed to be posed on  the constraints over D
Using multi-view geometry considerations, Nasihatkon et al
[N0] rightly pointed out in the  Generalized Projective Reconstruction Theorem (GPRT)  that only specific configurations of the projective depths  matrix D can provide a solution leading to a correct ND  reconstruction
Except for [N], the previous methods mentioned above do not comply fully with this theorem
 Online or incremental methods for matrix factorization  are a preferable choice when the data matrix is of considerable size as shown by [NN, NN] in the case of affine SfM,  especially to handle outliers [NN]
However, such a solution, computationally viable and reconstruction friendly, is  still not available for PSfM in the literature and in the next  section we will show our contributions to this end
Our approach is related to [NN] limited to Henneberg constructive  extensions and adapted to the perspective case
 N.N
Proposed Approach and Contributions  First, we make explicit that (P, S) already contain the projective depths information thus being useless to reestimate such parameters as done in most previous approaches
This results in a more compact parametrisation  of the PSfM problem which is still bilinear
Moreover, we  show that a generalisation of the step-like mask constraint  on the projective depths of [N0] can be linearly transferred  to the projective estimation of (P, S)
This leads to efficient optimization based on alternating simple standard linear  Least Squares minimizations
 Then, similarly to the affine case [NN], our method adopts  an incremental procedure to solve the PSfM problem
This  strategy is key to success in the presence of outliers and high  ratio of missing data since it allows to select parts which  are solvable through a robust, RANSAC-based, fitting procedure to remove outliers which are then treated as missing  data
In this regard, we demonstrate, for the first time, that  PSfM can deal with large scale scenarios typical of the most  advanced bundle adjustment based pipelines [NN]
 N
Compact Factorization Based Formulation  We now present in this section a formulation of the  PSfM problem where the projective depths are eliminated
 This leads to the core linear Least Squares systems that are  the building blocks for our incremental efficient and robust  pipeline to solve the PSfM problem
 N.N
Projective Parameters Fundamental Relations  Let X and Q be the respective estimation of P and S up to  a N× N invertible projective ambiguity Y meaning X = PY and Q = Y−NS
An estimation of the projective parame- ters of a point sj (or a camera Pi) is then the N-vector qj  N0    corresponding to the column j of Q (or the N× N matrix Xi corresponding to rows Ni− N to Ni of X)
The fundamental relation between the projective parameters Xi and qj , the  projective depth di,j of point j in view i and the ND projec- tion mi,j is given by  di,jm̃i,j = Xiqj 
(N)  Having an estimation of the projective parameters Xi and  qj , it results that the projective depth can be estimated as  di,j = m̃ +  i,jXiqj 
(N)  Eliminating the projective depths di,j from Eq
(N) can be done as in the DLT method [NN] using the cross product  resulting in  E [m̃i,j ]× Xiqj = 0, (N)  where E is a N × N matrix containing the two first rows of the identity and is used to remove the linear dependency  between the third line and the first two
 Note that DLT leads to minimizing the algebraic error  and, following [NN], an appropriate normalization of the  data is necessary and introduced in Sec
N
Another elimination method can be obtained using Eq
(N) to substitute  the projective depths in Eq
(N)
While this provides a MLE  similar to the reprojection error, it seems less accurate experimentallyN
 Assuming we know the projective parameters of v views where the image projections of the point j are visible, the corresponding projective parameters qj must satisfy       E [m̃N,j ]× XN ..
 E [m̃v,j ]× Xv     qj = 0Nv
(N)  If the view i contains the image projections of p points for which estimations of their projective parameters are available, then its projective parameters Xi vectorized row by  row as xi are such that       E [m̃i,N]× GN ..
 E [m̃i,p]× Gp     xi = 0Np , Gj  ⊤ =      qj 0N 0N 0N qj 0N 0N 0N qj     
(N)  These two systems are homogeneous and linear in either qj or xi
They can be written generically as  Ay = 0, (N)  where A is of size Nv×N (point case) or Np×NN (view case)
 NCheck the supplementary material for more details
 (a) Tiles, each one has  norm or sum of some  elements fixed
 (b) Step-like mask, the  dots are the fixed entries
 (c) Cross-shaped matrix, the dots are the  only non-null entries
 Figure N
The black rectangular boxes represent the matrix D containing the projective depths for N cameras (rows) and NN points  (columns)
Dots represent single entries while small boxes are  tiles that contain possibly more than one entry
(b) and (a) show  examples of valid constraints
(c) is an invalid configuration of D
 N.N
Projective Parameters Constraints  In this section, we propose a new set of linear constraints  on the projective parameters which satisfy the conditions to  be reconstruction friendly with respect to the GPRT [N0]
 This theorem states that D must be diagonally equivalent to  the true depth matrix and satisfy the following conditions:  no null column or row and not cross-shaped, meaning a null  matrix except for a cross as in Fig
N(c)
 Using the same tiling as in Fig
N(a), for each tile we constrain the projective parameters of the corresponding point  or view to be estimated such that (  N  kv  kv∑  i=N  m̃+i,jXi  )  ︸ ︷︷ ︸  =c⊤  qj = N or     N  kp  kp∑  j=N  m̃+i,jGj      ︸ ︷︷ ︸  =c⊤  xi = N
 (N)  Note that the measurements must be available for all the  projection considered into this sum
However we do not  have to necessarily consider all the visible projectionsN
 These constraints can then be used to substitute one of  the projective parameters in Eq
(N) or Eq
(N), resulting in  a standard linear system which is faster to solve
Doing the  substitution to remove yN, the first entry of y in Eq
(N), we have to split A, y and c as  A = [ a A′  ] , y =  [ yN z  ]  and c =  [ cN c′  ]  , (N0)  where a and A′ are the first and remaining columns of A
 Then after the substitution, we need to minimize  Bz = b with  {  B = A′ − ac′ ⊤ /cN  b = −a/cN , (NN)  for the unknown vector z which is then a minimal  parametrization of the projective parameters that contains  only three degrees of freedom for a point and eleven for  a view
The resulting minimal or overdetermined linear  system can be solved or minimized efficiently in the Least  Squares sense after which we can retrieve yN as  yN = N  cN  (  N− c′ ⊤ z )  
(NN)  NN    From Eq
(N), our constraints can be transferred to the  projective depths
When the sum contains only the last element of each tile, they are actually equivalent to the steplike constraints presented in [N0] and illustrated in Fig
N(b),  which corresponds to the tiling of Fig
N(a)
This generalization was required as the projection of the last element of  each tile is not always visible and it has the advantage of  using all the data to build the constraints
To prevent crossshaped degeneracies, we impose in Sec
N.N the first tiles to  contain fixed entries forming a N × N tetris step-like block coloured green in Fig
N
As this sub-block cannot be crossshaped, the final reconstruction cannot be either
 N
Practical Projective SfM (PNSfM)  We describe here our approach to estimate the projective factors X and Q minimizing ∑  i,j∈Z ‖E [m̃i,j ]× Xiqj‖ N N  subject to the constraints of Eq
(N)
A graphical illustration  is provided in Fig
N and important details on each step are  given from Sec
N.N to N.N
 N.N
Overview of the Proposed Method  Before starting, the image projections are normalized to  improve the conditioning of the linear Least Squares systems to be solvedN
It is more computationally efficient to  compute all m̃+i,j and E [m̃i,j ]× only once and store them into sparse data matrices
The method then starts with  an initial sub-problem (Sec
N.N) and iterates by robustly  adding missing tiles (Sec
N.N) where each tile corresponds  to either a view (N-rows) or a point (a column)
Multiple  views or points can be added at the same time and the procedure continues until no further tile can be added
Searching for tiles to be added depends on the number of visible  projections and eligibility thresholds which are dynamically  adjusted (Sec
N.N)
After each inclusion, the reconstruction  is refined by re-estimating all the points and views already  added (Sec
N.N)
The complete method is then given in  Alg
N
The result is a normalized projective reconstruction  satisfying the GPRT [N0] and the set of inlier projections
 N.N
Initial Sub-Problem Selection and Estimation  The initial sub-problem can be of arbitrary size but in  general it is preferable to start from minimal configurations
 In such case, we need to find a set of frames and points, i.e
 a matrix sub-block as in Fig
N(a) that can be robustly solved  to get a valid initial projective reconstruction
This is done  in the standard way with robust fundamental matrix estimation [NN] after selecting two views using the pyramidal score  from [NN] in a way similar to the affinity score of [NN]
If  by chance the robust estimation of the fundamental matrix  fails, we move to the next higher score until a solvable submatrix is found
 NDetails on this step are given in the supplementary materials
 Algorithm N: Practical Projective SfM (PNSfM)
 N Normalize projections and compute data matrices ;  N Find an initial sub-problem and robustly solve it, see  sec
N.N ;  N while reconstruction is not complete and  (reconstruction was extended or an eligibility  threshold can be decreased) do  N Find currently eligibles views, see sec
N.N ;  N Try to add eligibles views robustly, see sec
N.N ;  N if at least one eligible view has been added then  N Increase the eligibility threshold for points ;  N Refine locally the reconstruction, see sec
N.N ;  N else if no view was eligible then  N0 Decrease the eligibility thresholds for views if  not minimum ;  NN Find currently eligibles points, see sec
N.N ;  NN Try to add eligibles points robustly, see sec
N.N ;  NN if at least one eligible point has been added then  NN Increase the eligibilty thresholds for views ;  NN Refine locally the reconstruction, see sec
N.N ;  NN else if no point was eligible then  NN Decrease the eligibility threshold for points if  not minimum ;  NN Refine globally the reconstruction, see sec
N.N ;  After extracting the epipolar geometry from the estimated fundamental matrix as in [NN, N0], an SVD of the  sub-matrix can be used to compute the projective parameters of the initial two views and the inlier points
The resulting projective parameters are then balanced to match the  constraints as defined in Sec
N.N
 N.N
Finding Eligibles Views and Points  Finding the views or points to add next is a critical issue
In order to do so, we first define a point or view as  known if an estimation of its projective parameters is available
Initially known points and views are therefore given  by the solution of the initial sub-problem
We then call a  point (or view) eligible if there are more visible projections  in known views (or points) than a given eligibility threshold, which is different for points and views
For views, we  also compute the pyramidal score [NN] of the visible known  points and reject them if below a threshold
 If the eligibility thresholds are set too high, which is desirable as it usually gives better estimations, it might happen  that no point or camera is eligible
In order to limit premature interruptions of the algorithm, the eligibility thresholds  are dynamically adjusted in Alg
N between an initial high  value and a minimum value both provided by the user
We  also included a rejection mechanism for points or views that  NN    (a) Initial sub-problem (green)
(b) Adding one view (blue)
(c) Adding three points (blue)
(d) Final reconstruction and inliers
 Figure N
Example of the incremental procedure to reconstruct a scene with N views and NN points
The dots indicates visible projections,  red ones are outliers
We start in (a) with a previously solved sub-problem of N views and N points in green, grey tiles indicates data not  yet considered
Then at each step, green tiles are the current reconstruction and tiles currently added to expand it are in blue
For instance,  in (b) we robustly add a view, automatically removing an outlier projection
In (c) we then robustly add three points
This is repeated until  we reach a final outliers free reconstruction in (d)
 previously failed the robust estimation (see next section)
 As a consequence, another condition to be eligible is that  the number of visible projections is greater than when the  last failure happened
 N.N
Robustly Adding a Point or View  Our method is based on minimizing the linear Least  Squares systems of Eq
(NN) to estimate the projective parameters, which are known to be sensible to outliers due to  mismatches or strong noise
To deal with this, the estimation is done robustly using a Locally Optimized RANSAC  [N] with the MSAC score [NN] and an adaptive stopping criterion given a minimum confidence of finding the optimal  inlier set
Projections detected as outliers are then removed  from the measurements matrix and treated as missing data
 During this procedure, we reject any random subset leading to a rank deficient B, a bad condition number of B or an  excessive error in Eq
(NN)
The two first cases can happen  with degenerate configurations of points or views but more  frequently when estimating a view [NN]
In order to get better estimation from the random subset, we also increased  its size
After selecting the inlier set of the visible projections by using a threshold on the reprojection errors, we  prune projections for which the projective depth is negative  or null
Finally we also reject the estimation if the resulting  inlier set is smaller than the random subset
 If no correct estimation can be found before a given maximum number of iterations, we temporary reject the view or  point
When new projections will be available for this view  or point, we try to add it again, ensuring the random subsets  contain at least one of the new projections
This is necessary as a complete random subset would most likely contains only previously rejected projections if there are just a  few new projections
The estimation would then fail as they  have already been through this procedure once
 N.N
Reconstruction Refinement  Because an incremental procedure does not consider all  the information at once, it can be affected by errors accumulation while iterating
To prevent this, we refine the  overall reconstruction after trying to add eligible points (or  views) if any addition was successful
The refinement is  done by alternating new estimations of all the projective parameters, starting from views (or points), and continue until the overall change in the projective parameters is small  enough
This is done without the robust procedure but using only projections previously accepted as inliers
While  re-estimating, we use the same visible projections to build  the constraints as when the points or views were first added
 A similar method was proposed in [N] but without any constraints on the projective depths
To speed up the process  while doing the completion, the refinement is done only  over the views and points added in the two last iterations  of the main loop
The last refinement in line NN of Alg
N  provides the final reconstruction and it is done over all the  estimated points and views
 N
Experimental Results  We validated the practicability of our approach with  both synthetic and real experiments evaluating performance  in realistic cases with high percentages of missing data  and outliers
We compared our method (PNSfM) with [N]  (YDHL) and [NN] (VarPro) that consistently outperform previous works thus making adequate the comparison with  these methods only
 N.N
Synthetic Dataset Results  To evaluate the proposed approach, N00 simulated se- quences were generated with a missing data pattern that  models points falling out from the cameras field of view  as it is advisable to avoid randomly removed matches [N]
 For each sequence, the ND shape was obtained by randomly  generating N00 points inside a cube of unit dimension
A set of NN cameras was simulated from random intrinsic and extrinsic parameters inside realistic ranges
Cameras were  placed randomly in a N.NN units cube, looking at a random  position inside a 0.N unit cube
Focal lengths are drawn  from [NN00; N0N00] pixels and sensor widths range from N00 to NN00 pixels with a N.NN or N.N aspect ratio
We ensured that each point was seen at least in four views and each view  contained at least NN points projections
 NN    To achieve exactly the tested ratios (from N0% to NN%), we removed very few random entries when necessary
Finally, noise was simulated with a centred Gaussian on each  visible image projection
For evaluating results, the ND error on one sequence is calculated as ∥ ∥S− SGT  ∥ ∥ F / ‖S‖F  after registering the estimated ND points with Procrustes  analysis
The ND error is computed as the root mean square  (rms) of all the reprojection errors
All errors are then averaged over all the sequences of the dataset
 N.N.N Robustness to Outliers  For this experiment, we generated up to eight outliers by  replacing randomly some projections with random coordinates inside the corresponding views
While all methods  have a very small reprojection error without outliers, even  one is enough to decrease drastically the performance of  previous works as it can be observed in Fig
N(a)
This impacts also the ND points reconstruction error which grow  quickly for them in Fig
N(b)
Differently, our approach  shows strong resilience to increasing number of outliers
 Note that for a given sequence, previous works return a  result for all points and views or for none of them while our  method always gives a reconstruction where some points  or views might be unestimated due to the rejection mechanism of Sec
N.N
In Fig
N(c), the entire synthetic dataset is  considered and the percentage of unestimated points corresponds to the number of failed sequences for YDHL  and VarPro and the cumulative unestimated points for our  method
We see that VarPro fails less often than YDHL and  is not afflicted much by a few outliers
PNSfM is unaffected  at all by outliers, the small percentage of unestimated points  is constant and induced by noise
 N.N.N Running Time Comparisons  Running times were obtained on a laptop having an intel  core iN-NN00HQ processor and NNGB memory
No outliers were added and the missing data ratio was kept to  N0%
Fig
N(a) shows all algorithms performance on small scale datasets of growing size
For each dataset size, ten  sequences were run and the average time is given
Both  VarPro and PNSfM are way faster than YDHL, clearly  demonstrating that including the projective depths as parameters of the problem is computationally expensive
 When dealing with medium scale sequences, Fig
N(b)(c)  show the behaviour when increasing the number of points  and views respectively
For each size, the running time is  averaged over five sequences
The online procedure and  LLS minimization are keys to reduce computational costs  compared to VarPro
Due to high memory usage, YDHL  could not be run on the three biggest sequences and the three  smallest gave no result after N hours of computation
 Notice that all methods have been implemented in MATLABN with no parallelization involved except for subroutines natively supporting it
Using another language and the  shared memory paradigm, PNSfM can be massively accelerated by estimating points (or views) in parallel
 N.N.N Missing Data and Noise Effect  Fig
N(a) shows the evolution of the ND reprojection error  when increasing the noise level from a 0 to N pixel standard  deviation at two ratios of missing data (NN% and N0%)
Both VarPro and PNSfM outperform YDHL even in the noise-free  case where they achieve an almost perfect reconstruction
 They have similar behaviour for noise growing up to N pixel,  and then the robust estimation of PNSfM starts filtering projections with high noise resulting in a decreased error
 The behaviour of the ND structure error is displayed on  Fig
N(b) when the missing data ratio grows from N0% to NN% in presence of noise (σ = 0.N or N.N pixels)
With a low noise, VarPro and PNSfM have a similar evolution with  low errors
Due to the limited size of the sequences, when  the noise is higher and projections are filtered by the robust  estimation, few data remain available to the LLS estimation in PNSfM and results in an higher error
In both cases,  YDHL achieves the lowest accuracy
 N.N
Real Data  In Tab
N, we also evaluated PNSfM on various real  datasets of different size available in the literature
When  necessary, feature extraction and matching have been done  off-line once for all methods prior to reconstructions using  the first stage of COLMAP [NN] and we built the measurement matrix from the output
To obtain an Euclidean reconstruction from the projective one, we used the metric upgrade method of [N]
Given timings do not include the time  required for all these steps
Further results are provided in  the supplementary materials
 N.N.N Small Scale Sequences  Six small scale sequences containing less than a million entries in the measurements matrix M were evaluated and results are given in Tab
N
PNSfM outperforms VarPro and  YDHL on both the ND rms reprojection error and the running time for all sequences
An example of the ND reconstruction obtained is given on Fig
N(b) for the famous Dino  sequence
It shows a dinosaur toy being rotated in front  of a camera, which results in elliptical trajectories for the  completed measurements as seen on Fig
N(a)
We used the  NNNN points experiment since the smaller Dino sequence is  mostly suited for affine structure from motion approaches  [N] and it has a low missing data ratio
 NOur implementation is freely available online at https://  bitbucket.org/lmagerand/ppsfm
 NN  https://bitbucket.org/lmagerand/ppsfm https://bitbucket.org/lmagerand/ppsfm   0 N N N N outliers number  0  N00  N00  N D   e rr  o r   (p ix  .) P NSfM 0.N  PNSfM N.N  YDHL 0.N YDHL N.N  VarPro 0.N VarPro N.N  (a) ND reprojection error
 0 N N N N  outliers number  0  0.N  0.N  N D   e rr  o r  P N SfM 0.N  P N SfM N.N  YDHL 0.N  YDHL N.N  VarPro 0.N  VarPro N.N  (b) ND points error
 0 N N N N  outliers number  0  N0  N00  u n e s ti  m a te  d  %  P N SfM 0.N  P N SfM N.N  YDHL 0.N  YDHL N.N  VarPro 0.N  VarPro N.N  (c) Unestimated points
 Figure N
Behavior with outliers for the reprojection error (a), the ND structure error (b) and the number of unestimated points (c) at two  different standard deviations of the noise (σ = 0.N and σ = N.N pixels) and N0% of missing data
YDHL and VarPro are quickly and  strongly afflicted by outliers while PNSfM is almost unaffected thanks to the RANSAC based estimation
 N0xN00 NNxNN0 N0xN00 NNxNN0 problem size  0  N00  N00  ti m  e  (  se c .) PNSfM  YDHL VarPro  N0xN00  (a) Small scale sequences  N N0 NN N0 NN points number x N0N  0  N0  N00 ti  m e  (  m in  .) P NSfM  VarPro  N  (b) With N00 views
 N0 N00 NN0 N00 NN0 N00 cameras number  0  N00  N00  N00  ti m  e  (  m in  .)  PNSfM  VarPro  (c) With N000 points
 Figure N
Running times of PNSfM compared to YDHL and VarPro on small scale sequences (a)
On the larger scale experiments, timings  are reported with increasing number of points (b) or views (c) for PNSfM and VarPro
If on small scale sequences VarPro is faster (a),  PNSfM has a clear advantage on larger scale sequences (b)(c)
YDHL is the slowest and cannot even handle medium scale sequences
 0 0.N N N.N N noise level (pix.)  0  N  N  N  N  N  N D   e rr  o r   (p ix  .)  PNSfM NN% PNSfM N0%  YDHL NN% YDHL N0%  VarPro NN% VarPro N0%  (a) ND reprojection error
 N0 NN N0 NN N0 NN missing %  0  0.00N  0.0N  0.0NN  N D   e rr  o r  PNSfM 0.N PNSfM N.N  YDHL 0.N YDHL N.N  VarPro 0.N VarPro N.N  (b) ND points error
 Figure N
Noise level effect on the ND reprojection error (a) and  behaviour of the error on ND structure (b) with increasing missing  data ratio
Both VarPro and PNSfM outperform YDHL
 (a) Completed measurements
(b) ND reconstruction
 Figure N
The dinosaur sequence
(a) shows the ND image trajectories after completion with a random colour for each one, making  evident the rotational motion of the dino
(b) presents the ND reconstruction after metric upgrade where the colours gradient corresponds to the depth along the reconstruction principal axis
 N.N.N Medium and Large Scale Sequences  As seen in Tab
N (the last four rows), existing PSfM  approaches are unable to reconstruct any of the medium  or large scale sequences evaluated which contain millions  of entries in the measurements matrix M
VarPro could  not complete them before exhausting available memory or  reaching a twelve hours time limit
YDHL is already having troubles processing some of the small scale sequences  and was not evaluated here
Differently, our method successfully delivers correct reconstructions, making it the first  PSfM method able to deal with such datasets
We compared  our results to COLMAP [NN], a standard bundle adjustment  based method implemented in C++ using highly optimized  libraries and a camera model with radial distortion
 The first sequence consists of high resolution images of  a cherubim statue [N]
The scene displayed in the second  and third one are parts of Alcatraz, showing a corner of the  courtyard with its water tower and the west side of the main  building [NN]
The feature points for these three sequences  have been extracted and matched using the first stage of  COLMAP
The last sequence is presented in [NN] and it is  taken around the Dome des Invalides in Paris
 A view of the corresponding ND reconstructions are  given in Fig
N and Fig
N (the ND reconstructed models are  available in the supplementary materials)
While COLMAP  achieves a lower reprojection error, we need about half the  time to process the two largest sequences
Given the size  of the Alcatraz West Side sequence (about NN0 millions entries in M), to the best of our knowledge this is the largest  successful test for a PSfM method
 NN    Sequence PNSfM VarPro [NN] YDHL [N] or COLMAP [NN]  Name Size Missing ND error Time (sec.) ND error Time (sec.) ND error Time (sec.)  House (VGG) N0× NNN NN.N% 0.NNNN N.NN 0.NNNN NN.N 0.NNNN N0NN  Y D  H L  Corridor (VGG) NN× NNN N0.N% 0.NNNN N.NN 0.NNNN NNN 0.NNNN NNN  Dinosaur NNN NN× NNN NN.N% 0.NNNN N.NN N.NNNN N0.N N.NNNN N0N  Dinosaur NNNN NN× NNNN N0.N% 0.NNNN NN N.NNNN NNNN Time Limit (NH)  Wilshire (Ponce) NN0× NNN N0.N% 0.N0NN NN 0.NNNN NNNN 0.NNNN NNNN  Blue Teddy Bear (Ponce) NNN× NNN N0.N% 0.N0NN NNN N.NNNN NNNNN Time Limit (NH)  Cherubim [N] NN× NNNNN NN.N% 0.NNNN NNN Time Limit (NNH) 0.NNNN NNN  C O  L M  A P  Alcatraz Courtyard [NN] NNN× NNNNN NN.N% 0.NNNN NNN Out of Memory (NGB) 0.NNNN NNNN  Alcatraz West Side [NN] NNN× NNNNNN NN.N% 0.NNNN NN0N Out of Memory (NGB) 0.NNNN NNNN  Dome des Invalides [NN] NN× NNNNN NN.N% 0.NNNN NNN Out of Memory (NGB) Not Available (no images)  Table N
Real sequences results
PNSfM, VarPro and YDHL were evaluated on six small scale sequences
The results confirm what is  observed on the synthetic dataset, VarPro and PNSfM outperform YDHL
On medium scale, PNSfM was evaluated on four sequences  against VarPro and COLMAP
VarPro could not provide a result for these sequences while COLMAP is usually slower than PNSfM
 (a) Cherubim
(b) Alcatraz Courtyard
(c) Dome des Invalides
 Figure N
Reconstructions obtained with PNSfM for the medium scale sequences
All PNSfM reconstructions are convincing with respect to  the scene observed
The colours gradient corresponds to the depth along the reconstruction principal axis
 Figure N
The Alcatraz West Side sequence reconstructed using  PNSfM and three sample images over the NNN
This is a reasonable  reconstruction of NNNNNN points obtained in only NN minutes
 N.N
Implementation Details  In order to obtain best results, we recommend setting the  minimum eligibility thresholds to at least NN for views and N for points whenever possible, and advise to not set them below NN and N respectively
We used initials values of NN for views and NN for points
The parameters for the robust estimation were: an outlier threshold of N pixels, a maxi- mum number of iterations fixed at N000, and a confidence  of NN.NN% of having found the optimum set on early exit
During the factorization, the refinement is halted when the  change in the projective parameters is less than N0−N or N0 iterations have been done
The final refinement is made with  a threshold of N0−N over the parameters change or a maxi- mum of NN0 iterations
 N
Conclusion  This paper presented PNSfM, an efficient method to solve  the PSfM problem in the case of strong ratios of missing data and relevant outliers corrupting the measurements
 Constraints has been included to comply with the GPRT,  ensuring a correct projective reconstruction
The method  was tested against challenging real scenarios with up to  NN% missing data ratio and it has shown comparable or bet- ter performance with respect to previous PSfM approaches,  making it a practical PSfM method
Future work will be  dedicated in adapting the method to hierarchical approaches  such as [NN, NN]
To improve error containment, a global  non linear refinement will be integrated
Detecting and  merging similar points tracks as COLMAP could also increase the quality of the reconstruction
Finally, to further  improves efficiency, a parallelized implementation is also  considered
 NN    References  [N] NDflow SRL
NDF Zephyr reconstruction showcase
http:  //www.Ndflow.net/, N0NN
N, N  [N] S
Bhojanapalli and P
Jain
Universal matrix completion
 In The NNst International Conference on Machine Learning  (ICML N0NN), N0NN
N  [N] A
M
Buchanan and A
W
Fitzgibbon
Damped newton algorithms for matrix factorization with missing data
In Computer Vision and Pattern Recognition (CVPR), IEEE Conference on, volume N, pages NNN–NNN, N00N
N  [N] M
Chandraker, S
Agarwal, F
Kahl, D
Kriegman, and  D
Nister
Autocalibration via rank-constrained estimation of  the absolute quadric
In Computer Vision and Pattern Recognition (CVPR), IEEE Conference on, Minneapolis, N00N
N  [N] Q
Chen and G
Medioni
Efficient iterative solution to mview projective reconstruction problem
In Computer Vision  and Pattern Recognition (CVPR), IEEE Conference on, volume N, NNNN
N, N, N  [N] S
Christy and R
Horaud
Euclidean shape and motion  from multiple perspective views by affine iterations
Pattern Analysis and Machine Intelligence, IEEE Transactions  on, NN(NN):N0NN–NN0N, NNNN
N, N  [N] O
Chum, J
Matas, and J
Kittler
Locally optimized ransac
 In DAGM-Symposium, pages NNN–NNN, N00N
N  [N] Y
Dai, H
Li, and M
He
Element-wise factorization for nview projective reconstruction
In European Conference on  Computer Vision (ECCV), volume NNNN of Lecture Notes in  Computer Science, pages NNN–N0N, N0N0
N, N  [N] Y
Dai, H
Li, and M
He
Projective multiview structure and motion from element-wise factorization
Pattern  Analysis and Machine Intelligence, IEEE Transactions on,  NN(N):NNNN–NNNN, Sept N0NN
N, N, N, N  [N0] N
Gillis and F
Glineur
Low-rank matrix approximation  with weights or missing data is np-hard
SIAM Journal on  Matrix Analysis and Applications, NN(N):NNNN–NNNN, N0NN
 N  [NN] R
Hartley and F
Kahl
Critical configurations for projective  reconstruction from multiple views
International Journal of  Computer Vision, NN(N):N–NN, N00N
N  [NN] R
Hartley and A
Zisserman
Multiple View Geometry in  Computer Vision
Cambridge Univ
Press, N00N
N, N  [NN] A
Heyden, R
Berthilsson, and G
Sparr
An iterative factorization method for projective structure and motion from  image sequences
Image and Vision Computing, NN(NN):NNN–  NNN, November NNNN
N, N  [NN] J.-H
Hong, C
Zach, A
Fitzgibbon, and R
Cipolla
Projective bundle adjustment from arbitrary initialization using  the variable projection method
In European Conference on  Computer Vision (ECCV), N0NN
N, N, N, N  [NN] H
Jia and A
M
Martinez
Low-rank matrix fitting based on  subspace perturbation analysis with applications to structure  from motion
Pattern Analysis and Machine Intelligence,  IEEE Transactions on, NN(N):NNN–NNN, N00N
N, N  [NN] F
Jiang, M
Oskarsson, and K
Astrom
On the minimal  problems of low-rank matrix factorization
In Computer Vision and Pattern Recognition (CVPR), IEEE Conference on,  June N0NN
N  [NN] R
Kennedy, L
Balzano, S
J
Wright, and C
J
Taylor
Online algorithms for factorization-based structure from motion
Computer Vision and Image Understanding, September  N0NN
N, N  [NN] S
Mahamud, M
Hebert, Y
Omori, and J
Ponce
Provablyconvergent iterative methods for projective structure from  motion
In Computer Vision and Pattern Recognition  (CVPR), IEEE Conference on, volume N, pages I–N0NN–I–  N0NN, N00N
N, N  [NN] D
Martinec and T
Pajdla
Nd reconstruction by fitting  low-rank matrices with missing data
In Computer Vision  and Pattern Recognition (CVPR), IEEE Conference on, volume N, pages NNN–N0N, N00N
N, N  [N0] B
Nasihatkon, R
Hartley, and J
Trumpf
A generalized  projective reconstruction theorem and depth constraints for  projective factorization
International Journal of Computer  Vision, pages N–NN, N0NN
N, N, N  [NN] D
Nistér, F
Kahl, and H
Stewénius
Structure from motion  with missing data is np-hard
In IEEE NNth International  Conference on Computer Vision (ICCV), pages N–N, N00N
N  [NN] J
Oliensis and R
Hartley
Iterative extensions of the  sturm/triggs algorithm: Convergence and nonconvergence
 Pattern Analysis and Machine Intelligence, IEEE Transactions on, NN(NN):NNNN–NNNN, Dec N00N
N, N  [NN] C
Olsson and O
Enqvist
Stable structure from motion for  unordered image collections
In Image Analysis, pages NNN–  NNN
N0NN
N, N  [NN] M
Oskarsson, K
Batstone, and K
Astrom
Trust no one:  Low rank matrix factorization using hierarchical ransac
 In The IEEE Conference on Computer Vision and Pattern  Recognition (CVPR), June N0NN
N, N, N  [NN] J
L
Schonberger and J.-M
Frahm
Structure-from-motion  revisited
In The IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), June N0NN
N, N, N, N, N  [NN] P
F
Sturm and B
Triggs
A factorization based algorithm  for multi-image projective structure and motion
In European Conference on Computer Vision (ECCV), pages N0N–  NN0, NNNN
N, N, N  [NN] R
Toldo, R
Gherardi, M
Farenzena, and A
Fusiello
Hierarchical structure-and-motion recovery from uncalibrated  images
Computer Vision and Image Understanding, NN0,  November N0NN
N, N  [NN] C
Tomasi and T
Kanade
Shape and motion from image  streams under orthography: a factorization method
International Journal of Computer Vision, N(N):NNN–NNN, NNNN
N,  N  [NN] P
Torr and A
Zisserman
Mlesac
Computer Vision and  Image Understanding, NN(N):NNN–NNN, Apr
N000
N  [N0] B
Triggs
Factorization methods for projective structure  and motion
In Computer Vision and Pattern Recognition  (CVPR), IEEE Conference on, pages NNN–NNN, Jun NNNN
N,  N  [NN] T
Ueshiba and F
Tomita
A factorization method for projective and euclidean reconstruction from multiple perspective  views via iterative depth estimation
In European Conference on Computer Vision (ECCV), volume NN0N of Lecture  Notes in Computer Science, pages NNN–NN0, NNNN
N, N  NN  http://www.Ndflow.net/ http://www.Ndflow.net/Globally-Optimal Inlier Set Maximisation for Simultaneous Camera Pose and Feature Correspondence   Globally-Optimal Inlier Set Maximisation for  Simultaneous Camera Pose and Feature Correspondence  Dylan CampbellN,N, Lars PeterssonN,N, Laurent KneipN and Hongdong LiN  NAustralian National University* NDataNN – CSIRO  {dylan.campbell,lars.petersson,laurent.kneip,hongdong.li}@anu.edu.au  Abstract  Estimating the N-DoF pose of a camera from a single image relative to a pre-computed ND point-set is an important  task for many computer vision applications
Perspective-n- Point (PnP) solvers are routinely used for camera pose es- timation, provided that a good quality set of ND–ND feature  correspondences are known beforehand
However, finding  optimal correspondences between ND key-points and a ND  point-set is non-trivial, especially when only geometric (position) information is known
Existing approaches to the  simultaneous pose and correspondence problem use local  optimisation, and are therefore unlikely to find the optimal solution without a good pose initialisation, or introduce restrictive assumptions
Since a large proportion of  outliers are common for this problem, we instead propose  a globally-optimal inlier set cardinality maximisation approach which jointly estimates optimal camera pose and  optimal correspondences
Our approach employs branchand-bound to search the ND space of camera poses, guaranteeing global optimality without requiring a pose prior
 The geometry of SE(N) is used to find novel upper and lower bounds for the number of inliers and local optimisation is integrated to accelerate convergence
The evaluation empirically supports the optimality proof and shows  that the method performs much more robustly than existing  approaches, including on a large-scale outdoor data-set
 N
Introduction  Estimating the pose of a calibrated camera given a set  of ND points in the camera frame and a set of ND points  in the world frame, as shown in Figure N, is a fundamental part of the general ND–ND registration problem of  aligning an image with a ND scene or model
When correspondences are known, this becomes the Perspectiven-Point (PnP) problem for which many solutions exist [NN, NN, NN, NN, NN]
Applications include camera localisation and tracking [NN, NN, NN], augmented reality [NN], motion segmentation [NN] and object recognition [NN, NN, N]
 *This research is supported by an Australian Government Research Training Program (RTP) Scholarship
 (a) ND point-set (grey and green), ND features (black dots) and groundtruth (black), RANSAC (red) and our (blue) camera poses
The groundtruth and our camera poses coincide, whereas the RANSAC pose has a  translation offset and a NN0◦ rotation offset
Best viewed in colour
 (b) Panoramic photograph and extracted ND features (top), building points  projected onto the image using the RANSAC camera pose (middle) and  building points projected using our camera pose (bottom)
 Figure N
Estimating the pose of a calibrated camera from a single image within a large-scale, unorganised ND point-set captured  by vehicle-mounted laser scanner
Our method solves the absolute pose problem while simultaneously finding feature correspondences, using a globally-optimal branch-and-bound approach with  tight novel bounds on the cardinality of the inlier set
 While hypothesise-and-test frameworks like RANSAC  [NN] can mitigate the sensitivity of PnP solvers to outliers in the correspondence set, few approaches are able to handle the case where ND–ND correspondences are not known  in advance
Unknown correspondences arise in many circumstances, including the general case of aligning an image  with a textureless ND point-set or CAD model
While feature extraction techniques provide a relatively robust and  reproducible way to detect interest points such as edges  or corners within each modality, finding correspondences  across the two modalities is much more challenging
Even  when the point-set has sufficient visual information associated with it, such as colour or SIFT features [NN], repetitive features, occlusions and perspective distortion make  the correspondence problem non-trivial
Moreover, appearN N    ance and thus visual features may change significantly between viewpoints, lighting conditions, weather and seasons,  whereas scene geometry is often less affected
When relocalising a camera in a previously mapped environment or  bootstrapping a tracking algorithm, we contend that geometry is often more reliable
Therefore, there is a need for  methods that solve for both pose and correspondences
 Efficient local optimisation algorithms for solving this  joint problem have been proposed [N, NN]
However, they  require a pose prior, search only for local optima and do  not provide an optimality guarantee, yielding erroneous  pose estimates without a reliable means of detecting failure
Hypothesise-and-test approaches such as RANSAC  [NN], when applied to the correspondence-free problem  [NN], are global methods that are not reliant on pose priors but quickly become computationally intractable as the  number of points and outliers increase and do not provide  an optimality guarantee
More recently, a global and ǫ- suboptimal method has been proposed [N], which uses a  branch-and-bound approach to find a camera pose whose  trimmed geometric error is within ǫ of the global minimum
This work is the first to propose a global and optimal inlier set cardinality maximisation solution to the simultaneous pose and correspondence problem
The approach employs the branch-and-bound framework to guarantee global  optimality without requiring a pose prior, ensuring that it is  not susceptible to local optima
We use a parametrisation  of SE(N) space that facilitates branching and derive novel bounds on the objective function
In addition, we also apply local optimisation whenever the algorithm finds a better  transformation, to accelerate convergence without voiding  the optimality guarantee
Cardinality maximisation allows  an exact optimiser to be found, unlike the ǫ-suboptimality inherent to the continuous objective function used in [N]
 More critically, cardinality maximisation is inherently robust to ND and ND outliers, while avoiding the problems  associated with trimming
The latter requires the user to  specify the inlier fraction, which can rarely be known and is  less intuitive to select than a geometrically meaningful inlier  threshold
If the inlier fraction is over- or under-estimated,  this approach may converge to the wrong pose, without a  means to detect failure
Figure N demonstrates how the  global optimum of a trimmed objective function, as used  by [N, NN], may not occur at the true pose, a problem that is  exacerbated when the inlier fraction is guessed incorrectly
 N
Related Work  A large body of work exists for solving the ND–ND registration problem when correspondences are provided
When  the correspondences are known perfectly, Perspective-n- Point (PnP) solvers [NN, NN, NN, NN, NN] are able to estimate the pose of a camera given a set of noisy image points and  their corresponding ND points
When outliers are present in  Figure N
Two zero-error but incorrect ND alignments of N pointsets with N trimmed ‘outliers’
With noise, the global optimum of  a trimmed objective function may not occur at the true pose, particularly if an incorrect trimming fraction is selected
The problem  is exacerbated with higher dimensions and degrees of freedom
 the correspondence set, the RANSAC framework [NN, N] or  robust global optimisation [NN, NN, N, NN, NN, NN] can be used  to find the inlier set
Alternatively, outlier removal schemes  can make the problem more tractable [NN, N0, N0, N]
Other  methods develop sophisticated matching strategies to avoid  outlier correspondences at the outset [N0, NN, NN, NN]
However, these methods require some correct correspondences
 For this reason, they are often only practical for ND models that have been constructed using stereopsis or Structurefrom-Motion (SfM)
These models associate an image feature with each ND point, facilitating inter-modality feature  matching
Generic point-sets do not have this property; a  point may lie anywhere on the underlying surfaces in a laser  scan, not just where strong image gradients occur
 When correspondences are unknown, the problem becomes more challenging
For the ND–ND case, problems  such as correspondence-free rigid registration [N, N], SfM  [N0, NN, NN] and relative camera pose [NN] have been addressed
For the ND–ND case, solution have been proposed  for registering a collection of images [NN] or multiple cameras [NN] to a ND point-set
The more general problem, however, is pose estimation from a single image
David et al
[N]  proposed the SoftPOSIT algorithm, which alternates correspondence assignment with an iterative pose update algorithm
Moreno-Noguer et al
[NN] proposed the BlindPnP  algorithm, which represents the pose prior as a Gaussian  mixture model from which a Kalman filter is initialised for  matching
It outperformed SoftPOSIT when large amounts  of clutter, occlusions and repetitive patterns were present
 However, both are susceptible to local optima, require a  pose prior and cannot guarantee global optimality
 Grimson [NN] applied a RANSAC-like approach to the  correspondence-free case, removing the need for a pose  prior, but the method is not optimal and quickly becomes  intractable as the number of points increase
In contrast,  globally-optimal methods find a camera pose that is guaranteed to be an optimiser of an error function without requiring a pose prior, but tractability remains a challenge
 A Branch-and-Bound (BB) [NN] strategy may be applied in  these cases, for which bounds need to be derived
For example, Breuel [N] used BB for ND–ND registration problems,  Hartley and Kahl [NN] for optimal relative pose estimation  by bounding the group of ND rotations, Li and Hartley [NN]  N    for rotation-only ND–ND registration, Olsson et al
[NN] for  ND–ND registration with known correspondences, Yang et  al
[NN] for full ND–ND registration and Campbell and Petersson [N] for robust ND–ND registration
While not optimal, Jurie [N0] used an approach similar to BB for ND–ND  alignment with a linear approximation of perspective projection
Brown et al
[N] proposed a global and ǫ-suboptimal method using BB
It finds a camera pose whose trimmed  geometric error, the sum of angular distances between the  bearings and their rotationally-closest ND points, is within ǫ of the global minimum
While not susceptible to local minima, it requires the inlier fraction to be specified, which can  rarely be known in advance, in order to trim outliers
 Our work is the first globally-optimal inlier set cardinality maximisation solution to the simultaneous pose and  correspondence problem
It is guaranteed to find the exact  global optimum without requiring a pose prior and is robust to ND and ND outliers while avoiding the distortion of  trimming
The rest of the paper is organised as follows: we  introduce the problem formulation in Section N, develop a  parametrisation of the domain of ND motions, a branching  strategy and a derivation of the bounds in Section N, propose  an algorithm for globally-optimal pose and correspondence  in Section N and evaluate its performance in Section N
 N
Inlier Set Cardinality Maximisation  Let p ∈ RN be a ND point and f ∈ RN be a bearing vector with unit norm, corresponding to a ND point imaged  by a calibrated camera
That is, f ∝ K−Nx̂ where K is the matrix of intrinsic camera parameters and x̂ is the homogeneous image point
Given a set of points P = {pj}Mj=N and bearing vectors F = {fi}Ni=N and an inlier threshold θ, the objective is to find a rotation R ∈ SO(N) and translation t ∈ RN that maximises the cardinality ν of the inlier set SI  ν∗ = max R, t |SI | (N)  SI = {f ∈ F | ∃p ∈ P : ∠(f , R(p− t)) N θ} (N) where ∠(·, ·) denotes the angular distance between vectors
An equivalent formulation is given by  ν∗ = max R, t  f(R, t) (N)  f(R, t) = ∑  f∈F  max p∈P  N (  θ − ∠(f , R(p− t)) )  (N)  where N(x) , NR≥0(x) is the indicator function that has the value N for all elements of the non-negative real numbers and the value 0 otherwise
The optimal transformation  parameters R∗ and t∗ allow us to find all correspondences  (fi,pj) with respect to θ by identifying all pairs for which ∠(fi, R  ∗(pj − t∗)) N θ
We maximise the cardinality of the set of bearing vector inliers, not the set of ND point inliers, to avoid the degenerate case of all points sharing the  same bearing vector inlier, which occurs when the camera  is translated far away from the point-set
 π  (a) Rotation Domain Ωr  τx  τz τy  (b) Translation Domain Ωt  Figure N
Parametrisation of SE(N)
(a) The rotation space SO(N) is parametrised by angle-axis N-vectors in a solid radius-π ball
 (b) The translation space RN is parametrised by N-vectors bounded  by a cuboid with half-widths [τx, τy, τz]
The domain is branched into sub-cuboids as shown using nested octree data structures
 N
Branch-and-Bound  To solve the highly non-convex cardinality maximisation  problem (N), the global optimisation technique of Branchand-Bound (BB) [NN] may be applied
To do so, a suitable means of parametrising and branching (partitioning)  the function domain must be found, as well as an efficient  way to calculate upper and lower bounds of the function for  each branch which converge as the size of the branch tends  to zero
While the bounds need to be computationally efficient to calculate, the time and memory efficiency of the  algorithm also depends on how tight the bounds are, since  tighter bounds reduce the search space quicker by allowing  suboptimal branches to be pruned
 N.N
Parametrising and Branching the Domain  To find a globally-optimal solution, the cardinality of the  inlier set SI must be maximised over the domain of ND mo- tions, that is, the group SE(N) = SO(N)×RN
However, the space of these transformations is unbounded, therefore we  restrict the space of translations to be within the bounded set  Ωt in order to use BB
For a suitably large Ωt, it is reason- able to assume that the camera centre lies within Ωt
That is, we can assume that the camera is less than a finite distance  from the ND points
The domains are shown in Figure N
 Rotation space SO(N) is minimally parametrised with angle-axis N-vectors r with rotation angle ‖r‖ and rotation axis r/‖r‖
The notation Rr ∈ SO(N) is used to denote the rotation matrix obtained from the matrix exponential map  of the skew-symmetric matrix [r]× induced by r
The Ro- drigues’ rotation formula can be used to efficiently calculate  this mapping
Using this parametrisation, the space of all  ND rotations can be represented as a solid ball of radius π in R  N
The mapping is one-to-one on the interior of the π-ball and two-to-one on the surface
For ease of manipulation,  we use the ND cube circumscribing the π-ball as the rota- tion domain Ωr [NN]
Translation space R  N is parametrised  with N-vectors in a bounded domain chosen as the cuboid Ωt containing the bounding box of P 
If the camera is known to be inside the ND scene, Ωt can be set to the bounding box, otherwise it is set to an expansion of the bounding box
 N    ψr  O  Rr0p  Rrp  (a) Rotation Uncertainty Angle  p− t0  ψt  O  p− t  (b) Translation Uncertainty Angle  Figure N
Uncertainty angles induced by rotation and translation  sub-cubes
(a) Rotation uncertainty angle ψr for Cr 
The optimal rotation of p may be anywhere within the umbrella-shaped region,  which is entirely contained by the cone defined by Rr0p and ψr 
 (b) Translation uncertainty angle ψt for Ct
The optimal transla- tion of p may be anywhere within the cuboidal region, which is  entirely contained by the cone defined by p− t0 and ψt
 During BB, the domain is branched into sub-cuboids using nested octree data structures
They are defined as  C(c, δ)={x ∈ RN | e⊺i (x−c) ∈ [−δi, δi], i = N, N, N} (N) where ei is the i  th standard basis vector
To simplify the  notation, we use Cr = C(r0, δr) and Ct = C(t0, δt)
The uncertainty angle induced by a rotation and translation sub-cuboid on a point p is shown in Figure N
The  transformed point may lie anywhere within an uncertainty  cone, with aperture angle equal to the sum of the rotation  and translation uncertainty angles
 N.N
Bounding the Branches  The success of a BB algorithm is predicated on the quality of its bounds
For inlier set maximisation, the objective  function (N) needs to be bounded within a transformation  domain
Some preparatory material is now presented
 To bound the uncertainty angle due to rotation, Lemmas  N and N from [NN] are used
For reference, the relevant parts  are merged into Lemma N, as in [NN]
The lemma indicates that the angle between two rotated vectors is less than  or equal to the Euclidean distance between their rotations’  angle-axis representations in RN
 Lemma N
For an arbitrary vector p and two rotations,  represented as RrN and RrN in matrix form and rN and rN in  angle-axis form,  ∠(RrNp, RrNp) N ‖rN − rN‖
(N) From this, the maximum angle between a vector p rotated by r0 and p rotated by r ∈ Cr can be found as follows
Lemma N
(Weak rotation uncertainty angle) Given a ND  point p and a rotation cube Cr of half side-length δr centred at r0, then ∀r ∈ Cr,  ∠(Rrp, Rr0p) N min( √ Nδr, π) , ψ  w r (Cr)
(N)  Proof
Inequality (N) can be derived as follows:  ∠(Rrp, Rr0p) N min(‖r− r0‖, π) (N) N min(  √ Nδr, π) (N)  where (N) follows from Lemma N and the maximum possible angle and (N) follows from max ‖r − r0‖ = √ Nδr (the  half space diagonal of the rotation cube) for r ∈ Cr
 However, a tighter bound can be found by observing  that a point rotated about an axis parallel to the point is  not displaced
To exploit this, we maximise the angle  ∠(Rrp, Rr0p) over the surface Sr of the cube Cr
 Lemma N
(Rotation uncertainty angle) Given a ND point  p and a rotation cube Cr centred at r0 with surface Sr, then ∀r ∈ Cr,  ∠(Rrp, Rr0p) N min(max r∈Sr  ∠(Rrp, Rr0p), π) , ψr(p, Cr)
(N0)  Proof
Inequality (N0) can be derived as follows:  ∠(Rrp, Rr0p) N min(max r∈Cr  ∠(Rrp, Rr0p), π) (NN)  = min(max r∈Sr  ∠(Rrp, Rr0p), π) (NN)  where (NN) is a consequence of the order-preserving mapping, with respect to the radial angle, from the convex cube  of angle-axis vectors to the spherical surface patch (see Figure Na), since the mapping is obtained by projecting from  the centre of the sphere to the surface of the sphere
See the  appendix for further details
 The uncertainty angle due to translation can be bounded  by observing that the translated points form a cube (Figure Nb)
When the cube does not contain the origin, the  angle can be found by maximising over the cube vertices
 Lemma N
(Translation uncertainty angle) Given a ND  point p and a translation cube Ct centred at t0 with ver- tices Vt, then ∀t ∈ Ct,  ∠(p− t,p− t0) N {  max t∈Vt  ∠(p− t,p− t0) if p /∈ Ct  π else  , ψt(p, Ct)
(NN)  Proof
Observe that for p ∈ Ct, the cube containing all translated points p − t also contains the origin
Therefore p− t can be proportional to −(p− t0) and thus the maxi- mum angle is π
For p /∈ Ct,  ∠(p− t,p− t0) N max t∈Ct  ∠(p− t,p− t0) (NN)  = max t∈Vt  ∠(p− t,p− t0) (NN)  N    0 N N N N N 0  N0  NN0  NN0  ‖p− t0‖/δt  ψ t (◦ )  (a) Ray through face centre  0 N N N N N 0  N0  NN0  NN0  ‖p− t0‖/δt      ψt  ψwt  (b) Ray through vertex  Figure N
Comparison of translation bounds when the cube centre  lies along a ray from the origin towards (a) any face centre and (b)  any vertex
Our bound ψt is tighter across the entire domain
 where (NN) follows from the convexity of the angle function  in this domain
The maximum of a convex function over a  convex set must occur at one of its extreme points (the vertices)
Geometrically, the cube p− t projects to a spherical hexagon on the unit sphere
The maximum geodesic from a  point in the hexagon to any other is to a vertex
 To avoid the non-physical case where a ND point is located within a very small value ζ of the camera centre we restrict the translation domain such that Ω′t = Ωt ∩ {t ∈ R  N | ‖p− t‖ > ζ, ∀p ∈ P}
The translation bound from [N] encloses a translation  cube with a sphere of radius ρt = √ Nδt and is given by  ψwt (p, Ct) , {  arcsin (  ρt ‖p−t0‖  )  if ρt N ‖p− t0‖ π else
 (NN)  Our bound is tighter with a maximum difference of NNN◦  for cubes and greater for cuboids
Figure N compares both  translation bounds across a range of values
 The preceding lemmas are used to bound the objective  function (N) within a transformation domain Cr × Ct
For brevity, we use the notation prt , Rr(p − t), pt , p − t and fr , (Rr)  −Nf 
 Theorem N
(Lower bound) For the domain Cr×Ct centred at (r0, t0), the lower bound of the inlier set cardinality can be chosen as  ¯ f(Rr, t) , f(Rr0 , t0)
(NN)  Proof
The validity of the lower bound follows from  max r, t  f(Rr, t) > f(Rr0 , t0)
(NN)  That is, the function value at a specific point within the domain is less than or equal to the maximum
 Theorem N
(Upper bound) For the domain Cr×Ct centred at (r0, t0), the upper bound of the inlier set cardinality can be chosen as  f̄(Rr, t), ∑  f∈F  max p∈P  N (  θ−∠(f ,pr0t0 )+ψr(f , Cr)+ψt(p, Ct) )  
 (NN)  fr  pt  pt0  α β γ  O  Figure N
The triangle inequality in spherical geometry, given by  β N α + γ or ∠(fr,pt0) N ∠(fr,pt) + ∠(pt,pt0)
The trans- formed points have been normalised to lie on the unit sphere
 Proof
Observe that ∀(r, t) ∈ (Cr × Ct), ∠(f ,prt) > ∠(f ,p  r0 t0 )− ∠(fr, fr0)− ∠(pt,pt0) (N0)  > ∠(f ,pr0t0 )− ψr(f , Cr)− ψt(p, Ct) (NN) where (N0) follows from the triangle inequality in spherical  geometry (see Figure N) and (NN) follows from Lemmas N  and N
Substituting (NN) into (N) completes the proof
 By inspecting the translation component of Theorem N,  a tighter upper bound may be found by removing one of  the two applications of the triangle inequality
A similar approach cannot be taken for the rotation component  since Rrp is a complex surface due to the nonlinear conversion from angle-axis to rotation matrix representations
 To reduce computation, it is only necessary to evaluate this  tighter bound when ∠(f ,pr0t0 ) N θ+ψr(f , Cr) +ψt(p, Ct), since otherwise the point is definitely an outlier and does  not need to be investigated further
 Theorem N
(Tighter upper bound) For the domain Cr ×Ct centred at (r0, t0), the upper bound of the inlier set cardi- nality can be chosen as  f̄(Rr, t) , ∑  f∈F  max p∈P  Γ(f ,p) (NN)  Γ(f ,p) = max t∈Ct  N (  θ − ∠(f ,pr0t ) + ψr(f , Cr) )  (NN)  Proof
Observe that ∀(r, t) ∈ (Cr × Ct), N (  θ−∠(f ,prt) )  N N (  θ − ∠(f ,pr0t ) + ∠(fr, fr0) )  (NN)  N max t∈Ct  N (  θ − ∠(f ,pr0t ) + ψr(f , Cr) )  (NN)  where (NN) follows from the triangle inequality in spherical  geometry and (NN) follows from Lemma N and maximising  over t
Substituting (NN) into (N) completes the proof
 Γ may be evaluated by observing that the minimum angle between a ray f and a cube pr0t is zero if the ray passes  through the cube and is otherwise the angle between the  ray and the point on the skeleton of the cube (vertices and  edges) with least angular displacement from f 
Thus, for  the translation domain Ct with skeleton Skt,  Γ =  {  max t∈Skt  N (  θ − ∠(f ,pr0t ) + ψr )  if ∠(f ,pr0t0 ) > ψt  N else
(NN)  N    N
The GOPAC Algorithm  The Globally-Optimal Pose And Correspondences  (GOPAC) algorithm for a calibrated camera is outlined in  Algorithms N and N
As in [NN], we employ a nested branchand-bound structure for computational efficiency
In the  outer breadth-first BB search, upper and lower bounds are  found for each translation cuboid Ct ∈ Ωt by running an inner BB search over rotation space SO(N) (denoted RBB)
The upper bound ν̄ , ν̄t (NN) of Ct is found by running RBB until convergence with the following bounds  ¯ νr ,  ∑  f∈F  max p∈P  N (  θ − ∠(f ,pr0t0 ) + ψt(p) )  (NN)  ν̄r , ∑  f∈F  max p∈P  N (  θ − ∠(f ,pr0t0 ) + ψt(p) + ψr(f) )  
(NN)  The tighter upper bound (NN) instead uses  ¯ νr ,  ∑  f∈F  max p∈P,t∈Ct  N (  θ − ∠(f ,pr0t ) )  (NN)  ν̄r , ∑  f∈F  max p∈P,t∈Ct  N (  θ − ∠(f ,pr0t ) + ψr(f) )  
(N0)  The lower bound ¯ ν ,  ¯ νt (NN) is found by running RBB  using bounds (NN) and (NN) with ψt set to zero
The nested structure has better memory and computational efficiency than directly branching over ND transformation space, since it maintains a queue for each ND subproblem, rather than one for the entire ND problem
This  requires significantly fewer simultaneously enqueued subcubes
Moreover, with rotation search nested inside translation search, ψt only has to be calculated once per trans- lation t, not once per pose (r, t), and F can be rotated (by R −N) instead of P which typically has more elements
This  makes it possible to precompute the rotated bearing vectors  and rotation bounds for the top five levels of the rotation  octree to reduce the amount of computation required in the  inner BB subroutine
 Line N of Algorithm N shows how local optimisation is  incorporated to refine the camera pose, in a similar manner to [NN, N]
Whenever the BB algorithm finds a sub-cube  pair (Cr, Ct) with a greater lower bound ¯ ν than half the bestso-far cardinality ν∗, the PnP problem is solved, with corre- spondences given by the inlier pairs at the pose (r0, t0)
We use nonlinear optimisation [NN], minimising the sum of angular distances between corresponding bearing vectors and  points, and update ν∗ if a larger ν is found
In this way, BB and PnP collaborate, with PnP finding the best pose given correspondences and BB guiding the search for correspondences
PnP accelerates convergence since the faster ν∗ is increased, the sooner sub-cubes (with ν̄ N ν∗) can be culled (Alg
N Line NN)
SoftPOSIT [N] is also applied at this stage  to help jump out of local minima
 Algorithm N GOPAC: a branch-and-bound algorithm for  globally-optimal camera pose & correspondence estimation  Input: bearing vector set F , point set P , inlier threshold θ, initial domains Ωr and Ωt  Output: optimal number of inliers ν∗, camera pose (r∗, t∗) and ND–ND correspondences  N: ν∗ ← 0 N: Add translation domain Ωt to priority queue Qt N: loop  N: Update greatest upper bound ν̄t from Qt N: Get cuboid Ct with greatest width δtx from Qt N: if ν∗ > ν̄t then terminate N: for all sub-cuboids Cti ∈ Ct do N: (  ¯ νti, r)← RBB(ν∗, t0i, ψt = 0)  N: if ν∗ < N ¯ νti then (ν  ∗, r∗, t∗)← PnP(r, t0i) N0: ν̄ti ← RBB(ν∗, t0i, ψt) NN: if ν∗ < ν̄ti then add Cti to queue Qt  Algorithm N RBB: a rotation search subroutine for GOPAC  Input: bearing vector set F , point set P , inlier threshold θ, initial domain Ωr, best-so-far cardinality ν  ∗, translation  t0, translation uncertainty ψt Output: optimal number of inliers ν∗r and rotation R  ∗  N: ν∗r ← ν∗ N: Add rotation domain Ωr to priority queue Qr N: loop  N: Read cube Cr with greatest upper bound ν̄r fromQr N: if ν∗r > ν̄r then terminate N: for all sub-cubes Cri ∈ Cr do N: Calculate  ¯ νri by (NN) or (NN)  N: if ν∗r < ¯ νri then ν  ∗ r ← ¯νri, r  ∗ ← r0 N: Calculate ν̄ri by (NN) or (N0)  N0: if ν∗r < ν̄ri then add Cri to queue Qr  As just observed, a large ν∗ reduces runtime
Therefore, if the user knows a lower bound on the number of ND inliers, ν∗ can be initialised to this value
However, this is rarely known
Instead, our algorithm implements an optional guess-and-verify approach, without loss of optimality or objective function distortion, which provides especial benefit when ND outliers are rare: set ν∗ = n; run GOPAC; stop if an optimality guarantee is found, otherwise  n ← max(n − s, 0) and repeat
We initialise n = N − N and s = ⌈0.NN⌉
 We also provide a multi-threaded implementation, where  the initial translation domain is divided into sub-domains  and GOPAC is run for each in separate CPU threads
The  algorithm returns the largest ν∗ and the associated pose and correspondences
While not supplied, a massively parallel  implementation on a GPU is very feasible
Further algorithmic details are provided in the appendix
 N    N
Results  The GOPAC algorithm was evaluated with respect to the  baseline RANSAC [NN], SoftPOSIT [N] and BlindPnP [NN]  algorithms, denoted GP, RS, SP and BP respectively, with  synthetic and real data
The RANSAC approach uses the  OpenGV framework [NN] and the PNP algorithm [NN] with  randomly-sampled correspondences
Since SoftPOSIT and  BlindPnP require pose priors to function, we use a torus  prior in the synthetic experiments
In general, the space of  camera poses is much larger than the restrictive torus prior  and a good prior can rarely be known in advance
Except  where otherwise specified, the inlier threshold θ was set to N◦, the rotation and translation bounds (N0) and (NN) were used, SoftPOSIT and nonlinear PnP refinement were ap- plied and multithreading was not used
It is crucial to observe that finding the global optimum does not necessarily  imply finding the ground-truth transformation
There may  be multiple global optima, particularly in the case of symmetries, and noise may create false optima
 N.N
Synthetic Data Experiments  To evaluate our algorithm in a setting where true priors  can be applied, we performed N0 independent Monte Carlo  simulations per parameter setting, using the framework of  [NN]: M random ND points were generated from [−N, N]N; a fraction ωND of the ND points were randomly selected as outliers to model occlusion; the inliers were projected to a  virtual image; normal noise was added with σ = N pixels; and random points were added to the image such that a fraction ωND of the ND points were outliers
To facilitate fair comparison with SoftPOSIT and BlindPnP, we use a pose  prior for these experiments
The torus prior constrains the  camera centre to a torus around the point-set with the optical axis directed towards the model, as in [NN]
BlindPnP  represents the poses with a N0 component Gaussian mixture  model, the means of which are used to initialise SoftPOSIT,  as in [NN]
GOPAC is given a set of translation cubes which  approximate the torus and is not given the rotation priors
 The results are shown in Figures N and Na
We repeated  the experiments for the repetitive CAD structure shown in  Figure Na, with results shown in Figure Nb
Two success  rates are reported: the fraction of trials where the true maximum number of inliers was found and the fraction where  the correct pose was found, where the angle between the  output rotation and the ground truth rotation is less than 0.N radians and the camera centre error ‖t − tGT‖/‖tGT‖ rela- tive to the ground truth tGT is less than 0.N, as in [NN]
The ND and ND outlier fractions were fixed to 0 when not being  varied and multithreading was used in the ND outlier experiments
GOPAC outperforms the other methods, reliably  finding the global optimum while still being relatively efficient, particularly when the fraction of ND outliers is low
 For the repetitive CAD structure, while GOPAC finds the  N0 N0 N0 N0 N0 N0 0  N  S u  c c e ss   R a te   ( In  li e rs  )  N0 N0 N0 N0 N0 N0 0  N  S u  c c e ss   R a te   ( P  o se  )  N0 N0 N0 N0 N0 N0 0  N0  N0  N0  N0  Number of Points  D u  ra ti  o n   ( s)  (a) ωND = 0  N0 N0 N0 N0 N0 N0  0  N  N0 N0 N0 N0 N0 N0  0  N  N0 N0 N0 N0 N0 N0  0  N0  N0  N0  N0  Number of Points  (b) ωND = 0.NN  N0 N0 N0 N0 N0 N0  0  N  N0 N0 N0 N0 N0 N0  0  N  N0 N0 N0 N0 N0 N0  0  N0  N0  N0  N0  Number of Points  (c) ωND = 0.N  N0 N0 N0 N0 N0 N0  0  N  N0 N0 N0 N0 N0 N0  0  N  N0 N0 N0 N0 N0 N0  0  N0  N0  N0  Number of Points        GP  RS  SP  BP  (d) ωND = 0.NN  Figure N
Mean success rates and median runtimes with respect to  the number of random ND points and the ND outlier fraction, for N0  Monte Carlo simulations per parameter value with the torus prior
 0 0.NN 0.N 0.NN 0  N  S u c c e ss   R a te   ( In  li e rs  )  0 0.N 0.N 0.N  0  N  0 0.NN 0.N 0.NN  0  N  0 0.N 0.N 0.N  0  N  0 0.NN 0.N 0.NN 0  N  S u c c e ss   R a te   ( P  o se  )  0 0.N 0.N 0.N  0  N  0 0.NN 0.N 0.NN  0  N  0 0.N 0.N 0.N  0  N  0 0.NN 0.N 0.NN 0  N  N0  NN  N0  NN  ND Outlier Fraction  D u ra  ti o n  (  s)  0 0.N 0.N 0.N  0  N00  N00  N00  ND Outlier Fraction  (a) Random Points M = N0  0 0.NN 0.N 0.NN  0  N  N0  NN  N0  ND Outlier Fraction  0 0.N 0.N 0.N  0  N00  N00  N00  N00  ND Outlier Fraction      GP  RS  SP  BP  (b) CAD Structure M = NN  Figure N
Mean success rates and median runtimes with respect to  the ND and ND outlier fractions for the random points and CAD  structure datasets, for N0 Monte Carlo simulations per parameter  value with the torus prior
 globally optimal number of inliers in all cases, the pose is  occasionally incorrect when NN% of the ND points are oc- cluded, due to the highly symmetric nature of the model
 The evolution of the global lower and upper bounds is  shown in Figure Nc: BB and PnP collaborate to increase the lower bound with BB guiding the search into better convergence basins and PnP refining the bound by jumping to the nearest local maximum (the staircase pattern)
The majority  of the time is spent decreasing the upper bound, indicating  it will often find the global optimum when terminated early
 To show the improvement attributable to the tighter upper bounds derived, we measured the runtime of the algorithm with N0 random ND points and N0% ND outliers using different upper bounds, shown in Figure N0
The weak  sphere-based bounding functions in (N) and (NN) are denoted  ψwr and ψ w t respectively, the tighter cuboid-based bounding  functions in (N0) and (NN) are denoted ψr and ψt respec- tively and the bounding function from (NN) is denoted Γ
Further results are provided in the appendix
 N    (a) ND Models (b) ND Alignment (c) Bound Evolution  Figure N
Sample ND and ND results for two trials using the random points and repetitive CAD model datasets
(a) ND models,  true and GOPAC-estimated camera fulcra (completely overlapping) and toroidal pose priors
Only non-occluded ND points are  shown
(b) True projections of non-occluded ND points are shown  as black dots, ND outliers as red dots, GOPAC projections as black  circles and GOPAC-classified ND outliers as red crosses
(c) Evolution over time of the upper and lower bounds (black), remaining  translation volume (blue) and translation queue size (green) as a  fraction of their maximum values
Best viewed in colour
 ψw t ψt Γ   R e la  ti v e  R  u n ti  m e      ψw r  ψr  Figure N0
Comparison of the different upper bound functions
 Runtime is plotted relative to the maximum (leftmost) value
The  weakest upper bound is N0% slower than the tightest upper bound
 N.N
Real Data Experiments  To evaluate the algorithm on real data, we use the  DATANN/NDND (formerly NICTA) dataset [NN], a large and  repetitive multi-modal outdoor dataset
Finding the pose  of a camera within a large laser-scanned point-set without a good initialisation represents an unsolved problem  in computer vision, which this work makes progress towards solving
For each image, we obtain the ground truth  camera pose from the provided ND–ND correspondences using EPnP [NN] followed by nonlinear PnP [NN]
Extracting points from a laser scan that correspond to known pixels in  an image is itself a challenging unsolved problem for ND–  ND registration pipelines
Due to the robust and optimal  nature of GOPAC, we can relax this problem to isolating  regions of the point-set that appear in the image and vice  versa, from which putative correspondences may be drawn
 We used semantic segmentations of the images and pointset to select regions that were potentially observable in both  modalities, in this case the ‘building’ class
We then used  grid downsampling and k-means clustering on the class pix- els and points independently to reduce them to a manageable size and converted the pixels to bearing vectors
While  we do not know the correspondences in advance, each bearing vector has a good chance of having a ND point as an  Table N
Camera pose results for the DATANN/NDND dataset
The  median translation error, rotation error and runtime and the mean  inlier recall and success rates are reported
⌊GP⌋ denotes truncated GOPAC, where search is terminated after N0s, with no optimality guarantee
RSK denotes RANSAC with K million iterations
 Method GP ⌊GP⌋ RSN0 RSNN0 Translation Error (m) N.N0 N.N0 N0.N NN.N  Rotation Error (◦) N.0N N.0N NNN NNN  Recall (Inliers) N.00 0.NN 0.NN 0.NN  Success Rate (Inliers) N.00 0.NN 0.00 0.00  Success Rate (Pose) 0.NN 0.NN 0.0N 0.0N  Runtime (s) NNN NN NN NNN  inlier
In this way, we constructed a dataset consisting of a  ND point-set with NN points, a set of NN images containing  N0 ND features and a set of ground truth camera poses
For  this experiment, we used an inlier threshold of θ = N◦, mul- tithreading and a ND outlier fraction guess of ωND = 0.NN
The translation domain was N0×N×Nm, covering two lanes of the road, making use of the knowledge that the camera  was mounted on a survey vehicle
SoftPOSIT and BlindPnP  failed to find the correct camera pose for every image in this  dataset, even when supplied the ground truth pose as a prior,  due to the weak ground truth correspondences and an inability to handle ND points behind the camera
Moreover, they  do not natively support panoramic imagery and required an  artificially restricted field of view to function
 Qualitative results for the GOPAC and RANSAC algorithms are shown in Figure N and quantitative results in  Table N
GOPAC finds the optimal number of inliers for  all frames and the correct camera pose for the majority of  frames, despite the weakness of the ND/ND point extraction  process, surpassing the other methods
The failure modes  for GOPAC were NN0◦ rotation flips, due to ambiguities arising from the low angular separation of points in the vertical direction
The difficulty of this ill-posed problem is  illustrated by the performance of truncated GOPAC, which  was not able to find all optima even after running for N0s,  motivating the necessity for globally-optimal guided search
 N
Conclusion  In this paper, we have introduced a robust and globallyoptimal solution to the simultaneous camera pose and correspondence problem using inlier set cardinality maximisation
The method applies the branch-and-bound paradigm  to guarantee optimality regardless of initialisation and uses  local optimisation to accelerate convergence
The pivotal  contribution is the derivation of the function bounds using  the geometry of SE(N)
The algorithm outperformed other local and global methods on challenging synthetic and real  datasets, finding the global optimum reliably
Further investigation is warranted to develop a complete ND–ND pipeline,  from segmentation and clustering to alignment
 N    References  [N] E
Ask, O
Enqvist, and F
Kahl
Optimal geometric fitting  under the truncated LN-norm
In Proc
N0NN Conf
Comput
 Vision Pattern Recognition, pages NNNN–NNNN
IEEE, N0NN
 N  [N] M
Aubry, D
Maturana, A
A
Efros, B
C
Russell, and  J
Sivic
Seeing ND chairs: exemplar part-based NDND alignment using a large dataset of CAD models
In  Proc
N0NN Conf
Comput
Vision Pattern Recognition, pages  NNNN–NNNN, N0NN
N  [N] P
J
Besl and N
D
McKay
A method for registration of N-D  shapes
IEEE Trans
Pattern Anal
Mach
Intell., NN(N):NNN–  NNN, NNNN
N  [N] T
M
Breuel
Implementation techniques for geometric  branch-and-bound matching methods
Computer Vision and  Image Understanding, N0(N):NNN–NNN, N00N
N  [N] M
Brown, D
Windridge, and J.-Y
Guillemaut
Globally optimal ND-ND registration from points or lines without correspondences
In Proc
N0NN Int
Conf
Comput
Vision, pages  NNNN–NNNN, N0NN
N, N, N, N  [N] D
Campbell and L
Petersson
GOGMA: Globally-Optimal  Gaussian Mixture Alignment
In Proc
N0NN Conf
Comput
 Vision Pattern Recognition, pages NNNN–NNNN
IEEE, June  N0NN
N  [N] T.-J
Chin, Y
Heng Kee, A
Eriksson, and F
Neumann
Guaranteed outlier removal with mixed integer linear programs
 In Proc
N0NN Conf
Comput
Vision Pattern Recognition,  pages NNNN–NNNN, N0NN
N  [N] O
Chum and J
Matas
Optimal randomized RANSAC
 IEEE Trans
Pattern Anal
Mach
Intell., N0(N):NNNN–NNNN,  N00N
N  [N] P
David, D
Dementhon, R
Duraiswami, and H
Samet
 SoftPOSIT: simultaneous pose and correspondence determination
Int
J
Comput
Vision, NN(N):NNN–NNN, N00N
N, N,  N  [N0] F
Dellaert, S
M
Seitz, C
E
Thorpe, and S
Thrun
Structure  from motion without correspondence
In Proc
N000 Conf
 Comput
Vision Pattern Recognition, volume N, pages NNN–  NNN
IEEE, N000
N  [NN] O
Enqvist, E
Ask, F
Kahl, and K
Åström
Robust fitting  for multiple view geometry
In Proc
N0NN European Conf
 Comput
Vision, pages NNN–NNN
Springer Berlin Heidelberg,  N0NN
N  [NN] O
Enqvist, E
Ask, F
Kahl, and K
Åström
Tractable algorithms for robust model estimation
Int
J
Comput
Vision,  NNN(N):NNN–NNN, N0NN
N  [NN] M
A
Fischler and R
C
Bolles
Random sample consensus: a paradigm for model fitting with applications to image  analysis and automated cartography
Communications of the  ACM, NN(N):NNN–NNN, NNNN
N, N, N  [NN] J
Fredriksson, V
Larsson, C
Olsson, and F
Kahl
Optimal  relative pose with unknown correspondences
In Proc
N0NN  Conf
Comput
Vision Pattern Recognition, pages NNNN–  NNNN
IEEE, N0NN
N  [NN] W
E
L
Grimson
Object Recognition by Computer: The  Role of Geometric Constraints
MIT Press, Cambridge, MA,  USA, NNN0
N  [NN] B
M
Haralick, C.-N
Lee, K
Ottenberg, and M
Nölle
Review and analysis of solutions of the three point perspective  pose estimation problem
Int
J
Comput
Vision, NN(N):NNN–  NNN, NNNN
N, N  [NN] R
I
Hartley and F
Kahl
Global optimization through rotation space search
Int
J
Comput
Vision, NN(N):NN–NN, N00N
 N, N  [NN] J
A
Hesch and S
I
Roumeliotis
A direct least-squares  (DLS) method for PnP
In Proc
N0NN Int
Conf
Comput
 Vision, pages NNN–NN0
IEEE, N0NN
N, N  [NN] D
P
Huttenlocher and S
Ullman
Recognizing solid objects by alignment with an image
Int
J
Comput
Vision,  N(N):NNN–NNN, NNN0
N  [N0] F
Jurie
Solution of the simultaneous pose and correspondence problem using Gaussian error model
Computer Vision  and Image Understanding, NN(N):NNN–NNN, NNNN
N  [NN] L
Kneip and P
Furgale
OpenGV: A unified and generalized  approach to real-time calibrated geometric vision
In Proc
 N0NN Int
Conf
Robotics and Automation, pages N–N
IEEE,  N0NN
N, N, N  [NN] L
Kneip, H
Li, and Y
Seo
UPnP: An optimal O(n) solution  to the absolute pose problem with universal applicability
In  Proc
N0NN European Conf
Comput
Vision, pages NNN–NNN
 Springer, N0NN
N, N  [NN] L
Kneip, D
Scaramuzza, and R
Siegwart
A novel  parametrization of the perspective-three-point problem for a  direct computation of absolute camera position and orientation
In Proc
N0NN Conf
Comput
Vision Pattern Recognition, pages NNNN–NNNN
IEEE, N0NN
N, N, N  [NN] L
Kneip, Z
Yi, and H
Li
SDICP: Semi-dense tracking  based on iterative closest points
In Proc
N0NN British Machine Vision Conference, pages N00.N–N00.NN
BMVA Press,  Sep
N0NN
N  [NN] A
H
Land and A
G
Doig
An automatic method of solving  discrete programming problems
Econometrica: Journal of  the Econometric Society, pages NNN–NN0, NNN0
N, N  [NN] V
Lepetit, F
Moreno-Noguer, and P
Fua
EPnP: An accurate  O(n) solution to the PnP problem
Int
J
Comput
Vision,  NN(N):NNN–NNN, N00N
N, N, N  [NN] H
Li
Consensus set maximization with guaranteed global  optimality for robust geometry estimation
In Proc
N00N Int
 Conf
Comput
Vision, pages N0NN–N0N0
IEEE, N00N
N  [NN] H
Li and R
Hartley
The ND-ND registration problem revisited
In Proc
N00N Int
Conf
Comput
Vision, pages N–N
 IEEE, N00N
N, N  [NN] Y
Li, N
Snavely, D
Huttenlocher, and P
Fua
Worldwide  pose estimation using ND point clouds
In Proc
N0NN European Conf
Comput
Vision, pages NN–NN
Springer-Verlag,  N0NN
N  [N0] Y
Li, N
Snavely, and D
P
Huttenlocher
Location recognition using prioritized feature matching
In Proc
N0N0 European Conf
Comput
Vision, pages NNN–N0N
Springer, N0N0
 N  [NN] W.-Y
Lin, L.-F
Cheong, P
Tan, G
Dong, and S
Liu
Simultaneous camera pose and correspondence estimation with  motion coherence
Int
J
Comput
Vision, NN(N):NNN–NNN,  N0NN
N  N    [NN] D
G
Lowe
Distinctive image features from scale-invariant  keypoints
Int
J
Comput
Vision, N0(N):NN–NN0, N00N
N  [NN] A
Makadia, C
Geyer, and K
Daniilidis
Correspondencefree structure from motion
Int
J
Comput
Vision,  NN(N):NNN–NNN, N00N
N  [NN] E
Marchand, H
Uchiyama, and F
Spindler
Pose estimation  for augmented reality: a hands-on survey
IEEE Trans
Vis
 Comput
Graphics, NN(NN):NNNN–NNNN, N0NN
N  [NN] F
Moreno-Noguer, V
Lepetit, and P
Fua
Pose priors for  simultaneously solving alignment and correspondence
In  Proc
N00N European Conf
Comput
Vision, pages N0N–NNN
 Springer, N00N
N, N  [NN] J
L
Mundy
Object recognition in the geometric era: A retrospective
In J
Ponce, M
Hebert, C
Schmid, and A
Zisserman, editors, Toward Category-Level Object Recognition,  volume NNN0 of Lecture Notes in Computer Science, pages  N–NN
Springer, Berlin, Heidelberg, N00N
N  [NN] S
T
Namin, M
Najafi, M
Salzmann, and L
Petersson
 A multi-modal graphical model for scene analysis
In  Proc
N0NN Winter Conf
Applications Comput
Vision, pages  N00N–N0NN
IEEE, N0NN
N  [NN] T
Nöll, A
Pagani, and D
Stricker
Markerless Camera  Pose Estimation - An Overview
In A
Middel, I
Scheler,  and H
Hagen, editors, Visualization of Large and Unstructured Data Sets - Applications in Geospatial Planning, Modeling and Engineering (IRTG NNNN Workshop), volume NN  of OpenAccess Series in Informatics (OASIcs), pages NN–  NN, Dagstuhl, Germany, N0NN
Schloss Dagstuhl–LeibnizZentrum fuer Informatik
N  [NN] C
F
Olson
A general method for geometric feature matching and model extraction
Int
J
Comput
Vision, NN(N):NN–  NN, N00N
N  [N0] C
Olsson, A
Eriksson, and R
Hartley
Outlier removal  using duality
In Proc
N0N0 Conf
Comput
Vision Pattern  Recognition, pages NNN0–NNNN
IEEE, N0N0
N  [NN] C
Olsson, F
Kahl, and M
Oskarsson
Branch-and-bound  methods for euclidean registration problems
IEEE Trans
 Pattern Anal
Machine Intelligence, NN(N):NNN–NNN, N00N
N  [NN] D
P
Paudel, A
Habed, C
Demonceaux, and P
Vasseur
 LMI-based ND-ND registration: From uncalibrated images to  Euclidean scene
In Proc
N0NN Conf
Comput
Vision Pattern  Recognition
N  [NN] D
P
Paudel, A
Habed, C
Demonceaux, and P
Vasseur
Robust and optimal sum-of-squares-based point-to-plane registration of image sets and structured scenes
In Proc
N0NN  Int
Conf
Comput
Vision, pages N0NN–N0NN, N0NN
N  [NN] T
Sattler, B
Leibe, and L
Kobbelt
Fast image-based localization using direct ND-to-ND matching
In Proc
N0NN Int
 Conf
Comput
Vision, pages NNN–NNN
IEEE, N0NN
N  [NN] T
Sattler, B
Leibe, and L
Kobbelt
Improving image-based  localization by active correspondence search
In Proc
N0NN  European Conf
Comput
Vision, pages NNN–NNN
SpringerVerlag, N0NN
N  [NN] K
Sim and R
Hartley
Removing outliers using the L∞ norm
In Proc
N00N Conf
Comput
Vision Pattern Recognition, volume N, pages NNN–NNN
IEEE, N00N
N  [NN] L
Svarm, O
Enqvist, F
Kahl, and M
Oskarsson
City-scale  localization for cameras with known vertical direction
IEEE  Trans
Pattern Anal
Machine Intelligence, N0NN
N  [NN] L
Svärm, O
Enqvist, M
Oskarsson, and F
Kahl
Accurate localization and pose estimation for large ND models
In  Proc
N0NN Conf
Comput
Vision Pattern Recognition, pages  NNN–NNN
IEEE, N0NN
N  [NN] J
Yang, H
Li, D
Campbell, and Y
Jia
Go-ICP: A globally optimal solution to ND ICP point-set registration
IEEE  Trans
Pattern Anal
Mach
Intell., NN(NN):NNNN–NNNN, N0NN
 N, N, N, N  [N0] J
Yu, A
Eriksson, T.-J
Chin, and D
Suter
An adversarial  optimization approach to efficient outlier removal
In Proc
 N0NN Int
Conf
Comput
Vision, pages NNN–N0N
IEEE, N0NN
 N  N0A Generic Deep Architecture for Single Image Reflection Removal and Image Smoothing   A Generic Deep Architecture for Single Image Reflection Removal  and Image Smoothing  Qingnan Fan∗N Jiaolong YangN Gang HuaN Baoquan ChenN,N David WipfN  NShandong University NMicrosoft Research NShenzhen Research Institute, Shandong University  fqnchina@gmail.com, {jiaoyan,davidwip,ganghua}@microsoft.com, baoquan@sdu.edu.cn  Abstract  This paper proposes a deep neural network structure that  exploits edge information in addressing representative lowlevel vision tasks such as layer separation and image filtering
Unlike most other deep learning strategies applied in  this context, our approach tackles these challenging problems by estimating edges and reconstructing images using  only cascaded convolutional layers arranged such that no  handcrafted or application-specific image-processing components are required
We apply the resulting transferrable  pipeline to two different problem domains that are both sensitive to edges, namely, single image reflection removal and  image smoothing
For the former, using a mild reflection  smoothness assumption and a novel synthetic data generation method that acts as a type of weak supervision, our  network is able to solve much more difficult reflection cases  that cannot be handled by previous methods
For the latter,  we also exceed the state-of-the-art quantitative and qualitative results by wide margins
In all cases, the proposed  framework is simple, fast, and easy to transfer across disparate domains
 N
Introduction  Inspired by the tremendous success of deep learning for  large-scale visual recognition tasks like ILSVRC [NN, N0],  a variety of recent work has investigated deep neural networks for low-level computer vision tasks such as image denoising [NN, NN], shadow removal [NN], and image smoothing [NN, NN]
Given that edges represent an important cue  in addressing many of these problems, networks that can  replace computationally-expensive or otherwise inflexible  edge-aware filters naturally show promise
 For example, the underlying goal of image smoothing is  to extract sparse salient structures, like perceptually important edges and contours, while minimizing the color differences in image regions with low amplitude
To approximate  ∗This work was done when Qingnan Fan was an intern at MSR
 different edge-sensitive image smoothing filters which potentially have slow runtimes [N, N, N, NN, NN, NN, NN, NN]  with deep networks, it has been proposed to first learn a  salient gradient/weight map and then subsequently filter images via simpler, weighted optimization procedures [NN] or  iterative recursive processing techniques [NN]
The above  approaches focus on solving a single/major problem using  a plain CNN model followed by more traditional, inflexible operations inspired by fixed filtering methods
Consequently, they are not fully extensible to implementing  broader image smoothing effects or other significantly different problems such as image layer separation
 In this latter regard, one typical case where gradient domain statistics are relevant is in dealing with image reflections, that are often at least partially out of focus, when provided with a single image
When taking a photo through  a glass window, the glare or reflection tends to distract the  eye from the scene behind the glass
Many attempts to mitigate these effects, such as using a polarizer [NN, NN], draping a large piece of black cloth over the lens and the glass  to block ambient light from behind, or changing positions  [NN, N0, NN], are simply infeasible in many practical situations
Moreover, when taking photographs in airplane, museum, aquarium, or related environments, there is no other  recourse but to shoot through the window
Consequently, it  is common for photographers to simply widen the aperture  of the camera and blur out the reflections
 To address this reflection removal problem from a computational perspective, traditional imaging models assume  that the captured image I is a linear combination of a background layer B and a reflection layer R, i.e., I = B + R
Obviously this is an ill-posed problem as there exist infinite  feasible solutions, and hence most reflection removal algorithms require multiple input images [N, N0, N, NN, NN, NN,  N0, NN] or manual user interactions [NN] to label reflectionand background-layer gradients, thus condensing the space  of candidate solutions
However, one exploitable property  in the reflection removal problem is that the gradients or  perceptual structures of the two layers exhibit different distributions, since reflections often display a greater degree  NNNNN    of blurring
This then naturally leads us towards edgebased solutions, with data-driven network variants considered herein
 In this paper, we present a Cascaded Edge and Image  Learning Network (CEILNet) that can be tailored to solve  different image processing tasks such as layer separation  (e.g., reflection removal) and image filtering (e.g., image  smoothing)
We rely on an overriding generic structure that  is specialized in each instance via domain-specific edge information
The core framework operates in a very intuitive way
In brief, we separate the difficult task of directly  predicting an image into two subproblems: (i) predicting  the edge maps of the target images via a deeply supervised  sub-network, and then (ii) reconstructing the target images  by leveraging the predicted edge maps
These tasks are  learned end-to-end by cascading two similar simple CNNs,  and no hand-crafted modules are required
The edge map  represents any color difference between each pair of adjacent pixels for task-specific target images, instead of sparse  salient structures as in edge detection problems
 Of course, these objectives require ample training data to  be feasible in practice
For image smoothing, this is not especially problematic provided sufficient computational resources are available for producing filter outputs across a  corpus of images
However, for many layer separation tasks  ground-truth instances are scarce
We therefore propose a  novel weakly supervised learning method for training our  reflection removal pipeline
This involves the use of images  synthetically corrupted via reflections that mimic the physical properties of those found in natural scenes
 Our contributions can be summarized as follows:  • We propose a new, generic Cascaded Edge and Image Learning Network (CEILNet) that relies only on convolutional layers and is specifically designed to tackle  edge-sensitive image processing tasks without resorting to any handcrafted, application-specific components
This structure is fast, extensible, and easy to  reproduce, facilitating the seamless transfer to different low-level vision problems
 • We are the first to solve the challenging layer- separation problem of reflection removal from single  images using deep learning techniques
We also propose a novel weakly supervised learning strategy combined with CEILNet
 • Beyond reflection removal, we demonstrated state-of- the-art visual and numerical performance using CEILNet on the image smoothing task, surpassing previous  methods by a wide margin
 N
Related Work  Reflection Removal: Reflection removal is fundamentally an underdetermined problem and therefore requires  prior knowledge or additional information to achieve any  degree of success
Perhaps the most popular practical remedy is to use multiple input images, such as flash/non-flash  image pairs [N], focus/defocus pairs [N0], video sequences  where background and reflection exhibit different motions  [N, NN, NN, N, NN, NN, NN, N0, NN], or those obtained through  a polarizer at two or more orientations [NN, NN, NN]
A few  ambitious approaches attempt single image reflection removal, a far more difficult but practical scenario
In [NN],  manual annotation is required to guide an optimizationbased layer separation
[NN] compensates for the limited  information by exploiting ghost cues, but this approach is  not applicable beyond this somewhat specialized situation,  or in the majority of practical cases
[NN] leverages a multiscale DoF computing strategy to separate reflection from  background
 In terms of automatic reflection removal from a single  image with minimal assumptions, the work most closely related to ours is [NN]
This approach assumes the reflected  layer is relatively blurry compared to the background scene,  thus large gradients in it are strongly penalized in their optimization
However, we observe that the reflection in many  real-world photographs, although indeed sometimes out of  focus or blurry, is nonetheless produced by bright lights and  often comprises the brightest portion of an image
The regional gradients associated with these reflections can therefore be quite large, violating the assumption in [NN]
In  this work, we synthesize a database of training samples that  better capture the background and reflection statistics, and  replace prior knowledge injected through explicit gradient  penalization or energy minimization with a particular deep  network to capitalize on this form of weak supervision
Empirically we will later show that indeed significant improvement is possible on real images
 Image Smoothing: Given the recent effectiveness of  parallel computation through GPUs, and the strong  learning capability of deep neural networks, replacing  computationally-expensive, optimization-based smoothing  filters with cheap neural modules has drawn a lot of attention [NN, NN]
However, because accurately capturing  smoothing effects with a fully convolutional deep network  can be challenging, [NN] trains a shallow CNN on the gradient domain followed by an optimized image reconstruction post-processing step with sensitive parameters tuned  for each different smoothing filter
From a somewhat different perspective, by treating spatially-variant recursive  networks as surrogates for a group of distinct filters, [NN]  combines sparse salient structure prediction implemented  as CNN with image filtering in a hybrid neural network
 While significant differences exist, all of these prior  methods lean on traditional optimization or filtering techniques at some point in their pipelines
Moreover, they are  mostly applied to image smoothing using filter- or effectNNNN    Co vo lu tio    St rid  e  N  Ba tc h  No  r  Re LU  Co vo lu tio  Ba tc h  No  r  Re LU  Co vo lu tio  Ba tc h  No  r  Re LU  Re sid  ua l B  lo ck  Re sid  ua l B  lo ck  Re sid  ua l B  lo ck  Re sid  ua l B  lo ck  De co  vo lu tio    St rid  e  N  Ba tc h  No  r  Re LU  Co vo lu tio  Ba tc h  No  r  Re LU  Co vo lu tio  I pu  t  Ou tp ut……  NN  lo ks  (a)  (b)  E‐CNN I‐CNN I put I age  I put Edge  I put I age  Target Edge Target I age  Figure N
The proposed deep network architecture CEILNet
(a) The cascaded edge and image prediction pipeline
Two CNN networks,  E-CNN and I-CNN, are used for edge prediction and image reconstruction, respectively
I-CNN takes the output of E-CNN as input, giving  rise to an end-to-end and fully convolutional solution
(b) The detailed CNN structure shared by E-CNN and I-CNN
 dependent implementations without a universal, trainable  parametric structure
This can potentially contribute to degraded performance since no single optimization or filtering  strategy is likely to generalize to all different image smoothing effects
In contrast, our method learns a generic, fullyconvolutional structure with no attendant postprocessing or  otherwise fixed, filter-inspired structures
Empirical experiments demonstrate that this revised strategy outperforms  the best existing work by a wide margin
 N
Network Structure  Our network consists of two cascaded sub-networks: an  edge prediction network E-CNN and an image reconstruction network I-CNN
Figure N is a schematic description of  the architecture, which is unchanged for both the reflection  removal and image smoothing applications
 N.N
E-CNN: The Edge Prediction Network  When dealing with edge-sensitive image processing  tasks like reflection removal and image smoothing, edgesrelated cues are naturally leveraged by many existing algorithms [NN, NN, N0, N0, NN]
Similarly, given a source image  I s, we apply a CNN to learn an edge map Et of the target  image It (i.e., the background layer for reflection removal  or the smoothed image for image smoothing)
Note that the  goal is to predict the edges of the target image, not the input  image, and it is crucial not to confuse this procedure with  conventional edge detection [N, NN]
 In this work, our edge map is not binary, as we empirically found binary edge maps are less informative for the  subsequent image reconstruction
Instead, we designed a  simple but effective edge representation: the mean absolute color difference between a center pixel and its fourconnected neighbors
Specifically, the edge map E of an  image I is computed by:  Ex,y = N  N  ∑  c  (  |Ix,y,c − Ix−N,y,c|+ |Ix,y,c − Ix+N,y,c|  + |Ix,y,c − Ix,y−N,c|+ |Ix,y,c − Ix,y+N,c| )  (N)  where x, y are the pixel coordinates and c refers to the channels in the RGB color space
 In order to ease the computation, we augment the source  image Is with its edge map Es as an additional channel for  input
The intuition behind is simple: either a reflectionfree background layer or an image smoothed via a filtering  process can be viewed as “simplified” versions of the original source images, and their edge maps are roughly “attenuated” versions of the source image edge maps
We observed that such an augmentation can not only lead to better results but also significantly accelerate the convergence  during training
In summary, E-CNN approximates the following function f :  E t = f(Is,Es) (N)  N.N
I-CNN: The Image Reconstruction Network  The second sub-network, I-CNN, is designed to reconstruct the target image It by learning how to process the  input image Is given the target edge map Et predicted by  E-CNN
In other words, it approximates the following function g:  I t = g(Is,Et) (N)  The input image and the target edge are combined to be  a N-channel tensor as input, similar to E-CNN, hence their  shared use of the same overall structure
Additionally, in the  context of the edge-based image reconstruction step of image smoothing tasks, the I-CNN serves as a multi-purpose,  data-driven substitution for traditional fixed filtering operations or optimization-based postprocessing structures
 NNN0    N
Train E-CNN and I-CNN in parallel, with loss functions of Eq
N and Eq
N respectively
 N
Jointly train (fine-tune) E-CNN and I-CNN end-toend, with loss in Eq
N
 Figure N
Our two-phase network training algorithm
 N.N
Details of CNN Layers  For simplicity, we employ the deep CNN structure  shown in Fig
N (b) for both E-CNN and I-CNN
The two  sub-nets only differ in the channel number of the final output, i.e., N for E-CNN vs
N for I-CNN
In each case, we  employ NN convolutional layers with the same N×N kernel size (except for the third-to-last layer; see below)
The intermediate N0 convolutional layers all have NN-dimensional  input and output feature maps
The first NN layers are followed by batch normalization (BN) and ReLU
To ensure  better contextual information, we enlarge the receptive field  by downsampling the internal feature map to half size and  then upsampling it back by changing the stride of the third  convolution layer to N and third-to-last convolution layer to  deconvolution with stride N and kernel size N×N
In this way, the receptive field is effectively enlarged without losing too much image detail, and meanwhile the computation  cost is halved
For better performance and faster convergence, we implement the middle NN convolution layers as  NN residual units [NN] similar to [N]
 Finally, to resolve the color attenuation issue [NN, NN] observed in deep networks, we slightly magnify the predicted  image It via sc , argminsc ‖I s c−sc ·I  t c‖  N N and I  t c ← sc ·I  t c
 This global color correction is implemented as a parameterfree layer after I-CNN
Its computational cost is negligible
 N
Network Training  This section first presents our training pipeline, that applies independently of the data source
Later we describe  application-specific means of generating training samples
 N.N
Training Details  We employ a two-phase network training algorithm  shown in Fig
N
Specifically, we first train the sub-networks  separately with ground-truth images and their edge maps  to ensure the best individual performances
We then finetune the entire network end-to-end, granting the two subnets more opportunity to cooperate accordingly
 The sub-nets are trained by minimizing the mean  squared errors (MSE) of their predictions
Let the symbol ∗ denote ground truth, the loss for edge prediction is  lE(θ) = ||E t −Et∗||NN
(N)  For image prediction, we minimize not only the color MSE  but also the discrepancy of gradients:  lI(θ) = α ||I t − It∗||NN  +β (||∇xI t −∇xI  t∗||N + ||∇yI t −∇yI  t∗||N)
(N)  The gradient discrepancy cost, though seemingly redundant, helps to prevent the deep convolutional network from  generating blurry images [NN]
In the joint training phase,  we train the entire network by minimizing the loss:  l(θ) = lI(θ) + γ lE(θ)
(N)  For all experiments across reflection removal and image  smoothing, the loss coefficients are empirically set as α= 0.N, β=γ=0.N (other selections produce similar results)
 We initialize the convolution weights using the approach  from [NN] and train all networks using ADAM [NN] with  mini-batch size fixed at N
When training the two sub-nets  separately, the learning rate is set to 0.0N over the initial  iterations, e.g., N0 and NN epochs for reflection and imaging  smoothing tasks respectively
The entire network is then  fine-tuned with the learning rate reduced to 0.00N
 N.N
Training Data Generation  Reflection Image Synthesis: Real images with ground  truth background layers are difficult to obtain
To generate enough training data, simply mixing two images with  different coefficients (such as 0.N for background and 0.N  for reflection) seems to be a straightforward and plausible  compromise
Indeed, this strategy has been widely used in  previous works [NN, NN, NN, NN, NN] for analysis and quantitative evaluation
However, we found that networks trained  on such images generalize poorly to real photographs
We  therefore propose a novel synthesis method to better approximate real-world reflection
 As previously mentioned, we assume that the reflection  is somewhat blurry relative to the background layer, which  tends to be more sharp and clear
This is a valid assumption for many cases, as the camera is usually focused on  the background target
Moreover, a photographer can easily  widen the camera’s aperture and blur out the reflections
A  similar assumption is used by [NN]
 We expand on this assumption using a simple complementary observation
First, according to the Fresnel equation, we know that when incident light travels across media  with different refractive indices (e.g., glass and air) in front  of some scene of interest, a portion of that light will be reflected back to the image plane
However, the actual visibility of this reflected light to the human eye or a camera  depends on the relative intensity of light transmitted from  the background scene
Therefore we may expect that only  portions of the background layer transmitting modest light  will be appreciably obstructed via a reflection layer, even if  the latter is uniformly present across a scene
And yet in regions where reflections are apparent, their intensity can still  NNNN    Randomly pick two natural images normalized to [0, N] as background B and reflection R respectively, then:  N
R̃← gauss blurσ(R) with σ ∼ U(N, N)  N
I← B+ R̃  N
m←mean({I(x, c) | I(x, c)>N, ∀x, ∀c=N,N,N})  N
R̃(x, c)←R̃(x, c)−γ · (m−N), ∀x, ∀c; γ set as N.N  N
R̃← clip[0,N](R̃)  N
I← clip[0,N] (  B+ R̃ )  Output I as the synthesized image with B as the  ground-truth background layer
 Figure N
Reflection image data synthesis for weakly-supervised  learning
The subtraction and clipping operators allow for reflection intensities that can saturate and vanish in various regions
 be arbitrarily large (even if partially blurred) and so a purely  additive model with a weakly scaled reflection component  is not always physically plausible
 Based on the above observations, we develop a new  method summarized in Fig
N to synthesize images with realistic background and reflection layers
One key difference  from naive image mixing is that the brightness overflow issue is avoided not by scaling down the brightness, but by  subtracting an adaptively computed value followed by clipping
In this way: (i) reflection-free regions are very likely  to appear which is consistent with natural images, (ii) strong  reflections can occur in other places, and (iii) the reflection  contrast is better maintained
Also note that we randomly  pick the σ of the Gaussian blur kernel between [N, N], in contrast to a fixed large value (σ = N) tested in [NN]
We are interested in handling a wider range of real cases, including cases with lesser blurry reflections
Figure N (top)  displays N synthetic images generated by our method, and  Fig
N shows a result comparison with naive image mixing
 For more comparisons and details regarding the synthesis  process, see the supplemental material
 Note that synthetically generated samples serve as a form  of weak supervision, as we ultimately deploy the trained  model on new real images containing natural reflections
 Generation of Smoothed Images: For image smoothing,  our network is trained to approximate the effect of existing filters
The training and testing data will simply be the  smoothed images generated by applying those filters to existing image databases
Various filters are tested in Sec
N
 N
Experiments  This section first presents self-comparison experiments  to analyze the importance of proposed network architecture  design choices
We then evaluate the full CEILNet against  the state-of-the-art algorithms on the single-image reflection removal and image smoothing tasks
 Table N
Result comparison for the image smoothing task (learning an L0 filter [NN])
CEILNet outperformed Domain Transform  (DT) [N0] and simple I-CNNs without E-CNN by large margins
 MSE PSNR SSIM  DT + input image edge NNN.NN NN.NN 0.N0N  DT + pred
edge by E-CNN NN.NN NN.NN 0.NNN  DT + GT edge NN.NN NN.NN 0.NNN  I-CNN only NN.NN NN.NN 0.NNN  I-CNN only (NN layers) NN.NN NN.NN 0.NNN  I-CNN with input edge (NN layers) NN.N0 NN.NN 0.NNN  CEILNet NN.NN NN.N0 0.NNN  N.N
Network Analysis  For simplicity, our analysis will be mainly based on the  representative results of approximating L0 smoothing [NN]
 These results were obtained on N00 PASCAL VOC test images (refer to Sec
N.N for training and testing details)
 Is the target edge map from E-CNN helpful? To verify the importance of the target edge map for image reconstruction, we removed E-CNN and trained a simple I-CNN  model without the predicted target edge or replacing the  predicted target edge with the input image edge
Table N  shows that I-CNN with predicted edge (i.e., our CEILNet)  outperformed I-CNN alone and I-CNN with input edge by  significant margins, demonstrating the importance of target  edge prediction
A visual comparison is shown in Fig
N
 Similar results were obtained for reflection removal: the  predicted background edges were found to be helpful for  layer separation
Figure N shows a typical example
 Does simply stacking more layers in I-CNN suffice?  Ideally, with enough depth, one may expect the network to  handle target edge prediction implicitly without the need  for an explicit E-CNN
We tried training a simple I-CNN  with more convolutional layers
and found that the performance gets saturated quickly after more than N0 layers  (a detailed figure is deferred to the supplementary material)
Our CEILNet, i.e., NN-layer E-CNN + NN-layer ICNN, achieved much better results than a NN-layer simple  I-CNN (as shown in Table N) and a best-performing N0-layer  one (PSNR NN.NN vs
NN.N0 by CEILNet)
 Is I-CNN better than a traditional method? To answer  this question, we replaced I-CNN with the Domain Transform (DT) technique [N0]
The predicted target edge map by  E-CNN and the input image are fed to DT to output smooth  images
We also tried the ground-truth target edge and the  input image edges
Table N shows that I-CNN with predicted edge from E-CNN (i.e., our CEILNet) outperformed  all DT results by large margins
A visual comparison is presented in Fig
N
 NNNN    Input image GT image CEILNet  I-CNN only DT + pred
edge DT + GT edge  Input edge GT edge Pred
edge by E-CNN  Figure N
Qualitative comparison for the image smoothing task  (learning an L0 filter [NN])
Our CEILNet generates a more satisfactory result than a simple I-CNN without E-CNN and than Domain Transform [N0]
Best viewed on screen with zoom
 Input image Input edge Pred
edge by E-CNN  CEILNet (naive data) I-CNN only CEILNet  Figure N
Qualitative reflection removal results on a real image
 Our CEILNet removes more reflection and generates a clearer  background image than a simple I-CNN without E-CNN, and than  CEILNet trained with a naive image mixing strategy for data generation
Best viewed on screen with zoom
 For reflection removal, we also tried applying the layer  separation algorithm in [NN] with our predicted edges as input, but no satisfactory results were obtained.N  N.N
Reflection Removal  Training Data: We applied the method described in  Sec
N.N to synthesize training data for the reflection removal task to accommodate our weakly-supervised learning  pipeline
We used NNK natural images from the PASCAL  N[NN] utilizes multiple images to identify background edges, which are  used as prior to guide layer septation
Their septation algorithm did not  work well with our edge maps as it assumes non-blurry reflections and  requires binary edge maps
 Table N
Quantitative comparison of our method with Li and  Brown [NN] on N00 synthetic images with reflection
 PSNR SSIM  [NN] Ours [NN] Ours  NN.N0 NN.NN 0.NNN 0.NNN  VOC dataset [N] for the synthesis
These images were collected from Flickr, and represent a wide range of viewing  conditions
Two natural images were used to generate one  synthetic image containing a background layer and a reflection layer, resulting in N.NK synthetic images in total
We  split these images into a training set of N,NNN images and a  test set with NN0 images for quantitative comparison
The  training images are also cropped to NNN×NNN
The algo- rithm described in Fig
N was then applied, and we did not  observe over-fitting in any of the training sub-tasks
 Method Comparison: We tested our CEILNet against  the state-of-the-art, single-image approach from [NN]
For  a quantitative comparison, we randomly selected N00 images in our test dataset, and evaluate the PSNR and SSIM  metrics for the predicted B from both algorithms
The default parameters of [NN] were used for evaluation
Table N  shows that CEILNet significantly outperformed [NN]
 Figure N presents some qualitative results of our method  compared against [NN] on both synthetic and real images
 The reflection image estimates are computed via R = I−B
We tuned the parameters of [NN] for each image to get the  best visual result
It can be seen that [NN] tends to generate  a blurry reflection layer with brightness covering the whole  image
It largely failed to remove less blurry, high contrast  or partially present reflections
This is because [NN] employs strong priors to penalize abrupt color transitions in R  which, however, may be common in real cases
In contrast,  our CEILNet is able to separate out the reflections reasonably well even if some of them are very bright and shiny,  and without jeopardizing the reflection-free regions
More  results and comparisons are deferred to the supplementary  material due to space limitation
 N.N
Image Smoothing  Training Data: For image smoothing, we used the NNK  natural images in the PASCAL VOC dataset as input, and  generated the filtered images using existing image smoothing algorithms as the ground truth
These images are fed to  the network without cropping
We also randomly pick N00  images in the PASCAL VOC dataset for testing
We again  use the algorithm in Fig
N to train our CEILNet
 Method Comparison: We tested N image smoothing algorithms for the network to approximate, including bilateral filter (BLF) [NN], iterative bilateral filter (IBLF) [N],  rolling guidance filter (RGF) [NN], RTV texture smoothing  NNNN    I B (Ours) R (Ours) B ([NN]) R ([NN]) I B (Ours) R (Ours) B ([NN]) R ([NN])  R ([  N N ])  B ([  N N ])  R (O  u rs  ) B  (O u  rs )  I  I B (Ours) R (Ours) B ([NN]) R ([NN])  Figure N
Qualitative results of the single image reflection removal task on synthetic (top two rows) and real (bottom rows) images
Visually  inspected, our method can largely remove the reflection and produce reasonably good background images under various situations
The  method of Li and Brown [NN] clearly underperformed
The last example is a partial failure case for our method due to the strong reflection  and weak transmitted light, but still the result is superior to [NN]
Best viewed on screen with zoom
 Table N
Quantitative comparison on the image smoothing tasks
We report the PSNR and SSIM metrics (larger is better) for N different  smoothing filters, and compare our method with Xu et al
[NN]
Average values are computed with the preceding N cases
 BLF IBLF L0 RGF RTV WLS WMF LN Ave
 PSNR [NN] NN.0N NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN Ours NN.NN NN.NN NN.N0 NN.0N NN.0N NN.NN NN.N0 NN.NN N0.N0  SSIM [NN] 0.NNN 0.NNN 0.NNN 0.NN0 0.NNN 0.NNN 0.NN0 0.NNN Ours 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NN0  Table N
Running time comparison (in seconds)
We compare the running time of our method against different traditional methods as well  as deep learning based methods of Xu et al
[NN] and Liu et al
[NN] at various resolutions
 BLF IBLF RGF L0 WMF RTV WLS LN [NN] [NN] Ours  QVGA (NN0×NN0) 0.0N 0.NN 0.NN 0.NN 0.NN 0.NN 0.N0 NN.NN 0.NN 0.0N 0.0N VGA (NN0×NN0) 0.NN 0.N0 0.NN 0.NN N.NN N.N0 N.NN NNN.0N 0.NN 0.NN 0.NN  NN0p (NNN0×NN0) 0.NN 0.NN N.NN N.NN N.NN N.NN NN.NN N0N.NN N.NN 0.NN 0.NN  NNNN    b ri  d g e  Input, NN.NN dB [NN], NN.NN dB Ours, NN.NN dB Ground Truth  h u  m a  n  Input, NN.NN dB [NN], NN.NN dB Ours, NN.NN dB Ground Truth  [NN], NN.NN dB Ours, NN.NN dB Ground Truth [NN], NN.NN dB Ours, NN.N0 dB Ground Truth  Figure N
Qualitative results on the image smoothing task
All the methods are trained to approximate L0 smoothing [NN]
Top: Comparison  with Xu et al
[NN]
Bottom: Comparison with Liu et al
[NN] on the NNN×NNN image size
Our results are visually much closer to the ground truth
The numbers show the PSNR values
Best viewed on screen with zoom
 Table N
Comparison with Liu et al
[NN] on image smoothing
 PSNR SSIM  [NN] Ours [NN] Ours  L0 NN.NN NN.NN 0.NNN 0.NNN  RGF NN.NN N0.N0 0.NNN 0.NNN  WLS NN.NN N0.NN 0.NNN 0.NNN  WMF NN.NN NN.NN 0.NNN 0.NNN  Ave
NN.NN NN.NN 0.NNN 0.NNN  (RTV) [NN], weighted least square smoothing (WLS) [N],  weighted median filter (WMF) [NN], L0 smoothing [NN] and  LN smoothing [N]
 Table N presents the quantitative results of our method  and [NN] on the test set with N00 images
In can been seen  that our network achieved much better results than [NN] for  all the N filters, on both the PSNR and SSIM metrics
We  also compare our results with [NN], whose models for N filters are publicly available
Note that at the time of writing,  the latest code of [NN] released by their authors cannot run  on arbitrary image size due to some implementation constraints, so we use their default size of NNN×NNN
Table N shows that our method also significantly outperformed [NN]  for all the N filtering algorithms
 Figure N presents two visual results of our method compared to others
It can be observed that the method of [NN]  generated obvious artifacts compared to the ground truth  for both two cases, while [NN] produced some unwanted  color transitions in the right and bottom left regions of the  “bridge” image, resulting in a PSNR even lower than the  raw input image
In contrast, our results are visually more  close to the ground truth
More results and discussions can  be found in the supplementary material
 Running Time: We evaluate the running time of the eight  traditional smoothing algorithms and the three deep learning based methods with respect to different image sizes on  the same computer (NVIDA DGX-N)
Table N shows that  our method runs faster than others in most of the cases
It  can approximate any traditional algorithm at over N fps for  NN0×NN0 images
 N
Conclusions and Future Work  We have proposed CEILNet, a generic deep architecture  for edge-sensitive image processing
We provided the first  learning-based solution to the challenging single image reflection removal problem using CEILNet and with the aid  of a novel reflection image synthesis method
We have also  significantly advanced the state-of-the-art in DNN-based  image smoothing
Our future work includes testing CEILNet on more image processing tasks
Promising results for  image denosing and inpainting have been obtained in our  preliminary experiments
 Acknowledgement This work was partially supported by Na- tional NNN Program (N0NNCBNNNN0N), Shenzhen Innovation Program (JCYJN0NN0N0NN0NNNN0NN)
 NNNN    References  [N] A
Agrawal, R
Raskar, S
K
Nayar, and Y
Li
Removing photography artifacts using gradient projection and flashexposure sampling
ACM Transactions on Graphics (TOG),  NN(N):NNN–NNN, N00N
N, N  [N] S
Bi, X
Han, and Y
Yu
An LN image transform for edgepreserving smoothing and scene-level intrinsic decomposition
ACM Transactions on Graphics (TOG), NN(N):NN, N0NN
 N, N  [N] J
Canny
A computational approach to edge detection
IEEE  Transactions on Pattern Analysis and Machine Intelligence  (T-PAMI), (N):NNN–NNN, NNNN
N  [N] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
International Journal of Computer Vision (IJCV),  NN(N):N0N–NNN, N0N0
N  [N] Q
Fan, D
Wipf, G
Hua, and B
Chen
Revisiting deep  image smoothing and intrinsic image decomposition
arXiv  preprint
arXiv:NN0N.0NNNN, N0NN
N  [N] Z
Farbman, R
Fattal, D
Lischinski, and R
Szeliski
Edgepreserving decompositions for multi-scale tone and detail  manipulation
ACM Transactions on Graphics (TOG), NN(N),  N00N
N, N  [N] H
Farid and E
H
Adelson
Separating reflections and lighting using independent components analysis
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  volume N, pages NNN–NNN, NNNN
N, N  [N] R
Fattal, M
Agrawala, and S
Rusinkiewicz
Multiscale  shape and detail enhancement from multi-light image collections
ACM Transactions on Graphics (TOG), NN(N), N00N
 N, N  [N] K
Gai, Z
Shi, and C
Zhang
Blind separation of superimposed moving images using image statistics
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), NN(N):NN–NN, N0NN
N  [N0] E
S
Gastal and M
M
Oliveira
Domain transform for edgeaware image and video processing
In ACM Transactions on  Graphics (TOG), volume N0, page NN
ACM, N0NN
N, N, N  [NN] M
Gharbi, G
Chaurasia, S
Paris, and F
Durand
Deep joint  demosaicking and denoising
ACM Transactions on Graphics (TOG), NN(N):NNN, N0NN
N  [NN] X
Guo, X
Cao, and Y
Ma
Robust separation of reflection  from multiple images
In IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), pages NNNN–NNNN,  N0NN
N, N, N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Delving deep into  rectifiers: Surpassing human-level performance on imagenet  classification
In IEEE International Conference on Computer Vision (ICCV), pages N0NN–N0NN, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NN0–NNN, N0NN
 N  [NN] S
H
Khan, M
Bennamoun, F
Sohel, and R
Togneri
Automatic shadow detection and removal from a single image
 IEEE Transactions on Pattern Analysis and Machine Intelligence (T-PAMI), NN(N):NNN–NNN, N0NN
N  [NN] J
Kim, J
Kwon Lee, and K
Mu Lee
Accurate image  super-resolution using very deep convolutional networks
In  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNNN–NNNN, N0NN
N  [NN] J
Kim, J
Kwon Lee, and K
Mu Lee
Deeply-recursive  convolutional network for image super-resolution
In IEEE  Conference on Computer Vision and Pattern Recognition  (CVPR), pages NNNN–NNNN, N0NN
N  [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
International Conference on Learning Representations (ICLR), N0NN
N  [NN] N
Kong, Y.-W
Tai, and J
S
Shin
A physically-based  approach to reflection separation: from physical modeling  to constrained optimization
IEEE Transactions on Pattern  Analysis and Machine Intelligence (T-PAMI), NN(N):N0N–  NNN, N0NN
N, N  [N0] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in Neural Information Processing Systems (NIPS),  pages N0NN–NN0N, N0NN
N  [NN] A
Levin and Y
Weiss
User assisted separation of reflections from a single image using a sparsity prior
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), NN(N), N00N
N, N, N  [NN] Y
Li and M
S
Brown
Exploiting reflection change for automatic reflection removal
In IEEE International Conference  on Computer Vision (ICCV), pages NNNN–NNNN, N0NN
N, N,  N, N  [NN] Y
Li and M
S
Brown
Single image layer separation using relative smoothness
In IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), pages NNNN–NNNN,  N0NN
N, N, N, N, N  [NN] S
Liu, J
Pan, and M.-H
Yang
Learning recursive filters for  low-level vision via a hybrid neural network
In European  Conference on Computer Vision (ECCV), N0NN
N, N, N, N  [NN] T
Narihira, M
Maire, and S
X
Yu
Direct intrinsics: Learning albedo-shading decomposition by convolutional regression
In IEEE International Conference on Computer Vision  (CVPR), pages NNNN–NNNN, N0NN
N  [NN] S
Paris and F
Durand
A fast approximation of the bilateral  filter using a signal processing approach
In European Conference on Computer Vision (ECCV), pages NNN–NN0, N00N
 N, N  [NN] J
S
Ren and L
Xu
On vectorization of deep convolutional  neural networks for vision tasks
AAAI Conference on Artificial Intelligence, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual  Recognition Challenge
International Journal of Computer  Vision (IJCV), NNN(N):NNN–NNN, N0NN
N  [NN] B
Sarel and M
Irani
Separating transparent layers through  layer information exchange
In European Conference on  Computer Vision (ECCV), pages NNN–NNN, N00N
N, N  [N0] Y
Y
Schechner, N
Kiryati, and R
Basri
Separation of  transparent layers using focus
International Journal of  Computer Vision (IJCV), NN(N):NN–NN, N000
N, N  NNNN    [NN] Y
Y
Schechner, J
Shamir, and N
Kiryati
Polarization  and statistical analysis of scenes containing a semireflector
 JOSA A, NN(N):NNN–NNN, N000
N, N  [NN] Y
Shih, D
Krishnan, F
Durand, and W
T
Freeman
Reflection removal using ghosting cues
In IEEE Conference  on Computer Vision and Pattern Recognition (CVPR), pages  NNNN–NN0N, N0NN
N  [NN] S
N
Sinha, J
Kopf, M
Goesele, D
Scharstein, and  R
Szeliski
Image-based rendering for scenes with reflections
ACM Transactions on Graphics (TOG), NN(N):N00–N,  N0NN
N  [NN] R
Szeliski, S
Avidan, and P
Anandan
Layer extraction from multiple images containing reflections and transparency
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), volume N, pages NNN–NNN, N000
 N, N  [NN] R
Wan, B
Shi, T
A
Hwee, and A
C
Kot
Depth of  field guided reflection removal
In Image Processing (ICIP),  N0NN IEEE International Conference on, pages NN–NN
IEEE,  N0NN
N  [NN] S
Xie and Z
Tu
Holistically-nested edge detection
In  International Conference on Computer Vision (ICCV), pages  NNNN–NN0N, N0NN
N  [NN] L
Xu, C
Lu, Y
Xu, and J
Jia
Image smoothing via L0 gradient minimization
In ACM Transactions on Graphics  (TOG), volume N0, page NNN, N0NN
N, N, N, N  [NN] L
Xu, J
S
Ren, Q
Yan, R
Liao, and J
Jia
Deep edge-aware  filters
In International Conference on Machine Learning  (ICML), pages NNNN–NNNN, N0NN
N, N, N, N, N  [NN] L
Xu, Q
Yan, Y
Xia, and J
Jia
Structure extraction from  texture via natural variation measure
ACM Transactions on  Graphics (TOG), N0NN
N, N  [N0] T
Xue, M
Rubinstein, C
Liu, and W
T
Freeman
A computational approach for obstruction-free photography
ACM  Transactions on Graphics (TOG), NN(N):NN, N0NN
N, N, N  [NN] J
Yang, H
Li, Y
Dai, and R
T
Tan
Robust optical flow  estimation of double-layer images under transparency or reflection
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNN0–NNNN, N0NN
N, N, N  [NN] Q
Zhang, X
Shen, L
Xu, and J
Jia
Rolling guidance filter
 In European Conference on Computer Vision (ECCV), pages  NNN–NN0, N0NN
N, N  [NN] Q
Zhang, L
Xu, and J
Jia
N00+ times faster weighted  median filter
In IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), N0NN
N, N  NNNNRobust Pseudo Random Fields for Light-Field Stereo Matching   Robust Pseudo Random Fields for Light-Field Stereo Matching  Chao-Tsung Huang  National Tsing Hua University, Taiwan  chaotsung@ee.nthu.edu.tw  Abstract  Markov Random Fields are widely used to model lightfield stereo matching problems
However, most previous approaches used fixed parameters and did not adapt to lightfield statistics
Instead, they explored explicit vision cues to  provide local adaptability and thus enhanced depth quality
But such additional assumptions could end up confining  their applicability, e.g
algorithms designed for dense light  fields are not suitable for sparse ones
 In this paper, we develop an empirical Bayesian  framework—Robust Pseudo Random Field—to explore intrinsic statistical cues for broad applicability
Based on  pseudo-likelihood, it applies soft expectation-maximization  (EM) for good model fitting and hard EM for robust depth  estimation
We introduce novel pixel difference models  to enable such adaptability and robustness simultaneously
 We also devise an algorithm to employ this framework on  dense, sparse, and even denoised light fields
Experimental  results show that it estimates scene-dependent parameters  robustly and converges quickly
In terms of depth accuracy  and computation speed, it also outperforms state-of-the-art  algorithms constantly
 N
Introduction  Light-field stereo matching is an effective way to infer  depth maps from color images
It is based on two properties: photo-consistency across views and depth continuity  between pixels
They are often formulated by Markov Random Fields (MRFs) [N], a statistical graph model, for global  optimization: the former as data energy and the latter as  smoothness energy
 However, most previous approaches applied global optimization heuristically, not statistically
For example, the  energy functions were not inferred from statistics but, instead, devised based on practical experience
They were  often given in robust clipping forms (with constant parameters), such as truncated linear [NN] and negative Gaussian  [NN], to preserve correct depth edges
Recent work has further explored advanced vision cues, such as depth consis(a) Input light field (b) ICCV’NN [NN] (c) Soft-EM MRF  (d) RPRF (e) RPRF on N×N (f) RPRF on N× N  (This work) views denoised views  Figure N
Robust Pseudo Random Fields (RPRFs)
(a) A challenging light field StillLife (N×N views) in HCI dataset [NN]
(b)-(f) The depth maps produced by (b) Wang et al
[NN], (c) MRF using  conventional soft-EM energy, (d) RPRF using robust hard-EM energy, (e) RPRF on a more sparse N×N light field, and (f) RPRF on a distorted N×N light field which is first corrupted by Gaussian noise (σ = N0) and then denoised by BMND [N]
 tency [NN], line segments [NN], and occlusion in angular  patches [NN], to achieve better depth quality
But these additional cues also narrow applicable scope correspondingly
 For example, features for dense light fields ([NN, NN]) may  not work for sparse ones
Also, image denoising which is  commonly used in low-light conditions could invalidate textural cues ([NN])
In this paper, we aim to construct MRFs  in a statistical way to infer robust energy functions for good  depth accuracy and estimate scene-dependent parameters  for broad applicability
 MRF parameter estimation via maximum likelihood is  usually intractable because the normalization factor for  unity is hard to calculate
Instead, pseudo-likelihood [N]  is a classical approximation by exploring local dependence
 One global MRF (likelihood) can be separated into lots of  local neighborhoods (pseudo-likelihood) to collect statistics  and perform distribution fitting
Nevertheless, this approach  has a major issue for stereo matching: empirical distribuNN    tions usually do not have robust clipping forms
Therefore,  good distribution fitting will result in non-robust energy and  thus over-smooth depth (Fig
N(c))
On the other hand,  keeping robust energy will lead to inaccurate fitting results
 Contributions
In this paper, we address this issue by  developing a novel framework—Robust Pseudo Random  Field (RPRF)
Inspired by [N], we model pixel differences  by scale mixtures with soft-edge hidden priors
For parameter estimation, we apply soft expectation-maximization  (EM) by marginalizing out the hidden priors to achieve  good pseudo-likelihood fitting
For MRF formulation, we  employ hard EM by maximizing energy with respect to the  priors to derive robust energy functions
Based on the proposed RPRF (Section N), we devise an empirical Bayesian  algorithm for light-field stereo matching in Section N
It is,  to our best knowledge, the first work of MRF parameter estimation for a single light field
 Extensive experimental results in Section N will show  that this work has good statistical adaptability and produces  great depth maps for not only dense light fields but also  sparse and denoised ones
The scene-dependent parameters can also be estimated robustly with fast convergence
 Finally, we demonstrate better depth accuracy and faster  computation speed than previous work [NN, NN, NN]
 N
Related Work  Pseudo-likelihood
It assumes that local neighborhoods  give independent observations; therefore, we can estimate  parameters by maximizing the pseudo-likelihood that aggregates all the local observations
This approach has been  widely used to learn MRF parameters for many different  applications from training datasets [NN, NN]
The reader is  referred to [N] for further details
In this paper, we estimate  parameters from a single light field
 Single-scene parameter estimation
Previous work for  similar purposes focused on stereo image pairs and used the  conventional framework: identical likelihood functions for  distribution fitting and MRF inference
Zhang et al
[NN]  aimed to build robust energy functions and achieved that  by performing soft EM on linear mixture models
However, the modeled distributions do not fit the histograms of  pixel difference well
Also, it takes six iterations to converge between parameters and depth maps
In contrast, Liu  et al
[NN] and Su et al
[NN] introduced advanced models to  fit natural images, but the inferred depth maps do not have  good accuracy
In this paper, we develop a new framework  with quick convergence in which separate likelihood functions are used: soft-EM ones for good model fitting and  hard-EM ones for robust energy functions
 Soft and Hard EM
They are conventional approaches  for maximum likelihood estimation with unobserved data  and usually used for different purposes
For example, the  EM algorithm (soft EM) for clustering minimizes likelihood and the K-means (hard EM) optimizes data distortion  [N0]
For neighborhood filters, Huang [N] proposed a neighborhood noise model (NNM) to estimate parameters
The  NNM fits heavy-tailed empirical distributions by soft EM  and reasons robust range-weighted filters by hard EM
In  this paper, we apply this approach to infer RPRFs for robust light-field stereo matching
We employ a similar model  for the data energy with a new kernel function and propose  a novel model for the smoothness energy to include depth  labels
 Light-field stereo matching
Light fields possess lots  of information for depth estimation, and previous work explored different vision cues for specific applications
Some  approaches employed features for dense light fields, such  as depth consistency [NN], reliable data terms [NN], bilateral consistency [N], phase shift [N], and occlusion in angular patches [NN]
Some others focused on clean physical or  textural cues, such as explicit occlusion [NN], ND line segments [NN], defocus [NN], sparse presentation [N], and convolutional networks [N]
In contrast, some targeted noisy  light fields and applied noise-resistant cues, such as focal  stack [NN] and adaptive refocus response [NN]
In this paper,  we get back to fundamentals in MRF inference and explore  intrinsic statistical cues for wide application scope
 N
RPRF Modeling  For a given light field, we estimate the disparity (inverse  depth) map for the center view in which each pixel p has a k-channel color signal zp and an unknown disparity label lp
The disparity map D = {lp} is derived by optimizing the global MRF energy  ∑  p      ∑  v∈V  Edpv(lp) + λ ∑  q∈NN(p)  Espq(lp, lq)     
(N)  The view-wise data energy Edpv measures photo- consistency by color difference between zp and the  corresponding signal yvp(lp) in a surrounding view v for the disparity lp
The edge-wise smoothness energy E  s pq  evaluates depth continuity by color-conditioned disparity  difference between a pixel pair p and q in N-connected neighborhood NN
Finally, the weight parameter λ determines their ratio of contribution to the global energy
 We reason and infer the MRF in (N) by constructing  pseudo-likelihood for pixel differences of each pixel p from its view-wise and spatial neighborhoods as shown in Fig
 N
In the following, we will introduce the view-wise pixel  difference model for the data energy first and then the novel  spatial model for the smoothness energy
 N.N
Pixel difference model for data energy  Let Xd be a random vector for the view-wise color difference xd , zp − yvp(lp)
Its empirical distribution is  NN    Figure N
Pseudo-likelihood modeling for RPRFs
For the viewwise neighborhood, color difference xd is modeled by a scale mixture with a soft-edge hidden variable wvp
For the spatial neighborhood, color difference xs and disparity difference h are formulated by different scale mixtures with an identical hidden upq 
 usually heavy-tailed, and we employ a model similar to the  NNM for good model fitting
We introduce a scale random  variable W to model color edges using soft decisions and formulate Xd in a Gaussian scale mixture (GSM):  Xd|W = w ∼ N (  0, σNd w  Ik  )  , (N)  fW (w) = N  Nd w−  k N eαdG(w), w ∈ [ǫd, N], (N)  where σd is a scale parameter for Xd, αd and ǫd are shape parameters for W , and Nd is the normalization factor for unity
The hidden prior function G(w) controls the distri- bution of W and will be shown its direct link to the energy function
Note that the σd here represents only distribution scaling, not for the noise intensity in the NNM
 Parameter estimation by soft EM Given an estimated  disparity map D̃ = {l̃p}, we can use the corresponding sig- nals from surrounding views {yvp(l̃p)} to update the pa- rameter set for data energy θd = (σd, αd, ǫd)  T 
Consider  a sufficient statistic td ,‖ xd ‖N
By marginalizing out the hidden W from the joint distribution fTd,W , we can have its soft-EM energy:  EsoftTd (td;θd) = − log ∫ N  ǫd  fTd,W (td, w;θd)dw
(N)  Note that it is a non-analytic function and requires numerical integrals for evaluation
The updated parameter θ̂d can  be derived by model fitting or, equivalently, global energy  optimization using empirical observations:  θ̂d = argmin θd  ∑  t̃d  EsoftTd (t̃d;θd), (N)  where t̃d =‖ zp − yvp(l̃p) ‖N
Then the parameter set θd can be estimated by iterative updates until converged
 Type Energy  E(x) Kernel  K(x) Hidden Prior  G(w)  Reciprocal − Nx+N N  (x+N)N N √ w − w  Gaussian −e−x e−x w(N− logw) Table N
Examples of robust hard-EM energy functions
We  adopt the Reciprocal energy as detailed in Section N.N
 Robust data energy by hard EM Define an energy function E(x) related to the prior G(w) by  E(x) , min w  (wx−G(w)) , x ≥ 0
(N)  Then we can construct the hard-EM energy for color difference Xd, given a parameter set θd, by maximizing the joint  distribution fXd,W with respect to the hidden W (select the best guess of w):  Ehard Xd  (xd) = − log max w  fXd,W (xd, w) (N)  = αdE  (‖ xd ‖NN NαdσNd  )  + C, (N)  where C is a constant offset and will be discarded be- cause only the energy difference matters for MRF inference
 Therefore, we have the view-wise data energy in (N) as  Edpv(lp) = E hard Xd  (zp − yvp(lp))
(N) For exploring the relationship between E(x) and G(w),  we define a kernel function K(x) by  K(x) , (G′) −N  (x)
(N0)  Then it can be shown using integration by parts that K(x) is exactly the derivative of E(x)
As a result, the three core functions are directly connected by  E′ = K = (G′) −N  
(NN)  Table N shows two examples, Reciprocal and Gaussian, for  a robust energy function E(x)
 N.N
Pixel difference model for smoothness energy  Let H be a random variable for the spatial disparity dif- ference h , |lp − lq|, Xs be a random vector for the corre- sponding color difference xs , zp − zq , and U be a soft- edge scale random variable
We propose a scale mixture of  generalized Gaussian distributions for fH|U to fit the thick- tailed distribution of H 
Along with a GSM for Xs, we construct the following joint model:  fH|U (h|u) = N  γ( N β + N)δ  u N  β e−u( h δ )β , (NN)  Xs|U = u ∼ N (  0, σNs u Ik  )  , (NN)  fU (u) = N  Ns u−(  k N + N  β )eαsG(u), u ∈ [ǫs, N], (NN)  NN    where δ and β are the scale and shape parameters for H , and the β is fixed to N.N in this paper
The σs, αs, ǫs and Ns serve the same purposes as the σd, αd, ǫd and Nd separately
 The disparity difference H shares the same edge prior U with the color difference Xs for inferring color-conditioned  MRF
Also, the additional factor u− N  β in fU (u), compared  to fW (w), is devised to cancel out the u N  β in fH|U (h|u) for the joint distribution fXs,H,U 
Thus the hard-EM smooth- ness energy can have a simple form similar to the case of  the data energy (N)
 Parameter estimation by soft EM Consider the parameter set θs = (δ, σs, αs, ǫs) T 
We update it using the empirical t̃s for a sufficient statistic ts ,‖ xs ‖N and also the em- pirical disparity difference h̃ from a given disparity map D̃
However, using their soft-EM joint energy, which marginalizes out U for fTs,H,U , will induce a computation issue: it needs to evaluate the non-analytic energy for every pair of  t̃s and h̃
Instead, we derive the updated θ̂s by using their marginal energy to speed up the process:  θ̂s = argmin θs      ∑  t̃s  EsoftTs (t̃s;θs) + ∑  h̃  EsoftH (h̃;θs)     
 (NN)  Robust smoothness energy by hard EM Similarly to  (N)-(N), we maximize the joint distribution fXs,H,U with re- spect to the hidden U and have the hard-EM energy for pixel differences xs and h as follows:  Ehard Xs,H  (xs, h) = αsE  (‖ xs ‖NN NαsσNs  + hβ  αsδβ  )  
(NN)  At last, we have the edge-wise smoothness energy in (N):  Espq(lp, lq) = E hard Xs,H  (zp − zq, |lp − lq|), (NN)  which constitutes a conditional random field
 N.N
On selection of energy function E(x)  The proposed models can fit light fields of different characteristics and track their heavy tails well as shown in Fig
 N(a)-(b)
The Reciprocal type in Table N can provide better  fitting accuracy than the conventional Gaussian one, especially for spatial difference ‖ xs ‖N
Therefore, we adopt it for the stereo matching algorithm in this paper
In addition, Fig
N(c)-(d) show the corresponding energy functions
The soft-EM ones are close to the empirical ones due  to the model fitting; however, they induce bad depth edges  as shown in Fig
N(c)
In contrast, the hard-EM ones have  similar values for small color difference but saturate quickly  as robust metrics for large difference
As a result, the depth  edges can be well preserved as Fig
N(d) shows
 N
Implementation for Stereo Matching  Parameter estimation Given an disparity map, we estimate the parameters θd and θs for data and smoothness  energy from the corresponding view-wise and spatial pixel  differences
For the θd, we applied the EM+ fitting method  designed for NNM in [N] to our soft-EM update formulation  (N)
Small changes were made for the model difference
For  the θs updated by (NN), we modified the method to include  the disparity difference with its additional parameter δ and statistics h̃
 Belief propagation Given the global MRF energy (N), we  use belief propagation (BP) [N] to solve the disparity map iteratively
We adopted the BP-M approach in [NN] for its efficient message propagation
We stop the BP-M if the global  energy is not decreased by more than N%, and around four iterations on average are performed in our experiments
 Energy approximation We use the linear-time algorithm  in [N] for fast message updating
To achieve this, we approximated the Reciprocal smoothness energy (NN) by the  truncated linear function with the least squared error:  Ehard Xs,H  ≃ αs min (  0.NNNN  δα N  β s b  N  β +N  h, N  b  )  + const, (NN)  where b = N + ‖xs‖  N  N  NαsσNs which can be calculated in advance  for each pixel pair p and q before running BP-M
 Adaptive selection for λ The parameters αd and αs sta- tistically determine the dynamic ranges of the data and  smoothness energy
We further use the heuristic parameter  λ to weight importance between them for different condi- tions
For example, denoised light fields relies on smoothness energy more than normal ones, so we assign larger values to λ for them
Also, we set the values proportional to the numbers of surrounding views, |V|
But they are lower trun- cated because the data energy using few views becomes unreliable
Lastly, we use two categories, λweak and λstrong, for scene-adaptive assignment as listed in Table N
 Entropy-based scene adaptability There are two typical  scenarios as shown in Fig
N: one needs a small λ, as Still- Life, to minimize depth errors, and the other prefers a large  λ, as Medieval
We found that the cross entropy H(h̃, h) be- tween the empirical and modeled disparity differences is a  good indicator for them
A small reduction of H(h̃, h) for λ from zero to a large value means the data energy outweighs  the smoothness one, so a weak λ is preferred
On the con- trary, a significant reduction encourages a strong λ
Based on this observation, we use the reduction ratio to adaptively  assign λweak and λstrong
 NN          Empirical Reciprocal Type Gaussian Type       Empirical Hard−EM (Reciprocal) Soft−EM  StillLife  0 N00 N0  −N  N0 −N  (KLD) Reciprocal: 0.NNNN Gaussian  : 0.NNNN  0 N00 N0  −N  N0 −N  (KLD) Reciprocal: 0.0NNN Gaussian  : 0.NNNN  0 N00 0  NN  0 N00 0  NN  Medieval  0 N00 N0  −N  N0 −N  (KLD) Reciprocal: 0.0NNN Gaussian  : 0.0NNN  0 N00 N0  −N  N0 −N  (KLD) Reciprocal: 0.0NNN Gaussian  : 0.NNNN  0 N00 0  NN  0 N00 0  NN  (a) Model fitting of  td =‖ xd ‖N  (b) Model fitting of  ts =‖ xs ‖N  (c) Data energy vs
 ‖ xd ‖N  (d) Smoothness energy vs
 ‖ xs ‖N (h = 0) Figure N
Modeling fitting and robust energy
The top row is for the light field StillLife and the bottom for Medieval
Ground-truth  disparity maps are used to generate empirical distributions
(a)-(b) Distribution fitting results using the Reciprocal and Gaussian types for  (a) view-wise color difference ‖ xd ‖N and (b) spatial difference ‖ xs ‖N with their Kullback-Leibler divergence (KLD) shown at the corners
(c)-(d) Corresponding energy functions of the Reciprocal type for (c) data energy and (d) smoothness energy
 Depth error Cross entropy  0 NN0 0  N  λ  N o r m a li z e d M S E        StillLife Medieval  0 NN0 0  N  −N0%  −NN%  λ  N or m al iz ed  H (h̃ ,h )  Figure N
Depth estimation quality vs
λ
Depth error is represented by mean squared error (MSE) in disparity
Cross entropy  measures the distance between the distributions of the empirical  disparity difference h̃ and the modeled h
Their values are both  normalized with respect to the case of λ = 0 for comparison
 Stereo matching algorithm At first, we initialize a disparity map Dini using MRF inference with default param- eters: λini = N00, θinid = (  √  N/N, N, 0.N), and θinis =  (0.0N, √  N/N, N, 0.N)
Then we update parameters and esti- mate depth iteratively
In each iteration, we use the λstrong  in Table N to infer the disparity map first
If the cross entropy H(h̃, h) is reduced by less than a ratio r, N0% in this paper, compared to the case of λ = 0, we will switch to the weak scenario and use λweak for inference instead
 N.N
Parameter estimation  N
Experiments  We perform extensive experiments on three datasets for  objective evaluation: HCI Blender [NN] and Berkeley [NN]  are synthetic, and HCI Gantry [NN] contains real pictures
 They are all dense light fields of N×N views, and we sub- sample them to produce sparse N×N and N×N test cases
We also generate denoised light fields by adding Gaussian noise  and then performing BMND [N]
For comparing objective  Condition λweak λstrong  Normal max(|V|/N, N) max(N|V|, NN) Denoised max(|V|/N, N) max(N|V|, NN)  Table N
Adaptive selection of λ
Two operating conditions are  considered: normal and denoised light fields
Two scene-adaptive  categories, λweak and λstrong, are further defined
 quality across view types, we calculate mean squared errors  (MSE) in disparity all with respect to the baselines of N×N light fields
The values are then multiplied by N00 to keep  significant figures and denoted by DMSE
 We will also show results for light fields captured by  Lytro ILLUM for demonstrating generalization capability
 Light-field RAW data from EPFL dataset [N0] and our own  pictures are processed using Lytro Power Tools Beta
Our  software and more results are available onlineN
 Robustness to initial conditions Different initial parameters lead to different initialized disparity maps Dini
For example, a large value of λini prefers the smoothness en- ergy and gives an over-smooth Dini
Note that an ideal Dini is the ground-truth depth itself
In this work, different Dini can generate similar MRF parameters as shown in Fig
N
 Such robustness can be explained by the corresponding empirical distributions in Fig
N
They differ mainly in distribution tails but behave similarly for small pixel difference;  therefore, the inferred hard-EM energy functions are also  similar
Note that the difference of the tails results in the  variation of ǫd and ǫs, but these two parameters do not af- fect the hard-EM energy and thus the MRF inference
 Nhttp://www.ee.nthu.edu.tw/chaotsung/rprf  NN  http://www.ee.nthu.edu.tw/chaotsung/rprf   Initial condition Ground-truth depth Data energy preferred Smoothness preferred Default in this work  (λ;σd, αd; δ, σs, αs) (0.N; 0.N, NN.0; 0.0N ,N.N, N.0) (N00.0; 0.N, N.0; 0.0N ,N.0, N0.0) (N00.0; 0.N, N.0; 0.0N ,N.N, N.0)  Initialized disparity  map Dini  DMSE=N.NN DMSE=NN.NN DMSE=N.NN  (Updated parameters) (N.0; N.N, N.N; 0.0N ,N.N, N.0) (N.0; N.N, N.0; 0.0N ,N.N, N.N) (N.0; N.N, N.N; 0.0N ,N.N, N.N) (N.0; N.N, N.N; 0.0N ,N.N, N.N)  Updated disparity  map D̃ (m=N)  DMSE=N.NN DMSE=N.NN DMSE=N.NN DMSE=N.NN  Figure N
Robust update for parameters and depth
Four initial disparity maps Dini for the N×N case of StillLife are used for comparison
One uses ground-truth depth (ideal)
The others are initialized by different parameter sets: one is noisy by weighting data energy more  (λ=0.N), one is over-smooth by weighting smoothness term more (λ=N00), and the last one is moderate using the default setting (λ=N00)
 The updated parameters for MRF energy are all similar in one iteration, and the accordingly estimated disparity maps show little difference
 Quick convergence The robustness to initialized disparity maps also results in the quick convergence of parameterdepth update iterations
Fig
N shows the accuracy of the  parameters estimated by the default initial condition compared to the ideal one
All MRF parameters can be well  estimated and converged in one iteration except δ
But such inaccuracy of δ only affects depth quality slightly, e.g
the second iteration increases its accuracy but only improves  the DMSE by 0.N% on average
Therefore, we set the default iteration number to one
 Parameter variation Consider the two bandwidth parameters of MRF inference: αdσ N d (data) and αsσ  N s (smoothness)
Over NxN light fields, their value ranges are [N.N,  N.N] and [N0.0, NN.N], respectively
They distribute more  uniformly than a normal distribution, and their excess kurtosis are -0.N and -0.N
For denoised light fields (σ=N0), the data bandwidth becomes N.Nx larger and the smoothness one 0.Nx smaller
All these variations can be estimated  well by this work
 N.N
Depth estimation  We compare our results (RPRF) with the globally consistent depth labeling (GCDL) [NN], line-assisted graph cut  (LAGC) [NN], phase-shift cost volume (PSCV) [N], and  occlusion-aware depth estimation (OADE) [NN]
We use the  codes provided by the authors
 Dense and sparse light fields Table N details the depth  estimation errors for dense N×N test cases, and our work has better performance
Fig
N further compares the results        Ground trunth Data preferred Smoothness Default  0 N00 N0  −N  N0 0  View-wise difference t̃d  0 N0 N0  −N  N0 0  Disparity difference h̃  Figure N
Empirical distributions
They are collected using the  initialized disparity maps in Fig
N
Note that the distributions of  t̃s are not related to disparity maps and are all the same
       σ d  α d  σ s  α s  δ   0% NN% N0% 0.N  0.N  N  Parameter Difference  C D  F  First iteration (m=N)   0% NN% N0% 0.N  0.N  N  Parameter Difference  C D  F  Second iteration (m=N)  Figure N
Parameter estimation accuracy
Accuracy is measured  by relative absolute difference, and cumulative distribution functions (CDFs) derived from all test cases for HCI Blender dataset
 from dense to sparse light fields
The GCDL and OADE fail  in sparse cases, while the LAGC becomes worse in denser  ones
In contrast, PSCV and our work have constant quality  over these view types
We demonstrate that sparse views  can generate depth maps as great as dense ones do if robust  energy from clean images is used
Fig
N shows an example  for subjective evaluation
 NN    Figure N
Depth estimation error vs
Light-field view type
The average errors are represented in DMSE (log scale)
 Ground truth GCDL LAGC PSCV OADE RPRF  N×N  N×N  Figure N
Estimated depth maps for LivingRoom
This work infers good depth edges in both N×N and N×N test cases, especially around the chair arm and the lamps
GCDL and OADE fail in the N×N case, and LAGC and PSCV produce over-smooth depth
 Table N
Depth errors in DMSE for dense N×N light fields
 Denoised light fields Applying the RPRF to the light  fields denoised by BMND produces better depth than directly applying to the noisy ones
It can even outperform  the algorithms designed for noisy light fields in [NN, NN] as  shown in Table N
The results regarding denoising conditions are summarized in Fig
N0
In these cases, fewer views  will decrease depth quality owing to the less reliable energy
 Real scenes Fig
NN shows results for light fields captured  by Lytro ILLUM
Our work constantly produces similarly  good depth maps using only N×N views compared to Lytro software that uses raw light fields
Other algorithms also  perform well but occasionally cause obvious artifacts
 Execution time We implement parameter estimation in  MATLAB and the other parts in C++
For one light field,  the parameter estimation and BP-M take about N and N0  seconds respectively on average
The remaining computation is mostly contributed by computing data energy
The  run times are summarized in Table N
Our work runs much  faster for its simple but efficient MRF formulation
 N
Discussion and Limitation  Occlusion handling
Instead of explicit handling as  [NN], we show great depth quality can be achieved by implicit modeling with the soft-edge prior w
A value toward zero represents more likely occlusion
In this case, hard-EM  energy will saturate data cost, which equivalently separates  the occlusion pixel in a soft way
Therefore, the fact that  hard-EM energy is better than soft-EM one also confirms  the necessity of occlusion formulation
 Scene statistics
Consider a scene has two regions of  different statistics, e.g
tablecloth and fruits in StillLife
In  NN    Figure N0
Depth estimation error vs
Denoising conditions
The cases with too large DMSE are omitted for clarity
 Center view Lytro (Raw) LAGC (N×N) PSCV (N×N) OADE (N×N) RPRF (N×N)  Figure NN
Estimated depth maps for real scenes
The first two light fields are from EPFL dataset, and the last two captured by us
 Table N
Depth errors in DMSE for noisy light fields
The numbers reported by Lin et al
[NN] and Williem et al
[NN] are used
 Table N
Average run time in seconds per light field
GCDL ran  on a GPU, GeForce GT NN0, and others on a N.N GHz CPU
 this case, our model will capture mixed statistics, and the  result could be sub-optimal
In this viewpoint, a possible  extension of this work is to segment a scene into different  regions and then learn parameters separately
 Hyper-parameter λ
It cannot be explained by RPRF and thus requires heuristic estimation
We found that depth  quality is not sensitive to its small variation, so we applied  an entropy-based heuristic for coarse-level adaptability
A  possible extension of this work is to devise a more delicate  heuristic to improve accuracy
 N
Conclusion  In this paper, we propose an empirical Bayesian  framework—RPRF—to provide statistical adaptability and  good depth quality for light-field stereo matching
Two  scale mixtures with soft-edge priors are introduced to model  the data and smoothness energy
We estimate scenedependent parameters by pseudo-likelihood fitting via soft  EM and infer depth maps using robust energy functions via  hard EM
Accordingly, we build a stereo matching algorithm with efficient implementation
The effectiveness is  demonstrated by experimental results on dense, sparse, and  denoised light fields
It outperforms state-of-the-art algorithms in terms of depth accuracy and computation speed
 We also believe that this framework can be extended in  many possible ways to achieve better depth quality
 Acknowledgment  This work was supported by the Ministry of Science and  Technology, Taiwan, R.O.C
under Grant No
MOST N0NNNNN-E-00N-00N-MYN
 NN    References  [N] J
Besag
Statistical analysis of non-lattice data
Journal  of the Royal Statistical Society Series D (The Statistician),  NN(N):NNN–NNN, Sept
NNNN
N  [N] A
Blake, P
Kohli, and C
Rother
Markov Random Fields  for Vision and Image Processing
The MIT Press, N0NN
N, N  [N] C
Chen, H
Lin, Z
Yu, S
B
Kang, and J
Yu
Light field  stereo matching using bilateral statistics of surface cameras
 In IEEE International Conference on Computer Vision and  Pattern Recognition, pages NNNN–NNNN, N0NN
N  [N] K
Dabov, A
Foi, V
Katkovnik, and K
Egiazarian
Image denoising by sparse N-D transform-domain collaborative filtering
IEEE Transactions on Image Processing,  NN(N):N0N0–N0NN, Aug
N00N
N, N  [N] P
F
Felzenszwalb and D
P
Huttenlocher
Efficient belief  propagation for early vision
International Journal of Computer Vision, N0(N):NN–NN, Oct
N00N
N  [N] S
Heber and T
Pock
Convolutional networks for shape  from light field
In IEEE International Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN,  N0NN
N  [N] C.-T
Huang
Bayesian inference for neighborhood filters  with application in denoising
IEEE Transactions on Image  Processing, NN(NN):NNNN–NNNN, Nov
N0NN
N, N  [N] H.-G
Jeon, J
Park, G
Choe, J
Park, Y
Bok, Y.-W
Tai,  and I
S
Kweon
Accurate depth map estimation from a  lenslet light field camera
In IEEE International Conference  on Computer Vision and Pattern Recognition, pages NNNN–  NNNN, N0NN
N, N  [N] O
Johannsen, A
Sulc, and B
Goldluecke
What sparse light  field coding reveals about scene structure
In IEEE International Conference on Computer Vision and Pattern Recognition, pages NNNN–NNN0, N0NN
N  [N0] M
Kearns, Y
Mansour, and A
Y
Ng
An informationtheoretic analysis of hard and soft assignment methods for  clustering
In Proceedings of the Thirteenth Conference on  Uncertainty in Artificial Intelligence, pages NNN–NNN, NNNN
 N  [NN] C
Kim, H
Zimmer, Y
Pritch, A
Sorkine-Hornung, and  M
Gross
Scene reconstruction from high spatio-angular  resolution light fields
ACM Transactions on Graphics,  NN(N):NN:N–NN:NN, July N0NN
N  [NN] V
Kolmogorov and R
Zabih
Multi-camera scene reconstruction via graph cuts
In Proceedings of the Nth European  Conference on Computer Vision, pages NN–NN, N00N
N  [NN] S
Kumar and M
Hebert
Discriminative random fields: a  discriminative framework for contextual interaction in classification
In IEEE International Conference on Computer  Vision, volume N, pages NNN0–NNNN, N00N
N  [NN] H
Lin, C
Chen, S
B
Kang, and J
Yu
Depth recovery from  light field using focal stack symmetry
In IEEE International  Conference on Computer Vision, pages NNNN–NNNN, N0NN
N,  N, N, N  [NN] Y
Liu, L
K
Cormack, and A
C
Bovik
Statistical modeling  of N-D natural scenes with application to Bayesian stereopsis
IEEE Transactions on Image Processing, N0(N):NNNN–  NNN0, Sept
N0NN
N  [NN] D
Scharstein and C
Pal
Learning conditional random fields  for stereo
In IEEE International Conference on Computer  Vision and Pattern Recognition, N00N
N  [NN] C.-C
Su, L
K
Cormack, and A
C
Bovik
Color and depth  priors in natural images
IEEE Transactions on Image Processing, NN(N):NNNN–NNNN, June N0NN
N  [NN] R
Szeliski, R
Zabih, D
Scharstein, O
Veksler, V
Kolmogorov, A
Agarwala, M
Tappen, and C
Rother
A  comparative study of energy minimization methods for  Markov random fields with smoothness-based priors
IEEE  Transactions on Pattern Analysis and Machine Intelligence,  N0(N):N0NN–N0N0, June N00N
N  [NN] M
W
Tao, S
Hadap, J
Malik, and R
Ramamoorthi
Depth  from combining defocus and correspondence using lightfield cameras
In IEEE International Conference on Computer Vision, pages NNN–NN0, N0NN
N  [N0] M
Řeřábek and T
Ebrahimi
New light field image dataset
 In Nth International Conference on Quality of Multimedia  Experience (QoMEX), N0NN
N  [NN] T.-C
Wang, A
A
Efros, and R
Ramamoorthi
Occlusionaware depth estimation using light-field cameras
In IEEE  International Conference on Computer Vision, pages NNNN–  NNNN, N0NN
N, N, N, N, N  [NN] S
Wanner and B
Goldluecke
Globally consistent depth labeling of ND light fields
In IEEE International Conference  on Computer Vision and Pattern Recognition, pages NN–NN,  N0NN
N, N, N  [NN] S
Wanner, S
Meister, and B
Goldluecke
Datasets and  benchmarks for densely sampled ND light fiels
In Vision,  Modeling, and Visualization, pages NNN–NNN, N0NN
N, N  [NN] W
Williem and I
K
Park
Robust light field depth estimation for noisy scene with occlusion
In IEEE International  Conference on Computer Vision and Pattern Recognition,  pages NNNN–NN0N, N0NN
N, N, N  [NN] Z
Yu, X
Guo, H
Ling, A
Lumsdaine, and J
Yu
Line assisted light field triangulation and stereo matching
In IEEE  International Conference on Computer Vision, pages NNNN–  NNNN, N0NN
N, N, N  [NN] L
Zhang and S
M
Seitz
Estimating optimal parameters for  MRF stereo from a single image pair
IEEE Transactions on  Pattern Analysis and Machine Intelligence, NN(N):NNN–NNN,  Feb
N00N
N  NNA Lightweight Approach for On-The-Fly Reflectance Estimation   A Lightweight Approach for On-the-Fly Reflectance Estimation  Kihwan KimN Jinwei GuN Stephen TyreeN Pavlo MolchanovN Matthias NießnerN Jan KautzN  NNVIDIA NTechnical University of Munich  http://research.nvidia.com/publication/reflectance-estimation-fly  Abstract  Estimating surface reflectance (BRDF) is one key component for complete ND scene capture, with wide applications in virtual reality, augmented reality, and human  computer interaction
Prior work is either limited to controlled environments (e.g., gonioreflectometers, light stages,  or multi-camera domes), or requires the joint optimization  of shape, illumination, and reflectance, which is often computationally too expensive (e.g., hours of running time) for  real-time applications
Moreover, most prior work requires  HDR images as input which further complicates the capture process
In this paper, we propose a lightweight approach for surface reflectance estimation directly from N- bit RGB images in real-time, which can be easily plugged  into any ND scanning-and-fusion system with a commodity RGBD sensor
Our method is learning-based, with an  inference time of less than N0ms per scene and a model  size of less than NN0K bytes
We propose two novel network architectures, HemiCNN and Grouplet, to deal with  the unstructured input data from multiple viewpoints under  unknown illumination
We further design a loss function to  resolve the color-constancy and scale ambiguity
In addition, we have created a large synthetic dataset, SynBRDF,  which comprises a total of N00K RGBD images rendered with a physically-based ray tracer under a variety of natural illumination, covering N000 materials and N000 shapes
SynBRDF is the first large-scale benchmark dataset for reflectance estimation
Experiments on both synthetic data  and real data show that the proposed method effectively recovers surface reflectance, and outperforms prior work for  reflectance estimation in uncontrolled environments
 N
Introduction  Capturing scene properties in the wild, including its ND  geometry and surface reflectance, is one of the ultimate  goals of computer vision, with wide applications in virtual reality, augmented reality, and human computer interaction
While ND geometry recovery has achieved high accuracy, especially with recent RGBD-based scanning-andfusion approaches [N, NN, NN], surface reflectance estima(a) (b) (c)  Figure N
Overview of our method: we take RGBD image sequences as inputs (a)
During a reconstruction process, each view  contributes voxels as an observation
In (b), we visualize the color  of visible voxel samples from a specific view (red circle among  red dots indicating locations of other views)
These samples from  all views are evaluated through either HemiCNN (Sec
N.N.N) or  Grouplet networks (Sec
N.N.N) to estimate the BRDF in real time
 In (c), we show a rendered image with predicted BRDF and captured lighting and shape
More examples are shown in Sec
N
 tion still remains challenging
At one extreme, most of  the fusion methods assume Lambertian reflectance and recover surface texture only
At the other extreme, most of the  prior work on surface BRDF (bidirectional reflectance distribution function) estimation [NN, N0, NN] aims to recover  the full ND BRDF function, but are often limited to controlled, studio-like environments (e.g., gonioreflectometers,  light stages, multi-camera domes, planar samples)
 Recently, a few methods [NN, NN, NN, NN, N0, NN, N] were  proposed to recover surface reflectance in uncontrolled environments (e.g., unknown illumination or shape) by utilizing statistical priors on natural illumination and/or BRDF
 These methods formulate the inverse rendering problem as  a joint, alternative optimization among shape, reflectance,  and/or lighting
Despite their accuracy, these methods are  computationally quite expensive (e.g., hours or days of  running time and tens of gigabytes of memory consumption) and are often run in a post-process rather than a realtime setting
Moreover, these methods often require highresolution HDR images as input, which further complicates  the capturing process for real-time or interactive applications on consumer-grade mobile devices
 N N0  http://research.nvidia.com/publication/reflectance-estimation-fly   In this paper, we propose a lightweight and practical  approach for surface reflectance estimation directly from  N-bit RGB images in real-time
The method can be easily plugged into any ND scanning-and-fusion system with a  commodity RGBD sensor and enables physically-plausible  renderings at novel lighting/viewing conditions, as shown  in Fig
N
Similar to prior work [NN, NN], we use a simplified BRDF representation and focus on estimating surface  albedo and gloss rather than full ND BRDF function
Our  method is learning-based, with an inference time of less  than N0ms per scene and a model size of less than NN0K  bytes
 In order to deal with unstructured input data (e.g., each  surface point can have a different number of observations  due to occlusions) in the context of neural networks, we  propose two novel network architectures – HemiCNN and  Grouplet
HemiCNN projects and interpolates the sparse  observations onto a ND image, which enables the use of  standard convolutional neural networks
Grouplet learns directly from random samples of observations and uses multilayer perception networks
Both networks are also designed  to be lightweight in both inference time and model size for  real-time applications on consumer-grade mobile devices
 In addition, since the illumination is unknown, we have designed a novel loss function to resolve the color-constancy  and scale ambiguity (i.e., only given input images, we do  not know whether surfaces are reddish or the lighting is reddish, or whether surfaces are dark or the lighting is dim)
To  the best of our knowledge, this is the first lightweight, realtime approach for surface reflectance estimation in the wild
 We also created a large-scale synthetic dataset —  SynBRDF— for reflectance estimation
SynBRDF covers  N000 materials randomly sampled from OpenSurfaces [N], N000 shapes from ScanNet [N], and a total of N00K RGBD images (both HDR and LDR) rendered from multiple viewpoints with a physically-based ray tracer under N0 natural environmental illumination conditions, making it an ideal  benchmark dataset for complete image-based ND scene reconstruction
 Finally, we incorporated the proposed approach with  RGBD scanning-and-fusion for complete ND scene capture  (see Sec
N and Fig
N)
We trained our networks with SynBRDF and directly applied the trained models on real data  captured with a commodity RGBD sensor
Experiments on  both synthetic data and real data show that the proposed  method effectively recovers surface reflectance and outperforms prior work for surface reflectance estimation in uncontrolled environments
 N
Related Work  Surface Reflectance Estimation in Uncontrolled Environments Most prior work in this direction formulate the  inverse rendering problem as a joint optimization among  the three radiometric ingredients—lighting, geometry, and  reflectance—from observed images
Barron et al
[N] assume Lambertian surfaces and optimize all the three components
Others [N0, N, NN, N] optimize reflectance and illumination with known ND geometry, either from motion or  based on statistical priors on natural illumination and materials
Recently, Wu et al
[NN] and Lombardi et al
[NN]  proposed to jointly estimate lighting, reflectance, and ND  shape from a RGBD sensor, even in the presence of interreflection
Chandraker et al
[N] investigated the theoretical  limits of material estimation from a single image
Despite  their effectiveness, these methods solve complicated optimization problems iteratively, which is computationally too  expensive for real-time applications (e.g., hours of running  time)
As the optimization relies heavily on the parametric forms of statistical priors, these methods generally require good initialization and HDR images as input
In contrast, our proposed method is a lightweight and practical  approach that can estimate surface reflectance directly from  N-bit RGB images on-the-fly, which is suitable for real-time  applications
 Material Perception and Recognition Our work is also  inspired from prior work on material perception and recognition from images
Pellacini et al
[NN] designed a  perceptually-uniform representation of the isotropic Ward  BRDF model [NN], and Wills et al
[NN] extend to datadriven models with measured reflectance
Fores et al
[NN]  studied the metrics used for BRDF fitting [NN]
Fleming et al
[N0] found that natural illumination is key for the  perception of surface reflectance
Bell et al
[N] released a  large dataset—OpenSurfaces—with annotated surface appearance from real-world materials
These prior works inspired us in designing the regression loss and creating a  synthetic dataset for training
For learning-based material  recognition, Liu et al
[NN] proposed a Bayesian approach  based on a bag of visual features
Bell et al
[N] used CNNs  (convolutional neural networks) for material recognition  from material context input
Recently, Wang et al
[NN] proposed a CNN-based method for material recognition from  light field images
These prior work shows neural networks  are capable of learning discriminative features for material  perception from images
 Reflectance Maps Estimation and Intrinsic Image Decomposition Intrinsic image decomposition aims to factor an input image into a shading-only image and a  reflectance-only image
Recently, CNNs has been successfully employed for intrinsic image decomposition [NN, NN]  from a single image
Bell et al
[N] proposed a dense CRFbased method and released a large intrinsic image dataset  generated by crowdsourcing
Zhou et al
[NN] used deep  learning to infer data-driven priors from sparse human annotations
Rematas et al
[NN] used CNNs to estimate reNN    (a) (b) (c) (d) (e) (f) Figure N
Overview of our framework: (a) BRDF examples from OpenSurface [N], (b) Input image Ii and depth Di streams
Integrated  volume is shown in (c), where the colors shown from ith view Ii (the red circle with pose Ti) are visualized
Small red dots refer to the  locations of other views
(d) shows the data that we extract from each voxel vk for training: normal nk, observation vector oi (the green  arrow), and color values Cik from the observation Ii at the voxel vk
In (e), these measurements together with color statistics (F̄i and B̄i)  are fed into one of the two networks, HemiCNN (Sec
N.N.N) and Grouplet (Sec
N.N.N) for BRDF estimation
 flectance maps (defined as ND reflectance under fixed, unknown illumination) from a single image
These methods  recover only the illumination-dependent reflectance map,  while our method estimates the full BRDF that enables rendering under novel illumination and viewing conditions
 N
Method  Our goal is to develop a module for real-time surface  reflectance estimation that can be plugged into any ND  scanning-and-fusion methods for ND scene capture, with  potential applications in VR/AR
In this paper, we make a  significant step towards this goal, and propose two novel  networks for homogeneous surface reflectance estimation  from RGBD image input
 N.N
Framework and Reflectance Model  Our framework takes as input RGBD image/depth sequences from a commodity depth sensor (Fig
N)
We  denote RGB color observation as C : ΩC → RN, images with I : ΩI → RN and depth maps with D : ΩD → R
The N acquired RGBD frames consist of RGB color images Ii, and depth maps Di (with frame index i ∈ N 


N )
We also denote the absolute camera poses Ti = (R, t) ∈ SE(N), t ∈ RN and R ∈ SO(N) of the re- spective frames, which is computed from standard volumebased pose-estimation algorithm [N].N As shown in Fig
N,  the input Ii, and Di are aligned and integrated into a ND volume  A  with signed-distance fusion [NN], from which we  extract voxels vk ∈  A  (k ∈ N 

.M ) that contain observed color Cik from the corresponding view Ii, its surface normal nk, and camera orientation oi, see Fig
N(d)
Additionally,  we compute the color statistics for each view by simply taking the average of foreground and background pixels in Ii, NDuring training, we randomly generated poses for rendering scenes
 denoted as F̄i and B̄i, respectively
We will discuss these further in Sec
N.N
 For the representation of surface reflectance, similar to  prior work [NN, NN], we choose a parametric BRDF model—  the isotropic Ward BRDF model [NN]—for two reasons: (N)  the Ward BRDF model has a simple form but is representative for a wide variety of real-world materials [NN], and (N)  prior studies [NN, NN] on BRDF perception are based on the  Ward BRDF model
Specifically, the isotropic Ward BRDF  model is given by:  f(ωi, ωo; Θ) = ρd π  + ρs · exp  (  − tanN θh/αN )  NπαN √ cos θi cos θo  , (N)  where ωi = (θi, φi) and ωo = (θo, φo) are the incident and viewing directions, θh is the half angle, and Θ = (ρd, ρs, α) is the parameter to be estimated
 An equivalent, but perceptually-uniform representation  of the Ward BRDF model was proposed in [NN], where the  diffuse albedo ρd is converted from RGB to CIE Lab col- orspace, (L, a, b), and the gloss is described by variables c, the contrast of gloss, and d, the distinctness of gloss
Vari- ables c and d are related to the BRDF parameters by [NN]:  c = N √  ρs + ρd/N− N √  ρd/N, d = N− α
(N)  Thus, an alternative representation for the BRDF parameters is Θ = (L, a, b, c, d)
Our problem is thus formulated as follows
Given a  set of voxels from any ND scanning-and-fusion pipeline,  {vk} = {{Cik,oi} ,nk}, we estimate the optimal BRDF parameters Θ with neural networks
Two problems need to be solved for learning
First, what is a good loss function that can resolve the color constancy and scale ambiguities due to unknown illumination? For example, just from  input images, we cannot tell whether the material is reddish or the illumination is reddish, or whether the material  NN    Name Ed(Θ, Θ̂)  RMSEN ||ρd − ρ̂d|| N + ||ρs − ρ̂s||  N + ||α− α̂||N  RMSEN ||Lab − ˆLab|| N + λg||cd − ĉd||  N  Cubic Root N √  ∫  ωi,ωo ||f(ωi, ωo; Θ)− f(ωi, ωo; Θ̂)|| cos θidωodωo  Table N
Three options for the distance function Ed(Θ, Θ̂) for BRDF estimation
RMSEN and RMSEN are root mean squared  error using Θ = (ρd, ρs, α) and Θ = (L, a, b, c, d), respectively, where the latter is the sum of the perceptual color difference and  the perceptual gloss difference (λg = N) [NN]
Cubic Root is the cosine-weighted ℓN-norm of the difference of two ND BRDF functions, inspired by BRDF fitting [NN, NN]
 is dark or the illumination is dim
Second, the input data  is unstructured — different voxels have different numbers  of observations due to occlusion
What is a good network  architecture for such unstructured input data? We address  these two problems in the following sections
 N.N
Design of the Loss Function  A key part for network training is an appropriate loss  function
Prior work [NN, NN] has shown that the commonlyused ℓN norm (i.e., MSE) is not optimal for BRDF fitting
 We design the following loss:  J = Ed(Θ, Θ̂) + λEc(Θ̂, {Cik}), (N)  where Ed(·, ·) measures the discrepancy between the esti- mated BRDF parameters and the ground truth and Ec(·, ·) is a regularization term which relates the estimated reflectance  Θ̂ with observed image intensities {Cik}
Ec aims to resolve the aforementioned scale and color constancy ambiguities  and is weighted by λ = 0.0N in all our experiments Table (N) lists three options for Ed(Θ, Θ̂) implemented  in this paper
RMSEN and RMSEN are the root mean  squared error with Θ = (ρd, ρs, α) and Θ = (L, a, b, c, d), respectively, where the latter is the sum of the perceptual  color difference and the perceptual gloss difference (λg = N) [NN]
Cubic Root is inspired from BRDF fitting [NN, NN], which is a cosine weighted ℓN-norm of the difference between two ND BRDF functions
 For Ec, we use the color statistics computed for each  view, F̄i and B̄i, to approximately constrain the estimation  of ρd and ρs
Specifically, Ec is derived based on the rendering equation [NN] as  Ec = ∑  i  ||(ρd + ρs) · B̄ γ i − F̄  γ i ||  N, (N)  where F̄i and B̄i are the average image intensity of the foreground and background regions of the i-th input image Ii,  γ = N.N is used to convert the input N-bit RGB images to linear images; see the Appendix for a detailed derivation
 Even though Eq
(N) is only an approximation of the rendering equation, it imposes a soft constraint on the scale and  Figure N
Details of HemiCNN
Top row: generating a voxel hemisphere image, from the sparse ND set of the observations {Cik} of voxel vk to a dense ND hemisphere representation
Bottom row:  the HemiCNN siamese convolutional neural network architecture
 color cues for the BRDF estimation
We found it quite effective for working with real data (see Fig
N)
 N.N
Network Architectures  As shown in Fig
N, our input data is unstructured because the observations Cik for each voxel vk are irregular, sparse samples on the ND slice of the ND BRDF
Different voxels may have different numbers of observations due  to occlusion
In order to feed the unstructured input data  into networks for learning, we propose two new neural network architectures
One is called Hemisphere-based CNN  (HemiCNN) which projects and interpolates the sparse observations onto a ND image, enabling the use of standard  convolutional neural networks
The other architectures,  called Grouplet, learns directly from randomly sampled observations and uses a multilayer perceptron network
Both  networks are also designed to be lightweight in both inference time per scene (≤N0ms) and model size (≤NN0KB)
 N.N.N Hemisphere-based CNN (HemiCNN)  For HemiCNN, as shown in the top row of Fig
N, the RGB  observations {Cik} of voxel vk are projected onto a unit sphere centered at the sample voxel vk
The unit sphere  is rotated so the positive z-axis is aligned with the voxel’s  surface normal nk
Observations {Cik} on the positive hemisphere (i.e., z > 0) are projected onto the ND x-y plane
Finally, a dense ND image, denoted a sample hemisphere image, is generated using nearest-neighbor interpolation among the projected observations
 A siamese convolutional neural network is used to predict BRDF parameters from a collection of N sample hemisphere images, one for each of a representative set of voxels, e.g., chosen by clustering on voxel positions or surface  normals
As shown in Fig
N, in the first of two stages  the siamese convolutional network operates on each sample  hemisphere image individually to produce a vector representation, after which the representations are merged across  NN    �������������  �	������������  ������  ���  �	������������  ������  ���  ��	�������	���  ���  ���  �������������  �����������������	���������  ���  ���  �������	�����	�  ���� 	����������  �������������  �	������������  ������  ���  �	������������  ������  ���  ���  ���  �!"  ���  �!"  ���  ���  ���  ���  ����#������������#��������  ��	�������	���  $  ��  "%&  �	�����������' �(�) ��	*������*����	 �(��+������	������	����� &(�,*�	�������	��	���������	 %(�,*�	����� ��-�	���������	  � �������������	������*�.�� � �� 	���������	�����  �!"  ���� 	����������  Figure N
The Grouplet model for BRDF estimation relies on aggregating results from a set of weak regressors (nodes)
Each  node operates on a randomly sampled voxel from the object
M  branches form the input to a node; each samples randomly from  a set of observations
Intermediate representation from multiple  nodes are combined by a moment pooling layer
BRDF parameters are regressed from the output of the moment pooling layer
 the N samples (i.e., voxels) by computing an element-wise  maximum
The network includes two convolutional layers, each with NN sets of N× N filters with ReLU activa- tions, a single N×N max-pooling layer, and a single fully- connected layer with NN neurons
After aggregating the N feature vectors with an elementwise-maximum, we use a  fully-connected layer with NN neurons, followed by tanh ac- tivation, and a final fully-connected layer to produce the  BRDF prediction
In most experiments with HemiCNN,  we set N = NN
The model size is NNKB and the average inference time is NNms per scene
 N.N.N Sampling-based Network (Grouplet)  The second proposed network architecture is called Grouplet (Fig
N)
Unlike HemiCNN, where we transform sparse  observations to ND images to use standard convolution layers, Grouplet directly operates on each observation Cik for each voxel vk
Grouplet relies on aggregating results from  a set of weak regressors called nodes
Each node estimates  an intermediate representation of the BRDF parameters of a  single voxel (vk) from M randomly sampled observations  Γk = {CNk, · · · , CMk}, as shown in Fig
N
A different sub- sets of observations is sampled for each voxel
Each observation from Γk is processed by a two-layer multilayer per- ceptron (MLP) with NNN neurons per layer, called a branch
Inputs to each branch are observed color (Cik), viewing di- rection (oi), averaged foreground color (F̄i) and averaged  background color (B̄i)
 Next, the output of M branches are concatenating together with the voxel’s surface normal (nk)
This vector  is processed by another two-layer MLP with NNN and NNN neurons in the layers, the output of which is the intermediate  representation of the BRDF parameters
During BRDF estimation, we operate on N voxels, each of which is processed  by different nodes with shared weights
To combine the intermediate representations computed from several voxels,  we use a moment pooling operator that is invariant to the  number of nodes
We pool with the first and second central moments which represent expected value and variance  of the intermediate representation across nodes
The output  of the pooling operator is a NNN-dimensional pooled repre- sentation
 The final part of the network estimates the BRDF parameters from the pooled representation by another MLP with  two hidden layers of NNN neurons each, and one final out- put layer
All layers throughout the model use hyperbolic  tangent activation functions except for the last output layer
 Grouplet is able to work with any number of nodes due  to the use of pooling operators
It also does not require that  the number of nodes be the same during training and testing
However, the order of the M observations in the branch  networks is important
We found the best results by sorting  observations by the cosine distance between the observation  vector (oi) and the voxel’s surface normal (nk)
For BRDF  estimation, Grouplet is applied in two forms, Grouplet-fast  and Grouplet-slow, with N = N0 and N = NNN voxels, re- spectively
Each constructs M=N nodes per voxel
The av- erage inference time is Nms for Grouplet-fast and N0ms for Grouplet-slow
The model size for both Grouplet-fast and  Grouplet-slow is NNNKB
Unless otherwise noted, Grouplet refers to Grouplet-slow
 For both HemiCNN and GroupLet, we set λg = N and explore a range of λ, finding 0.N ≤ λ ≤ N to be a reasonable range
We train HemiCNN using RMSProp with learning  rate 0.000N and N00K minibatches
For Grouplet training, we use stochastic gradient descent with fixed learning rate  0.0N and momentum 0.N for NNK minibatches
 N.N
SynBRDF: A Large Benchmark Dataset  Deep learning requires a large amount of data
Yet, for  BRDF estimation, it is extremely challenging to obtain a  large dataset with measured BRDF data due to the complex  settings required for BRDF acquisition [NN, NN]
Moreover,  while there are quite a few recent works for ND shape recovery and reflectance estimation in the wild [NN, NN, NN],  we are not aware of a large-scale, benchmark dataset with  ground-truth shape, reflectance, and illumination
 With these motivations, we created SynBRDF which is,  to our knowledge, the first large-scale, synthetic benchmark dataset for BRDF estimation
SynBRDF covers N000 materials randomly sampled from OpenSurfaces [N], N000 shapes randomly sampled from ScanNet [N], and a total  NN    Figure N
SynBRDF: (Left) Thumbnails of the first frame of each  example (each contains N00 different observations), (Right) Some  examples with depth map (insets)  of N00K RGB and depth images (both HDR and LDR) rendered from multiple viewpoints with a physically-based  raytracer [NN], under variants of N0 natural environmental illumination maps
SynBRDF thus has ground truth for  ND shape, BRDF, illumination, and camera pose, making  it an ideal benchmark dataset for evaluating image-based  ND scene reconstruction and BRDF estimation algorithms
 As shown in Fig
N, each scene is labeled with ground truth  Ward BRDF parameters (Eq
N and N)
For more flexible  evaluation that allows other types of rendering (e.g., global  illumination) for the same scene, we will also provide the  N0K XML files that indicate the original OpenSurface ma- terials and contain the variations of environmental and object model settings
We believe this dataset will be valuable  for further study in this direction
 In our experiments, we used SynBRDF for training and  evaluation
We randomly chose N00 from the total N000 scenes as a holdout test set, and the remaining NN00 scenes for training
For real data experiments, we directly applied  the trained models without any domain adaptation
 Network Loss RMSE User Rank  Grouplet RMSEN+Ec 0.NNN N Grouplet RMSEN 0.NNN N  HemiCNN RMSEN 0.NNN N HemiCNN CubeRoot 0.NNN N HemiCNN RMSEN 0.NNN N HemiCNN CubeRoot+Ec 0.NNN N  Grouplet RMSEN 0.NNN N  Table N
Average RMSE (w.r.t ground truth BRDF parameters) on  the test set of SynBRDF and the rank of user preferences from  a perceptual study of rendered materials
Among the variants of  our proposed method, we list the top seven methods based on the  results of the user study
In general these provide most plausible  results among all testing data (see results in Fig
N and Fig
N)
 Note that RMSE ranking is not always consistent with the ranking  of user study
Further, as shown in Fig
N, CubeRoot+Ec provides  most plausible results for the real data
Additional evaluations are  found in the Supplementary Material
 Figure N
Error versus the number of observations for three methods: HemiCNN (blue), Grouplet-fast (N0 voxel samples, red), and  Grouplet-slow (NNN voxel samples, orange)
A larger set of observations yields noticeably improved predictions for all three methods, but begins to saturate around N0
The common scan-and-fuse  method does not always guarantee a rich coverage of observations
 N
Experimental Results  We evaluated multiple variants of the proposed networks, changing the loss function Ed and BRDF representa- tions as described in Sec N.N
In Sec
N.N, we evaluate these  settings on SynBRDF, showing quantitatively and qualitatively that several combinations give accurate predictions  on our synthetic dataset
In Sec
N.N, we compare with prior  work [NN]
Finally in Sec
N.N, we demonstrate the proposed  methods within the KinectFusion pipeline for complete ND  scene capture with real data
 N.N
Results on Synthetic Data  We evaluated all variants of the proposed methods on the  test set of SynBRDF
Evaluating the quality of BRDF estimation is challenging [NN]—perceived quality often varies  with the illumination, ND shape, and even display settings
 Estimates with the lowest RMSE error on the BRDF parameters are not necessarily the best for visual perception
 Thus, in addition to computing the RMSE with respect to  the ground truth BRDF parameters, we also conducted a  user study
We randomly chose N0 materials from the test set, and rendered the BRDF predictions for each material  under (a) natural illumination and (b) moving point light  sources
The rendered images are similar to Fig
N
We then  asked N0 users to rank the methods on each material based on the perceptual similarity between the ground truth and  the images rendered from each method
 Table N lists the top seven methods based on average user  score, together with the RMSE w.r.t ground truth BRDF parameters.N (Additional evaluation results are provided in  the Supplementary Material.) We found the RMSE ranking is not always consistent with the ranking of the user  study
Adding the regularization term Ec can improve the ranking (e.g., Grouplet-RMSEN-Ec), while the choice of  NRMSE is computed after normalization of the BRDF parameters to  zero mean and unit standard deviation, based on the mean and standard  deviation of the training set
Thus a random prediction (with the same  statistics) will have RMSE ≈ N.0
 NN    Material NNN: top: GT, middle: ours (RMSEN+Ec), bottom: Lombardi et al
[NN]  Material NNNN: top: GT, middle: ours (RMSEN), bottom: Lombardi et al
[NN]  Figure N
Comparison with Lombardi et al
[NN]: ours (middle row  for each example) is closer to the ground truth example (top row)  even under varying lighting
 Ed has mixed effect on performance
We find that the  Θ = (L, a, b, c, d) BRDF representation provides more ac- curate estimation of gloss (e.g., HemiCNN-RMSEN)
Also,  HemiCNN seems able to obtain better estimate for the  gloss, while Grouplet estimates the diffuse albedo better
 Fig
N shows three random examples of BRDF estimation
We show the rendered images under natural illumination with the ground truth BRDF, as well as the estimated  BRDF from two variants of our proposed method
Qualitatively, the BRDF estimations accurately reproduce the color  and gloss of the surface materials
 N.N
Comparison with [NN]  As mentioned previously, it is difficult to compare with  prior work on BRDF estimation in the wild [NN, NN], given  the lack of code and common datasets for comparison
 Lombardi et al
[NN] is the only method with released codes
 Strictly speaking, it is not a direct apples-to-apples comparison, because Lombardi et al
[NN] requires a single image  and a precise surface normal map as input and estimates  both DSBRDF and lighting, while our methods take multiple RGB-D images as input and estimate the Ward BRDF
 Moreover, Lombardi et al
[NN] takes about N minutes to run,  while our methods are real-time (≤ N0ms)
Nevertheless, Lombardi et al
[NN] is the only available option for comparison, and both its input requirements and running time are  similar to ours
For comparison, we randomly chose two  materials from SynBRDF, rendered a sphere image under  Head model  Pumpkin model  BRDF estimation without regularization Ec  Figure N
Real data evaluation: For both examples in the top two  sections, head and pumpkin, top-left is the input (real) scene, topmiddle shows the rendered scene with estimated BRDF parameters, top-right shows a different rendered view of the same scene,  bottom-left shows a rendered sphere with the estimated BRDF, and  bottom-right shows rendered spheres with varying point lighting
 Grouplet and HemiCNN with CubeRoot+Ec were used for the head and pumpkin examples, respectively
The bottom section  shows three rendered views from methods trained without regularization Ec (Eq
N)
 natural illumination, and used it (together with the sphere  normal map) as the input for [NN]
Fig
N shows the comparison
Our proposed method closely matches the ground  truth and outperforms [NN]
 N.N
Results on Real Data  Previously, in Sec
N and Sec
N.N, we discussed the potential issues of scale ambiguity present in real-world data,  due in part to our use of commodity RGBD camera output  rather than HDR videos
As expected, the regularization  (Eq
N) plays an important role in achieving correct results,  as illustrated in Fig
N
Notice that the result with the regularization better captures brightness as well as plausible  gloss
Additional views of the real examples are shown in  the supplementary video
 NN    N
Conclusions and Limitations  In this paper, we proposed a lightweight and practical approach for surface reflectance estimation directly from N-bit  RGB images in real-time
The method can be plugged into  ND scanning-and-fusion systems with a commodity RGBD  sensor for scene capture
Our approach is learning-based,  with the inference time less than N0ms per material and  model size less than NN0K bytes
Compared to prior work,  our method is a more feasible solution for real-time applications (VR/AR) on mobile devices
We proposed two novel  network architectures, HemiCNN and Grouplet, to handle  the unstructured measured data from input images
We also  designed a novel loss function that is both perceptuallybased and able to resolve the scale ambiguity and colorconstancy ambiguity for reflectance estimation
In addition, we also provided the first large-scale synthetic data set  (SynBRDF) as a benchmark dataset for training and evaluation for surface reflectance estimation in uncontrolled environments
 Our method has several limitations that we plan to address in future work
First, our method estimates homogeneous reflectance
While GroupLet and HemiCNN can  in theory operate for each voxel separately and thus could  estimate spatially-varying reflectance, in practice we found  using more voxels as input results in more robust estimation
One future direction is to jointly learn several basis  reflectance functions and weight maps to estimate spatiallyvarying BRDF
Second, we use the isotropic Ward model  for BRDF representation
In the future, we plan to investigate more general, data-driven models such as DSBRDF [NN] and the related perceptually-based loss [NN]
Finally, we are interested in using neural networks to jointly  refine both ND geometry and reflectance estimation, and  leveraging domain adaption techniques to further improve  the performance on real data
 Appendix  Derivation of Eq.(N) For viewing direction ωo, the ob- served scene radiance Lo is given by  Lo =  ∫  ωi  f(ωi, ωo; Θ) · Li ·max(cos θi, 0)dωi, (N)  where Li is the environmental illumination in the direc- tion ωi
We simplified the above rendering equation so that all terms can be computed from the input fed into the networks
Suppose the environment illumination is uniform,  i.e., Li = L̄, by integrating the reflected radiance from the entire hemisphere, the measured radiance is:  L̄o ≈ (ρd + ρs)L̄
(N)  Both L̄o and L̄ can be approximated from input images, where the average intensity of the foreground object is close  Material NNN  G ro  u n d  tr u th  B R  D F  G ro  u p le  t  R M  S E N + E  c  Material NNNN  G ro  u n d  tr u th  B R  D F  H em  iC N  N  R M  S E N  Material NN0N G  ro u n d  tr u th  B R  D F  H em  iC N  N  R M  S E N  Figure N
Qualitative results for three randomly selected materials
 For each material, images in first row are rendered from ground  truth BRDF, while images in the second row are rendered from the  estimated BRDF using one of the methods from the list in Table N
 The ground truth image indicated with a red border is sampled  from the image sequence used for inference (inputs)
To demonstrate how different objects and environmental lights can change  the appearance of the scene even with the same BRDF, the three  images from second to fourth columns are rendered with the same  BRDF but different models and lighting
More examples from different are included in the Supplementary Material
 to L̄o, and the average intensity of the background is close to L̄
Since the input images are N-bit images in the sRGB color space rather than linear HDR images, we need to apply an additional gamma transformation between pixel intensities and scene radiance (γ = N.N for sRGB)
Thus, we have L̄o ≈ F̄ γ and L̄ ≈ B̄γ , where F̄ and B̄ are the av- erage image intensities for the foreground and background
 Putting all together, we have  F̄ γi ≈ (ρd + ρs) · B̄ γ i , (N)  and thus we have the Ec term in Eq.(N)
 NN    References  [N] J
T
Barron and J
Malik
Shape, illumination, and reflectance from shading
IEEE Trans
Pattern Anal
Mach
 Intell., N0NN
N, N  [N] S
Bell, K
Bala, and N
Snavely
Intrinsic images in the wild
 ACM Trans
Graph., NN(N):NNN:N–NNN:NN, July N0NN
N  [N] S
Bell, P
Upchurch, N
Snavely, and K
Bala
Opensurfaces:  A richly annotated catalog of surface appearance
ACM  Trans
Graph
(SIGGRAPH), N0NN
N, N, N  [N] S
Bell, P
Upchurch, N
Snavely, and K
Bala
Material  recognition in the wild with the materials in context database
 Computer Vision and Pattern Recognition (CVPR), N0NN
N  [N] M
Chandraker and R
Ramamoorthi
What an image reveals  about material reflectance
In ICCV, pages N–N, N0NN
N  [N] A
Dai, A
X
Chang, M
Savva, M
Halber, T
Funkhouser,  and M
Nießner
Scannet: Richly-annotated Nd reconstructions of indoor scenes
http://arxiv.org/, N0NN
N, N  [N] A
Dai, M
Nießner, M
Zollhöfer, S
Izadi, and C
Theobalt
 Bundlefusion: Real-time globally consistent ND reconstruction using on-the-fly surface re-integration
arXiv, N0NN
N,  N  [N] Y
Dong, G
Chen, P
Peers, J
Zhang, and X
Tong
 Appearance-from-motion: Recovering spatially varying surface reflectance under unknown lighting
ACM Trans
 Graph., N0NN
N  [N] R
O
Dror, E
Adelson, and A
Willsky
Estimating surface  reflectance properties from images under unknown illumination
In SPIE, Human Vision and Electronic Imaging, N00N
 N  [N0] R
W
Fleming, R
O.Dror, and E
Adelson
Real-world illumination and the perception of surface reflectance properties
 Journal of Vision, N00N
N  [NN] A
Fores, J
Ferwerda, and J
Gu
Toward a perceptually  based metric for brdf modeling
In Twentieth Color and  Imaging Conference
Los Angeles, California, USA, pages  NNN–NNN, November N0NN
N, N, N  [NN] W
Jakob
Mitsuba Renderer, N0N0
http://www.mitsubarenderer.org
N  [NN] J
T
Kajiya
The rendering equation
In SIGGRAPH, NNNN
 N  [NN] H
P
A
Lensch, J
Kautz, M
Goesele, W
Heidrich, and H.P
Seidel
Image-Based Reconstruction of Spatially Varying  Materials
In S
J
Gortle and K
Myszkowski, editors, Eurographics Workshop on Rendering, N00N
N  [NN] L
Lettry, K
Vanhoey, and L
V
Gool
Darn: A deep adversial residual network for iintrinsic image decomposition:
 Arxiv
N  [NN] C
Liu, L
Sharan, R
Rosenholtz, and E
H
Adelson
Exploring features in a bayesian framework for material recognition
In CVPR, N0N0
N  [NN] S
Lombardi and K
Nishino
Reflectance and natural illumination from a single image
In ECCV, pages NNN–NNN, N0NN
 N, N, N, N, N  [NN] S
Lombardi and K
Nishino
Radiometric scene decomposition: Scene reflectance, illumination, and geometry from  rgb-d images
In NDV, N0NN
N, N, N  [NN] W
Matusik, H
Pfister, M
Brand, and L
McMillan
A datadriven reflectance model
SIGGRAPH, N00N
N, N  [N0] D
McAllister
A Generalized Surface Appearance Representation for Computer Graphics
PhD thesis, University of  North Carolina at Chapel Hill, N00N
N  [NN] T
Narihira, M
Maire, and S
X
Yu
Direct iintrinsic: Learning albedo-shading decomposition by convolutional regression
In ICCV, N0NN
N  [NN] R
Newcombe, A
Davison, S
Izadi, P
Kohli, O
Hilliges,  J
Shotton, D
Molyneaux, S
Hodges, D
Kim, and  A
Fitzgibbon
KinectFusion: Real-time dense surface mapping and tracking
In ISMAR, N0NN
N  [NN] A
Ngan, F
Durand, and W
Matusik
Experimental analysis  of brdf models
In Proceedings of the Eurographics Symposium on Rendering, pages NNN–NNN
Eurographics Association, N00N
N, N, N  [NN] M
Nießner, M
Zollhöfer, S
Izadi, and M
Stamminger
 Real-time ND reconstruction at scale using voxel hashing
 ACM Transactions on Graphics (TOG), NN(N):NNN, N0NN
N  [NN] K
Nishino and S
Lombardi
Directional statistics-based reflectance model for isotropic bidirectional reflectance distribution functions
OSA Journal of Optical Society of America  A, NN(N):N–NN, N0NN
N  [NN] K
Nishino and S
Lombardi
Single image multimaterial  estimation
CVPR, pages NNN–NNN, N0NN
N  [NN] G
Oxholm and K
Nishino
Shape and reflectance estimation  in the wild
TPAMI, NN(N):NNN–NNN, N0NN
N, N, N  [NN] F
Pellacini, J
A
Ferwerda, and D
P
Greenberg
Toward  a psychophysically-based light reflection model for image  synthesis
In Proceedings of the NNth Annual Conference  on Computer Graphics and Interactive Techniques, SIGGRAPH ’00, pages NN–NN, N000
N, N, N  [NN] K
Rematas, T
Ritschel, M
Fritz, E
Gavves, and T
Tuytelaars
Deep reflectance maps
In CVPR, N0NN
N  [N0] F
Romeiro and T
Zickler
Blind reflectometry
In ECCV,  N0N0
N, N  [NN] T
Wang, J
Zhu, E
Hiroaki, M
Chandraker, A
Efros, and  R
Ramamoorthi
A Nd light-field dataset and cnn architectures for material recognition
In ECCV, N0NN
N  [NN] G
J
Ward
Measuring and modeling anisotropic reflection
In Proceedings of the NNth Annual Conference on Computer Graphics and Interactive Techniques, SIGGRAPH ’NN,  pages NNN–NNN, NNNN
N, N  [NN] T
Whelan, S
Leutenegger, R
S
Moreno, B
Glocker, and  A
Davison
Elasticfusion: Dense slam without a pose graph
 In Proceedings of Robotics: Science and Systems, Rome,  Italy, July N0NN
N  [NN] J
Wills, S
Agarwal, D
Kriegman, and S
Belongie
Toward  a perceptual space for gloss
ACM Trans
Graph., N00N
N,  N, N  [NN] H
Wu, Z
Wang, and K
Zhou
Simultaneous localization  and appearance estimation with a consumer rgb-d camera
 IEEE Trans
Visualization and Computer Graphics, N0NN
N,  N, N, N, N  [NN] T
Zhou, P
Krahenbuhl, and A
A
Efros
Learning datadriven reflectance priors for intrinsic image decomposition
 In ICCV, N0NN
N  NNDeepSetNet: Predicting Sets With Deep Neural Networks   DeepSetNet: Predicting Sets with Deep Neural Networks  S
Hamid Rezatofighi Vijay Kumar B G Anton Milan  Ehsan Abbasnejad Anthony Dick Ian Reid  School of Computer Science, The University of Adelaide, Australia  Abstract  This paper addresses the task of set prediction using  deep learning
This is important because the output of many  computer vision tasks, including image tagging and object  detection, are naturally expressed as sets of entities rather  than vectors
As opposed to a vector, the size of a set is  not fixed in advance, and it is invariant to the ordering of  entities within it
We define a likelihood for a set distribution and learn its parameters using a deep neural network
 We also derive a loss for predicting a discrete distribution  corresponding to set cardinality
Set prediction is demonstrated on the problem of multi-class image classification
 Moreover, we show that the proposed cardinality loss can  also trivially be applied to the tasks of object counting and  pedestrian detection
Our approach outperforms existing  methods in all three cases on standard datasets
 N
Introduction  Deep neural networks have shown state-of-the-art performance on many computer vision problems, including semantic segmentation [NN], visual tracking [NN], image captioning [NN], scene classification [NN], and object detection [NN]
However, traditional convolutional architectures  require a problem to be formulated in a certain way: in particular, they are designed to predict a vector (or a matrix,  or a tensor in a more general sense), that is either of a fixed  length or whose size depends on the input (cf 
fully convolutional architectures)
 For example, consider the task of scene classification  where the goal is to predict the label (or category) of a given  image
Modern approaches typically address this by a series  of convolutional layers, followed by a number of fully connected layers, which are finally mapped to predict a fixedsized vector [NN, NN, NN]
The length of the predicted vector corresponds to the number of candidate categories, e.g
 N,000 for the ImageNet challenge [NN]
Each element is a  score or probability of a particular category, and the final  prediction is a probability distribution over all categories
 This strategy is perfectly admissible if one expects to find  CNN  I  Vector  CNN CNN  Set  s h e e p  b ik e  p e rs o n  b o o k  p iz z a  b ir d  s c is s o rs  c a r  o v e n  b o w l  a p p le  p e rs o n  p iz z a  c a r  Figure N: Left: Traditional CNNs learn parameters θ∗ to  predict a fixed vector Y 
Right: In contrast, we propose to  train a separate CNN to learn a parameter vector w∗, which  is used to predict the set cardinality of a particular output
 exactly one or at least the same number of categories across  all images
However, natural images typically show multiple entities (e.g
table, pizza, person, etc.), and what is  perhaps more important, this number differs from image to  image
During evaluation, this property is not taken into account
The ImageNet Large Scale Visual Recognition Challenge (ILSVRC) only counts an error if the “true” label is  not among the top-N candidates
Another strategy to account for multiple classes is to fix the number to a certain  value for all test instances, and report precision and recall  by counting false positive and false negative predictions, as  was done in [NN, NN]
Arguably, both methods are suboptimal because in a real-world scenario, where the correct  labelling is unknown, the prediction should in fact not only  rank all labels according to their likelihood of being present,  but also to report how many objects (or labels) are actually  present in one particular image
Deciding how many objects  are present in an image is a crucial part of human perception and scene understanding but is missing from our current evaluation of automated image understanding methods
 As a second example, let us consider pedestrian detecNNNNN    tion
The parallel to scene classification that we motivated  above is that, once again, in real scenarios, the number of  people in a particular scene is not known beforehand
The  most common approach is to assign a confidence score to  a number of region candidates [N, NN, NN, NN], which are  typically selected heuristically by thresholding and nonmaxima suppression
We argue that it is important not to  simply discard the information about the actual number of  objects at test time, but to exploit it while selecting the subset of region proposals
 The examples above motivate and underline the importance of set prediction in certain applications
It is important  to note that, in contrast to vectors, a set is a collection of elements which is invariant under permutation and the size of  a set is not fixed in advance
To this end, we use a principled  definition of a set as the union of cardinality distribution and  family of joint distributions over each cardinality value
In  summary, our main contributions are as follows: (i) Starting from the mathematical definition of a set distribution,  we derive a loss that enables us to employ existing machine  learning methodology to learn this distribution from data
 (ii) We integrate our loss into a deep learning framework  to exploit the power of a multi-layer architecture
(iii) We  present state-of-the-art results for multi-label image classification and pedestrian detection on standard datasets and  competitive results on the task of object counting
 N
Related Work  A sudden success in multiple applications including  voice recognition [NN], machine translation [N0] and image classification [NN], has sparked the deployment of  deep learning methods throughout numerous research areas
 Deep convolutional (CNN) and recurrent (RNN) neural networks now outperform traditional approaches in tasks like  semantic segmentation [N, NN], image captioning [NN] or object detection [NN]
Here, we will briefly review some of the  recent approaches to image classification and object detections and point out their limitations
 Image or scene classification is a fundamental task of  understanding photographs
The goal here is to predict  a scene label for a given image
Early datasets, such as  Caltech-N0N [N0], mostly contained one single object and  could easily be described by one category
Consequently,  a large body of literature focused on single-class prediction [NN, NN, NN, N0]
However, real-world photographs typically contain a collection of multiple objects and should  therefore be captioned with multiple tags
Surprisingly,  there exists rather little work on multi-class image classification that makes use of deep architectures
Gong et  al
[NN] combine deep CNNs with a top-k approximate  ranking loss to predict multiple labels
Wei et al
[NN] propose a Hypotheses-Pooling architecture that is specifically  designed to handle multi-label output
While both methods  open a promising direction, their underlying architectures  largely ignore the correlation between multiple labels
To  address this limitation, recently, Wang et al
[NN] combined  CNNs and RNNs to predict a number of classes in a sequential manner
RNNs, however, are not suitable for set prediction mainly for two reasons
First, the output represents a  sequence rather than a set and is thus highly dependent on  the prediction order, as was shown recently by Vinyals et  al
[NN]
Second, the final prediction may not result in a feasible solution (e.g
it may contain the same element multiple  times), such that post-processing or heuristics such as beam  search must be employed [NN, NN]
Here we show that our  approach not only guarantees to always predict a valid set,  but also outperforms previous methods
 Pedestrian detection can also be viewed as a classification problem
Traditional approaches follow the slidingwindow paradigm [NN, N, NN, NN, N], where each possible  (or rather plausible) image region is scored independently  to contain a person or not
More recent methods like Fast RCNN [NN] or the single-shot multi-box detector (SSD) [NN]  learn the relevant image features rather than manually engineering them, but retain the sliding window approach
 All the above approaches require some form of postprocessing to suppress spurious detection responses that  originate from the same person
This is typically addressed by non-maximum suppression (NMS), a greedy  optimisation strategy with a fixed overlap threshold
Recently, several alternatives have been proposed to replace  the greedy NMS procedure, including sequential head detection with LSTMs [NN], a global optimisation approach  to NMS [NN, NN], or even learning NMS end-to-end using  CNNs [NN]
None of the above methods, however, explicitly consider the number of objects while selecting the final  set of boxes
Contrary to existing pedestrian detection approaches, we incorporate the cardinality into the NMS algorithm itself and show an improvement over the state of the  art on two benchmarks
Note that the idea of considering the  number of pedestrians can be applied in combination with  any of the aforementioned detection techniques to further  improve their performances
 It is important to bear in mind that unlike many existing  approaches that learn to count [N, N, NN, N0, NN, NN, N0, NN],  our main goal is not object counting
Rather, we derive  a formulation for set prediction using deep learning
Estimating the cardinality of objects and thereby their count  is a byproduct of our approach
To demonstrate the effectiveness of our formulation, we also conduct experiments  on the task of object counting, outperforming many recent  methods on the widely used USCD dataset
 N
Random Vectors vs
Random Finite Sets  In statistics, a continuous random variable y is a variable that can take an infinite number of possible values
A  NNNN    continuous random vector can be defined by stacking several continuous random variables into a fixed length vector,  Y = (yN, · · · , ym)
The mathematical function describing the possible values of a continuous random vector and their  associated joint probabilities is known as a probability density function (PDF) p(Y ) such that ∫  p(Y )dY = N
 In contrast, a random finite set (RFS) Y is a finite-set valued random variable Y = {yN, · · · , ym}
The main dif- ference between an RFS and a random vector is that for the  former, the number of constituent variables, m, is random  and the variables themselves are random and unordered
 Throughout the paper, we use Y = {yN, · · · , ym} for a set with unknown cardinality, Ym = {yN, · · · , ym}|| for a set  with known cardinality and Y = (yN, · · · , ym) for a vector with known dimension
 A statistical function describing a finite-set variable Y is a combinatorial probability density function p(Y) which consists of a discrete probability distribution, the so-called  cardinality distribution, and a family of joint probability  densities on both the number and the values of the constituent variables
Similar to the definition of a PDF for  a random variable, the PDF of an RFS must sum to unity  over all possible cardinality values and all possible element  values and their permutations [NN]
The PDF of an mdimensional random vector can be defined in terms of an  RFS as  p(yN, · · · , ym) , N  m! p({yN, · · · , ym}||)
(N)  The normalisation factor m! = ∏m  k=N k appears because the  probability density for a set {yN, · · · , ym}|| must be equally distributed among all the m! possible permutations of the vector [NN]
 Conventional machine learning approaches, such as  Bayesian learning and convolutional neural networks, have  been proposed to learn the optimal parameters θ∗ of the distribution p(Y |x,θ∗) which maps the input vector x to the output vector Y 
In this paper, we instead propose an approach that can learn a pair (θ∗,w∗) of parameter vectors for a set distribution that allow one to map the input vector  x into the output set Y , i.e
p(Y|x,θ∗,w∗)
The additional parameters w∗ define a PDF over the set cardinality, as we  will explain in the next section
 N
Deep Set Network  Let us begin by defining a training set D = {Yi,xi}, where each training sample i = N, 


, n is a pair consist- ing of an input feature xi ∈ R  l and an output (or label) set  Yi = {yN, yN, 


, ymi}, yk ∈ R d, mi ∈ N  0
In the following we will drop the instance index i for better readability
 Note that m := |Y| denotes the cardinality of set Y 
The  probability of a set Y is defined as  p(Y|x,θ,w) =p(m|x,w)×  m!× Um × p(yN, yN, · · · , ym|x,θ), (N)  where p(m|·, ·) and p(yN, yN, · · · , ym|·, ·) are respectively a cardinality distribution and a symmetric joint probability  density distribution of the elements
U is the unit of hypervolume in Rd, which makes the joint distribution unitless [NN, NN]
θ denotes the parameters that estimate the  joint distribution of set element values for a fixed cardinality,N while w represents the collection of parameters which  estimate the cardinality distribution of the set elements
 The above formulation represents the probability density  of a set which is very general and completely independent  from the choices of both cardinality and spatial distributions
It is thus straightforward to transfer it to many applications that require the output to be a set
However, to  make the problem amenable to mathematical derivation and  implementation, we adopt two assumptions: i) the outputs  (or labels) in the set are independent and identically distributed (i.i.d.) and ii) their cardinality follows a Poisson  distribution with parameter λ
Thus, we can write the distribution as  p(Y|x,θ,w) =  ∫  p(m|λ)p(λ|x,w)dλ×  m!× Um×  (  m ∏  k=N  p(yk|x,θ)  )  
 (N)  N.N
Posterior distribution  To learn the parameters θ and w, we first define the posterior distribution over them as  p(θ,w|D) ∝ p(D|θ,w)p(θ)p(w)  ∝ n ∏  i=N  [ ∫  p(mi|λ)p(λ|xi,w)dλ×mi!  Umi  (  mi ∏  k=N  p(yk|xi,θ)  )]  p(xi)p(θ)p(w)
 (N)  A closed form solution for the integral in Eq
(N) can be  obtained by using conjugate priors:  m ∼ P(m;λ)  λ ∼ G(λ;α(x,w), β(x,w))  α(x,w), β(x,w) > 0 ∀x,w  θ ∼ N (θ; 0, σNNI)  w ∼ N (w; 0, σNNI),  NThis is also known as spatial distribution of points in point process  statistics
 NNNN    where P(·, λ), G(·;α, β), and N (·; 0, σNI) represent re- spectively a Poisson distribution with parameters λ, a  Gamma distribution with parameters (α, β) and a zero mean normal distribution with covariance equal to σNI
 We assume that the cardinality follows a Poisson distribution whose mean, λ, follows a Gamma distribution, with  parameters which can be estimated from the input data x
 Note that the cardinality distribution in Eq
(N) can be replaced by any other discrete distribution
For example, it is  a valid assumption to model the number of objects in natural images by a Poisson distribution [N]
Thus, we could  directly predict λ to model this distribution by formulating  the cardinality as p(m|x,w) = P(m;λ(x,w))
However, this would limit the model’s expressive power because two  visually entirely different images with the same number of  objects would be mapped to the same λ
Instead, to allow  for uncertainty of the mean, we model it with another distribution, which we choose to be Gamma for mathematical  convenience
Consequently, the integrals in p(θ,w|D) are simplified and form a negative binomial distribution,  NB (m; a, b) = Γ(m+ a)  Γ(m+ N)Γ(a) .(N− b)abm, (N)  where Γ is the Gamma function
Finally, the full posterior distribution can be written as  p(θ,w|D) ∝ n ∏  i=N  [  NB  (  mi;α(xi,w), N  N + β(xi,w)  )  ×mi!× U mi ×  (  mi ∏  k=N  p(yk|xi,θ)  )  ]  p(θ)p(w)
 (N)  N.N
Learning  For simplicity, we use a point estimate for the posterior p(θ,w|D), i.e
p(θ,w|D) = δ(θ = θ∗,w = w∗|D), where (θ∗,w∗) are computed using the following MAP es- timator:  (θ∗,w∗) = argmax θ,w  log (p (θ,w|D)) 
(N)  The optimisation problem in Eq
(N) can be decomposed  w.r.t
the parameters θ and w
Therefore, we can learn them  independently as  θ ∗ = argmax  θ  −γN‖θ‖+ n ∑  i=N  mi ∑  k=N  log (p(yk|xi,θ)) (N)  and  w ∗ =argmax  w  n ∑  i=N  [  log  (  Γ(mi + α(xi,w))  Γ(mi + N)Γ(α(xi,w))  )  +log  (  β(xi,w) α(xi,w)  (N + β(xi,w)α(xi,w)+mi)  )  ]  − γN‖w‖,  (N)  where γN and γN are the regularisation parameters, proportional to the predefined covariance parameters σN and σN
 These parameters are also known as weight decay parameters and commonly used in training neural networks
 The learned parameters θ∗ in Eq
(N) are used to map an  input feature vector x into an output vector Y 
For example,  in image classification, θ∗ is used to predict the distribution  Y over all categories, given the input image x
Note that  θ ∗ can generally be learned using a number of existing machine learning techniques
In this paper we rely on deep  CNNs to perform this task
 To learn the highly complex function between the input feature x and the parameters (α, β), which are used for estimating the output cardinality distribution, we train  a second deep neural network
Using neural networks  to predict a discrete value may seem counterintuitive, because these methods at their core rely on the backpropagation algorithm, which assumes a differentiable loss
 Note that we achieve this by describing the discrete distribution by continuous parameters α, β (Negative binomial  NB(·, α, NN+β )), and can then easily draw discrete samples from that distribution
More formally, to estimate w∗, we  compute the partial derivatives of the objective function in  Eq
(N) w.r.t
α(·, ·) and β(·, ·) and use standard backpropa- gation to learn the parameters of the deep neural network
 We refer the reader to the supplementary material for  the complete derivation of the partial derivatives, a more  detailed derivation of the posterior in Eqs
(N)-(N) and the  proof for decomposition of the MAP estimation in Eq
(N)
 N.N
Inference  Having the learned parameters of the network (w∗,θ∗), for a test feature x+, we use a MAP estimate to generate a  set output as Y∗ = argmaxY p(Y|D,x +), where  p(Y|D,x+) =  ∫  p(Y|x+,θ,w)p(θ,w|D)dθdw  and p(θ,w|D) = δ(θ = θ∗,w = w∗|D)
Since the unit of hypervolume U in most practical application in unknown,  to calculate the mode of the set distribution p(Y|D,x+), we use sequential inference as explained in [NN]
To this end,  we first calculate the mode m∗ of the cardinality distribution  m∗ = argmaxm p(m|x +,w∗), where p(m|x+,w∗) =  NB (  m;α(x+,w∗), NN+β(x+,w∗)  )  
Then, we calculate the  mode of the joint distribution for the given cardinality m∗  as  Y∗ = argmax Ym∗  p({yN, · · · , ym∗}|||x +,θ∗)
(N0)  To estimate the most likely set Y∗ with cardinality m∗, we use the first CNN with the parameters θ∗ which predicts  p(yN, · · · , yM |x +,θ∗), where M is the maximum cardinality of the set, i.e
{yN, · · · , ym∗} ⊆ {yN, · · · , yM}, ∀m ∗
 NNN0    Since the samples are i.i.d., the joint probability maximised  when the probability of each element in the set is maximised
Therefore, the most likely set Y∗ with cardinality m∗ is obtained by ordering the probabilities of the set elements yN, · · · , yM as the output of the first CNN and choos- ing m∗ elements with highest probability values
 Note that the assumptions in Sec
N are necessary to  make both learning and inference computationally tractable  and amenable to an elegant mathematical formulation
A  major advantage of this approach is that we can use any  state-of-the-art classifier/detector as the first CNN (θ∗) to  further improve its performance
Modifying any of the  assumptions, e.g
non-i.i.d
set elements, leads to serious  mathematical complexities [NN], and is left for future work
 N
Experimental Results  Our proposed method is best suited for applications that  expect the solution to be in the form of a set, i.e
permutation invariant and of an unknown cardinality
To that end,  we perform an experiment on multi-label image classification in Sec
N.N
In addition, we explore our cardinality estimation loss on the object counting problem in Sec
N.N and  then show in Sec
N.N how incorporating cardinality into a  state-of-the art pedestrian detector and formulating it as a  set problem can boost up its performance
 N.N
Multi-Label Image Classification  As opposed to the more common and more studied  problem of (single-label) image classification, the task  here is rather to label a photograph with an arbitrary, apriori unknown number of tags
We perform experiments  on two standard benchmarks, the PASCAL VOC N00N  dataset [N] and the Microsoft Common Objects in Context  (MS COCO) dataset [NN]
 Implementation details
In this experiment, similar  to [NN], we build on the NN-layers VGG network [NN], pre- trained on the N0NN ImageNet dataset
We adapt VGG for  our purpose by modifying the last fully connected prediction layer to predict N0 classes for PASCAL VOC, and N0 classes for MS COCO
We then fine-tune the entire network for each of these datasets using two commonly  used losses for multi-label classification, softmax and binary cross-entropy (BCE)N [NN, NN]
To learn both classifiers, we set the weight decay to N ·N0−N, with a momentum of 0.N and a dropout rate of 0.N
The learning rate is ad- justed to gradually decrease after each epoch, starting from  0.0N for softmax and from 0.00N for binary cross-entropy
The learned parameters of these classifiers correspond to θ∗  for our proposed deep set network (cf 
Eq
(N) and Fig
N)
 NWeighted Approximate Ranking (WARP) objective is another commonly used loss for multi-label classification
However, it does not perform as well as softmax and binary cross-entropy for the used datasets [NN]
 To learn the cardinality distribution, we use the same  VGG-NN network as above and modify the final fully connected layer to predict N values followed by two weighted sigmoid activation functions for α and β
It is important  to note, that the outputs must be positive to describe a  valid Gamma distribution
We therefore also append two  weighted sigmoid transfer functions with weights αM , βM to ensure that the values predicted for α and β are in a  valid range
Our model is not sensitive to these parameters and we set their values to be large enough (αM = NN0 and βM = N0) to guarantee that the mode of the distribu- tion can accommodate the largest cardinality existing in the  dataset
We then fine-tune the network on cardinality distribution using the objective loss defined in Eq
(N)
To train  the cardinality CNN, we set a constant learning rate 0.00N, weight decay N·N0−NN, momentum rate 0.N and dropout 0.N
 Evaluation protocol
To evaluate the performance of the  classifiers and our deep set network, we employ the commonly used evaluation metrics for multi-label image classification [NN, NN]: precision and recall of the generated labels per-class (C-P and C-R) and overall (O-P and O-R)
 Precision is defined as the ratio of correctly predicted labels  and total predicted labels, while recall is the ratio of correctly predicted labels and ground-truth labels
In case no  predictions (or ground truth) labels exist, i.e
the denominator becomes zero, precision (or recall) is defined as N00%
To generate the predicted labels for a particular image, we  perform a forward pass of the CNN and choose top-k labels according to their scores similar to [NN, NN]
Since  the classifier always predicts a fixed-sized prediction for  all categories, we sweep k from 0 to the maximum num- ber of classes to generate a precision/recall curve, which is  common practice in multi-label image classification
However, for our proposed DeepSet Network, the number of labels per instance is predicted from the cardinality network
 Therefore, prediction/recall is not dependent on value k and  one single precision/recall value can be computed
 To calculate the per-class and overall precision/recall,  their average values over all classes and all examples are  computed, respectively
In addition, we also report the FN  score (the harmonic mean of precision and recall) averaged  over all classes (C-FN) and all instances and classes (O-FN)
 PASCAL VOC N00N
The Pascal Visual Object Classes  (VOC) [N] benchmark is one of the most widely used  datasets for detection and classification
It consists of NNNN images with a N0/N0 split for training and test, where objects from N0 pre-defined categories have been annotated by bounding boxes
Each image may contain between N and N unique objects
We compare our results with a state- of-the-art classifier as described above
The resulting precision/recall plots are shown in Fig
N(a) together with our  proposed approach using the estimated cardinality
Note  NNNN    GT: motorcycle chair, dining-table, book, tv, couch,  potted-plant, vase  chair, dining-table, book, tv, couch,  potted-plant, vase Prediction:  person, chair, car, dining-table,  cup, knife, fork, pizza, wine-glass  person, chair, car, dining-table,  cup, knife, fork, pizza, wine-glass  motorcycle  ----Figure N: Qualitative results of our multi-class image labelling approach
For each image, the ground truth tags and our  predictions are denoted below
Note that we show the exact output of our network, without any heuristics or post-processing
 GT:  Prediction:  chair, cup, book,   keyboard, mouse  chair, cup, book,   keyboard, mouse, tv  banana  banana, bottle  person, toothbrush  person, toothbrush,  cell-phone  teddy-bear  teddy-bear, bird, car, person  oven  oven, toaster, fridge, bowl,  sink, microvawe  Figure N: Interesting failure cases of our method
The “spurious” TV class predicted on the left is an artifact in annotation  because in many examples, computer monitors are actually labelled as TV
In other cases, our network can correctly reason  about the number of objects or concepts in the scene, but is constrained by a fixed list of categories defined in the dataset
 that by enforcing the correct cardinality for each image, we  are able to clearly outperform the baseline w.r.t
both measures
Note also that our prediction (+) can nearly replicate  the oracle (∗), where the ground truth cardinality is known
The mean absolute cardinality error of our prediction on  PASCAL VOC is 0.NN± 0.NN
 Microsoft COCO
Another popular benchmark for image  captioning, recognition, and segmentation is the recent Microsoft Common Objects in Context (MS-COCO) [NN]
The  dataset consists of NNN thousand images, each labelled with per instance segmentation masks of N0 classes
The num- ber of unique objects for each image can vary between 0 and NN
Around N00 images in the training dataset do not contain any of the N0 classes and there are only a handful of images that have more than N0 tags
The majority of the images contain between one and three labels
We use NNNNN images as training and validation split (N0% - N0%), and the remaining N0N0N images as test data
We predict the cardi- nality of objects in the scene with a mean absolute error of  0.NN and a standard deviation of 0.NN
 Fig
N(b) shows a significant improvement of precision  and recall and consequently the FN score using our deep set  0 0.N N  Recall  0  0.N  0.N  0.N  0.N  N  P re  c is  io n  SoftMax SoftMax DS (ours)  SoftMax DS (GT card.) BCE BCE DS (ours)  BCE DS (GT card.)  k=0 k=N  k=N  k=N0  (a)  PASCAL VOC N00N  0 0.N N  Recall  0  0.N  0.N  0.N  0.N  N  SoftMax SoftMax DS (ours)  SoftMax DS (GT card.) BCE BCE DS (ours)  BCE DS (GT card.)  k=0 k=N  k=N  k=N0  (b)  MS COCO  Figure N: Experimental results on multi-label image classification
The baselines (solid curves) represent state-of-theart classifiers, fine-tuned for each dataset, using two different loss functions
The methods are evaluated by choosing  the top-k predictions across the entire dataset, for different  k
Our approach predicts k and is thus evaluated only on  one single point (+)
It outperforms both classifiers significantly in terms of precision and recall and comes very close  to the performance when the true cardinality is known (∗)
 network compared to the softmax and binary cross-entropy  classifiers for all ranking values k
We also outperform the  state-of-the art multi-label classifier CNN-RNN [NN], for  the reported value of k = N
Our results, listed in Tab
N,  NNNN    Table N: Quantitative results for multi-label image classification on the MS COCO dataset
 Classifier Eval
C-P C-R C-FN O-P O-R O-FN  Softmax k=N NN.N NN.N NN.N N0.N NN.N NN.0  BCE k=N NN.N N0.N NN.N NN.N NN.N NN.N  CNN-RNN [NN] k=N NN.0 NN.N N0.N NN.N NN.N NN.N  Ours (Softmax) k=m∗ NN.N NN.N NN.N NN.N NN.N NN.N  Ours (BCE) k=m∗ NN.N NN.N NN.N N0.N NN.N NN.N  show around N percentage points improvement for the FN score on top of the baseline classifiers and about N percent- age points improvement compared to the state of the art on  this dataset
Examples of perfect label prediction using our  proposed approach are shown in Fig
N
The deep set network can properly recognise images with no labels at all, as  well as images with many tags
We also investigated failure cases where either the cardinality CNN or the classifier  fails to make a correct prediction
We showcase some of  these cases in Fig N
We argue here that some of the failure cases are simply due to a missed ground truth annotation, such as the left-most example, but some are actually  semantically correct w.r.t
the cardinality prediction, but are  penalized during evaluation because a particular object category is not available in the dataset
This is best illustrated  in the second example in Fig
N
Here, our network correctly predicts the number of objects in the scene, which  is two, however, the can does not belong to any of the N0  categories in the dataset and is thus not annotated
Similar  situations also appear in other images further to the right
 N.N
Object Counting  To show the robustness of our cardinality loss, we first  evaluate our cardinality estimation on the common crowd  counting application
To this end, we apply our approach  on the widely used UCSD dataset [N] and compare our results to four state-of-the art approaches [N, NN, NN, N0]
The  USCD dataset includes a N000-frames long video sequence,  captured by a fixed outdoor surveillance camera
In addition  to the video, the region of interest (ROI), the perspective  map of the scene and the location annotations of all pedestrians in each frame are also provided
 Implementation details
We build our cardinality network  structure on top of the well-known AlexNet [NN] architecture
However, we replace the first convolutional layer with  a single channel filter to accept grayscale images as input,  and the last fully connected layer with N layers outputs, sim- ilar to the case above (cf 
Sec
N.N)
To estimate the counts,  we calculate the mode of the negative binomial distribution
 As input, we use a grayscale image constructed by superimposing all region proposals and their scores generated by  an off-the-shelf pedestrian detector (before non-maximum  suppression)
We use the multi-scale deep CNN approach  (MS-CNN) [N] trained on the KITTI dataset [NN] for our  Table N: Count mean absolute error on UCSD dataset
 Method max downscale upscale min overall  C-Forest [NN] N.NN N.N0 N.NN N.NN N.NN  IOC [N] N.NN N.NN N.NN N.NN N.NN  Cs-CCNN [N0] N.N0 N.NN N.NN N.NN N.NN  CCNN [NN] N.NN N.NN N.NN N.N0 N.NN  Hydra Ns [NN] N.NN N.NN N.NN N.NN N.NN  Hydra Ns [NN] N.NN N.NN N.NN N.NN N.NN  Ours N.NN N.N0 0.NN N.NN N.NN  purpose
We found, that this input provides a stronger signal than the raw RGB images, yielding better results
Note  that we process the input images with a pedestrian detector,  however, we do not use any location or perspective information that is available for this dataset
During learning,  we only rely on the object count for each image region
 We follow exactly the same data split used in [NN] by  conducting four different and separate experiments on maximal, downscale, upscale and minimal subsets in UCSD  dataset
In order to train our network, similar to [NN] we  use data augmentation in each experiment by extracting N00 random patches from each training image and their corresponding ground truth counts
We also randomly flip each  patch during training
To ensure that we can count all pedestrians from the entire image at test time, we choose the  patch sizes to be exactly half of the image size (NN × NNN pixels) and then perform inference on the resulting N non- overlapping regions
The weights are initialised randomly  and the network is trained for N00 epochs
All hyperparam- eters are set as in Sec
N.N
 Results
Tab
N shows the mean absolute error between the  predicted and the ground truth counts
We show competitive or superior performance in each experiment except for  the ‘minimal’ subset
The main reason is that the training  set size is too small (only N0 images) in this particular split and even data augmentation cannot generalize the cardinality model for the test images
Moreover, unlike other  methods, we do not utilize any location information but  only provide the object count as ground truth
Considering  the overall performance, our approach outperforms state-ofthe-art counting approaches that do not use the perspective  map (Hydra Ns and Ns) and performs favourably compared  to many existing methods that exploit localisation and perspective information
 Discussion
One obvious alternative for our proposed cardinality loss may seem to directly regress for m
This alternative, however, has two main drawbacks
First, it cannot  be formulated within a Bayesian set framework to model  uncertainty, and second, the regression loss does not yield  a discrete distribution and hence does not fit the underlying  mathematical foundation of this paper
Nevertheless, we  have run the same experiments as above using a standard  regression loss but did not reach the same performance
Using the regression loss we achieve a mean cardinality error  NNNN    (a) Proposals (b) MS-CNN [N] (c) Our result  Figure N: Example pedestrian detection result of our approach
To select relevant detection candidates from an  overcomplete set of proposals (a), state-of-the-art methods  rely on non-maximum suppression (NMS) with a fixed setting (b)
We show that a better result can be achieved by  adjusting the NMS threshold adaptively, depending on the  number of instances in each image (N in this case) (c)
 (MCE) of 0.NN on MS COCO, while our loss yields an MCE of 0.NN
This is also reflected in the O-FN score which drops from NN.N to NN.N when directly regressing for m
 N.N
Pedestrian Detection  In this section, we cast the task of pedestrian detection  as a set prediction problem and demonstrate that incorporating cardinality prediction (person count) can be beneficial to  improve performance
To this end, we perform experiments  on two widely used datasets, Caltech Pedestrians [N] and  MOTNN from the MOTChallenge benchmark [NN]
Recalling Eqs
(N) and (N), we need two networks with parameters  w ∗ and θ∗ for cardinality estimation and detection proposals, respectively
For the cardinality network, we use the  exact same architecture and setup as in Sec
N.N and train it  on the training sets of these datasets
Note that it is not our  intention to engineer a completely novel pedestrian detector  here
Rather, for θ∗, we take an off-the-shelf state-of-theart system (MS-CNN) [N] and show how it can be further  improved by taking the cardinality prediction into account
 To generate the final detection outputs, most detectors  often rely on non-maximum suppression (NMS), which  greedily picks the boxes with highest scores and suppresses  any boxes that overlap more than a pre-defined threshold  TO
This heuristic makes the solution more ad-hoc than  what is expressed in our set formulation in Eq
(N)
However, we are still able to improve the detector performance  by adjusting this threshold for each frame separately
To obtain the final detection output, we use the prediction on the  number of people (m∗) in the scene to choose an adaptive NMS threshold for each image
In particular, we start from  the default value of TO, and increase it gradually until the  number of boxes reaches m∗
In the case if the number of  final boxes is larger than m∗, we pick m∗ boxes with the  highest scores, which corresponds to the MAP set prediction as discussed in Sec
N.N
To ensure a fair comparison,  we also determine the best (global) value for TO = 0.N for  Table N: Pedestrian detection results measured by FN score  (higher is better) and log-average miss rate (lower is better)
 FN-score ↑ MR ↓  Method Caltech MOTNN Calt
MOTNN  MS-CNN [N] NN.NN NN.0N N0.N NN.N  MS-CNN-DS (ours) NN.NN NN.NN N0.N NN.N  MS-CNN-DS (GT card.) NN.NN NN.NN N0.N NN.N  the MS-CNN baseline
Fig
N demonstrates an example of  the adjusted NMS threshold when considering the number  of pedestrians in the image
 To quantify the detection performance, we adapt the  same evaluation metrics and follow the protocols used on  the Caltech detection benchmark [N]
The evaluation metrics used here are log-average miss rate (MR) over false positive per image
Additionally, we compute the FN score (the  harmonic mean of precision and recall)
The FN score is  computed from all detections predicted from our DeepSet  network and is compared with the highest FN score along  the MS-CNN precision-recall curve
To calculate MR, we  concatenate all boxes resulted from our adaptive NMS approach and change the threshold over all scores from our  predicted sets
Quantitative detection results are shown in  Tab
N
Note that we do not retrain the detector, but are still  able to improve its performance by predicting the number  of pedestrians in each frame in these two dataset
 N
Conclusion  We proposed a deep learning approach for predicting  sets
To achieve this goal, we derived a loss for learning a  discrete distribution over the set cardinality
This allowed us  to use standard backpropagation for training a deep network  for set prediction
We have demonstrated the effectiveness  of this approach on crowd counting, pedestrian detection  and multi-class image classification, achieving competitive  or superior results in all three applications
As our network  is trained independently, it can be trivially applied to any existing classifier or detector, to further improve performance
 Note that this decoupling is a direct consequence of our  underlying mathematical derivation due to the i.i.d
assumptions, which renders our approach very general and applicable to a wide range of models
In future, we plan to extend  our method to multi-class cardinality estimation and investigate models that do not make i.i.d
assumptions
Another  potential avenue could be to exploit the Bayesian nature of  the model to include uncertainty as opposed to relying on  the MAP estimation alone
 Acknowledgments
This research was supported by the  Australian Research Council through the Centre of Excellence in Robotic Vision, CENN0N000NN, and through Laureate Fellowship FLNN0N00N0N to IDR
 NNNN    References  [N] C
Arteta, V
Lempitsky, J
A
Noble, and A
Zisserman
Interactive object counting
In ECCV, pages N0N–NNN, N0NN
 N, N  [N] R
Benenson, M
Mathias, R
Timofte, and L
V
Gool
Pedestrian detection at N00 frames per second
In CVPR N0NN
N  [N] Z
Cai, Q
Fan, R
Feris, and N
Vasconcelos
A unified  multi-scale deep convolutional neural network for fast object  detection
In ECCV N0NN
N, N  [N] A
B
Chan, Z.-S
J
Liang, and N
Vasconcelos
Privacy preserving crowd monitoring: Counting people without people  models or tracking
In CVPR, pages N–N, N00N
N  [N] A
B
Chan and N
Vasconcelos
Bayesian poisson regression  for crowd counting
In CVPR, pages NNN–NNN, N00N
N, N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected CRFs
In ICLR, N0NN
 N  [N] N
Dalal and B
Triggs
Histograms of oriented gradients for  human detection
In CVPR N00N, pages NNN–NNN
N  [N] P
Dollár, C
Wojek, B
Schiele, and P
Perona
Pedestrian detection: An evaluation of the state of the art
IEEE T
Pattern  Anal
Mach
Intell., NN, N0NN
N  [N] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn, and  A
Zisserman
The PASCAL Visual Object Classes Challenge  N00N (VOCN00N) Results
N  [N0] L
Fei-Fei, R
Fergus, and P
Perona
One-shot learning  of object categories
IEEE T
Pattern Anal
Mach
Intell.,  NN(N):NNN–NNN, Apr
N00N
N  [NN] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and  D
Ramanan
Object detection with discriminatively trained  part based models
IEEE T
Pattern Anal
Mach
Intell.,  NN(N):NNNN–NNNN, N0N0
N  [NN] L
Fiaschi, U
Koethe, R
Nair, and F
A
Hamprecht
Learning to count with regression forest and structured labels
In  ICPR, pages NNNN–NNNN, N0NN
N  [NN] A
Geiger, P
Lenz, and R
Urtasun
Are we ready for autonomous driving? The KITTI Vision Benchmark Suite
In  CVPR N0NN
N  [NN] R
Girshick
Fast R-CNN
In ICCV N0NN
N  [NN] Y
Gong, Y
Jia, T
Leung, A
Toshev, and S
Ioffe
Deep  convolutional ranking for multilabel image annotation
arXiv  preprint arXiv:NNNN.NNNN, N0NN
N, N  [NN] Y
Gong, Y
Jia, T
Leung, A
Toshev, and S
Ioffe
Deep convolutional ranking for multilabel image annotation
CoRR,  abs/NNNN.NNNN, N0NN
N  [NN] A
Graves, A.-r
Mohamed, and G
E
Hinton
Speech recognition with deep recurrent neural networks
In IEEE International Conference on Acoustics, Speech and Signal Processing, ICASSP N0NN, Vancouver, BC, Canada, May NN-NN,  N0NN, pages NNNN–NNNN, N0NN
N  [NN] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural Comput., N(N):NNNNNNN0, Nov
NNNN
N  [NN] J
Hosang, R
Benenson, and B
Schiele
Learning nonmaximum suppression
In CVPR N0NN, July N0NN
N  [N0] H
Idrees, I
Saleemi, C
Seibert, and M
Shah
Multi-source  multi-scale counting in extremely dense crowd images
In  CVPR, N0NN
N  [NN] J
Johnson, A
Karpathy, and L
Fei-Fei
Densecap: Fully  convolutional localization networks for dense captioning
In  CVPR N0NN
N, N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
ImageNet  classification with deep convolutional neural networks
In  NIPS*N0NN, pages N0NN–NN0N
N, N, N  [NN] D
Lee, G
Cha, M.-H
Yang, and S
Oh
Individualness  and determinantal point processes for pedestrian detection
 In ECCV N0NN, pages NN0–NNN
DOI: N0.N00N/NNN-N-NNNNNNNN-N N0
N  [NN] V
Lempitsky and A
Zisserman
Learning to count objects  in images
In NIPS, pages NNNN–NNNN, N0N0
N  [NN] G
Lin, A
Milan, C
Shen, and I
Reid
RefineNet: Multipath refinement networks for high-resolution semantic segmentation
In CVPR N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, L
Bourdev, R
Girshick, J
Hays, P
Perona, D
Ramanan, C
L
Zitnick, and  P
Dollár
Microsoft COCO: Common objects in context
 arXiv:NN0N.0NNN [cs], May N0NN
arXiv: NN0N.0NNN
N, N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.Y
Fu, and A
C
Berg
SSD: Single shot multibox detector
 In ECCV N0NN, Lecture Notes in Computer Science, pages  NN–NN, N0NN
DOI: N0.N00N/NNN-N-NNN-NNNNN-0 N
N, N  [NN] R
P
Mahler
Statistical multisource-multitarget information  fusion, volume NNN
Artech House Boston, N00N
N, N, N  [NN] A
Milan, L
Leal-Taixé, I
Reid, S
Roth, and K
Schindler
 MOTNN: A benchmark for multi-object tracking
 arXiv:NN0N.00NNN [cs], Mar
N0NN
N  [N0] V
N
Murthy, V
Singh, T
Chen, R
Manmatha, and D
Comaniciu
Deep decision network for multi-class image classification
In CVPR N0NN, June N0NN
N  [NN] H
Nam and B
Han
Learning multi-domain convolutional  neural networks for visual tracking
In CVPR N0NN
N  [NN] D
Onoro-Rubio and R
J
López-Sastre
Towards  perspective-free object counting with deep learning
In  ECCV, pages NNN–NNN, N0NN
N  [NN] G
Papandreou, L.-C
Chen, K
P
Murphy, and A
L
Yuille
 Weakly- and semi-supervised learning of a deep convolutional network for semantic image segmentation
In ICCV  N0NN, Dec
N0NN
N  [NN] T
T
Pham, S
Hamid Rezatofighi, I
Reid, and T.-J
Chin
 Efficient point process inference for large-scale object detection
In CVPR N0NN
N  [NN] V.-Q
Pham, T
Kozakaya, O
Yamaguchi, and R
Okada
 COUNT forest: Co-voting uncertain number of targets using random forest for crowd density estimation
In ICCV,  pages NNNN–NNNN, N0NN
N, N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS*N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
Imagenet large scale visual recognition challenge
Int
J
Comput
Vision, NNN(N):NNN–NNN,  N0NN
N  NNNN    [NN] P
Sermanet, D
Eigen, X
Zhang, M
Mathieu, R
Fergus,  and Y
LeCun
OverFeat: Integrated recognition, localization and detection using convolutional networks
CoRR,  abs/NNNN.NNNN, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
CoRR,  abs/NN0N.NNNN, N0NN
N, N  [N0] I
Sutskever, O
Vinyals, and Q
V
Le
Sequence to sequence  learning with neural networks
In NIPS*N0NN, pages NN0N–  NNNN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
CoRR, abs/NN0N.NNNN,  N0NN
N  [NN] O
Vinyals, S
Bengio, and M
Kudlur
Order matters: Sequence to sequence for sets
arXiv:NNNN.0NNNN [cs, stat],  Nov
N0NN
arXiv: NNNN.0NNNN
N  [NN] O
Vinyals, M
Fortunato, and N
Jaitly
Pointer networks
In  NIPS*N0NN, pages NNNN–NN00
N  [NN] P
Viola and M
J
Jones
Robust real-time face detection
Int
 J
Comput
Vision, NN(N):NNN–NNN, May N00N
N  [NN] B.-N
Vo et al
Model-based classification and novelty detection for point pattern data
In ICPR, N0NN
N  [NN] S
Walk, N
Majer, K
Schindler, and B
Schiele
New features and insights for pedestrian detection
In CVPR N0N0
 N  [NN] J
Wang, Y
Yang, J
Mao, Z
Huang, C
Huang, and W
Xu
 CNN-RNN: A unified framework for multi-label image classification
In CVPR, June N0NN
N, N, N, N, N  [NN] Y
Wei, W
Xia, J
Huang, B
Ni, J
Dong, Y
Zhao,  and S
Yan
CNN: Single-label to multi-label
CoRR,  abs/NN0N.NNNN, N0NN
N  [NN] M
D
Zeiler and R
Fergus
Visualizing and understanding convolutional networks
In ECCV N0NN, pages NNN–NNN
 DOI: N0.N00N/NNN-N-NNN-N0NN0-N NN
N  [N0] C
Zhang, H
Li, X
Wang, and X
Yang
Cross-scene crowd  counting via deep convolutional neural networks
In CVPR,  pages NNN–NNN, N0NN
N, N  [NN] Y
Zhang, D
Zhou, S
Chen, S
Gao, and Y
Ma
Singleimage crowd counting via multi-column convolutional neural network
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages NNN–NNN, N0NN
 N  NNNNQuantitative Evaluation of Confidence Measures in a Machine Learning World   Quantitative evaluation of confidence measures in a machine learning world  Matteo Poggi, Fabio Tosi, Stefano Mattoccia  University of Bologna  Department of Computer Science and Engineering (DISI)  Viale del Risorgimento N, Bologna, Italy  {matteo.poggiN, fabio.tosiN, stefano.mattoccia}@unibo.it  Abstract  Confidence measures aim at detecting unreliable depth  measurements and play an important role for many purposes and in particular, as recently shown, to improve  stereo accuracy
This topic has been thoroughly investigated by Hu and Mordohai in N0N0 (and N0NN) considering NN confidence measures and two local algorithms on  the two datasets available at that time
However, since then  major breakthroughs happened in this field: the availability  of much larger and challenging datasets, novel and more  effective stereo algorithms including ones based on deep  learning and confidence measures leveraging on machine  learning techniques
Therefore, this paper aims at providing an exhaustive and updated review and quantitative  evaluation of NN (actually, NN considering variants) stateof-the-art confidence measures - focusing on recent ones  mostly based on random-forests and deep learning - with  three algorithms on the challenging datasets available today
Moreover we deal with problems inherently induced by  learning-based confidence measures
How are these methods able to generalize to new data? How a specific training improves their effectiveness? How more effective confidence measures can actually improve the overall stereo accuracy?  N
Introduction  Although depth from stereo still represents an open problem [N, NN, NN], in recent years this field has seen notable improvements concerning the effectiveness of such algorithms  (e.g., [NN, N0]) and confidence measures, aimed at detecting  unreliable disparity assignments, proved to be very effective cues when plugged in stereo vision pipelines as shown  in [NN, NN, NN, N0]
However, shortcomings of stereo algorithms have been emphasized by the availability of very  challenging datasets with ground-truth such as KITTI N0NN  (KNN) [N], KITTI N0NN (KNN) [NN] and Middlebury N0NN  (MNN) [NN]
Thus, the ability to reliably predict failures of a  stereo algorithm by means of a confidence measure is fundamental and many approaches have been proposed for this  purpose
Hu and Mordohai [NN] exhaustively reviewed confidence measures available at that time, with two variants of  a standard local algorithm, and defined a very effective metric to evaluate their effectiveness on the small and mostly  unrealistic dataset [NN] with ground-truth available
However, since then there have been major breakthroughs in this  field:  • Novel and more reliable confidence prediction methods, in particular those based on random-forests [N, NN,  NN, NN] and deep learning [NN, N0]  • Much larger datasets with ground-truth depicting very  challenging and realistic scenes acquired in indoor  [NN] and outdoor environments [N, NN]  • Novel and more effective stereo algorithms, some  leveraging on deep learning techniques [NN, NN], more  and more often coupled with confidence measures  [N0, NN, NN]
Moreover, in recent years, SGM [N] became the preferred disparity optimization method for  most state-of-the-art stereo algorithms (e.g., [NN, N0])  Considering these facts, we believe that this field deserves a further and deeper analysis
Therefore, in this paper  we aim at i) extending and updating the taxonomy provided  in [NN] including novel confidence measure and in particular those based on machine learning techniques, ii) exhaustively assessing their performance on the larger and much  more challenging datasets [NN, NN] available today, iii) understanding the impact of training data on the effectiveness  of confidence measures based on machine learning, iv) assessing their performance when dealing with new data and  state-of-the-art stereo algorithms, v) and evaluating their behavior when plugged into a state-of-the-art stereo pipeline
 Although our focus is mostly on approaches based on  machine learning, for completeness, we include in our taxonomy and evaluation any available confidence measure
 NNNNN    Overall, we assess the performance of NN measures, actually NN considering their variants, providing an exhaustive  evaluation of state-of-the-art in this field with three stereo  algorithms on the three challenging datasets with groundtruth KNN, KNN and MNN available today
 N
Related work  The most recent taxonomy and evaluation of confidence  measures for stereo was proposed by Hu and Mordohai  [NN]
They exhaustively categorized the NN confidence measures available at that time in six categories according to  the cues exploited to infer depth reliability
Moreover, they  proposed an effective metric to clearly assess the effectiveness of confidence prediction based on area under the curve  (AUC) analysis and quantitatively evaluated the considered  measures on former indoor Middlebury [NN, NN] and outdoor Fountain PNN [NN] datasets with a standard local algorithm using sum of absolute differences (SAD) and normalized cross correlation (NCC) as matching costs
 However, since then novel confidence measures were  proposed [NN, NN, NN, N, N, N, NN, NN, NN] and more importantly this field was affected by methodologies inspired  by machine learning
In their seminal work, Hausler et al
 [N] proposed to infer match reliability by feeding a random  forest, trained for classification, with multiple confidence  measures showing that this fusion strategy yields much better performance with respect to any other considered confidence measure
Following this strategy, the reliability of  confidence measures was further improved in [NN] and [NN]  considering more effective features
In this context, [NN]  enables to infer a confidence measure leveraging only features extracted in constant time from the left disparity map
 Differently from [N], in [NN, NN, NN] the random-forests  are trained in regression mode
Concerning methodologies  based on Convolutional Neural Networks (CNN), Seki and  Pollefeys [N0] proposed to infer a confidence measure by  processing features extracted from the left and right disparity maps while Poggi and Mattoccia [NN] learned from  scratch a confidence measure by feeding to a CNN the left  disparity map
Moreover, in [NN] was proposed a method  to combine multiple hand-crafted cues and in [NN] a strategy to improve confidence accuracy exploiting local consistency
Concerning unsupervised training of confidence  measures, Mostegel et al
[NN] proposed to determine training labels exploiting contradictions between multiple depth  maps computed from different viewpoints while Tosi et al
 [NN] leveraging on a pool of confidence measures
 This field has also seen the deployment of confidence  measures plugged into stereo vision pipelines to improve  the overall accuracy as proposed in [NN, NN, NN, N0, NN,  NN, NN, NN, N], to deal with occlusions [N, NN] or to improve accuracy near depth discontinuities [N]
Most of  these approaches are aimed at improving the accuracy of  Semi Global Matching (SGM) [N] algorithm exploiting as  cue an estimated match reliability
Confidence measures  have been effectively deployed for sensor fusion combining  depth maps from multiple sensors [N0, NN]
Finally, confidence measures suited for embedded stereo systems have  been analyzed in [NN]
 Recent years have also witnessed the availability of very  challenging datasets depicting indoor, such as the MNN [NN],  and outdoor environments, such as KNN [N] and KNN [NN]
 Differently from former standard dataset [NN] used to test  algorithms, the novel ones clearly emphasize that stereo is  still an open research problem
This fact also paved the  way to most recent trend in stereo vision aimed at tackling stereo with CNNs
In this context [NN] Zbontar and  Le Cun proposed the first successful attempt to infer an effective matching cost from a stereo pair with a CNN now  deployed by almost any top-performing stereo method on  KNN, KNN and MNN datasets
Following this strategy Chen  et al
[N] and Luo et al
[NN] proposed very efficient architectures enabling real-time stereo matching while [N0]  enables to combine multiple disparity maps with a CNN
 A further step forward, aimed at departing from a conventional stereo pipeline, is represented by Mayer et al
[NN]
 In this case, given a stereo pair, the left-right stereo correspondence is regressed from scratch with a CNN trained  end-to-end
 N
Taxonomy of confidence measures  Despite the large number of confidence measures proposed, all of them process (a subset of) information concerning the cost curve, the relationship between left and  right images or disparity maps
Following [NN], confidence  measures can be grouped into categories according to their  input cues
To better clarify which cues are processed by  each single measure we introduce the following notation
 Given a stereo pair made of left (L) and right (R) images,  a generic stereo algorithm assigns a cost curve c to each  pixel of L
We denote the minimum of such curve as cN and  its corresponding disparity hypothesis as dN
We refer to  the second minimum of the curve as cN (and to its disparity  hypothesis as dN), while cNm denotes the second local minimum (it may coincide with cN)
In our taxonomy we group  the considered NN confidence measures (and their variants)  in the following N categories
 N.N
Minimum cost and local properties of the cost curve  These methods analyze local properties of the cost curve  encoded by cN, cN and cNm
As confidence values for each  point, the matching score measure (MSM) [NN] simply assumes the negation of minimum cost cN
Maximum margin  (MM) computes the difference between cNm and cN while  its variant maximum margin naive (MMN) [NN] replaces  NNNN    cNm with cN
Non linear margin (NLM) [N] computes a  non linear transformation according to the difference between cNm and cN while its variant non linear margin naive  (NLMN) replaces cNm with cN
Curvature (CUR) [NN] and  local curve LC [NN] analyze the behavior of the cost curve  around the minimum cN and its two neighbors at (dN-N) and  (dN+N) according two similar, yet different, strategies
Peak  ratio (PKR) [N0, NN] computes the ratio between cNm and  cN
In one of its variants, peak ratio naive (PKRN) [NN],  cNm is replaced with the second minimum cN
In average  peak ratio (APKR) [NN] the confidence value is computed  averaging PKR values on a patch
We include in our evaluation a further variant, based on the same patch-based average strategy adopted by APKR and referred to as average peak ratio naive (APKRN)
Similarly and respectively,  weighted peak ratio (WPKR) [NN] and weighted peak ratio naive (WPKRN), average on a patch the original confidence measures PKR and PKRN with binary weights computed according to the reference image content
Finally, we  include in this category two confidence measures belonging  to the pool of features proposed in [N]
Disparity ambiguity measure (DAM) computes the distance between dN and  dN, while semi-global energy (SGE) relies on a strategy inspired by the SGM algorithm [N]
It sums, within a patch,  the cN costs of points laying on multiple scanlines penalized,  if their disparity is not the same of the point under examination, by PN when the difference is N and by PN (>PN)  otherwise
 N.N
Analysis of the entire cost curve  Differently from previous confidence measures, those  belonging to this category analyze for each point the overall distribution of matching costs
Perturbation (PER) [N]  measures the deviation of the cost curve to an ideal one
 Maximum likelihood measure (MLM) [NN, NN] and attainable likelihood measure (ALM) [NN, NN] infer from the  matching costs a probability density function (pdf) with respect to an ideal cN, respectively, equal to zero for MLM and  to the actual cN for ALM
Number of inflections (NOI) [NN]  determines the number of local minima in the cost curve  while local minima in neighborhood (LMN) [NN] counts,  on a patch, the number of points with local minimum at the  same disparity dN of the examined point
Winner margin  measure (WMN) [NN] normalizes for each point the difference between cNm and cN by the sum of all costs while its  variant winner margin measure naive (WMNN) [NN] adopts  the same strategy replacing cNm with cN
Finally, negative  entropy measure (NEM) [NN, NN] relates the degree of uncertainty of each point to the negative entropy of its matching costs
 N.N
Left and right consistency  This category evaluates the consistency between corresponding points according to two different cues: one, symmetric, based on left and right maps and one, asymmetric,  based only on the left map
Confidence measures adopting  the first strategy are: left-right consistency (LRC) [N, NN],  that assigns as confidence the negation of the absolute difference between the disparity of a point in L and its homologous point in R, and left-right difference (LRD) [NN] that  computes the difference between cN and cN divided by the  absolute difference between cN and the minimum cost of the  homologous point in R
We include in this category zeromean sum of absolute differences (ZSAD) [N] that evaluates the dissimilarity between patches centered on homologous points in the stereo pair
It is worth pointing out that  for LRC and ZSAD the full cost volume is not required
 On the other hand, confidence measures based only on the  analysis of the reference disparity map exploit the uniqueness constraint
Asymmetric consistency check (ACC) [NN]  and uniqueness constraint (UC) [N] detect the pool of multiple colliding points at the same coordinate in the right image
ACC verifies, according to a binary strategy, whether  the candidate with the largest disparity in the pool has the  smallest cost with respect to any other one while UC simply selects as valid the candidate with the minimum cost
 Moreover, we consider two further non binary variants of  this latter strategy
One referred to as uniqueness constraint  cost (UCC), that assumes as confidence the negative of cN,  and one referred to as uniqueness constraint occurrences  (UCO), that assumes that confidence is inversely proportional to the number of collisions
For the latter four outlined strategies the other candidates in the pool of colliding  points are always set to invalid
 N.N
Disparity map features  Confidence measures belonging to this group are obtained by extracting features from the reference disparity  map
Therefore they are potentially suited to infer confidence for any ND sensing device
Distance to discontinuity  (DTD) [NN, NN] determines for each point the distance to  the supposed closest depth boundary while, for the same  purpose, disparity map variance (DMV) computes the disparity gradient module [N]
Remaining confidence measures  belonging to this category extract features on a patch centered on the examined point
Variance of disparity (VAR)  [NN, NN] computes the disparity variance, disparity agreement (DA) [NN] counts the number of points having the  same disparity of the central one, median deviation of disparity (MDD) [NN, NN, NN] computes the difference between  disparity and its median and disparity scattering (DS) [NN]  encodes the number of different disparity assignments on  the patch
 NNN0    N.N
Reference image features  Confidence measures belonging to this category use as  domain only the reference image
Distance to border (DB)  [NN, NN] aims at detecting invalid disparity assignments often originated in the image border due to the stereo setup
 Assuming the left image as reference a more meaningful  variant of DB, referred to as distance to left border (DLB),  deploys the distance to the left border
Both measures rely  on prior information and not on image content
The last two  confidence measure of this category extract features from  the reference image: horizontal gradient measure (HGM)  [N, NN] analyses the response to horizontal gradients in order to detect image texture while distance to edge (DTE)  attempts to detect depth boundaries, sometimes unreliable  for stereo algorithms, according to the distance to the closest edge
 N.N
Image distinctiveness  The idea behind these confidence measures is to exploit  the notion of distinctiveness of the examined point within  its neighborhoods along the horizontal scanline of the same  image
Distinctiveness (DTS) [NN, NN] exactly leverages  on such definition by assuming as confidence for a given  point the lowest self-matching cost computed within a certain prefixed range excluding the point under examination
 Distinctive similarity measure (DSM) [NN, NN] assigns as  confidence value to a given point the product of two DTSs,  one computed on the reference image and the other one on  the right image in the location of the assumed homologous  point, divided by the square of cN [NN] or cN [NN]
For a given  point the self-aware matching measure (SAMM) [NN, NN]  computes the zero mean normalized correlation between the  left-right cost curve, appropriately translated according to  the assumed disparity, and the left-left cost curve
 N.N
Learning-based approaches  Recently, some authors proposed to infer confidence  measures exploiting machine learning frameworks
A common trend in such approaches consists in feeding a random  forest classifier with multiple confidence measures [N, NN,  NN, NN] or deploying for the same purpose deep learning  architectures [N0, NN]
A notable difference with conventional confidence measures reviewed so far, is that learningbased approaches require a training phase, on datasets with  ground-truth or by means of appropriate methodologies  [NN, NN], to infer the degree of uncertainty of disparity assignments
 N.N.N Random forest approaches  In this category a seminal approach is represented by ensemble learning (ENSc) [N]
This method infers a confidence measure by feeding to a random forest, trained  for classification, a feature vector made of NN confidence measures extracted from the original stereo pair, the  left and right disparity maps and the cost volumes computed on the stereo pair at different scales
Then, the  resulting features are up-sampled to the original resolution
The feature vector consists of the following measures: PKRN,N,N, NEMN,N,N, PERN,N,N, LRCN, HGMN,N,N,  DMVN,N,N, DAMN,N,N, ZSADN,N,N and SGEN
The superscript refers to the scale: N original resolution, N halfresolution and N quarter-resolution
The authors advocate  to train the random-forest with such feature vector for classification ”as confidence measures do not contain matching  error magnitude information”, by extracting the posterior  probability of the predicted class at inference time
However, the average response over all the trees in the forest  can be used as well by training in regression
Therefore,  we also include in our evaluation ensemble learning in regression mode (ENSr) that to the best of our knowledge  has not been considered before
In ground control point  (GCP) [NN] the confidence measure is inferred by feeding to  a random forest, trained in regression mode, a feature vector  containing N measures computed at the original scale
The  features extracted from left image, left and right disparity  maps and the cost volume are: MSM, DB, MMN, AML,  LRC, LRD, DTD and MDD
In leveraging stereo (LEV)  [NN] a feature vector containing NN measures extracted from  the left image, left and right disparity maps and cost volume is fed to a random forest trained for regression
The  feature vector, superscript encodes the patch size, consists  of: PKR, PKRN, MSM, MM, WMN, MLM, PER, NEM,  LRC, LRD, LC, DTD, VARN,N,N,N, MDDN,N,N,N, HGM and  DLB
Differently from previous approaches, O(N) disparity features (ON) [NN] proposes a method entirely based on  features extracted in constant time from the left disparity  map
The feature vector, superscript encodes the patch size,  consists of: DAN,N,N,N, DSN,N,N,N, MEDN,N,N,N, MDDN,N,N,N  and VARN,N,N,N, being MED the median of disparity
As for  ENSr, GCP and LEV the feature vector is fed to a random  forest trained in regression mode
We conclude this section  observing that ENS [N] and LEV [NN] also propose variants  of the original method with a reduced number of features,  respectively N and N
For LEV, the features are selected analyzing the importance of variable once trained the random  forest with the full NN feature vector and then retraining the  network
However, as reported in [N] and [NN], being higher  the effectiveness of full feature vectors, we consider in our  evaluation such versions of ENS, in classification and regression mode, and LEV
 N.N.N CNN approaches  As for many other computer vision fields, convolutional  neural networks have recently proven to be very effective  NNNN    also for confidence estimation
In patch based confidence  prediction (PBCP) [N0] the input of a CNN consists of two  channels pN and pN computed, on a patch basis, from left  and right disparity maps
Being patch values strictly related  to their central pixel, confidence map computation is pretty  demanding
A faster solution, made of patches no longer  related to central pixels, allows for a very efficient confidence map prediction according to common optimization  techniques in deep learning, with a minor reduction of effectiveness
However, being the full-version more effective  we consider this one in our experiments
 A step towards a further abstraction is represented by  confidence CNN (CCNN) [NN]
In fact, in this approach  confidence prediction is regressed by a CNN without extracting any cue from the input data
The deep network,  trained on patches, learns from scratch a confidence measure by processing only the left disparity map
This property, shared with ON, makes these methods potentially  suited to any ND sensor [NN, NN]
 N.N
SGM specific  This category groups two approaches intrinsically related to SGM [N]
The idea behind these approaches is to exploit intermediate results available in such stereo algorithm  to infer a confidence map
Specifically, the local-global  relation (PS) [N0] combines the cues available in the cost  curve before and after semi-global optimization, while sum  of consistent scanlines (SCS) [NN] counts for each pixel the  number of scanlines voting for the same disparity assigned  by the full SGM pipeline
 N
Evaluation protocol and experimental results  In this section, we report exhaustive experimental results  concerning different aspects related to the examined confidence measures on the following datasets KNN (NNN images), KNN (N00 images) and MNN (NN images)
For each  dataset we consider the stereo pairs belonging to the training set being the ground-truth available
We include in the  evaluation all the measures previously reviewed including  any variant
Moreover, for patch-based ones (i.e., APKR,  APKRN, WPKR, WPKRN, DA, DS, MED, VAR) we consider patches of different size (i.e., N × N, N × N, N × N and NN × NN corresponding to superscript N,N,N,N in LEV and ON features) being the scale effective according to [NN, NN]
 Of course, we consider state-of-the-art methods based on  random forests, including variant ENSr, and the two approaches based on CNNs
Overall, we evaluate NN confidence measuresN
In Section N.N we assess with three stereo  algorithms the performance of such measures when dealNSource code available at vision.disi.unibo.it/˜mpoggi/  code.html  ing with the selection of correct matches by means of the  ROC curve analysis proposed in [NN] and widely adopted  in this field [N, NN, NN, NN, NN, N0]
Moreover, since machine learning is the key technology behind most recent approaches, in Section N.N we report how training affects their  effectiveness focusing in particular on the amount of training samples and the capability to generalize across different  data (i.e., datasets)
Finally, being confidence measures often employed to improve stereo accuracy [NN, NN, NN, N0],  in Section N.N we assess the performance of the most effective confidence measures when plugged in one of such  state-of-the-art methods [NN]
 N.N
Detection of correct matches  The ability to distinguish correct disparity assignments  from wrong ones is the most desirable property of a confidence measure
To quantitatively evaluate this, [NN] adopted  ROC curve analysis, measuring the capability of removing  errors from a disparity map according to the confidence values
That is, given a disparity map, a subset p of pixels is  extracted in order of decreasing confidence (e.g., N% of the  total pixels) and the error rate on such sample is computed,  as the percentage of points with an absolute distance from  ground-truth value higher than a threshold τ , varying with  the dataset
Then, the subset is increased by extracting more  pixels (e.g., an additional N%) and the error rate is computed, until all the pixels in the image are considered
Ties  are solved by including all the tying pixels in the subsample
The relation between each sub-sample p and its error  rate draws a ROC curve and its AUC measures the capability of the confidence measure to effectively distinguish  good matches from wrong ones
Considering a disparity  map with a portion ε ∈ [0, N] of erroneous pixels, an opti- mal measure would be able to achieve a 0 error rate when  extracting the first (N − ε) points
Thus, the optimal AUC value [NN] can be obtained as follows  AUCopt =  ∫ ε N−ε  p− (N− ε)  p dp = ε+ (N− ε) ln (N− ε)  (N)  Following this protocol, we evaluate the NN confidence  measures on KNN, KNN and MNN with three popular stereo  algorithms adopting the winner takes all strategy for disparity selection:  • AD-CENSUS: aggregates matching costs, computed  on N × N patches with census transform [NN], with a N× N box-filter
 • MC-CNN [NN]: local method inferring costs from image patches using a CNN
We used the same networks  trained by the authors on KNN, KNN and MNN
 NNNN  vision.disi.unibo.it/~mpoggi/code.html vision.disi.unibo.it/~mpoggi/code.html   (a)  KNN (ε = NN.NN%) KNN (ε = NN.NN%) MNN (ε = NN.NN%)  Category measure rank AUC measure rank AUC measure rank AUC  N.N APKRNN N NN 0.NN0N APKRNN N  NN 0.NNNN APKRNN N N 0.NNNN  N.N WMNN NNN 0.NNNN WMN NNN 0.N0NN WMN NNN 0.NNNN  N.N LRD NN0 0.NNNN LRD NNN 0.NNNN LRD NNN 0.NNNN  N.N DANN N N 0.NNNN DANN N  N 0.NNNN DANN N N 0.NNNN  N.N DB NNN 0.NNNN DB NNN 0.NN0N DLB NNN 0.NNNN  N.N SAMM NNN 0.N0N0 SAMM NN0 0.NNNN DSM NN0 0.NNNN  N.N.N ON NN 0.NN0N ON NN 0.NNNN ON NN 0.NNNN  N.N.N CCNN NN 0.NNNN CCNN NN 0.N0NN CCNN NN 0.NNNN  Optimal 0.N0NN 0.0NNN 0.0NNN  Categories N.N.N and N.N.N  Measure KNN KNN MNN  ENSc N NN NN  ENSr N N NN  GCP N N N  LEV N N N  ON N N N  PBCP N N N  CCNN N N N  (b)  (c)  KNN (ε = NN.N0%) KNN (ε = NN.NN%) MNN (ε = NN.N0%)  Category measure rank AUC measure rank AUC measure rank AUC  N.N APKRNN N NN 0.0NNN APKRNN N  NN 0.0N0N APKRNN N N 0.0NNN  N.N WMN NN0 0.0NNN WMN NNN 0.0NNN WMN NNN 0.0NNN  N.N LRD NNN 0.0NNN LRD NNN 0.0NNN UCC NNN 0.0NNN  N.N DSN N N 0.0NNN DSN N  N 0.0NNN DSNN N NN 0.N0NN  N.N DLB NNN 0.NNNN HGM NNN 0.NNNN DLB NNN 0.NNN0  N.N SAMM NNN 0.0NNN SAMM NNN 0.0NNN DSM NN0 0.NNNN  N.N.N ON NN 0.0NNN ON NN 0.0NNN ON NN 0.0NN0  N.N.N CCNN NN 0.0NNN CCNN NN 0.0NNN CCNN NN 0.0NNN  Optimal 0.0NNN 0.0NNN 0.0NNN  Categories N.N.N and N.N.N  Measure KNN KNN MNN  ENSc N N NN  ENSr N N NN  GCP N N NN  LEV N N N  ON N N N  PBCP N N N  CCNN N N N  (d)  (e)  KNN (ε = NN.NN%) KNN (ε = NN.NN%) MNN (ε = NN.NN%)  Category measure rank AUC measure rank AUC measure rank AUC  N.N APKRNN N N 0.0NNN APKRNN N  N 0.0NNN APKRN N N 0.0NNN  N.N WMN NNN 0.0NNN WMN NNN 0.0N0N WMN NN 0.0.NNN  N.N UCC NNN 0.0NNN UCC NNN 0.0NN0 UCC NNN 0.0NNN  N.N DSNN N NN 0.0NNN DSNN N  NN 0.0N0N DSNN N NN 0.0NNN  N.N DB NNN 0.NNNN DB NNN 0.NNNN DLB NN0 0.NNNN  N.N DSM NNN 0.0NNN DSM NNN 0.0NNN DSM NNN 0.N0NN  N.N.N LEV NN 0.0NNN ON NN 0.0NNN ON NN 0.0NNN  N.N.N CCNN NN 0.0NNN CCNN NN 0.0N0N CCNN NN 0.0NNN  N.N SCS NNN 0.0NNN SCS NNN 0.0NN0 SCS NNN 0.N0N0  Optimal 0.0NNN 0.0NNN 0.0NNN  Categories N.N.N and N.N.N  Measure KNN KNN MNN  ENSc NN NN NN  ENSr N N NN  GCP N N NN  LEV N N NN  ON N N N  PBCP N N N  CCNN N N N  (f)  Table N
Detection of correct matches with three stereo algorithms - top (a,b) AD-CENSUS, middle (c,d) MC-CNN and bottom (e,f) SGM  - and three datasets KNN, KNN and MNN
For each algorithm there are two tables
On the left the best confidence measure for each category  (e.g., N.N refers to measures belonging to the category reviewed in Section N.N), the ranking (within categories and, in superscript, absolute)  and the AUC
On the right, the absolute ranking of learning-based confidence measures
We also report average error rate ε for each  dataset on the top labels
Concerning categories N.N.N and N.N.N we trained each confidence measure on the first N0 images of KNN with the  considered algorithm (i.e., (a,b) with AD-CENSUS, (c,d) with MC-CNN and (e,f) with SGM)
 • SGM [N]: eight scanline implementation with ADCENSUS aggregated costs as data term and PN and PN,  respectively, 0.N and 0.N (being costs normalized)
 Concerning confidence measures based on machine  learning, for each stereo algorithm, we train each one on a  subset of images from the KNN dataset (the first N0 images,  extracting a sample from each pixel with available groundtruth, for a total of N.N million samples) and evaluate it on  all the datasets (for KNN excluding the training images), in  order to assess their performance on very different scenes
 For approaches based on random forests we train on N0 trees  as suggested in [NN] and adopting a fixed number of iteration as termination criteria (e.g., proportional to the number  of trees), while we train CNN based measures for NN epochs  (resulting in about N million iterations), with a batch of size  NN, learning rate of 0.00N and momentum of 0.N, by minimizing the loss functions reported in [NN, N0]
Different  training sets (e.g., datasets, number of samples and so on)  may lead to different performance
This fact will be thoroughly evaluated in Section N.N
For the evaluation reported  in this section we trained only on KNN in order to assess  how much a confidence measure is able to generalize its behavior across different datasets which is an important and  desirable feature in most practical applications
We adopt  as error bound τ = N for KNN and KNN and τ = N for MNNN  as suggested in the corresponding papers
 In Table N we summarize results in terms of AUC averaged on each dataset (KNN, KNN and MNN) for AD-CENSUS  (a,b), MC-CNN (c,d) and SGM (d,e), reporting the averNMiddlebury frames have been processed at quarter resolution to level  out the original disparity range with other datasets (N00 vs NNN for KITTIs)
 NNNN    age error rate ε for each dataset
For each algorithm we  report on the left table the best measure for each category  described in Section N and its absolute ranking and, on the  right table, the absolute ranking for confidence measures  based on machine learning
Observing tables N (a,c,e),  we can notice that these latter measures always yield the  best results, with CCNN systematically the top-performing  one in terms of AUC, and the ones based on random forest following very close (with ON the best in its category  in N out of N experiments)
Focusing on categories N.N.N  and N.N.N, we can notice that in most cases PBCP, ON and  LEV perform very well with the exception of the SGM algorithm and MNN (Table N(f))
In this specific case, excluding CCNN, APKRNN performs better than approaches  based on machine learning
Anyway, in this case too, the  effectiveness of ON and PBCP seems acceptable
This fact  highlights that some confidence measure based on learning  approaches (in particular CCNN but also ON and PBCP)  have excellent performance across different data
Interestingly, such measures use as input cue only the disparity  maps
Tables N (b,d,f) also show that for other measures  such as ENSc, ENSr, GCP and LEV this behavior is not  always verified, in particular with MNN
Finally, we observe that ENSr always (and sometimes significantly) outperforms ENSc
Concerning other categories, we can notice that APKR yields good results in all the experiments  and not only with MNN and SGM as already highlighted
 Other interesting confidence measures are those belonging  to category N.N and in particular DA with AD-CENSUS and  DS with MC-CNN and SGM
Such results confirm that processing cues from the disparity map only, as done by best  learning-based approaches, yields reliable confidence estimation
Other categories do not seem particularly effective,  especially those based only on left image cues have always  the overall worst performance
For measures belonging to  category N.N, though not very effective excluding experiments with SGM, WMN always achieves the best results
 Besides, it’s worth pointing out that naive versions of traditional strategies produce worse AUC values than their original counterparts
Regarding SGM-specific methods, SCS  always outperforms PS but with AUC values quite far from  the top-performing approaches
Finally, concerning categories N.N and N.N, such measures on the three datasets do  not grant reliable confidence prediction
 N.N
Impact of training data  Having assessed the performance of the confidence measure with different algorithms and datasets, this section aims  at analyzing the impact of training data on the effectiveness  of learning-based measures
To quantitatively compare the  results between different training configuration, we define  ∆k as the ratio between the AUC value achieved by the measure k and the AUCopt as,  Figure N
Ratio between the average AUC achieved by learningbased confidence measures trained with different number of samples from KNN and the optimal AUC
Evaluated on the rest of KNN  with AD-CENSUS algorithm
 ∆k = AUCk  AUCopt (N)  The lower the ∆k, the better the training configuration
The first issue we are going to evaluate is the amount  of training samples required and how it affects the overall effectiveness of each confidence measure
We carried  out multiple trainings with a different number of samples  obtained from N, N0, NN, N0 and NN stereo pairs of KNN  dataset starting from the first image
These subsets provide, respectively, about 0.N, N.N, N, N.N and N.N million  samples with available ground-truth for training
By using  more data we can deploy more complex random forests as  well
Nevertheless, we keep the same parameters and termination criteria described in Section N.N to compare the  behavior of the same forest fed with different feature vectors when more samples are available
Figure N reports ∆k, as a function of the number of training samples, for the best  six measures based on machine learning (i.e., ENSr, GCP,  LEV, ON, CCNN and PBCP) trained on AD-CENSUS algorithm
We can notice how the amount of training data  slightly changes the effectiveness of the methods based on  random forest (less than 0.0N ∆k improvement), highlight- ing how the best AUC is obtained starting from N.N million  samples
Conversely, measures based on CNNs improve  their effectiveness by a significant margin only when trained  on a sufficiently larger amount of data, but such improvement almost saturates at N.N million samples
In particular,  we can observe how CCNN achieves the worst results when  trained with the smallest subset of images, resulting to be  the best measure with a larger training set (with a ∆k mar- gin of about 0.NN)
Excluding LEV and ENSr at N.NM, all  the measures show a monotonic improvement in terms of  AUC by increasing the number of samples
 The second issue evaluated concerns how much a conNNNN    Figure N
Experimental results on MNN
Ratio between the average  AUC achieved by each confidence measure, trained on KNN (blue)  and MNN (orange), and the optimal AUC evaluated on the rest of  MNN with AD-CENSUS algorithm
 fidence measure can generalize across different environments/scenes (i.e., datasets)
To quantitatively evaluate  this behavior, we trained with AD-CENSUS the confidence  measures on a subset of MNN, processing an almost equivalent amount of training samples with respect to the training configuration adopted in Section N.N
Then, we compared the results achieved with this configuration to the one  used in Section N.N with AD-CENSUS on the remaining  data from MNN, computing ∆k as defined in Equation N
A confidence measure achieving similar ∆k in the two con- figuration is able to generalize well between the two very  different scenarios
Figure N plots the two values for the six  confidence measures
We can clearly notice how measures  based on CNNs better generalize with respect to random  forest approaches, with CCNN being more effective in this  sense than PBCP
Moreover, ON appears to better adapt to  different data, achieving a lower margin between the two  ∆k with respect to ENSr, GCP and LEV
This experiment highlights once again that confidence measures using as input cue the disparity map(s) (i.e., CCNN, PBCP and ON)  seem less prone to under-fitting
 N.N
Improvements to stereo accuracy  The final issue we investigated is the impact of confidence measures on stereo accuracy, a topic that recently  gained a lot of attention (e.g., [NN, NN, NN, N0])
For this  evaluation we choose the cost modulation proposed by Park  and Yoon [NN]
The reason is that differently from [NN],  which is specific for SGM algorithm, and [NN, N0], based on  parameters potentially different from measure to measure,  [NN] is suited for any stereo algorithm and parameter-free
 SGM was tuned as reported in Section N.N
We plugged in  [NN] the machine learning based measures, as well as three  standalone measures (i.e., APKR, SAMM and DANN)
On  the three datasets KNN, KNN and MNN, from Table N we can  notice that confidence measures based on machine learnKNN KNN MNN  badN avg badN avg badN avg  SGM NN.NN N.N0 NN.NN N.NN NN.NN N.NN  APKRNN NN.NN N0 N.N0N0 N.NNN0 N.NNN0 NN.NNN N.NNN0  SAMM N0.NNN N.NNN N.NNN N.NNN NN.0NN0 N.NNN  DANN NN.NN N N.N0N N.N0N N.NNN NN.NNN N.N0N  ENSc N0.NN N N.NNN N.0NN N.NNN NN.NNN N.00N  ENSr N0.NN N N.NNN N.0NN N.NNN NN.NNN N.NNN  GCP NN.0NN N.NNN N.NNN N.NNN NN.NNN N.NNN  LEV N0.NNN N.NNN N.NNN N.NNN NN.NNN N.NNN  ON N0.NNN N.NNN N.NNN N.NNN NN.NNN N.0NN  PBCP N0.NNN N.N0N N.NNN N.NNN NN.NNN N.NNN  CCNN N0.NNN N.NNN N.NNN N.N0N NN.NNN N.NNN  Table N
Error rate (percentage) and average pixel error on the  three datasets achieved by vanilla SGM (first row) and the confidence modulation proposed in [NN] plugging: APKRNN, SAMM,  DANN, ENSc, ENSr , GCP, LEV (the one proposed in [NN]), ON,  PBCP and CCNN
Learning-based confidence measures trained,  with AD-CENSUS, on the first N0 images of KNN
 ing are overall more effective than other ones
In particular, ON achieves the lowest error rate with KNN and CCNN  and PBCP outperforms other ones in KNN and MNN
This  experiment highlights that there is not a direct relationship  with the effectiveness of the confidence measure in terms of  AUC
However, most effective confidence measures (i.e.,,  CCNN, PBCP and ON) according to this metric achieve  the best results
Finally we point out that in this experiments, ENSc and ENSr, frequently perform better than others confidence measures, conventional and learning-based  ones
Moreover, for their deployment in cost modulation  ENSc outperforms ENSr most of the times, conversely to  what observed in terms of AUC
 N
Conclusions  In this paper we have reviewed and evaluated state-ofthe-art confidence measures focusing our attention on recent  ones based on machine learning techniques
Our exhaustive evaluation, with three stereo algorithms and three large  and challenging datasets, clearly highlights that learningbased ones are much more effective than conventional approaches
In particular, those using as input cue the disparity maps achieve better results in terms of detection of  correct match, capability to adapt to new data and effectiveness to improve stereo accuracy
In such methods training  is certainly an additional issue but, as reported in our evaluation, the overall amount of training data required is limited  and best learning-based confidence measures much better  generalize to new data
 Acknowledgement  We gratefully acknowledge the support of NVIDIA Corporation with the donation of the Titan X Pascal GPU used  for this research
 NNNN    References  [N] Z
Chen, X
Sun, L
Wang, Y
Yu, and C
Huang
A deep  visual correspondence embedding model for stereo matching  costs
In Proceedings of the IEEE International Conference  on Computer Vision, pages NNN–NN0, N0NN
N  [N] L
Di Stefano, M
Marchionni, and S
Mattoccia
A fast areabased stereo matching algorithm
Image and vision computing, NN(NN):NNN–N00N, N00N
N, N  [N] G
Egnal, M
Mintz, and R
P
Wildes
A stereo confidence  metric using single view imagery
In PROC
VISION INTERFACE, pages NNN–NN0, N00N
N  [N] F
Garcia, B
Mirbach, B
E
Ottersten, F
Grandidier, and  I
Cuesta-Contreras
Pixel weighted average strategy for  depth sensor data fusion
In ICIP, pages NN0N–NN0N
IEEE,  N0N0
N  [N] A
Geiger, P
Lenz, C
Stiller, and R
Urtasun
Vision meets  robotics: The kitti dataset
Int
J
Rob
Res., NN(NN):NNNN–  NNNN, sep N0NN
N, N  [N] R
Gherardi
Confidence-based cost modulation for stereo  matching
In Pattern Recognition, N00N
ICPR N00N
NNth  International Conference on, pages N–N, Dec N00N
N  [N] R
Haeusler and R
Klette
Evaluation of stereo confidence  measures on synthetic and recorded image data
In N0NN International Conference on Informatics, Electronics and Vision, ICIEV N0NN, pages NNN–NNN, N0NN
N, N  [N] R
Haeusler, R
Nair, and D
Kondermann
Ensemble learning for confidence measures in stereo vision
In CVPR
Proceedings, pages N0N–NNN, N0NN
N
N, N, N, N, N  [N] H
Hirschmuller
Stereo processing by semiglobal matching and mutual information
IEEE Transactions on Pattern  Analysis and Machine Intelligence (PAMI), N0(N):NNN–NNN,  feb N00N
N, N, N, N, N  [N0] H
Hirschmüller, P
R
Innocent, and J
Garibaldi
Real-time  correlation-based stereo vision with reduced border errors
 Int
J
Comput
Vision, NN(N-N), apr N00N
N  [NN] H
Hirschmuller, M
Buder, and I
Ernst
Memory efficient  semi-global matching
In ISPRS Annals of the Photogrammetry, Remote Sensing and Spatial Information Sciences,  pages NNN–NNN, N0NN
N  [NN] H
Hirschmller
Evaluation of cost functions for stereo  matching
In IEEE Computer Society Conference on Computer Vision and Pattern Recognition, N00N
N  [NN] X
Hu and P
Mordohai
A quantitative evaluation of confidence measures for stereo vision
IEEE Transactions on Pattern Analysis and Machine Intelligence (PAMI), pages NNNN–  NNNN, N0NN
N, N, N, N, N  [NN] S
Kim, D
g
Yoo, and Y
H
Kim
Stereo confidence metrics using the costs of surrounding pixels
In N0NN NNth International Conference on Digital Signal Processing, pages  NN–N0N, Aug N0NN
N, N  [NN] S
Kim, C
Y
Jang, and Y
H
Kim
Weighted peak ratio for  estimating stereo confidence level using color similarity
In  N0NN IEEE Asia Pacific Conference on Circuits and Systems  (APCCAS), pages NNN–NNN, Oct N0NN
N, N  [NN] D
Kong and H
Tao
A method for learning matching errors  in stereo computation
In British Machine Vision Conference  (BMVC), N00N N00N
N  [NN] S
Lefebvre, S
Ambellouis, and F
Cabestaing
A  colour correlation-based stereo matching using ND windows
In IEEE, editor, Third International IEEE Conference on Signal-Image Technologies and Internet-Based System, SITIS’0N, pages N0N–NN0, Shanghai, China, Dec N00N
 IEEE
N  [NN] W
Luo, A
G
Schwing, and R
Urtasun
Efficient Deep  Learning for Stereo Matching
In Proc
CVPR, N0NN
N  [NN] R
Manduchi and C
Tomasi
Distinctiveness maps for image  matching
In Image Analysis and Processing, NNNN
Proceedings
International Conference on, pages NN–NN
IEEE, NNNN
 N  [N0] G
Marin, P
Zanuttigh, and S
Mattoccia
Reliable fusion of  tof and stereo depth driven by confidence measures
In NNth  European Conference on Computer Vision (ECCV N0NN),  pages NNN–N0N, N0NN
N, N  [NN] L
Matthies
Stereo vision for planetary rovers: Stochastic  modeling to near real-time implementation
Int
J
Comput
 Vision, N(N), jul NNNN
N  [NN] N
Mayer, E
Ilg, P
Häusser, P
Fischer, D
Cremers,  A
Dosovitskiy, and T
Brox
A large dataset to train convolutional networks for disparity, optical flow, and scene flow  estimation
In The IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), N0NN
N, N  [NN] M
Menze and A
Geiger
Object scene flow for autonomous  vehicles
In Conference on Computer Vision and Pattern  Recognition (CVPR), N0NN
N, N  [NN] P
Merrell, A
Akbarzadeh, L
Wang, P
Mordohai, J
M
 Frahm, R
Yang, D
Nister, and M
Pollefeys
Real-time  visibility-based fusion of depth maps
In N00N IEEE NNth International Conference on Computer Vision, pages N–N, Oct  N00N
N, N  [NN] D
B
Min and K
Sohn
An asymmetric post-processing  for correspondence problem
Sig
Proc.: Image Comm.,  NN(N):NN0–NNN, N0N0
N, N  [NN] P
Mordohai
The self-aware matching measure for stereo
In  The International Conference on Computer Vision (ICCV),  pages NNNN–NNNN
IEEE, N00N
N, N  [NN] C
Mostegel, M
Rumpler, F
Fraundorfer, and H
Bischof
 Using self-contradiction to learn confidence measures in  stereo vision
In The IEEE Conference on Computer Vision  and Pattern Recognition (CVPR), N0NN
N, N  [NN] M
G
Park and K
J
Yoon
Leveraging stereo matching with  learning-based confidence measures
In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  June N0NN
N, N, N, N, N, N, N  [NN] D
Pfeiffer, S
Gehrig, and N
Schneider
Exploiting the  power of stereo confidences
In IEEE Computer Vision and  Pattern Recognition, pages NNN–N0N, Portland, OR, USA,  June N0NN
N  [N0] M
Poggi and S
Mattoccia
Deep stereo fusion: combining  multiple disparity hypotheses with deep-learning
In Proceedings of the Nth International Conference on ND Vision,  NDV, N0NN
N  [NN] M
Poggi and S
Mattoccia
Learning a general-purpose confidence measure based on o(N) features and a smarter aggregation strategy for semi global matching
In Proceedings of  NNNN    the Nth International Conference on ND Vision, NDV, N0NN
 N, N, N, N, N, N  [NN] M
Poggi and S
Mattoccia
Learning from scratch a confidence measure
In Proceedings of the NNth British Conference on Machine Vision, BMVC, N0NN
N, N, N, N, N  [NN] M
Poggi and S
Mattoccia
Learning to predict stereo reliability enforcing local consistency of confidence maps
In The  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), July N0NN
N  [NN] M
Poggi, F
Tosi, and S
Mattoccia
Efficient confidence  measures for embedded stereo
In NNth International Conference on Image Analysis and Processing (ICIAP N0NN),  September N0NN
N  [NN] M
Poggi, F
Tosi, and S
Mattoccia
Even more confident predictions with deep machine-learning
In NNth IEEE  Embedded Vision Workshop (EVWN0NN) held in conjunction with IEEE Conference on Computer Vision and Pattern  Recognition (CVPR), July N0NN
N  [NN] N
Sabater, A
Almansa, and J
M
Morel
Meaningful  Matches in Stereovision
IEEE Transactions on Pattern  Analysis and Machine Intelligence (PAMI), NN(N):NN0–NN,  dec N0NN
N  [NN] D
Scharstein, H
Hirschmller, Y
Kitajima, G
Krathwohl,  N
Nesic, X
Wang, and P
Westling
High-resolution stereo  datasets with subpixel-accurate ground truth
In GCPR,  pages NN–NN
N, N  [NN] D
Scharstein and R
Szeliski
Stereo matching with nonlinear diffusion
International Journal of Computer Vision,  NN:NNN–NNN, NNNN
N  [NN] D
Scharstein and R
Szeliski
A taxonomy and evaluation  of dense two-frame stereo correspondence algorithms
Int
J
 Comput
Vision, NN(N-N):N–NN, apr N00N
N, N  [N0] A
Seki and M
Pollefeys
Patch based confidence prediction  for dense disparity map
In British Machine Vision Conference (BMVC), N0NN
N, N, N, N, N, N  [NN] A
Spyropoulos, N
Komodakis, and P
Mordohai
Learning  to detect ground control points for improving the accuracy  of stereo matching
In The IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), pages NNNN–NNNN
 IEEE, N0NN
N, N, N, N, N, N  [NN] C
Strecha, W
von Hansen, L
J
V
Gool, P
Fua, and  U
Thoennessen
On benchmarking camera calibration and  multi-view stereo for high resolution imagery
In N00N  IEEE Computer Society Conference on Computer Vision and  Pattern Recognition (CVPR), NN-NN June N00N, Anchorage,  Alaska, USA, N00N
N  [NN] F
Tosi, M
Poggi, A
Tonioni, L
Di Stefano, and S
Mattoccia
Learning confidence measures in the wild
In NNth  British Machine Vision Conference (BMVC N0NN), September N0NN
N, N  [NN] A
Wedel, A
Meiner, C
Rabe, U
Franke, and D
Cremers
 Detection and Segmentation of Independently Moving Objects from Dense Scene Flow
In Proceedings of the Nth  International Conference on Energy Minimization Methods  in Computer Vision and Pattern Recognition, pages NN–NN,  Bonn, Germany, August N00N
Springer
N  [NN] K.-J
Yoon and I.-S
Kweon
Distinctive similarity measure  for stereo matching under point ambiguity
Computer Vision  and Image Understanding, NNN(N):NNN–NNN, N00N
N  [NN] R
Zabih and J
Woodfill
Non-parametric local transforms  for computing visual correspondence
In Proceedings of  the Third European Conference on Computer Vision (Vol
 II), ECCV ’NN, pages NNN–NNN, Secaucus, NJ, USA, NNNN
 Springer-Verlag New York, Inc
N  [NN] J
Zbontar and Y
LeCun
Stereo matching by training a convolutional neural network to compare image patches
Journal of Machine Learning Research, NN:N–NN, N0NN
N, N, N  NNNNEnd-To-End Face Detection and Cast Grouping in Movies Using Erdos-Renyi Clustering   End-to-end Face Detection and Cast Grouping in Movies  Using Erdős-Rényi Clustering  SouYoung JinN, Hang SuN, Chris StaufferN, and Erik Learned-MillerN  NCollege of Information and Computer Sciences, University of Massachusetts, Amherst NVisionary Systems and Research (VSR)  Figure N: Clustering results from Hannah and Her Sisters
Each unique color shows a particular cluster
It can be seen  that most individuals appear with a consistent color, indicating successful clustering
 Abstract We present an end-to-end system for detecting and clustering faces by identity in full-length movies
Unlike works  that start with a predefined set of detected faces, we consider the end-to-end problem of detection and clustering  together
We make three separate contributions
First,  we combine a state-of-the-art face detector with a generic  tracker to extract high quality face tracklets
We then introduce a novel clustering method, motivated by the classic  graph theory results of Erdős and Rényi
It is based on the  observations that large clusters can be fully connected by  joining just a small fraction of their point pairs, while just  a single connection between two different people can lead  to poor clustering results
This suggests clustering using a  verification system with very few false positives but perhaps  moderate recall
We introduce a novel verification method,  rank-N counts verification, that has this property, and use  it in a link-based clustering scheme
Finally, we define a  novel end-to-end detection and clustering evaluation metric  allowing us to assess the accuracy of the entire end-to-end  system
We present state-of-the-art results on multiple video  data sets and also on standard face databases
 Project page: http://souyoungjin.com/erclustering  This research is based in part upon work supported by the Office of the  Director of National Intelligence (ODNI), Intelligence Advanced Research  Projects Activity (IARPA) under contract number N0NN-NN0NNN000N0
 The views and conclusions contained herein are those of the authors and  should not be interpreted as necessarily representing the official policies or  endorsements, either expressed or implied, of ODNI, IARPA, or the U.S
 Government
The U.S
Government is authorized to reproduce and distribute reprints for Governmental purpose notwithstanding any copyright  annotation thereon
 N
Introduction  The problem of identifying face images in video and  clustering them together by identity is a natural precursor to  high impact applications such as video understanding and  analysis
This general problem area was popularized in the  paper “Hello! My name is...Buffy” [N], which used text  captions and face analysis to name people in each frame of  a full-length video
In this work, we use only raw video  (with no captions), and group faces by identity rather than  naming the characters
In addition, unlike face clustering  methods that start with detected faces, we include detection  as part of the problem
This means we must deal with false  positives and false negatives, both algorithmically, and in  our evaluation method
We make three contributions:  • A new approach to combining high-quality face de- tection [NN] and generic tracking [NN] to improve both  precision and recall of our video face detection
 • A new method, Erdős-Rényi clustering, for large-scale clustering of images and video tracklets
We argue  that effective large-scale face clustering requires face  verification with fewer false positives, and we introduce rank-N counts verification, showing that it indeed  achieves better true positive rates in low false positive  regimes
Rank-N counts verification, used with simple  link-based clustering, achieves high quality clustering  results on three separate video data sets
 • A principled evaluation for the end-to-end problem of face detection and clustering in videos; until now there  has been no clear way to evaluate the quality of such an  NNNN    Figure N: Overview of approach
Given a movie, our approach generates tracklets (Sec
N) and then does Erdős-Rényi  Clustering and FAD verification between all tracklet pairs
(Sec
N) Our final output is detections with unique character Ids
 end-to-end system, but only to evaluate its individual  parts (detection and clustering)
 We structure the paper as follows
In Section N we discuss related work
In Section N, we describe the first phase  of our system, in which we use a face detector and generic  tracker to extract face tracklets
In Section N, we introduce  Erdős-Rényi clustering and rank-N counts verification
Sections N and N present experiments and discussions
 N
Related Work  In this section, we first discuss face tracking and then the  problem of naming TV (or movie) characters
We can divide the character-naming work into two categories: fully  unsupervised and with some supervision
We then discuss  prior work using reference images
Related work on clustering is covered in Section N.N
 Recent work on robust face tracking [NN, NN, NN] has  gradually expanded the length of face tracklets, starting  from face detection results
Ozerov et al
[NN] merge results from different detectors by clustering based on spatiotemporal similarity
Clusters are then merged, interpolated,  and smoothed for face tracklet creation
Similarly, Roth et  al
[NN] generate low-level tracklets by merging detection  results, form high-level tracklets by linking low-level tracklets, and apply the Hungarian algorithm to form even longer  tracklets
Tapaswi et al
[NN] improve on this [NN] by removing false positive tracklets
 With the development of multi-face tracking techniques,  the problem of naming TV charactersN has been also widely  studied [NN, NN, N, N, NN, N0, NN]
Given precomputed face  tracklets, the goal is to assign a name or an ID to a group  of face tracklets with the same identity
Wu et al
[NN, N0]  iteratively cluster face tracklets and link clusters into longer  tracks in a bootstrapping manner
Tapaswi et al
[NN] train  classifiers to find thresholds for joining tracklets in two  stages: within a scene and across scenes
Similarly, we aim  to generate face clusters in a fully unsupervised manner
 NAnother related problem is person re-identification [NN, NN, N] in  which the goal is to tell whether a person of interest seen in one camera has been observed by another camera
Re-identification typically uses  the whole body on short time scales while naming TV characters focuses  on faces, but over a longer period of time
 Though solving this problem may yield a better result  for face tracking, some forms of supervision specific to  the video or characters in the test data can improve performance
Tapaswi et al
[NN] perform face recognition, clothing clustering and speaker identification, where face models  and speaker models are first trained on other videos containing the same main characters as in the test set
In [N, N],  subtitles and transcripts are used to obtain weak labels for  face tracks
More recently, Haurilet et al
[NN] solve the  problem without transcripts by resolving name references  only in subtitles
Our approach is more broadly applicable  because it does not use subtitles, transcripts, or any other  supervision related to the identities in the test data, unlike  these other works [NN, NN, N, N]
 As in the proposed verification system, some existing  work [N, NN] uses reference images
For example, index  code methods [NN] map each single image to a code based  upon a set of reference images, and then compare these  codes
On the other hand, our method compares the relative distance of two images with the distance of one of  the images to the reference set, which is different
In addition, we use the newly defined rank-N counts, rather than  traditional Euclidean or Mahalanobis distance measures to  compare images [N, NN] for similarity measures
 N
Detection and tracking  Our goal is to take raw videos, with no captions or annotations, and to detect all faces and cluster them by identity
We start by describing our method for generating face  tracklets, or continuous sequences of the same face across  video frames
We wish to generate clean face tracklets that  contain face detections from just a single identity
Ideally,  exactly one tracklet should be generated for an identity from  the moment his/her face appears in a shot until the moment  it disappears or is completely occluded
 To achieve this, we first detect faces in each video frame  using the Faster R-CNN object detector [NN], but retrained  on the WIDER face data set [NN], as described by Jiang et  al
[NN]
Even with this advanced detector, face detection  sometimes fails under challenging illumination or pose
In  videos, those faces can be detected before or after the chalNNNN    lenging circumstances by using a tracker that tracks both  forward and backward in time
We use the distribution  field tracker [NN], a general object tracker that is not trained  specifically for faces
Unlike face detectors, the tracker’s  goal is to find in the next frame the object most similar to  the target in the current frame
The extra faces found by the  tracker compensate for missed detections (Fig
N, bottom  of block N)
Tracking helps not only to catch false negatives, but also to link faces of equivalent identity in different  frames
 One simple approach to combining a detector and tracker  is to run a tracker forward and backward in time from every single face detection for some fixed number of frames,  producing a large number of “mini-tracks”
A Viterbi-style  algorithm [N0, N] can then be used to combine these minitracks into longer sequences
This approach is computationally expensive since the tracker is run many times on overlapping subsequences, producing heavily redundant minitracks
To improve performance, we developed the following novel method for combining a detector and tracker
 Happily, it also improves precision and recall, since it takes  advantage of the tracker’s ability to form long face tracks of  a single identity
 The method starts by running the face detector in each  frame
When a face is first detected, a tracker is initialized  with that face
In subsequent frames, faces are again detected
In addition, we examine each current tracklet to see  where it might be extended by the tracking algorithm in the  current frame
We then check the agreement between detection and tracking results
We use the intersection over  union (IoU) between detections and tracking results with  threshold 0.N, and apply the Hungarian algorithm[NN] to establish correspondences among multiple matches
If a detection matches a tracking result, the detection is stored in  the current face sequence such that the tracker can search  in the next frame given the detection result
For the detections that have no matched tracking result, a new tracklet  is initiated
If there are tracking results that have no associated detections, it means that either a) the tracker could  not find an appropriate area on the current frame, or b) the  tracking result is correct while the detector failed to find  the face
The algorithm postpones its decision about the  tracked region for the next α consecutive frames (α = N0)
If the face sequence has any matches with detections within  α frames, the algorithm will keep the tracking results
Otherwise, it will remove the tracking-only results
The second block of Fig
N summarizes our proposed face tracklet  generation algorithm and shows examples corrected by our  joint detection-tracking strategy
Next, we describe our approach to clustering based on low false positive verification
 N
Erdős-Rényi Clustering and Rank-N Counts Verification  In this section, we describe our approach to clustering  face images, or, in the case of videos, face tracklets
We  adopt the basic paradigm of linkage clustering, in which  each pair of points (either images or tracklets) is evaluated  for linking, and then clusters are formed among all points  connected by linked face pairs
We name our general approach to clustering Erdős-Rényi clustering since it is inspired by classic results in graph theory due to Erdős and  Rényi [N], as described next
 Consider a graph G with n vertices and probability p of  each possible edge being present
This is the Erdős-Rényi  random graph model [N]
The expected number of edges  is (  n N  )  p
One of the central results of this work is that, for  ǫ > 0 and n sufficiently large, if  p > (N + ǫ) lnn  n , (N)  then the graph will almost surely be connected (there exists a path from each vertex to every other vertex)
Fig
N  shows this effect on different graph sizes, obtained through  simulation
 0 0.N 0.N 0.N 0.N 0.N  Probability p of each edge  0  0.N  N  P ro  b (c  o n n e c te  d ) N=NNN  N=NNN  N=NNN  N=NN  N=NN  N=NN  Figure N: Simulation of cluster connectedness as a function  of cluster size, N , and the probability p of connecting point  pairs
The figure shows that for various N (different colored  lines), the probability that the cluster is fully connected (on  the y-axis) goes up as more pairs are connected
For larger  graphs, a small probability of connected pairs still leads to  high probability that the graph will be fully connected
 Consider a clustering system in which links are made  between tracklets by a verifier (a face verification system),  whose job is to say whether a pair of tracklets is the “same”  person or two “different” people
While graphs obtained  in clustering problems are not uniformly random graphs,  the results of Erdős and Rényi suggest that this verifier can  have a fairly low recall (percentage of same links that are  connected) and still do a good job connecting large clusters
In addition, false matches may connect large clusters  of different identities, dramatically hurting clustering performance
This motivates us to build a verifier that focuses  on low false positives rather than high recall
In the next  section, we present our approach to building a verifier that  is designed to have good recall at low false positive rates,  NNNN    and hence is appropriate for clustering problems with large  clusters, like grouping cast members in movies
 N.N
Rank-N counts for fewer false positives  Our method compares images by comparing their multidimensional feature vectors
More specifically, we count  the number of feature dimensions in which the two images  are closer in value than the first image is to any of a set  of reference images
We call this number the rank-N count  similarity
Intuitively, two images whose feature values are  “very close” for many different dimensions are more likely  to be the same person
Here, an image is considered “very  close” to a second image in one dimension if it is closer to  the second image in that dimension than to any of the reference images
 More formally, to compare two images IA and IB , our  first step is to obtain feature vectors A and B for these images
We extract N0NN-D feature vectors from the fcN layer  of a standard pre-trained face recognition CNN [NN]
In  addition to these two images, we use a fixed reference set  with G images (we typically set G = N0), and compute CNN feature vectors for each of these reference images.N  Let the CNN feature vectors for the reference images be  RN, RN, ..., RG
We sample reference images from the TV  Human Interactions Dataset [NN], since these are likely to  have a similar distribution to the images we want to cluster
 For each feature dimension i (of the N0NN), we ask  whether  |Ai −Bi| < min j  |Ai −R j i |
 That is, is the value in dimension i closer between A and B  than between A and all the reference images? If so, then we  say that the ith feature dimension is rank-N between A and  B
The cumulative rank-N counts feature R is simply the  number of rank-N counts across all N0NN features:  R = N0NN ∑  i=N  I  [  |Ai −Bi| < min j  |Ai −R j i |  ]  ,  where I[·] is an indicator function which is N if the expres- sion is true and 0 otherwise
 Taking inspiration from Barlow’s notion that the brain  takes special note of “suspicious coincidences” [N], each  rank-N feature dimension can be considered a suspicious  coincidence
It provides some weak evidence that A and  B may be two images of the same person
On the other  hand, in comparing all N0NN feature dimensions, we expect  to obtain quite a large number of rank-N feature dimensions  even if A and B are not the same person
 When two images and the reference set are selected randomly from a large distribution of faces (in this case they  NThe reference images may overlap in identity with the clustering set,  but we choose reference images so that there is no more than one occurrence of each person in the reference set
 are usually different people), the probability that A is closer  to B in a particular feature dimension than to any of the  reference images is just  N  G+ N 
 Repeating this process N0NN times means that the expected  number of rank-N counts is simply  E[R] = N0NN  G+ N ,  since expectations are linear (even in the presence of statistical dependencies among the feature dimensions)
Note that  this calculation is a fairly tight upper bound on the expected  number of rank-N features conditioned on the images being  of different identities, since most pairs of images in large  clustering problems are different, and conditioning on ”different” will tend reduce the expected rank-N count
Now if  two images IA and IB have a large rank-N count, it is likely  they represent the same person
The key question is how to  set the threshold on these counts to obtain the best verification performance
 Recall that our goal, as guided by the Erdős-Rényi random graph model, is to find a threshold on the rank-N  counts R so that we obtain very few false positives (declaring two different faces to be “same”) while still achieving  good recall (a large number of same faces declared to be  “same”)
Fig
N shows distributions of rank-N counts for various subsets of image pairs from Labeled Faces in the Wild  (LFW) [NN]
The red curve shows the distribution of rank-N  counts for mismatched pairs from all possible mismatched  pairs in the entire data set (not just the test sets)
Notice  that the mean is exactly where we would expect with a  gallery size of N0, at N0NN NN  ≈ N0
The green curve shows the distribution of rank-N counts for the matched pairs, which  is clearly much higher
The challenge for clustering, of  course, is that we don’t have access to these distributions  since we don’t know which pairs are matched and which are  not
The yellow curve shows the rank-N counts for all pairs  of images in LFW, which is nearly identical to the distribution of mismatched rank-N counts, since the vast majority  of possibe pairs in all of LFW are mismatched
This is the  distribution to which the clustering algorithm has access
 If the N,0NN CNN features were statistically independent (but not identically distributed), then the distribution  of rank-N counts would be a binomial distribution (blue  curve)
In this case, it would be easy to set a threshold on  the rank-N counts to guarantee a small number of false positives, by simply setting the threshold to be near the right end  of the mismatched (red) distribution
However, the dependencies among the CNN features prevent the mismatched  rank-N counts distribution from being binomial, and so this  approach is not possible
 NNNN    Figure N: LFW distribution of rank-N counts
Each distribution is normalized to sum to N
 Table N: Verification performance comparisons on all possible LFW pairs
The proposed rank-N counts gets much  higher recall at fixed FPRs
 FPR R a  n k  N co  u n  t  L N  T em  p la  te  A d  ap ta  ti o  n [N  ]  R an  k -O  rd er  D is  ta n  ce [ N  N ]  NE-N 0.0NNN 0.00NN 0.00NN 0.00NN  NE-N 0.0NNN 0.00NN 0.00NN 0.00NN  NE-N 0.0NNN 0.0NN0 0.00NN 0.00NN  NE-N 0.NNNN 0.NNNN 0.0NNN 0.00NN  NE-N 0.NN00 0.NNNN 0.0NNN 0.0NNN  NE-N 0.N0NN 0.NN00 0.NNNN 0.NNNN  NE-N 0.NNNN 0.NNNN 0.NNNN 0.NNNN  NE-N 0.NNN0 0.NNNN 0.NN0N 0.NNNN  NE-N 0.NNNN 0.NNNN 0.NNNN 0.NNNN  N.N
Automatic determination of rank-N count threshold  Ideally, if we could obtain the rank-N count distribution  of mismatched pairs of a test set, we could set the threshold  such that the number of false positives becomes very low
 However, it is not clear how to get the actual distribution of  rank-N counts for mismatched pairs at test time
 Instead, we can estimate the shape of the mismatched  pair rank-N count distribution using one distribution (LFW),  and use it to estimate the distribution of mismatched rank-N  counts for the test distribution
We do this by fitting the left  half of the LFW distribution to the left half of the clustering  distribution using scale and location parameters
The reason we use the left half to fit the distribution is that this part  of the rank-N counts distribution is almost exclusively influence by mismatched pairs
The right side of this matched  distribution then gives us an approximate way to threshold  the test distribution to obtain a certain false positive rate
It  is this method that we use to report the results in the leftmost column of Table N
 A key property of our rank-N counts verifier is that it  has good recall across a wide range of the low false positive regime
Thus, our method is relatively robust to the  setting of the rank-N counts threshold
In order to show  that our rank-N counts feature has good performance for the  types of verification problems used in clustering, we construct a verification problem using all possible pairs of the  LFW database [NN]
In this case, the number of mismatched  pairs (quadratic in N ) is much greater than the number of  matched pairs
As shown in Table N, we observe that our  verifier has higher recall than three competing methods (all  of which use the same base CNN representation) at low  false positive rates
 Using rank-N counts verification for tracklet clustering
In our face clustering application, we consider every  pair (I, J) of tracklets, calculate a value akin to the rank-N count R, and join the tracklets if the threshold is exceeded
 In order to calculate an R value for tracklets, we sample a  random subset of N0 face images from each tracklet, compute a rank-N count R for each pair of images, and take the  maximum of the resulting R values
 N.N
Averaging over gallery sets  While our basic algorithm uses a fixed (but randomly selected) reference gallery, the method is susceptible to the  case in which one of the gallery images happens to be similar in appearance to a person with a large cluster, resulting  in a large number of false negatives
To mitigate this effect,  we implicitly average the rank-N counts over an exponential  number of random galleries, as follows
 The idea is to sample random galleries of size g from a  larger super-gallery with G images; we used g = N0, G = N000
We are interested rank-N counts, in which image A’s feature is closer to B than to any of the gallery of size g
 Suppose we know that among the N000 super-gallery images, there are K (e.g., K = N) that are closer to A than B is
The probability that a random selection (with replacement) of g images from the super-gallery would contain  none of the K closer images (and hence represent a rank-N  count) is  r(A,B) =  (  N.0− K  G  )g  
 That is, r(A,B) is the probability of having a rank-N count with a random gallery, and using r(A,B) as the count is equivalent to averaging over all possible random galleries
 In our final algorithm, we sum these probabilities rather  than the deterministic rank-N counts
 N.N
Efficient implementation  For simplicity, we discuss the computational complexity  of our fixed gallery algorithm; the complexity of the average  gallery algorithm is similar
With F , G, and N indicating  the feature dimensionality, number of gallery images, and  NNN0    number of face tracklets to be clustered, the time complexity of the naive rank-N count algorithm is O(F ∗G ∗NN)
However, for each feature dimension, we can sort N test  image feature values and G gallery image feature values in  time O((N +G) log(N +G))
Then, for each value in test image A, we find the closest gallery value, and increment  the rank-N count for the test images that are closer to A
Let  Y be the average number of steps to find the closest gallery  value
This is typically much smaller than N 
The time  complexity is then O(F ∗ [(N +G) log(N +G)+N ∗Y ])
 N.N
Clustering with do-not-link constraints  It is common in clustering applications to incorporate  constraints such as do-not-link or must-link, which specify  that certain pairs should be in separate clusters or the same  cluster, respectively [NN, NN, NN, NN, NN]
They are also often  seen in the face clustering literature [N, NN, N0, NN, NN, NN]
 These constraints can be either rigid, implying they must be  enforced [NN, NN, NN, NN], or soft, meaning that violations  cause an increase in the loss function, but those violations  may be tolerated if other considerations are more important  in reducing the loss [NN, NN, NN, N0, NN]
 In this work, we assume that if two faces appear in the  same frame, they must be from different people, and hence  their face images obey a do-not-link constraint
Furthermore, we extend this hard constraint to the tracklets that  contain faces
If two tracklets have any overlap in time,  then the entire tracklets represent a do-not-link constraint
 We enforce these constraints on our clustering procedure
Note that connecting all pairs below a certain dissimilarity threshold followed by transitive closure is equivalent to single-linkage agglomerative clustering with a joining threshold
In agglomerative clustering, a pair of closest  clusters is found and joined at each iteration until there is a  single cluster left or a threshold met
A naı̈ve implementation will simply search and update the dissimilarity matrix  at each iteration, making the whole process O(nN) in time
There are faster algorithms giving the optimal time complexity O(nN) for single-linkage clustering [NN, NN]
Many of these algorithms incur a dissimilarity update at each iteration, i.e
update d(i, k) = min(d(i, k), d(j, k)) after com- bining cluster i and j (and using i as the cluster id of the  resulting cluster)
If the pairs with do-not-link constraints  are initialized with +∞ dissimilarity, the aforementioned update rule can be modified to incorporate the constraints  without affecting the time and space complexity:  d(i, k) =        min(d(i, k), d(j, k)) d(i, k) N= +∞ AND d(j, k) N= +∞  +∞ otherwise  N
Experiments  We evaluate our proposed approach on three video data  sets: the Big Bang Theory (BBT) Season N (s0N), Episodes  (a) Rank-N Count (b) Rank-Order Distance [NN]  Figure N: Visualization of the combined detection and clustering metric for the first few minutes of the Hannah set
 N-N (e0N-e0N) [N], Buffy the Vampire Slayer (Buffy) Season N (s0N), Episodes N-N (e0N-e0N) [N], and Hannah and  Her Sisters (Hannah) [NN]
Each episode of the BBT and  Buffy data set contains N-N and NN-NN characters respectively, while Hannah has annotations for NNN characters.N  Buffy and Hannah have many occlusions which make the  face clustering problem more challenging
In addition to the  video data sets, we also evaluate our clustering algorithm on  LFW [NN] which contains NNN0 subjects.N  An end-to-end evaluation metric
There are many evaluation metrics used to independently evaluate detection,  tracking, and clustering
Previously, it has been difficult to  evaluate the relative performance of two end-to-end systems  because of the complex trade-offs between detection, tracking, and clustering performance
Some researchers have attempted to overcome this problem by providing a reference  set of detections with suggested metrics [N0], but this approach precludes optimizing complete system performance
 To support evaluation of the full video-to-identity pipeline,  in which false positives, false negatives, and clustering errors are handled in a common framework, we introduce unified pairwise precision (UPP) and unified pairwise recall  (UPR) as follows
 Given a set of annotations, {aN, aN, ..., aA} and detec- tions, {dN, dN, ..., dD}, we consider the union of three sets of tuples: false positives resulting from unannotated face  detections {di, ∅}; valid face detections {di, aj}; and false negatives resulting from unmatched annotations {∅, aj}
Fig
N visualizes every possible pair of tuples ordered by  false positives, valid detections, and false negatives for the  first few minutes of the Hannah data set
Further, groups of  tuples have been ordered by identity to show blocks of identity to aid our understanding of the visualization, although  the order is inconsequential for the numerical analysis
 In Fig N, the large blue region (and the regions it contains) represents all pairs of annotated detections, where  we have valid detections corresponding to their best annotation
In this region, white pairs are correctly clustered,  magenta pairs are the same individual but not clustered,  cyan pairs are clustered but not the same individual, and  NWe removed garbage classes such as ‘unknown’ or ‘false positive’
NAll known ground truth errors are removed
 NNNN    Table N: Clustering performance comparisons on various data sets
The leftmost shows our rankNcount by setting a threshold  automatically
For the rest of the columns, we show f-scores using optimal (oracle-supplied) thresholds
For BBT and Buffy,  we show average scores over six episodes
The full table with individual episode results is given in Supp
Mat
Best viewed  in color (Nst place, Nnd place, Nrd place)
 Verification system + Link-based clustering algorithm Other clustering algorithms  Test set R a  n k  -N C  o u  n t  (a u  to m  a ti  c th  re sh  o ld  )  R a  n k  -N C  o u  n t  L N  T em  p la  te  A d  ap ta  ti o  n [N  ]  R an  k -O  rd er  D is  ta n  ce [ N  N ]  R an  k -O  rd er  D is  ta n  ce  b as  ed C  lu st  er in  g [N  N ]  A ffi  n it  y  P ro  p ag  at io  n [N  N ]  D B  S C  A N  [ N ]  S p  ec tr  al  C lu  st er  in g  [ N N ]  B ir  ch [N  N ]  M in  iB at  ch  K M  ea n  s [ N  0 ]  Video  BBT s0N [N] .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN  Buffy s0N [N] .NNNN .NNNN .NNNN .NNNN .NNN0 .NNNN .NN0N .NN0N .NNNN .NNNN .NNNN  Hannah [NN] .NNNN .NNNN .NNNN .NNN0 .NNNN .NNNN .NNNN .NNN0 .NNNN .NNN0 .N0NN  Image LFW [NN] .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .NNNN .0NNN .NNNN .NNN0 .NNNN  blue pairs are not clustered pairs from different individuals
 The upper left portion of the matrix represents false positives with no corresponding annotation
The green pairs in  this region correspond to any false positive matching with  any valid detection
The lower right portion of the matrix  corresponds to the false negatives
The red pairs in this  region correspond to any missed clustered pairs resulting  from these missed detections
The ideal result would contain blue and white pairs, with no green, red, cyan, or magenta
 The unified pairwise precision (UPP) is the fraction of  pairs, {di, aj} within all clusters with matching identi- ties, i.e., the number of white pairs divided by the number of white, cyan, and green pairs
UPP decreases if: two  matched detections in a cluster do not correspond to the  same individual; if a matched detection is clustered with  a false positive; for each false positive regardless of its clustering; and for false positives clustered with valid detections
Similarly, the unified pairwise recall (UPR) is the  fraction of pairs within all identities that have been properly  clustered, i.e., the number of white pairs divided by number  of white, magenta, and red pairs
UPR decreases if: two  matched detections of the same identity are not clustered;  a matched detection should be matched but there is no corresponding detection; for each false negative; and for false  negative pairs that should be detected and clustered
The  only way to achieve perfect UPP and UPR is to detect every  face with no false positives and cluster all faces correctly
 At a glance, our visualization in Fig
N shows that our detection produces few false negatives, many more false positives, and is less aggressive in clustering
Using this unified  metric, others can tune their own detection, tracking, and  clustering algorithms to optimize the unified performance  metrics
Note that for image matching without any detection failures, the UPP and UPR reduce to standard pairwise  precision and pairwise recall
 The UPP and UPR can be summarized with a single Fmeasure (the weighted harmonic mean) providing a single,  unified performance measure for the entire process
It can  be α-weighted to alter the relative value of precision and  recall performance:  Fα = N  α UPP  + N−α UPR  (N)  where α ∈ [0, N]
α = 0.N denotes a balanced F-measure
 N.N
Threshold for rank-N counts  The leftmost column in Table N shows our clustering results when the threshold is set automatically by the validation set
We used LFW as a validation set for BBT, Buffy  and Hannah while Hannah was used for LFW
Note that the  proposed method is very competitive even when the threshold is automatically set
 N.N
Comparisons  We can divide other clustering algorithms into two broad  categories–link-based clustering algorithms (like ours) that  use a different similarity function and clustering algorithms  that are not link-based (such as spectral clustering [NN])
 Table N shows the comparisons to various distance functions [N, NN, NN] with our link-based clustering algorithm
 LN shows competitive performance in LFW while the performance drops dramatically when a test set has large pose  variations
We also compare against a recent so-called  “template adaptation” method [N] which also requires a reference set
It takes Nnd and Nrd place on Buffy and BBT
In  addition, we compare to the rank-order method [NN] in two  different ways: link-based clustering algorithms using their  rank-order distance, and rank-order distance based clustering
 In addition, we compare against several generic clustering algorithms (Affinity Propagation [NN], DBSCAN [N],  NNNN    (a) (b) (c) (d) (e)  (f) (g) (h) (i) (j)  Figure N: Clustering results from Buffy the Vampire Slayer
A failure example can be seen in frame (e), in which the main  character Buffy (otherwise in a purple box) in shown in a pink box
 (a) (b) (c) (d) (e)  (f) (g) (h) (i) (j)  Figure N: Clustering results from the Big Bang Theory
A failure example can be seen in frame (d), in which the main  character Howard (otherwise in a magenta box) in shown in a gray box
 Spectral Clustering [NN], Birch [NN], KMeans [N0]), where  LN distance is used as pairwise metric
For algorithms that  can take as input the similarity matrix (Affinity Propagation,  DBSCAN, Spectral Clustering), do-not-link constraints are  applied by setting the distance between the corresponding  pairs to ∞
Note that this is just an approximation, and in general does not guarantee the constraints in the final clustering result (e.g
for single-linkage agglomerative clustering, a modified update rule is also needed in Section N.N)
 Note that all other settings (feature encoding, tracklet  generation) are common for all methods
In Table N, except  for the leftmost column, we report the best F0.N scores using  optimal (oracle-supplied) thresholds for (number of clusters, distance)
The link-based clustering algorithm with  rank-N counts outperforms the state-of-the-art on all four  data sets in F0.N score
Figures N and N show some clustering results on Buffy and BBT
 N
Discussion  We have presented a system for doing end-to-end clustering in full length videos and movies
In addition to a  careful combination of detection and tracking, and a new  end-to-end evaluation metric, we have introduced a novel  approach to link-based clustering that we call Erdős-Rényi  clustering
We demonstrated a method for automatically estimating a good decision threshold for a verification method  based on rank-N counts by estimating the underlying portion  of the rank-N counts distribution due to mismatched pairs
 This decision threshold was shown to result in good recall at a low false-positive operating point
Such operating  points are critical for large clustering problems, since the  vast majority of pairs are from different clusters, and false  positive links that incorrectly join clusters can have a large  negative effect on clustering performance
 There are several things that could disrupt our algorithm:  a) if a high percentage of different pairs are highly similar  (e.g
family members), b) if only a small percentage of pairs  are different (e.g., one cluster contains N0% of the images),  and if same pairs lack lots of matching features (e.g., every cluster is a pair of images of the same person under  extremely different conditions)
Nevertheless, we showed  excellent results on N popular video data sets
Not only do  we dominate other methods when thresholds are optimized  for clustering, but we outperform other methods even when  our thresholds are picked automatically
 NNNN    References  [N] H
Barlow
Cerebral cortex as model builder
In Matters of  Intelligence, pages NNN–N0N
Springer, NNNN
N  [N] M
Bauml, M
Tapaswi, and R
Stiefelhagen
Semisupervised learning with constraints for person identification  in multimedia data
In Proc
CVPR, N0NN
N, N, N  [N] R
G
Cinbis, J
Verbeek, and C
Schmid
Unsupervised metric learning for face identification in TV video
In Proc
 ICCV, N0NN
N  [N] N
Crosswhite, J
Byrne, C
Stauffer, O
M
Parkhi, Q
Cao,  and A
Zisserman
Template adaptation for face verification  and identification
In Face and Gesture, N0NN
N, N, N  [N] S
J
Davey, M
G
Rutten, and B
Cheung
A comparison of  detection performance for several track-before-detect algorithms
EURASIP Journal on Advances in Signal Processing,  N00N:NN, N00N
N  [N] B
DeCann and A
Ross
Modeling errors in a biometric reidentification system
IET Biometrics, N(N):N0N–NNN, N0NN
 N  [N] P
Erdős and A
Rényi
On the evolution of random graphs
 Publications of the Mathematical Institute of the Hungarian  Academy of Sciences, N:NN–NN, NNN0
N  [N] M
Ester, H.-P
Kriegel, J
Sander, and X
Xu
A densitybased algorithm for discovering clusters in large spatial  databases with noise
KDD, NN(NN):NNN–NNN, NNNN
N  [N] M
Everingham, J
Sivic, and A
Zisserman
”Hello! My  name is..
Buffy” Automatic naming of characters in TV  video
In Proc
BMVC, N00N
N, N  [N0] G
D
Forney
The Viterbi algorithm
Proceedings of the  IEEE, NN(N):NNN–NNN, NNNN
N  [NN] B
J
Frey and D
Dueck
Clustering by passing messages  between data points
Science, NNN(NNNN):NNN–NNN, N00N
N  [NN] A
Gyaourova and A
Ross
Index codes for multibiometric  pattern retrieval
IEEE Transactions on Information Forensics and Security (TIFS), N(N):NNN–NNN, April N0NN
N  [NN] M.-L
Haurilet, M
Tapaswi, Z
Al-Halah, and R
Stiefelhagen
Naming TV characters by watching and analyzing dialogs
In Proc
CVPR, N0NN
N  [NN] G
B
Huang, M
Mattar, T
Berg, and E
Learned-Miller
Labeled faces in the wild: A database for studying face recognition in unconstrained environments
In The Workshop on  Faces in Real-Life Images at ECCV, N00N
N, N, N, N  [NN] H
Jiang and E
Learned-Miller
Face detection with the  Faster R-CNN
In Face and Gesture, N0NN
N, N  [NN] H
W
Kuhn
The hungarian method for the assignment problem
Naval research logistics quarterly, N(N-N):NN–NN, NNNN
 N  [NN] Z
Li, J
Liu, and X
Tang
Pairwise constraint propagation  by semidefinite programming for semi-supervised classification
In Proc
ICML, N00N
N  [NN] G
Lisanti, I
Masi, A
D
Bagdanov, and A
D
Bimbo
Person re-identification by iterative re-weighted sparse ranking
 TPAMI, NN(N):NNNN–NNNN, August N0NN
N  [NN] Z
Lu and T
K
Leen
Penalized probabilistic clustering
 Neural Computation, NN(N):NNNN–NNNN, N00N
N  [N0] A
Milan, L
Leal-Taixé, I
Reid, S
Roth, and K
Schindler
 MOTNN: A benchmark for multi-object tracking
 arXiv:NN0N.00NNN [cs], Mar
N0NN
arXiv: NN0N.00NNN
N  [NN] S
Miyamoto and A
Terami
Semi-supervised agglomerative  hierarchical clustering algorithms with pairwise constraints
 In Fuzzy Systems (FUZZ), pages N–N
IEEE, N0N0
N  [NN] F
Murtagh and P
Contreras
Algorithms for hierarchical  clustering: an overview
Wiley Interdisciplinary Reviews:  Data Mining and Knowledge Discovery, N(N):NN–NN, N0NN
 N  [NN] C
Otto, D
Wang, and A
K
Jain
Clustering millions of  faces by identity
TPAMI, Mar
N0NN
N  [NN] A
Ozerov, J.-R
Vigouroux, L
Chevallier, and P
Pérez
On  evaluating face tracks in movies
In Proc
ICIP, N0NN
N, N,  N  [NN] A
Ozerov, J.-R
Vigouroux, L
Chevallier, and P
Pérez
On  evaluating face tracks in movies
In Proc
ICIP
IEEE, N0NN
 N  [NN] O
M
Parkhi, A
Vedaldi, and A
Zisserman
Deep face  recognition
In bmvc, N0NN
N  [NN] A
Patron-Perez, M
Marszaek, A
Zisserman, and I
D
Reid
 High five: Recognising human interactions in tv shows
In  Proc
BMVC, N0N0
N  [NN] S
Ren, K
He, R
B
Girshick, and J
Sun
Faster R-CNN:  towards real-time object detection with region proposal networks
In Proc
NIPS, N0NN
N  [NN] M
Roth, M
Bauml, R
Nevatia, and R
Stiefelhagen
Robust  multi-pose face tracking by multi-stage tracklet association
 In Proc
ICPR, N0NN
N  [N0] D
Sculley
Web-scale k-means clustering
In Proc
WWW,  pages NNNN–NNNN
ACM, N0N0
N, N  [NN] L
Sevilla-Lara and E
Learned-Miller
Distribution fields for  tracking
In Proc
CVPR, N0NN
N, N  [NN] N
Shental, A
Bar-Hillel, T
Hertz, and D
Weinshall
Computing Gaussian mixture models with EM using equivalence  constraints
In Proc
NIPS, N00N
N  [NN] J
Shi and J
Malik
Normalized cuts and image segmentation
TPAMI, NN(N):NNN–N0N, N000
N, N  [NN] R
Sibson
SLINK: an optimally efficient algorithm for the  single-link cluster method
The computer journal, NN(N):N0–  NN, NNNN
N  [NN] M
Tapaswi, M
Bauml, and R
Stiefelhagen
”Knock!  Knock! Who is it?” Probabilistic person identification in TV  series
In Proc
CVPR, N0NN
N  [NN] M
Tapaswi, C
C
Corez, M
Bauml, H
K
Ekenel, and  R
Stiefelhagen
Cleaning up after a face tracker: False positive removal
In Proc
ICIP, N0NN
N  [NN] M
Tapaswi, O
M
Parkhi, E
Rahtu, E
Sommerlade,  R
Stiefelhagen, and A
Zisserman
Total cluster: A person  agnostic clustering method for broadcast videos
In ICVGIP,  N0NN
N, N  [NN] K
Wagstaff, C
Cardie, S
Rogers, S
Schrödl, et al
Constrained k-means clustering with background knowledge
In  Proc
ICML, N00N
N  [NN] B
Wu, S
Lyu, B.-G
Hu, and Q
Ji
Simultaneous clustering and tracklet linking for multi-face tracking in videos
In  Proc
ICCV, N0NN
N, N  [N0] B
Wu, Y
Zhang, B.-G
Hu, and Q
Ji
Constrained clustering and its application to face clustering in videos
In Proc
 CVPR, N0NN
N, N  NNNN    [NN] S
Yang, P
Luo, C
C
Loy, and X
Tang
Wider face: A face  detection benchmark
In CVPR, N0NN
N  [NN] T
Zhang, R
Ramakrishnan, and M
Livny
Birch: an efficient data clustering method for very large databases
In  SIGMOD
ACM, NNNN
N, N  [NN] Z
Zhang, P
Luo, C
C
Loy, and X
Tang
Joint face representation adaptation and clustering in videos
In Proc
 ECCV, N0NN
N  [NN] L
Zheng, Y
Yang, and A
G
Hauptman
Person reidentification: Past, present and future
arXiv, Oct
N0NN
 N  [NN] C
Zhu, F
Wen, and J
Sun
A rank-order distance based  clustering algorithm for face tagging
In Proc
CVPR, N0NN
 N, N, N  NNNNLearning From Video and Text via Large-Scale Discriminative Clustering   Learning from Video and Text via Large-Scale Discriminative Clustering  Antoine MiechN,N Jean-Baptiste AlayracN,N Piotr BojanowskiN Ivan Laptev N,N Josef SivicN,N,N  NÉcole Normale Supérieure NInria NCIIRC  Abstract  Discriminative clustering has been successfully applied to a number of weakly supervised learning tasks
 Such applications include person and action recognition,  text-to-video alignment, object co-segmentation and colocalization in videos and images
One drawback of discriminative clustering, however, is its limited scalability
 We address this issue and propose an online optimization  algorithm based on the Block-Coordinate Frank-Wolfe algorithm
We apply the proposed method to the problem  of weakly supervised learning of actions and actors from  movies together with corresponding movie scripts
The  scaling up of the learning problem to NN feature-length  movies enables us to significantly improve weakly supervised action recognition
 N
Introduction  Action recognition has been significantly improved in recent years
Most existing methods [NN, NN, NN, N0] rely on  supervised learning and, therefore, require large-scale, diverse and representative action datasets for training
Collecting such datasets, however, is a difficult task given the  high costs of manual search and annotation of the video
 Notably, the largest action datasets today are still orders  of magnitude smaller (UCFN0N [NN], ActivityNet [N]) compared to large image datasets, they often contain label noise  and target specific domains such as sports (SportsNM [N0])
 Weakly supervised learning aims to bypass the need  of manually-annotated datasets using readily-available, but  possibly noisy and incomplete supervision
Examples of  such methods include learning of person names from image captions or video scripts [N, N0, NN, NN]
Learning actions from movies and movie scripts has been approached  in [N, N, N, NN]
Most of the work on weakly supervised person and action learning, however, has been limited to one or  a few movies
Therefore the power of leveraging large-scale  NDépartement d’informatique de l’ENS, École normale supérieure,  CNRS, PSL Research University, NN00N Paris, France
NCzech Institute of Informatics, Robotics and Cybernetics at the  Czech Technical University in Prague
 Figure N: We automatically recognize actors and their actions in a  of dataset of NN movies with scripts as weak supervision
 weakly annotated video data has not been fully explored
 In this work we aim to scale weakly supervised learning of actions
In particular, we follow the work of [N] and  learn actor names together with their actions from movies  and movie scripts
While actors are learned separately for  each movie, differently from [N], our method simultaneously learns actions from all movies and movie scripts available for training
Such an approach, however, requires solving a large-scale optimization problem
We address this  issue and propose to scale weakly supervised learning by  adapting the Block-Coordinate Frank-Wolfe approach [NN]
 Our optimization procedure enables action learning from  tens of movies and thousands of action samples, readily  available from our subset of movies or other recent datasets  with movie descriptions [NN]
This, in turn, results in large  improvements in action recognition
 Besides the optimization, our work introduces a new  model for background class in the form of a constraint
It  enables better and automatic modeling of the background  class (i.e
unknown actors and actions)
We evaluate our  method on NN movies and demonstrate significant improvements for both actor and action recognition
Example results are illustrated in Figure N
 NNNN    N.N
Related Work  This section reviews related work on discriminative clustering, Frank-Wolfe optimization and its applications to the  weakly supervised learning of people and actions in video
 Discriminative clustering and Frank-Wolfe
The  Frank-Wolfe algorithm [NN, NN] allows to minimize large  convex problems over convex sets by solving a sequence  of linear problems
In computer vision, it has been used in  combination with discriminative clustering [N] for action  localization [N], text-to-video alignment [N, N], object  co-localization in videos and images [NN] or instance-level  segmentation [NN]
A variant of Frank-Wolfe with randomized block coordinate descent was proposed in [NN]
This  extension leads to lower complexity in terms of time and  memory requirements while preserving the convergence  rate
In this work we build on [NN] and adapt it for the  problem of large-scale weakly supervised learning of  actions from movies
 Weakly supervised action recognition
Movie scripts  are used as a source of weak supervision for temporal action localization in [N]
An extension of this work [N] exploits the temporal order of actions as a learning constraint
 Other [NN] target spatio-temporal action localization and  recognition in video using a latent SVM
A weakly supervised extension of this method [NN] localizes actions without location supervision at the training time
Another recent  work [NN] proposes a multi-fold Multiple-Instance Learning (MIL) SVM to localize actions given video-level supervision at training time
Closer to us is the work of [N]  that improves weakly supervised action recognition by joint  action-actor constraints derived from scripts
While the approach in [N] is limited to a few action classes and movies,  we propose here a scalable solution and demonstrate significant improvements in action recognition when applied to  the large-scale weakly supervised learning of actions from  many movies
 Weakly supervised person recognition
Person recognition in TV series has been studied in [N0, NN] where the  authors propose a solution to the problem of associating  speaker names in scripts and faces in videos
Speakers in  videos are identified by detecting face tracks with lip motion
The method in [N] presents an alternative solution by  formulating the association problem using a convex surrogate loss
Parkhi et al
[NN] present a method for person  recognition combining a MIL SVM with a model for the  background class
Most similar to our model is the one presented in [N]
The authors propose a discriminative clustering cost under linear constraints derived from scripts to  recover the identities and actions of people in movies
Apart  from scaling-up the approach of [N] to much larger datasets,  our model extends and improves [N] with a new background  constraint
 Contributions
In this paper we make the following  contributions: (i) We propose an optimization algorithm  based on Block-Coordinate Frank-Wolfe that allows scaling up discriminative clustering models [N] to much larger  datasets
(ii) We extend the joint weakly supervised PersonAction model of [N], with a simple yet efficient model of the  background class
(iii) We apply the proposed optimization  algorithm to scale-up discriminative clustering to an order  of magnitude larger dataset, resulting in significantly improved action recognition performance
 N
Discriminative Clustering for Weak Supervision  We want to assign labels (e.g
names or action classes)  to samples (e.g
person tracks in the video)
As opposed  to the standard supervised learning setup, the exact labels  of samples are not known at training time
Instead, we are  given only partial information that some samples in a subset  (or bag) may belong to some of the labels
This ambiguous  setup, also known as multiple instance learning, is common,  for example, when learning human actions from videos and  associated text descriptions
 To address this challenge of ambiguous and partial  labels, we build on the discriminative clustering criterion based on a linear classifier and a quadratic loss  (DIFFRAC [N])
This framework has shown promising results in weakly supervised and unsupervised computer vision tasks [N, N, N, N, NN, NN, N0, NN]
In particular, we use  this approach to group samples into linearly separable clusters
Suppose we have N samples to group into K classes
 We are given d-dimensional features X ∈ RN×d, one for each of the N samples, and our goal is to find a binary matrix Y ∈ {0, N}N×K assigning each of the N samples to one of the labels, where Ynk = N if and only if the sample n (e.g
a person track in a movie) is assigned to the label k  (e.g
action class running)
 First, suppose the assignment matrix Y is given
In this  case finding a linear classifier W can be formulated as a  ridge regression problem  min W∈Rd×K  N  NN ‖Y −XW‖NF +  λ  N ‖W‖NF, (N)  where X is a matrix of input features, Y is the given labels assignment matrix, ‖.‖F is the matrix norm (or Frobe- nius norm) induced by the matrix inner product 〈., .〉F (or Frobenius inner product) and λ is a regularization hyperparameter set to a fixed constant
The key observation is  that we can resolve the classifier W ∗ in closed form as  W ∗(Y ) = (X⊤X +NλI)−NX⊤Y
(N)  In our weakly supervised setting, however, Y is unknown
Therefore, we treat Y as a latent variable and optimize (N) w.r.t
W and Y 
In details, plugging the optimal  NNNN    solution W ∗ (N) in the cost (N) removes the dependency on  W and the cost can be written as a quadratic function of Y ,  i.e
C(Y ) = 〈Y,A(X,λ)Y 〉F, where A(X,λ) is a matrix that only depends on the data X and a regularization parameter λ
Finding the best assignment matrix Y can then  be written as the minimization of the cost C(Y ):  min Y ∈{0,N}N×K  〈Y,A(X,λ)Y 〉F
(N)  Solving the above problem, however, can lead to degenerate solutions [N] unless additional constraints on Y are  provided
In section N, we incorporate weak supervision in  the form of constraints on the latent assignment matrices  Y 
The constraints on Y used for weak supervision generally decompose into small independent blocks
This block  structure is the key for our optimization approach that we  will present next
 N.N
Large-Scale optimization  The Frank-Wolfe (FW) algorithm has been shown effective for optimizing convex relaxation of (N) in a number of  vision problems [N, N, N, N, NN, NN]
It only requires solving  linear programs on a set of constraints
Therefore, it avoids  costly projections and allows the use of complicated constraints such as temporal ordering [N]
However, the standard FW algorithm is not well suited to solve (N) for a large  number of samples N 
 First, storing the N ×N matrix A(X,λ) in memory be- comes prohibitive (e.g
the size of A becomes ≥ N00GB for N ≥ N00000)
Second, each update of the FW algorithm requires a full pass over the data resulting in a space and  time complexity of order N for each FW step
 Weakly supervised learning is, however, largely motivated by the desire of using large-scale data with “cheap”  and readily-available but incomplete and noisy annotation
 Scaling up weakly supervised learning to a large number of  samples is, therefore, essential for its success
We address  this issue and develop an efficient version of the FW algorithm
Our solution builds on the Block-Coordinate FrankWolfe (BCFW) [NN] algorithm and extends it with a smart  block-dependent update procedure as described next
The  proposed update procedure is one of the key contribution of  this paper
 N.N.N Block-coordinate Frank-Wolfe (BCFW)  The Block-Coordinate version of the Frank-Wolfe algorithm [NN] is useful when the convex feasible set Y can be written as a Cartesian product of n smaller sets of constraints: Y = Y(N) × 


× Y(n)
Inspired by coordinate descent techniques, BCFW consists of updating one variable block Y(i) at a time with a reduced Frank-Wolfe step
This method has potentially n times cheaper iterates both  in space and time
We will see that most of the weakly supervised problems exhibit such a block structure on latent  variables
 N.N.N BCFW for discriminative clustering  To benefit from BCFW, we have to ensure that the time and  space complexity of the block update does not depend on  the total number of samples N (e.g
person tracks in all  movies) but only depends on the size Ni of smaller blocks  of samples i (e.g
person tracks within one movie)
After  a block is sampled, the update consists of two steps
First,  the gradient with respect to the block is computed
Then the  linear oracle is called to obtain the next iterate
As we show  below, the difficult part in our case is to efficiently compute  the gradient with respect to the block
 Block gradient: a naive approach
Let’s denote Ni the  size of block i
The objective function f of problem (N) is  f(Y ) = 〈Y,A(X,λ)Y 〉F, where (see [N])  A(X,λ) = N  NN (IN −X(X  ⊤X +NλId) −NX⊤)
(N)  To avoid storing matrix A(X,λ) of size N×N , one can pre- compute the matrix P = (X⊤X +NλId)  −NX⊤ ∈ Rd×N 
We can write the block gradient with respect to a subset of  samples i as follows:  ∇(i)f(Y ) = N  N (Y (i) −X(i)PY ), (N)  where Y (i) ∈ RNi×K and X(i) ∈ RNi×d are the label as- signment variable and the feature matrix for block i (e.g
 person tracks in movie i), respectively
Because of the PY  matrix multiplication, naively computing this formula has  the complexity O(NdK), where N is the total number of samples, d is the dimensionality of the feature space and K  is the number of classes
As this depends linearly on N , we  aim to find a more efficient way to compute block gradients,  as described next
 Block gradient: a smart update
We now propose an update procedure that avoids re-computation of block gradients and whose time and space complexity at each iteration  depends on Ni instead of N 
The main intuition is that we  need to find a way to store information about all the blocks  in a compact form
A natural way of doing so is to maintain  the weights of the linear regression parameters W ∈ Rd×K 
From (N) we have W = PY 
If we are able to maintain the variable W at each iteration with the desired complexity  O(NidK), then the block gradient computation (N) can be reduced fromO(NdK) toO(NidK)
We now explain how to effectively achieve that
 NNNN    At each iteration t of the algorithm, we only update a  block i of Y while keeping all other blocks fixed
We denote  the direction of the update by Dt ∈ R N×K and the step size  by γt
With this notation the update becomes  Yt+N = Yt + γtDt
(N)  The update rule for the weight variable Wt can now be written as follows:  Wt+N = P (Yt + γtDt)  Wt+N = Wt + γtPDt, (N)  Recall that at iteration t, BCFW only updates block i, therefore Dt has non zero value only at block i
In block notation  we can therefore write the matrix product PDt as:  [  P (N), · · · , P (i), · · · , P (n) ]  ×      0  D (i) t  0      = P (i)D (i) t ,  (N)  where P (i) ∈ Rd×Ni and D (i) t ∈ R  Ni×K are the i-th blocks  of matrices P and Dt, respectively
The outcome is an update of the following form  Wt+N = Wt + γtP (i)D  (i) t , (N)  where the computational complexity for updating W has  been reduced to O(NidK) compared to O(NdK) in the standard update
 We have designed a Block-Coordinate Frank-Wolfe update with time and space complexity depending only on the  size of the blocks and not the entire dataset
This allows  to scale discriminative clustering to problems with a very  large number of samples
The pseudo-code for the algorithm is summarized in Algorithm N
Next, we describe an  application of this large-scale discriminative clustering algorithm to weakly supervised person and action recognition  in movies
 N
Weakly supervised Person-Action model  We now describe an application of our large-scale discriminative clustering algorithm with weak-supervision
 The goal is to assign to each person track a name and an action
Both names and actions are mined from movie scripts
 For a given movie i, we assume to have Ni automatically  extracted person tracks as well as the parsing of a movie  script into person names and action classes
We also assume that scripts and movies have been roughly aligned in  time
In such a setup we can assign labels (e.g
a name or  an action) from a script section to a subset of tracksN from the corresponding time interval of a movie (see Figure N for  example)
In the following, we explain how to convert such  form of weak supervision into a set of constraints on latent  Algorithm N BCFW for Discriminative Clustering [N]  Initiate Y0, P := (X ⊤X + NλId)  −NX⊤, W0 = PY0, gi = +∞, ∀i
for t = N 

.Niter do  i← sample from distribution proportional to g [NN]  ∇(i)f(Yt)← N N (Y  (i) t −X  (i)Wt) # Block gradient Ymin ← argminx∈Y(i)〈∇(i)f(Yt), x〉F # Linear oracle  D ← Ymin − Y (i)  gi ← −〈D,∇(i)f(Yt)〉F # Block gap γ ← min(N, giN  N 〈D,D−X(i)P (i)D〉F  ) # Line-search  Wt+N ←Wt + γP (i)D # W update  Y (i) t+N ← Y  (i) t + γD # Block update  end for  variables corresponding to the names and actions of people
 We will also show how these constraints easily decompose  into blocks
We denote Z the latent variable assignment  matrix for person names and T for actions
 N.N
Weak-supervision as constraints  We use linear constraints to incorporate weak supervision from movie scripts
In detail, we define constraints  on subsets of person tracks that we call “bags”
In the following we explain the procedure for construction of bags  together with the definition of the appropriate constraints
 ‘At least one’ constraint
Suppose a script reveals the  presence of a person p in some time interval of the movie
 We construct a set N with all person tracks in this interval
As first proposed by [N], we model that at least one track in  N is assigned to person p by the following constraint  ∑  n∈N  Znp ≥ N
(N0)  We can apply the same type of constraint when solving for  action assignment T 
 Person-Action constraint
Scripts can also provide information that a person p is performing an action a in a scene
 In such cases we can formulate stricter and more informative constraints as follows
We construct a setN containing all person tracks appearing in this scene
Following [N], we  formulate a joint constraint on presence of a person performing a specific action as  ∑  n∈N  ZnpTna ≥ N
(NN)  Mutual exclusion constraint
We also model that each  person track can only be assigned to exactly one label
This  NNN0    Movie N:      …      Le  st er               ..
     …      V irg  in ia               ..
 …      Lester gets up and   starts after Jane taking   his plate with him…      …      S ta  nd  u  p               …                D  riv e  …   0  N  0  N  N  0  N  0  0  0  0  0  Script N:  Virginia is driving   while Buster intently   studies the terrain…  Script i:  Movie i:  : Person assignment matrix of movie i  : # of known characters in movie i : # of person tracks in movie i  : Subset of tracks with constraints : Action assignment matrix  : # of action classes in the model : # of person tracks in ALL movies  Figure N: Overview of the Person-Action weakly supervised model, see text for detailed explanations
 restriction can be formalized by the mutual exclusion constraint  ZNP = NN , (NN)  for Z (i.e
rows sum up to N)
Same constraint holds for T 
 Background class constraint
One of our contributions  is a novel way of coping with the background class
As  opposed to previous work [N], our approach allows us to  have background model that does not require any external  data
Also it does not require a specific background class  classifier as in [NN]
 Our background class constraint can be seen as a way  to supervise people and actions that are not mentioned in  scripts
We observe that tracks that are not subject to constraints from Eq
(N0) and tracks that belong to crowded  shots are likely to belong to the background class
Let us  denote by B the set of such tracks
We impose that at least a certain fraction α ∈ [0, N] of tracks in B must belong to the background class
Assuming that person label p = N cor- responds to the background, we obtain the following linear  constraint (similar constraint can be defined for actions on  T ):  ∑  n∈B  ZnN ≥ α | B | 
(NN)  N.N
Person-Action model formulation  Here we summarize the complete formulation of the person and action recognition problems
 Solving for names
We formulate the person recognition  problem as discriminative clustering, where XN are face descriptors:  min Z∈{0,N}N×P  〈Z,A(XN, λ)Z〉F, (Discriminative cost) (NN)  such that            ∑  n∈N Znp ≥ N, (At least one) ∑  n∈B ZnN ≥ α | B |, (Background)  ZNP = NN 
(Mutual exclusion)  Solving for actions
After solving the previous problem  for names separately for each movie, we vertically concatenate all person name assignment matrices Z
We also define  a single action assignment variable T in {0, N}M×A, where M is the total number of tracks across all movies and XN are action descriptors (details given later)
We formulate  our action recognition problem as a large QP:  min T∈{0,N}M×A  〈T,A(XN, µ)T 〉F, (Discriminative cost) (NN)  such that                    ∑  n∈N Tna ≥ N, (At least one) ∑  n∈N ZnpTna ≥ N, (Person-Action) ∑  n∈B TnN ≥ β | B |, (Background)  TNA = NM 
(Mutual exclusion)  Block-Separable constraints
The set of linear constraints on the action assignment matrix T is block separable  since each movie has it own set of constraints, i.e
there are  no constraints spanning multiple movies
Therefore, we can  fully demonstrate here the power of our large-scale discriminative clustering optimization (Algorithm N)
 N
Experimental Setup  N.N
Dataset  Our dataset is composed of NN Hollywood feature-length  movies (see the list in Appendix [NN]) that we obtained  from either BluRay or DVD
For all movies, we downloaded their scripts (on www.dailyscript.com) that  we temporally aligned with the videos and movie subtitles  using the method described in [NN]
The total number of  frames in all NN movies is NN,NN0,NNN
The number of body  tracks detected across all movies (see N.N for more details)  is M = N0NNNN
 N.N
Text pre-processing  To provide weak supervision for our method we process  movie scripts to extract occurrences of the NN most frequent  action classes: Stand Up, Eat, Sit Down, Sit Up,  NNNN  www.dailyscript.com   Hand Shake, Fight, Get Out of Car, Kiss, Hug,  Answer Phone, Run, Open Door and Drive
To do  so, we collect a corpus of movie scripts different from the  set of our NN movies and train simple text-based action classifiers using linear SVM and a TF-IDF representation of  words composed of uni-grams and bi-grams
After retrieving actions in our target movie scripts, we also need to identify who is performing the action
We used spaCy [NN] to  parse every sentence classified as describing one of the NN  actions and get every subject for each action verb
 N.N
Person detection and Features  Face tracks
To obtain tracks of faces in the video, we  run the multi-view face detector [NN] based on the DPM  model [NN]
We then extract face tracks using the same  method as in [N0, NN]
For each detected face, we compute facial landmarks [NN] followed by the face alignment  and resizing of face images to NNNxNNN pixel
We use pretrained vgg-face features [NN] to extract descriptors for each  face
We kept the features of dimension N0NN computed by  the network at the last fully-connected layer that we LN normalized
For each face track, we choose the top K (in practice, we choose K=N) faces that have the best facial landmark confidence
Then we represent each track by averaging the features of the top K faces
 Body tracks
To get the person body tracks, we run  the Faster-RCNN network with VGG-NN architecture finetuned on VOC 0N [NN]
Then we track bounding boxes using the same tracker as used to obtain face tracks
To get  person identity for body tracks, we greedily link each body  track to one face track by maximizing a spatio-temporal  bounding box overlap measure
However if a body track  does not have an associated face track as the actor’s face  may look away from the camera, we cannot obtain its identity
Such tracks can be originating from any actor in the  movie
To capture motion features of each body track, we  compute bag-of-visual-words representation of dense trajectory descriptors [NN] inside the bounding boxes defined  by the body track
We use N000 cluster centers for each of  the HOF, MBHx and MBHy channels
In order to capture  appearance of each body track we extract ResNet-N0 [NN]  pre-trained on ImageNet
For each body bounding box, we  compute the average RoI-pooled [NN] feature map of the last  convolutional layer within the bounding box, which yields  a feature vector of dimension N0NN for each box
We extract  a feature vector every N0th frame, average extracted feature  vectors over the duration of the track and LN normalize
Finally, we concatenate the dense trajectory descriptor and the  appearance descriptor resulting in a NN0NN-dimensional descriptor for each body track
 Method Acc
Multi-Class AP Background AP  Cour et al
[N] NN% NN% − Sivic et al
[NN] NN% NN% − Bojanowski et al
[N] NN% NN% NN% Parkhi et al
[NN] NN% NN% NN% Our method NN% NN% NN%  Table N: Comparison on the Casablanca benchmark [N]
 Episode N N N N N  Sivic et al
[NN] N0 NN N0 NN NN  Parkhi et al
[NN] NN N0 NN NN NN  Ours NN NN NN NN NN  Table N: Comparison on the Buffy benchmark [NN] using AP
 α 0 0.N 0.N 0.N 0.N 0.N 0.NN N.0  Accuracy NN NN N0 NN NN NN NN NN  AP NN NN N0 NN NN NN NN NN  Table N: Sensitivity to hyper-parameter α (NN) on Casablanca
 N
Evaluation  N.N
Evaluation of person recognition  We compare our person recognition method to several  other methods on the Casablanca benchmark from [N] and  the Buffy benchmark from [NN]
All methods are evaluated on the same inputs: same face tracks, scripts and characters
Table N shows the Accuracy (Acc.) and Average  Precision (AP) of our approach compared to other methods  on the Casablanca benchmark [N]
In particular we compare to Parkhi et al
[NN] which is a strong baseline using  the same CNN face descriptors as in our method
We also  show the AP of classifying the background character class  (Background AP)
We compare in Table N our approach to  other methods [NN, NN] reporting results on season N of the  TV series “Buffy the Vampire Slayer”
Both of these methods [NN, NN] use speaker detection to mine additional strong  (but possibly incorrect) labels from the script, which we  also incorporate (as additional bags) to make the comparison fair
Our method demonstrates significant improvement  over the previous results
It also outperforms other methods  on the task of classifying background characters
Finally,  Table N shows the sensitivity to hyper-parameter α from  the background constraint (NN) on the Casablanca benchmark
Note that in contrast to other methods, our background model does not require supervision for the background class
This clearly demonstrates the advantage of  our proposed background model
For all experiments the  hyper-parameter α of the background constraint (NN) was  set to N0%
Figure N illustrates our qualitative results for character recognition in different movies
 NNNN    METHOD # movies Joint-Model St.U
E
S.D
Si.U
H.S
F
G.C
K
H
A
R
O.D
D
mAP  (a) Random ∅ No 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N N.N 0.N 0.N 0.N (b) Script only ∅ No N.0 N.N N.N N.N N.N N.N N.N NN.N N.N N.N NN.N N.N N.0 N.N (c) Fully-supervised N No NN.N 0.N NN.N 0.N 0.N N.N N.N N.N N.N N.0 NN.N NN.N N.N N.N  (d) Few training movies N Yes NN.N N.N NN.N N.N N.N N.N N.0 N.0 N.N N.N NN.0 NN.N NN.N NN.N  (e) No Joint Model NN No N0.N N.0 NN.N N.N NN.0 NN.N N.0 NN.N N.N N.N NN.N NN.N NN.N NN.N  (f) Full setup NN Yes NN.0 N.N NN.N N.N N.N N.N N.0 NN.N N.N N.N NN.N NN.N NN.N NN.N  Table N: Average Precision of actions evaluated on N movies
(St.U: Stand Up, E.: Eat, S.D: Sit Down, Si.U.: Sit Up, H.S: Hand  Shake, F.: Fight, G.C.: Get out of Car, K.: Kiss, H.: Hug, A.: Answer Phone, R.: Run, O.D.: Open Door, D.: Drive)  0 0.N 0.N 0.N 0.N 0.N 0.N 0.N  recall  0  0.N  0.N  0.N  0.N  N  p re  c is  io n  Ours (NN movies) AP=0.NN  Ours (N movie) AP=0.NN  Bojanowski et al
AP=0.0N  Figure N: PR curves of action SitDown from Casablanca
 0 N0 N0 N0 N0 N0 N0  Number of movies  NN.N  NN  NN.N  NN  NN.N  NN  NN.N  NN  NN.N  m A  P  Figure N: Action recognition mAP with increasing number of  training movies
 N.N
Evaluation of action recognition  First, we compare our method to Bojanowski et al
 N0NN [N]
Their evaluation uses different body tracks than  ours, we design here an algorithm-independent evaluation  setup
We compare our model using the Casablanca movie  and the Sit Down action
For the purpose of evaluation,  we have manually annotated all person tracks in the movie  and then manually labeled whether or not they contain the  Sit Down action
Given this ground truth, we assess the  two models in a similar way as typically done in object detection
Figure N shows a precision-recall curve evaluating  recognition of the Sit Down action
We show our method  trained on Casablanca only (as done in [N]) and then on all  NN movies
Our method trained on Casablanca is already  better than [N]
The improvement becomes even more evident when training our method on all NN movies
 To evaluate our method on all NN action classes, we use  five movies (American Beauty, Casablanca, Double Indemnity, Forrest Gump and Fight Club)
For each of these  movies we have manually annotated all person tracks produced by our tracker according to NN target action classes  and the background action class
We assume that each track  corresponds to at most one target action
In rare cases where  Method R@N R@N R@N0 Median Rank  Yu et al
[NN] N.N% NN.N% NN.N% N0 Levi et al
[NN] N.N% NN.N% NN.N% NN Our baseline N.N% NN.N% NN.N% NN  Table N: Baseline comparison against winners of the LSMDCN0NN  movie clip retrieval challenge  this assumption is violated, we annotate the track by one of  the correct action classes
 In Table N we compare results of our model to different  baselines
The first baseline (a) corresponds to the random  assignment of action classes
The second baseline (b) Script  only uses information extracted from the scripts: each time  an action appears in a bag, all person tracks in this bag are  then simply annotated with this action
Baseline (c) is using  our action descriptors but trained in a fully supervised setup on a small subset of annotated movies
To demonstrate  the strength of this baseline we have used the same action  descriptors on the LSMDCN0NNN movie clip retrieval challenge
This is the largest public benchmark [NN] related to  our work that considers movie data (but without person localization as we do in our work)
Table N shows our features  employed in simple CCA method as done in [NN] achieving  state-of-the-art on this benchmark
The fourth baseline (d)  is our method train only using the five evaluated movies
 The fifth baseline (e) is our model without the joint personaction constraint (NN), but still trained on all NN movies
Finally, the last result (f) is from our model using all the NN  training movies and person-action constraints (NN)
Results  demonstrate that optimizing our model on more movies  brings the most significant improvement to the final results
 We confirm the idea from [N] that adding the information  of who is performing the action in general helps identifying actions
However we also notice it is not always true  for actions with interacting people such as: Fight, Hand  Shake, Hug or Kiss
Knowing who is doing the action  does not seems to help for these actions
Figure N shows improvements in action recognition when gradually increasing the number of training movies
Figure N shows qualitative results of our model on different movies
The statistics  about the ground truth and constraints together with additional results are provided in Appendix [NN]
Nhttps://sites.google.com/site/describingmovies/lsmdc-N0NN  NNNN    Figure N: Qualitative results for face recognition
Green bounding boxes are face tracks correctly classified as background characters
 Figure N: Qualitative results for action recognition
P stands for for the name of the character and A for the action performed by P
Last  row (in red) shows mislabeled tracks with high confidence (e.g
hugging labeled as kissing, sitting in a car labeled as driving)
 N
Conclusion We have proposed an efficient online optimization  method based on the Block-Coordinate Frank-Wolfe algorithm
We use this new algorithm to scale-up discriminative  clustering model in the context of weakly supervised person  and action recognition in feature-length movies
Moreover,  we have proposed a novel way of handling the background  class, which does not require collecting background class  data as required by the previous approaches, and leads to  better performance for person recognition
In summary, the  proposed model significantly improves action recognition  results on NN feature-length movies
The significance of the  technical contribution goes beyond the problem of personaction recognition as the proposed optimization algorithm  can scale-up other problems recently tackled by discriminative clustering
Examples include: unsupervised learning from narrated instruction videos [N], text-to-video alignment [N], co-segmentation [NN], co-localization in videos  and images [NN] or instance-level segmentation [NN], which  can be now scaled-up to an order of magnitude larger  datasets
 Acknowledgments
This work has been supported by ERC grants  ACTIVIA (no
N0NNNN) and LEAP (no
NNNNNN), CIFAR Learning in Machines & Brains program, ESIF, OP Research, development and education Project IMPACT No
CZ.0N.N.0N/0.0/0.0/NN 00N/0000NNN and a  Google Research Award
 NNNN    References  [N] J.-B
Alayrac, P
Bojanowski, N
Agrawal, I
Laptev, J
Sivic,  and S
Lacoste-Julien
Unsupervised learning from narrated  instruction videos
In CVPR, N0NN
N, N, N  [N] F
Bach and Z
Harchaoui
Diffrac: a discriminative and flexible framework for clustering
In NIPS, N00N
N, N, N  [N] T
L
Berg, A
C
Berg, J
Edwards, M
Maire, R
White, Y.W
Teh, E
Learned-Miller, and D
A
Forsyth
Names and  faces in the news
In CVPR, volume N, pages II–NNN, N00N
 N  [N] P
Bojanowski, F
Bach, I
Laptev, J
Ponce, C
Schmid, and  J
Sivic
Finding Actors and Actions in Movies
In ICCV,  N0NN
N, N, N, N, N, N, N  [N] P
Bojanowski, R
Lajugie, F
Bach, I
Laptev, J
Ponce,  C
Schmid, and J
Sivic
Weakly supervised action labeling in videos under ordering constraints
In ECCV, N0NN
N,  N, N  [N] P
Bojanowski, R
Lajugie, E
Grave, F
Bach, I
Laptev,  J
Ponce, and C
Schmid
Weakly-supervised alignment of  video with text
In ICCV, N0NN
N, N, N  [N] F
Caba Heilbron, V
Escorcia, B
Ghanem, and J
Carlos Niebles
Activitynet: A large-scale video benchmark  for human activity understanding
In CVPR, pages NNN–NN0,  N0NN
N  [N] T
Cour, B
Sapp, C
Jordan, and B
Taskar
Learning from  ambiguously labeled images
In CVPR, N00N
N, N  [N] O
Duchenne, I
Laptev, J
Sivic, F
Bach, and J
Ponce
Automatic annotation of human actions in video
In ICCV, N00N
 N, N  [N0] M
Everingham, J
Sivic, and A
Zisserman
“Hello! My  name is..
Buffy” – Automatic Naming of Characters in TV  Video
In BMVC, N00N
N, N, N  [NN] M
Frank and P
Wolfe
An algorithm for quadratic programming
Naval Research Logistics Quarterly, NNNN
N  [NN] R
B
Girshick, P
F
Felzenszwalb, and D
McAllester
Discriminatively trained deformable part models, release N
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep Residual Learning  for Image Recognition
In CVPR, N0NN
N  [NN] M
Honnibal and M
Johnson
An improved non-monotonic  transition system for dependency parsing
In EMNLP, N0NN
 N  [NN] M
Jaggi
Revisiting Frank-Wolfe: Projection-free sparse  convex optimization
In ICML, N0NN
N  [NN] A
Joulin, F
Bach, and J
Ponce
Discriminative Clustering  for Image Co-segmentation
In CVPR, N0N0
N, N  [NN] A
Joulin, F
Bach, and J
Ponce
Multi-class cosegmentation
 In CVPR, N0NN
N  [NN] A
Joulin, K
Tang, and L
Fei-Fei
Efficient image and video  co-localization with frank-wolfe algorithm
In ECCV, N0NN
 N, N  [NN] A
Joulin, K
Tang, and L
Fei-Fei
Efficient image and video  co-localization with Frank-Wolfe algorithm
In ECCV, N0NN
 N  [N0] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In CVPR, pages NNNN–NNNN, N0NN
 N  [NN] S
Lacoste-Julien, M
Jaggi, M
Schmidt, and P
Pletscher
 Block-coordinate frank-wolfe optimization for structural  SVMs
In ICML, N0NN
N, N, N  [NN] T
Lan, Y
Wang, and G
Mori
Discriminative figure-centric  models for joint action localization and recognition
In  ICCV, N0NN
N  [NN] I
Laptev, M
Marszalek, C
Schmid, and B
Rozenfeld
 Learning realistic human actions from movies
In CVPR,  N00N
N, N  [NN] G
Levi, D
Kaufman, L
Wolf, and T
Hassner
Video Description by Combining Strong Representation and a Simple  Nearest Neighbor Approach
In ECCV LSMDCN0NN Workshop, N0NN
N  [NN] M
Mathias, R
Benenson, M
Pedersoli, and L
Van Gool
 Face detection without bells and whistles
In ECCV, N0NN
N  [NN] A
Miech, J
B
Alayrac, P
Bojanowski, I
Laptev, and  J
Sivic
Supplementary material (appendix)
https:  //hal.inria.fr/hal-0NNNNNN0
N, N  [NN] A
Osokin, J.-B
Alayrac, I
Lukasewitz, P
Dokania, and  S
Lacoste-Julien
Minding the gaps for block frank-wolfe  optimization of structured svms
In ICML, N0NN
N  [NN] O
Parkhi, E
Rahtu, and A
Zisserman
It’s in the bag:  Stronger supervision for automated face labelling
In ICCV  Workshop, N0NN
N, N, N  [NN] O
Parkhi, A
Vedaldi, and A
Zisserman
Deep Face Recognition, British Machine Vision Conference
In BMVC, N0NN
 N  [N0] V
Ramanathan, A
Joulin, P
Liang, and L
Fei-Fei
Linking people in videos with “their” names using coreference  resolution
In ECCV, N0NN
N  [NN] A
Rohrbach, M
Rohrbach, N
Tandon, and B
Schiele
A  dataset for movie description
In CVPR, N0NN
N, N  [NN] G
Seguin, P
Bojanowski, R
Lajugie, and I
Laptev
 Instance-level video segmentation from object tracks
In  CVPR, N0NN
N, N, N  [NN] R
Shaoqing, H
Kaiming, G
Ross, and S
Jian
Faster rcnn: Towards real-time object detection with region proposal  networks
In NIPS, N0NN
N  [NN] N
Shapovalova, A
Vahdat, K
Cannons, T
Lan, and  G
Mori
Similarity constrained latent support vector machine: An application to weakly supervised action classification
In ECCV, N0NN
N  [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In NIPS, pages  NNN–NNN, N0NN
N  [NN] J
Sivic, M
Everingham, and A
Zisserman
“Who are you?”  - Learning person specific classifiers from video
In CVPR,  N00N
N, N, N  [NN] K
Soomro, A
R
Zamir, and M
Shah
UCFN0N: A dataset  of N0N human actions classes from videos in the wild
arXiv  preprint arXiv:NNNN.0N0N, N0NN
N  [NN] M
Tapaswi, M
Bauml, and R
Stiefelhagen
“Knock!  Knock! Who is it?” probabilistic person identification in  tv-series
In CVPR, N0NN
N  [NN] H
Wang and C
Schmid
Action Recognition with Improved  Trajectories
In ICCV, N0NN
N, N  NNNN  https://hal.inria.fr/hal-0NNNNNN0 https://hal.inria.fr/hal-0NNNNNN0   [N0] L
Wang, Y
Qiao, and X
Tang
Action recognition with  trajectory-pooled deep-convolutional descriptors
In CVPR,  pages NN0N–NNNN, N0NN
N  [NN] P
Weinzaepfel, X
Martin, and C
Schmid
Towards  weakly-supervised action localization
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [NN] Y
Yu, H
Ko, J
Choi, and G
Kim
Video captioning  and retrieval models with semantic attention
In ECCV  LSMDCN0NN Workshop, N0NN
N  NNNNConvolutional Dictionary Learning via Local Processing   Convolutional Dictionary Learning via Local Processing  Vardan Papyan  vardanp@campus.technion.ac.il  Yaniv Romano  yromano@tx.technion.ac.il  Jeremias Sulam  jsulam@cs.technion.ac.il  Michael Elad  elad@cs.technion.ac.il  Technion - Israel Institute of Technology  Technion City, Haifa NN000, Israel  Abstract  Convolutional sparse coding is an increasingly popular model in the signal and image processing communities,  tackling some of the limitations of traditional patch-based  sparse representations
Although several works have addressed the dictionary learning problem under this model,  these relied on an ADMM formulation in the Fourier domain, losing the sense of locality and the relation to the  traditional patch-based sparse pursuit
A recent work suggested a novel theoretical analysis of this global model, providing guarantees that rely on a localized sparsity measure
 Herein, we extend this local-global relation by showing how  one can efficiently solve the convolutional sparse pursuit  problem and train the filters involved, while operating locally on image patches
Our approach provides an intuitive  algorithm that can leverage standard techniques from the  sparse representations field
The proposed method is fast to  train, simple to implement, and flexible enough that it can  be easily deployed in a variety of applications
We demonstrate the proposed training scheme for image inpainting  and image separation, achieving state-of-the-art results
 N
Introduction  The celebrated sparse representation model has led to  impressive results in various applications over the last  decade [N0, N, NN, N0, N]
In this context one assumes that a  signal X ∈ RN is a linear combination of a few columns, also called atoms, taken from a matrix D ∈ RN×M termed a dictionary; i.e
X = DΓ where Γ ∈ RM is a sparse vector
Given X, finding its sparsest representation, called  sparse pursuit, amounts to solving the following problem  min Γ  ‖Γ‖0 s.t
‖X−DΓ‖N ≤ ǫ, (N)  where ǫ stands for the model mismatch or an additive noise  strength
The solution for the above can be approximated  using greedy algorithms such as Orthogonal Matching Pursuit (OMP) [N] or convex formulations such as BP [N]
The  task of learning the model, i.e
identifying the dictionary D  that best represents a set of training signals, is called dictionary learning and several methods have been proposed for  tackling it, including K-SVD [N], MOD [NN], online dictionary learning [NN], trainlets [NN], and more
 When dealing with high-dimensional images, traditional  image processing algorithms train a local model for fully  overlapping patches extracted from X, process these independently using the trained local prior and then average the  results to obtain a global estimate
This approach, which  we refer to in what follows as patch averaging, gained  much popularity and success due to its simplicity and highperformance [N0, N0, N, N0]
A different approach is the  Convolutional Sparse Coding (CSC), which works with a  global model by imposing a specific structure on the dictionary involved [NN, N, NN, NN, NN, NN, NN]
In particular, this  assumes that D is a banded convolutional dictionary, implying that the signal is a superposition of a few local atoms,  or filters, shifted to different positions
Unlike patch averaging that restores independently the same information in  the image several times, CSC treats the information jointly  and only once
Several works have presented algorithms  for training convolutional dictionaries [N, NN, NN], circumventing some of the computational burdens of this problem  by relying on ADMM solvers that operate in the Fourier  domain
In doing so, these methods lost the connection to  the patch-based processing paradigm, as widely practiced  in many signal and image processing applications
 In this work, we propose a novel approach for training  the CSC model, called slice-based dictionary learning
Unlike current methods, we leverage a localized strategy enabling the solution of the global problem in terms of only  local computations in the original domain
The main advantages of our method over existing ones are:  N
It operates locally on patches, while solving faithfully  NNNNN    Figure N: Top: Patches extracted from natural images
Bottom: Their corresponding slices
Observe how the slices are far  simpler, and contained by their corresponding patches
 the global CSC problem;  N
It is easy to implement and intuitive to understand;  N
It reveals how one should modify current (and any)  dictionary learning algorithms to solve the CSC problem in a variety of applications;  N
It can leverage standard techniques from the sparse  representations field, such as OMP, LARS, K-SVD,  MOD, online dictionary learning and trainlets;  N
When compared to state-of-the-art methods, it can be  applied to standard-sized images, converges faster and  provides a better model; and  N
It can naturally allow for a different number of nonzeros in each spatial location, according to the local  signal complexity
 The rest of this paper is organized as follows: Section N  reviews the CSC model
The proposed method is presented  in Section N and contrasted with conventional approaches  in Section N
Section N shows how our method can be employed to tackle the tasks of image inpainting and separation, and later in Section N we demonstrate empirically our  algorithms
We conclude this work in Section N
 N
Convolutional sparse coding  The CSC model assumes that a global signal X can be  decomposed as X = ∑m  i=N di ∗ Γi, where di ∈ R n are  local filters that are convolved with their corresponding features maps (or sparse representations) Γi ∈ R N 
Alternatively, following Figure N, the above can be written in matrix  =  �i ∈ ℝ −  �i� ∈ ℝ  �i ∈ ℝ  � ∈ ℝ�×�� ∈ ℝ� � ∈ ℝ�  ⋮�� ∈ ℝ × � ∈ ℝ × −  Figure N: The CSC model and its constituent elements
 form as X = DΓ; where D ∈ RN×Nm is a banded con- volutional dictionary built from shifted versions of a local  matrix DL, containing the atoms {di} m i=N as its columns,  and Γ ∈ RNm is a global sparse representation obtained by interlacing the {Γi}  m i=N
In this setting, a patch RiX taken  from the global signal equals Ωγi, where Ω ∈ R n×(Nn−N)m  is a stripe dictionary and γi ∈ R (Nn−N)m is a stripe vector
 Here we defined Ri ∈ R n×N to be the operator that extracts  the i-th n-dimensional patch from X
 The work in [NN] suggested a theoretical analysis of  this global model, driven by a localized sparsity measure
 Therein, it was shown that if all the stripes γi are sparse,  the solution to the convolutional sparse pursuit problem is  unique and can be recovered by greedy algorithms, such as  the OMP [N], or convex formulations such as the Basis Pursuit (BP) [N]
This analysis was then extended in [NN] to a  noisy regime showing that, under similar sparsity assumptions, the global problem formulation and the pursuit algorithms are also stable
Herein, we leverage this local-global  relation from an algorithmic perspective, showing how one  can efficiently solve the convolutional sparse pursuit problem and train the dictionary (i.e., the filters) involved, while  only operating locally
 Note that the global sparse vector Γ can be broken into a  set of non-overlapping m-dimensional sparse vectors αNi=N,  which we call needles
The essence of the presented algorithm is in the observation that one can express the global  signal as X = ∑N  i=N R T i DLαi, where R  T i ∈ R  N×n is  the operator that puts DLαi in the i-th position and pads  the rest of the entries with zeros
Denoting by si the i-th  slice DLαi, we can write the above as X = ∑N  i=N R T i si
 It is important to stress that the slices do not correspond  to patches extracted from the signal, RiX, but rather to  much simpler entities
They represent only a fraction of  the i-th patch, since RiX = Ri ∑N  j=N R T j sj , i.e
a patch  is constructed from several overlapping slices
Unlike current works in signal and image processing, which train a  local dictionary on the patches {RiX} N i=N, in what follows  we define the learning problem with respect to the slices,  {si} N i=N, instead
In other words, we aim to train DL instead of Ω
As a motivation, we present in Figure N a set of  patches RiX extracted from natural images and their corresponding slices si, obtained from the proposed algorithm,  which will be presented in Section N
Indeed, one can observe that the slices are simpler than the patches, as they  contain less information
 NNNN    N
Proposed method: slice-based dictionary  learning  The convolutional dictionary learning problem refers to  the following optimizationN objective,  min D,Γ  N  N ‖X−DΓ‖NN + λ‖Γ‖N, (N)  for a convolutional dictionary D as in Figure N and a Lagrangian parameter λ that controls the sparsity level
Employing the decomposition of X in terms of its slices, and  the separability of the ℓN norm, the above can be written as  the following constrained minimization problem,  min DL,{αi}  N  i=N ,  {si} N  i=N  N  N ‖X−  N ∑  i=N  RTi si‖ N N + λ  N ∑  i=N  ‖αi‖N  s.t
si = DLαi
 (N)  One could tackle this problem using half-quadratic splitting  [NN] by introducing a penalty term over the violation of the  constraint and gradually increasing its importance
Alternatively, we can employ the ADMM algorithm [N] and solve  the augmented Lagrangian formulation (in its scaled form),  min DL,{αi}  N  i=N ,  {si} N  i=N ,{ui}  N  i=N  N  N ‖X−  N ∑  i=N  RTi si‖ N N  +  N ∑  i=N  (  λ‖αi‖N + ρ  N ‖si −DLαi + ui‖  N N  )  ,  (N)  where {ui} N i=N are the dual variables that enable the constrains to be met
 N.N
Local sparse coding and dictionary update  The minimization of Equation (N) with respect to all the  needles {αi} N i=N is separable, and can be addressed independently for every αi by leveraging standard tools such  as LARS
This also allows for having a different number  of non-zeros per slice, depending on the local complexity
 Similarly, the minimization with respect to DL can be done  using any patch-based dictionary learning algorithm such as  the K-SVD, MOD, online dictionary learning or trainlets
 Note that in the dictionary update stage, while minimizing  for DL, one could refrain from iterating until convergence,  and instead perform only a few iterations before proceeding  with the remaining variables
In addition, when employing  the K-SVD dictionary update, the {αi} are also updated subject to the support found in the sparse pursuit stage
 NHereafter, we assume that the atoms in the dictionary are normalized  to a unit ℓN norm
 N.N
Slice update via local Laplacian  The minimization of Equation (N) with respect to all the  slices {si} N i=N amounts to solving the quadratic problem  min {si}Ni=N  N  N ‖X−  N ∑  i=N  RTi si‖ N N +  ρ  N  N ∑  i=N  ‖si −DLαi + ui‖ N N
 (N)  Taking the derivative with respect to the variables  sN, sN, 


sN and nulling them, we obtain the following system of linear equations  RN(  N ∑  i=N  RTi si −X) + ρ(sN −DLαN + uN) = 0  ..
 RN (  N ∑  i=N  RTi si −X) + ρ(sN −DLαN + uN ) = 0
 (N)  Defining  R̄ =        RN ..
 RN       S̄ =        sN ..
 sN       Z̄ =        DLαN − uN ..
 DLαN − uN       , (N)  the above can be written as  0 = R̄ (  R̄T S̄−X )  + ρ (  S̄− Z̄ )  =⇒ S̄ = (  R̄R̄T + ρI )−N (  R̄X+ ρZ̄ )  
(N)  Using the Woodbury matrix identity and the fact that  R̄T R̄ = ∑N  i=N R T i Ri = nI, where I is the identity matrix, the above is equal to  S̄ =  (  N  ρ I−  N  ρN R̄  (  I+ N  ρ R̄T R̄  )−N  R̄T  )  (  R̄X+ ρZ̄ )  = (  I− R̄ (ρI+ nI) −N  R̄T )  (  N  ρ R̄X+ Z̄  )  
 (N)  Plugging the definitions of R̄, S̄ and Z̄, we obtain  si =  (  N  ρ RiX+DLαi − ui  )  −Ri      N  ρ+ n  N ∑  j=N  RTj  (  N  ρ RjX+DLαj − uj  )     
 (N0)  Although seemingly complicated at first glance, the above  is simple to interpret and implement in practice
This expression indicates that one should (i) compute the estimated  slices pi = N ρ RiX+DLαi − ui, then (ii) aggregate them  to obtain the global estimate X̂ = ∑N  j=N R T j pj , and finally  NNNN    Algorithm N: Slice-based dictionary learning  Input : Signal X, initial dictionary DL Output: Trained dictionary DL, needles {αi}  N i=N and  slices {si} N i=N  Initialization:  si = N  n RiX, ui = 0 (NN)  for iteration = N : T do  Local sparse pursuit (needle):  αi = argmin α̂i  ρ  N ‖si −DLα̂i + ui‖  N N + λ‖α̂i‖N  (NN)Slice reconstruction:  pi = N  ρ RiX+DLαi − ui (NN)  Slice aggregation:  X̂ =  N ∑  j=N  RTj pj (NN)  Slice update via local Laplacian:  si = pi − N  ρ+ n RiX̂ (NN)  Dual variable update:  ui = ui + si −DLαi (NN)  Dictionary update:  DL, {αi} N i=N = argmin  DL,{α̂i}Ni=N  N ∑  i=N  ‖si −DLα̂i + ui‖ N N  s.t
supp(α̂i) = supp(αi) (NN)end  (iii) subtract from pi the corresponding patch from the aggregated signal, i.e
RiX̂
As a remark, since this update  essentially subtracts from pi an averaged version of it, it  can be seen as a patch-based local Laplacian operation
 N.N
Boundary conditions  In the description of the CSC model (see Figure N), we  assumed for simplicity circulant boundary conditions
In  practice, however, natural signals such as images are in general not circulant and special treatment is needed for the  boundaries
One way of handling this issue is by assuming that X = MDΓ, where M ∈ RN×N+N(n−N) is matrix that crops the first and last n − N rows of the dictionary D (see Figure N)
The change needed in Algorithm N to incorporate M is minor
Indeed, one has to simply replace  the patch extraction operator Ri, with RiM T , where the  operator MT ∈ RN+N(n−N)×N pads a global signal with n − N zeros on the boundary and Ri extracts a patch from the result
In addition, one has to replace the patch placement operator RTi with MR T i , which simply puts the input  in the location of the i-th patch and then crops the result
 N.N
From patches to slices  The ADMM variant of the proposed algorithm, named  slice-based dictionary learning, is summarized in Algorithm N
While we have assumed the data corresponds to  one signal X, this can be easily extended to consider several signals
 At this point, a discussion regarding the relation between  this algorithm and standard (patch-based) dictionary learning techniques is in place
Indeed, from a quick glance  the two approaches seem very similar: Both perform local sparse pursuit on local patches extracted from the signal, then update the dictionary to represent these patches  better, and finally apply patch-averaging to obtain a global  estimate of the reconstructed signal
Moreover, both iterate  this process in a block-coordinate descent manner in order  to minimize the overall objective
So, what is the difference  between this algorithm and previous approaches?  The answer lies in the migration from patches to slices
 While originally dictionary learning algorithms aimed to  represent patches RiX taken from the signal, our scheme  suggests to train the dictionary to construct slices, which  do not necessarily reconstruct the patch fully
Instead, only  the summation of these slices results in the reconstructed  patches
To illustrate this relation, we show in Figure N  the decomposition of several patches in terms of their constituent slices
One can observe that although the slices are  simple in nature, they manage to construct the rather complex patches
The difference between this illustration and  that of Figure N is that the latter shows patches RiX and  only the slices that are fully contained in them
 Note that the slices are not mere auxiliary variables, but  rather emerge naturally from the convolutional formulation
After initializing these with patches from the signal,  Figure N: The first column contains patches extracted from  the training data, and second to eleventh columns are the  corresponding slices constructing these patches
Only the  ten slices with the highest energy are presented
 NNNN    si = N n RiX, each iteration progressively “carves” portions  from the patch via the local Laplacian, resulting in simpler  constructions
Eventually, these variables are guaranteed to  converge to DLαi – the slices we have defined
 Having established the similarities and differences between the traditional patch-based approach and the slice  alternative, one might wonder what is the advantage of  working with slices over patches
In the conventional approach, the patches are processed independently, ignoring  their overlap
In the slice-based case, however, the local  Laplacian forces the slices to communicate and reach a consensus on the reconstructed signal
Put differently, the CSC  offers a global model, while earlier patch-based methods  used local models without any holistic fusion of them
 N
Comparison to other methods  In this section we explain further the advantages of our  method, and compare it to standard algorithms for training  the CSC model such as [NN, NN]
Arguably the main difference resides in our localized treatment, as opposed to the  global Fourier domain processing
Our approach enables  the following benefits:  N
The sparse pursuit step can be done separately for each  slice and is therefore trivial to parallelize
 N
The algorithm can work in a complete online regime  where in each iteration it samples a random subset of  slices, solves a pursuit for these and then updates the  dictionary accordingly
Adopting a similar strategy in  the competing algorithms [NN, NN] might be problematic, since these are deployed in the Fourier domain on  global signals and it is therefore unclear how to operate  on a subset of local patches
 N
Our algorithm can be easily modified to allow a different number of non-zeros in each location of the global  signal
Such local complexity adaptation cannot be offered by the Fourier-oriented algorithms
 We now turn to comparing the proposed algorithm to  alternative methods in terms of computational complexity
 Denote by I the number of signals on which the dictionary  is trained, and by k the maximal number of non-zeros in a  needleN αi
At each iteration of our algorithm we employ  LARS that has a complexity of O(kN+mkN+nm) per slice [N0], resulting in O(IN(kN +mkN + nm) + nmN) compu- tations for all N slices in all the I images
The last term,  nmN, corresponds to the precomputation of the Gram of the  dictionary DL (which is in general negligible)
Then, given  the obtained needles, we reconstruct the slices, requiring  O(INnk), aggregate the results to form the global estimate,  NAlthough we solve the Lagrangian formulation of LARS, we also limit  the maximal number of non-zeros per needle to be at most k
 Method Time Complexity  [NN]  I < m mI  N N + (q − N)mIN  ︸ ︷︷ ︸  linear systems  + qImN log (N) ︸ ︷︷ ︸  FFT  + qImN ︸ ︷︷ ︸  thresholding  [NN]  I ≥ m m  N N + (q − N)m  N N  ︸ ︷︷ ︸  linear systems  + qImN log (N) ︸ ︷︷ ︸  FFT  + qImN ︸ ︷︷ ︸  thresholding  Ours INnm + IN(k N + mk  N )  ︸ ︷︷ ︸  LARS / OMP  +nm N  ︸ ︷︷ ︸  Gram  + INk(n + m) + nm N  ︸ ︷︷ ︸  K-SVD  Table N: Complexity analysis
For the convenience of the  reader, the dominant term is highlighted in red color
 incurring O(INn), and update the slices, which requires an additional O(INn)
These steps are negligible compared to the sparse pursuits and are thus omitted in the final expression
Finally, we update the dictionary using the K-SVD,  which is O(nmN + INkn + INkm) [NN]
We summarize the above in Table N
In addition, we present in the same  table the complexity of each iteration of the (Fourier-based)  algorithm in [NN]
In this case, q corresponds to the number of inner iterations in their ADMM solver of the sparse  pursuit and dictionary update
 The most computationally demanding step in our algorithm is the local sparse pursuit, which is O(NI(kN+mkN+ nm))
Assuming that the needles are very sparse, which indeed happens in all of our experiments, this reduces to  O(NImn)
On the other hand, the complexity in the algo- rithm of [NN] is dominated by the computation of the FFT,  which is O(NImq log(N))
We conclude that our algo- rithm scales linearly with the global dimension, while theirs  grows as N log(N)
Note that this also holds for other re- lated methods, such as that of [NN], which also depend on  the global FFT
Moreover, one should remember the fact  that in our scheme one might run the pursuits on a small  percentage of the total number of slices, meaning that in  practice our algorithm can scale as O(µNInm), where µ is a constant smaller than one
 N
Image processing via CSC  In this section, we demonstrate our proposed algorithm  on several image processing tasks
Note that the discussion thus far focused on one dimensional signals, however  it can be easily generalized to images by replacing the convolutional structure in the CSC model with block-circulant  circulant-block (BCCB) matrices
 N.N
Image inpainting  Assume an original image X is multiplied by a diagonal  binary matrix A ∈ RN×N , which masks the entries Xi in which A(i, i) = 0
In the task of image inpainting, given the corrupted image Y = AX, the goal is to restore the original unknown X
One can tackle this problem by solving the following CSC problem  min Γ  N  N ‖Y −ADΓ‖NN + λ‖Γ‖N, (NN)  NN00    where we assume the dictionary D was pretrained
Using  similar steps to those leading to Equation (N), the above can  be written as  min {αi}  N  i=N ,{si}  N  i=N ,  {ui} N  i=N  N  N ‖Y −A  N ∑  i=N  RTi si‖ N N  +  N ∑  i=N  (  λ‖αi‖N + ρ  N ‖si −DLαi + ui‖  N N  )  
 (NN)  This objective can be minimized via the algorithm described  in the previous section
Moreover, the minimization with  respect to the local sparse codes {αi} N i=N remains the same
 The only difference regards the update of the slices {si} N i=N,  in which case one obtains the following expression  si =  (  N  ρ RiY +DLαi − ui  )  −Ri      N  ρ+ n A  N ∑  j=N  RTj  (  N  ρ RjY +DLαj − uj  )     
 (N0)  The steps leading to the above equation are almost identical to those in subsection N.N, and they only differ in the  incorporation of the mask A
 N.N
Texture and cartoon separation  In this task the goal is to decompose an image X into  its texture component XT that contains highly oscillating  or pseudo-random patterns, and a cartoon part XC that is  a piece-wise smooth image
Many image separation algorithms tackle this problem by imposing a prior on both  components
For cartoon, one usually employs the isotropic  (or anisotropic) Total Variation norm, denoted by ‖XC‖TV 
The modeling of texture, on the other hand, is more difficult  and several approaches have been considered over the years  [NN, N, NN, NN]
 In this work, we propose to model the texture component using the CSC model
As such, the task of separation  amounts to solving the following problem  min DT ,ΓT ,  XC  N  N ‖X−DTΓT −XC‖  N N + λ ‖ΓT ‖N + ξ‖XC‖TV ,  (NN)  where DT is a convolutional (texture) dictionary, and ΓT is  its corresponding sparse vector
Using similar derivations  to those presented in Section N.N, the above is equivalent to  min DL,α  i  T ,si  T ,  XC ,ZC  N  N  ∥  ∥  ∥  ∥  ∥  X−  N ∑  i=N  RTi s i T −XC  ∥  ∥  ∥  ∥  ∥  N  N  +λ  N ∑  i=N  ∥  ∥α i T  ∥  ∥  N + ξ‖ZC‖TV  s.t
siT = DLα i T , XC = ZC ,  (NN)  where we split the variable XC into XC = ZC in order to facilitate the minimization over the TV norm
Its corresponding ADMM formulationN is given by  min DL,α  i  T ,si  T ,ui  T ,  XC ,ZC ,VC  N  N  ∥  ∥  ∥  ∥  ∥  X−  N ∑  i=N  RTi s i T −XC  ∥  ∥  ∥  ∥  ∥  N  N  +  N ∑  i=N  (ρ  N  ∥  ∥siT −DLα i T + u  i T  ∥  ∥  N  N + λ  ∥  ∥α i T  ∥  ∥  N  )  + η  N ‖XC − ZC +VC‖  N N + ξ‖ZC‖TV ,  (NN)  where {siT } N i=N, {α  i T }  N i=N and {u  i T }  N i=N are the texture  slices, needles and dual variables, respectively, and VC is  the dual variable of the global cartoon XC 
The above optimization problem can be minimized by slightly modifying  Algorithm N
The update for {αi} N i=N is a sparse pursuit and  the update for the ZC variable is a TV denoising problem
 Then, one can update the {siT } N i=N and XC jointly by  siT = N  ρ piT −  N ρ  N + n N  ρ + N  η  Ri      N  ρ  N ∑  j=N  RTj p j T +  N  η QC      XC = N  η QC −  N η  N + n N  ρ + N  η      N  ρ  N ∑  j=N  RTj p j T +  N  η QC     ,  (NN)  where piT = RiX + ρ (  DLα i T − u  i T  )  and QC = X + η (ZC −VC)
The final step of the algorithm is updat- ing the texture dictionary DL via any dictionary learning  method
 N
Experiments  We turn to demonstrate our proposed slice-based dictionary learning
Throughout the experiments we use the  LARS algorithm [N] to solve the LASSO problem and the  K-SVD [N] for the dictionary learning
The reader should  keep in mind, nevertheless, that one could use any other  pursuit or dictionary learning algorithm for the respective  updates
In all experiments, the number of filters trained are  N00 and they are of size NN× NN
 N.N
Slice-based dictionary learning  Following the test setting presented in [NN], we run our  proposed algorithm to solve Equation (N) with λ = N on the Fruit dataset [NN], which contains ten images
As in  [NN], the images were mean subtracted and contrast normalized
We present in Figure N the dictionary obtained  NDisregarding the training of the dictionary, this is a standard twofunction ADMM problem
The first set of variables are {si T }N i=N  and XC ,  and the second are {αi T }N i=N  and ZC 
 NN0N    after several iterations using our proposed slice-based dictionary learning, and compare it to the result in [NN] and  also to the method AVA-AMS in [NN]
Note that all three  methods handle the boundary conditions, which were discussed in Section N.N
We compare in Figure N the objective  of the three algorithms as function of time, showing that our  algorithm is more stable and also converges faster
In addition, to demonstrate one of the advantages of our scheme,  we train the dictionary on a small subset (N0%) of all slices and present the obtained result in the same figure
 (a) Proposed - Iteration N
(b) Proposed - Iteration N00
 (c) [NN]
(d) [NN]
 Figure N: The dictionary obtained after N and N00 iterations  using the slice-based dictionary learning method
Notice  how the atoms become crisper as the iterations progress
 For comparison, we present also the result of [NN] and [NN]
 Figure N: Our method versus the those in [NN] and [NN]
 N.N
Image inpainting  We turn to test our proposed algorithm on the task of image inpainting, as described in Section N.N
We follow the  experimental setting presented in [NN] and compare to their  state-of-the-art method using their publicly available code
 The dictionaries employed in both approaches are trained  on the Fruit dataset, as described in the previous subsection (see Figure N)
For a fair comparison, in the inference  stage, we tuned the parameter λ for both approaches
Table  N presents the results in terms of peak signal-to-noise ratio  (PSNR) on a set of publicly available standard test images,  showing our method leads to quantitatively better resultsN
 Figure N compares the two visually, showing our method  also leads to better qualitative results
 A common strategy in image restoration is to train the  dictionary on the corrupted image itself, as shown in [NN],  as opposed to employing a dictionary trained on a separate  collection of images
The algorithm presented in Section  N.N can be easily adapted to this framework by updating the  local dictionary on the slices obtained at every iteration
To  exemplify the benefits of this, we include the resultsN obtained by using this approach in Table N and Figure N
 N.N
Texture and cartoon separation  We conclude by applying our proposed slice-based dictionary learning algorithm to the task of texture and cartoon  separation
The TV denoiser used in the following experiments is the publicly available software of [N]
We run  our method on the synthetic image Sakura and a portion  extracted from Barbara, both taken from [NN], and on the  image Cat, originally from [NN]
For each of these, we compare with the corresponding methods
We present the results of all three experiments in Figure N, together with the  NThe PSNR is computed as N0 log( √ N/‖X−X̂‖N), where X and X̂  are the original and restored images
Since the images are normalized, the  range of PSNR values is non-standard
NA comparison with the method of [NN] was not possible in this case, as  their implementation cannot handle training a dictionary on standard-sized  images
 Figure N: Visual comparison on a cropped region extracted  from the image Barbara
Left: [NN] (PSNR = N.NNdB)
Middle: Ours (PSNR = N.NNdB)
Right: Ours with dictionary  trained on the corrupted image (PSNR = NN.NNdB)
 NN0N    Barbara Boat House Lena Peppers C.man Couple Finger Hill Man Montage  Heide et al
NN.00 N0.NN N0.NN NN.NN N.NN N.NN NN.NN NN.NN N0.NN NN.N0 NN.NN  Proposed NN.NN N0.NN N0.NN NN.NN N.NN N.NN NN.NN NN.0N N0.NN NN.NN NN.N0  Image specific NN.N0 NN.N0 NN.NN NN.NN NN.NN N0.NN NN.NN NN.0N N0.N0 NN.NN NN.NN  Table N: Comparison between the slice-based dictionary learning and the algorithm in [NN] on the task of image inpainting
 (a) Original image
(b) Enhanced output
 Figure N: Enhancement of the image Flower via cartoontexture separation
 trained dictionaries
Lastly, as an application for our texture separation algorithm, we enhance the image Flower by  multiplying its texture component by a scalar factor (greater  than one) and combining the result with the original image
 We treat the colored image by transforming it to the Lab  color space, manipulating the L channel, and finally transforming the result back to the original domain
The original  image and the obtained result are depicted in Figure N
One  can observe that our approach does not suffer from halos,  gradient reversals or other common enhancement artifacts
 N
Conclusion  In this work we proposed the slice-based dictionary  learning algorithm
Our method employs standard patchbased tools from the realm of sparsity to solve the global  CSC problem
We have shown the relation between our  method and the patch-averaging paradigm, clarifying the  main differences between the two: (i) the migration from  patches to the simpler entities called slices, and (ii) the application of a local Laplacian that results in a global consensus
Finally, we illustrated the advantages of the proposed  algorithm in a series of applications and compared it to related state-of-the-art methods
 N
Acknowledgments  The research leading to these results has received funding in part from the European Research Council under EU’s  Nth Framework Program, ERC under Grant NN0NNN, and in  part by Israel Science Foundation (ISF) grant no
NNN0/NN
 (a) Original
(b) Dictionary
(c) Original
(d) Dictionary
(e) Original
(f) Dictionary
 (g) Our cartoon
(h) Our texture
(i) Our cartoon
(j) Our texture
(k) Our cartoon
(l) Our texture
 (m) [NN]
(n) [NN]
(o) [NN]
(p) [NN]
(q) [NN]
(r) [NN]
 Figure N: Texture and cartoon separation for the images Sakura, Barbara and Cat
 NN0N    References  [N] M
Aharon, M
Elad, and A
Bruckstein
K-svd: An algorithm for designing overcomplete dictionaries for sparse  representation
IEEE Transactions on Signal Processing,  NN(NN):NNNN–NNNN, N00N
 [N] J.-F
Aujol, G
Gilboa, T
Chan, and S
Osher
Structuretexture image decompositionmodeling, algorithms, and parameter selection
International Journal of Computer Vision,  NN(N):NNN–NNN, N00N
 [N] S
Boyd, N
Parikh, E
Chu, B
Peleato, and J
Eckstein
 Distributed optimization and statistical learning via the alternating direction method of multipliers
Foundations and  Trends R© in Machine Learning, N(N):N–NNN, N0NN
 [N] H
Bristow, A
Eriksson, and S
Lucey
Fast convolutional  sparse coding
In Proceedings of the IEEE Conference on  Computer Vision and Pattern Recognition, pages NNN–NNN,  N0NN
 [N] S
H
Chan, R
Khoshabeh, K
B
Gibson, P
E
Gill, and T
Q
 Nguyen
An augmented lagrangian method for total variation  video restoration
IEEE Transactions on Image Processing,  N0(NN):N0NN–NNNN, N0NN
 [N] S
Chen, S
A
Billings, and W
Luo
Orthogonal least squares  methods and their application to non-linear system identification
International Journal of Control, N0(N):NNNN–NNNN,  NNNN
 [N] S
S
Chen, D
L
Donoho, and M
A
Saunders
Atomic  Decomposition by Basis Pursuit
SIAM Review, NN(N):NNN–  NNN, N00N
 [N] W
Dong, L
Zhang, G
Shi, and X
Wu
Image deblurring  and super-resolution by adaptive sparse domain selection and  adaptive regularization
IEEE Transactions on Image Processing, N0(N):NNNN–NNNN, N0NN
 [N] B
Efron, T
Hastie, I
Johnstone, R
Tibshirani, et al
Least  angle regression
The Annals of Statistics, NN(N):N0N–NNN,  N00N
 [N0] M
Elad and M
Aharon
Image denoising via sparse  and redundant representations over learned dictionaries
 IEEE Transactions on Image Processing, NN(NN):NNNN–NNNN,  N00N
 [NN] M
Elad and M
Aharon
Image denoising via sparse and  redundant representations over learned dictionaries
IEEE  Transactions on Image Processing, NN(NN):NNNN–NNNN, Dec
 N00N
 [NN] M
Elad, J.-L
Starck, P
Querre, and D
L
Donoho
Simultaneous cartoon and texture image inpainting using morphological component analysis (mca)
Applied and Computational Harmonic Analysis, NN(N):NN0–NNN, N00N
 [NN] K
Engan, S
O
Aase, and J
H
Husoy
Method of optimal  directions for frame design
In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP),  volume N, pages NNNN–NNNN
IEEE, NNNN
 [NN] D
Geman and C
Yang
Nonlinear image recovery with halfquadratic regularization
IEEE Transactions on Image Processing, N(N):NNN–NNN, NNNN
 [NN] R
Grosse, R
Raina, H
Kwong, and A
Y
Ng
ShiftInvariant Sparse Coding for Audio Classification
In Uncertainty in Artificial Intelligence, N00N
 [NN] S
Gu, W
Zuo, Q
Xie, D
Meng, X
Feng, and L
Zhang
 Convolutional sparse coding for image super-resolution
In  Proceedings of the IEEE International Conference on Computer Vision, pages NNNN–NNNN, N0NN
 [NN] F
Heide, W
Heidrich, and G
Wetzstein
Fast and flexible  convolutional sparse coding
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNNN–  NNNN
IEEE, N0NN
 [NN] F
Huang and A
Anandkumar
Convolutional dictionary  learning through tensor factorization
In Feature Extraction:  Modern Questions and Challenges, pages NNN–NNN, N0NN
 [NN] B
Kong and C
C
Fowlkes
Fast convolutional sparse coding (fcsc)
Department of Computer Science, University of  California, Irvine, Tech
Rep, N0NN
 [N0] J
Mairal, F
Bach, J
Ponce, et al
Sparse modeling for image  and vision processing
Foundations and Trends R© in Computer Graphics and Vision, N(N-N):NN–NNN, N0NN
 [NN] J
Mairal, F
Bach, J
Ponce, and G
Sapiro
Online dictionary  learning for sparse coding
In Proceedings of the NNth Annual  International Conference on Machine Learning, pages NNN–  NNN
ACM, N00N
 [NN] S
Ono, T
Miyata, and I
Yamada
Cartoon-texture image decomposition using blockwise low-rank texture characterization
IEEE Transactions on Image Processing, NN(N):NNNN–  NNNN, N0NN
 [NN] V
Papyan, J
Sulam, and M
Elad
Working locally thinking globally-part I: Theoretical guarantees for convolutional  sparse coding
arXiv preprint arXiv:NN0N.0N00N, N0NN
 [NN] V
Papyan, J
Sulam, and M
Elad
Working locally thinking  globally-part II: Stability and algorithms for convolutional  sparse coding
arXiv preprint arXiv:NN0N.0N00N, N0NN
 [NN] R
Rubinstein, M
Zibulevsky, and M
Elad
Efficient implementation of the k-svd algorithm using batch orthogonal  matching pursuit
Cs Technion, N0(N):N–NN, N00N
 [NN] J
Sulam, B
Ophir, M
Zibulevsky, and M
Elad
Trainlets:  Dictionary learning in high dimensions
IEEE Transactions  on Signal Processing, NN(NN):NNN0–NNNN, N0NN
 [NN] B
Wohlberg
Efficient convolutional sparse coding
In IEEE  International Conference on Acoustics, Speech and Signal  Processing (ICASSP), pages NNNN–NNNN
IEEE, N0NN
 [NN] B
Wohlberg
Boundary handling for convolutional sparse  representations
In Image Processing (ICIP), N0NN IEEE International Conference on, pages NNNN–NNNN
IEEE, N0NN
 [NN] J
Wright, A
Y
Yang, A
Ganesh, S
S
Sastry, and Y
Ma
 Robust face recognition via sparse representation
IEEE  Transactions on Pattern Analysis and Machine Intelligence,  NN(N):NN0–NNN, N00N
 [N0] J
Yang, J
Wright, T
S
Huang, and Y
Ma
Image superresolution via sparse representation
IEEE Transactions on  Image Processing, NN(NN):NNNN–NNNN, N0N0
 [NN] M
D
Zeiler, D
Krishnan, G
W
Taylor, and R
Fergus
Deconvolutional networks
In IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), pages NNNN–NNNN
 IEEE, N0N0
 [NN] H
Zhang and V
M
Patel
Convolutional sparse codingbased image decomposition
In BMVC, N0NN
 NN0NTALL: Temporal Activity Localization via Language Query   TALL: Temporal Activity Localization via Language Query  Jiyang GaoN Chen SunN Zhenheng YangN Ram NevatiaN  NUniversity of Southern California NGoogle Research  {jiyangga, zhenheny, nevatia}@usc.edu, chensun@google.com  Abstract  This paper focuses on temporal localization of actions  in untrimmed videos
Existing methods typically train classifiers for a pre-defined list of actions and apply them in  a sliding window fashion
However, activities in the wild  consist of a wide combination of actors, actions and objects; it is difficult to design a proper activity list that meets  users’ needs
We propose to localize activities by natural  language queries
Temporal Activity Localization via Language (TALL) is challenging as it requires: (N) suitable design of text and video representations to allow cross-modal  matching of actions and language queries; (N) ability to locate actions accurately given features from sliding windows  of limited granularity
We propose a novel Cross-modal  Temporal Regression Localizer (CTRL) to jointly model text  query and video clips, output alignment scores and action  boundary regression results for candidate clips
For evaluation, we adopt TaCoS dataset, and build a new dataset for  this task on top of Charades by adding sentence temporal  annotations, called Charades-STA
We also build complex  sentence queries in Charades-STA for test
Experimental  results show that CTRL outperforms previous methods significantly on both datasets
 N
Introduction  Activities in the wild consist of a diverse combination  of actors, actions and objects over various periods of time
 Earlier work focused on classification of video clips that  contained a single activity, i.e
where the videos were  trimmed
Recently, there has also been significant work in  localizing activities in longer, untrimmed videos [N0, NN]
 One major limitation of existing action localization methods is that they are restricted to pre-defined list of actions
 Although the lists of activities can be relatively large [N],  they still face difficulty in covering complex activity questions, for example, “A person runs to the window and then  look out.” , as shown in Figure N
Hence, it is desirable to  use natural language queries to localize activities
Use of  natural language not only allows for an open set of activiN.N s NN.N s  Language Query: A person runs to the window and then look out  Figure N
Temporal activity localization via language query in an  untrimmed video
 ties but also natural specification of additional constraints,  including objects and their properties as well as relations  between the involved entities
We propose the task of Temporal Activity Localization via Language (TALL): given a  temporally untrimmed video and a natural language query,  the goal is to determine the start and end times for the described activity inside the video
 For traditional temporal action localization, most current  approaches [N0, NN, NN, NN, NN] apply activity classifiers  trained with optical flow-based methods [NN, NN] or Convolutional Neural Networks (CNNs) [NN, NN] in a sliding  window fashion
A direct extension to support natural language query is to map the queries into a discrete label space
 However, it is non-trivial to design a label space which has  enough coverage for such activities without losing useful  details in users’ queries
 To go beyond discrete activity labels, one possible solution is to embed visual features and sentence features into  a common space [N0, NN, NN]
However, for temporal localization of activities, it is unclear what a proper visual  model to extract visual features for retrieval is, and how  to achieve high precision of predicted start/end time
Although one could densely sample sliding windows at different scales, doing so is not only computationally expensive but also makes the alignment task more challenging,  as the search space increases
An alternative to dense sampling is to adjust the temporal boundaries of proposals by  learning regression parameters; such an approach has been  successful for object localization, as in [NN]
However, temNNNN    poral regression has not been attempted in the past work  and is more difficult as the activities are characterized by  a spatio-temporal volume, which may lead to more background noise
 These challenges motivate us to propose a novel Crossmodal Temporal Regression Localizer (CTRL) model to  jointly model text query, video clip candidates and their  temporal context information to solve the TALL task
 CTRL generates alignment scores along with location regression results for candidate clips
It utilizes a CNN model  to extract visual features of the clips and a Long Shortterm Memory (LSTM) network to extract sentence embeddings
A cross-modal processing module is designed to  jointly model the text and visual features, which calculates  element-wise addition, multiplication and direct concatenation
Finally, multilayer networks are trained for visualsemantic alignment and clip location regression
We design  the non-parameterized and parameterized location offsets  for temporal coordinate regression
In parameterized setting, the length and the central coordinate of the clip is first  parameterized by the ground truth length and coordinate
 In non-parameterized setting, the start and end coordinates  are used directly
We show that the non-parameterized one  works better, unlike the case for object boundary regression
 To facilitate research of TALL, we also generate sentence temporal annotations for Charades [NN] dataset
 We name it Charades-STA.We evaluate our methods on  TACoS and Charades-STA datasets by the metric of “R@n,  IoU=m”, which represents the percentage of at least one of  the top-n results ( start and end pairs ) having IoU with the  ground truth larger than m
Experimental results demonstrate the effectiveness of our proposed CTRL framework
 In summary, our contributions are two-fold:  (N) We propose a novel problem formulation of Temporal  Activity Localization via natural Language (TALL) query
 (N) We introduce an effective Cross-modal Temporal  Regression Localizer (CTRL) which estimates alignment  scores and temporal action boundary by jointly modeling  language query and video clips.N  N
Related Work  Action classification and temporal localization
There  have been tremendous explorations about action classification in videos using deep convolutional neural networks  (ConvNets)
Representative methods include two-stream  ConvNets, CND (ND ConvNets) and ND ConvNets with  temporal LSTM or mean pooling
Specifically, Simonyan  and Zisserman [NN] modeled the appearance and motion  information in two separate ConvNets and combined the  scores by late fusion
Tran et al
[NN] used ND convolutional filters to capture motion information in neighboring  NSource codes are available in https://github.com/jiyanggao/TALL 
 frames
[NN] [N0] proposed to use ND ConvNets to extract  deep features for one frame and use temporal mean pooling  or LSTM to model temporal information
 For temporal action localization task, Shou et al
[NN]  trained CND [NN] with localization loss and achieved stateof-the-art performance on THUMOS NN
Ma et al
[NN]  used a temporal LSTM to generate frame-wise prediction  scores and then merged the detection intervals based on  the predictions
Singh et al
[N0] extended two-stream  [NN] framework with person detection and bi-directional  LSTMs and achieved state-of-the-art performance on MPIICooking dataset [NN]
Gao et al
[N] proposed to use temporal coordinate regression to refine action boundary for temporal localization
 Sentence-based image/video retrieval
Given a set of  candidate videos/images and a sentence query, this task requires retrieving the videos/images that match the query
 Karpathy et al
[N] proposed Deep Visual-Semantic Alignment (DVSA) model
DVSA used bidirectional LSTMs to  encode sentence embeddings, and R-CNN object detectors  [N] to extract features from object proposals
Skip-thought  [NN] learned a SentNVec model by applying skip-gram [NN]  on sentence level and achieved top performance in sentencebased image retrieval task
Sun et al
[NN] proposed to discover visual concepts from image-sentence pairs and apply  the concept detectors for image retrieval
Gao et al
[N]  proposed to learn verb-object pairs as action concepts from  image-sentence pairs
Hu et al
[N] and Mao et al
[NN]  formulated the problem of natural language object retrieval
 As for video retrieval, Lin et al
[NN] parsed the sentence descriptions into a semantic graph, which are then  matched to visual concepts in the videos by generalized bipartite matching
Bojanowski et al
[N] tackled the problem  of video-text alignment: given a video and a set of sentences  with temporal ordering, assigning a temporal interval for  each sentence
In our settings, only one sentence query is  input to the system and temporal ordering is not used
 Object detection
Our work is partly inspired by the  success of recent object detection approaches
R-CNN [N]  consists of selective search, CNN feature extraction, SVM  classification and bounding box regression
Fast-RCNN [N]  designs RoI pooling layer and the model could be trained  by end-to-end framework
One of the key element shared  in those successful object detection frameworks [NN, NN, N]  is the bounding box regression layer
We show that, unlike object boundary regression using parameterized offsets,  non-parameterized offsets work better for action boundary  regression
 N
Methods  In this section, we describe our Cross-modal Temporal  Regression Localizer (CTRL) for Temporal Activity Localization via Language (TALL) and training procedure in deNNNN    Sentence  Query  Sentence Embedding  Skip‐Thoughts  LSTM Word  Embedding  OR  Clip­level feature extractor  multi­modal  Processing concatenation  FC  FC FC  alignment  score  location  regressor  FC  Add  Mul  pooling  Visual Encoder  �" #  �"  �$  �$ %&'  �$ ()"& ,%&+  ��-,./  �-,/  Sentence Encoder  �"$  �$ (0N ,%&+  Figure N
Cross-modal Temporal Regression Localizer (CTRL) architecture
CTRL contains four modules: a visual encoder to extract  features for video clips, a sentence encoder to extract embeddings, a multi-modal processing network to generate combined representations  for visual and text domain, and a temporal regression network to produce alignment scores and location offsets
 tail
CTRL contains four modules: a visual encoder to extract features for video clips, a sentence encoder to extract  embeddings, a multi-modal processing network to generate  combined representations for visual and text domain, and  a temporal regression network to produce alignment scores  and location offsets between the input sentence query and  video clips
 N.N
Problem Formulation  We denote a video as V = {ft} T t=N, T is the frame number of the video
Each video is associated with temporal  sentence annotations: A = {(sj , τ s j , τ  e j )}  M j=N, M is the sentence annotation number of the video V , sj is a natural lan- guage sentence of a video clip, which has τ sj and τ  e j as start  and end time in the video
The training data are the sentence and video clip pairs
The task is to predict one or more  (τ sj , τ e j ) for the input natural language sentence query
 N.N
CTRL Architecture  Visual Encoder
For a long untrimmed video V , we gen- erate a set of video clips C = {(ci, t  s i , t  e i )}  H i=N by temporal  sliding windows, where H is the total number of the clips of the video V , tsi and t  e i are the start and end time of video  clip ci
We define visual encoder as a function Fve(ci) that maps a certain clip ci and its context to a feature vector fv , whose dimension is ds
Inside the visual encoder, a fea- ture extractor Ev is used to extract clip-level feature vec- tors, whose input is nf frames and output is a vector with dimension dv 
For one video clip ci, we consider itself (as the central clip) and its surrounding clips (as context clips)  ci,q, q ∈ [−n, n], j is the clip shift, n is the shift boundary
 We uniformly sample nf frames from each clip (central and context clips)
The feature vector of central clip is denoted  as f ctlv 
For the context clips, we use a pooling layer to calculate a pre-context feature fprev = N n  ∑−N q=−n Ev(ci,q) and  post-context feature fpostv = N n  ∑n q=N Ev(ci,q)
Pre-context  feature and post-context feature are pooled separately, as  the end and the start of an activity can be quite different and  both could be critical for temporal localization
fprev , f ctl v  and fpostv are concatenated and then linearly transformed to the feature vector fv with dimension ds, as the visual repre- sentation for clip ci
 Sentence Encoder
A sentence encoder is a function  Fse(sj) that maps a sentence description sj to a embedding space, whose dimension is ds( the same as visual feature space )
Specifically, a sentence embedding extractor Es is used to extract a sentence-level embedding f ′s and is fol- lowed by a linear transformation layer, which maps f ′s to fs with dimension ds, the same as visual representation fv 
We experiment two kinds of sentence embedding extractors,  one is a LSTM network which takes a word as input at each  step, and the hidden state of final step is used as sentencelevel embedding; the other is an off-the-shelf sentence encoder, Skip-thought [NN]
More details would be discussed  in Section N
 Multi-modal Processing Module
The inputs of the  multi-modal processing module are a visual representation  fv and a sentence embedding fs, which have the same di- mension ds
We use vector element-wise addition (+), vec- tor element-wise multiplication (×) and vector concatena- tion (‖) followed by a Fully Connected (FC) layer to com- bine the information from both modalities
Addition and  NNNN    multiplication operation allow additive and multiplicative  interaction between two modalities and don’t change the  feature dimension
The FC layer allows interaction among all elements
The input dimension of the FC layer is N ∗ ds and the output dimension is ds
The outputs from all three operations are concatenated to construct a multi-modal representation fsv = (fs × fv) ‖ (fs + fv) ‖ FC(fs ‖ fv), which is the input for our core module, temporal localization regression networks
 Temporal Localization Regression Networks
Temporal localization regression network takes the multi-modal  representation fsv as input, and has two sibling output lay- ers
The first one outputs an alignment score csi,j between the sentence sj and the video clip ci
The second one out- puts clip location regression offsets
We design two location  offsets, the first one is parameterized offset: t = (tc, tl), where tc and tl are parameterized central point offset and length offset respectively
The parameterization is as follows:  tp = (p− pc)/lc, tl = log(l/lc) (N)  where p and l denote the clip’s center coordinate and clip length respectively
Variables p, pc are for predicted clip and test clip (like wise for l)
The second offset is non- parameterized offset: t = (ts, te), where ts and te are the start and end point offsets
 ts = s− sc, te = e− ec (N)  where s and e denote the clip’s start and end coordinate re- spectively
Temporal coordinate regression can be thought  as clip location regression from a test clip to a nearby  ground-truth clip, as the original clip could be either too  tight or too loose, the regression process tend to find better  locations
 N.N
CTRL Training  Multi-task Loss Function
CTRL contains two sibling  output layers, one for alignment and the other for regression
We design a multi-task loss L on a mini-batch of training samples to jointly train for visual-semantic alignment and clip location regression
 L = Laln + αLreg (N)  where Laln is for visual-semantic alignment and Lreg is for clip location regression, and α is a hyper-parameter, which controls the balance between the two task losses
The alignment loss encourages aligned clip-sentence pairs to have  positive scores and misaligned pairs to have negative scores
 Laln = N  N  N∑  i=0  [αclog(N + exp(−csi,i))+  N∑  j=0,j N=i  αwlog(N + exp(csi,j))] (N)  clip c  �" �#  Intersection  Union  Non Intersection  Length  Figure N
Intersection over Union (IoU) and non-Intersection over  Length (nIoL)
 where N is the batch size, csi,j is the alignment score be- tween sentence sj and video clip ci, αc and αw are the hy- per parameters which control the weights between positive  ( aligned ) and negative ( misaligned ) clip-sentence pairs
 The regression loss Lreg is calculated for the aligned clip-sentence pairs
A sentence sj annotation contains start and end time (τ sj , τ  e j )
The aligned sliding window clip ci  has (tsi , t e i )
The ground truth offsets t  ∗ are calculated from  start and end times
 Lreg = N  N  N∑  i=0  [R(t∗x,i − tx,i) +R(t ∗ y,i − ty,i)] (N)  where x and y indicate p and l for parameterized offsets, or s and e for non-parameterized offsets
R(t) is smooth LN function
 Sampling Training Examples
To collect training samples, we use multi-scale temporal sliding windows with  [NN, NNN, NNN, NNN] frames and N0% overlap
(Note that,  at test time, we only use coarsely sampled clips.) We  use the following strategy to collect training samples T = {[(sh, τ  s h, τ  e h), (ch, t  s h, t  e h)]}  NT h=0
Each training sample contains a sentence description (sh, τ s h, τ  e h) and a video clip  (ch, t s h, t  e h)
For a sliding window clip c from C with temporal annotation (ts, te) and a sentence description s with temporal annotation (τ s, τe), we align them as a pair of training samples if they satisfy (N) Intersection over Union  (IoU) is larger than 0.N; (N) non Intersection over Length  (nIoL) is smaller than 0.N and (N) one sliding window clip  can be aligned with only one sentence description
The reason we use nIoL is that we want the the most part of the  sliding window clip to overlap with the assigned sentence,  and simply increasing IoU threshold would harm regression  layers ( regression aims to move the clip from low IoU to  high IoU)
As shown in Figure N, although the IoU between  c and sN is about 0.N, if we assign c to sN, then it will disturb the model ,because c contains information of sN
 N
Evaluation  In this section, we describe the evaluation settings and  discuss the experiment results  NNN0    N.N
Datasets  TACoS [NN]
This dataset was built on the top of MPIICompositive dataset [NN] and contains NNN videos
Every  video is associated with two type of annotations
The first  one is fine-grained activity labels with temporal location  (start and end time)
The second set of annotations is natural  language descriptions with temporal locations
The natural language descriptions were obtained by crowd-sourcing  annotators, who were asked to describe the content of the  video clips by sentences
In total, there are NNNNN pairs of  sentence and video clips
We split it in N0% for training,  NN% for validation and NN% for test
 Charades-STA
Charades [NN] contains around N0k  videos and each video contains temporal activity annotation (from NNN activity categories) and multiple video-level  descriptions
TALL needs clip-level sentence annotation:  sentence descriptions with start and end time, which are  not provided in the original Charades dataset
We noticed  that the names of activity categories in Charades are parsed  from the video-level descriptions, so many of activity names  appear in descriptions
Another observation we make is  that most descriptions in Charades share a similar syntactic structure: consisting of multiple sub-sentences, which  are connected by comma, period and conjunctions, such as  “then”, “while”, “after”, “and”
For example, “A person is  sitting down by the door
They stand up and start carefully  leaving some dishes in the sink”
 Based on these observations, we designed a semiautomatic way to generate sentence temporal annotation
 The first step is sentence decomposition: a long sentence  is split to sub-sentences by a set of conjunctions (which are  collected by hand ), and for each sub-sentence, the subject (  parsed by Stanford CoreNLP [NN] ) of the original long sentence is added to start
The second step is keyword matching: we extract keywords for each activity categories and  match them to sub-sentences, if they are matched, the temporal annotation (start and end time) are assigned to the subsentences
The third step is a human check: for each pair  of sub-sentence and temporal annotation, we (two of the  co-authors) checked whether the sentence made sense and  whether they matched the activity annotation
An example  is shown in Figure N
 Although TACoS and Charades-STA are challenging,  their lengths of queries are limited to single sentences
 To explore the potential of CTRL framework on handling  longer and more complex sentences, we build a complex  sentence set
Inside each video, we connect consecutive  sub-sentences to make complex query, each complex query  contains at least two sub-sentences, and is checked to make  sure that the time span is less than half of the video length
 We use them for test purpose only
In total, there are NNNNN  clip-sentence pairs in Charades-STA training set, NNNN clipsentence pairs in test set and NNNN complex sentence quires
 Sit Stand Up  Video  Activity Annotation  Sentence  Sub­sentences  decomposition Sub Sub  keyword matching  A person is sitting down by the  door. They stand up and start   carefully leaving some dishes in   the sink
 Sub 0:A person is sitting down by the door.  Sub N: They stand up
 Sub N: They start carefully leaving some   dishes in the sink  [N.N, N.N]: Stand up  [N.N, N.N]: Sit  Sentence  Activity Annotation STA  Sub­Sentences  [N.N, N.N]: A person is sitting   down by the door  [N.N, N.N]: They stand up
 Figure N
Charades-STA construction
 On average, there are N.N words per non-complex sentence,  and NN.N words per complex sentence
 N.N
Experiment Settings  We will introduce evaluation metric, baseline methods  and our system variants in this part
 N.N.N Evaluation Metric  We adopted a similar metric used by [N] to compute “R@n, IoU=m”, which means that the percentage of at least one of the top-n results having Intersection over Union (IoU) larger than m
This metric itself is on sentence level, so the overall performance is the average among all the sentences
R(n,m) = NN ∑N  i=N r(n,m, si), where r(n,m, si) is the recall for a query si, N is total number of queries and R(n,m) is the averaged overall performance
 N.N.N Baseline Methods  We consider two sentence based image/video retrieval  baseline methods: visual-semantic alignment with LSTM  (VSA-RNN ) [N] and visual-semantic alignment with Skipthought vector (VSA-STV) [NN]
For these two baseline  methods, we use the same training samples and test sliding  windows as those for CTRL
 VSA-RNN
This baseline method is similar to the model  in DVSA [N]
We use a regular LSTM instead of BRNN  to encode the input description
The size of hidden state  of LSTM is N0NN and the output size is N000
Video  clips are processed by a CND network that is pre-trained  on SportsNM [N0]
The N0NN dimensional fcN vector is extracted and linearly transformed to N000 dimensional,  which is used as the clip-level feature
Cosine similarity  is used to calculate the confidence score between the clip  and the sentence
Hinge loss is used to train the model
 NNNN    At test time, we compute the alignment score between input sentence query and all the sliding windows in the video
 VSA-STV: Instead of using RNN to extract sentence embedding, we use an off-the-shelf Skip-thought [NN] sentence  embedding extractor
A skip-thought vector is NN00 dimensional, we linearly transform it to N000 dimensional
Visual  encoder is the same as for VSA-RNN
 Verb and Object Classifiers
We also implemented  baseline methods based on annotations of pre-defined actions and objects
TACoS dataset also contains pre-defined  actions and object annotations at clip-level
These objects and actions annotations are from the original MPIICompositive dataset [NN]
NN categories of actions and NN  categories of objects are involved in training set
We use  the same CND feature as above to train action classifiers and  object classifiers
The classifier is based on a N-layer fully  connected network, the size of first layer is N0NN and the  size of second layer is the number of categories
The test  sentences are parsed by Stanford CoreNLP [NN], and verbobject (VO) pairs are extracted using the sentence dependencies
The VO pairs are matched with action and object  annotations based on string matching
The alignment score  between a sentence query and a clip is the score of matched  action and object classifier responses
Verb means that we  only use action classifier; Verb+Obj means that both action  classifiers and object classifiers are used
 N.N.N System Variants  We experimented with variants of our system to test the effectiveness of our method
CTRL(aln): we don’t use regression, train the CTRL with only alignment loss Laln
CTRL(reg-p): train the CTRL with alignment loss Laln and parameterized regression loss Lreg−p
CTRL(reg-np): context information is considered and CTRL is trained with  alignment loss Laln and non-parameterized regression loss Lreg−np
CTRL(loc): SCNN [NN] proposed to use overlap loss to improve activity localization performance
Based on  our pure alignment(without regression), we implemented a  similar loss function considering clip overlap as in SCNN
 Lloc = ∑n  i (0.N∗( N/(N+e−csi,i )N  IoUi −N)), where csi,i and IoUi  are respectively the alignment score and Intersection over  Union (IoU) between the aligned pairs of sentence and clip  in a mini-batch
The major difference is that SCNN solved a  classification problem, so they use Softmax score, however  in our case, we consider an alignment problem
The overall loss function is Lscnn = Laln + Lloc
For this method, we use CND as the visual encoder and Skip-thought as the  sentence encoder
 N.N
Experiments on TACoS  In this part, we discuss the experiment results on TACoS
 First we compare the performance of different visual en0.N 0.N 0.N 0.N 0.N Intersection over Union (IoU)  0.0  0.N  0.N  0.N  0.N  0.N  0.N  R e ca  ll  R@N0  R@N  R@N  Comparison of Visual Features  CND VGG MeanPooling LRCN  Figure N
Performance comparison of different visual encoders
 coders; second we compare two sentence embedding methods; third we compare the performance of CTRL variants  and baseline methods
The length of sliding windows for  test is NNN with overlap 0.N, multi-scale windows are not  used
We empirically set the context clip number n as N and the length of context window as NNN frames
The dimension of fv , fs and fsv are all set to N000
We set batch size as NN, the networks are optimized by Adam [NN] optimizer on  a Nvidia TITAN X GPU
 Comparison of visual features
We consider three cliplevel visual encoders: CND [NN], LRCN [N], VGG+Mean  Pooling [N0]
Each of them takes a clip with NN frames as  input and outputs a N000-dimensional feature vector
For  CND, fcN feature vector is extracted and then linearly trans- formed to N000-dimension
For LRCN and VGG poolng,  we extract fcN of VGG-NN for each frame
The LSTM’s hidden state size is NNN.We use Skip-thought as the sentence embedding extractor and other parts of the model  are the same to CTRL(aln)
There are three groups of  curves, which are Recall@N0, Recall@N and Recall@N respectively, shown in Figure
N
We can see that CND performs generally better than other two methods
LRCN’s  performance is inferior, the reason maybe that the dataset is  relatively small, not enough to train the LSTM well
 Comparison of sentence embedding
For sentence  encoder, we consider two commonly used methods:  wordNvec+LSTM [N] and Skip-thought [NN]
In our implementation of wordNvec, we train skip-gram model on English Dump of Wikipedia
The dimension of the word vector is N00 and the hidden state size of the LSTM is NNN
For  Skip-thought vector, we linearly transform it from NN00dimension to N000-dimension
We use CND as the visual  feature extractor and other parts are the same to CTRL(aln)
 From the results, we can see that the performance of Skipthought is generally better than wordNvec+LSTM
We conjecture the reason is that the scale of TACoS is not large  enough to train the LSTM (comparing with the counterpart  NNNN    0.N 0.N 0.N 0.N 0.N Intersection over Union (IoU)  0.00  0.0N  0.N0  0.NN  0.N0  0.NN  0.N0  R e ca  ll@ N  Comparison of Sentence Embedding  Skip-thought LSTM  Figure N
Performance comparison of different sentence embedding
 datasets in object detection, like ReferIt [NN], FlickrN0k Entities [N0], which contains over N00k sentences)
 Comparison with other methods
We test our system  variants and baseline methods on TACoS and report the result for IoU ∈ {0.N, 0.N, 0.N} and Recall@{N, N}
The results are shown in Table N
“Random” means that we  randomly select n windows from the test sliding windows and evaluate Recall@n with IoU=m
All methods use the same CND features
VSA-RNN uses the end-to-end trained  LSTM as the sentence encoder and all other methods use  pre-trained Skip-thought as sentence embedding extractor
 We can see that visual retrieval baselines (i.e
VSARNN, VSA-STV) lead to inferior performance, even compared with our pure alignment model CTRL(aln)
We believe the major reasons are two-fold: N) the multilayer alignment network learns better alignment than the simple cosine  similarity model, which is trained by hinge loss function; N)  visual retrieval models do not encode temporal context information in a video
Pre-defined classifiers also produce  inferior results
We think it is mainly because the predefined actions and objects are not precise enough to represent sentence queries
By comparing Verb and Verb+Obj,  we can see that additional object (such as “knife”, “egg”)  information helps to represent sentence queries
 Temporal action boundary regression As described  before, we implemented a temporal localization loss function similar to the one in SCNN [NN], which consider clip  overlap
Experiment results show that CTRL(loc) does  not bring much improvement over CTRL(aln), perhaps because CTRL(loc) still relies on clip selection from sliding  windows, which may not overlap with ground truth well
 CTRL(reg-np) outperforms CTRL(aln) and CTRL(loc) significantly, showing the effectiveness of temporal regression  model
By comparing CTRL(reg-p) and CTRL(reg-np) in  Table N, it can be seen that non-parameterized setting helps  the localizer regress the action boundary to a more accurate  location
We think the reason is that unlike objects can be  re-scaled in images due to camera projection, actions’ time  spans can not be easily rescaled in videos (we don’t consider  slow motion and quick motion)
Thus, to do the boundary  regression effectively, the object bounding box coordinates  Table N
Comparison of different methods on TACoS  Method R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  Random 0.NN N.NN N.NN N.NN N.0N NN.0N  Verb N.NN N.NN N.NN N.NN N.NN NN.NN  Verb+Obj N.NN NN.NN NN.NN NN.NN NN.N0 NN.N0  VSA-RNN N.NN N.NN N.NN N.N0 NN.N0 NN.0N  VSA-STV N.NN N0.NN NN.0N NN.N0 NN.NN NN.NN  CTRL (aln) N0.NN NN.NN NN.NN NN.NN NN.0N NN.0N  CTRL (loc) N0.N0 NN.NN NN.NN NN.NN NN.N0 NN.NN  CTRL (reg-p) NN.NN NN.NN NN.NN NN.0N NN.NN NN.NN  CTRL (reg-np) NN.N0 NN.NN NN.NN NN.NN NN.NN NN.NN  Table N
Comparison of different methods on Charades-STA  Method R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  Random N.NN N.0N NN.NN NN.0N  VSA-RNN N0.N0 N.NN NN.NN N0.NN  VSA-STV NN.NN N.NN NN.NN NN.NN  CTRL (aln) NN.NN N.NN NN.NN NN.NN  CTRL (loc) N0.NN N.NN NN.NN NN.NN  CTRL (reg-p) NN.NN N.NN NN.NN NN.NN  CTRL (reg-np) NN.NN N.NN NN.NN NN.NN  Table N
Experiments of complex sentence query
 Method R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  R@N  IoU=0.N  Random NN.NN N.NN NN.NN NN.NN  CTRL NN.0N N.0N NN.NN NN.NN  CTRL+Fusion NN.NN N.NN NN.NN NN.NN  should be first normalized to some standard scale, but for  actions, time itself is the standard scale
 Some prediction and regression results are shown in Figure N
We can see that the alignment prediction gives  a coarse location, which is limited by the fixed window  length; the regression model helps to refine the clip’s bounding box to a higher IoU location
 N.N
Experiments on Charades-STA  In this part, we evaluate CTRL models and baseline  methods on Charades-STA and report the results for IoU ∈ {0.N, 0.N} and Recall@{N, N}, which are shown in Table N
The lengths of sliding windows for test are NNN and NNN,  window’s overlap is 0.N
It can be seen that the results  are consistent with those in TACoS
CTRL(reg-np) shows  a significant improvement over CTRL(aln) and CTRL(loc)
 The non-parameterized settings (CTRL(reg-np)) work consistently better than the parameterized settings (CTRL(regp))
Figure N shows some prediction and regression results
 We also test complex sentence query on Charades-STA
 As shown in Table
N, “CTRL” means that we simply input the whole complex sentence into CTRL model
 “CTRL+fusion” means that we input each sentence of a  complex query separately into CTRL, and then do a late fusion
Specifically, we compute the average alignment score  NNNN    ground truth  alignment prediction  regression refinement  NN.0 s NN.N s  NN.N s N0.N s  NN.0 s NN.N s  Query: He gets a cutting board and knife
 Query: The person sets up two separate glasses on the counter
 NN.0 s NN.N s  NN.N s NN.0 s  NN.N s NN.N s  ground truth  alignment prediction  regression refinement  Figure N
Alignment prediction and regression refinement examples in TACoS
The row with gray background shows the ground truth for  the given query; the row with blue background shows the sliding window alignment results; the row with green background shows the clip  regression results
 ground truth  alignment prediction  regression refinement  Query:A person runs to the window and then look out  Complex Query:A person is walking around the room
She is eating something  ground truth N.0 s NN.N s  alignment prediction N.N s N.N s  regression refinement N.N s NN.N s  N.N s NN.Ns  N.Ns NN.N s  N0.N s NN.N s  regression refinement + fusion N.Ns NN.N s  Figure N
Alignment prediction and regression refinement examples in Charades-STA
 over all sentences, take the minimum of all start times and  maximum of all end times as start and end time of the complex query
Although the random performance in Table
N  (complex) is higher than that in Table N (single), the gain  over random performance remains similar, which indicates  that CTRL is able to handle complex query consisting multiple activities well
Comparing CTRL and CTRL+Fusion,  we can see that CTRL could be an effective first step for  complex query, if combined with other fusion methods
 In general, we observed two types of common hard  cases: (N) long query sentences increase chances of failure,  likely because the sentence embeddings are not discriminative enough; (N) videos that contain similar activities but  with different objects (e.g
in TACOS dataset, put a cucumber on chopping board, and put a knife on chopping board)  are hard to distinguish amongst each other
 N
Conclusion  We addressed the problem of Temporal Activity Localization via Language (TALL) and proposed a novel Crossmodal Temporal Regression Localizer (CTRL) model,  which uses temporal regression for activity location refinement
We showed that non-parameterized offsets works  better than parameterized offsets for temporal boundary regression
Experimental results show the effectiveness of our  method on TACoS and Charades-STA
 NNNN    References  [N] P
Bojanowski, R
Lajugie, E
Grave, F
Bach, I
Laptev,  J
Ponce, and C
Schmid
Weakly-supervised alignment of  video with text
In ICCV, N0NN
 [N] F
Caba Heilbron, V
Escorcia, B
Ghanem, and J
Carlos Niebles
Activitynet: A large-scale video benchmark for  human activity understanding
In CVPR, N0NN
 [N] J
Donahue, L
Anne Hendricks, S
Guadarrama,  M
Rohrbach, S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual  recognition and description
In CVPR, N0NN
 [N] J
Gao, C
Sun, and R
Nevatia
Acd: Action concept discovery from image-sentence corpora
In ICMR
ACM, N0NN
 [N] J
Gao, Z
Yang, C
Sun, K
Chen, and R
Nevatia
Turn  tap: Temporal unit regression network for temporal action  proposals
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [N] R
Girshick
Fast r-cnn
In ICCV, N0NN
 [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
 [N] R
Hu, H
Xu, M
Rohrbach, J
Feng, K
Saenko, and T
Darrell
Natural language object retrieval
In CVPR, N0NN
 [N] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, N0NN
 [N0] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In CVPR, N0NN
 [NN] S
Kazemzadeh, V
Ordonez, M
Matten, and T
L
Berg
 Referitgame: Referring to objects in photographs of natural  scenes
In EMNLP, N0NN
 [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
In ICLR, N0NN
 [NN] R
Kiros, Y
Zhu, R
R
Salakhutdinov, R
Zemel, R
Urtasun,  A
Torralba, and S
Fidler
Skip-thought vectors
In NIPS,  N0NN
 [NN] D
Lin, S
Fidler, C
Kong, and R
Urtasun
Visual semantic  search: Retrieving videos via complex textual queries
In  CVPR, N0NN
 [NN] S
Ma, L
Sigal, and S
Sclaroff
Learning activity progression in lstms for activity detection and early detection
In  CVPR, N0NN
 [NN] C
D
Manning, M
Surdeanu, J
Bauer, J
R
Finkel,  S
Bethard, and D
McClosky
The stanford corenlp natural language processing toolkit
In ACL, N0NN
 [NN] J
Mao, J
Huang, A
Toshev, O
Camburu, A
L
Yuille, and  K
Murphy
Generation and comprehension of unambiguous  object descriptions
In CVPR, N0NN
 [NN] J
Mao, W
Xu, Y
Yang, J
Wang, Z
Huang, and A
Yuille
 Deep captioning with multimodal recurrent neural networks  (m-rnn)
In ICLR, N0NN
 [NN] T
Mikolov, I
Sutskever, K
Chen, G
S
Corrado, and  J
Dean
Distributed representations of words and phrases  and their compositionality
In NIPS, N0NN
 [N0] B
A
Plummer, L
Wang, C
M
Cervantes, J
C
Caicedo,  J
Hockenmaier, and S
Lazebnik
FlickrN0k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models
In ICCV, N0NN
 [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In  CVPR, N0NN
 [NN] M
Regneri, M
Rohrbach, D
Wetzel, S
Thater, B
Schiele,  and M
Pinkal
Grounding action descriptions in videos
 Transactions of the Association for Computational Linguistics, N:NN–NN, N0NN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
 [NN] M
Rohrbach, S
Amin, M
Andriluka, and B
Schiele
A  database for fine grained activity detection of cooking activities
In CVPR, N0NN
 [NN] M
Rohrbach, M
Regneri, M
Andriluka, S
Amin,  M
Pinkal, and B
Schiele
Script data for attribute-based  recognition of composite activities
In ECCV, N0NN
 [NN] Z
Shou, D
Wang, and S.-F
Chang
Temporal action localization in untrimmed videos via multi-stage cnns
In CVPR,  N0NN
 [NN] G
A
Sigurdsson, G
Varol, X
Wang, A
Farhadi, I
Laptev,  and A
Gupta
Hollywood in homes: Crowdsourcing data  collection for activity understanding
In ECCV, N0NN
 [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In NIPS, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 [N0] B
Singh, T
K
Marks, M
Jones, O
Tuzel, and M
Shao
A  multi-stream bi-directional recurrent neural network for finegrained action detection
In CVPR, N0NN
 [NN] C
Sun, C
Gan, and R
Nevatia
Automatic concept discovery from parallel text and visual corpora
In ICCV, N0NN
 [NN] D
Tran, L
Bourdev, R
Fergus, L
Torresani, and M
Paluri
 Learning spatiotemporal features with Nd convolutional networks
In ICCV, N0NN
 [NN] H
Wang, A
Kläser, C
Schmid, and C.-L
Liu
Action recognition by dense trajectories
In CVPR, N0NN
 [NN] S
Yeung, O
Russakovsky, G
Mori, and L
Fei-Fei
Endto-end learning of action detection from frame glimpses in  videos
In CVPR, N0NN
 [NN] J
Yuan, B
Ni, X
Yang, and A
A
Kassim
Temporal action  localization with pyramid of score distribution features
In  CVPR, N0NN
 [NN] J
Yue-Hei Ng, M
Hausknecht, S
Vijayanarasimhan,  O
Vinyals, R
Monga, and G
Toderici
Beyond short snippets: Deep networks for video classification
In ICCV, N0NN
 NNNNActive Decision Boundary Annotation With Deep Generative Models   Active Decision Boundary Annotation with Deep Generative Models  Miriam Huijser  Aiir Innovations  Amsterdam, The Netherlands  https://aiir.nl/  Jan C
van Gemert  Delft University of Technology  Delft, The Netherlands  http://jvgemert.github.io/  Abstract  This paper is on active learning where the goal is to  reduce the data annotation burden by interacting with a  (human) oracle during training
Standard active learning  methods ask the oracle to annotate data samples
Instead,  we take a profoundly different approach: we ask for annotations of the decision boundary
We achieve this using a  deep generative model to create novel instances along a Nd  line
A point on the decision boundary is revealed where the  instances change class
Experimentally we show on three  data sets that our method can be plugged into other active  learning schemes, that human oracles can effectively annotate points on the decision boundary, that our method is  robust to annotation noise, and that decision boundary annotations improve over annotating data samples
 N
Introduction  If data is king, then annotation labels are its crown jewels
Big image data sets are relatively easy to obtain; it’s  the ground truth labels that are expensive [N, NN]
With the  huge success of deep learning methods critically depending  on large annotated datasets, there is a strong demand for  reducing the annotation effort [NN, N0, NN, NN, NN]
 In active learning [NN] the goal is to train a good predictor while minimizing the human annotation effort for large  unlabeled data sets
During training, the model can interact  with a human oracle who provides ground truth annotations  on demand
The challenge is to have the model automatically select a small set of the most informative annotations  so that prediction performance is maximized
 In this paper we exploit the power of deep generative  models for active learning
Active learning methods [NN, NN,  NN] typically ask the oracle to label an existing data sample
 Instead, we take a radically different approach: we ask the  oracle to directly annotate the decision boundary itself
To  achieve this, we first use all unlabeled images to learn a Kdimensional embedding
In this K-dimensional embedding  we select a N-dimensional query line and employ a deep  Figure N: Active decision boundary annotation using a deep  generative model
In a K-dimensional feature space (top), a  N-dimensional query line (blue) is converted to a row of images (bottom) by generating visual samples along the line
 The oracle annotates where the generated samples change  classes (red point)
This point lies close to the decision  boundary and we use that point to improve classification
 generative model to generate visual samples along this line
 We visualize the N-dimensional line as an ordered row of  images and simply ask the oracle to visually annotate the  point where the generated visual samples changes classes
 Since the generated images are ordered, the oracle does not  need to examine and annotate each and every image, merely  identifying the change point is enough
The point between  two samples of different classes is a point that lies close to  the decision boundary and we use that point to improve the  classification model
In figure N we show an illustration of  our approach
 NNNN  https://aiir.nl/ http://jvgemert.github.io/   We make the following contributions
First, we use a  deep generative model to present a N-dimensional query  line to the oracle
Second, we directly annotate the decision boundary instead of labeling data samples
Third, we  learn a decision boundary from both labeled instances and  boundary annotations
Fourth, we evaluate if the generative  model is good enough to construct query lines that a human  oracle can annotate, how much noise the decision boundary  annotations allow, and how our decision boundary annotation version of active learning compares to traditional active  learning where data samples are labeled
 N
Related work  Active learning [NN] is an iterative framework and starts  with the initialization of a prediction model either by training on a small set of labeled samples or by random initialization
Subsequently, a query strategy is used to interactively query a oracle which can be another model or a human annotator
The annotated query is then used to retrain  the model and starts the next iteration
Active learning in  computer vision includes work on selecting the most influential images [NN], refraining from labeling unclear visuals [NN], zero-shot transfer learning [NN], multi-label active  learning [NN]
Similar to these methods, our paper uses active learning in the visual domain to minimize the number  of iterations while maximizing prediction accuracy
 There are several settings of active learning
A poolbased setting [NN, NN] assumes the availability of a large  set of unlabeled instances
Instead, a stream-based setting  [N, N] is favorable for online learning where a query is selectively sampled from an incoming data stream
Alternatively,  in a query synthesis setting [N, N, N0] an input data distribution is used to generate queries for the oracle in the input space
Recently, [N] and [N] proposed methods for efficiently learning halfspaces, i.e
linear classifiers, using synthesized instances
Instead of using an existing setting, our  paper proposes a new active learning setting: active boundary annotation
Other active learners use sample instances  to query the oracle
Instead, we generate a row of instances  and query the point where the instances change class label
 We do not query an annotation of a sample, we query an  annotation of the decision boundary
 Query strategies in active learning are the informativeness measures used to select or generate new queries
Much  work has been done on this topic [NN], here we describe a  few prominent strategies
Uncertainty sampling [NN] is a  query strategy that selects the unlabeled sample of which  the current model is least certain
This could be obtained  by sampling closest to the decision boundary [N, NN] or  based on entropy [NN]
The Uncertainty-dense sampling  method [NN, NN] aims to correct for the problems associated with uncertainty sampling by selecting samples that  are not only uncertain but that also lie in dense areas of  the data distribution and are thus representative of the data
 In Batch methods [NN, NN, NN], not a single sample is  queried at each iteration, but a set of samples
In Queryby-committee [NN] multiple models are used as a committee  and queries the samples where the disagreement is highest
Our active boundary annotation method can plug-in  any sampling method and thus does not depend on a particular query strategy
We will experimentally demonstrate  that our boundary annotation method can readily be applied  to various query strategies
 In our active learning approach we make use of deep  generative models
Probabilistic generative models such as  variational autoencoders [N0, NN], learn an inference model  to map images to a latent space and a decoder to map  from latent space back again to image space
Unfortunately  the generated images are sometimes not very sharp
Nonprobabilistic models such as the generative adversarial nets  (GAN) [NN] produce higher-quality images than variational  autoencoders [N0], but can only map from latent space to  image space
These models randomly sample latent vectors from a predefined distribution and then learn a mapping  from these latent vectors to images; there is no mapping that  can embed real images in the latent space
To perform classification on real images in the embedding we need an inference step to map images to the latent space
Fortunately, recent generative adversarial models can produce high-quality  images and provide efficient inference [NN, NN]
In this paper we use such a GAN model
 N
Active decision boundary annotation  We have N data samples x ∈ RD, each sample is paired with a class label (X,Y) = {(xN, yN), 


, (xN , yN )} where for clarity we focus on the binary case y ∈ {−N, N} and a linear classification model
As often done in active  learning we assume an initial set A which contains a hand- ful of annotated images
Each data sample xi has a corresponding latent variable zi ∈ R K 
For clarity we omit the  index i whenever it is clear that we refer to a single data  point
The tightest hypersphere that contains all latent variables zi is denoted by Ω
Every iteration in active learning estimates a decision boundary θ̂ where the goal is to best  approximate the real decision boundary θ while minimizing  the number of iterations
 In figure N we show an overview of our method
Each  image x is embedded in a manifold as a latent variable  Gz(x) = z
In this embedding we use a standard active learning query strategy to select the most informative query  sample z∗
We then construct a query line q in such a  way that it intersects the query sample z∗ and is perpendicular to the current estimate of the decision boundary θ̂ at  point zp
We uniformly sample latent points z along the Ndimensional query line q and for each point on the line generate an estimate of the corresponding image Gx(z) = x̂
 NNNN    Figure N: Overview of our method
Data labels y are the red  squares and yellow stars; unlabeled data is a gray circle
A  deep generative model is used to map an image sample x to  its corresponding latent variable Gz(x) = z
Vice versa, an image is generated from the latent variable z by Gx(z) = x̂
The hypersphere Ω bounds the latent space
The true de- cision boundary (black line) is θ and the current estimate  of the decision boundary (green line) is θ̂
The query line  q (blue) goes through the query sample z∗ and is perpendicular to the current estimate of the decision boundary θ̂,  intersecting it at point zp
The query line q is uniformly  sampled (blue bars) and bounded by Ω, which gives a row of generated images as illustrated at the right
Note how a  ‘0’ morphes to an ‘N’ after it passes the decision boundary  θ
The latent boundary annotation point is given by zq∩θ
 On this generated row of images we ask the oracle to provide the point where the images change class, this is where  the decision boundary intersects the query line q ∩ θ and the latent variable zq∩θ is a decision boundary annotation
 Using the boundary annotation we can assign a label to the  query sample z∗ which we add to the set of annotated samples A
All annotated decision boundary points are stored in the set B
The estimate of the decision boundary θ̂ is found by optimizing a joint classification/regression loss
 The classification loss is computed on the labeled samples  A while at the same time the regression aims to fit the deci- sion boundary through the annotations in B
 Deep generative embedding
We make use of GANs  (Generative Adversarial Nets) [NN] to obtain a high-quality  embedding
In GANs, a generative model G can create realistic-looking images from a latent random variable  G(z) = x̂
The generative model is trained by making use  of a strong discriminator D that tries to separate synthetic  generated images from real images
Once the discriminator  cannot tell synthetic images from real images, the generator  is well trained
The generator and discriminator are simultaneously trained by playing a two-player minimax game,  for details see [NN]
 Deep generative inference
Because we perform classification in the embedding we need an encoder to map the  image samples to the latent space
Thus, in addition to  a mapping from latent space to images (decoding) as in a  standard GAN, we also need an inference model to map  images to latent space (encoding) [NN, NN]
This is done  by optimizing two joint distributions over (x, z): one for the encoder q(x, z) = q(x)q(z|x) and one for the decoder p(x, z) = p(z)p(x|z)
Since both marginals are known, we can sample from them
The encoder marginal q(x) is the empirical data distribution and the decoder marginal is defined as p(z) = N (0, I)
The encoder and the decoder are trained together to fool the discriminator
This is achieved  by playing an adversarial game to try and match the encoder  and decoder joint distributions where the adversarial game  is played between the encoder and decoder on the one side  and the discriminator on the other side, for details see [NN]
 Query strategy
For compatibility with standard active  learning methods we allow plugging in any query strategy that selects an informative query sample z∗
Such  a plug-in is possible by ensuring that the query sample  z∗ is part of the query line q
An example of a popular query sample strategy is uncertainty sampling that selects the sample whose prediction is the least confident:  z∗ = argmaxz N− Pθ̂(ŷ|z), where ŷ is the class label with the highest posterior probability under the current prediction model θ̂
This can be interpreted as the sample where  the model is least certain about
We experimentally validate  our method’s compatibility with common query strategies
 Constructing the query line
The query line q determines which images will be shown to the oracle
To  make decision boundary annotation possible these images  should undergo a class-change
A reasonable strategy is  then to make sure the query line intersects the current estimate of the decision boundary θ̂
A crisp class change  will make annotating the point of change easier
Thus, we  increase the likelihood of a crisp class-change by ensuring  that q is perpendicular to θ̂
Since the query sample z∗ lies  on the query line and q ⊥ θ̂ this is enough information to construct the query line q
Let the current estimation  of the linear decision boundary θ̂ be a hyperplane which  is parametrized by a vector ŵ perpendicular to the plane  and offset with a bias b̂
Then the query line is given by  q(t) = zp + (z∗ − zp)t, where zp is the projection of the  query point z∗ on the current decision boundary θ̂ and is  defined as zp = z∗ − (ŵ ⊺z∗+b̂) ŵ⊺ŵ  ŵ, for derivation details see  the supplemental material
 NNNN    Constructing the image row
We have to make a choice  about which image samples along the query line q to show  to the oracle
We first restrict the size of q to lie within the  tightest hypersphere Ω that captures all latent data samples
The collection of points H that lie within and on the sur- face of the hypersphere Ω is defined as H = {z ∈ RK : ||z− z̄|| ≤ r} where r = maxNi=0 ||zi − z̄|| and z̄ is the av- erage vector over all latent samples z
As a second step we  uniformly sample s latent samples from q∩H which are de- coded to images with the deep generative model G(z) = x̂
 Annotating the decision boundary
Annotating the decision boundary can be done by a human oracle or by a  given model
A model is often used as a convenience to  do large scale experiments where human experiments are  too time-consuming
For example, large scale experiments  for active learning methods that require query sample annotation are typically performed by replacing the human  annotation by the ground truth annotation
This assumes  that the human will annotate identically to the ground truth,  which may be violated close to the decision boundary
To  do large scale experiments for our decision boundary annotation method we also use a model based on the ground  truth, like commonly done for query sample annotation
We  train an oracle-classifier on ground truth labels and use the  that model as a ground truth decision boundary where the  intersection zq∩θ = q∩θ can be computed
For both oracle types –the oracle-classifier and the human oracle– we store  the decision boundary annotations in B and we also ask the annotators for the label y of the query point z∗, for which  we store the pair (z∗, y) in A
We experimentally evaluate human and model-based annotations
 Model optimization using boundary annotations
At  every iteration of active learning we update θ̂
We use a  classification loss on the labeled samples in A while at the same time we optimize a regression loss to fit the decision  boundary through the annotations in B
In this paper we restrict ourselves to the linear case and parametrize θ̂ with a  linear model ŵ⊺z+ b̂ = 0
For the classification loss we use a standard SVM hinge loss over the labeled samples (z, y) in A as  Lclass = N  |A|  ∑  (z,y)∈A  max (  0, N− y(ŵ⊺z+ b̂) )  
(N)  For the regression, we use a simple squared loss to fit the  model ŵ + b̂ to the annotations in B  Lregress = N  |B|  ∑  z∈B  (  ŵ⊺z+ b̂ )N  
(N)  The final loss L jointly weights the classification and re- gression losses equally and simply becomes  L = N  N Lclass +  N  N Lregress + λ||ŵ||  N, (N)  where the parameter λ controls the influence of the regularization term ||ŵ||N where we use λ = N in all experiments
Note that because Lclass and Lregress are both convex losses, the joint loss L is convex as well
 N
Experiments  We perform active learning experiments on three  datasets
MNIST contains N0,000 binary digit images, N0k  to train and N0k in the test set
The SVHN dataset [NN] contains challenging digit images from Google streetview, it  has NN,NNN train and NN,0NN test images
The SVHN dataset  has NNN,NNN extra images, which we use to train the embedding
In the third dataset we evaluate our method on more  variable images than digits
We create the Shoe-Bag dataset  of N0,000 train and NN,000 test images by taking subsets  from the Handbags dataset [NN] and the Shoes dataset [NN]
 For every dataset we train a deep generative embedding  following [NN]
For MNIST and SVHN we train an embedding with N00 dimensions, for the more varied Shoe-Bag  we train an embedding of NNN dimensions
For the training  of the embeddings we set dropout = 0.N for the layers of  the discriminator
All embeddings are trained on the train  data, except for the SVHN embedding; which is trained on  the larger “extra” dataset following [NN]
We will make our  code available [N]
All learning is done in the embedding  and the experiments that do not use a human oracle use an  SVM trained on all labels as the oracle
 We evaluate active learning with the Area Under the (accuracy) Learning Curve (AULC) measure [NN, NN]
The  Area under the Learning Curve is computed by integrating  over the test accuracy scores of N active learning iterations  using the trapezoidal rule:  AULC = N ∑  i=N  N  N (acci−N + acci), (N)  where acc0 is the test accuracy of the initial classifier before the first query
The AULC score measures the informativeness per annotation and is high for active learning  methods that quickly learn high-performing models with  few queries, i.e
in few iterations
We also report Mean  Average Precision (MAP) results in supplemental material
 We first evaluate essential properties of our method on  two classes: ‘0’ versus ‘N’
Later we evaluate on all classes
 N.N
Exp N: Evaluating various query strategies  Our method can re-use standard active learning query  strategies that select a sample for oracle annotation
We  evaluate four sample-based query strategies
Random sampling is a common baseline for query strategies [N0]
Uncertainty sampling selects the least confident sample point [NN]
 Uncertainty-dense sampling [NN] selects samples that are  not only uncertain but that also lie in dense areas of the data  NNNN    Experiment N: Evaluating various query strategies  MNIST 0 vs
N SVHN 0 vs
N Shoe-Bag  Strategy Sample Boundary (ours) Sample Boundary (ours) Sample Boundary (ours)  Uncertainty NNN.0± 0.N NNN.N ± 0.N NNN.N± N.N NNN.N ± N.0 NNN.N± 0.N NNN.N ± 0.N Uncertainty-dense NNN.N± N0.N NNN.0 ± N0.N NN.N± N.N NNN.N ± N.N NNN.0± N.N NNN.N ± N.0 N Cluster centroid NNN.N± 0.N NNN.0 ± 0.N NN.0± N.N N0N.N ± N.N NNN.0± N.N NNN.N ± 0.N Random NNN.N± N.0 NNN.N ± 0.N NNN.N± N.N NNN.N ± N.N NN0.N± N.N NNN.0 ± 0.N  Table N: AULC results for four active learning query strategies
Results are on MNIST (classifying 0 and N), SVHN (classifying 0 and N) and Shoe-Bag after NN0 queries, where the maximum possible AULC score is NN0
The results are averaged  over NN repetitions
For each row, the significantly best result is shown in bold, where significance is measured with a paired  t-test with p < 0.0N
SVHN is the most difficult dataset
Uncertainty sampling is generally the best query strategy
Boundary  annotation significantly outperforms sample annotations for all datasets for all query strategies
 distribution
The K-cluster centroid [NN] uses batches of K  samples, where we set K = N
We plug in each query sam- ple strategy in our line query construction approach used for  decision boundary annotation
 The results in Table N show that for all three datasets  and for all four query strategies our boundary annotations  outperform sample annotations
For the uncertainty-dense  method the improvement is the largest, which may be due to  this method sampling from dense areas of the distribution,  and boundary annotations add complementary information
 The uncertainty sampling gives best results for both active  learning methods and all three datasets
It is also the strategy where our method improves the least, and is thus the  most challenging to improve upon
We select uncertainty  sampling for the remainder of the experiments
 N.N
Exp N: Evaluating generative model quality  The generative model that we plug into our method  should be able to construct recognizable line queries so that  human oracles can annotate them
In figure N we show some  line queries generated for all three datasets by our active  learning method with uncertainty sampling
Some query  lines are difficult to annotate, as shown in figure N(b) and  others are of good quality as shown in figure N(a)
 We quantitatively evaluate the generation quality per  dataset by letting N0 humans each annotate the same N0 line  queries
Line queries are subsampled to have a fixed sample  resolution of 0.NN, i.e
the distance between each image on the line and thus vary in length depending on their position  in the hypersphere Ω
The human annotators are thus pre- sented with more images for longer query lines and fewer  images for shorter query lines
For all N0 line queries we  evaluate the inter-human annotation consistency
A higher  consistency suggests that the query line is well-recognizable  and thus that the generative model has a good image generation quality
We instructed the human oracles to annotate  the point where they saw a class change; or indicate if they  see no change, this happens for N out of the N0 lines
 Experiment N: Evaluating inter-human annotations  lines without change samples deviation  MNIST 0 vs
N N N SVHN 0 vs
N N N Shoe-Bag N N  Table N: Annotation consistency results averaged over N0  query line annotations from N0 human oracles
We show  the number of lines marked as having no class change and  the average deviation in number of images, rounded up,  from the average annotation per line
Human consistency  is worse for the non-uniform Shoe-Bag dataset
The more  uniform datasets MNIST and SVHN have quite accurate human consistency
 In Table N we show the results for the inter-human annotation consistency
The Shoe-Bag embedding does not seem  to be properly trained because the human annotators see no  change in half of the query lines
In addition, the variance  between the images make the consistency lower
MNIST  has a deviation of N images and N lines were reported with  no change
SVHN provides the highest quality query lines the human annotators agreed collectively on the inadequacy  of only one query line and the human annotators are most  consistent for this dataset
 N.N
Exp N: Evaluating annotation noise  In experiment N we show that there is variation in the annotations between human oracles
Here we aim to answer  the question if that variation matters
We evaluate the effect  of query line annotation noise on the classification performance
We vary the degree of additive line annotation noise  with respect to SVM oracle decision boundary annotations  on the N-dimensional line query
We vary the standard deviation σ of Gaussian noise to σ ∈ {N, 


, N} image samples away from the oracle
 NNN0    (a) Query lines with high human consistency
 (b) Query lines with low human consistency
 Figure N: Examples of line queries for MNIST (top two rows) SVHN (middle two rows) and shoe-bag (bottom two rows)
Red  triangles indicate the mean annotation per line and the gray bar indicates the standard deviation from the mean annotation
 (a) Query lines for which the N0 human annotators were most consistent and (b) query lines for which the human annotators  were most inconsistent
For visibility these query lines are subsampled in NN images; the human annotators were presented  with more or fewer images depending on the length of the line query
The human annotators are more consistent for query  lines with clearer images and a more sudden change of class, such as the third and fourth row from the top
It should be noted,  however, that the class-changes on these lines are not as sudden as is visualized here; the human annotators were presented  with more images, also seeing the images in between the images presented here
 The results in Table N show that adding sampling noise  of up to about σ = N images to the SVM oracle annota- tions has a slight negative effect on the performance of our  boundary annotation method, but it is still significantly better than sample annotation
Comparing these results to the  inter-human annotation consistency results in Table N shows  that Shoe-Bag annotation variation is around N, and thus the quality of the generator will likely degrade accuracy
For  MNIST and SVHN the human consistency is around or below N images which is well-handled
 N.N
Exp N: Evaluating a human oracle  In this experiment we evaluate classification performance with a human oracle
For all three datasets we have a  human oracle annotate the first N0 line queries, selected using uncertainty sampling
We repeat the same experimental  setup for sample-based active learning
The results are averaged over NN repetitions
 The results in Table N show that an oracle-SVM outperforms a human annotator
This is probably because the active learner method that is being trained is also an SVM,  NNNN    Experiment N: Evaluating annotation noise  Sampling noise MNIST 0 vs
N SVHN 0 vs
N Shoe-Bag  (# images) Sample Boundary (ours) Sample Boundary (ours) Sample Boundary (ours)  0 NNN.N± 0.N NNN.0 ± 0.N NNN.N± N.N NNN.0 ± 0.N NNN.N± 0.N NNN.N ± 0.N N NNN.N± 0.N NNN.N ± 0.N NNN.N± N.N NNN.N ± N.N NNN.N± 0.N NNN.N ± 0.N N NNN.N± 0.N NNN.N ± 0.N NNN.N± N.N NNN.N ± N.N NNN.N± 0.N NNN.N ± 0.N N NNN.N± 0.N NNN.0 ± 0.N NNN.N± N.N NNN.N ± N.N NNN.N± 0.N NNN.N ± 0.N N NNN.N± 0.N NNN.N± 0.N NNN.N± N.N NNN.N± 0.N NNN.N± 0.N NNN.N ± 0.N N NNN.N ± 0.N NNN.N± 0.N NNN.N± N.N NNN.N± N0.N NNN.N± 0.N NNN.0± 0.N  Table N: AULC results for noisy boundary active learning with uncertainty sampling for MNIST (classifying 0 and N), SVHN  (classifying 0 and N) and Handbags vs
Shoes after NN0 queries (maximum possible score is NN0)
Each experiment is  repeated NN times
For each row, the significantly best result is shown in bold, where significance is measured with a paired  t-test with p < 0.0N
Noise has been added to the boundary annotation points; not to the image labels
Results worsen with  more added noise, with the turning point of the significant better performance of Boundary around a sampling noise of N  images for MNIST and SVHN, and N images for Shoe-Bag
 Experiment N: Evaluating a human oracle  MNIST 0 vs
N SVHN 0 vs
N Shoe-Bag  Annotation Sample Boundary (ours) Sample Boundary (ours) Sample Boundary (ours)  Human oracle N.N± 0.N N.N ± 0.N N.N± 0.N N.N± 0.N N.N± 0.N N.N± 0.N SVM oracle N.N± 0.N N.N± 0.N N.N± 0.N N.N± 0.N N.N± 0.N N.N± 0.N  Table N: AULC results for a human and a SVM oracle for sample-based active learning and our boundary active learning  for MNIST (classifying 0 and N), SVHN (classifying 0 and N) and Shoe-Bag after N0 queries (maximum possible score is  N0)
The experiments are repeated NN times and significant results per row are shown in bold for p < 0.0N
Results always  improve for boundary annotation, but these improvements are not significant for SVHN and Shoe-Bag
 and since the oracle is also an SVM it will choose the perfect samples
For humans, boundary annotation always improves over sample annotation
Yet, for SVHN and ShoeBag this improvement is not significant
This is probably  due to the small number of queries, where our method after  only N0 iterations has not yet achieved peak performance as  corroborated by the learning curves in figure N
 N.N
Exp N: Generalization over classes  Up to now we have shown that our method outperforms  sample-based active learning on a subset of MNIST and  SVHN
To see whether our method generalizes to the other  classes we evaluate the performance averaged over all the  SVHN and MNIST class pairs using uncertainty sampling  as query strategy
We show results in Table N and plot  the learning curves in figure N
Averaged over all datasets  and class pairs our method is significantly better than the  sample-based approach
 We also evaluate on the CIFAR-N0 dataset using GIST  features following Jain et al
(EH-Hash) [NN] for uncertainty  sampling
Results in figure N show we clearly outperform  EH-Hash in terms of AUROC improvement
 Experiment N: Full dataset evaluation  Sample Boundary (ours)  MNIST NNN.N± 0.0N NNN.N ± 0.0N SVHN NNN.N± 0.N NN0.N ± N.N Shoe-Bag NNN.N± 0.N NNN.N ± 0.N  Table N: AULC results for sample-based active learning and  boundary active learning for all datasets after NN0 queries  (maximum possible score is NN0), averaged over all class  pairs
The experiments are repeated N times and significant  results are shown in bold
Significance is measured with  a paired t-test with p < 0.0N
For all datasets our method  significantly improves over sample-based active learning
 N
Discussion  We extend active learning with a method for direct decision boundary annotation
We use a deep generative model  to synthesize new images along a N-dimensional query line,  and ask an oracle to annotate the point where the images  change class: this point is an annotation of the decision  boundary
Note that this may not lead to a direct accelerNNNN    0 N0 N0 N0 N0 N0 N0 N0 N0 N0 N00 NN0 NN0 NN0 NN0 NN0  Number of queries  0.NN  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  N.00  T e st   a cc  u ra  cy  Boundary (ours)  Sample  Supervised  (a) MNIST averaged over all classes
 0 N0 N0 N0 N0 N0 N0 N0 N0 N0 N00 NN0 NN0 NN0 NN0 NN0  Number of queries  0.NN  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  N.00  T e st   a cc  u ra  cy  Boundary (ours)  Sample  Supervised  (b) SVHN averaged over all classes
 0 N0 N0 N0 N0 N0 N0 N0 N0 N0 N00 NN0 NN0 NN0 NN0 NN0  Number of queries  0.NN  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  N.00  T e st   a cc  u ra  cy  Boundary (ours)  Sample  Supervised  (c) Shoe-Bag both classes
 Figure N: Learning curves over all datasets and all class pairs using uncertainty sampling as query strategy
The experiments  are repeated N times, standard deviations are indicated by line width
The fully supervised oracle-SVM is the upper bound
 Our boundary method outperforms the sample-based method
 0 N0 N00 NN0 N00 NN0 N00  Number of queries  0.00  0.0N  0.N0  0.NN  0.N0  0.NN  0.N0  A U  R O  C  I m  p ro  v e m  e n t  Boundary (ours) Jain et al
 Figure N: Comparison of our method against Jain et al
[NN]  on CIFAR-N0 using NNN-d GIST descriptors
The results are  averaged over N repeats
We outperform Jain et al
[NN]  ation of the annotation speed
If the annotation cost is per  hour, e.g
a radiologist, then ease of annotation and system  speed become key
If costs are per task, e.g
Amazon Mechanical Turk, then the informativeness of each annotation  should be maximally exploited
Our method falls in the latter category: We increase the informativeness per annotation
 One disadvantage of our method is that it is very easy to  annotate changes visually, but this is not so straightforward  in other domains
The core of our method can in principle  also be used on any input data, but actually using a human  oracle to detect a class change for non-visual data would become tedious fast
For example, having a human annotate a  class-change for raw sensor data, speech or text documents  would be quite difficult in practice
Our method could still  be applicable if the non-visual data can be easily visualized
 Another problem is precisely annotating the decision  boundary when the margin between classes is large
With  a large margin the generated samples may all look similar  to each other and it is difficult to annotate the class change  exactly
A solution could be to annotate the margin on each  side instead of the decision boundary
 Our method depends critically on the quality of the generative model
We specifically evaluated this by including the Shoe-Bag dataset where the quality of the generated samples impairs the consistency of human annotation
 If the generative model is of low quality, our method will  fail as well
Deep generative models are an active area  of research, so we are confident that the quality of generative models will improve
One possible direction for future  work could be to exploit the knowledge of the annotated  decision boundary to update the generative model
 In this work we consider linear models only
The decision boundary hyperplane lives in a K-N dimensional space,  and thus K-N independents points span the plane exactly
 The reason why we need more than K-N query annotations  is that boundary annotation points may not be independent
 Future work on a new query strategy that would enforce  boundary point independence may be promising to reduce  the number of annotations required
For non-linear models, a non-linear N-dimensional query line could perhaps  work better
Also, when data sets are not linearly separable  we may require more than one annotation of the decision  boundary for N query line
This is left for future work
 Our paper showed that boundary annotation for visual  data is possible and improves results over only labeling  query samples
We show that our method can plug in existing active learning strategies, that humans can consistently annotate the boundary if the generative model is good  enough, that our method is robust to noise, and that it significantly outperforms sample-based methods for all evaluated  classes and data sets
 NNNN    References  [N] https://github.com/MiriamHu/  ActiveBoundary,
 [N] I
M
Alabdulmohsin, X
Gao, and X
Zhang
Efficient active  learning of halfspaces via query synthesis
In AAAI, pages  NNNN–NNNN, N0NN
 [N] D
Angluin
Queries and concept learning
Machine learning, N(N):NNN–NNN, NNNN
 [N] L
E
Atlas, D
A
Cohn, R
E
Ladner, M
A
El-Sharkawi,  R
J
Marks, M
Aggoune, and D
Park
Training connectionist networks with queries and selective sampling
In NIPS,  pages NNN–NNN, NNNN
 [N] E
B
Baum and K
Lang
Query learning can work poorly  when a human oracle is used
In International Joint Conference on Neural Networks, volume N, page N, NNNN
 [N] C
Campbell, N
Cristianini, A
Smola, et al
Query learning with large margin classifiers
In ICML, pages NNN–NNN,  N000
 [N] L
Chen, H
Hassani, and A
Karbasi
Dimension coupling:  Optimal active learning of halfspaces via query synthesis
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [N] D
Cohn, L
Atlas, and R
Ladner
Improving generalization with active learning
Machine learning, NN(N):N0N–NNN,  NNNN
 [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
In  CVPR
IEEE, N00N
 [N0] E
L
Denton, S
Chintala, a
szlam, and R
Fergus
Deep  generative image models using a laplacian pyramid of adversarial networks
In C
Cortes, N
D
Lawrence, D
D
Lee,  M
Sugiyama, and R
Garnett, editors, Advances in Neural  Information Processing Systems NN, pages NNNN–NNNN
Curran Associates, Inc., N0NN
 [NN] J
Donahue, P
Krähenbühl, and T
Darrell
Adversarial feature learning
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] V
Dumoulin, I
Belghazi, B
Poole, A
Lamb, M
Arjovsky,  O
Mastropietro, and A
Courville
Adversarially learned inference
arXiv preprint arXiv:NN0N.00N0N, N0NN
 [NN] A
Freytag, E
Rodner, and J
Denzler
Selecting influential examples: Active learning with expected model output changes
In European Conference on Computer Vision,  pages NNN–NNN
Springer, N0NN
 [NN] E
Gavves, T
Mensink, T
Tommasi, C
G
M
Snoek, and  T
Tuytelaars
Active transfer learning with zero-shot priors:  Reusing past datasets for future tasks
In ICCV, N0NN
 [NN] I
Goodfellow, J
Pouget-Abadie, M
Mirza, B
Xu,  D
Warde-Farley, S
Ozair, A
Courville, and Y
Bengio
Generative adversarial nets
In Advances in Neural Information  Processing Systems, pages NNNN–NNN0, N0NN
 [NN] Y
Guo and D
Schuurmans
Discriminative batch mode active learning
In Advances in neural information processing  systems, pages NNN–N00, N00N
 [NN] J.-H
Jacobsen, J
van Gemert, Z
Lou, and A
W
Smeulders
 Structured receptive fields in cnns
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNN0–NNNN, N0NN
 [NN] P
Jain, S
Vijayanarasimhan, and K
Grauman
Hashing hyperplane queries to near points with applications to largescale active learning
In Advances in Neural Information  Processing Systems, pages NNN–NNN, N0N0
 [NN] C
Kading, A
Freytag, E
Rodner, P
Bodesheim, and J
Denzler
Active learning and discovery of object categories in the  presence of unnameable instances
In CVPR, N0NN
 [N0] D
P
Kingma, S
Mohamed, D
J
Rezende, and M
Welling
 Semi-supervised learning with deep generative models
In  Advances in Neural Information Processing Systems, pages  NNNN–NNNN, N0NN
 [NN] D
P
Kingma and M
Welling
Auto-encoding variational  bayes
ICLR, N0NN
 [NN] D
Lewis and G
A
William
A sequential algorithm for  training text classifiers
In Proceedings of the NNth annual  international ACM SIGIR conference on Research and development in information retrieval, pages N–NN
SpringerVerlag New York, Inc., NNNN
 [NN] D
D
Lewis and J
Catlett
Heterogeneous uncertainty sampling for supervised learning
In Proceedings of the eleventh  international conference on machine learning, pages NNN–  NNN, NNNN
 [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV, N0NN
 [NN] P
Mettes, J
C
van Gemert, and C
G
Snoek
Spot on:  Action localization from pointly-supervised proposals
In  European Conference on Computer Vision, pages NNN–NNN
 Springer, N0NN
 [NN] Y
Netzer, T
Wang, A
Coates, A
Bissacco, B
Wu, and A
Y
 Ng
Reading digits in natural images with unsupervised feature learning
In NIPS workshop on deep learning and unsupervised feature learning, N0NN
 [NN] A
Owens, J
Wu, J
H
McDermott, W
T
Freeman, and  A
Torralba
Ambient sound provides supervision for visual  learning
In ECCV
Springer, N0NN
 [NN] J
ONeill, S
J
Delany, and B
MacNamee
Model-free and  model-based active learning for regression
In Advances in  Computational Intelligence Systems, pages NNN–NNN, N0NN
 [NN] D
Pathak, P
Krahenbuhl, J
Donahue, T
Darrell, and A
A
 Efros
Context encoders: Feature learning by inpainting
In  CVPR, N0NN
 [N0] M
E
Ramirez-Loaiza, M
Sharma, G
Kumar, and M
Bilgic
Active learning: an empirical study of common baselines
Data Mining and Knowledge Discovery, NN(N):NNN–  NNN, N0NN
 [NN] B
Settles
Active learning literature survey
University of  Wisconsin, Madison, NN(NN-NN):NN, N0N0
 [NN] B
Settles and M
Craven
An analysis of active learning  strategies for sequence labeling tasks
In Proceedings of the  conference on empirical methods in natural language processing, pages N0N0–N0NN
Association for Computational  Linguistics, N00N
 [NN] H
S
Seung, M
Opper, and H
Sompolinsky
Query by committee
In Proceedings of the fifth annual workshop on Computational learning theory, pages NNN–NNN
ACM, NNNN
 NNNN  https://github.com/MiriamHu/ActiveBoundary https://github.com/MiriamHu/ActiveBoundary   [NN] X
Shen and C
Zhai
Active feedback in ad hoc information  retrieval
In Proceedings of the NNth annual international  ACM SIGIR conference on Research and development in information retrieval, pages NN–NN
ACM, N00N
 [NN] S
Tong and D
Koller
Support vector machine active learning with applications to text classification
Journal of machine learning research, N(Nov):NN–NN, N00N
 [NN] Z
Wang, B
Du, L
Zhang, L
Zhang, M
Fang, and D
Tao
 Multi-label active learning based on maximum correntropy  criterion: Towards robust and discriminative labeling
In  ECCV, N0NN
 [NN] Z
Xu, R
Akella, and Y
Zhang
Incorporating diversity and  density in active learning for relevance feedback
In European Conference on Information Retrieval, pages NNN–NNN
 Springer, N00N
 [NN] A
Yu and K
Grauman
Fine-grained visual comparisons  with local learning
In Proceedings of the IEEE Conference  on Computer Vision and Pattern Recognition, pages NNN–  NNN, N0NN
 [NN] J
Zhu, H
Wang, B
K
Tsou, and M
Ma
Active learning  with sampling by uncertainty and density for data annotations
IEEE Transactions on audio, speech, and language  processing, NN(N):NNNN–NNNN, N0N0
 [N0] J.-J
Zhu and J
Bento
Generative adversarial active learning
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] J.-Y
Zhu, P
Krähenbühl, E
Shechtman, and A
A
Efros
 Generative visual manipulation on the natural image manifold
In European Conference on Computer Vision, pages  NNN–NNN
Springer, N0NN
 NNNNAdaptive Feeding: Achieving Fast and Accurate Detections by Adaptively Combining Object Detectors   Adaptive Feeding: Achieving Fast and Accurate Detections by Adaptively  Combining Object Detectors  Hong-Yu Zhou Bin-Bin Gao Jianxin Wu  National Key Laboratory for Novel Software Technology  Nanjing University, China  {zhouhy,gaobb}@lamda.nju.edu.cn, wujxN00N@nju.edu.cn  Abstract  Object detection aims at high speed and accuracy simultaneously
However, fast models are usually less accurate,  while accurate models cannot satisfy our need for speed
 A fast model can be N0 times faster but N0% less accurate  than an accurate model
In this paper, we propose Adaptive  Feeding (AF) to combine a fast (but less accurate) detector  and an accurate (but slow) detector, by adaptively determining whether an image is easy or hard and choosing an  appropriate detector for it
In practice, we build a cascade  of detectors, including the AF classifier which make the easy  vs
hard decision and the two detectors
The AF classifier  can be tuned to obtain different tradeoff between speed and  accuracy, which has negligible training time and requires  no additional training data
Experimental results on the  PASCAL VOC, MS COCO and Caltech Pedestrian datasets  confirm that AF has the ability to achieve comparable speed  as the fast detector and comparable accuracy as the accurate one at the same time
As an example, by combining the  fast SSDN00 with the accurate SSDN00 detector, AF leads  to N0% speedup over SSDN00 with the same precision on  the VOCN00N test set
 N
Introduction  Speed and accuracy are two main directions that current object detection systems are pursuing
Fast and accurate  detection systems would make autonomous cars safer, enable computers to understand scene information dynamically, and help robots act more intelligently
 The community have strived to improve both speed and  accuracy of detectors
Most recent state-of-the-art detection systems are based on deep convolutional neural networks
The basic pipeline of these modern detectors can be  summarized as: generate bounding box proposals, extract  features for each proposal, and apply a high-quality classifier
To obtain higher accuracy, better pretrained models, imTable N: Speed (fps) and accuracy (mAP) of various modern de- tection systems
“0N+NN” means these models are trained on the  combined train and validation sets of VOC0N and VOCNN
The  models are evaluated on the VOC0N test set
We measure the speed  with batch size N
 Method train set FPS mAP  Fast-Yolo [NN] 0N+NN NNN NN.N  Yolo [NN] 0N+NN NN NN.N  SSDN00 [NN]N 0N+NN NN NN.N  SSDN00 [NN]N 0N+NN NN NN.N  R-FCN [N] 0N+NN N NN.0 N https://arxiv.org/pdf/NNNN.0NNNNvN.pdf
 proved region proposal methods, context information, and  novel training strategies can be utilized
But, these methods  often suffer from high computational costs, e.g., tens of thousands of region proposals are required to obtain high accuracy
On the other hand, there are a few works focusing  on building faster detectors by hacking regular stages designed for traditional systems
YOLO replaces the general  region proposal procedures by generating bounding boxes  from regular grids [NN]
Single Shot MultiBox Detectors  (SSD) make several improvements on existing approaches,  and the core of it is to calculate category scores and box  offsets at a fixed set of bounding boxes using small and separated convolution kernels [NN]
Although these approaches  speed up the detection process, their accuracy rates are still lower than those slow but accurate detectors
In fact, as  shown in Table N, accuracy drops as speed increases
 We humans are able to adaptively tune between detection speed and recognition accuracy
When you step into  the kitchen, it might seem easy to find the cabinet in few  milliseconds, but it surely will cost you longer to locate the  toaster
However, most modern detection models “look at”  different input images in the same way
Specifically, the  time cost is nearly the same across different images
For  example, regardless of the number of persons in the foreNN0N  https://arxiv.org/pdf/NNNN.0NNNNvN.pdf   ground, the region proposal network in Faster R-CNN [NN]  generates tens of thousands of proposals which will definitely decrease the processing speed in images containing  only few or even no people
 In this paper, we propose to adaptively process different  test images using different detection models, in which we  utilize two detectors: one fast but inaccurate, and one accurate but slow
We first decide whether an input image is  “easy” (suitable for the fast detector) or “hard” (for which  the accurate detector is desirable), such that the test image  can be adaptively fed to different detectors
We hope the  entire detector to be as fast as the fast detector while maintaining the accuracy in the accurate one
 To make this promising tradeoff, we propose a novel  technique, adaptive feeding (AF), to efficiently extract features that are useful for this purpose and to learn a classifier  that is simple and fast
Specifically, we build a cascade of  object detectors, in which an extremely fast detector is first  used to generate few instance proposals, based on which the  AF classifier is able to adaptively choose either the fast or  the accurate model to finish the detection task
Experiments  (including timing and accuracy analyses) on several detector pairs and datasets show that there are three benefits in  our AF pipeline:  • The AF detector runs much faster than the accurate model (in many cases its speed is similar to or comparable to the fast model)
Meanwhile, the accuracy of  AF is much higher than the fast model (in many cases  close to or comparable to the accurate model)
Hence,  by combining a fast (but inaccurate) and an accurate  (but slow) model, we simultaneously achieve fast and  accurate detection in AF
 • AF can directly utilize existing models even with dif- ferent architectures
And there is no need for additional training data
 • AF employs an imbalanced learning framework to dis- tinguish easy from hard images, in which it is easy to  adjust the tradeoff between the speed and accuracy of  the combined system
 N
Related Work  Object detection is one of the most fundamental challenges in computer vision, which generally consists of feature extraction at various locations (grids or proposals) and  classification or bounding box regression
Prior to fast  R-CNN, these two steps were usually optimized separately
Fast R-CNN [NN] employed an end-to-end learning  approach to optimize the whole detector, and Faster RCNN [NN] further incorporated the proposal generation process into learning
Unlike these methods, we focus on the  utilization of pretrained models
In this section, we review  existing methods, in particular those trying to accelerate the  detection
 Detection systems
The deformable parts model (DPM) is a classic object detection method based on mixtures  of multiscale deformable part models [N0], which can capture significant variations in object appearances
It is trained  using a discriminative procedure that only requires bounding boxes for the objects
DPM uses disjoint steps and histogram of gradients features [N], which is not as competitive  as ConvNet-based approaches
 R-CNN [NN] starts another revolution of object detection  after DPM
R-CNN is among the first to employ deep features into detection systems, and obtained significant improvements over existing detectors at its time
However, the resulting system is very slow because features are  extracted from every object proposal
Compared with RCNN, Fast R-CNN [NN] not only trained the very deep VGGNN [NN] network but also uses ROI pooling layer [NN]  to perform feature extraction, and was N00× faster at test time
After that, to speed up the proposal generation  process, Faster R-CNN [NN] proposed the region proposal network (RPN) to generate bounding box proposals and  thus achieves improvements on both speed and accuracy
 Recently, ResNet [NN] begins to replace the VGG net in  some detection systems, such as Faster R-CNN [NN] and  R-FCN [N]
However, state-of-the-art accurate detectors are  in general significantly slower than real-time
 Fast detectors
Accelerating the test process is a hot  research topic in object detection
As rich object categories  often have many variations, few research focus on the speed  optimization prior to DPM
Fast detectors mainly focused  on detecting a specific object, such as face and human detectors [NN, NN]
After DPM was invented, many DPM-based  detectors [NN, N0, N] focused on optimizing different parts  in the pipeline
Dean et al
[N] exploited a locality-sensitive  hashing method which achieves a mean average precision  of 0.NN over the full set of N00,000 object classes
Yan et al
 [N0] accelerated three prohibitive steps in the cascade version of DPM, and then get an 0.NN second average time on  PASCAL VOC N00N while maintaining nearly the same performance as DPM
Sadeghi and Forsyth [NN] reimplemented the deformable parts model and achieved a near real-time  version
 In recent years, after R-CNN’s invention, many works  tend to speed up the detection pipeline by importing new  functional layers in deep models
However, it is not until  recently that we begin to approach real-time detection
YOLO [NN] framed object detection as a regression problem  to spatially separated bounding boxes and associated class  probabilities, and proposed a unified architecture which is  extremely fast
The SSD models [NN] leave separated convolution kernels in charge of default proposal boxes with  different size and ratios
Both YOLO and SSD share someNN0N    thing in common: a) decrease the number of default bounding box proposals; b) employ a unified network and incorporate different stages into the same framework
These fast  detectors are, however, less accurate than slow but accurate  models such as R-FCN (c.f 
Table N)
 Recently, there are also some researches utilizing cascaded and boosting methods [NN] [N] [NN] [NN] [NN]
Shrivastava et al
[NN] make the traditional boosting algorithm  available on deep networks which achieves higher accuracy  and maintain the same detection speed
Similar to ours, Angelova et al
[N] is based on sliding window and processes different image regions independently
However, recent deep  detectors use fully-convolutional networks and take the whole image as input
On the contrary, our adaptive feeding  method make a choice on the image level (not the region  level) and thus saves a lot of time
 The proposed adaptive feeding method follows a different route: to seek a combination of two detection systems  with different strengths
In this paper, we tackle such a situation: one detector is fast, the other is slow but more accurate, which widely exist as aforementioned
Our approach  looks a bit like the ensemble methods because both of them  rely on the diversity of different models
However, ensemble methods often suffer from enormous computations and  are difficult to implement in real-time, while our method  approaches the accuracy advantage of the accurate detector  and maintains the speed advantage of the fast one
 N
Motivation: “Easy” vs
“Hard” Images  Given a fast and an accurate detector, the motivation and  foundation of adaptive feeding is the following empirical  observation: although on average the accurate model has  higher accuracy than the fast model, in most images the fast  model is as precise as the accurate one (and in few cases it  is even better)
For convenience, we use “easy” to denote  these images for which the fast detector is as precise as the  accurate one, and the rest the “hard”
Furthermore, when  combining these detectors, we call the fast model the basic  model, and the other more accurate detector as the partner  model
In Figure N, examples are shown for cases where the  basic model is better than, same as or worse than the partner  model
 In order to create groundtruth labels for the easy vs
hard  distinction, we apply the mean average precision (mAP) detection evaluation metric to one single image
In the PASCAL VOC Challenge [N, N], the interpolated average precision [NN] (AP) is used to evaluate detection results
A  detection system submits a bounding box for each detection, with a confidence level and a predicted class for each  bounding box
For a given class, the precision/recall curve  is computed from a method’s ranked output based on the  confidence scores
The AP metric summarizes the shape  of precision/recall curve, and a further mAP (mean average  Table N: Easy vs
hard ratios under different settings
Basic and Partner models are trained on VOC0N+NN trainval
 Basic Partner Set PN − PN  ≤ 0 (Easy) > 0 (Hard)  SSDN00 SSDN00 0N+NN trainval NN.N% NN.N%  SSDN00 SSDN00 0N test NN.N% NN.N%  SSDN00 R-FCN 0N+NN trainval NN.N% N0.N%  SSDN00 R-FCN 0N test NN.N% NN.N%  precision) averages the AP in all classes
In PASCAL VOC  and MS COCO [NN], mAP is calculated by the formula  mAP = N  N  N ∑  i=N  APi , (N)  where N is the number of classes in the dataset
 However, our evaluation target is a single image
Hence,  we apply Equation N but focus on one image (N = N), as  P = N  S  S ∑  i=N  APi , (N)  where S is the number of classes in this image, APi is the  average precision for class i in this image, and P represents  the mean average precision in this image
In the rest of  this paper, we call Equation N mAPI, which stands for mean  Average Precision per Image
 Given two models mN and mN, we assume mN is more  accurate than mN, but mN runs much faster than mN
We  evaluate both models on a set of M images, which returns  {PN,N, PN,N, 


, PN,M} and {PN,N, PN,N, 


, PN,M}, where Pi,j is the mAPI for model i (i ∈ {N, N}) and image j (N ≤ j ≤ M )
We then split the difference set into two parts, the easy and the hard, according to a simple rule: if PN,j > PN,j (i.e., if the accurate model has larger mAPI on image j than  the fast one), this image is a “hard” one; if PN,j ≤ PN,j (i.e., if the fast model performs as good as or better than the  accurate detector), this image is an “easy” one
 We can now collect statistics about easy vs
hard images  with the groundtruth labels defined, as shown in Table N for  different setups
In Table N, SSDN00 and SSDN00 are the  SSD models applied to different input image sizes (N00 × N00 and N00 × N00, respectively) [NN]
Results in Table N show that most (around N0%) images are easy
 That is, for a large proportion (N0% or so) of easy images, we can use mN (the fast model) for detection, which  has both fast speed and accurate results; for the rest small  portion (N0% or so) of hard images, we apply mN (the slow  accurate detector) to maintain high accuracy
However, since the percentage of hard examples is small, they will not  significantly reduce the overall detection speed
 NN0N    (a) PN − PN < 0 (b) PN − PN = 0 (c) PN − PN > 0  Figure N: Easy and hard images
In each figure, the left picture is the results of SSDN00, and the right of R-FCN
SSDN00 (the fast model) is better than, same as, or worse than R-FCN (the accurate detector) in Figure Na, Nb, Nc, respectively
PN and PN stands for mAPI of the  fast and accurate detector for the image in consideration, respectively
(Best if viewed in color.)  N
Adaptive Feeding  The proposed adaptive feeding (AF) is straightforward  if we know how to separate easy images from hard ones
 Figure N shows the framework which mainly contains three  parts: instance proposals, a binary classifier and a pair of detectors
At the first step, an instance generator is employed  to generate instance proposals, based on which the binary  classifier decides either to feed a test image to the basic or  the partner
In this section, we propose techniques for how  to make this decision
 N.N
Instance Proposals: Features for easy vs
hard  Since the label of “easy” or “hard” is determined by  the detection results, we argue that instances in the image  should play a major role
This encourages us to put an instance generator at the first stage in AF (Figure N) to extract  features for easy vs
hard classification
To obtain both high  speed and accuracy in the following classification, we require that these proposals carry predicted class labels which  will provide detailed information; and, just a few of them  are able to describe the whole image well
As a result, an  extremely fast detector with reasonable accuracy should be  the first choice
In this paper, we use Tiny YOLO, an improved version of Fast YOLO [NN], as our instance generator
Tiny Yolo takes less than Nms to process one image on  a modern GPU and achieves NN.N% mAP on VOC0N, which  makes the features powerful and fast to extract
 The proposals generated by the instance generator that  have top confidence values contain a lot of information  about objects in an image
Specifically, one proposal include three components: C (predicted class label), S (confidence score) and B (bounding box coordinates)
We extract features based on the top K proposals with highest  confidence scores to determine whether this image is easy  or hard using a binary linear support vector machine (SVM)  classifier
Since the feature length is small if K is small, the  rest SVM classification takes little time (less than 0.Nms)  per image, and is negligible
 Ablation Analysis
Several ways are available to organize information in {C, S,B} into a feature vector
Ab- lation studies are carried out to find out the best practice  using the PASCAL VOC0N dataset (which has N0 classes),  with results in Table N
For the simplest case, we utilize  an VGG-NN model pretrained on ImageNet to do the binary  classification and report the mAP in row 0
We can see that  the basic image classification model usually has bad performance on this simple task
 The predicted class labels of K proposals (C) can form a N0-dim histogram (denoted as “N0” in Table N), or K  N0-dim confidence vectors (one for each proposal, denoted  as “N0-prob”)
Comparing rows N and N, we find that the  histogram of predicted classes is not only shorter, but also  more accurate
We believe the confidence for each proposal  (S, denoted as “conf” in Table N) is useful and it is included  in all setups of Table N
The B information are reorganized  to have two formats: “Ns” and “Nc”
A comparison between  row N, N and N shows that removing these coordinates will  reduce the mAP by at most 0.N%, and those features including bounding box size are more powerful (comparing row  N with row N)
Summarizing observations from these experiments, we use “N0+(conf+Ns)×K” as our features
The coordinates are normalized to be within 0 and N
 We also evaluated the effect of K
Comparing rows N, N  and N, we find that too many proposals (K = N0) not only reduces speed, but also lowers accuracy
Too few (K = N0) proposal also lead to lower accuracy
Hence, we choose  K = NN to extract our feature vector on Pascal VOC dataset, which is the last row in Table N
 N.N
Learning the easy vs
hard classifier  It is not trivial to learn the easy vs
hard SVM classifier,  even after we have fixed the feature representation
This  classification problem is both imbalanced [NN] and costsensitive [N], whose training requires special care
 As shown in Table N, most images are easy (i.e., suitable for the fast detector)
Hence, a classifier with low error  rate may make a lot of errors in the hard images
For example, if the classifier simply classifies all images as easy,  its classification accuracy is around N0% (could even be as  NN0N    0  N  N0  score_N  xmin_N  ymin_N  w_N  h_N score_N  xmin_N  ymin_N  w_N  h_N  score_k  xmin_k  ymin_k  w_k  h_k  … …  … …  … …  … …  … …  Instance Proposals Hard or Easy ?  ea sy  ha rd  Fast or Accurate ?  Basic  Partner Figure N: The proposed adaptive feeding framework
 Table N: Ablation studies about features of easy vs
hard classi- fication
‘N0”: histogram of predicted classes in top K proposals;  “N0-prob”: predicted class probabilities for one proposal; “conf”:  confidence score for one proposal; “Nc”: (xmin, ymin, xmax, ymax), where (xmin, ymin) and (xmax, ymax) are coordinates of  the top left and bottom right corners, respectively; “Ns”: (xmin,  ymin, w, h), where (w, h) is the size of each proposal; “×K”:  concatenate information from top K proposals;; “Acc.” and “Recall”: accuracy and recall of the easy vs
hard classification
Note  that SVM is trained on 0N+NN trainval
And “easy” and “hard”  images are balanced during the training process
 # Feature Acc
Recall 0N mAP FPS  0 raw inputs NN.N NN.N NN.N N N0+(conf+Nc)×NN NN.0 NN.N NN.N NN  N N0+(conf)×NN NN.N NN.N NN.N NN  N (N0-prob+conf+Ns)×NN NN.N NN.N NN.N NN  N N0+(conf+Ns)×N0 NN.N NN.N NN.N NN  N N0+(conf+Ns)×N0 NN.N NN.N NN.N NN  N N0+(conf+Ns)×NN NN.N N0.0 NN.0 NN  high as NN.N%, c.f 
Table N)
However, this simple classifier  will reduce AF to the fast detector, whose detections are not  accurate enough
 Beyond the classification accuracy, we also want the  classifier to correctly predict a high percentage of hard examples
This requires a high recall, which is the percentage of positive examples (y = N, i.e., hard images) to be correctly classified
A high recall ensures that the slow accurate detector is applied to appropriate examples such that  the AF detection results will be accurate
 A simple approach to solve the class imbalance problem  is to assign different resampling weights for hard and easy  images
Because we use SVM as our easy vs
hard classifier, this can be equivalently achieved by assigning different  misclassification costs for hard and easy images
Suppose  we have a set of training examples (xi, yi) (N ≤ i ≤ n),  where xi is the AF features extracted for the i-th image, and  yi ∈ {−N, N} is its label (easy or hard)
A linear classifier sgn(wTx+ b) is learned by solving the following standard SVM problem:  min w,b  N  N w  T w + C  n ∑  i=N  ξi (N)  s.t
yi (  w T xi + b  )  ≥ N− ξi, ξi ≥ 0, N ≤ i ≤ n , (N)  in which C > 0 is a hyperparameter that balances between large margin and small empirical error, and ξi is the cost  associated with the i-th image xi
 In this standard SVM formulation, easy and hard images  are treated equally
In order to obtain high recall, a classic  method is to assign different weights for different images
 We fix the resampling weight for easy images as c −N = N  and the resampling weight c+N > N for hard images [N]
A larger c+N value puts more emphasis on the correct classification of hard images, and hence will in general lead to  higher recall
The SVM problem is now  min w,b  N  N w  T w + C  n ∑  i=N  cyiξi (N)  s.t
yi (  w T xi + b  )  ≥ N− ξi, ξi ≥ 0, N ≤ i ≤ n 
(N)  Ablation Analysis
We start c+N from the balanced resampling ratio (i.e., ratio between the number of easy and  hard images), and gradually decrease it
In the pair of SSDN00 vs
SSDN00, as shown in Figure Na and Figure Na,  treating easy and hard examples equally (c+N = N) leads to a low recall rate and low detection mAP, but the detection  speed is very fast
When c+N gradually increases, the classification accuracy and fps gradually decrease but the recall  rate keeps increasing
Accordingly, the detection becomes  more accurate, but at the price of dropped detection speed
 The same trends hold for SSDN00 vs
R-FCN, too, as shown  in Figure Nb and Figure Nb
 We note that it is a practical method to adjust the tradeoff  between detection speed and accuracy by adjusting the reNN0N    (a) SSDN00 vs
SSDN00 (b) SSDN00 vs
R-FCN  Figure N: Impact of sampling weights on accuracy and recall of hard images of easy vs
hard classification
The experiments are  performed on VOC0N test
 (a) SSDN00 vs
SSDN00 (b) SSDN00 vs
R-FCN  Figure N: Impact of sampling weights on mAP and FPS
The experiments are performed on VOC0N test
 sampling weight c+N
When you care more about the precision, a balanced weight could be the first choice, otherwise  a lower weight might fit the situation
However, in both  cases, our AF achieves considerable speed-up ratio
 N
Experimental Results on Object Detection  We evaluate our method on the VOC N00N test set, VOC  N0NN test set [N] as well as MS COCO [NN]
We demonstrate  the effect on achieving fast and accurate detections when  combining two models using our AF approach
 N.N
Setup  We implement the SVM using scikit-learn [N0] and set  C = N
We use the LIBLINEAR solver in the primal s- pace
We use the default mode in scikit-learn for setting  the resampling weight
For the basic and partner models,  we directly use those publicly available pretrained detection  models without any change if not specifically mentioned
 All evaluations were carried out on an Nvidia Titan X GPU  card, using the Caffe deep learning framework [NN]
 For all experiments listed below, we utilize the Tiny YOLO detector as the instance generator if not otherwise  specified
We choose Tiny YOLO because it is one of the  fastest deep learning based general object detectors
SSDN00 is used as the basic model because it runs fast and  performs well
On the other side, SSDN00 and R-FCN are  Table N: VOC N00N test set detection mAP (%)
All detectors and the instance generator are trained on VOC0N+NN trainval
SVM is  trained on VOC0N+NN trainval
The Speed-Up Ratio (SUR) and  Decreased mAP (DmAP) are all based on partner model
A: the  accurate mode
F: the fast mode
W: the sampling weight of easy  to hard when training SVM
 Method W mAP FPS SUR DmAP  SSDN00 - NN.N NN - SSDN00 - NN.0 NN - SSDN00 - NN.N NN - Simple Ensemble - NN.0 NN - R-FCN - NN.0 N - N00-N00-A N.NN NN.0 NN NN% -0.N  N00-N00-F N NN.N NN NN% 0.N  N00-R-FCN-A N.NN NN.N NN NNN% 0.N  N00-R-FCN-F N NN.N NN N00% N.N  the partner models in different experiments, because their  detections are more accurate than SSDN00
 On the PASCAL VOC datasets, these models are trained  on VOC0N+NN trainval, and tested on both VOC0N test and  VOCNN test
For the sake of fairness, we don’t train with extra data (VOC0N test) when testing on VOCNN test
We also  conduct experiments on MS COCO [NN] and report numbers from the test-dev N0NN evaluation server
 N.N
PASCAL VOC N00N results  On this dataset, we perform experiments on two pairs  of models: SSDN00 vs
SSDN00 and SSDN00 vs
R-FCN,  and compare against SSDN00, SSDN00 and R-FCN
Specifically, the training data is VOC0N+NN trainval (NNNNN images) and test set is VOC0N test (NNNN images)
Experimental results are displayed in Table N
Since we do not  have an independent validation set, we train the SVM on  VOC0N+NN trainval
We randomly split the NNNNN images  into two parts, where the first part contains NN,000 images  and the second keeps the rest N,NNN images
We train our  SVM on the NNk set and validate it on the N.Nk images
We  use N0+(conf+Ns)×NN as the features for SVM learning be- cause it performs the best among different types of features  in Table N
 We provide two modes during the combination: accurate  (A) or fast (F)
The accurate mode takes a balanced sampling weight (N.NN for SSDN00, and N.NN for R-FCN), while  the fast mode uses a lower weight (N and N, respectively)
 Compared with SSDN00, N00-N00-A has a slightly higher  performance because the classifier makes the right choice  for those images fit for the basic model
N00-R-FCN-F even  outperforms SSDN00 by two percent points while runs Nfps faster
If we compare AF with R-FCN, N00-R-FCN-A  achieves NNN% speed-up ratio at a slight cost in mAP (0.N  NNN0    points)
As additional baselines, we also make experiments  on SSDN00 and a simple ensemble of SSDN00 and SSDN00
 We implement SSDN00 following the instructions in [NN]
 N00-N00-F surpass SSDN00 by 0.N points while reaches the  same speed with negligible training cost
The Simple Ensemble method brutely combines the detection results of  SSDN00 and SSDN00 but its mAP is worse than SSDN00
 N.N
PASCAL VOC N0NN results  The same two pairs of detectors are evaluated on the  VOCNN test set
Accurate and fast modes are both performed, respectively
For consideration of consistence, we  take the same group of sampling weights for all four combinations: N.NN for N00-N00-A, N.0 for N00-N00-F, N.NN for  N00-R-FCN-A, and N.0 for N00-R-FCN-F
Similar to VOC  N00N test, the SVM classifier is also trained on instance proposals from VOC0N+NN trainval
 Table N shows the results on VOC N0NN test
The AF  approach shows different effects in different pairs
For the  accurate mode, N00-N00-A improves the mAP from NN.N%  (SSDN00) to NN.N% which is the same as SSDN00, but is  Nfps faster than SSDN00 (about NN% speed-up ratio)
Even  though its speed (NNfps) is slower than the basic model (SSDN00, NNfps), its speed is still faster than what is required  for real-time processing
N00-R-FCN-A runs twice as fast  as R-FCN, while only loses N.0 mAP
For the fast mode,  N00-N00-F runs much faster while keeps comparable precision with SSDN00
N00-R-FCN-F not only performs N.0  points higher but also is Nfps faster when compared with  SSDN00
 N.N
MS COCO results  To further validate our approach, we train and test our  approach on the MS COCO dataset [NN]
All models are  trained on trainvalNNk [N] (including Tiny YOLO)
For convenience, we use minivalN0NN (about N,000 images) to train  the SVM
Note that minivalN0NN is not contained in trainvalNNk
Since there are N0 classes in MS COCO, we take  the top N0 proposals and the feature is (N0+(conf+Ns)×N0) for SVM learning
This datasets exhibits different property than the PASCAL VOC datasets
From Table N, in both  pairs, the ratio of easy to hard images is only slightly larger  than N, which can be explained by the fact that MS COCO is  “harder” than PASCAL VOC, because there are many small objects in it
However, although this ratio is not as large  as that in VOC datasets, the adaptive feeding method still  achieves convincing results
 Table N shows the AF results on MS COCO test-dev  N0NN
For the accurate mode, on the standard COCO evaluation metric, SSDN00 scores N0.N% AP, and our approach  improves it to NN.N%
It is also interesting to note that, since  SSDN00 and R-FCN are better at small and medium sized  objects, our approach improves SSDN00 mainly on these tTable N: Statistics on COCO minivalN0NN
 Basic Partner Set PN-PN  ≤ 0 (Easy) > 0 (Hard)  SSDN00 SSDN00 minivalN0NN NN.N% NN.N%  SSDN00 R-FCN minivalN0NN NN.N% NN.N%  Table N: MS COCO N0NN test-dev detection AP (%)
The SSD results are from [NN]
R-FCN is trained by ourselves because the  model in [N] hasn’t been released yet (slightly lower results than  the official ones)
 Method W test FPS AP APN0 APNN APS APM APL  SSDN00 - test-dev NN N0.N NN.0 N0.N N.N NN.N NN.N  SSDN00 - test-dev NN NN.N NN.N NN.N N.N NN.N N0.N  R-FCN - test-dev N NN.N NN.N N0.N N.N NN.N NN.N  N00-N00-A N.NN test-dev NN NN.N NN.N NN.N N.N NN.N NN.N  N00-N00-F N test-dev NN NN.0 NN.N NN.N N.0 NN.N NN.N  N00-R-FCN-A N.0N test-dev NN NN.0 NN.N NN.N N.N NN.N NN.N  N00-R-FCN-F N test-dev NN NN.N NN.N NN.N N.N NN.N NN.N  Table N: We categorize features into groups, which are defined in Table N
The table below shows the sum of weights  in each group (there are N0 and N0 classes in VOC and COCO, respectively)
For better comparison, we normalize the  sum of weights to N
 Method dataset class conf xmin ymin width height  N00-N00 voc 0.NN N.NN 0.0N 0.0N -0.NN -0.NN  N00-R-FCN voc 0.NN N.NN 0.NN 0.0N -0.NN -0.NN  N00-N00 coco 0.NN N.NN -0.0N -0.0N -0.NN -0.NN  N00-R-FCN coco 0.NN N.NN -0.0N -0.0N -0.N0 -0.NN  wo parts
 N.N
Feature Visualization  In this part, we want to find what makes an input image  an “easy” or “hard” one
To achieve this goal, a visualization of the learned SVM model is needed
We use linear  SVM and set +N for hard while -N for easy ones
Features  are learned on VOC0N+NN trainval and COCO minival, respectively
We only report results for the natural balancing weights, and it is worth noting that there can be small  changes when employing different class weights
 From Table N, we can see that "conf" (confidence in top-k  proposals) is the most influential factor
Thus, many highconfidence proposals make the input image a "hard" one
 This fits our intuition: an image with many objects might  be hard for a detector
 There are some other interesting observations N) large  proposal hints "easy" images
We argue that images with  NNNN    Table N: VOC N0NN test detection AP (%), mAP and speed (in FPS)
All detectors and the instance generator are trained on VOC0N+NN trainval
 Method W mAP FPS aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv  Fast R-CNN - N0.0 N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N N0.N NN.N N0.N N0.N  R-FCN - NN.N N NN.N N0.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.0 NN.N NN.0 NN.N NN.N NN.N  SSDN00 - NN.N NN NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N N0.N NN.N NN.N N0.N NN.N N0.N NN.N  SSDN00 - NN.N NN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  N00-N00-A N.NN NN.N NN NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  N00-N00-F N N0.N NN NN.N NN.0 N0.N NN.N NN.0 NN.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.0 NN.N NN.N NN.N NN.N NN.N N0.N NN.N  N00-R-FCN-A N.NN NN.N NN NN.N N0.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N  N00-R-FCN-F N NN.0 NN NN.0 NN.N NN.0 N0.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N  large objects often have few instances, especially in VOC  and COCO; N) "easy" images prefer shorter proposals  (w>h) while "hard" images like taller instances; N) xmin  and ymin have small weights, hence positions of proposals  have small impact
 N
Pedestrian Detection  Since pedestrian detection can be regarded as a special  case of object detection, we also apply our adaptive feeding approach to existing detectors on the Caltech Pedestrian  dataset
 The basic model employs a stand-alone region proposal network (RPN)
The original RPN in Faster R-CNN [NN]  is developed as a class-agnostic detector with multi-scale  anchors to describe different ratios of objects at each position
Zhang et al
[NN] found that a single RPN also comes  with good performance in pedestrian detection
In our experiments, we build RPN based on VGG-NN and follow the  strategies in [NN] when designing anchors on the Caltech  Pedestrian dataset
For the partner model, we directly use  CompACT [N], which proposed a novel algorithm for learning a complexity-aware cascade
In our case, we make use  of CompACT-Deep which incorporates CNN features into  the cascading detectors
 Note that RPN achieves an MR (missing rate) of NN.N%  on the Caltech Pedestrian testset at N0fps
CompACT-Deep  reaches NN.0% MR, but is Nfps slower than RPN
The easy  to hard ratio between these two detectors is N.NN, which  seems to be a good situation for AF
RPN is also used as  an instance generator here, which means every input image should first pass the RPN to make a decision
For  the feature inputs to SVM, we employ a similar format:  ((conf+Ns)×NN)
The settings of linear SVM are the same as those in object detection
 Experimental results are reported in Table N
With RPN  as the basic model, AF achieves satisfying speed-up ratio  while maintaining acceptable miss rate
With a sampling  weight of N.NN, the accurate mode is NN% faster than the  original CompACT-Deep model, at a cost of 0.N higher MR
When the weight drops to N, RPN-CompACT-F has N.0  Table N: MR (%) of AF on Caltech Pedestrian dataset
The Speed-Up Ratio (SUR) and Decreased MR (DMR) are all based  on the partner model
 Method W MR FPS SUR DMR  RPN - NN.N N0 - CompACT-Deep - NN.0 N - RPN-CompACT-A N.NN NN.N N NN% 0.N  RPN-CompACT-F N NN.N N NNN% N.N  points lower miss rate than RPN at a comparable running  speed
These experiments also show that the basic model  can be used as an instance generator
 N
Conclusions  We presented adaptive feeding (AF), a simple but effective method to combine existing object detectors for speed  or accuracy gains
Given one input image, AF makes a  choice on either the fast (but less accurate) or the accurate (but slow) detector should be applied
Hence, AF can  achieve fast and accurate detection simultaneously
The  other advantage of AF is that it needs no additional training data and the training time is negligible
By combining  different pairs of models, we reported state-of-the-art results on the PASCAL VOC, MS COCO and Caltech Pedestrian  datasets when detection speed and accuracy are both taken  into account
 Though we used pairs of models (one basic and one  partner model) throughout this paper, we believe AF can  be used with combinations of more than two region-based  ConvNet detectors
For example, a triplet combination can  adds an extra model, which is more accurate but slower than  those models in our experiments [NN, NN], to further improve  the detection accuracy without losing AF’s speed benefits
 Acknowledgements  This work was supported in part by the National Natural  Science Foundation of China under Grant No
NNNNNN0N
 NNNN    References  [N] A
Angelova, A
Krizhevsky, V
Vanhoucke, A
S
Ogale,  and D
Ferguson
Real-Time Pedestrian Detection with Deep  Network Cascades
In BMVC, pages NN.N–NN.NN, N0NN
N  [N] S
Bell, C
L
Zitnick, and R
Girshick
Inside-Outside Net:  Detecting Objects in Context with Skip Pooling and Recurrent Neural Networks
In CVPR, pages NNNN–NNNN, N0NN
 N  [N] Z
Cai, M
Saberian, and N
Vasconcelos
Learning  Complexity-Aware Cascades for Deep Pedestrian Detection
 In ICCV, pages NNNN–NNNN, N0NN
N  [N] J
Dai, Y
Li, K
He, and J
Sun
R-FCN: Object Detection  via Region-based Fully Convolutional Networks
In NIPS,  pages NNN–NNN, N0NN
N, N, N  [N] N
Dalal and B
Triggs
Histograms of oriented gradients for  human detection
In CVPR, volume N, pages NNN–NNN
IEEE,  N00N
N  [N] T
Dean, M
A
Ruzon, M
Segal, J
Shlens, S
Vijayanarasimhan, and J
Yagnik
Fast, Accurate Detection of  N00,000 Object Classes on a Single Machine
In CVPR,  pages NNNN–NNNN, N0NN
N  [N] C
Elkan
The Foundations of Cost-Sensitive Learning
In  IJCAI, volume N, pages NNN–NNN, N00N
N, N  [N] M
Everingham, S
M
Ali Eslami, L
Van Gool,  C
K
I
Williams, J
Winn, and A
Zisserman
The PASCAL  Visual Object Classes Challenge: A Retrospective
IJCV,  NNN(N):NN–NNN, N0NN
N, N  [N] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn, and  A
Zisserman
The PASCAL Visual Object Classes (VOC)  Challenge
IJCV, NN(N):N0N–NNN, N0N0
N  [N0] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and D
Ramanan
Object Detection with Discriminatively Trained Part  Based Models
PAMI, NN(N):NNNN–NNNN, N0N0
N  [NN] S
Gidaris and N
Komodakis
Object detection via a multiregion and semantic segmentation-aware CNN model
In ICCV, pages NNNN–NNNN, N0NN
N  [NN] R
Girshick
Fast R-CNN
In ICCV, pages NNN0–NNNN, N0NN
 N  [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, pages NN0–NNN, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Spatial pyramid pooling  in deep convolutional networks for visual recognition
PAMI,  NN(N):NN0N–NNNN, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep Residual Learning  for Image Recognition
In CVPR, pages NN0–NNN, N0NN
N  [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional Architecture for Fast Feature Embedding
In ACM MM,  pages NNN–NNN, N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, L
Bourdev, R
Girshick,  J
Hays, P
Perona, D
Ramanan, C
L
Zitnick, and P
Dollar
 Microsoft COCO: Common Objects in Context
In arXiv  preprint arXiv:NN0N.0NNNvN, N0NN
N, N, N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Christian,  F
Cheng-Yang, and C
Alexander
SSD: Single Shot MultiBox Detector
In ECCV, pages NN–NN, N0NN
N, N, N, N  [NN] X.-Y
Liu, J
Wu, and Z.-H
Zhou
Exploratory undersampling for class-imbalance learning
IEEE Trans
on  Systems, Man, and Cybernetics – Part B: Cybernetics,  NN(N):NNN–NN0, N00N
N  [N0] F
Pedregosa, G
Varoquaux, A
Gramfort, V
Michel,  B
Thirion, O
Grisel, M
Blondel, P
Prettenhofer, R
Weiss,  V
Dubourg, et al
Scikit-learn: Machine learning in Python
 JMLR, NN(Oct):NNNN–NNN0, N0NN
N  [NN] H
Qin, J
Yan, X
Li, and X
Hu
Joint Training of Cascaded  CNN for Face Detection
In CVPR, pages NNNN–NNNN, N0NN
 N  [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  Only Look Once: Unified, Real-Time Object Detection
In  CVPR, pages NNN–NNN, N0NN
N, N, N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN:  Towards Real-Time Object Detection with Region Proposal  Networks
In NIPS, pages NN–NN, N0NN
N, N  [NN] M
A
Sadeghi and D
Forsyth
N0Hz Object Detection with  DPM VN
In ECCV, pages NN–NN, N0NN
N  [NN] G
Salton and M
J
McGill
Introduction to Modern Information Retrieval
McGraw-Hill, Inc
New York, NNNN
N  [NN] A
Shrivastava, A
Gupta, and R
Girshick
Training RegionBased Object Detectors with Online Hard Example Mining
 In CVPR, pages NNN–NNN, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N  [NN] S
Teerapittayanon, B
McDanel, and H
Kung
BranchyNet:  Fast Inference via Early Exiting from Deep Neural Networks
In ICPR, pages NNNN–NNNN, N0NN
N  [NN] P
Viola and M
Jones
Rapid Object Detection using a Boosted Cascade of Simple Features
In CVPR, pages NNN–NNN,  N00N
N  [N0] J
Yan, Z
Lei, L
Wen, and S
Z
Li
The Fastest Deformable  Part Model for Object Detection
In CVPR, pages NNNN–  NN0N, N0NN
N  [NN] B
Yang, J
Yan, Z
Lei, and S
Z
Li
Craft objects from  images
In CVPR, N0NN
N  [NN] F
Yang, W
Choi, and Y
Lin
Exploit All the Layers: Fast  and Accurate CNN Object Detector with Scale Dependent  Pooling and Cascaded Rejection Classifiers
In CVPR, pages  NNNN–NNNN, N0NN
N  [NN] L
Zhang, L
Liang, X
Liang, and K
He
Is Faster R-CNN  Doing Well for Pedestrian Detection? In ECCV, pages NNN–  NNN, N0NN
N  [NN] Q
Zhu, Y
Mei-Chen, K.-T
Cheng, and S
Avidan
Fast human detection using a cascade of histograms of oriented gradients
In CVPR, pages NNNN–NNNN, N00N
N  NNNNWeakly Supervised Object Localization Using Things and Stuff Transfer   Weakly Supervised Object Localization Using Things and Stuff Transfer  Miaojing ShiN,N Holger CaesarN  NUniversity of Edinburgh NTencent Youtu Lab  name.surname@ed.ac.uk  Vittorio FerrariN  Abstract  We propose to help weakly supervised object localization  for classes where location annotations are not available, by  transferring things and stuff knowledge from a source set  with available annotations
The source and target classes  might share similar appearance (e.g
bear fur is similar to  cat fur) or appear against similar background (e.g
horse  and sheep appear against grass)
To exploit this, we acquire  three types of knowledge from the source set: a segmentation model trained on both thing and stuff classes; similarity relations between target and source classes; and cooccurrence relations between thing and stuff classes in the  source
The segmentation model is used to generate thing  and stuff segmentation maps on a target image, while the  class similarity and co-occurrence knowledge help refining  them
We then incorporate these maps as new cues into a  multiple instance learning framework (MIL), propagating  the transferred knowledge from the pixel level to the object proposal level
In extensive experiments, we conduct  our transfer from the PASCAL Context dataset (source) to  the ILSVRC, COCO and PASCAL VOC N00N datasets (targets)
We evaluate our transfer across widely different thing  classes, including some that are not similar in appearance,  but appear against similar background
The results demonstrate significant improvement over standard MIL, and we  outperform the state-of-the-art in the transfer setting
 N
Introduction  The goal of object class detection is to place a tight  bounding box on every instance of an object class
Given  an input image, recent object detectors [N, N, N, N] first extract object proposals [N, N, N] and then score them with  a classifier to determine their probabilities of containing an  instance of the class
Manually annotated bounding boxes  are typically required for training (full supervision)
 Annotating bounding boxes is tedious and timeconsuming
In order to reduce the annotation cost, many  previous works learn the detector in a weakly supervised  setting [N, N, N, N0, NN, NN, NN, NN, NN], i.e
given a set of  images known to contain instances of a certain object class,  but without their locations
This weakly supervised object  localization (WSOL) bypasses the need for bounding box  annotation and substantially reduces annotation time
 Despite the low annotation cost, the performance of  WSOL is considerably lower than that of full supervision
 To improve WSOL, various advanced cues can be added,  e.g
objectness [N0, NN, N, NN, NN, NN], which gives an estimation of how likely a proposal contains an object; cooccurrence among multiple classes in the same training  images [NN]; object size estimates based on an auxiliary  dataset with size annotations [NN]; and appearance models  transferred from object classes with bounding box annotations to new object classes [NN, N0, NN]
 There are two types of classes that can be transferred  from a source set with manually annotated locations: things  (objects) and stuff (materials and backgrounds)
Things  have a specific spatial extent and shape (e.g
helicopter,  cow, car), while stuff does not (e.g
sky, grass, road)
Current transfer works mostly focus on transferring appearance  models among similar thing classes [NN, NN, N0] (things-tothings)
In contrast, using stuff to find things [NN, NN] is  largely unexplored, particularly in the WSOL setting (stuffto-things)
 In this paper, we transfer a fully supervised segmentation model from the source set to help WSOL on the target  set
We introduce several schemes to conduct the transfer  of both things and stuff knowledge, guided by the similarity  between classes
Particularly, we transfer the co-occurrence  knowledge between thing and stuff classes in the source via  a second order scheme to thing classes in the target
We  propagate the transferred knowledge from the pixel level to  the object proposal level and inject it as a new cue into a  multiple instance learning framework (MIL)
 In extensive experiments, we show that our method: (N)  improves over a standard MIL baseline on three datasets:  ILSVRC [NN], COCO [NN], PASCAL VOC N00N [NN];  (N) outperforms the things-to-things transfer method [NN]  and the state-of-the-art WSOL methods [NN, N, NN] on  VOC N00N; (N) outperforms another things-to-things transfer method (LSDA [N0]) on ILSVRC
 NNNN    Transferring Knowledge to Target B  Pixels-to-Proposals   Score R  Semantic Segmentation Model   bear  dog  table cat  car 0.N  0.N  0.N 0.N  Acquiring Knowledge from Source A  SimilarityCo-occurrence  Weighting Stuff  Weighting Things  Contrast   Weighting  Label   Weighting  Area   Weighting  Things T  Label L   Input: Bear  (cat, grass)  (cat, ground)  (cat, tree)  Stuff S  Things-to-Things  Stuff-to-Things  tree  tree cat  table car  Figure N: An overview of our things and stuff transfer (TST) method
We acquire the N) segmentation model, N) co-occurrence relation and N) similarity relation from the source A and transfer them to the target B
We use the segmentation model to generate two maps:  thing (T ) and stuff (S) maps; each of them contains one score (R) map and one label (L) map
The knowledge of class similarity and  co-occurrence is specifically transferred as weighting functions to the thing and stuff label maps
Based on the transferred knowledge,  we propose three scoring schemes (label weighting, contrast weighting, and area weighting) to propagate the information from pixels to  proposals
The rightmost image column illustrates some highly ranked proposals in the image by gradually adopting the three schemes
 N
Related Work  Weakly supervised object localization
In WSOL the  training images are known to contain instances of a certain  object class but their locations are unknown
The task is  both to localize the objects in the training images and to  learn a detector for the class
 Due to the use of strong CNN features [NN, N0], recent  works on WSOL [N, N, NN, NN, NN, NN] have shown remarkable progress
Moreover, researchers also tried to incorporate various advanced cues into the WSOL process,  e.g
objectness [N, N0, NN, NN, NN], object size [NN], cooccurrence [NN] among classes, and transferring appearance  models of the source thing classes to help localize similar  target thing classes [NN, NN, N0]
This paper introduces a  new cue called things and stuff transfer (TST), which learns  a semantic segmentation model from the source on both  things and stuff annotations and transfers its knowledge to  help localize the target thing class
 Transfer learning
The goal of transfer learning is to improve the learning of a target task by leveraging knowledge from a source task [NN]
It is intensively studied  in image classification, segmentation and object detection [NN, NN, NN, NN, NN, NN, NN]
Many methods use the  parameters of the source classifiers as priors for the target  model [NN, NN, NN]
Other works [NN, NN] transfer knowledge through an intermediate attribute layer, which captures visual qualities shared by many object classes (e.g
 “striped”, “yellow”)
A third family of works transfer object parts between classes [NN, NN, NN], e.g
wheels between  cars and bicycles
 In this work we are interested in the task where we have  the location annotations in the source and transfer them to  help learn the classes in the target [NN, NN, NN, NN, NN, NN]
 We categorize the transfer into two types: N) Things-tothings
Guillaumin et al
[NN] transferred spatial location,  appearance, and context information from the source thing  classes to localize the things in the target; Shi et al
[NN] and  Rochan et al
[NN] follow a similar spirit to [NN]; while Kuettel et al
[NN] instead transferred segmentation masks
N)  Stuff-to-things
Heitz et al
[NN] proposed a context model  to utilize stuff regions to find things, in a fully supervised  setting for the target objects; Lee et al
[NN] also made use  of stuff annotations in the source to discover things in the  target, in an unsupervised setting
 Our work offers several new elements over these: (N)  we encode the transfer as a combination of both things-tothings and stuff-to-things; (N) we propose a model to propagate the transferred knowledge from the pixel level to the  proposal level; (N) we introduce a second order transfer, i.e
 stuff-to-things-to-things
 N
Overview of our method  In this section we define the notations and introduce our  method on a high level, providing some details for each part
 Notations
We have a source set A and a target set B
We have every image pixelwise annotated for both stuff and  things in A; whereas we have only image level labels for images in B
We denote by AT the set of thing classes in A, and at an individual thing class; analogue we have AS  and as for stuff classes in A and BT and bt for thing classes in B
Note that there are no stuff classes in B, as datasets labeled only by thing classes are more common in practice  (e.g
PASCAL VOC [N0], ImageNet [NN], COCO [NN])
 Method overview
Our goal is to conduct WSOL on B, where the training images are known to contain instances  of a certain object class but their locations are unknown
A  standard WSOL approach, e.g
MIL, treats images as bags  NNNN    of object proposals [N, N, N] (instances)
The task is both  to localize the objects (select the best proposal) in the training images and to learn a detector for the target class
To  improve MIL, we transfer knowledge from A to B, incor- porating new cues into it
 Fig
N illustrates our transfer
We first acquire three types  of knowledge in the source A (Sec
N): N) a semantic seg- mentation model (Sec
N.N), N) the thing class similarities  between A and B (Sec
N.N) and N) the co-occurrence fre- quencies between thing and stuff classes in A (Sec
N.N)
Afterwards, we transfer the knowledge to B (Sec
N)
Given an image in B, we first use the segmentation model to gener- ate the thing (T ) and stuff (S) maps of it (Sec
N.N)
T contains one score map (R) and one label (L) map, so does S
 The segmentation model transfers knowledge generically to  every image in B
Building upon its result, we propose three proposal scoring schemes: label weighting (LW, Sec
N.N),  contrast weighting (CW, Sec
N.N), and area weighting (AW,  Sec
N.N)
These link the pixel level segmentation to the proposal level score
In each scheme, two scoring functions are  proposed separately on thing and stuff maps
We combine  the three schemes to provide an even better proposal score  to help MIL (Sec N.N)
 Scoring schemes
LW transfers the similarity and cooccurrence relations as weighting functions to the thing and  stuff label maps, respectively
Since we do not have stuff  annotations on B, we conduct the co-occurrence knowl- edge transfer as a second-order transfer by finding the target  class’ most similar thing class in A
We believe that the tar- get class should appear against a similar background with  its most similar class
For example, in Fig
N target class  bear’s most similar class in A is cat, LW up-weights the cat score on T and its frequently co-occurring tree score on S
 LW favours small proposals with high weighted scores
 To counter this effect, we introduce the CW score
It measures the dissimilarity of a proposal to its surroundings,  measured on the thing/stuff score maps (Fig
N)
CW upweights proposals that are more likely to contain an entire  object in T or an entire stuff region in S
 Finally, the AW score encourages proposals to incorporate as much as possible of the connected components of  pixels on a target’s K most similar classes in A (e.g
Fig
N: the cat area in the T map)
While CW favors objects in general, AW focuses on objects of the target class in particular
 N
Acquiring knowledge from the source A  N.N
Segmentation model  We employ the popular fully convolutional network  (FCN-NNs) [NN] to train an end-to-end semantic segmentation model on both thing and stuff classes of A
Given a new image, the FCN model is able to predict a likelihood  distribution over all classes at each pixel
Notice that the  FCN model is first pretrained for image classification on  ILSVRC N0NN [NN], then fine-tuned for semantic segmentation on A
While it is possible that some of the target classes are seen during pretraining, only image-level labels  are used
Therefore the weakly supervised setting still holds  for the target classes
 N.N
Similarity relations  We compute the thing class similarities V (at, bt) be- tween any thing class pair (at, bt)
We propose two simi- larity measures to compute V as follows:  Appearance similarity
Every image in A or B is repre- sented by a N0NN-dimensional CNN feature vector covering the whole image, using the output of the fcN layer of  the AlexNet CNN architecture [N0]
The similarity of two  images is the inner product of their feature vectors
The  similarity VAPP(a t, bt) is therefore the average similarity  between images in at and images in bt
 Semantic similarity
We compute the commonly used  Lin [NN] similarity VSEM(a t, bt) between two nouns bt and  at in the WordNet hierarchy [NN]
 N.N
Co-occurrence relation  We denote by U(as, at) the co-occurrence frequency of any stuff and thing class pair (as, at) in A
This frequency is computed and normalized over all the images in A
 N
Transferring knowledge to the target B  This section transfers the source knowledge to the target set B
In this set, we have access only to image level labels, but no location annotations
We call the classes that  are listed on the image level label list target classes
Given a  new image of class bt, we first use the FCN model trained on  A to generate the thing (T ) and stuff (S) segmentations sep- arately (Sec
N.N)
Then we introduce three proposal scoring schemes to propagate the information from pixel level to  proposal level (Sec
N.N - N.N)
Finally we combine the three  scoring schemes into a single window score (Sec
N.N)
The  scoring scheme parameters are learned in Sec
N.N
 N.N
Generating thing and stuff segmentations  We apply the trained FCN model (Sec
N.N) to a target  image in B
Usually, the output semantic segmentation is obtained by maximizing over all the class scores at each  pixel [NN, NN, NN, NN, NN, NN]
In this paper, we instead generate two output segmentations, one for things T and one  for stuff S
We denote i as the i-th pixel in the image
We  use RT = {rT i } and LT = {lT  i } to denote the score (R)  and label (L) maps for T 
They are generated by keeping  the maximum score and the corresponding label over all the  thing classes AT at each pixel i
Similar to RT and LT , RS = {rS  i } and LS = {lS  i } are generated by keeping the  maximum score over all the stuff classes AS at each pixel
 NNNN    cat  potted plant  wall  bt: bear RT V(li T, bt)  bt: baby bed RS U(li S, NN(bt))  Figure N: Label weighting example
Top: thing label weighting (class bear); bottom: stuff label weighting (class baby bed)
RT  and RS denote the thing and stuff score heatmaps, respectively;  while V (lTi , b t) and U(lSi ,NN(b  t)) denote the thing and stuff la- bel weighting heatmaps
We illustrate some proposals in each image
We print the dominantly predicted labels in the proposals to  show how label weighting favours bt’s NN class in thing maps and  its frequently co-occurring stuff class in stuff maps
 Fig
N shows an example of a bear image (target)
The  thing and stuff maps are produced by the semantic segmentation model
The R heatmaps indicate the probability of  assigning a certain thing or stuff label to each pixel
Building upon these heatmaps, we propose three proposal scoring  schemes to link the pixel level result to the proposal level  score (Sec
N.N - N.N)
These try to give high scores to proposals containing the target class
 N.N
Label weighting (LW)  Because bear is more similar to cat than to table, we want  to up-weight the proposal area in the thing map if it is predicted as cat
Meanwhile, because bear frequently appears  against tree, we also want to up-weight the proposal area in  the stuff map if it is predicted as tree
To do this, we transfer  the knowledge of similarity and co-occurrence relations acquired in the source to the target class (bear), and use both  relations to modulate the segmentation scores in T and S
 Both relations and segmentation scores play a role in the  label weighting proposal scoring scheme
 Thing label weighting
We can generate a thing label  weighting map depending on how close the predicted class  lT i  at pixel i in LT is to the target class bt
The thing label  (lT i  ) weight is given by the class similarity score V (lT i , bt)  (Sec
N.N)
In Fig
N the target class bear is more similar  to cat than to table
If a pixel is predicted as cat, then we  assign a high label weight, otherwise we assign a low one
 Stuff label weighting
We do not have stuff annotations  in B
To conduct the stuff label weighting, we first find bt’s most similar thing class in AT according to a similarity rela- tion V (we denote it by NN(bt))
We believe that bt should appear against a similar background (stuff) as its most similar thing class NN(bt)
We employ the co-occurrence fre- quency U(lS  i ,NN(bt)) of NN(bt) as the corresponding stuff  label weight for lS i  at pixel i as stuff label weighting LS 
 In Fig
N, cat frequently co-occurs with trees, and so does  bear
So, if a certain pixel is predicted as tree, it gets assigned a high stuff label weight
 Proposal scoring
To score the proposals in an image,  we multiply the label weights V (lT i , bt) and U(lS  i ,NN(bt))  with the segmentation scores rT i  and rS i  at each pixel
 The weighting scheme is conducted separately on T and  S
Given a window proposal w, we average the weighted  scores inside w:  LWt(w,αt) = f( N|w| ∑  i∈w r T i V (lT  i , bt), αt)  LWs(w,αs) = f( N|w| ∑  i∈w r S i U(lS  i ,NN(bt)), αs)  (N)  where |w| denotes the size of w (area in pixels)
We apply an exponential function f(x) = exp(α ·x) to both thing and stuff LWs, αt and αs are the parameters
 Fig
N offers two examples (bear and baby bed) for our  thing and stuff label weighting schemes
The red proposal  in the top row is mostly classified as a cat and the green  proposal as a potted plant
Both proposals have high scores  in the thing score map RT , but the red proposal has a higher  thing label weight V (lT i , bt), because cat is more similar to  bear than to potted plant
In contrast, the green proposal in  the bottom row has low scores in RS but a high label weight  U(lT i ,NN(bt)), as baby bed co-occurs more frequently with  wall
 Notice that the thing label weighting can be viewed as a  first-order transfer where the information goes directly from  the source thing classes to the target thing classes
Instead,  the stuff label weighting can be viewed as second-order  transfer where the information first goes from the source  stuff classes to the source thing classes, and then to the  target thing classes
To the best of our knowledge, such  second-order transfer has not been proposed before
 N.N
Contrast weighting (CW)  The LW scheme favours small proposals with high label weights, which typically cover only part of an object  (top right image in Fig
N)
To counter this effect, contrast  weighting (CW) measures the dissimilarity of a proposal  to its immediate surrounding area on the thing/stuff score  maps
It up-weights proposals that are more likely to contain an entire object or an entire stuff region
 The surrounding Surr(w, θ) of a proposal w is a rectan- gular ring obtained by enlarging it by a factor θ in all directions [N] (Fig
N, the yellow ring)
The CW between a window and its surrounding ring is computed as the Chi-square  distance between their score map (R) histograms h(·)  CW(w, θ) = χN(h(w), h(Surr(w, θ))) (N)  We apply the CW scheme on both RT and RS and obtain  CWt(w, θt) and CWs(w, θs)
In Fig
N the red proposal has a higher CWt score compared to the green one
 NNNN    Figure N: Contrast weighting example
An image of a tape player and some of its window proposals (left)
CWt is computed on the thing score map Rt (right)
The red proposal has a higher contrast  CWt (with its surrounding dashed ring) than the green one
 N.N
Area weighting (AW)  Thing area weighting
Fig
N gives an example of an  electric fan and its semantic segmentation map
Its N-NN classes in terms of appearance similarity (Sec
N.N) are table,  chair and people
Between the white and yellow proposals, the CW scheme gives a bigger score to the white one,  because its contrast is high
Instead, the yellow proposal  incorporates most of the fan area, but is unfortunately predicted as table and chair
The thing area weighting scheme  helps here boosting the yellow proposal’s score
We find  the K-NN classes of bt in AT by using one of the similar- ity measures in Sec
N.N
Given a window w, we denote by  Area(w, bt) the segment areas of any K-NN(bt) inside w; while Area(O(w), bt) is the area that expands the current segments to their connected components inside and outside  w
We measure the area ratio between the segments and  their corresponding connected components:  Ratiot(w) = Area(w, bt)  Area(O(w), bt) (N)  If none of the K-NN classes occurs in w, we simply set  Ratiot to zero
Throughout this paper, K is set to N
 Stuff area weighting
In Fig
N among the three proposals,  the green one is the best detection of the fan
However, its  score is not the highest according to LWt, CWt and AWt, as it contains some stuff area (wall) surrounding the fan
A  bounding box usually has to incorporate some stuff area to  fit an object tightly, as objects are rarely perfectly rectangleshaped
We propose to up-weight a window w if stuff occupies a small but non-zero fraction of the window
We denote  with Ratios(w) the percentage of stuff pixels in window w
For thing and stuff area weighting we apply a cumulative  distribution function (CDF) of the normal distribution  AWt(w, µt, σt) = CDF(Ratiot(w)|µt, σt) AWs(w, µs, σs) = CDF(Ratios(w)|µs, σs)  (N)  where µt and σt are the mean and standard deviation
We  choose µt = µs = 0 and σt, σs are free parameters (Sec
N.N)
 Figure N: Area weighting example
An image of an electric fan (left) and its semantic segmentation (right)
Thing area weighting  favours the yellow proposal compared to the white one, as it incorporates most of the the connected component area of table and  chair
Stuff area weighting further favours the green proposal as it  allows certain stuff area in a proposal as the surrounding area of  electric fan
 N.N
Combining the scoring schemes  For each proposal in an image, the above scoring  schemes can be independently computed, each on the thing  and stuff map
The scoring schemes tackle different problems, and are complementary to each other
This sections  combines them to give our final TST (things and stuff transfer) window score W 
 All the scoring functions on the thing map are multiplied  together as a thing score W t = LWt ∗ CWt ∗ AWt
This gives a higher score if a proposal mostly contains a target  thing labeled as present in that image
Similarly, we have  the stuff score W s = LWs ∗ CWs ∗ AWs, which gives a higher score if a proposal mostly contains stuff
To combine  the thing and stuff scores, we simply subtract W s from W t  W = W t −W s (N)  N.N
Parameter learning  In the WSOL setting, we do not have the ground truth  bounding box annotations in the target set B
Thus we learn the score parameters αt, αs, θt, θs, σt and σs on the source  set A, where we have ground truth
We train the semantic segmentation model on the train set of A, and then apply it to the val set of A
For each image in the val set, we rank all its proposals using (N)
We jointly learn the score  parameters by maximizing the performance over the entire  validation set
 N
Overall system  In WSOL, given the target training set in B with image level labels, the goal is to localize the object instances in  it and to train good object detectors for the target test set
 We explain here how we build a complete WSOL system by  building on a MIL framework and incorporating our transfer  cues into it
 Basic MIL
We build a Basic MIL pipeline as follows
We  represent each image in the target set B as a bag of ob- ject proposals extracted using Edge Boxes [N]
They return  NNNN    Method APP SEM  Basic MIL NN.N  DT ≈ [NN] (transfer only) NN.0 DT + MIL ≈ [NN] (full) NN.N TST NN.N NN.0  Basic MIL + Objectness [N] NN.N  DT ≈ [NN] (transfer only) NN.N DT + MIL ≈ [NN] (full) NN.N TST NN.N NN.N  Deep MIL + Objectness [N] NN.N  TST NN.0 NN.N  TST + ILSVRC-dets - NN.N  Table N: CorLoc on ILSVRC-N0; DT: direct transfer; DT+MIL: direct transfer plus MIL
TST is our method; ILSVRC-dets:  Sec
N.N, last paragraph
The transfers are guided by either the  semantic (SEM) or the appearance (APP) class similarity
 about N,000 proposals per image, likely to cover all objects
 Following [NN, N, NN, NN, NN], we describe the proposals by  the output of the fcN layer of the AlexNet CNN architecture [N0]
The CNN model is pre-trained for whole-image  classification on ILSVRC [NN], using the Caffe implementation [NN]
This produces a N,0NN-dimensional feature vector for each proposal
Based on this feature representation  for each target class, we iteratively build an SVM appearance model (object detector) in two alternating steps: (N)  Re-localization: in each positive image, we select the highest scoring proposal by the SVM
This produces the positive set which contains the current selection of one instance  from each positive image
(N) Re-training: we train the  SVM using the current selection of positive samples, and  all proposals from the negative images as negative samples
 As in [N0, NN, N, NN, NN], we also linearly combine the SVM  score with a general measure of objectness [N, N]
This leads  to a higher MIL baseline
 Incorporating things and stuff transfer (TST)
We incorporate our things and stuff transfer (TST) into Basic MIL by  linearly combining the SVM score with our proposal scoring function (N)
Note how the behavior of (N) depends on  the class similarity measure used within it (either appearance or semantic similarity, Sec
N.N)
 Deep MIL
Basic MIL uses an SVM on top of fixed deep  features as the appearance model
Now we change the  model to fine-tune all layers of the deep network during the  re-training step of MIL
We take the output of Basic MIL as  an initialization for two additional MIL iterations
During  these iterations, we use Fast R-CNN [N]
 N
Experiments  N.N
Datasets and evaluation protocol  We use one source set A (PASCAL Context) and several different target sets B in turn (ILSVRC-N0, COCO-0N and  PASCAL VOC N00N)
Each target set contains a training set  and a test set
We perform WSOL on the target training set  to localize objects within it
Then we train a Fast R-CNN [N]  detector from it and apply it on the target test set
 Evaluation protocol
We quantify localization performance in the target training set with the CorLoc measure [N, N, N0, N0, NN, NN]
We quantify object detection performance on the target test set using mean average precision (mAP)
As in most previous WSOL methods [N, N, N, N, N0, NN, NN, NN, NN, NN], our scheme returns  exactly one bounding-box per class per training image
At  test time the object detector is capable of localizing multiple objects of the same class in the same image (and this is  captured in the mAP measure)
 Source set: PASCAL Context
PASCAL Context [NN]  augments PASCAL VOC N0N0 [NN] with class labels at every pixel
As in [NN], we select the NN most frequent classes
 We categorize them into things and stuff
There are N0 thing  classes, including the original N0 PASCAL classes and new  classes such as book, cup and window
There are NN stuff  classes, such as sky, water and grass
We train the semantic  segmentation model (Sec
N.N) on the train set of A and set the score parameters (Sec
N.N) on the val set, using the N0  PASCAL classes from A as targets
 Target set: ILSVRC-N0
The ILSVRC [NN] dataset  originates from the ImageNet dataset [NN], but is much  harder [NN]
As the target training set we use the trainN0k  subset [NN] of ILSVRC N0NN
As the target test set we use  the N0k images of the validation set
To conduct WSOL on  trainN0k, we carefully select N0 target classes: ant, babybed, basketball, bear, burrito, butterfly, cello, coffee-maker,  electric-fan, elephant, goldfish, golfcart, monkey, pizza,  rabbit, strainer, tape-player, turtle, waffle-iron and whale
 ILSVRC-N0 contains N,NNN target training set images and  NNN target test set images
This selection is good because:  (N) they are visually considerably different from any source  class; (N) they appear against similar background classes  as the source classes, so we can show the benefits of stuff  transfer; (N) they are diverse, covering a broad range of object types
 Target set: COCO-0N
The COCO N0NN [NN] dataset has  fewer object classes (N0) than ILSVRC (N00), but more instances
COCO is generally more difficult than ILSVRC  for detection, as objects are smaller [NN]
There are also  more instances per image: N.N in COCO compared to N.0  in ILSVRC [NN]
We select N target classes to carry out  WSOL: apple, giraffe, kite, microwave, snowboard, tennis  racket and toilet
COCO-0N contains NN,NNN target training  set images and N,NNN target test set images
 Target set: PASCAL VOC N00N
The PASCAL VOC  N00N [N0] dataset is one of the most important object detection datasets
It includes N,0NN training (trainval) images  and N,NNN test images, which we directly use as our target  NNNN    Class ant bbed bask bear burr butt cell cmak efan elep gfis gcar monk pizz rabb stra tpla turt wiro whal Avg
Avg.(N)  LSDA [N0] - - - - - - - - - - - - NN.N NN.N N0.N N.N NN.N NN.N N.N N0.N - NN.N  Deep MIL+Obj
NN.N NN.N 0.N NN.0 NN.N NN.N NN.N N.N NN.N NN.N N.N N0.N NN.0 NN.N NN.N NN.N NN.0 NN.0 N.N N.N NN.N N0.N  +TST (APP) NN.N NN.0 0.N NN.N NN.N NN.N NN.0 N.0 NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N N.N N.N NN.N NN.N  +TST (SEM) NN.N NN.N 0.N NN.N NN.N NN.N NN.N N.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N N.N N.N NN.0 NN.N  + ILSVRC-dets NN.N NN.N N.N NN.N NN.N NN.N NN.N N.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N.N N.N NN.N NN.N  Table N: mAP Performance on the test set of ILSVRC-N0
All our methods start from DeepMIL with objectness
For comparison we also show the performance on the N classes common to our target set and that of LSDA [N0]
 training set and target test set, respectively
For our experiments we use all N0 thing classes in VOC N00N
Since the  thing classes in our source set (PASCAL Context) overlap  with those of VOC N00N, when doing our TST transfer to  a target class we remove it from the sources
For example,  when we transfer to “dog” in VOC N00N, we remove “dog”  from the FCN model trained on PASCAL Context
 N.N
ILSVRC-N0  Table N presents results for our method (TST) and several  alternative methods on ILSVRC-N0
 Our transfer (TST)
Our results (TST) vary depending  on the underlying class similarity measure used, either appearance (APP) or semantic (SEM) (Sec
N.N)
TST (APP)  leads to slightly better results than TST (SEM)
We achieve  a +N% improvement in CorLoc (NN.N) compared to Basic MIL without objectness, and +N% improvement (NN.N) over Basic MIL with objectness
Hence, our transfer method is  effective, and is complementary to objectness
Fig
N shows  example localizations by Basic MIL with objectness and  TST (APP)
 Comparison to direct transfer (DT)
We compare here to  a simpler way to transfer knowledge
We train a fully supervised object detector for each source thing class
Then,  for every target class we find the most similar source class  from the N0 PASCAL Context thing classes, and use it to  directly detect the target objects
For the appearance similarity measure (APP) all NN classes of ILSVRC-N0 are  part of PASCAL VOC and PASCAL Context
Therefore  we have bounding box annotations for these classes
However, for the semantic similarity measure (SEM) not all NN  classes of ILSVRC-N0 are part of PASCAL VOC
Therefore  we do not have bounding box annotations for these classes  and cannot apply DT
DT is similar to the ‘transfer only’  method in [NN] (see Sec
N.N and Table N in [NN])
 As Table N shows, the results are quite poor as the source  and target classes are visually quite different, e.g
the most  similar class to ant according to APP is bird; while for  waffle-iron, it is table; for golfcart, it is person
This shows  that the transfer task we address (from PASCAL Context to  ILSVRC-N0) is challenging and cannot be solved by simply  using object detectors pre-trained on the source classes
 Comparison to direct transfer with MIL (DT+MIL)
We  improve the direct transfer method by using the DT detector  to score all proposals in a target image, and then combining  this score with the standard SVM score for the target class  during the MIL re-localization step
This is very similar to  the full method of [NN] and is also close to [NN]
The main  difference from [NN] is that we train the target class’ SVM  model in an MIL framework (Sec
N), whereas [NN] simply  trains it by using proposals with high objectness as positive  samples
 As Table N shows, DT+MIL performs substantially better than DT alone, but it only slightly exceeds MIL without transfer, again due to the source and target classes being visually different (+N.N% over Basic MIL with object- ness)
Importantly, our method (TST) achieves higher results, demonstrating that it is a better way to transfer knowledge (+N% over Basic MIL with objectness)
 Deep MIL
As Table N shows, Deep MIL improves slightly  over Basic MIL (from NN.N to NN.N, both with objectness)
 When built on Deep MIL, our TST transfer raises CorLoc  to NN.0 (APP) and NN.N (SEM), a +N% improvement over Deep MIL (confirming what we observed when building on  Basic MIL)
Table N shows the mAP of Deep MIL and our  method (TST) on the test set
The observed improvements  in CorLoc on the training set nicely translate to better mAP  on the test set (+N.N% over Deep MIL)
 Comparison to LSDA [N0]
We compare to LSDA [N0],  which trains fully supervised detectors for N00 classes of the  ILSVRC N0NN dataset (sources) and transfers to the other  N00 classes (targets)
We report in Table N the mAP on  the N classes common to both their target set and ours
On  these N classes, we improve on [N0] by +N.N% mAP while using a substantially smaller source set (NK images in PASCAL Context, compared to N0NK images in their N00 source  classes from ILSVRC N0NN)
 Furthermore, we can also incorporate detectors for their  N00 source classes in our method, in a similar manner as  for the DT+MIL method
For each target class we use  the detector of the N most similar source classes as a pro- posal scoring function during MIL’s re-localization step
 We choose the SEM measure to guide the transfer as it  is fast to compute
This new scoring function is referred  to as ILSVRC-dets in Table N and N
When using the  ILSVRC-dets score, our mAP improves further, to a final  value +N.N% better than LSDA [N0]
 NNNN    turtle               tape-player                    goldfish                      elephant                       monkey   baby-bed basketball     bear              burrito                butterfly             cello         whale          rabbit  Figure N: We show localizations on ILSVRC-N0 of Basic MIL with objectness (blue) and our TST (APP) method (green)
 Method training (CorLoc) test (mAP)  Deep MIL + Obj
NN.N N.N  +TST (SEM) NN.0 NN.0  +TST (APP) NN.N NN.N  Table N: CorLoc and mAP on COCO-0N
Objectness [N] is added on top of the baseline
TST (SEM) and TST (APP) are separately  added to the baseline with objectness
 N.N
COCO-0N  Table N presents results on COCO-0N, which is a harder  dataset
Compared to Deep MIL with objectness, our  transfer method improves CorLoc by +N.0% and mAP by +N.N% (APP)
 N.N
PASCAL VOC N00N  Table N presents results on PASCAL VOC N00N
As  our baseline system, we use both objectness and multifolding [N] in Deep MIL
This performs at N0.N CorLoc and NN.N mAP
Our transfer method TST strongly improves CorLoc  to NN.N (+N.N%) and mAP to NN.N (+N.N%)
 Comparison to [NN]
They present results on this dataset  in a transfer setting, by using detectors trained in a fully  supervised setting for all N00 classes of ILSVRC (excluding the target class)
Adopting their protocol, we also  use those detectors in our method (analog to the LSDA  comparison above)
This leads to our highest CorLoc of  N0.N, which outperforms [NN], as well as state-of-the-art WSOL works [NN, NN, N] (which do not use such transfer)
 For completeness, we also report the corresponding mAPs
 Our mAP NN.N matches the result of [NN] based on their  ’S’ neural network, which corresponds to the AlexNet we  use
They propose an advanced WSOL technique that integrates both recognition and detection tasks to jointly train  a weakly supervised deep network, whilst we build on a  weaker MIL system
We believe our contributions are complementary: we could incorporate our TST transfer cues  Method ILSVRC-dets CorLoc mAP  Wang et al
[NN] NN.N NN.N  Bilen and Vedaldi [NN] (S) NN.N NN.N  Cinbis et al
[N] NN.N NN.N  Rochan and Wang [NN] X NN.N Deep MIL + Obj
+ MF N0.N NN.N  +TST (SEM) NN.N NN.N  +TST (SEM) X N0.N NN.N  Table N: Performance on PASCAL VOC N00N
We start from Deep MIL with objectness [N] and multifolding [N] as a baseline
 Then we add our method TST (SEM) to it
Rochan and Wang [NN]  do not report mAP
(S) denotes the S model (roughly AlexNet)  in [NN], which corresponds to the network architecture we use in  all experiments
ILSVRC-dets indicates using detectors trained  from ILSVRC during transfer
 into their WSOL technique and get even better results
 Finally, we note that our experimental protocol guarantees no overlap in either images nor classes between source  and target sets (Sec
N.N)
However, in general VOC N00N  and PASCAL Context (VOC N0N0) share similar attributes,  which makes this transfer task easier in our setting
 N
Conclusion  We present weakly supervised object localization using  things and stuff transfer
We transfer knowledge by training  a semantic segmentation model on the source set and using  it to generate thing and stuff maps on a target image
Class  similarity and co-occurrence relations are also transferred  and used as weighting functions
We devise three proposal  scoring schemes on both thing and stuff maps and combine  them to produce our final TST score
We plug the score  into an MIL pipeline and show significant improvements  on the ILSVRC-N0, VOC N00N and COCO-0N datasets
We  compare favourably to two previous transfer works [NN, N0]
 Acknowledgements
Work supported by the ERC Starting  Grant VisCul
 NNNN    References  [N] R
Girshick, F
Iandola, T
Darrell, and J
Malik
Deformable  part models are convolutional neural networks
In CVPR,  pages NNN–NNN, N0NN
N, N  [N] R
Girshick
Fast R-CNN
In ICCV, N0NN
N, N  [N] R.G
Cinbis, J
Verbeek, and C
Schmid
Multi-fold mil  training for weakly supervised object localization
In CVPR,  N0NN
N, N  [N] R.G
Cinbis, J
Verbeek, and C
Schmid
Weakly supervised  object localization with multi-fold multiple instance learning
IEEE Trans
on PAMI, N0NN
N, N, N, N  [N] B
Alexe, T
Deselaers, and V
Ferrari
What is an object? In  CVPR, N0N0
N, N, N, N  [N] J
R
R
Uijlings, K
E
A
van de Sande, T
Gevers, and  A
W
M
Smeulders
Selective search for object recognition
 IJCV, N0NN
N, N  [N] P
Dollar and C
Zitnick
Edge boxes: Locating object proposals from edges
In ECCV, N0NN
N, N, N, N, N  [N] H
Bilen, M
Pedersoli, and T
Tuytelaars
Weakly supervised  object detection with posterior regularization
In BMVC,  N0NN
N, N  [N] H
Bilen, M
Pedersoli, and T
Tuytelaars
Weakly supervised  object detection with convex clustering
In CVPR, N0NN
N,  N, N  [N0] T
Deselaers, B
Alexe, and V
Ferrari
Localizing objects  while learning their appearance
In ECCV, N0N0
N, N, N  [NN] O
Russakovsky, Y
Lin, K
Yu, and L
Fei-Fei
Objectcentric spatial pooling for image classification
In ECCV,  N0NN
N, N  [NN] P
Siva and T
Xiang
Weakly supervised object detector  learning with model drift detection
In ICCV, N0NN
N, N,  N  [NN] H.O
Song, R
Girshick, S
Jegelka, J
Mairal, Z
Harchaoui,  and T
Darell
On learning to localize objects with minimal  supervision
In ICML, N0NN
N, N, N  [NN] H.O
Song, Y.J
Lee, S
Jegelka, and T
Darell
Weaklysupervised discovery of visual pattern configurations
In  NIPS, N0NN
N, N  [NN] M
Shi and V
Ferrari
Weakly supervised object localization  using size estimates
In ECCV, N0NN
N, N  [NN] B
Alexe, T
Deselaers, and V
Ferrari
Measuring the objectness of image windows
IEEE Trans
on PAMI, N0NN
N,  N  [NN] K
Tang, A
Joulin, L-J
Li, and L
Fei-Fei
Co-localization  in real-world images
In CVPR, N0NN
N, N, N  [NN] Z
Shi, P
Siva, and T
Xiang
Transfer learning by ranking  for weakly supervised object annotation
In BMVC, N0NN
N,  N  [NN] M
Guillaumin and V
Ferrari
Large-scale knowledge transfer for object localization in imagenet
In CVPR, N0NN
N, N,  N, N  [N0] J
Hoffman, S
Guadarrama, E
Tzeng, R
Hu, and J
Donahue
LSDA: Large scale detection through adaptation
In  NIPS, N0NN
N, N, N, N  [NN] M
Rochan and Y
Wang
Weakly supervised localization of  novel objects using appearance transfer
In CVPR, N0NN
N,  N, N, N, N  [NN] G
Heitz and D
Koller
Learning spatial context: Using stuff  to find things
In ECCV, N00N
N, N  [NN] Y
J
Lee and K
Grauman
Learning the easy things first:  Self-paced visual category discovery
In CVPR, N0NN
N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
Berg, and L
Fei-Fei
ImageNet large scale visual recognition challenge
IJCV, N0NN
N, N, N, N  [NN] T-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C.L
Zitnick
Microsoft COCO: Common objects in context
In ECCV, N0NN
N, N, N  [NN] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn, and  A
Zisserman
The PASCAL Visual Object Classes (VOC)  Challenge
IJCV, N0N0
N, N  [NN] H
Bilen and A
Vedaldi
Weakly supervised deep detection  networks
In CVPR, N0NN
N, N, N, N  [NN] C
Wang, W
Ren, J
Zhang, K
Huang, and S
Maybank
 Large-scale weakly supervised object localization via latent  category learning
IEEE Transactions on Image Processing,  NN(N):NNNN–NNNN, N0NN
N, N, N, N  [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
N, N  [N0] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, N0NN
N, N, N  [NN] S
K
Pan and Q
Yang
A survey on transfer learning
IEEE  Trans
on KDE, N0N0
N  [NN] Y
Aytar and A
Zisserman
Tabula rasa: Model transfer for  object category detection
In ICCV, N0NN
N  [NN] Y
Aytar and A
Zisserman
Enhancing exemplar svms using  part level transfer regularization
In BMVC, N0NN
N  [NN] T
Tommasi, F
Orabona, and B
Caputo
Safety in numbers:  Learning categories from few examples with multi model  knowledge transfer
In CVPR
IEEE, N0N0
N  [NN] C
Lampert, H
Nickisch, and S
Harmeling
Learning to detect unseen object classes by between-class attribute transfer
 In CVPR, N00N
N  [NN] M
Rohrbach, M
Stark, G
Szarvas, I
Gurevych, and  B
Schiele
What helps where - and why? semantic relatedness for knowledge transfer
In CVPR, N0N0
N  [NN] P
Ott and M
Everingham
Shared parts for deformable partbased models
In CVPR, N0NN
N  [NN] M
Stark, M
Goesele, and B
Schiele
A shape-based object  class model for knowledge transfer
In ICCV, N00N
N  NNNN    [NN] D
Kuettel, M
Guillaumin, and V
Ferrari
Segmentation  Propagation in ImageNet
In ECCV, N0NN
N  [N0] M
Everingham, S
Eslami, L
van Gool, C
Williams,  J
Winn, and A
Zisserman
The PASCAL visual object  classes challenge: A retrospective
IJCV, N0NN
N, N  [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
N  [NN] D
Lin
An information-theoretic definition of similarity
In  ICML, NNNN
N  [NN] C
Fellbaum
Wordnet: An on-line lexical database, NNNN
N  [NN] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected CRFs
In ICLR, N0NN
 N  [NN] D
Eigen and R
Fergus
Predicting depth, surface normals  and semantic labels with a common multi-scale convolutional architecture
In ICCV, N0NN
N  [NN] C
Farabet, C
Couprie, L
Najman, and Y
LeCun
Learning hierarchical features for scene labeling
IEEE Trans
on  PAMI, NN(N):NNNN–NNNN, N0NN
N  [NN] H
Noh, S
Hong, and B
Han
Learning deconvolution network for semantic segmentation
In ICCV, N0NN
N  [NN] P
Pinheiro and R
Collobert
Recurrent convolutional neural  networks for scene parsing
In ICML, N0NN
N  [NN] Y
Jia
Caffe: An open source convolutional architecture for fast feature embedding
http://caffe
 berkeleyvision.org/, N0NN
N  [N0] Z
Shi, T.M
Hospedales, and T
Xiang
Bayesian joint  modelling for object localisation in weakly labelled images
 IEEE Trans
on PAMI, N0NN
N  [NN] R
Mottaghi, X
Chen, X
Liu, N.-G
Cho, S.-W
Lee, S
Fidler, R
Urtasun, and A
Yuille
The role of context for object  detection and semantic segmentation in the wild
In CVPR,  N0NN
N  [NN] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
Feifei
ImageNet: A large-scale hierarchical image database
In  CVPR, N00N
N  NNN0  http://caffe.berkeleyvision.org/ http://caffe.berkeleyvision.org/Deep Globally Constrained MRFs for Human Pose Estimation   Deep globally constrained MRFs for Human Pose Estimation  Ioannis Marras, Petar Palasek and Ioannis Patras  School of Electronic Engineering and Computer Science, Queen Mary University of London, United Kingdom  {i.marras, p.palasek, i.patras}@qmul.ac.uk  Abstract  This work introduces a novel Convolutional Network architecture (ConvNet) for the task of human pose estimation,  that is the localization of body joints in a single static image
We propose a coarse to fine architecture that addresses  shortcomings of the baseline architecture in [NN] that stem  from the fact that large inaccuracies of its coarse ConvNet cannot be corrected by the refinement ConvNet that  refines the estimation within small windows of the coarse  prediction
We overcome this by introducing a Markov Random Field (MRF)-based spatial model network between the  coarse and the refinement model that introduces geometric  constraints on the relative locations of the body joints
We  propose an architecture in which a) the filters that implement the message passing in the MRF inference are factored in a way that constrains them by a low dimensional  pose manifold the projection to which is estimated by a separate branch of the proposed ConvNet and b) the strengths  of the pairwise joint constraints are modeled by weights  that are jointly estimated by the other parameters of the  network
The proposed network is trained in an end-toend fashion
Experimental results show that the proposed  method improves the baseline model and provides state of  the art results on very challenging benchmarks
 N
Introduction  The problem of human pose estimation in monocular  RGB images, that is the problem of precise localization of  important landmarks of the human body, has received substantial attention in the Computer Vision community
Due  to the availability of ever larger and more comprehensive  datasets [N, NN, NN] and to the success of Deep Learning architectures, especially ConvNets [N, NN, NN, NN], there has  been significant progress in this problem over the recent  years
 A central issue in human pose estimation, when seen as  a special case of a Machine Learning problem with structured outputs, is the enforcement of constraints between the  different outputs, that is the enforcement of geometric con(a) (b)  Figure N
The probability of the hip location given the head location on (a) Fashion Pose and (b) MPII databases
 straints on the relative locations of the body joints
This  is typically modeled at the later layers of a ConvNet
For  example, in [NN] a MRF that models pairwise relations between different joints is encoded in a single CNN layer
In  such a network the filters ea|c encode the conditional probabilities of the location of joint a, given the location of another joint c
A major drawback with such an approach is  that a single filter is used to model all of the pairwise relations
This works well when applied to simpler datasets,  such as FashionPose, where there is little pose variation and  therefore the conditional probabilities have a few distinct  modes
However, for more complex datasets the conditionals become more uninformative as they attempt to model  pairwise relations under wide variety of poses - for example, the relative location of the head and the hip both in upright and in laying poses
This is evident in Figure N, where  the probability of the hip location given the head is depicted  in Figure N(a) for images of the FashionPose dataset and  in Figure N(b) for images of the MPII benchmark
Other  works, such as [N] where the geometric constraints are implicitly modeled in the latest layers of the network, suffer  from similar shortcomings
For example, in [N] the last layers that incorporates intensity constraints, imposes pairwise  constraints encoded in a single filter (that is, one filter per  pair of joints)
 In this paper, we present a three stage coarse-to-fine Convolutional Network architecture for the task of human pose  NNNNN    estimation
Our model comprises of: a) a coarse ConvNet  that provides coarse low(er) resolution heat-maps for the  joint locations, b) a part-based constrained MRF model that  enforces geometric constraints conditioned on a global projection on a low dimensional manifold, and c) a refinement  (coarse to fine) ConvNet, that refines the estimation within  windows around the peaks of the coarse heat-maps
The  combined model is trained in an end to end fashion to minimize the weighted sum of the costs of each of the three  ConvNets
The coarse to fine architecture, that is the coarse  and the refinement models, is similar to the baseline model  of [NN] and is reminiscent of recent works [NN] that reuse  early layers at the later stages of the architecture
A major  challenge in such an architecture is that large inaccuracies  of its coarse ConvNet, i.e
when spurious peaks are chosen,  cannot be corrected by the refinement ConvNet
For example, the coarse-to-fine ConvNet in [NN] relies little on the refinement ConvNet, as evidenced by the low weight assigned  to the corresponding cost, resulting only in a moderate improvement of the final localization accuracy
 In this work, we introduce a novel MRF-part based spatial model network between the coarse and the refinement  model that enforces spatial geometric constraints between  joints (Section N.N)
The proposed MRF model is a general idea that could be applied at other ConvNet systems
 It builds on the the geometric model used in [NN] that expresses message-passing as convolution operations that can  be implemented using ConvNets - the filters expressing the  conditional dependencies between the location of different  joints
By contrast to it, in our formulation, each of the filters that perform the convolution operations is assumed to  be a linear combination of K filters
The weights of this  linear combination are the projection of the heat-maps into  a K dimensional manifold that encodes global constraints,  such as the global pose
Unlike all other architectures, the  filters that are applied in our architecture at test time are not  static but dynamic, while the projection of the heat-map volume to the low dimensional manifold is performed by a side  auto-encoder ConvNet that is jointly trained with the other  ConvNets
Thus, the weights are learned by a cost function  that combines both a generative term that comes from the  auto-encoder ConvNet and a discriminative cost that comes  from the heat-map prediction
In this way, the conditionals  become more informative as they attempt to model pairwise  relations under specific global constraints
Additionally, in  our formulation, different pairwise constraints are given different weights
The above constraints amount to a factorization of the filter tensor
Finally, inspired by the work in  [N], the message passing procedure is applied in an iterative  manner to better mask-out the incorrect joints’ activations
 In addition to these central methodological contributions, we make two additional ones that considerably improve the performance
First, we use cropping windows of  varying sizes at the peaks of the heat-maps from the coarse  network to ensure that the cropped window that is used  in the ”refinement” network encloses the target joint (Section N.N)
This is in contrast to [NN] that uses a fixed window  size and therefore relies little on the ”refinement” network  for the final pose estimation, as evidenced by the fact that a  small weight to the cost of the ”refinement” network is used  during training
Secondly, we use a novel data augmentation and a learning procedure that were both adapted to the  difficulty of the specific data instances/images (Section N.N)
 More specifically, hard instances (i.e
training images with  a large prediction error) were assigned a lower learning rate  and were augmented by applying more transformations (rotation, scaling, shearing, stretching and flipping) to them
 Furthermore, we have trained our learning framework in a  way that is beneficial for our unified learning framework  (Section N.N)
 The proposed architecture is trained in an end-to-end  fashion
We show experimentally (Section N) that the combination of the three proposed ConvNets into a unified  learning framework: a) significantly outperforms the methods proposed in [NN] and [NN] and b) provides state of the  art results on very challenging benchmarks
 N
Related Work  Many methods extract, learn, or reason over entire body  features
Some use a combination of local detectors and  structural constraints [NN] for coarse tracking or for person  dependent tracking [N]
Methods using ”Pictorial Structures”, such as [NN], made this approach tractable with so  called ”Deformable Part Models (DPM)”
Subsequently a  large number of related models were developed [N, N0, N0]
 Algorithms which model more complex joint relationships,  such as [N0], use a flexible mixture of templates modeled  by linear SVMs
A cascade of body part detectors to obtain  more discriminative templates was employed in [NN]
Most  recent approaches aim to model higher-order part relationships
A model that augments the DPM model with Poselet [N] priors was proposed in [NN, N0] in order to capture  spatial relationships of body-parts
A multi-modal model  which includes both holistic and local cues for mode selection and pose estimation was proposed in [NN]
Following the Poselets approach, the Armlets approach in [NN]  employs a semi-global classifier for part configuration and  shows good performance on real-world data
This approach  exhibits good performance on real-world data, however it  is demonstrated only on arms
All these approaches use  hand crafted features (i.e
edges, contours, HoG features  and color histograms), which have been shown to have poor  generalization performance and discriminative power
 With the introduction of ”DeepPose” in [NN], the research on human pose estimation shifted to deep network  approaches
A network to directly regress the ND coordiNNNN    nates of joints was used in [NN]
In addition to the use of  graphical models, there are several examples of iterative or  multi-stage training methods in a sequential, cascaded fashion [NN]
 In [N], the ConvNet predictions were improving iteratively in a process called Iterative Error Feedback (IEF)
 Each successive run through their network takes as input  the image along with predictions from the previous forward pass and further refines them
This way it iteratively  improves part detections using error feedback, but uses a  Cartesian representation as in [NN] which does not preserve  spatial uncertainty and results in lower accuracy in the high  precision regime
In [NN], an extension based on the work  of multi-stage pose machines [NN] by using ConvNets for  feature extraction without an explicit graphical model-style  inference was proposed
A ”stacked hourglass” network design for predicting human pose was proposed in [NN]
This  network tries to capture and consolidate information across  all scales of the image by pooling down to a very low resolution, then upsampling and combining features across multiple resolutions
 The combination of a low-dimensional representation of  the input image produced by a ConvNet in [N] and an image dependent spatial model show improvement over the  work proposed in [NN]
In other words, detections were  clustered into typical orientations so that when their classifier makes predictions additional information is available  indicating the likely location of a neighbouring joint
In the  literature, multi-resolution ConvNet architectures were developed in order to perform heat-map likelihood regression  for each joint (rougher pose estimators)
These architectures were trained jointly with a MRF-based spatial model  network [NN] or with a pose refinement model [NN]
Others have recently tackled the problem of learning typical  spatial relationships between joints in similar ways [NN, NN]  with variations on how to approach unary score generation  and pairwise comparison of adjacent joints
Similarly, motion features can be added to the input of a multi-resolution  ConvNet architecture to further improve accuracy [NN]
In  [N], a ConvNet cascaded architecture designed for learning  part relationships and spatial context is presented
 N
Model Architecture  The overall architecture is shown in Figure N
It consists  of a coarse heat-map regression model, our proposed spatial geometric model, the module to sample and crop the  convolutional feature maps at a specified (x, y) location for each joint, and the fine heat-map regression (coarse to fine)  model
In this Section we give a description of each ConvNet used in our framework focusing on the proposed partbased spatial model
 N.N
Coarse Heat-Map Regression Model  The coarse heat-map regression model takes as input an  RGB Gaussian pyramid of three levels (in Figure N only two  levels are shown for brevity) and for each body joint it outputs a heat-map, that is a per-pixel likelihood that the joint  in question is depicted at that location
We use an input resolution of NNN×NNN pixels at the highest level of the pyra- mid
The first layer of the network performs local contrast  normalization (LCN) using the same filter kernel in each of  the three resolution banks
Each LCN image is then input to  a ten layer multi-resolution ConvNet
Due to the presence  of pooling the output heat-map is at a lower resolution than  the input image
 N.N
Part-based Spatial Model  In this Section we describe in detail the spatial model  that introduces the geometric constraints between the body  parts
Our model, depicted in Figure N, builds on the MRFbased spatial model proposed in [NN, NN], that formulates a  tree-structured MRF over spatial locations using a random  variable for each joint
In that formulation, the message  passing that performs inference is expressed using convolutional filtering operations and therefore can be implemented  as a specialized layer in a ConvNet
In this way the filters  that produce the unary and the pairwise potentials of the  MRF model can be learned by supervised training, either  of the last layer, or of the whole network in an end-to-end  fashion
For our NN×NN pixel heat-map input to this model, this results in large NN×NN convolution kernels to account for a joint displacement radius of maximum NN pixels
The  convolution sizes are adjusted so that the largest joint displacement is covered within the convolution window
In  such a network, the filters, denoted by fa|c, are functions of  the conditional probabilities ea|c of the location of joint a,  given the location of another joint c
That is, the refinement  ea of the heat-map for a joint a, is given by filtering operations on functions of the heatmaps ec of the other joints c
 More specifically,  ea = exp  (  ∑  c∈V  log[fa|c ∗ReLU(ec)+SoftP lus(ba|c)]  )  ,  (N)  where fa|c = SoftP lus(ea|c) (see [NN] for more details)
 A major drawback with such an approach is that a single filter, i.e
fa|c is used to model the pairwise relations  between joints
In the case of a dataset containing a large  variety of poses (e.g
both standing and laying) this results  with rather uninformative filters
To deal with this problem  the proposed MRF-based loopy belief propagation network  is constrained by a low dimensional latent model
In the  proposed model, each of the filters fa|c is a linear combination of K filters fka|c, where the weights w ∈ R K of  this linear combination are determined by the projection of  NNNN    Figure N
Overview of our unified learning framework
 Figure N
Architecture of our coarse heat-map regression model
 the heat-maps into a K dimensional manifold that encodes  global constraints, such as the global pose
That is:  fa|c =  K ∑  k=N  wk ∗ f k a|c = w  T              fNa|c 
 
 
 fKa|c              
(N)  The projection of the heat-map volume to the low dimensional manifold, that is the calculation of the weights  w, is performed by a separate branch of the network that  performs dimensionality reduction on the heat maps
It consists of convolutional and fully-connected layers and is depicted as the lower branch in Figure N
The parameters of  that branch are jointly trained with the main network using  both a discriminative and a generative cost - the latter being  essentially a classical auto-encoder cost
Thus, the weights  w are learned by a cost function that combines both a generative term that comes from the auto-encoder ConvNet and  a discriminative cost that comes from the heat-map prediction
In this way, the conditionals become more informative  as they attempt to model pairwise relations under specific  global constraints as those are encoded in the coordinates  w at the global pose manifold
 Another drawback of the baseline model of Eq
N is  that it assumes that the learned pair-wise joint distributions/relations should contribute equally to marginal likelihood of location of a joint
We relax this assumption by  applying, for each of the K dimensions of the pose manifold, a weighting scheme that determines the strength of the  joints’ spatial relationships
That is, we allow that, conditioned on a global pose, some pairwise relations between  different joints are more informative that others
This is expressed as a filtering operation with weights βka|c
That is:  NNNN    Figure N
The proposed constrained convolutional MRF-part based spatial model architecture
The lower branch is an auto-encoder ConvNet which learns the low Kth dimensional pose manifold
The weights w ∈ RK are learned by a cost function that combines both a generative term that comes from the auto-encoder ConvNet and a discriminative cost that comes from the heat-map prediction
 ea = exp  (  ∑  c∈V  [  K ∑  k=N  βka|c log [  wk ∗ f k a|c ∗ReLU(ec)  +SoftP lus(ba|c) ]]  )  , (N)  The weights βka|c, (N ≤ k ≤ K) are learned jointly with the other parameters of the network using back-propagation
 Note, that w are not fixed weights that are learned during  training and fixed during testing, but weights that are estimated at test time, by the auto-encoder ConvNet
 Finally, the baseline model of [NN] applies only one step  of the MRF-based inference
Inspired by the ConvNet in [N]  that uses a self-correcting model that progressively changes  an initial solution by feeding back error predictions, we apply the filtering steps of Eq
N in an iterative manner updating the same fa|c, ba|c and βa|c parameters
That is, the  output heat-maps of the proposed MRF-part based spatial  model are progressively changing by being fed back to the  model as inputs
This is depicted by the feedback loop in  Figure N
 N.N
Fine Heat-Map Regression Model  VNThe goal of using a fine regression model is to recover  the spatial accuracy lost by pooling in the coarse regression  model
Thus, an additional ConvNet proposed in [NN] was  used to refine the localization result of the unified coarse  model
More specifically, by reusing existing convolution  features this model is trained to estimate the joint offset location within a small region of the image extracted around  the estimates of the unified coarse model, reducing in that  way the number of trainable parameters in the cascade
This  network outputs a high resolution per-pixel heat-map which  corresponds to this small region, that is a per-pixel likelihood for key joint locations on the human skeleton
 N
Training and Data Augmentation  N.N
Model Training  All of the ConvNets described above do not estimate  the positions of the body joints directly [NN, NN], but estimate instead one heat-map for each of the joint positions
 Those heat-maps (i.e
the output of last convolutional layer)  form a fixed-size M × N × J−dimensional tensor (here NN×NN×J), where M,N and J denote the height, the width and the number of joints, respectively
In case of the coarse  heat-map regression model and the MRF-part based spatial model the output heat-maps have fixed spatial dimensions, M=N=NN, while in case of the fine heat-map regression model these two dimensions depend on the size of the  cropping region as described before
 NNN0    At training time, the ground truth labels for all ConvNets  are heat-maps that are constructed for each joint separately  by placing a Gaussian with fixed variance (σ ≈ N.N pix- els) at the ground truth position of the corresponding joint
 We then use an ℓN loss, that is we optimize the sum of the  squared pixel-wise differences between the output heat-map  and the constructed ground truth heat-map
 Let us denote by (Ii, Ci) the i-th training example, where Ci ∈ R  NJ denote the coordinates of the J joints in the image Ii
Given a training dataset N = {(Ii, Ci)} and the ConvNet regressor φ (the output of last convolutional layer),  we train our ConvNet by estimating the network weights p  that minimize the objective function E<ConvNet>:  E<ConvNet> = ∑  (I,C)∈N  ∑  m,j  ‖Hm,n,j(Cj)−φm,n,j(I, p)‖ N,  (N)  where Hm,n,j(Cj) = N  Nπσ e−[(C  N  j−m) N+(CNj−n)  N]/Nσ is a  Gaussian centred at Cj with σ fixed
Then, ECoarse,  EGeometric = EMRF + γEManifold and ECoarseNFine denote the objective function for each of our three ConvNets
EManifold denotes the objective function for the  auto-encoder ConvNet which creates the low dimensional  pose manifold, while γ is a constant used to provide a tradeoff between the relative importance of the two sub-tasks
 N.N
Joint Inference And Training  Given an input image, the joint inference is done as follows
First we do forward propagation through the coarse  heat-map model and our geometric model and infer all joint  (x, y) locations by finding the maximal value in each joint’s heat-map
This coarse (x, y) location is then used to sam- ple and crop the first two convolutional layer feature maps  at each of the joint locations
We do this for all the resolution banks, keeping the contextual size of the window constant by scaling the cropped area at each higher resolution  level
After that, the resulting features are further propagated through a fine heat-map model to give a (∆x, ∆y) offset within the cropped sub-window
Finally, by adding  the position refinement to the coarse location we end up  with the final (x, y) location prediction for each joint
Regarding the joint training, our proposed constrained  convolutional MRF-part based spatial network is combined  with the coarse heat-map regression model described in section N.N into a single unified coarse heat-map regression  model
This is done by firstly training the coarse heatmap regression model separately by minimizing ECoarse and storing the heat-map outputs
The outputs are then used  to train firstly our pose manifold generator by minimizing  EManifold, and secondly, our geometric model by minimizing EGeometric (we used γ = 0.N)
After that, the trained  coarse and geometric model are combined and fine-tuned  using back-propagation through the unified coarse heatTable N
Window sizes that were used for the different body joints  at the higher resolution input image
Cropping Window Size (in pixels) Per Joint  Head Shoulder Elbow Hip Knee Wrist Ankle  NN NN NN NN NN NN NN  map regression model by minimizing EUnified Coarse = ECoarse + EGeometric and storing the heat-map outputs
Subsequently, the outputs are used to train the coarse-to-fine  heat-map regression model by minimizing ECoarseNFine
 After that, the trained unified coarse and coarse-to-fine  models are combined and jointly fine-tuned using backpropagation through the unified coarse heat-map regression model by minimizing EUnified = EUnified Coarse + λECoarseNFine, where λ is a constant used to provide a  trade-off between the relative importance of the two subtasks
λ is another network hyper-parameter and is chosen  to optimize performance over the validation set (we used λ  = 0.NN)
This unified fine-tuning further improves performance, because the geometric model is able to effectively  reduce the output dimension of possible heat-map activations and therefore the coarse model can use the available  learning capacity to better localize the precise target activation
 In practice, many of the failure cases were caused by  either an occluded or a mis-attributed limb and refinement  of the position within a local window would not result in  improvements
In both cases the prediction error was large  and therefore the small fixed window used in [NN], would  not include the correct target location and the refinement  model could not therefore lead to an improved estimation
 For this reason, in [NN] the contribution of this part of the  network architecture is small (λ = 0.N)
In this work, we do  not use windows of fixed length to ensure that in the vast  majority of cases (more than NN% in the training set), the true target location is within the used window
This way,  we overcome the problems that [NN] faces in the case of  occlusions and in the case that the coarse model provides  estimates that are far from the true target location, and rely  more (λ = 0.NN) on the refinement model when training the  proposed architecture in an end-to-end fashion
In Table  N, we report the window sizes that were used for different  body joints at the higher resolution input image
 In order to better exploit the fine heat-map model by  keeping at the same time the cropping regions small we  used the training procedure described below
In the beginning of the training procedure, only the images with  small prediction error were used
Once the joint estimation accuracy on the training data was significantly improved by the ConvNet, the rest of the images were gradually included, based on the corresponding prediction errors
This is also important since in the beginning we used  quite a large learning rate, while when the most difficult  NNNN    images were processed the learning rate was significantly  decreased
During each training/validation iteration, each  input image is randomly rotated (with r ∈ [−N0o,+N0o]), scaled (with s ∈ [0.N, N.N]), sheared (with shear factor in pixels ∈ [−N, N]), stretched (with stretching factor equal to N.N−N) and flipped horizontally (with probability equal to 0.N) - those transformations are introduced in order to improve the generalization performance on the validation set
 In addition, for images whose prediction error was significantly higher than the mean data prediction error, we apply  more than one random image transformation (two in our  experiments)
Finally, we use more than one random image  transformation per image in the validation procedure too we used four in our experiments
 N
Evaluation  N.N
Datasets / Training Details  In this work we used the FashionPose [N], the MPII [N]  and the LSP [NN] databases
FashionPose dataset consists  of N,NNN accurately annotated images downloaded from a  variety of fashion blogs and it is annotated by NN joints
 MPII Human Pose dataset is the most diverse set of human pose-labeled images, it is a full-body dataset and it is  a video dataset
This dataset is very challenging and it includes a wide variety of full-body pose annotations within  the NN,NNN training and NN,N0N test examples
LSP dataset  consists of NN,000 images for training and N,000 images for  testing and is annotated by NN joints
 We implemented our network using the Lasagne library  within the Theano [N] framework and optimized the parameters using Adagrad [N]
The training of the coarse heatmap regression model takes approximately N days, the partbased spatial model N days and the coarse to fine heat-map  regression model takes N days on a NNGB Nvidia Tesla KN0  GPU
The forward-propagation for a single image through  all networks takes around NNNms
For MPII, it is standard to  utilize the scale and center annotations provided with all images
All images were cropped after centering on the person  and then scaled to get a NNN×NNN input for the network such that a standing-up human has height N00 pixels
In case of  severely occluded joints we used a ground truth heat-map  of all zeros for supervision
 N.N
Experimental Results  In order to qualitatively show the complexity of the used  datasets and illustrate the performance of our method, in  Figure N we depict some examples where our system estimates the human pose well
For generating final test predictions we run both the original input and a flipped version of  the image through the network and average the heat-maps  together [NN]
The chosen examples have PCK-0.N error  less than 0.NN, that is, the average error for all joints is less  than 0.NN of the half body height
 In order to show the influence of our contributions and  compare our results with [NN] and [NN], we report the PCK  (Probability of Correct Keypoints [N])
In Table N we summarize the results at accuracy of PCK = 0.NN
In order to  show the influence of the individual contributions, we report results for the MPII database for a) the coarse model  (CM), b) the coarse plus the coarse to fine models (CM  + CNFM), c) the full model comprising of the coarse plus  the MRF plus the coarse to fine models (CM+MRF+CNFM)  (full model) with only one iteration of our MRF model, d)  the full model when one filter is used to model the joint pairwise potentials (K=N), e) the full model when K=N, f) the  coarse model and g) the full model of [NN]
It is clear that  in both datasets our coarse model outperforms the coarse  model of [NN], illustrating the influence of the proposed architectural changes in the size of depth and filter size of  the coarse ConvNet
The results also show the influence of  the proposed contributions after the coarse model since both  the iterative MRF process as well as the constrained MRF  model significantly improve the performance of the system
 Our full model improves our coarse model N.NN% more than the full model of [NN] over its coarse model
Note that, as  described in Section N.N, even when K=N our MRF model  is roughly equivalent to [NN]
To limit the framework’s complexity we did not perform experiments for K >N
Considering that (a) the coarse to fine model is a siamese ConvNet,  and (b) in the iterative process of our geometric model we  use weight sharing, the total number of our training parameters is similar to other state-of-the-art techniques
 Table N
Comparison with prior-art
PCK @ 0.NN for MPII and  FashionPose Database compared to the state-of-the-art methods MPII Database  Methods Full Body  Tompson et al., CVPR N0NN - CM ConvNet NN.NN  Tompson et al., CVPR N0NN - Full Model NN.0N  Andriluka et al., CVPR N00N NN.NN  Toshev et al., CVPR N0NN NN.N0  Our System - CM ConvNet NN.NN  Our System - (CM+CNFM) ConvNet NN.NN  Our System - (CM+CNFM+MRF) ConvNet NN.NN  Our System - Full Model (one MRF loop) NN.N0  Our System - Full Model when K=N NN.N0  Our System - Full Model when K=N NN.NN  Our System - Full Model with data  augmentation of Tompson et al., CVPR N0NN NN.NN  FashionPose Database  Methods Full Body  Dantone et al., PAMI N0NN NN.NN  Our System - CM ConvNet NN.NN  Our System - (CM+CNFM) ConvNet NN.NN  Our System - Full Model N0.NN  The proposed architecture introduces the ConvNet with  geometric constraints before the refinement ConvNet
This  is in contrast to other methods in the literature, e.g
[NN]  that introduce such constraints at the final layers of their  NNNN    (a) (b)  Figure N
Human pose estimation (PCK-0.N error<0.NN) on sample images from (a) FashionPose and (b) MPII testing datasets
 Table N
Comparison with prior-art
Error per joint for the MPII dataset compared to the state-of-the-art methods
PCKh @ 0.NN PCKh @ 0.N  Methods Head Shoulder Elbow Wrist Hip Knee Ankle Full Body Head Shoulder Elbow Wrist Hip Knee Ankle Full Body  Wei et al., CVPR N0NN NN.N NN.N NN.N N0.N NN.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.0 NN.N NN.N NN.N NN.N  Pishchulin et al., CVPR N0NN NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  Carreira et al., CVPR N0NN NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Bulat et al., ECCV N0NN NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Newell et al., ECCV N0NN NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N N0.N  Tompson et al., NIPS N0NN NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N N0.N N0.N NN.N NN.N NN.N NN.N NN.N  Tompson et al., CVPR N0NN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.0  Our System - Full Model NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.NN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
Comparison with prior-art
Error per joint for the LSP dataset compared to the state-of-the-art methods
PCK@0.N  Methods Head Shoulder Elbow Wrist Hip Knee Ankle Full Body  Wei et al., CVPR N0NN NN.N NN.N NN.0 NN.N NN.N N0.N NN.N N0.N  Pishchulin et al., CVPR N0NN NN.0 NN.0 NN.N NN.N NN.0 NN.N NN.0 NN.N  Carreira et al., CVPR N0NN N0.N NN.N NN.N NN.N NN.N N0.N NN.0 NN.N  Bulat et al., ECCV N0NN NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N  Tompson et al., NIPS N0NN N0.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N  Our System - Full Model NN.N NN.N N0.N NN.N NN.N NN.N N0.N NN.N  networks
The motivation for doing so is that in our architecture the main purpose of that ConvNet is to provide  a better initial estimate such that the refinement network  can provide an accurate estimate within a window that contains high resolution image information
In order to validate  our choice, we provide results in Table N when a ConvNet  that introduces geometric constraints is placed after the refinement ConvNet (CM+CNFM+MRF)
Our choice is justified by the fact that the performance of the overall system  drops considerably from NN.NN% to NN.NN% when the geo- metric constraints are introduced at the final layers of our  networks
In order to validate that our data augmentation  procedure enhances the performance of our model, in Table N we provide the performance of our model when the  augmentation procedure of [NN] is used
In this case, the  performance of the overall system drops from NN.NN% to NN.NN%  In Tables N and N we report the error per joint for the  MPII and LSP datasets - as reported in other works in the  literature, wrists and angles that exhibit larger variations in  their motion are the ones that are harder to localize
Based  on the experimental results, it is clear that the proposed unified learning framework outperforms existing state-of-theart techniques on both of these challenging datasets
Furthermore, the performance of our system is considerably  better in the case of the harder joints (i.e
arms, wrists and  ankles) even at high levels of accuracy
 N
Conclusions  In this paper, we presented a cascaded architecture for  human body pose estimation that combines coarse, partbased spatial models and fine scale ConvNets
This work introduces a MRF-based spatial ConvNet between the coarse  and the refinement model that introduces geometric constraints
We propose an MRF architecture in which a) the  filters that implement the message passing in the MRF inference are factored so as to be constrained by a low dimensional pose manifold the projection to which is estimated  by a separate branch of the proposed ConvNet, and b) the  strength of the pairwise joint constraints are modeled by  weights that are jointly estimated with the other parameters of the network
These three ConvNets were trained into  a unified learning framework achieving state-of-the-art results on challenging datasets for human pose estimation
 NNNN    References  [N] M
Andriluka, L
Pishchulin, P
Gehler, and S
Bernt
Nd human pose estimation: New benchmark and state of the art  analysis
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June N0NN
N, N  [N] J
Bergstra, O
Breuleux, F
Bastien, P
Lamblin, R
Pascanu,  G
Desjardins, J
Turian, D
Warde-Farley, and Y
Bengio
 Theano: a CPU and GPU math expression compiler
In Proceedings of the Python for Scientific Computing Conference  (SciPy), N0N0
N  [N] L
Bourdev and J
Malik
Poselets: Body part detectors  trained using Nd human pose annotations
IEEE International Conference on Computer Vision (ICCV), N00N
N  [N] P
Buehler, A
Zisserman, and M
Everingham
Learning sign  language by watching tv (using weakly aligned subtitles)
In  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), N00N
N  [N] A
Bulat and G
Tzimiropoulos
Human pose estimation via  convolutional part heatmap regression
In European Conference on Computer Vision (ECCV), N0NN
N, N  [N] J
Carreira, P
Agrawal, K
Fragkiadaki, and J
Malik
Human pose estimation with iterative error feedback
In IEEE  Conference on Computer Vision and Pattern Recognition  (CVPR), N0NN
N, N, N  [N] X
Chen and A
L
Yuille
Articulated pose estimation by a  graphical model with image dependent pairwise relations
In  Advances in Neural Information Processing Systems, pages  NNNN–NNNN, N0NN
N, N  [N] M
Dantone, J
Gall, C
Leistner, and L
Van Gool
Body  parts dependent joint regressors for human pose estimation  in still images
IEEE Transactions on Pattern Analysis and  Machine Intelligence (PAMI), N0NN
N, N  [N] J
Duchi, E
Hazan, and Y
Singer
Adaptive subgradient methods for online learning and stochastic optimization
 Journal of Machine Learning Research (JMLR), NN:NNNN–  NNNN, N0NN
N  [N0] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn,  and A
Zisserman
The PASCAL Visual Object Classes  Challenge N00N (VOCN00N) Results
http://www.pascalnetwork.org/challenges/VOC/vocN00N/workshop/index.html
 N  [NN] X
Fan, K
Zheng, Y
Lin, and S
Wang
Combining local appearance and holistic view: Dual-source deep neural networks for human pose estimation
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [NN] P
Felzenszwalb, D
McAllester, and D
Ramanan
A discriminatively trained, multiscale, deformable part model
In  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), N00N
N  [NN] G
Gkioxari, P
Arbeláez, L
Bourdev, and J
Malik
Articulated pose estimation using discriminative armlet classifiers
 In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), N0NN
N  [NN] A
Jain, J
Tompson, Y
LeCun, and C
Bregler
Modeep: A  deep learning framework using motion features for human  pose estimation
arXiv preprint arXiv:NN0N.NNNN, N0NN
N, N  [NN] S
Johnson and M
Everingham
Clustered pose and nonlinear appearance models for human pose estimation
In Proceedings of the British Machine Vision Conference (BMVC),  N0N0
N  [NN] S
Johnson and M
Everingham
Learning effective human  pose estimation from inaccurate annotation
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  pages NNNN–NNNN, N0NN
N, N  [NN] A
Newell, K
Yang, and J
Deng
Stacked hourglass networks for human pose estimation
In European Conference  on Computer Vision (ECCV), N0NN
N, N  [NN] T
Pfister, K
Simonyan, J
Charles, and A
Zisserman
Deep  convolutional neural networks for efficient pose estimation  in gesture videos
In Asian Conference on Computer Vision  (ACCV), N0NN
N  [NN] L
Pishchulin, M
Andriluka, P
Gehler, and B
Schiele
Poselet conditioned pictorial structures
In IEEE Conference on  Computer Vision and Pattern Recognition (CVPR), N0NN
N  [N0] L
Pishchulin, M
Andriluka, P
Gehler, and B
Schiele
 Strong appearance and expressive spatial models for human  pose estimation
In IEEE International Conference on Computer Vision (ICCV), pages NNNN–NNNN, N0NN
N  [NN] L
Pishchulin, E
Insafutdinov, S
Tang, B
Andres, M
Andriluka, P
Gehler, and B
Schiele
Deepcut: Joint subset  partition and labeling for multi person pose estimation
In  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), N0NN
N  [NN] V
Ramakrishna, D
Munoz, M
Hebert, J
A
Bagnell, and  Y
Sheikh
Pose machines: Articulated pose estimation via  inference machines
In European Conference on Computer  Vision (ECCV), N0NN
N  [NN] D
Ramanan, D
Forsyth, and A
Zisserman
Strike a pose:  Tracking people by finding stylized poses
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  N00N
N  [NN] B
Sapp and B
Taskar
Modec: Multimodal decomposable  models for human pose estimation
In IEEE Conference on  Computer Vision and Pattern Recognition (CVPR), N0NN
N,  N  [NN] J
Tompson, R
Goroshin, A
Jain, Y
LeCun, and C
Bregler
 Efficient object localization using convolutional networks
In  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), N0NN
N, N, N, N, N, N  [NN] J
Tompson, A
Jain, Y
LeCun, and C
Bregler
Join training  of a convolutional network and a graphical model for human  pose estimation
In Neural Information Processing Systems  (NIPS), N0NN
N, N, N, N, N  [NN] A
Toshev and C
Szegedy
Deeppose: Human pose estimation via deep neural networks
In IEEE Conference on  Computer Vision and Pattern Recognition (CVPR), N0NN
N,  N, N, N  [NN] S
E
Wei, V
Ramakrishna, T
Kanade, and Y
Sheikh
Convolutional pose machines
In IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), N0NN
N  [NN] H
Yang and I
Patras
Mirror, mirror on the wall, tell me,  is the error small? In IEEE Conference on Computer Vision  and Pattern Recognition (CVPR), pages NNNN–NNNN, N0NN
N  NNNN    [N0] Y
Yang and D
Ramanan
Articulated human detection with  flexible mixtures of parts
IEEE Transactions on Pattern  Analysis and Machine Intelligence (PAMI), NN(NN):NNNN–  NNN0, N0NN
N  NNNNMonocular ND Human Pose Estimation by Predicting Depth on Joints   Monocular ND Human Pose Estimation by Predicting Depth on Joints  Bruce Xiaohan NieN∗, Ping WeiN,N∗, and Song-Chun ZhuN  NCenter for Vision, Cognition, Learning, and Autonomy, UCLA, USA NXi’an Jiaotong University, China  xiaohan.nie@gmail.com, pingwei@xjtu.edu.cn, sczhu@stat.ucla.edu  Abstract  This paper aims at estimating full-body ND human poses from monocular images of which the biggest challenge  is the inherent ambiguity introduced by lifting the ND pose  into ND space
We propose a novel framework focusing on  reducing this ambiguity by predicting the depth of human  joints based on ND human joint locations and body part  images
Our approach is built on a two-level hierarchy of  Long Short-Term Memory (LSTM) Networks which can be  trained end-to-end
The first level consists of two components: N) a skeleton-LSTM which learns the depth information from global human skeleton features; N) a patch-LSTM  which utilizes the local image evidence around joint locations
The both networks have tree structure defined on the  kinematic relation of human skeleton, thus the information  at different joints is broadcast through the whole skeleton in  a top-down fashion
The two networks are first pre-trained  separately on different data sources and then aggregated in  the second layer for final depth prediction
The empirical evaluation on HumanN.NM and HHOI dataset demonstrates  the advantage of combining global ND skeleton and local  image patches for depth prediction, and our superior quantitative and qualitative performance relative to state-of-theart methods
 N
Introduction  N.N
Motivation and Objective  This paper aims at reconstructing full-body ND human  poses from a single RGB image
Specifically we want to localize the human joints in ND space, as shown in Fig
N
Estimating ND human pose is a classic task in computer vision  and serves as a key component in many human related practical applications such as intelligent surveillance, humanrobot interaction, human activity analysis, human attention  recognition,etc
There are some existing works which estimate ND poses in constrained environment from depth im∗Bruce Xiaohan Nie and Ping Wei are co-first authors
 Skeleton  LSTM  Patch  LSTMFirst level  Second  level  LSTM Zhead  Zelbow Zhand  
 
 
 Figure N
Our two-level hierarchy of LSTM for ND pose estimation
The skeleton-LSTM and patch-LSTM captures information  from global ND skeleton and local image patches respectively at  the first level
The global and local features are integrated in the  second level to predict the depth on joints
The ND pose is recovered by attaching depth values onto the ND pose
We render the  ND pose for better visualization
 ages [N0, NN] or RGB images captured simultaneously at  multiple viewpoints [N, N0]
Different from them, we focus  on recognizing ND pose directly from a single RGB image  which is easier to be captured from general environment
 Estimating ND human poses from a single RGB image  is a challenging problem due to two main reasons: N) the  target person in the image always exhibits large appearance and geometric variation because of different clothes,  postures, illuminations, camera viewpoints and so on
The  highly articulated human pose also brings about heavy selfocclusions
N) even the ground-truth ND pose is given, recovering the ND pose is inherently ambiguous since that  there are infinite ND poses which can be projected onto the  same ND pose when the depth information is unknown
 One inspiration of our work is the huge progress of ND  human pose estimation made by recent works based on deep  architectures [NN, NN, NN, NN, N]
In those works, the appearance and geometric variation are implicitly modeled in feedforward computations in networks with hierarchical deep  structure
The self-occlusion is also addressed well by filters from different layers capturing features at different sNNNNN    cales
Another inspiration of our work is the effectiveness  that deep CNN has demonstrated in depth map prediction  and segmentation from monocular image [N, NN, NN, NN] instead of stereo images
Most of those approaches directly predict the pixel-wise depth map using deep networks  and some of them build markov random fields on the output of deep networks
The largest benefit is that they are  not bothered by designing geometric priors or hand-crafted  features, and most models can be trained end-to-end using  back-propagation
Based on the two above inspirations, in  this paper, we propose a novel framework to address the  challenge of lifting ND pose to ND pose by predicting the  depth of joints from two cues: global ND joint locations and  local body part images
The ND joint locations are predicted  from off-the-shelf pose estimation methods
 N.N
Method Overview  Our approach is built on a two-level hierarchy of LSTM networks to predict the depth on human joints and then  recover ND full-body human pose
The first level of our  model contains two key components: N) the skeleton-LSTM  network which takes the predicted ND joint locations to estimate depth of joints
This is based on the assumption that  the global human depth information such as global scale and  rough depth can be inferred from the correlation between  ND skeleton and ND pose
This global skeleton feature can  help to remove the physically implausible ND joint configuration and predict depth with considerable accuracy; N) the  patch-LSTM network which takes the local image patches of body parts as input to predict depth
This network  addresses the correlation between human part appearance  and depth information
To better model the kinematic relation of human skeletons, the two recurrent networks have  tree-structures similar to the models in [NN, NN, NN]
During training, the features at different joints are broadcasted through the whole skeleton and in testing the depth are  predicted for each joint in top-down fashion
The skeletonLSTM is first pre-trained on ND-ND pose pairs without any  image so infinite training examples can be generated by projecting ND poses onto ND space under arbitrary viewpoint
 The patch-LSTM is pre-trained on human body patches extracted around ND joints
To increase appearance variation  and reduce overfitting we employ multi-task learning on the  combination of two data sources: the MoCap data with the  task of depth regression and in-the-wild pose data with the  task of ND pose regression
The two networks are aggregated in the second layer and finetuned together for final  depth prediction
We evaluate our method extensively on  HumanN.NM dataset [NN] using two protocols
To test the  generalization ability, we test our method on HHOI dataset  using the model trained on HumanN.NM dataset
The results  demonstrate that we achieve better performance over state  of the art quantitatively and qualitatively
 N.N
Related Works and Our Contributions  Monocular ND human pose estimation
With the success of deep networks on a wide range of computer vision tasks and especially ND human pose estimation, the  ND pose estimation from monocular image using deep networks [NN, NN, NN, NN, NN] have received lots of attentions  recently
Some approaches [NN, NN] directly predict the ND  pose from images so their training and testing are restricted  to the ND MoCap data in a constrained environment, and  some methods [NN, N] reconstruct ND poses only from the  ND landmarks which are from other off-the-shelf methods
Li et al
[NN] applies a deep network to regress ND pose  and detect ND body parts simultaneously
In this method  there is no explicit constraint to guarantee that the predicted  ND pose can be projected to the detected ND part locations
Li et al
[NN] learn the common embedding space for  both image and ND pose using a deep network
The matching score of pose and image is computed as the dot product  between their embedding vectors
Some methods [NN, NN]  use two different data sources for training ND pose estimator and ND pose predictor
The ND poses are recovered by  minimizing the ND-ND projection error
The benefit is that  their ND pose estimators can be trained from another data  source instead of the ND Mocap data which is captured in a  constrained environment
Zhou et al
[NN] predict ND poses  from a video sequence by using temporal information
The  ND pose estimation is conducted via an EM type algorithm over the entire sequence and the ND joint uncertainties  are marginalized out during inference
Yasin et al
[NN] propose a dual-source approach to combine ND pose estimation  with ND pose retrieval
The first data source only contains  images with ND pose annotations for training ND pose estimator and the second source consists of ND MoCap data for  ND pose retrieval
Our approach is similar to those works  in that we also use two data sources for ND pose prediction:  The Mocap data and in-the-wild ND pose data, however, we  do not assume that the ND pose is conditional independent of image given ND pose
The cues from global ND pose  and local image patches are jointly modeled in our two-level  network
The ND pose images are used for the auxiliary task  of ND pose regression to compensate the lack of appearance  variation of Mocap data
Another work worth mention is  [NN] which use regression forest to infer the depth information and estimate ND joint location probabilities from image patches
The independent joint probabilities are used  with the pictorial structure model to infer the full skeleton
 We also infer joint depth from image patches, however, our  deep network is built on the pictorial structure model and  can be trained end-to-end
 Depth estimation from monocular image
Predicting  depth from monocular image is a long-standing problem  and recent approaches [N, NN, NN, N, NN] using deep neural networks have made a great progress in this area
Eigen  NNNN    ND pose images  ND pose library  Skeleton-LSTM Patch-LSTMFinal LSTM  +  N fc N conv+N fc  Mocap images Depth regression Depth regression ND pose regression  Depth regression  Figure N
Overview of our model structure and training process
In the first level, the skeleton-LSTM is pre-trained with ND-ND pose  pairs to predict depth from global skeleton features
The patch-LSTM predicts depth from local image patch evidence of body parts
The  tree-structure of two networks are defined on the kinematic relation of human joints, so the state of current joint is composed of the hidden  states of its parents and the input feature of itself
The two networks are integrated into another LSTM at the second level for end-to-end  training
To reduce overfitting of patch-LSTM, we borrow in-the-wild ND pose images and train the network with multi-task loss: the depth  prediction loss and ND pose regression loss
 et al
[N] apply a multi-scale network to directly regress  depth
The coarse-scale network is learned to predict overall depth map structure and the fine-scale network refines  the rough depth map using local image evidence
Wang et  al
[NN] proposed a unified framework for joint learning of  depth estimation and sematic segmentation
A deep CNN  is trained to do both tasks and a hierarchical CRF is applied  in inference to get fine-level details
Liu [NN] learn a continuous CRF and a deep CNN jointly
The unary and binary  potentials are from the deep CNN
They formulate the depth  prediction as a MAP problem and provide closed-form solutions
Chen [N] train a deep network with ranking loss  to produce pixel-wise depth using only annotations of relative depth
In this work, we integrate the global and local  information from ND skeleton and local image patches to  infer the depth of human joints and our objective function  considers both absolute and relative depth loss based on the  tree-structured human skeleton
 The contribution of our approach is three-fold:  i) We explore the ability of deep network for predicting the depth of human joints and then recover ND pose
 Our framework is more flexible than others because complex optimization is not needed and model can be trained  end-to-end
 ii) We incorporate both global ND skeleton features and  local image patch features in a two-level LSTM network  and the tree-structure topology of our model naturally represents the kinematic relation of human skeleton
The features at different joints are aggregated in top-down fashion
 iii) The extensive experiments demonstrate the superior  quantitative and qualitative performance of our work relative to other state of the art methods
 N
Models  In this section, we will first describe the relationship between ND pose estimation and depth prediction, and then  introduce our model and its corresponding formulations
 N.N
Recover ND Pose by Depth Prediction  The ND human pose is represented by a set of locations  of human joints in ND space
We use W ∈ RN×N to denote the ND pose in the world’s coordinate system where N is the  number of joints
Each ND joint location in W is denoted by  the ND coordinate wi = [Xi, Yi, Zi], i ∈ N, ..., N 
The ND pose is defined in the same way as S ∈ RN×N and each ND joint is denoted as si = [xi, yi]
The relationship between each ND joint and ND joint can be described as a perspective  projection:  zi ·      xi yi N     = f · [R|T ] ·          Xi Yi Zi N          , i ∈ N, ..., N (N)  where R ∈ RN×N denotes the camera rotation matrix, f de- notes focal length and zi denotes depth of joint i
Note that  in Eq
N there is no weak perspective assumption about the  relationship between ND pose and ND pose
Given ND joint  locations [xi, yi] and focal length f we need the depth val- ue for each joint zi, global rotation R and translation T to  recover all ND joint locations
Since there are infinite combinations of transformation matrix [R|T ] and world coordi- nate [Xi, Yi, Zi] which can produce the same [xi, yi] and zi with unknown camera position, therefore in this work we  focus on predicting z = [zN, ..., zN ] to recover the ND pose in the camera’s coordinate system Ŵ = [R|T ] · [W |N]T 
 NNNN    In order to predict the depth, we define the joint distribution of depth z, ND pose S and image I:  P (z, S, I) = P (z|S, I) · P (S|I) · P (I), (N)  where P (S|I) represents the ND pose estimation from sin- gle image I which can be handled by any off-the-shelf ND  pose estimation method
The separate estimation of depth  and ND pose allows P (S|I) to be modeled by any complex method
Any improvement made in P (S|I) can be imme- diately plugged into P (z, S, I) and re-training of the whole system is not needed
P (z|S, I) is modeled as a two-level hierarchy of LSTM which utilizes the ND pose S and image  evidence I in the first level, and integrates two networks in  the second level for final depth prediction
The details of  our model are described below
 N.N
Components of our Model  To take advantage of the global skeleton feature and local image feature to predict depth, we use a deep structure  of LSTMs with two levels
As shown in Fig
N, the first  level consists of two recurrent networks: a skeleton-LSTM  stacked with a ND pose encoding network which takes the  predicted ND pose S as input and a patch-LSTM stacked  with image patch encoding network which takes the local  image patches I(si), i ∈ [N, ..., N ] around ND joints as in- put
The hidden states of the two networks at each joint are  max pooled and forwarded to the LSTM at the second level  to predict the final real valued depth di for each joint
 Inspired by those graphical model based pose estimation  methods [NN, NN, N0, NN], we represent human pose as a  tree structure based on the kinematic relation of skeleton
 The articulated relation are better represented and the correlation of features at parent joint and child joint are better captured within tree structure than the flat or sequential  structure
Similar to the framework of [NN], we adapt the  tree-structured LSTM for modeling human pose and integrating global and local features
The aggregated contextual  information are propagated efficiently through the edges between joints
In experiments we evaluate different choices  of model structure and demonstrate the empirical strength  of tree-structure over the flat or sequential model
 The three tree-structured LSTMs in our model share the  same equation and only differ in the input data
At joint j,  the state of the LSTM unit is composed of the current input  feature xj , all hidden states hk and memory states ck from  its parents, and the output is the hidden state hj and memory  state cj which is forwarded to the child joint:  (hj , cj) = LSTM(xj , {hk}k∈N(j), {ck}k∈N(j),W,U) (N)  where W and U are weight matrices
To obtain a fixed dimension of the input feature, the hidden states from all parents of joint j are mean pooled:  h̄j = ( ∑  k∈N(j) hk)  /  |N(j)| (N)  h̄j is used to compute LSTM gates of joint j as below:  ij = σ(W ixj + U  ih̄j + b i)  fjk = σ(W fxj + U  fhk + b f )  oj = σ(W oxj + U  oh̄j + b o)  c̃j = tanh(W cxj + U  ch̄j + b c)  cj = ij ⊙ c̃j + ∑  k∈N(j) (fjk ⊙ ck)  hj = oj ⊙ tanh(cj)  (N)  where ij is the input gate, fjk is the forget gate of parent k,  oj is the output gate, σ denotes the sigmoid function and ⊙ denotes the element-wise multiplication
Note that our LSTM has different forget gates for different parent joint and  the multiplication of each fjk and ck indicates the influence  of parent k on current joint j
 ND pose encoding
As shown on the left of Fig
N,  the skeleton-LSTM utilize the global information from ND  skeleton S to predict the depth
In order to have a better  representation of the ND skeleton, we apply a multi-layer  perceptron network shared by all joints to extract the global pose feature
The input feature of the skeleton-LSTM at  joint j is xsj = MP (Ŝj) where Ŝj is the normalized ND pose by subtracting each joint location by the current joint  location [xj , yj ]
The structure of the multi-layer percep- tron is visualized in Fig
N
It is trained together with the  skeleton-LSTM
 N0NN N0NN NNN  LSTM fc relu dropout  fc relu dropout  fc relu dropout  Figure N
The multi-layer perceptron network for ND pose encoding
 Image patch encoding
As shown on the right of Fig
N,  the patch-LSTM utilizes the local information from image  patches of body parts for depth prediction
The input of  LSTM unit at joint j is the encoded feature of the corresponding image patch around that joint
We use x p j =  CNN(I(xj , yj)) to denote the input feature which is the last layer of a small ConvNet shared by all joints
The structure of the ConvNet is visualized in Fig
N
 For the final LSTM at the second layer, the input feature at joint j is the element-wise max pooling of hidden states from skeleton-LSTM and patch-LSTM: xj = max(hsj , h  p j )
The real-value depth in log space log(zj) at  each joint is predicted by attaching another fully-connected  layer on the hidden state hj : log(zj) = σ(W zhj + b  z)
 NNN0    N0NN N0NN NNN  LSTM fc relu dropout  fc relu dropout  fc relu dropout  NN  NN NxN conv relu NxN conv dropout  NxN conv relu NxN conv dropout  fc relu dropout  NNNNN NNN  LSTM  Figure N
The convolutional network for image patch encoding
 N
Learning  The model weights that we need to learn include the  weights of three LSTMs, and the weights of the ND pose  encoding network and image patch encoding network
The  learning process consists of three phrases:  N) The skeleton-LSTM and skeleton encoding network  are first pre-trained from Mocap data using the ND-ND pose  pairs with depth prediction loss
The RGB images are not  needed and infinite ND-ND pose pairs can be generated by  projecting each ND pose into ND poses under different camera viewpoints
 N) The patch-LSTM and image encoding network are  first pre-trained on RGB images from both MoCap dataset  and in-the-wild ND pose dataset with multi-task loss
Although the ND pose dataset does not have depth data, they  act as a regulariser with loss function of ND pose regression
 N) The last step is to combine the two LSTMs in the second layer for end-to-end training
 N.N
Objective Function  The loss functions for depth regression at the above three  phrases share the same formulation but use different input  feature and hyper parameters
Inspired by [N], we define the  loss based on both relative error and absolute error:  di = log(zi)− log(z ∗  i )  dij = (log(zi)− log(zj))− (log(z ∗  i )− log(z ∗  j ))  L(z, z∗) = λ N  n  n ∑  i=N  dNi + β N  |E|  ∑  (i,j)∈E  dNij  (N)  where z is the vector of all depth values on joints, n is the  number of joints and E denotes the set of edges in the tree  structure
The first term of L(z, z∗) is the mean squared error which enforces the absolute depth at each joint to be  correct and the second term penalizes the difference of relative depth between each parent-child pairs
Instead of considering all pairwise depth relations in [N], we focus on the  parent-child depth relations represented by edges in the tree  structure of our model
The hyper parameters λ and β control the balance between absolute depth loss and relative  depth loss
We set different λ and β for training skeletonLSTM and patch-LSTM since they are good at minimizing  different losses with different features
 N.N
Multi-task learning for patch-LSTM  As mentioned in Section N.N, the patch-LSTM needs to  be trained on image data with depth values of joints, and  the images from Mocap data are captured from a highly  constrained environment with small appearance variation  which may lead to severe over-fitting
To decrease overfitting, we argument training data using in-the-wild images  with annotations of ND poses from public pose datasets
Although the ND pose datasets do not have depth, we apply the  multi-task learning [N] to combine it with Mocap dataset  in the same network
Specifically, we add another fullyconnect layer on top of the hidden state of LSTM to regress  the ND joint locations which are normalized following [NN]
 The overall training loss is the sum of depth prediction loss  which only operates on Mocap data and ND pose regression  loss which operates on both Mocap data and ND pose data
 N
Results  Datasets
For empirical evaluation of our ND pose estimation we use two datasets: HumanN.NM dataset (HN.NM)  [NN] and UCLA Human-Human-Object Interaction Dataset  (HHOI) [NN]
The HumanN.NM dataset is a large-scale  dataset which includes accurate N.N million ND human poses captured by Mocap system
It also includes synchronized  videos and projected ND poses from N cameras so the NDND pose pairs are available
There are total NN actors performing NN actions such as Sitting, Waiting and Walking
 This dataset is captured in a controlled indoor environment
 The HHOI dataset contains human interactions captured by  MS Kinect vN sensor
It includes N types of human-human  interactions: shake hands, high five and pull up and N types  of human-object-human interactions: throw and catch, hand  over a cup
There are N actors performing NN.N instances  per interaction on average
The data is collected in a common office with clutter background
For in-the-wild ND  pose dataset, we use the MPII-LSP-extended dataset [NN]  which is a combination of the extend LSP [NN] and the MPII  dataset [N]
After flipping each image horizontally, we get a  total of N0000 images with ND pose annotations
 Implementation details
We use the public deep learning library Keras [N] to implement our method
The training and testing are conducted on a single NVIDIA Titan  X (Pascal) GPU
To train the skeleton-LSTM, we use the  ND-ND pose pairs down-sampled at N fps from HumanN.NM  dataset
Each ND pose is projected onto ND poses under  N camera viewpoints
The ND pose encoding network is  stacked with skeleton LSTM for joint training with parameter λ = 0.N and β = 0.N
To train the patch-LSTM, we use image frames down-sampled at NN fps from HumanN.NM  and all images from MPII-LSP-extended dataset
The image patch encoding network is stacked with patch-LSTM  for joint training with parameter λ = 0.N and β = 0.N
 NNNN    Methods Direct Discuss Eat Greet Phone Pose Purchase Sit SitDown Smoke Photo Wait Walk WalkDog WalkTo Mean  Yasin[NN] NN.N NN.N N0N.N NN0.N NN.N NN.N N0N.N NNN.0 NN0.N N0N.N NNN.N NN.N NN.N NNN.N N0N.0 NN0.N  Gall[NN] − − − − − − − − − − − − − − − NNN.N  Rogez[NN] − − − − − − − − − − − − − − − NN.N  our(s) N0.N NN.0 NN.0 NN.N NN.N NN.N N0.N N0N.N NNN.N NN.N NN.N N0.N NN.N NN.N NN.0 NN.N  our(p) NN.N NN.0 NNN.N N0N.N NNN.N N0.0 NN.N NNN.N NNN.N NNN.N NNN.N NN.N NN.N NNN.N NN.N N0N.N  our(s+p) NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N N0N.N NN.0 NN.N N0.N NN.N NN.N NN.N NN.N  Table N
Quantitative comparison of mean per joint errors (mm) on HumanN.NM dataset (Protocol N)
 Methods Direct Discuss Eat Greet Phone Pose Purchase Sit SitDown Smoke Photo Wait Walk WalkDog WalkTo Mean  Tekin[N0] N0N.N NNN.N NN.N NNN.N NNN.N NNN.0 NNN.N N0N.N NNN.N N0N.N NNN.N NNN.N NNN.N NN.N NN.N NNN.N  Zhou[NN] NN.N N0N.N NN.N N0N.N NNN.N NNN.N N0N.N NN.N NNN.N NNN.N N0N.N NNN.N NNN.N NN.N NN.N NNN.0  Rogez[NN] − − − − − − − − − − − − − − − NNN.N  our(s+p) N0.N NN.N NN.N NN.N N0N.N NN.N N0.N NNN.N NNN.N NN.N N0N.0 NN.N NN.0 N0.N NN.N NN.N  Table N
Quantitative comparison of mean per joint errors (mm) on HumanN.NM dataset (Protocol N)
 After separate training of the two networks, we finally combine them with the final LSTM for end to end training using  λ = β = 0.N
RMSprop [NN] is used for mini-batch gradi- ent descent and the learning rate is 0.0000N for all networks
 The batch size is NNN for skeleton-LSTM and NN for others
 Baseline
In addition to comparing our final system  with state of the art methods, we also use two variations  of our method as baselines : N) To isolate the impact of image feature, we only keep the skeleton-LSTM and the ND  pose encoding network and train them jointly to predict the  depth and then recover ND pose
This baseline is denoted  as ‘ours(s)’; N) We only keep patch-LSTM and image patch  encoding network and it is denoted as ‘ours(p)’
 N.N
Evaluation on HumanN.NM Dataset  We compare our results with state of the art approaches  in ND pose estimation on HumanN.NM dataset
We follow  the evaluation protocol in [NN]
The image frames and poses  from subject SN, SN, SN, SN, SN and SN are used for training  and SNN for testing
The testing data from SNN is downsampled at NNfps and some poses without synchronized images are removed so the total testing set has NNNN poses
 The training set has around N.N million ND/ND poses with  synchronized images
The ND pose error is measured by  the mean per joint position error (MPJPE) [NN] at NN joints  up to a rigid transformation
We refer to this protocol by PN
 The quantitative results are presented in Table N
The  method ‘our(s)’ and ‘our(p)’ are two method variations and  ‘our(s+p)’ is our final system
In all model variations, we  apply the pre-trained off-the-shelf ND pose estimator from  [N] to detect ND poses without any re-training or fine-tuning  because it is easy for the model to overfit the HumanN.NM  dataset which is captured in a highly constrained environment with limited appearance variation
 Table N shows that our model variation ‘our(s)’ outperforms other approaches which demonstrates the powerfulness of predicting depth from only ND pose
The human Nd  pose can be seen as a strong cue to estimate the corresponding ND pose
Although there are some ambiguities in the  perspective projection, with only ND pose features our model already captures helpful information to predict the depth  of joint
This result also indicates that predicting the joint  depth is more robust than predicting the whole ND pose
 Our method variation ‘our(p)’ achieves similar results  with [NN] which also uses image patches to predict ND joint  locations
To train the patch-LSTM, we focus on the pairwise relative losses as shown in Eq
N because it is hard to  predict the absolute depth with resized body part images
 After integrating the skeleton-LSTM and patch-LSTM we  further decrease the error to NN.Nmm which outperforms the  second best result by N.N%
 We also report results for protocol N (PN) which is employed in [NN, N0, NN]
The ND/ND poses and image frames  of subject SN, SN, SN, SN and SN are used for training and  SN, SN are used for testing
The estimated ND pose and  ground truth pose are aligned with their root locations and  MPJPE is calculated without rigid transformation
The results of our final system and state-of-the-art approaches are  presented in Table N
Our method clearly outperforms the  second best result [NN] by NN.NN% even though they use  temporal information to help ND pose estimation
 N.N
Evaluation on HHOI Dataset  To evaluate how our method can be generalized to data from a totally different environment, we train model on  HumanN.NM dataset and test it on HHOI dataset which is  captured with Kinect in a casual environment
We pick NN  joints defined by Kinect and use mean per joint error as the  evaluation metric
Each action instance is down-sampled  at N0fps for efficient computation and both persons in each  action are evaluated
We still use the focal length from HumanN.NM to recover ND poses and the poses are compared  NNNN    Figure N
Qualitative results from HHOI dataset
We visualize ten frames and their estimated ND poses from action ‘Pull Up’ and ‘Hand  Over’
Besides the original results, we show pose rendering results for better visualization
 up to a rigid transformation and also scale transformation
 The method of [N] is used to produce ND poses
Some qualitative results are presented in Fig
N
For better visualization  of ND pose, we do pose rendering using the code released  from [NN]
The two poses at each frame are recovered independently so their relative depth may not be correct
We  regress the relative mean depth between two persons using  the Nd distance on y axis between two persons’ feet
 There is no public code for recent methods compared in  HumanN.NM dataset so we implement another baseline ‘Nearest’ which match the predicted ND pose with ND poses  from HumanN.NM and select the depth from the ND pose  paired with the nearest ND pose as the predicted depth
 Note that the Kinect may produce unreasonable ND poses  because of occlusions and the evaluation with those poses  cannot reflect true performance of compared methods, thus  we looked at each action video and carefully select some  of them for quantitative comparison
Specifically we keep  all videos from ‘PullUp’ and ‘HandOver’, and a few videos  from ‘HighFive’ and ‘ShakeHands’
We select the smaller error calculated among the predicted pose and its flipped  one due to the left-right confusion of Kinect
The quantitative results are summarized in Table N
The action ‘Pull  Up’ gets the biggest error among all actions due to the large  Method PullUp HandOver HighFive ShakeHands  Nearest NNN.N NNN.N NNN.N NNN.N  our(s) NNN.N N0N.N NN.N NNN.N  our(p) NNN.N N0N.N N0N.0 NNN.0  our(s+p) NNN.N N0N.N NN.N NNN.N  Table N
Quantitative comparison of mean per joint errors (mm)  on HHOI dataset
 pose variation
Our final model outperforms other baselines  in three actions
 N.N
Diagnostic Experiments  To better justify the contribution of each component of  our method, we do several diagnostic comparisons in the  following
The HumanN.NM and protocol N is used for all  comparisons
 Effect of ND poses
We first evaluate our method on ND  pose estimation when ground truth ND poses are given and  compare it with [NN]
The results are presented in Table N  (a) and indicate the potential of improvement when a more  accurate ND pose estimator is available
 We also consider the effect of performance of ND pose  estimation
To generate ND poses with different errors, we  NNNN    (a)  Method Error  Yasin et al
[NN] N0.N  our(s) NN.N  our(p) NN.N  our(s+p) NN.N  (b)  Method No scale scale  our(s) NN.N N0.N  our(p) N0N.N N0N.N  our(s+p) NN.N NN.0  Table N
Quantitative comparison of mean per joint errors (mm) on  HumanN.NM (a) given ground truth ND poses; (b) with and without  scale transformation
 Figure N
Depth and ND pose error with different number of disturbed joints
 add disturbance to locations of different number of joints
 Specifically, for each testing ND pose, we randomly choose  certain number of joints and add a uniform random noise in  the range [0, e], e = 0.N · max(h,w), where h and w are the height and width of the pose respectively
The absolute  depth error and ND pose error are calculated at each number  of disturbed joints
The results are visualized in Fig
N
Although the absolute depth error increases quickly with the  error of ND pose estimation, the ND pose error increases slowly which indicates that the relative depth relations are  not effected too much by the disturbed ND pose
 Scale transformation
In general, it is impossible to estimate the global scale of the person from monocular image  so we evaluate different variations of our model with a scale transformation
The results are presented in Table N (b)  which show that there is a consistent improvement on all  model variations when the scale transformation is allowed
 Different model structures
We consider the effect of  model structure on the ND pose estimation performance  when only ND skeleton features are used
We compare the  following structures with the loss function defined in Eq
N:  -ske-econding
We remove the tree-structure LSTM network and only keep the ND pose encoding network
In this  setting, the effect of explicit modeling of relations between  joints are removed
 Method Error  ske-encoding NNN.0  ske-seq NN.0  ske-tree NN.N  whole-vgg NNN.N  patch-seq NNN.N  patch-tree N0N.N  Table N
Comparison between different model structures
 -ske-seq
We change the tree structure of the skeleton  LSTM to a sequential chain structure with a fixed order of  joints
It is impossible to evaluate all permutations of joints  so we choose the order which is more similar to the tree  structure: head-left limb-right limb
 -ske-tree
The skeleton-LSTM used in our final system
 We also evaluate the effect of the model structure when  only body part image features are used
We remove ND pose  features and compare the three model variations:  -whole-vgg
We apply the VGG model [NN] to predict  the depth from the cropped image of the whole person instead of body part
 -patch-seq
It has the same sequential structure as skeseq
 -patch-tree
The patch-LSTM used in our final system
 The results are shown in Table N
The method with LSTM network boost performance a lot on both skeleton  features (NN.N vs NNN.0) and image patch features (N0N.N  vs NNN.N) which demonstrates that the modeling of relationships between joints are essential for predicting depth
 The comparison between sequential chain structure and tree  structure demonstrates that the latter is more appropriate for  modeling human skeleton than the former on both features
 N
Conclusions  In this paper, we propose a framework to predict the  depth of human joints and recover the ND pose
Our approach is built on a two level hierarchy of LSTM by utilizing two cues: the ND skeleton feature which is captured  by skeleton-LSTM and image feature of body part which  is captured by patch-LSTM
The whole framework can be  trained end to end and it allows any off-the-shelf ND pose  estimator to be plugged in
The experiments demonstrate  our better performance qualitatively and quantitatively
In  the future, we plan to extend this work to video-domain  and combine it with ND scene reconstruction by considering temporal constraints and person-object relations
 Acknowledgment  This research was supported by grants DARPA XAI  project NNN00N-NN-N-N0NN, ONR MURI project N000NNNN-N-N00N, NSF IIS-NNNNN0N, and NSFC NNN0NNNN
 NNNN    References  [N] M
Andriluka, L
Pishchulin, P
Gehler, and B
Schiele
Nd  human pose estimation: New benchmark and state-of- theart analysis
In CVPR, N0NN
 [N] A.Yao, G
J, and L
V
Gool
Coupled action recognition and  pose estimation from multiple views
IJCV, N00(N):NN–NN,  N0NN
 [N] Z
Cao, T
Simon, S
Wei, and Y
Sheikh
Realtime multiperson Nd pose estimation using part affinity fields
In CVPR,  N0NN
 [N] C.-H
Chen and D
Ramanan
Nd human pose estimation =  Nd pose estimation + matching
In CVPR, N0NN
 [N] W
Chen, Z
Fu, D
Yang, and J
Deng
Single-image depth  perception in the wild
In NIPS, N0NN
 [N] F
Chollet
https://github.com/fchollet/keras
N0NN
 [N] R
Collobert and J
Weston
Unified architecture for natural  language processing: deep neural networks with multitask  learning
In ICML, N00N
 [N] D
Eigen, C
Puhrsch, and R
Fergus
Depth map prediction  from a single image using a multi-scale deep network
In  NIPS, N0NN
 [N] S
X
Haoshu Fang and C
Lu
RMPE: Regional multi-person  pose estimation
arXiv preprint arXiv:NNNN.00NNN, N0NN
 [N0] M
Hofmann and D
M
Gavrila
Multi-view Nd human pose  estimation in complex environment
IJCV, NN(N):N0N–NNN,  N0NN
 [NN] C
Ionescu, D
Papava, V
Olaru, and C
Sminchisescu
 HumanN.Nm: Large scale datasets and predictive methods for Nd human sensing in natural environments
TPAMI,  NN(N):NNNN–NNNN, N0NN
 [NN] S
Johnson and M
Everingham
Learning effective human  pose estimation from inaccurate annotation
In CVPR, N0NN
 [NN] I
Kostrikov and J
Gall
Depth sweep regression forests for  estimating Nd human pose from images
In BMVC, N0NN
 [NN] S
Li and A
Chan
Nd human pose estimation from monocular images with deep convolutional neural network
In ACCV, N0NN
 [NN] S
Li, W
Zhang, and A
Chan
Maximum-margin structured  learning with deep networks for Nd human pose estimation
 In ICCV, N0NN
 [NN] F
Liu, C
Shen, and G
Lin
Deep convolutional neural fields  for depth estimation from a single image
In CVPR, N0NN
 [NN] A
Newell, K
Yang, and J
Deng
Stacked hourglass networks for human pose estimation
In ECCV, N0NN
 [NN] B
Nie, C
Xiong, and S
Zhu
Joint action recognition and  pose estimation from video
In CVPR, N0NN
 [NN] S
Park, X
Nie, and S.-C
Zhu
Attribute and-or grammar  for joint parsing of human pose, parts and attributes
IEEE  TPAMI, (NN), N0NN
 [N0] L
Pishchulin, M
Andriluka, P
Gehler, and B
Schiele
 Strong appearance and expressive spatial models for human  pose estimation
In ICCV, N0NN
 [NN] L
Pishchulin, E
Insafutdinov, S
Tang, B
Andres, M
Andriluka, P
Gehler, and B
Schiele
Deepcut: Joint subset  partition and labeling for multi person pose estimation
In  CVPR, N0NN
 [NN] V
Ramakrishna, T
Kanade, and Y
Sheikh
Reconstructing  Nd human pose from Nd image landmarks
In ECCV, N0NN
 [NN] G
Rogez and C
Schmid
Mocap-guided data augmentation  for Nd pose estimation in the wild
In NIPS, N0NN
 [NN] B
Rothrock, S
Park, and S
Zhu
Integrating Grammar and  Segmentation for Human Pose Estimation
In CVPR, N0NN
 [NN] A
Roy and S
Todorovic
Monocular depth estimation using  neural regression forest
In CVPR, N0NN
 [NN] J
Shotton, A
W
Fitzgibbo, M
Cook, T
Sharp, M
Finocchio, R
Moore, A
Kipman, and A
Blake
Real-time human  pose recognition in parts from single depth images
In CVPR,  N0NN
 [NN] T
Shu, M
S
Ryoo, and S
Zhu
Learning social affordance  for human-robot interaction
In IJCAI, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv technical  report, N0NN
 [NN] K
Tai, R
Socher, and C
D
Manning
Improved semantic  representations from tree-structured long short-term memory  networks
In ACL, N0NN
 [N0] B
Tekin, A
Rozantsev, V
Lepetit, and P
Fua
Direct prediction of Nd body poses from motion compensated sequences
 In CVPR, N0NN
 [NN] T
Tieleman and G
Hinton
Lecture N.N-rmsprop: Divide the  gradient by a running average of its recent magnitude
In  Coursera: Neural networks for machine learning
 [NN] J
Tompson, A
Jain, Y
LeCun, and C
Bregler
Joint training  of a convolutional network and a graphical model for human  pose estimation
In NIPS, N0NN
 [NN] A
Toshev and C
Szegedy
Deeppose: Human pose estimation via deep neural networks
In CVPR, N0NN
 [NN] J
Wang, X
Nie, Y
Xia, Y
Wu, and S.-C
Zhu
Cross-view  action modeling, learning, and recognition
In N0NN IEEE  Conference on Computer Vision and Pattern Recognition,  pages NNNN–NNNN, N0NN
 [NN] P
Wang, X
Shen, Z
Lin, S
Cohen, B
Price, and A
Yuille
 Towards unified depth and semantic prediction from a single  image
In CVPR, N0NN
 [NN] W
Wang, J
Shen, and F
Porikli
Saliency-aware geodesic  video object segmentation
In CVPR, pages NNNN–NN0N,  N0NN
 [NN] S
Wei, V
Ramakrishna, T
Kanade, and Y
Sheikh
Convolutional pose machines
In ECCV, N0NN
 [NN] Y
Yang and D
Ramanan
Articulated human detection with  flexible mixtures of parts
TPAMI, NN(NN):NNNN–NNN0, N0NN
 [NN] H
Yasin, U
Iqbal, B
Kruger, A
Weber, and J
Gall
A  dualsource approach for Nd pose estimation from a single  image
In CVPR, N0NN
 [N0] M
Ye, X
Wang, R
Yang, L
Ren, and M
Pollefeys
Accurate Nd pose estimation from a single depth image
In ICCV,  N0NN
 [NN] X
Zhou, M
Zhu, S
Leonardos, K
G
Derpanis, and  K
Daniilidis
Sparseness meets deepness: Nd human pose  estimation from monocular video
In CVPR, N0NN
 [NN] Y
Zhu, C
Jiang, Y
Zhao, D
Terzopoulos, and S.-C
Zhu
 Inferring forces and learning human utilities from videos
In  CVPR, N0NN
 NNNNLarge-Scale Image Retrieval With Attentive Deep Local Features   Large-Scale Image Retrieval with Attentive Deep Local Features  Hyeonwoo Noh† Andre Araujo∗ Jack Sim∗ Tobias Weyand∗ Bohyung Han†  †POSTECH, Korea  {shgusdngogo,bhhan}@postech.ac.kr  ∗Google Inc
 {andrearaujo,jacksim,weyand}@google.com  Abstract  We propose an attentive local feature descriptor suitable  for large-scale image retrieval, referred to as DELF (DEep  Local Feature)
The new feature is based on convolutional  neural networks, which are trained only with image-level  annotations on a landmark image dataset
To identify semantically useful local features for image retrieval, we also propose an attention mechanism for keypoint selection, which  shares most network layers with the descriptor
This framework can be used for image retrieval as a drop-in replacement for other keypoint detectors and descriptors, enabling  more accurate feature matching and geometric verification
 Our system produces reliable confidence scores to reject false  positives—in particular, it is robust against queries that have  no correct match in the database
To evaluate the proposed  descriptor, we introduce a new large-scale dataset, referred  to as Google-Landmarks dataset, which involves challenges  in both database and query such as background clutter, partial occlusion, multiple landmarks, objects in variable scales,  etc
We show that DELF outperforms the state-of-the-art  global and local descriptors in the large-scale setting by  significant margins
 N
Introduction  Large-scale image retrieval is a fundamental task in computer vision, since it is directly related to various practical  applications, e.g., object detection, visual place recognition,  and product recognition
The last decades have witnessed  tremendous advances in image retrieval systems—from handcrafted features and indexing algorithms [NN, NN, NN, NN] to,  more recently, methods based on convolutional neural networks (CNNs) for global descriptor learning [N, NN, NN]
 Despite the recent advances in CNN-based global descriptors for image retrieval in small or medium-size datasets [NN,  NN], their performance may be hindered by a wide variety  of challenging conditions observed in large-scale datasets,  such as clutter, occlusion, and variations in viewpoint and  illumination
Global descriptors lack the ability to find patchDELF Pipeline  Large-Scale Index  Features  DELF  Features  Query Image  DELF Pipeline  Index Query   NN Features  Attention Scores  Database Images  Retrieved Images  Geometric  Verification  Figure N: Overall architecture of our image retrieval system, using DEep Local Features (DELF) and attention-based keypoint  selection
On the left, we illustrate the pipeline for extraction and  selection of DELF
The portion highlighted in yellow represents an  attention mechanism that is trained to assign high scores to relevant  features and select the features with the highest scores
Feature  extraction and selection can be performed with a single forward  pass using our model
On the right, we illustrate our large-scale  feature-based retrieval pipeline
DELF for database images are  indexed offline
The index supports querying by retrieving nearest  neighbor (NN) features, which can be used to rank database images  based on geometrically verified matches
 level matches between images
As a result, it is difficult to  retrieve images based on partial matching in the presence of  occlusion and background clutter
In a recent trend, CNNbased local features are proposed for patch-level matching  [NN, NN, N0]
However, these techniques are not optimized  specifically for image retrieval since they lack the ability to  detect semantically meaningful features, and show limited  accuracy in practice
 Most existing image retrieval algorithms have been evaluated in small to medium-size datasets with few query images,  i.e., only NN in [NN, NN] and N00 in [NN], and the images in  the datasets have limited diversity in terms of landmark locations and types
Therefore, we believe that the image  retrieval community can benefit from a large-scale dataset,  comprising more comprehensive and challenging examples,  to improve algorithm performance and evaluation methodolNNNN    ogy by deriving more statistically meaningful results
 The main goal of this work is to develop a large-scale  image retrieval system based on a novel CNN-based feature  descriptor
To this end, we first introduce a new large-scale  dataset, Google-Landmarks, which contains more than NM  landmark images from almost NNK unique landmarks
This  dataset covers a wide area in the world, and is consequently  more diverse and comprehensive than existing ones
The  query set is composed of an extra N00K images with diverse  characteristics; in particular, we include images that have  no match in the database, which makes our dataset more  challenging
This allows to assess the robustness of retrieval  systems when queries do not necessarily depict landmarks
 We then propose a CNN-based local feature with attention, which is trained with weak supervision using imagelevel class labels only, without the need of object- and patchlevel annotations
This new feature descriptor is referred to  as DELF (DEep Local Feature), and Fig
N illustrates the  overall procedure of feature extraction and image retrieval
 In our approach, the attention model is tightly coupled with  the proposed descriptor; it reuses the same CNN architecture  and generates feature scores using very little extra computation (in the spirit of recent advances in object detection [N0])
 This enables the extraction of both local descriptors and keypoints via one forward pass over the network
We show that  our image retrieval system based on DELF achieves the stateof-the-art performance with significant margins compared to  methods based on existing global and local descriptors
 N
Related Work  There are standard datasets commonly used for the evaluation of image retrieval techniques
OxfordNk [NN] has  N,0NN building images captured in Oxford with NN query  images
ParisNk [NN] is composed of N,NNN images of landmarks in Paris, and also has NN query images
These two  datasets are often augmented with N00K distractor images  from FlickrN00k dataset [NN], which constructs OxfordN0Nk  and ParisN0Nk datasets, respectively
On the other hand, Holidays dataset [NN] provides N,NNN images including N00 query  images, which are from personal holiday photos
All these  three datasets are fairly small, especially having a very small  number of query images, which makes it difficult to generalize the performance tested in these datasets
Although  PittsNN0k [NN] is larger, it is specialized to visual places with  repetitive patterns and may not be appropriate for the general  image retrieval task
 Instance retrieval has been a popular research problem for  more than a decade
See [NN] for a recent survey
Early systems rely on hand-crafted local features [NN, N, N], coupled  with approximate nearest neighbor search methods using KD  trees or vocabulary trees [N, NN]
Still today, such featurebased techniques combined with geometric re-ranking provide strong performance when retrieval systems need to  operate with high precision
 More recently, many works have focused on aggregation  methods of local features, which include popular techniques  such as VLAD [NN] and Fisher Vector (FV) [NN]
The main  advantage of such global descriptors is the ability to provide  high-performance image retrieval with a compact index
 In the past few years, several global descriptors based  on CNNs have been proposed to use pretrained [N, NN] or  learned networks [N, NN, NN]
These global descriptors are  most commonly trained with a triplet loss, in order to preserve the ranking between relevant and irrelevant images
 Some retrieval algorithms using these CNN-based global  descriptors make use of deep local features as a drop-in replacement for hand-crafted features in conventional aggregation techniques such as VLAD or FV [NN, NN]
Other works  have re-evaluated and proposed different feature aggregation  methods using such deep local features [N, NN]
 CNNs have also been used to detect, represent and compare local image features
Verdie et al
[NN] learned a regressor for repeatable keypoint detection
Yi et al
[NN] proposed  a generic CNN-based technique to estimate the canonical  orientation of a local feature and successfully deployed it to  several different descriptors
MatchNet [NN] and DeepCompare [NN] have been proposed to jointly learn patch representations and associated metrics
Recently, LIFT [N0] proposed an end-to-end framework to detect keypoints, estimate  orientation, and compute descriptors
Different from our  work, these techniques are not designed for image retrieval  applications since they do not learn to select semantically  meaningful features
 Many visual recognition problems employ visual attention based on deep neural networks, which include object  detection [NN], semantic segmentation [NN], image captioning [NN], visual question answering [NN], etc
However, visual attention has not been explored actively to learn visual  features for image retrieval applications
 N
Google-Landmarks Dataset  Our dataset is constructed based on the algorithm described in [NN]
Compared to the existing datasets for image  retrieval [NN, NN, NN], the new dataset is much larger, contains diverse landmarks, and involves substantial challenges
 It contains N, 0N0, N0N images from NN, NNN landmarks, and NNN, 0NN additional query images
The images in the dataset are captured at various locations in the world, and each image is associated with a GPS coordinate
Example images  and their geographic distribution are presented in Fig
N  and Fig
N, respectively
While most images in the existing  datasets are landmark-centric, which makes global feature  descriptors work well, our dataset contains more realistic images with wild variations including foreground/background  clutter, occlusion, partially out-of-view objects, etc
In particular, since our query images are collected from personal  NNNN    (a) Sample database images  (b) Sample query images  Figure N: Example database and query images from GoogleLandmarks
They have a lot of variations and challenges including  background clutter, small objects, and multiple landmarks
 Figure N: Image geolocation distribution of our Google-Landmarks  dataset
The landmarks are located in N,NNN cities in NNN countries
 photo repositories, some of them may not contain any landmarks and should not retrieve any image from the database
 We call these query images distractors, which play a critical  role to evaluate robustness of algorithms to irrelevant and  noisy queries
 We use visual features and GPS coordinates for groundtruth construction
All images in the database are clustered  using the two kinds of information, and we assign a landmark identifier to each cluster
If physical distance between  the location of a query image and the center of the cluster  associated with the retrieved image is less than a threshold,  we assume that the two images belong to the same landmark
 Note that ground-truth annotation is extremely challenging,  especially considering the facts that it is hard to predefine  what landmarks are, landmarks are not clearly noticeable  sometimes, and there might be multiple instances in a single  image
Obviously, this approach for ground-truth construction is noisy due to GPS errors
Also, photos can be captured  from a large distance for some landmarks (e.g., Eiffel Tower,  Golden Gate Bridge), and consequently the photo location  might be relatively far from the actual landmark location
 However, we found very few incorrect annotations with the  threshold of NNkm when checking a subset of data manually
Even though there are few minor errors, it is not problematic, especially in relative evaluation, because algorithms are  unlikely to be confused between landmarks anyway if their  visual appearances are sufficiently discriminative
 N
Image Retrieval with DELF  Our large-scale retrieval system can be decomposed into  four main blocks: (i) dense localized feature extraction, (ii)  keypoint selection, (iii) dimensionality reduction and (iv)  indexing and retrieval
This section describes DELF feature  extraction and learning algorithm followed by our indexing  and retrieval procedure in detail
 N.N
Dense Localized Feature Extraction  We extract dense features from an image by applying a  fully convolutional network (FCN), which is constructed by  using the feature extraction layers of a CNN trained with  a classification loss
We employ an FCN taken from the  ResNetN0 [NN] model, using the output of the convN x convolutional block
To handle scale changes, we explicitly  construct an image pyramid and apply the FCN for each  level independently
The obtained feature maps are regarded  as a dense grid of local descriptors
Features are localized  based on their receptive fields, which can be computed by  considering the configuration of convolutional and pooling  layers of the FCN
We use the pixel coordinates of the center  of the receptive field as the feature location
The receptive  field size for the image at the original scale is NNN × NNN
Using the image pyramid, we obtain features that describe  image regions of different sizes
 We use the original ResNetN0 model trained on ImageNet [NN] as a baseline, and fine-tune for enhancing the  discriminativeness of our local descriptors
Since we consider a landmark recognition application, we employ annotated datasets of landmark images [N] and train the network  with a standard cross-entropy loss for image classification as  illustrated in Fig
N(a)
The input images are initially centercropped to produce square images and rescaled to NN0×NN0
Random NNN × NNN crops are then used for training
As a result of training, local descriptors implicitly learn representations that are more relevant for the landmark retrieval  NNNN    Features  Attention Scores  Features  (a) Descriptor Fine-tuning (b) Attention-based Training  Figure N: The network architectures used for training
 problem
In this manner, neither object- nor patch-level  labels are necessary to obtain improved local descriptors
 N.N
Attention-based Keypoint Selection  Instead of using densely extracted features directly for  image retrieval, we design a technique to effectively select a subset of the features
Since a substantial part of the  densely extracted features are irrelevant to our recognition  task and likely to add clutter, distracting the retrieval process, keypoint selection is important for both accuracy and  computational efficiency of retrieval systems
 N.N.N Learning with Weak Supervision  We propose to train a landmark classifier with attention to explicitly measure relevance scores for local feature descriptors
 To train the function, features are pooled by a weighted sum,  where the weights are predicted by the attention network
 The training procedure is similar to the one described in  Sec
N.N including the loss function and datasets, and is illustrated in Fig
N(b), where the attention network is highlighted  in yellow
This generates an embedding for the whole input  image, which is then used to train a softmax-based landmark  classifier
 More precisely, we formulate the training as follows
Denote by fn ∈ Rd, n = N, ..., N the d-dimensional features to be learned jointly with the attention model
Our goal is  to learn a score function α(fn; θ) for each feature, where θ denotes the parameters of function α(·)
The output logit y of the network is generated by a weighted sum of the feature  vectors, which is given by  y = W  (  ∑  n  α(fn; θ) · fn )  , (N)  where W ∈ RM×d represents the weights of the final fully- connected layer of the CNN trained to predict M classes
 For training, we use cross entropy loss, which is given by  L = −y∗ · log (  exp (y)  NT exp (y)  )  , (N)  where y∗ is ground-truth in one-hot representation and N is  one vector
The parameters in the score function α(·) are trained by backpropagation, where the gradient is given by  ∂L ∂θ  = ∂L ∂y  ∑  n  ∂y  ∂αn  ∂αn  ∂θ =  ∂L ∂y  ∑  n  Wfn ∂αn  ∂θ , (N)  where the backpropagation of the output score αn ≡ α(fn; θ) with respect to θ is same as the standard multi-layer perceptron
 We restrict α(·) to be non-negative, to prevent it from learning negative weighting
The score function is designed  using a N-layer CNN with a softplus [N] activation at the top
 For simplicity, we employ the convolutional filters of size  N×N, which work well in practice
Once the attention model is trained, it can be used to assess the relevance of features  extracted by our model
 N.N.N Training Attention  In the proposed framework, both the descriptors and the attention model are implicitly learned with image-level labels
 Unfortunately, this poses some challenges to the learning  process
While the feature representation and the score function can be trained jointly by backpropagation, we found  that this setup generates weak models in practice
Therefore,  we employ a two-step training strategy
First, we learn descriptors with fine-tuning as described in Sec
N.N, and then  the score function is learned given the fixed descriptors
 Another improvement to our models is obtained by random image rescaling during attention training process
This  is intuitive, as the attention model should be able to generate effective scores for features at different scales
In this  case, the input images are initially center-cropped to produce square images, and rescaled to N00 × N00
Random NN0 × NN0 crops are then extracted and finally randomly scaled with a factor γ ≤ N
 N.N.N Characteristics  One unconventional aspect of our system is that keypoint  selection comes after descriptor extraction, which is different  from the existing techniques (e.g., SIFT [NN] and LIFT [N0]),  where keypoints are first detected and later described
Traditional keypoint detectors focus on detecting keypoints repeatably under different imaging conditions, based only on their  low-level characteristics
However, for a high-level recognition task such as image retrieval, it is also critical to select  keypoints that discriminate different object instances
The  proposed pipeline achieves both goals by training a model  NNNN    that encodes higher level semantics in the feature map, and  learning to select discriminative features for the classification task
This is in contrast to recently proposed techniques  for learning keypoint detectors, i.e., LIFT [N0], which collect  training data based on SIFT matches
Although our model is  not constrained to learn invariances to pose and viewpoint,  it implicitly learns to do so—similar to CNN-based image  classification techniques
 N.N
Dimensionality Reduction  We reduce the dimensionality of selected features to obtain improved retrieval accuracy, as common practice [NN]
 First, the selected features are LN normalized, and their dimensionality is reduced to N0 by PCA, which presents a  good trade-off between compactness and discriminativeness
 Finally, the features once again undergo LN normalization
 N.N
Image Retrieval System  We extract feature descriptors from query and database  images, where a predefined number of local features with the  highest attention scores per image are selected
Our image  retrieval system is based on nearest neighbor search, which  is implemented by a combination of KD-tree [N] and Product  Quantization (PQ) [NN]
We encode each descriptor to a N0bit code using PQ, where each N0D feature descriptor is split  into N0 subvectors with equal dimensions, and we identify NN  centroids per subvector by k-means clustering to achieve N0bit encoding
We perform asymmetric distance calculation,  where the query descriptors are not encoded to improve  the accuracy of nearest neighbor retrieval
To speed up the  nearest neighbor search, we construct an inverted index for  descriptors, using a codebook of size NK
To reduce encoding  errors, a KD-tree is used to partition each Voronoi cell, and  a Locally Optimized Product Quantizer [N0] is employed for  each subtree with fewer than N0K features
 When a query is given, we perform approximate nearest  neighbor search for each local descriptor extracted from a  query image
Then for the top K nearest local descriptors  retrieved from the index, we aggregate all the matches per  database image
Finally, we perform geometric verification  using RANSAC [N0] and employ the number of inliers as  the score for retrieved images
Many distractor queries are  rejected by this geometric verification step because features  from distractors may not be consistently matched with the  ones from landmark images
 This pipeline requires less than NGB memory to index N  billion descriptors, which is sufficient to handle our largescale landmark dataset
The latency of the nearest neighbor  search is less than N seconds using a single CPU under our  experiment setup, where we soft-assign N centroids to each  query and search up to N0K leaf nodes within each inverted  index tree
 N
Experiments  This section mainly discusses the performance of DELF  compared to existing global and local feature descriptors in  our dataset
In addition, we also show how DELF can be  employed to achieve good accuracy in the existing datasets
 N.N
Implementation Details  Multi-scale descriptor extraction We construct image  pyramids by using scales that are a √ N factor apart
For  the set of scales with range from 0.NN to N.0, N different scales are used
The size of receptive field is inversely proportional to the scale; for example, for the N.0 scale, the receptive field of the network covers NNN× NNN pixels
Training We employed landmarks dataset [N] for finetuning descriptors and training keypoint selection
In the  dataset, there are the “full” version, referred to as LF (after  removal of overlapping classes with OxfNk/ParNk, by [NN]),  containing NN0,NNN images from NNN landmarks, and the  “clean” version (LC) obtained by a SIFT-based matching  procedure [NN], with NN,NNN images of NNN landmarks
We  use LF to train our attention model, and LC is employed to  fine-tune the network for image retrieval
 Parameters We identify the top K(= N0) nearest neigh- bors for each feature in a query and extract up to N000 local features from each image—each feature is N0-dimensional
 N.N
Compared Algorithms  DELF is compared with several recent global and local  descriptors
Although there are various research outcomes  related to image retrieval, we believe that the following  methods are either relevant to our algorithm or most critical  to evaluation due to their good performance
 Deep Image Retrieval (DIR) [NN] This is a recent global  descriptor that achieves the state-of-the-art performance in  several existing datasets
DIR feature descriptors are N, 0NN dimensional and multi-resolution descriptors are used in all  cases
We also evaluate with query expansion (QE), which  typically improves accuracy in the standard datasets
We use  the released source code that implements the version with  ResNetN0N [NN]
For retrieval, a parallelized implementation  of brute-force search is employed to avoid penalization by  the error from approximate nearest neighbor search
 siaMAC [NN] This is a recent global descriptor that obtains  high performance in existing datasets
We use the released  source code with parallelized implementation of brute-force  search
The CNN based on VGGNN [NN] extracts NNN di- mensional global descriptor
We also experiment with query  expansion (QE) as in DIR
 CONGAS [N, NN] CONGAS is a N0D hand-engineered local feature, which has been widely used for instance-level  NNN0    image matching and retrieval [N, NN]
This feature descriptor  is extracted by collecting Gabor wavelet responses at the  detected keypoint scale and orientation, and is known to  have very similar performance and characteristic to other  gradient-based local descriptors like SIFT
A Laplacian-ofGaussian keypoint detector is used
 LIFT LIFT [N0] is a recently proposed feature matching  pipeline, where keypoint detection, orientation estimation  and keypoint description are jointly learned
Features are  NNN dimensional
We use the source code publicly available
 N.N
Evaluation  Image retrieval systems have typically been evaluated  based on mean average precision (mAP), which is computed by sorting images in descending order of relevance per  query and averaging AP of individual queries
However, for  datasets with distractor queries, such evaluation method is  not representative since it is important to determine whether  each image is relevant to the query or not
In our case, the absolute retrieval score is used to estimate the relevance of each  image
For performance evaluation, we employ a modified  version of precision (PRE) and recall (REC) by considering  all query images at the same time, which are given by  PRE =  ∑  q |RTPq | ∑  q |Rq| and REC =  ∑  q  |RTPq |, (N)  where Rq denotes a set of retrieved images for query q given a threshold, and RTPq (⊆ Rq) is a set of true positives
This is similar to the micro-AP metric introduced in [NN]
We  prefer unnormalized recall values, which present the number  of retrieved true positives
Instead of summarizing our result  in a single number, we present a full precision-recall curve to  inspect operating points with different retrieval thresholds
 N.N
Quantitative Results  Fig
N presents the precision-recall curve of DELF (denoted by DELF+FT+ATT), compared to other methods
The  results of LIFT could not be shown because feature extraction is extremely slow and large-scale experiment is infeasibleN
DELF clearly outperforms all other techniques significantly
Global feature descriptors, such as DIR, suffer in our  challenging dataset
In particular, due to a large number of  distractors in the query set, DIR with QE degrades accuracy  significantly
CONGAS does a reasonably good job, but is  still worse than DELF with substantial margin
 To analyze the benefit of fine-tuning and attention for image retrieval, we compare our full model (DELF+FT+ATT)  with its variations: DELF-noFT, DELF+FT and DELFnoFT+ATT
DELF-noFT means that extracted features are  based on the pretrained CNN on ImageNet without finetuning and attention learning
DELF+FT denotes the model  NLIFT feature extraction approximately takes N min/image using a GPU
 Figure N: Precision-recall curve for the large-scale retrieval experiment on the Google-Landmarks dataset, where recall is presented in  absolute terms, as in Eq
(N)
DELF shows outstanding performance  compared with existing global and local features
Fine-tuning and  attention model in DELF are critical to performance improvement
 The accuracy of DIR drops significantly with query expansion, due  to many distractor queries in our dataset
 with fine-tuning but without attention modeling while DELFnoFT+ATT corresponds to the model without fine-tuning  but using attention
As illustrated in Fig
N, both fine-tuning  and attention modeling make substantial contributions to  performance improvement
In particular, note that the use  of attention is more important than fine-tuning
This demonstrates that the proposed attention layers effectively learn to  select the most discriminative features for the retrieval task,  even if the features are simply pretrained on ImageNet
 In terms of memory requirement, DELF, CONGAS and  DIR are almost equally complex
DELF and CONGAS  adopt the same feature dimensionality and maximum number  of features per image; they require approximately NGB of  memory
DIR descriptors need NKB per image, summing up  to approximately NGB to index the entire dataset
 N.N
Qualitative Results  We present qualitative results to illustrate performance  of DELF compared to two competitive algorithms based on  global and local features—DIR and CONGAS, respectively
 Also, we analyze our attention-based keypoint detection  algorithm by visualization
 DELF vs
DIR Fig
N shows retrieval results, where DELF  outperforms DIR
DELF obtains matches between specific  local regions in images, which helps significantly to find the  same object in different imaging conditions
Common failure cases of DIR happen when the database contains similar  objects or scenes, e.g., obelisks, mountains, harbors, as illustrated in Fig
N
In many cases, DIR cannot distinguish these  NNNN    (a) (b) (c)  Figure N: Examples where DELF+FT+ATT outperforms DIR: (a)  query image, (b) top-N image of DELF+FT+ATT, (c) top-N image  of DIR
The green borders denote correct results while the red ones  mean incorrect retrievals
Note that DELF deals with clutter in  query and database images and small landmarks effectively
 specific objects or scenes; although it finds semantically similar images, they often do not correspond to the instance of  interest
Another weakness of DIR and other global descriptors is that they are not good at identifying small objects of  interest
Fig
N shows the cases that DIR outperforms DELF
 While DELF is able to match localized patterns across different images, this leads to errors when the floor tiling or  vegetation is similar across different landmarks
 DELF vs
CONGAS The main advantage of DELF over  CONGAS is its recall; it retrieves more relevant landmarks  than CONGAS, which suggests that DELF descriptors are  more discriminative
We did not observe significant examples where CONGAS outperforms DELF
Fig
N shows pairs  of images from query and database, which are successfully  matched by DELF but missed by CONGAS, where feature  (a) (b) (c)  Figure N: Examples where DIR outperforms DELF+FT+ATT: (a)  query image, (b) top-N image of DELF+FT+ATT, (c) top-N image  of DIR
The green and red borders denotes correct and incorrect  results, respectively
 correspondences are presented by connecting the center of  the receptive fields for matching features
Since the receptive  fields can be fairly large, some features seem to be localized  in undiscriminative regions, e.g., ocean or sky
However, in  these cases, the features take into account more discriminative regions in the neighborhood
 Analysis of keypoint detection methods Fig
N visualizes  three variations of keypoint detection, where the benefit of  our attention model is clearly illustrated qualitatively while  the LN norm of fine-tuned features is marginally different  from the one without fine-tuning
 N.N
Results in Existing Datasets  We demonstrate the performance of DELF in existing  datasets such as OxfNk, ParNk and their extensions, OxfN0Nk  and ParN0Nk, for completeness
For this experiment, we simply obtain the score per image using the proposed method,  and make a late fusion with the score from DIR by computing a weighted mean of two normalized scores, where the  weight for DELF is set to 0.NN
The results are presented in  Tab
N
We present accuracy of existing methods in their original papers and our reproductions using public source codes,  which are very close
DELF improves accuracy nontrivially  in the datasets when combined with DIR, although it does not  show the best performance by itself
This fact indicates that  DELF has capability to encode complementary information  that is not available in global feature descriptors
 N
Conclusion  We presented DELF, a new local feature descriptor that  is designed specifically for large-scale image retrieval apNNNN    Figure N: Visualization of feature correspondences between images in query and database using DELF+FT+ATT
For each pair, query and  database images are presented side-by-side
DELF successfully matches landmarks and objects in challenging environment including partial  occlusion, distracting objects, and background clutter
Both ends of the red lines denote the centers of matching features
Since the receptive  fields are fairly large, the centers may be located outside landmark object areas
For the same queries, CONGAS fails to retrieve any image
 (a) (b) (c) (d)  Figure N: Comparison of keypoint selection methods
(a) Input  image (b) LN norm scores using the pretrained model (DELFnoFT) (c) LN norm scores using fine-tuned descriptors (DELF+FT)  (d) Attention-based scores (DELF+FT+ATT)
Our attention-based  model effectively disregards clutter compared to other options
 Table N: Performance evaluation on existing datasets in mAP (%)
 All results of existing methods are based on our reproduction using  public source codes
We tested LIFT only on OxfNk and ParNk due  to its slow speed
(* denotes the results from the original papers.)  Dataset OxfNk OxfN0Nk ParNk ParN0Nk  DIR [NN] NN.N NN.N NN.N N0.N  DIR+QE [NN] NN.N NN.N NN.N NN.N  siaMAC [NN] NN.N NN.N NN.N NN.N  siaMAC+QE [NN] NN.N NN.N NN.N NN.N  CONGAS [N] N0.N NN.N NN.N NN.N  LIFT [N0] NN.0 – NN.N –  DIR+QE* [NN] NN.0 NN.N NN.N N0.N  siaMAC+QE* [NN] NN.N NN.N NN.N NN.N  DELF+FT+ATT (ours) NN.N NN.N NN.0 NN.N  DELF+FT+ATT+DIR+QE (ours) N0.0 NN.N NN.N NN.N  plications
DELF is learned with weak supervision, using  image-level labels only, and is coupled with our new attention mechanism for semantic feature selection
In the proposed CNN-based model, one forward pass over the network  is sufficient to obtain both keypoints and descriptors
To  properly evaluate performance of large-scale image retrieval  algorithm, we introduced Google-Landmarks dataset, which  consists of more than NM database images, NNK unique landmarks, and N00K query images
The evaluation in such a  large-scale setting shows that DELF outperforms existing  global and local descriptors by substantial margins
We also  present results on existing datasets, and show that DELF  achieves excellent performance when combined with global  descriptors
 Acknowledgement This work was performed while the  first and last authors were in Google Inc., CA
It was partly  supported by the ICT R&D program of MSIP/IITP [N0NN-000NNN] and the NRF grant [NRF-N0NN-00NNNNN] in Korea
 NNNN    References  [N] H
Aradhye, G
Toderici, and J
Yagnik
VideoNtext: Learning  to Annotate Video Content
In Proc
IEEE International  Conference on Data Mining Workshops, N00N
N  [N] R
Arandjelović, P
Gronat, A
Torii, T
Pajdla, and J
Sivic
 NetVLAD: CNN Architecture for Weakly Supervised Place  Recognition
In Proc
CVPR, N0NN
N, N  [N] A
Babenko and V
Lempitsky
Aggregating Local Deep  Features for Image Retrieval
In Proc
ICCV, N0NN
N  [N] A
Babenko, A
Slesarev, A
Chigorin, and V
Lempitsky
 Neural Codes for Image Retrieval
In Proc
ECCV, N0NN
N,  N, N  [N] H
Bay, A
Ess, T
Tuytelaars, and L
Van Gool
SpeededUp Robust Features (SURF)
Computer Vision and Image  Understanding, NN0(N):NNN–NNN, N00N
N  [N] J
S
Beis and D
G
Lowe
Shape Indexing Using Approximate Nearest-Neighbour Search in High-Dimensional Spaces
 In Proc
CVPR, NNNN
N  [N] J
L
Bentley
Multidimensional Binary Search Trees Used for  Associative Searching
Communications of the ACM, NN(N),  NNNN
N  [N] U
Buddemeier and H
Neven
Systems and Methods for  Descriptor Vector Computation, N0NN
US Patent N,0NN,NNN
 N, N, N  [N] C
Dugas, Y
Bengio, C
Nadeau, and R
Garcia
Incorporating Second-Order Functional Knowledge for Better Option  Pricing
In Proc
NIPS, N00N
N  [N0] M
Fischler and R
Bolles
Random Sample Consensus: A  Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography
Communications of the  ACM, NN(N):NNN–NNN, NNNN
N  [NN] A
Gordo, J
Almazan, J
Revaud, and D
Larlus
Deep Image  Retrieval: Learning Global Representations for Image Search
 In Proc
ECCV, N0NN
N, N, N, N  [NN] X
Han, T
Leung, Y
Jia, R
Sukthankar, and A
C
Berg
 MatchNet: Unifying Feature and Metric Learning for PatchBased Matching
In Proc
CVPR, N0NN
N, N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep Residual Learning  for Image Recognition
In Proc
CVPR, N0NN
N, N  [NN] S
Hong, H
Noh, and B
Han
Decoupled Deep Neural  Network for Semi-supervised Semantic Segmentation
In  Proc
NIPS, N0NN
N  [NN] H
Jégou and O
Chum
Negative Evidences and CoOccurences in Image Retrieval: The Benefit of PCA and  Whitening
In Proc
ECCV, N0NN
N  [NN] H
Jégou, M
Douze, and C
Schmid
Hamming Embedding  and Weak Geometric Consistency for Large Scale Image  Search
In Proc
ECCV, N00N
N, N  [NN] H
Jegou, M
Douze, and C
Schmid
Product Quantization  for Nearest Neighbor Search
IEEE Transactions on Pattern  Analysis and Machine Intelligence, NN(N), N0NN
N  [NN] H
Jégou, M
Douze, C
Schmidt, and P
Perez
Aggregating  Local Descriptors into a Compact Image Representation
In  Proc
CVPR, N0N0
N  [NN] H
Jégou, F
Perronnin, M
Douze, J
Sanchez, P
Perez, and  C
Schmid
Aggregating Local Image Descriptors into Compact Codes
IEEE Transactions on Pattern Analysis and  Machine Intelligence, NN(N), N0NN
N  [N0] Y
Kalantidis and Y
Avrithis
Locally Optimized Product  Quantization for Approximate Nearest Neighbor Search
In  Proc
CVPR, N0NN
N  [NN] Y
Kalantidis, C
Mellina, and S
Osindero
CrossDimensional Weighting for Aggregated Deep Convolutional  Features
In Proc
ECCV Workshops, N0NN
N  [NN] D
Lowe
Distinctive Image Features from Scale-Invariant  Keypoints
International Journal of Computer Vision, N0(N),  N00N
N, N, N  [NN] H
Neven, G
Rose, and W
G
Macready
Image Recognition with an Adiabatic Quantum Computer I
Mapping to Quadratic Unconstrained Binary Optimization
 arXiv:0N0N.NNNN, N00N
N  [NN] J
Y.-H
Ng, F
Yang, and L
S
Davis
Exploiting Local  Features from Deep Networks for Image Retrieval
In Proc
 CVPR Workshops, N0NN
N  [NN] D
Nistér and H
Stewenius
Scalable Recognition with a  Vocabulary Tree
In Proc
CVPR, N00N
N  [NN] F
Perronnin, Y
Liu, and J.-M
Renders
A Family of Contextual Measures of Similarity between Distributions with  Application to Image Retrieval
In Proc
CVPR, N00N
N  [NN] J
Philbin, O
Chum, M
Isard, J
Sivic, and A
Zisserman
 Object Retrieval with Large Vocabularies and Fast Spatial  Matching
In Proc
CVPR, N00N
N, N  [NN] J
Philbin, O
Chum, M
Isard, J
Sivic, and A
Zisserman
 Lost in Quantization: Improving Particular Object Retrieval  in Large Scale Image Databases
In Proc
CVPR, N00N
N, N  [NN] F
Radenović, G
Tolias, and O
Chum
CNN Image Retrieval  Learns from BoW: Unsupervised Fine-Tuning with Hard Examples
In Proc
ECCV, N0NN
N, N, N, N  [N0] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN:  Towards Real-Time Object Detection with Region Proposal  Networks
In Proc
NIPS, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh, S
Ma,  Z
Huang, A
Karpathy, A
Khosla, M
Bernstein, et al
ImageNet Large Scale Visual Recognition Challenge
International Journal of Computer Vision, NNN(N), N0NN
N  [NN] K
Simonyan and A
Zisserman
Very Deep Convolutional  Networks for Large-Scale Image Recognition
In Proc
ICLR,  N0NN
N  [NN] J
Sivic and A
Zisserman
Video Google: A Text Retrieval  Approach to Object Matching in Videos
In Proc
ICCV, N00N
 N  [NN] G
Tolias, R
Sicre, and H
Jégou
Particular Object Retrieval  with Integral Max-Pooling of CNN Activations
In Proc
 ICLR, N0NN
N  [NN] A
Torii, J
Sivic, T
Pajdla, and M
Okutomi
Visual Place  Recognition with Repetitive Structures
In Proc
CVPR, N0NN
 N  [NN] T
Uricchio, M
Bertini, L
Seidenari, and A
Bimbo
Fisher  Encoded Convolutional Bag-of-Windows for Efficient Image  Retrieval and Social Image Tagging
In Proc
ICCV Workshops, N0NN
N  [NN] Y
Verdie, K
M
Yi, P
Fua, and V
Lepetit
TILDE: A Temporally Invariant Learned Detector
In Proc
CVPR, N0NN
 N  NNNN    [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
Courville, R
Salakhudinov,  R
Zemel, and Y
Bengio
Show, Attend and Tell: Neural  Image Caption Generation with Visual Attention
In Proc
 ICML, N0NN
N  [NN] Z
Yang, X
He, J
Gao, L
Deng, and A
Smola
Stacked  Attention Networks for Image Question Answering
In Proc
 CVPR, N0NN
N  [N0] K
M
Yi, E
Trulls, V
Lepetit, and P
Fua
LIFT: Learned  Invariant Feature Transform
In Proc
ECCV, N0NN
N, N, N, N,  N, N  [NN] K
M
Yi, Y
Verdie, P
Fua, and V
Lepetit
Learning to Assign  Orientations to Feature Points
In Proc
CVPR, N0NN
N  [NN] S
Zagoruyko and N
Komodakis
Learning to Compare Image  Patches via Convolutional Neural Networks
In Proc
CVPR,  N0NN
N, N  [NN] L
Zheng, Y
Yang, and Q
Tian
SIFT Meets CNN: A Decade  Survey of Instance Retrieval
arXiv:NN0N.0NN0N, N0NN
N  [NN] Y.-T
Zheng, M
Zhao, Y
Song, H
Adam, U
Buddemeier,  A
Bissacco, F
Brucher, T.-S
Chua, and H
Neven
Tour the  World: Building a Web-Scale Landmark Recognition Engine
 In Proc
CVPR, N00N
N, N  [NN] B
Zhou, A
Khosla, A
Lapedriza, A
Oliva, and A
Torralba
 Learning Deep Features for Discriminative Localization
In  CVPR, N0NN
N  NNNNPredicting Visual Exemplars of Unseen Classes for Zero-Shot Learning   Predicting Visual Exemplars of Unseen Classes for Zero-Shot Learning  Soravit Changpinyo  U
of Southern California  Los Angeles, CA  schangpi@usc.edu  Wei-Lun Chao  U
of Southern California  Los Angeles, CA  weilun@usc.edu  Fei Sha  U
of Southern California  Los Angeles, CA  feisha@usc.edu  Abstract  Leveraging class semantic descriptions and examples of  known objects, zero-shot learning makes it possible to train  a recognition model for an object class whose examples are  not available
In this paper, we propose a novel zero-shot  learning model that takes advantage of clustering structures  in the semantic embedding space
The key idea is to impose the structural constraint that semantic representations  must be predictive of the locations of their corresponding  visual exemplars
To this end, this reduces to training multiple kernel-based regressors from semantic representationexemplar pairs from labeled data of the seen object categories
Despite its simplicity, our approach significantly  outperforms existing zero-shot learning methods on standard benchmark datasets, including the ImageNet dataset  with more than N0,000 unseen categories
 N
Introduction  A series of major progresses in visual object recognition  can largely be attributed to learning large-scale and complex models with a huge number of labeled training images
 There are many application scenarios, however, where collecting and labeling training instances can be laboriously  difficult and costly
For example, when the objects of interest are rare (e.g., only about a hundred of northern hairynosed wombats alive in the wild) or newly defined (e.g.,  images of futuristic products such as Tesla’s Model S), not  only the amount of the labeled training images but also the  statistical variation among them is limited
These restrictions do not lead to robust systems for recognizing such objects
More importantly, the number of such objects could  be significantly greater than the number of common objects
 In other words, the frequencies of observing objects follow  a long-tailed distribution [NN, NN]
 Zero-shot learning (ZSL) has since emerged as a promising paradigm to remedy the above difficulties
Unlike supervised learning, ZSL distinguishes between two types of  classes: seen and unseen, where labeled examples are available for the seen classes only
Crucially, zero-shot learners  have access to a shared semantic space that embeds all categories
This semantic space enables transferring and adapting classifiers trained on the seen classes to the unseen ones
 Multiple types of semantic information have been exploited  in the literature: visual attributes [NN, NN], word vector representations of class names [NN, NN, NN], textual descriptions  [N0, NN, NN], hierarchical ontology of classes (such as WordNet [NN]) [N, NN, NN], and human gazes [NN]
 Many ZSL methods take a two-stage approach: (i) predicting the embedding of the image in the semantic space;  (ii) inferring the class labels by comparing the embedding  to the unseen classes’ semantic representations [NN, NN, NN,  NN, NN, NN, NN, NN]
Recent ZSL methods take a unified  approach by jointly learning the functions to predict the semantic embeddings as well as to measure similarity in the  embedding space [N, N, NN, NN, NN, N0, N]
We refer the readers to the descriptions and evaluation on these representative  methods in [NN]
 Despite these attempts, zero-shot learning is proved to  be extremely difficult
For example, the best reported accuracy on the full ImageNet with NNK categories is only N.N%  [N], where the state-of-the-art performance with supervised  learning reaches NN.N% [N]N
 There are at least two critical reasons for this
First, class  semantic representations are vital for knowledge transfer  from the seen classes to unseen ones, but these representations are hard to get right
Visual attributes are humanunderstandable so they correspond well with our object  class definition
However, they are not always discriminative [NN, NN], not necessarily machine detectable [N, NN], often correlated among themselves (“brown” and “wooden”’)  [NN], and possibly not category-independent (“fluffy” animals and “fluffy” towels) [N]
Word vectors of class names  have shown to be inferior to attributes [N, N]
Derived from  texts, they have little knowledge about or are barely aligned  with visual information
 NComparison between the two numbers is not entirely fair due to different training/test splits
Nevertheless, it gives us a rough idea on how huge  the gap is
This observation has also been shown on small datasets [N]
 NNNNN    : class exemplar  Semantic representations  a House Wren  a Cardinal  a Cedar Waxwing  v Cardinal  Visual features  (ac) ≈ vc PCA  Semantic embedding space  (au) for NN classification or to improve existing ZSL approaches  a Gadwall  aMallard  Figure N
Given the semantic information and visual features of the seen classes, our method learns a kernel-based regressor ψ(·) such that the semantic representation ac of class c can predict well its class exemplar (center) vc that characterizes the clustering structure
The  learned ψ(·) can be used to predict the visual feature vectors of the unseen classes for nearest-neighbor (NN) classification, or to improve the semantic representations for existing ZSL approaches
 The other reason is that the lack of data for the unseen  classes presents a unique challenge for model selection
The  crux of ZSL involves learning a compatibility function between the visual feature of an image and the semantic representation of each class
But, how are we going to parameterize this function? Complex functions are flexible but at  risk of overfitting to the seen classes and transferring poorly  to the unseen ones
Simple ones, on the other hand, will result in poorly performing classifiers on the seen classes and  will unlikely perform well either on the unseen ones
For  these reasons, the success of ZSL methods hinges critically  on the insight of the underlying mechanism for transfer and  how well that insight is in accordance with data
 One particular fruitful (and often implicitly stated) insight is the existence of clustering structures in the semantic  embedding space
That is, images of the same class, after  embedded into the semantic space, will cluster around the  semantic embedding of that class
For example, ConSE [NN]  aligns a convex composition of the classifier probabilistic  outputs to the semantic representations
A recent method of  synthesized classifiers (SynC) [N] models two aligned manifolds of clusters, one corresponding to the semantic embeddings of all objects and the other corresponding to the  “centers”N in the visual feature space, where the pairwise  distances between entities in each space are used to constrain the shapes of both manifolds
These lines of insights  have since yielded excellent performance on ZSL
 In this paper, we propose a simple yet very effective ZSL  algorithm that assumes and leverages more structural relations on the clusters
The main idea is to exploit the intuition  that the semantic representation can predict well the location of the cluster characterizing all visual feature vectors  from the corresponding class (c.f
Sect
N.N)
 More specifically, the main computation step of our approach is reduced to learning (from the seen classes) a predictive function from semantic representations to their corresponding centers (i.e., exemplars) of visual feature vecNThe centers are defined as the normals of the hyperplanes separating  different classes
 tors
This function is used to predict the locations of visual exemplars of the unseen classes that are then used to  construct nearest-neighbor style classifiers, or to improve  the semantic information demanded by existing ZSL approaches
Fig
N shows the conceptual diagram of our approach
 Our proposed method tackles the two challenges for ZSL  simultaneously
First, unlike most of the existing ZSL methods, we acknowledge that semantic representations may not  necessarily contain visually discriminating properties of objects classes
As a result, we demand that the predictive constraint be imposed explicitly
In our case, we assume that  the cluster centers of visual feature vectors are our target  semantic representations
Second, we leverage structural  relations on the clusters to further regularize the model,  strengthening the usefulness of the clustering structure assumption for model selection
 We validate the effectiveness of our proposed approach  on four benchmark datasets for ZSL, including the full ImageNet dataset with more than N0,000 unseen classes
Despite its simplicity, our approach outperforms other existing  ZSL approaches in most cases, demonstrating the potential  of exploiting the structural relatedness between visual features and semantic information
Additionally, we complement our empirical studies with extensions from zero-shot  to few-shot learning, as well as analysis of our approach
 The rest of the paper is organized as follows
We describe our proposed approach in Sect
N
We demonstrate  the superior performance of our method in Sect
N
We discuss relevant work in Sect
N and finally conclude in Sect
N
 N
Approach  We describe our methods for addressing zero-shot learning, where the task is to classify images from the unseen  classes into the label space of the unseen classes
Our approach is based on the structural constraint that takes advantage of the clustering structure assumption in the semantic  embedding space
The constraint forces the semantic representations to be predictive of their visual exemplars (i.e.,  NNNN    cluster centers)
In this section, we describe how we achieve  this goal
First, we describe how we learn a function to predict the visual exemplars from the semantic representations
 Second, given a novel semantic representation, we describe  how we apply this function to perform zero-shot learning
 Notations We follow the notation system introduced in  [N] to facilitate comparison
We denote by D = {(xn ∈ R  D, yn)} N n=N the training data with the labels from the label  space of seen classes S = {N, N, · · · , S}
we denote by U = {S + N, · · · , S + U} the label space of unseen classes
For each class c ∈ S ∪ U , let ac be its semantic representation
 N.N
Learning to predict the visual exemplars from the semantic representations  For each class c, we would like to find a transformation  function ψ(·) such that ψ(ac) ≈ vc, where vc ∈ R d is the  visual exemplar for the class
In this paper, we create the  visual exemplar of a class by averaging the PCA projections  of data belonging to that class
That is, we consider vc = N  |Ic|  ∑ n∈Ic  Mxn, where Ic = {i : yi = c} and M ∈  R d×D is the PCA projection matrix computed over training  data of the seen classes
We note thatM is fixed for all data  points (i.e., not class-specific) and is used in Eq
(N)
 Given training visual exemplars and semantic representations, we learn d support vector regressors (SVR) with  the RBF kernel — each of them predicts each dimension of  visual exemplars from their corresponding semantic representations
Specifically, for each dimension d = N, 


, d, we use the ν-SVR formulation [NN]
Details are in the supplementary material
 Note that the PCA step is introduced for both the computational and statistical benefits
In addition to reducing dimensionality for faster computation, PCA decorrelates the  dimensions of visual features such that we can predict these  dimensions independently rather than jointly
 See Sect
N.N.N for analysis on applying SVR and PCA
 N.N
Zero-shot learning based on the predicted vi- sual exemplars  Now that we learn the transformation functionψ(·), how do we use it to perform zero-shot classification? We first  apply ψ(·) to all semantic representations au of the unseen classes
We consider two main approaches that depend on  how we interpret these predicted exemplars ψ(au)
 N.N.N Predicted exemplars as training data  An obvious approach is to useψ(au) as data directly
Since there is only one data point per class, a natural choice is to  use a nearest neighbor classifier
Then, the classifier outputs  the label of the closest exemplar for each novel data point x  that we would like to classify:  ŷ = argmin u  disNN (Mx,ψ(au)), (N)  where we adopt the (standardized) Euclidean distance as  disNN in the experiments
 N.N.N Predicted exemplars as the ideal semantic representations  The other approach is to use ψ(au) as the ideal semantic representations (“ideal” in the sense that they have knowledge about visual features) and plug them into any existing  zero-shot learning framework
We provide two examples
 In the method of convex combination of semantic embeddings (ConSE) [NN], their original semantic embeddings  are replaced with the corresponding predicted exemplars,  while the combining coefficients remain the same
In the  method of synthesized classifiers (SynC) [N], the predicted  exemplars are used to define the similarity values between  the unseen classes and the bases, which in turn are used to  compute the combination weights for constructing classifiers
In particular, their similarity measure is of the form exp{−dis(ac,br)}∑  R  r=N exp{−dis(ac,br)}  , where dis is the (scaled) Euclidean  distance and br’s are the semantic representations of the  base classes
In this case, we simply need to change this  similarity measure to exp{−dis(ψ(ac),ψ(br))}∑  R  r=N exp{−dis(ψ(ac),ψ(br))}  
 We note that, recently, Chao et al
[N] empirically show  that existing semantic representations for ZSL are far from  the optimal
Our approach can thus be considered as a way  to improve semantic representations for zero-shot learning
 N.N
Comparison to related approaches  One appealing property of our approach is its scalability: we learn and predict at the exemplar (class) level so the  runtime and memory footprint of our approach depend only  on the number of seen classes rather the number of training  data points
This is much more efficient than other ZSL algorithms that learn at the level of each individual training  instance [NN, NN, NN, N, NN, NN, NN, NN, NN, NN, N, NN, NN, N0,  NN, N]
 Several methods propose to learn visual exemplarsN by  preserving structures obtained in the semantic space [N, NN,  N0]
However, our approach predicts them with a regressor  such that they may or may not strictly follow the structure  in the semantic space, and thus they are more flexible and  could even better reflect similarities between classes in the  visual feature space
 Similar in spirit to our work, [NN] proposes using nearest class mean classifiers for ZSL
The Mahalanobis metric  learning in this work could be thought of as learning a linear  NExemplars are used loosely here and do not necessarily mean classspecific feature averages
 NNNN    Table N
Key characteristics of the datasets  Dataset # of seen classes # of unseen classes # of images  AwA† N0 N0 N0,NNN  CUB‡ NN0 N0 NN,NNN  SUN‡ NNN/NNN NN/NN NN,NN0  ImageNet§ N,000 N0,NNN NN,NNN,NNN  †: on the prescribed split in [NN]
‡: on N (or N0, respectively) random splits [N], reporting average
§: Seen and unseen classes from ImageNet ILSVRC N0NN NK [NN] and  Fall N0NN release [N, NN, NN]
 transformation of semantic representations (their “zero-shot  prior” means, which are in the visual feature space)
Our  approach learns a highly non-linear transformation
Moreover, our EXEM (NNNS) (cf
Sect
N.N) learns a (simpler,  i.e., diagonal) metric over the learned exemplars
Finally,  the main focus of [NN] is on incremental, not zero-shot,  learning settings (see also [NN, NN])
 [NN] proposes to use a deep feature space as the semantic embedding space for ZSL
Though similar to ours, they  do not compute average of visual features (exemplars) but  train neural networks to predict all visual features from their  semantic representations
Their model learning takes significantly longer time than ours
Neural networks are more  prone to overfitting and give inferior results (cf
Sect
N.N.N)
 Additionally, we provide empirical studies on much largerscale datasets for both zero-shot and few-shot learning, and  analyze the effect of PCA
 N
Experiments  We evaluate our methods and compare to existing stateof-the-art models on four benchmark datasets with diverse  domains and scales
Despite variations in datasets, evaluation protocols, and implementation details, we aim to  provide a comprehensive and fair comparison to existing  methods by following the evaluation protocols in [N]
Note  that [N] reports results of many other existing ZSL methods based on their settings
Details on these settings are  described below and in the supplementary material
 N.N
Setup  Datasets We use four benchmark datasets for zero-shot  learning in our experiments: Animals with Attributes  (AwA) [NN], CUB-N00-N0NN Birds (CUB) [NN], SUN  Attribute (SUN) [N0], and ImageNet (with full NN,NNN  classes) [NN]
Table N summarizes their key characteristics
 The supplementary material provides more details
 Semantic representations We use the publicly available  NN, NNN, and N0N dimensional continuous-valued attributes  for AwA, CUB, and SUN, respectively
For ImageNet,  there are two types of semantic representations of the class  names
First, we use the N00 dimensional word vectors [N] obtained from training a skip-gram model [NN] on  Wikipedia
We remove the class names without word vectors, making the number of unseen classes to be N0,NNN (out  of N0,NNN)
Second, we derive NN,NNN dimensional semantic  vectors of the class names using multidimensional scaling  (MDS) on the WordNet hierarchy, as in [NN]
We normalize  the class semantic representations to have unit ℓN norms
 Visual features We use GoogLeNet features (N,0NN dimensions) [N0] provided by [N] due to their superior performance [N, N] and prevalence in existing literature on ZSL
 Evaluation protocols For AwA, CUB, and SUN, we  use the multi-way classification accuracy (averaged over  classes) as the evalution metric
On ImageNet, we describe  below additional metrics and protocols introduced in [NN]  and followed by [N, NN, NN]
 First, two evaluation metrics are employed: Flat hit@K  (F@K) and Hierarchical precision@K (HP@K)
F@K is  defined as the percentage of test images for which the model  returns the true label in its top K predictions
Note that,  F@N is the multi-way classification accuracy (averaged over  samples)
HP@K is defined as the percentage of overlapping (i.e., precision) between the model’s top K predictions  and the ground-truth list
For each class, the ground-truth  list of its K closest categories is generated based on the ImageNet hierarchy [N]
See the Appendix of [NN, N] for details
 Essentially, this metric allows for some errors as long as the  predicted labels are semantically similar to the true one
 Second, we evaluate ZSL methods on three subsets of  the test data of increasing difficulty: N-hop, N-hop, and  All
N-hop contains N,N0N (out of N,NNN) unseen classes  that are within N tree hops of the NK seen classes according to the ImageNet hierarchy
N-hop contains N,NNN (out  of N,NN0) unseen classes that are within N tree hops of seen  classes
Finally, All contains all N0,NNN (out of N0,NNN) unseen classes in the ImageNet N0NN NNK dataset that are not  in the ILSVRC N0NN NK dataset
 Note that word vector embeddings are missing for certain class names with rare words
For the MDS-WordNet  features, we provide results for All only for comparison to  [NN]
In this case, the number of unseen classes is N0,NNN
 Baselines We compare our approach with several state-ofthe-art and recent competitive ZSL methods summarized in  Table N
Our main focus will be on SYNC [N], which has  recently been shown to have superior performance against  competitors under the same setting, especially on largescale datasets [NN]
Note that SYNC has two versions: oneversus-other loss formulation SYNCo-v-o and the CrammerSinger formulation [N] SYNCstruct
On small datasets, we  also report results from recent competitive baselines LATEM  [NN] and BIDILEL [NN]
For additional details regarding  other (weaker) baselines, see the supplementary material
 Finally, we compare our approach to all ZSL methods that  provide results on ImageNet
When using word vectors  of the class names as semantic representations, we comNNNN    Table N
We compute the Euclidean distance matrix between the  unseen classes based on semantic representations (Dau ), predicted exemplars (Dψ(au)), and real exemplars (Dvu )
Our  method leads to Dψ(au) that is better correlated with Dvu than  Dau is
See text for more details
 Dataset Correlation to Dvu name Semantic distances Predicted exemplar distances  Dau Dψ(au)  AwA 0.NNN 0.NNN  CUB 0.NNN ± 0.0NN 0.N0N ± 0.0NN  SUN 0.NNN ± 0.0NN 0.NNN ± 0.0NN  pare our method to CONSE [NN] and SYNC [N]
When using MDS-WordNet features as semantic representations, we  compare our method to SYNC [N] and CCA [NN]
 Variants of our ZSL models given predicted exemplars  The main step of our method is to predict visual exemplars  that are well-informed about visual features
How we proceed to perform zero-shot classification (i.e., classifying test  data into the label space of unseen classes) based on such  exemplars is entirely up to us
In this paper, we consider  the following zero-shot classification procedures that take  advantage of the predicted exemplars:  • EXEM (ZSL method): ZSL method with predicted exemplars as semantic representations, where ZSL  method = CONSE [NN], LATEM [NN], and SYNC [N]
• EXEM (NNN): N-nearest neighbor classifier with the  Euclidean distance to the exemplars
 • EXEM (NNNS): N-nearest neighbor classifier with the standardized Euclidean distance to the exemplars,  where the standard deviation is obtained by averaging  the intra-class standard deviations of all seen classes
 EXEM (ZSL method) regards the predicted exemplars as  the ideal semantic representations (Sect
N.N.N)
On the  other hand, EXEM (NNN) treats predicted exemplars as data  prototypes (Sect
N.N.N)
The standardized Euclidean distance in EXEM (NNNS) is introduced as a way to scale the  variance of different dimensions of visual features
In other  words, it helps reduce the effect of collapsing data that is  caused by our usage of the average of each class’ data as  cluster centers
 Hyper-parameter tuning We simulate zero-shot scenarios  to perform N-fold cross-validation during training
Details  are in the supplementary material
 N.N
Predicted visual exemplars  We first show that predicted visual exemplars better reflect visual similarities between classes than semantic representations
Let Dau be the pairwise Euclidean distance  matrix between unseen classes computed from semantic  representations (i.e., U by U), Dψ(au) the distance matrix  computed from predicted exemplars, and Dvu the distance  matrix computed from real exemplars (which we do not  have access to)
Table N shows that the correlation between  Dψ(au) and Dvu is much higher than that between Dau and Dvu 
Importantly, we improve this correlation without  access to any data of the unseen classes
See also similar  results using another metric in the supplementary material
 We then show some t-SNE [NN] visualization of predicted visual exemplars of the unseen classes
Ideally, we  would like them to be as close to their corresponding real  images as possible
In Fig
N, we demonstrate that this is indeed the case for many of the unseen classes; for those unseen classes (each of which denoted by a color), their real  images (crosses) and our predicted visual exemplars (circles) are well-aligned
 The quality of predicted exemplars (in this case based on  the distance to the real images) depends on two main factors: the predictive capability of semantic representations  and the number of semantic representation-visual exemplar  pairs available for training, which in this case is equal to  the number of seen classes S
On AwA where we have only  N0 training pairs, the predicted exemplars are surprisingly accurate, mostly either placed in their corresponding clusters or at least closer to their clusters than predicted exemplars of the other unseen classes
Thus, we expect them to  be useful for discriminating among the unseen classes
On  ImageNet, the predicted exemplars are not as accurate as  we would have hoped, but this is expected since the word  vectors are purely learned from text
 We also observe relatively well-separated clusters in the  semantic embedding space (in our case, also the visual feature space since we only apply PCA projections to the visual  features), confirming our assumption about the existence of  clustering structures
On CUB, we observe that these clusters are more mixed than on other datasets
This is not surprising given that it is a fine-grained classification dataset of  bird species
 N.N
Zero-shot learning results  N.N.N Main results  Table N summarizes our results in the form of multiway classification accuracies on all datasets
We significantly outperform recent state-of-the-art baselines when using GoogLeNet features
In the supplementary material, we  provide additional quantitative and qualitative results, including those on generalized zero-shot learning task [N]
 We note that, on AwA, several recent methods obtain  higher accuracies due to using a more optimistic evaluation  metric (per-sample accuracy) and new types of deep features [NN, NN]
This has been shown to be unsuccessfully  replicated (cf
Table N in [NN])
See the supplementary  material for results of these and other less competitive baselines
 Our alternative approach of treating predicted visual  exemplars as the ideal semantic representations significantly outperforms taking semantic representations as  NNN0    Figure N
t-SNE [NN] visualization of randomly selected real images (crosses) and predicted visual exemplars (circles) for the unseen classes  on (from left to right) AwA, CUB, SUN, and ImageNet
Different colors of symbols denote different unseen classes
Perfect predictions  of visual features would result in well-aligned crosses and circles of the same color
Plots for CUB and SUN are based on their first splits
 Plots for ImageNet are based on randomly selected NN unseen classes from N-hop and word vectors as semantic representations
Best  viewed in color
See the supplementary material for larger figures
 Table N
Comparison between existing ZSL approaches in multiway classification accuracies (in %) on four benchmark datasets
 For each dataset, we mark the best in red and the second best in  blue
Italic numbers denote per-sample accuracy instead of perclass accuracy
On ImageNet, we report results for both types  of semantic representations: Word vectors (wv) and MDS embeddings derived from WordNet (hie)
All the results are based on  GoogLeNet features [N0]
 .Approach AwA CUB SUN ImageNet wv hie  CONSE† [NN] NN.N NN.N NN.N N.N BIDILEL [NN] NN.N NN.N§ - - LATEM‡ [NN] NN.N NN.0 NN.N - CCA [NN] - - - - N.N  SYNCo-vs-o [N] NN.N NN.N NN.N N.N N.0  SYNCstruct [N] NN.N NN.N NN.N N.N EXEM (CONSE) N0.N NN.N N0.0 - EXEM (LATEM)‡ NN.N NN.N NN.N - EXEM (SYNCO-VS-O ) NN.N NN.N NN.N N.N N.0  EXEM (SYNCSTRUCT ) NN.N NN.N NN.N - EXEM (NNN) NN.N NN.N NN.N N.N N.0  EXEM (NNNS) NN.N NN.N NN.N N.N N.0  §: on a particular split of seen/unseen classes
†: reported in [N]
‡: based on the code of [NN], averaged over N different initializations
 given
EXEM (SYNC), EXEM (CONSE), EXEM (LATEM)  outperform their corresponding base ZSL methods relatively by N.N-N.N%, NN.N-NN.N%, and N.N-NN.N%, respectively
This again suggests improved quality of semantic  representations (on the predicted exemplar space)
 Furthermore, we find that there is no clear winner between using predicted exemplars as ideal semantic representations or as data prototypes
The former seems to perform better on datasets with fewer seen classes
Nonetheless, we remind that using N-nearest-neighbor classifiers  clearly scales much better than zero-shot learning methods;  EXEM (NNN) and EXEM (NNNS) are more efficient than  EXEM (SYNC), EXEM (CONSE), and EXEM (LATEM)
 Finally, we find that in general using the standardized  Euclidean distance instead of the Euclidean distance for  nearest neighbor classifiers helps improve the accuracy, especially on CUB, suggesting there is a certain effect of collapsing actual data during training
The only exception is  on SUN
We suspect that the standard deviation values computed on the seen classes on this dataset may not be robust  enough as each class has only N0 images
 N.N.N Large-scale zero-shot classification results  We then provide expanded results for ImageNet, following  evaluation protocols in the literature
In Table N and N, we  provide results based on the exemplars predicted by word  vectors and MDS features derived from WordNet, respectively
We consider SYNCo-v-o, rather than SYNCstruct, as  the former shows better performance on ImageNet [N]
Regardless of the types of metrics used, our approach outperforms the baselines significantly when using word vectors  as semantic representations
For example, on N-hop, we are  able to improve the F@N accuracy by N% over the state-ofthe-art
However, we note that this improvement is not as  significant when using MDS-WordNet features as semantic  representations
 We observe that the N-nearest-neighbor classifiers perform better than using predicted exemplars as more powerful semantic representations
We suspect that, when the  number of classes is very high, zero-shot learning methods  (CONSE or SYNC) do not fully take advantage of the meaning provided by each dimension of the exemplars
 N.N.N From zero-shot to few-shot learning  In this section, we investigate what will happen when  we allow ZSL algorithms to peek into some labeled data  from part of the unseen classes
Our focus will be on  All categories of ImageNet, two ZSL methods (SYNCo-vs-o  and EXEM (NNN)), and two evaluation metrics (F@N and  F@N0)
For brevity, we will denote SYNCo-vs-o and EXEM  (NNN) by SYNC and EXEM, respectively
 Setup We divide images from each unseen class into two  sets
The first N0% are reserved as training examples that  may or may not be revealed
This corresponds to on average NNN images per class
If revealed, those peeked unseen  classes will be marked as seen, and their labeled data can  be used for training
The other N0% are for testing
The test  NNNN    Table N
Comparison between existing ZSL approaches on ImageNet using word vectors of the class names as semantic representations
 For both metrics (in %), the higher the better
The best is in red
The numbers of unseen classes are listed in parentheses
†: reported in [N]
Test data Approach Flat Hit@K Hierarchical precision@K  K= N N N N0 N0 N N N0 N0  CONSE† [NN] N.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  SYNCo-vs-o [N] N0.N NN.N NN.N N0.N NN.0 NN.N NN.N N0.N NN.N  N-hop (N,N0N) EXEM (SYNCO-VS-O ) NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N  EXEM (NNN) NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  EXEM (NNNS) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0  CONSE† [NN] N.N N.N N.N NN.N NN.N N.N NN.N NN.N NN.N  SYNCo-vs-o [N] N.N N.N N.N NN.N N0.N N.N NN.N NN.N NN.N  N-hop (N,NNN) EXEM (SYNCO-VS-O ) N.N N.N N0.N NN.N NN.N N.N NN.N NN.N NN.N  EXEM (NNN) N.N N.N N0.N NN.N NN.N N.N NN.N NN.N N0.N  EXEM (NNNS) N.N N.N N0.N NN.N NN.N N.N NN.N NN.N NN.N  CONSE† [NN] N.N N.N N.N N.N N.N N.N N.N N0.N NN.0  SYNCo-vs-o [N] N.N N.N N.N N.N N0.N N.N N.0 N0.N NN.N  All (N0,NNN) EXEM (SYNCO-VS-O ) N.N N.N N.0 N.N NN.N N.N N.N NN.0 NN.N  EXEM (NNN) N.N N.N N.N N.N NN.N N.N N0.N NN.N NN.N  EXEM (NNNS) N.N N.N N.N N.N NN.N N.N N0.N NN.N NN.N  Table N
Comparison between existing ZSL approaches on ImageNet (with N0,NNN unseen classes) using MDS embeddings  derived from WordNet [NN] as semantic representations
The  higher, the better (in %)
The best is in red
Test data Approach Flat Hit@K  K= N N N N0 N0  CCA [NN] N.N N.0 N.N N.N N.N  All SYNCo-vs-o [N] N.0 N.N N.0 N.N NN.N  (N0,NNN) EXEM (SYNCO-VS-O ) N.0 N.N N.N N.0 NN.N  EXEM (NNN) N.0 N.N N.N N.N NN.N  EXEM (NNNS) N.0 N.N N.N N.N NN.N  Seen class index Unseen class index  In st  a n  ce  i n  d e  x N0 % for   revealing  N0 % for   testing  : training data from seen classes                                           : additional training data from peeked unseen classes  : test data  : untouched data  Figure N
Data split for zero-to-few-shot learning on ImageNet  set is always fixed such that we have to do few-shot learning  for peeked unseen classes and zero-shot learning on the rest  of the unseen classes
Fig
N summarizes this protocol
 We then vary the number of peeked unseen classes B
 Also, for each of these numbers, we explore the following  subset selection strategies (more details are in the supplementary material): (i) Uniform random: Randomly selected B unseen classes from the uniform distribution; (ii)  Heavy-toward-seen random Randomly selected B classes  that are semantically similar to seen classes according to  the WordNet hierarchy; (iii) Light-toward-seen random  Randomly selected B classes that are semantically far away  from seen classes; (iv) K-means clustering for coverage  Classes whose semantic representations are nearest to each  cluster’s center, where semantic embeddings of the unseen  classes are grouped by k-means clustering with k = B; (v) DPP for diversity Sequentially selected classes by a greedy  algorithm for fixed-sized determinantal point processes (kDPPs) [NN] with the RBF kernel computed on semantic representations
 Results For each of the ZSL methods (EXEM and SYNC),  we first compare different subset selection methods when  the number of peeked unseen classes is small (up to N,000)  in Fig
N
We see that the performances of different subset selection methods are consistent across ZSL methods
Moreover, heavy-toward-seen classes are preferred for  strict metrics (Flat Hit@N) but clustering is preferred for  flexible metrics (Flat Hit@N0)
This suggests that, for a  strict metric, it is better to pick the classes that are semantically similar to what we have seen
On the other hand, if  the metric is flexible, we should focus on providing coverage for all the classes so each of them has knowledge they  can transfer from
 Next, using the best performing heavy-toward-seen selection, we focus on comparing EXEM and SYNC with  larger numbers of peeked unseen classes in Fig
N
When  the number of peeked unseen classes is small, EXEM outperforms SYNC
(In fact, EXEM outperforms SYNC for each  subset selection method in Fig
N.) However, we observe  that SYNC will finally catch up and surpass EXEM
This  is not surprising; as we observe more labeled data (due to  the increase in peeked unseen set size), the setting will become more similar to supervised learning (few-shot learning), where linear classifiers used in SYNC should outperform nearest center classifiers used by EXEM
Nonetheless,  we note that EXEM is more computationally advantageous  than SYNC
In particular, when training on NK classes of  ImageNet with over NM images, EXEM takes N mins while  SYNC N hour
We provide additional results under this scenario in the supplementary material
 N.N.N Analysis  PCA or not? Table N investigates the effect of PCA
In  general, EXEM (NNN) performs comparably with and without PCA
Moreover, decreasing PCA projected dimension d  from N0NN to N00 does not hurt the performance
Clearly, a  NNNN    0 N00 N000 NN00 N000 0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  # peeked unseen classes               a  c c u ra  c y  EXEM: F@N       uniform  heavy−seen  clustering  light−seen  DPP  0 N00 N000 NN00 N000 0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  # peeked unseen classes               a  c c u ra  c y  SynC: F@N       uniform  heavy−seen  clustering  light−seen  DPP  0 N00 N000 NN00 N000 0.N  0.NN  0.NN  0.NN  0.NN  0.N  # peeked unseen classes               a  c c u ra  c y  EXEM: F@N0       uniform  heavy−seen  clustering  light−seen  DPP  0 N00 N000 NN00 N000 0.N  0.NN  0.NN  0.NN  0.NN  0.N  # peeked unseen classes               a  c c u ra  c y  SynC: F@N0       uniform  heavy−seen  clustering  light−seen  DPP  Figure N
Accuracy vs
the number of peeked unseen classes for  EXEM (top) and SYNC (bottom) across different subset selection  methods
Evaluation metrics are F@N (left) and F@N0 (right)
 0 N000 N0000 NN000 0  0.0N  0.0N  0.0N  0.0N  0.N  0.NN  0.NN  0.NN  # peeked unseen classes               a  c c u ra  c y  F@N        EXEM (heavy−seen)  SynC (heavy−seen)  0 N000 N0000 NN000 0.N  0.NN  0.N  0.NN  0.N  0.NN  0.N  0.NN  0.N  # peeked unseen classes               a  c c u ra  c y  F@N0        EXEM (heavy−seen)  SynC (heavy−seen)  Figure N
Accuracy vs
the number of peeked unseen classes for  EXEM and SYNC for heavy-toward-seen class selection strategy
 Evaluation metrics are F@N (left) and F@N0 (right)
 Table N
Accuracy of EXEM (NNN) on AwA, CUB, and SUN when  predicted exemplars are from original visual features (No PCA)  and PCA-projected features (PCA with d = N0NN and d = N00)
 Dataset No PCA PCA PCA  name d = N0NN d = N0NN d = N00  AwA NN.N NN.N NN.N  CUB NN.N NN.N NN.N  SUN NN.N NN.N NN.N  Table N
Comparison between EXEM (NNN) with support vector regressors (SVR) and with N-layer multi-layer perceptron (MLP) for  predicting visual exemplars
Results on CUB are for the first split
 Each number for MLP is an average over N random initialization
 Dataset How to predict No PCA PCA PCA  name exemplars d = N0NN d = N0NN d = N00  AwA SVR NN.N NN.N NN.N  MLP NN.N ± 0.N NN.N ± 0.N NN.N ± N.N  CUB SVR NN.N NN.N NN.N  MLP NN.N ± 0.N NN.N ± 0.N NN.N ± 0.N  smaller PCA dimension leads to faster computation due to  fewer regressors to be trained
See additional results with  other values for d in the supplementary material
 Kernel regression vs
Multi-layer perceptron We compare two approaches for predicting visual exemplars:  kernel-based support vector regressors (SVR) and N-layer  multi-layer perceptron (MLP) with ReLU nonlinearity
 MLP weights are ℓN regularized, and we cross-validate the  regularization constant
Additional details are in the supplementary material
 Table N shows that SVR performs more robustly than  MLP
One explanation is that MLP is prone to overfitting due to the small training set size (the number of seen  classes) as well as the model selection challenge imposed  by ZSL scenarios
SVR also comes with other benefits; it is  more efficient and less susceptible to initialization
 N
Related Work  ZSL has been a popular research topic in both computer vision and machine learning
A general theme is to  make use of semantic representations such as attributes or  word vectors to relate visual features of the seen and unseen  classes, as summarized in [N]
 Our approach for predicting visual exemplars is inspired  by [NN, NN]
They predict an image’s semantic embedding  from its visual features and compare to unseen classes’ semantic embeddings
As mentioned in Sect
N.N, we perform  “inverse prediction”: given an unseen class’s semantic representation, we predict where the exemplar visual feature  vector for that class is in the semantic embedding space
 There has been a recent surge of interest in applying deep  learning models to generate images [NN, NN, NN]
Most of  these methods are based on probabilistic models (in order to  incorporate the statistics of natural images)
Unlike them,  our prediction is to purely deterministically predict visual  exemplars (features)
Note that, generating features directly  is likely easier and more effective than generating realistic  images first and then extracting visual features from them
 N
Discussion  We have proposed a novel ZSL model that is simple but  very effective
Unlike previous approaches, our method directly solves ZSL by predicting visual exemplars — cluster centers that characterize visual features of the unseen  classes of interest
This is made possible partly due to the  well separate cluster structure in the deep visual feature  space
We apply predicted exemplars to the task of zeroshot classification based on two views of these exemplars:  ideal semantic representations and prototypical data points
 Our approach achieves state-of-the-art performance on multiple standard benchmark datasets
Finally, we also analyze  our approach and compliment our empirical studies with an  extension of zero-shot to few-shot learning
 Acknowledgements This work is partially supported by USC Graduate Fellowship, NSF IIS-N0NNNNN, NNNNNNN,  NNNNNNN/NNNNN0N, NN0NN00, CCF-NNNNNNN, a Google Research Award, an Alfred
P
Sloan Research Fellowship  and ARO# WNNNNF-NN-N-0NNN and WNNNNF-NN-N-0NNN
 NNNN    References  [N] Z
Akata, F
Perronnin, Z
Harchaoui, and C
Schmid
Labelembedding for attribute-based classification
In CVPR, N0NN
 N, N, N  [N] Z
Akata, S
Reed, D
Walter, H
Lee, and B
Schiele
Evaluation of output embeddings for fine-grained image classification
In CVPR, N0NN
N, N, N  [N] S
Changpinyo, W.-L
Chao, B
Gong, and F
Sha
Synthesized classifiers for zero-shot learning
In CVPR, N0NN
N, N,  N, N, N, N, N  [N] W.-L
Chao, S
Changpinyo, B
Gong, and F
Sha
An empirical study and analysis of generalized zero-shot learning for  object recognition in the wild
In ECCV, N0NN
N, N, N  [N] C.-Y
Chen and K
Grauman
Inferring analogous attributes
 In CVPR, N0NN
N  [N] T
Chilimbi, Y
Suzue, J
Apacible, and K
Kalyanaraman
 Project Adam: Building an efficient and scalable deep learning training system
In OSDI, N0NN
N  [N] K
Crammer and Y
Singer
On the algorithmic implementation of multiclass kernel-based vector machines
JMLR,  N:NNN–NNN, N00N
N  [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
In  CVPR, N00N
N  [N] K
Duan, D
Parikh, D
Crandall, and K
Grauman
Discovering localized attributes for fine-grained recognition
In  CVPR, N0NN
N  [N0] M
Elhoseiny, B
Saleh, and A
Elgammal
Write a classifier: Zero-shot learning using purely textual descriptions
In  ICCV, N0NN
N  [NN] A
Farhadi, I
Endres, D
Hoiem, and D
Forsyth
Describing  objects by their attributes
In CVPR, N00N
N, N  [NN] A
Frome, G
S
Corrado, J
Shlens, S
Bengio, J
Dean,  M
Ranzato, and T
Mikolov
Devise: A deep visual-semantic  embedding model
In NIPS, N0NN
N, N, N, N  [NN] D
Jayaraman and K
Grauman
Zero-shot recognition with  unreliable attributes
In NIPS, N0NN
N, N  [NN] D
Jayaraman, F
Sha, and K
Grauman
Decorrelating semantic visual attributes by resisting the urge to share
In  CVPR, N0NN
N  [NN] N
Karessli, Z
Akata, A
Bulling, and B
Schiele
Gaze embeddings for zero-shot image classification
In CVPR, N0NN
 N  [NN] A
Kulesza and B
Taskar
k-dpps: Fixed-size determinantal  point processes
In ICML, N0NN
N  [NN] C
H
Lampert, H
Nickisch, and S
Harmeling
Learning to  detect unseen object classes by between-class attribute transfer
In CVPR, N00N
N, N  [NN] C
H
Lampert, H
Nickisch, and S
Harmeling
Attributebased classification for zero-shot visual object categorization
TPAMI, NN(N):NNN–NNN, N0NN
N  [NN] J
Lei Ba, K
Swersky, S
Fidler, and R
Salakhutdinov
Predicting deep zero-shot convolutional neural networks using  textual descriptions
In ICCV, N0NN
N  [N0] Y
Long, L
Liu, L
Shao, F
Shen, G
Ding, and J
Han
From  zero-shot learning to conventional supervised classification:  Unseen visual data synthesis
In CVPR, N0NN
N  [NN] Y
Lu
Unsupervised learning of neural network outputs:  with application in zero-shot learning
In IJCAI, N0NN
N,  N, N, N, N, N  [NN] E
Mansimov, E
Parisotto, J
L
Ba, and R
Salakhutdinov
 Generating images from captions with attention
In ICLR,  N0NN
N  [NN] T
Mensink, E
Gavves, and C
G
Snoek
Costa: Cooccurrence statistics for zero-shot classification
In CVPR,  N0NN
N  [NN] T
Mensink, J
Verbeek, F
Perronnin, and G
Csurka
 Distance-based image classification: Generalizing to new  classes at near-zero cost
TPAMI, NN(NN):NNNN–NNNN, N0NN
 N, N  [NN] T
Mikolov, K
Chen, G
S
Corrado, and J
Dean
Efficient  estimation of word representations in vector space
In ICLR  Workshops, N0NN
N  [NN] G
A
Miller
Wordnet: a lexical database for english
Communications of the ACM, NN(NN):NN–NN, NNNN
N  [NN] M
Norouzi, T
Mikolov, S
Bengio, Y
Singer, J
Shlens,  A
Frome, G
S
Corrado, and J
Dean
Zero-shot learning  by convex combination of semantic embeddings
In ICLR,  N0NN
N, N, N, N, N, N, N, N  [NN] M
Palatucci, D
Pomerleau, G
E
Hinton, and T
M
 Mitchell
Zero-shot learning with semantic output codes
In  NIPS, N00N
N, N  [NN] D
Parikh and K
Grauman
Interactively building a discriminative vocabulary of nameable attributes
In CVPR, N0NN
 N  [N0] G
Patterson, C
Xu, H
Su, and J
Hays
The sun attribute  database: Beyond categories for deeper scene understanding
 IJCV, N0N(N-N):NN–NN, N0NN
N  [NN] S.-A
Rebuffi, A
Kolesnikov, and C
H
Lampert
iCaRL:  Incremental classifier and representation learning
In CVPR,  N0NN
N  [NN] S
Reed, Z
Akata, H
Lee, and B
Schiele
Learning deep  representations of fine-grained visual descriptions
In CVPR,  N0NN
N  [NN] S
Reed, Z
Akata, X
Yan, L
Logeswaran, H
Lee, and  B
Schiele
Generative adversarial text to image synthesis
 In ICML, N0NN
N  [NN] M
Ristin, M
Guillaumin, J
Gall, and L
Van Gool
Incremental learning of random forests for large-scale image  classification
TPAMI, NN(N):NN0–N0N, N0NN
N  [NN] B
Romera-Paredes and P
H
S
Torr
An embarrassingly  simple approach to zero-shot learning
In ICML, N0NN
N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual  Recognition Challenge
IJCV, N0NN
N  [NN] R
Salakhutdinov, A
Torralba, and J
Tenenbaum
Learning  to share visual appearance for multiclass object detection
In  CVPR, N0NN
N  [NN] B
Schölkopf, A
J
Smola, R
C
Williamson, and P
L
 Bartlett
New support vector algorithms
Neural computation, NN(N):NN0N–NNNN, N000
N  [NN] R
Socher, M
Ganjoo, C
D
Manning, and A
Ng
Zero-shot  learning through cross-modal transfer
In NIPS, N0NN
N, N  NNNN    [N0] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
N, N  [NN] L
Van der Maaten and G
Hinton
Visualizing data using  t-sne
JMLR, N(NNNN-NN0N):NN, N00N
N, N  [NN] C
Wah, S
Branson, P
Welinder, P
Perona, and S
Belongie
 The Caltech-UCSD Birds-N00-N0NN Dataset
Technical Report CNS-TR-N0NN-00N, California Institute of Technology,  N0NN
N  [NN] Q
Wang and K
Chen
Zero-shot visual recognition via bidirectional latent embedding
arXiv preprint  arXiv:NN0N.0NN0N, N0NN
N, N, N  [NN] Y
Xian, Z
Akata, and B
Schiele
Zero-shot learning – the  Good, the Bad and the Ugly
In CVPR, N0NN
N, N, N  [NN] Y
Xian, Z
Akata, G
Sharma, Q
Nguyen, M
Hein, and  B
Schiele
Latent embeddings for zero-shot classification
 In CVPR, N0NN
N, N, N, N  [NN] X
Yan, J
Yang, K
Sohn, and H
Lee
AttributeNimage: Conditional image generation from visual attributes
In ECCV,  N0NN
N  [NN] F
X
Yu, L
Cao, R
S
Feris, J
R
Smith, and S.-F
Chang
 Designing category-level attributes for discriminative visual  recognition
In CVPR, N0NN
N, N  [NN] L
Zhang, T
Xiang, and S
Gong
Learning a deep embedding model for zero-shot learning
In CVPR, N0NN
N, N  [NN] Z
Zhang and V
Saligrama
Zero-shot learning via semantic  similarity embedding
In ICCV, N0NN
N, N, N  [N0] Z
Zhang and V
Saligrama
Zero-shot learning via joint latent similarity embedding
In CVPR, N0NN
N, N  [NN] X
Zhu, D
Anguelov, and D
Ramanan
Capturing long-tail  distributions of object subcategories
In CVPR, N0NN
N  NNNNIncremental Learning of Object Detectors Without Catastrophic Forgetting   Incremental Learning of Object Detectors without Catastrophic Forgetting  Konstantin Shmelkov Cordelia Schmid  Inria∗  Karteek Alahari  Abstract  Despite their success for object detection, convolutional  neural networks are ill-equipped for incremental learning,  i.e., adapting the original model trained on a set of classes  to additionally detect objects of new classes, in the absence of the initial training data
They suffer from “catastrophic forgetting”—an abrupt degradation of performance  on the original set of classes, when the training objective  is adapted to the new classes
We present a method to address this issue, and learn object detectors incrementally,  when neither the original training data nor annotations for  the original classes in the new training set are available
 The core of our proposed solution is a loss function to balance the interplay between predictions on the new classes  and a new distillation loss which minimizes the discrepancy between responses for old classes from the original  and the updated networks
This incremental learning can  be performed multiple times, for a new set of classes in  each step, with a moderate drop in performance compared  to the baseline network trained on the ensemble of data
We  present object detection results on the PASCAL VOC N00N  and COCO datasets, along with a detailed empirical analysis of the approach
 N
Introduction  Modern detection methods, such as [N,NN], based on convolutional neural networks (CNNs) have achieved state-ofthe-art results on benchmarks such as PASCAL VOC [N0]  and COCO [NN]
This, however, comes with a high training time to learn the models
Furthermore, in an era where  datasets are evolving regularly, with new classes and samples, it is necessary to develop incremental learning methods
A popular way to mitigate this is to use CNNs pretrained on a certain dataset for a task, and adapt them to  new datasets or tasks, rather than train the entire network  from scratch
 Fine-tuning [NN] is one approach to adapt a network to  new data or tasks
Here, the output layer of the original network is adjusted, either by replacing it with classes corre∗Univ
Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, NN000  Grenoble, France
 Figure N
Catastrophic forgetting
An object detector network originally trained for three classes, including person, detects the rider  (top)
When the network is retrained with images of the new class  horse, it detects the horse in the test image, but fails to localize the  rider (bottom)
 sponding to the new task, or by adding new classes to the existing ones
The weights in this layer are then randomly initialized, and all the parameters of the network are tuned with  the objective for the new task
While this framework is very  successful on the new classes, its performance on the old  ones suffers dramatically, if the network is not trained on all  the classes jointly
This issue, where a neural network forgets previously learned knowledge when adapted to a new  task, is referred to as catastrophic interference or forgetting
 It has been known for over a couple of decades in the context of feedforward fully connected networks [NN, N0], and  needs to be addressed in the current state-of-the-art object  detector networks, if we want to do incremental learning
 Consider the example in Figure N
It illustrates catastrophic forgetting when incrementally adding a class, horse  in this object detection example
The first CNN (top) is  trained on three classes, including person, and localizes the  rider in the image
The second CNN (bottom) is an incrementally trained version of the first one for the category  horse
In other words, the original network is adapted with  images from only this new class
This adapted network localizes the horse in the image, but fails to detect the rider,  which it was capable of originally, and despite the fact that  the person class was not updated
In this paper, we present  a method to alleviate this issue
 NN00    Using only the training samples for the new classes, we  propose a method for not only adapting the old network to  the new classes, but also ensuring performance on the old  classes does not degrade
The core of our approach is a loss  function balancing the interplay between predictions on the  new classes, i.e., cross-entropy loss, and a new distillation  loss which minimizes the discrepancy between responses  for old classes from the original and the new networks
The  overall approach is illustrated in Figure N
 We use a frozen copy of the original detection network  to compute the distillation loss
This loss is related to the  concept of “knowledge distillation” proposed in [NN], but  our application of it is significantly different from this previous work, as discussed in Section N.N
We specifically  target the problem of object detection, which has the additional challenge of localizing objects with bounding boxes,  unlike other attempts [NN,NN] limited to the image classification task
We demonstrate experimental results on the PASCAL VOC and COCO datasets using Fast R-CNN [NN] as  the network
Our results show that we can add new classes  incrementally to an existing network without forgetting the  original classes, and with no access to the original training  data
We also evaluate variants of our method empirically,  and show the influence of distillation and the loss function
 Note that our framework is general and can be applied to  any other CNN-based object detectors where proposals are  computed externally, or static sliding windows are used
 N
Related work  The problem of incremental learning has a long history  in machine learning and artificial intelligence [N,NN,NN,NN]
 Some of the more recent work, e.g., [N, N], focuses on continuously updating the training set with data acquired from  the Internet
They are: (i) restricted to learning with a fixed  data representation [N], or (ii) keep all the collected data to  retrain the model [N]
Other work partially addresses these  issues by learning classifiers without access to the ensemble of data [NN, NN], but uses a fixed image representation
 Unlike these methods, our approach is aimed at learning the  representation and classifiers jointly, without storing all the  training examples
To this end, we use neural networks to  model the task in an end-to-end fashion
 Our work is also topically related to transfer learning  and domain adaptation methods
Transfer learning uses  knowledge acquired from one task to help learn another
 Domain adaptation transfers the knowledge acquired for  a task from a data distribution to other (but related) data
 These paradigms, and in particular fine-tuning, a special  case of transfer learning, are very popular in computer vision
CNNs learned for image classification [NN] are often used to train other vision tasks such as object detection [NN, N0] and semantic segmentation [N]
 An alternative to transfer knowledge from one network to another is distillation [N, NN]
This was originally  proposed to transfer knowledge between different neural  networks—from a large network to a smaller one for efficient deployment
The method in [NN] encouraged the large  (old) and the small (new) networks to produce similar responses
It has found several applications in domain adaptation and model compression [NN, NN, NN]
Overall, transfer learning and domain adaptation methods require at least  unlabeled data for both the tasks or domains, and in its absence, the new network quickly forgets all the knowledge  acquired in the source domain [NN, NN, NN, N0]
In contrast,  our approach addresses the challenging case where no training data is available for the original task (i.e., detecting objects belonging to the original classes), by building on the  concept of knowledge distillation [NN]
 This phenomenon of forgetting is believed to be caused  by two factors [NN, NN]
First, the internal representations  in hidden layers are often overlapping, and a small change  in a single neuron can affect multiple representations at the  same time [NN]
Second, all the parameters in feedforward  networks are involved in computations for every data point,  and a backpropagation update affects all of them in each  training step [NN]
The problem of addressing these issues  in neural networks has its origin in classical connectionist  networks several years ago [N, NN–NN, NN], but needs to be  adapted to today’s large deep neural network architectures  for vision tasks [NN, NN]
 Li and Hoiem [NN] use knowledge distillation for one of  the classical vision tasks, image classification, formulated  in a deep learning framework
However, their evaluation is  limited to the case where the old network is trained on a  dataset, while the new network is trained on a different one,  e.g., PlacesNNN for the old and PASCAL VOC for the new,  ImageNet for the old and PASCAL VOC for the new, etc
 While this is interesting, it is a simpler task, because: (i)  different datasets often contain dissimilar classes, (ii) there  is little confusion between datasets—it is in fact possible to  identify a dataset simply from an image [NN]
 Our method is significantly different from [NN] in two  ways
First, we deal with the more difficult problem of  learning incrementally on the same dataset, i.e., the addition of classes to the network
As shown in [NN], [NN] fails  in a similar setting of learning image classifiers incrementally
Second, we address the object detection task, where it  is very common for the old and the new classes to co-occur,  unlike the classification task
 Very recently, Rebuffi et al
[NN] address some of the  drawbacks in [NN] with their incremental learning approach  for image classification
They also use knowledge distillation, but decouple the classifier and the representation learning
Additionally, they rely on a subset of the original training data to preserve the performance on the old classes
In  comparison, our approach is an end-to-end learning frameNN0N    Figure N
Overview of our framework for learning object detectors incrementally
It is composed of a frozen copy of the detector (Network  A) and the detector (Network B) adapted for the new class(es)
See text for details
 work, where the representation and the classifier are learned  jointly, and we do not use any of the original training samples to avoid catastrophic forgetting
Alternatives to distillation are: growing the capacity of the network with new layers [NN], applying strong per-parameter regularization selectively [N0]
The downside to these methods is the rapid  increase in the number of new parameters to be learned [NN],  and their limited evaluation on the easier task of image classification [N0]
 In summary, none of the previous work addresses the  problem of learning classifiers for object detection incrementally, without using previously seen training samples
 N
Incremental learning of new classes  Our overall approach for incremental learning of a CNN  model for object detection is illustrated in Figure N
It contains a frozen copy of the original detector (denoted by Network A in the figure), which is used to: (i) select proposals  corresponding to the old classes, i.e., distillation proposals,  and (ii) compute the distillation loss
Network B in the figure is the adapted network for the new classes
It is obtained  by increasing the number of outputs in the last layer of the  original network, such that the new output layer includes  the old as well as the new classes
 In order to avoid catastrophic forgetting, we constrain  the learning process of the adapted network
We achieve  this by incorporating a distillation loss, to preserve the performance on the old classes, as an additional term in the  standard cross-entropy loss function (see §N.N)
Specifi- cally, we evaluate each new training sample on the frozen  copy (Network A) to choose a diverse set of proposals (distillation proposals in Figure N), and record their responses
 With these responses in hand, we compute a distillation loss  which measures the discrepancy between the two networks  for the distillation proposals
This loss is added to the crossentropy loss on the new classes to make up the loss function  for training the adapted detection network
As we show in  the experimental evaluation, the distillation loss as well as  the strategy to select the distillation proposals are critical in  preserving the performance on the old classes (see §N)
 In the remainder of this section, we provide details of  the object detector network (§N.N), the loss functions and the learning algorithm (§N.N), and strategies to sample the object proposals (§N.N)
 N.N
Object detection network  We use a variant of a popular framework for object  detection—Fast R-CNN [NN], which is a proposal-based  detection method built with pre-computed object proposals, e.g., [N, NN]
We chose this instead of the more recent Faster R-CNN [NN], which integrates the computation  of category-specific proposals into the network, because we  need proposals agnostic to object categories, such as EdgeBoxes [NN], MCG [N]
We use EdgeBoxes [NN] proposals  for PASCAL VOC N00N and MCG [N] for COCO
This allows us to focus on the problem of learning the representation and the classifier, given a pre-computed set of generic  object proposals
 In our variant of Fast R-CNN, we replaced the VGG-NN  trunk with a deeper ResNet-N0 [NN] component, which is  faster and more accurate than VGG-NN
We follow the suggestions in [NN] to combine Fast R-CNN and ResNet architectures
The network processes the whole image through a  sequence of residual blocks
Before the last strided convolution layer we insert a RoI pooling layer, which performs  maxpooling over regions of varied sizes, i.e., proposals, into  a N × N feature map
Then we add the remaining residual blocks, a layer for average pooling over spatial dimensions,  and two fully connected layers: a softmax layer for classification (PASCAL or COCO classes, for example, along with  the background class) and a regression layer for bounding  box refinement, with independent corrections for each class
 The input to the network is an image and about N000 precomputed object proposals represented as bounding boxes
 NN0N    During inference, the high-scoring proposals are refined according to bounding box regression
Then, a per-category  non-maxima suppression (NMS) is performed to get the final detection results
The loss function to train the Fast RCNN detector, corresponding to a RoI, is given by:  Lrcnn(p, k ∗, t, t∗) = − logpk∗ + [k  ∗ ≥ N]R(t− t∗), (N) where p is the set of responses of the network for all the  classes (i.e., softmax output), k∗ is a groundtruth class, t  is an output of bounding box refinement layer, and t∗ is the ground truth bounding box proposal
The first part of  the loss denotes log-loss over classes, and the second part  is localization loss
For more implementation details about  Fast R-CNN, refer to the original paper [NN]
 N.N
Dual-network learning  First, we train a Fast R-CNN to detect the original set of  classes CA
We refer to this network as A(CA)
The goal now is to add a new set of classes CB to this
We make two  copies of A(CA): one that is frozen to recognize classes CA through distillation loss, and the second B(CB) that is extended to detect the new classes CB , which were not  present or at least not annotated in the source images
The  extension is done only in the last fully connected layers, i.e.,  classification and bounding box regression
We create sibling (i.e., fully-connected) layers [NN] for new classes only  and concatenate their outputs with the original ones
The  new layers are initialized randomly in the same way as the  corresponding layers in Fast R-CNN
Our goal is to train  B(CB) to recognize classes CA ∪ CB using only new data and annotations for CB 
 The distillation loss represents the idea of “keeping all  the answers of the network the same or as close as possible”
If we train B(CB) without distillation, average preci- sion on the old classes will degrade quickly, after a few hundred SGD iterations
This is a manifestation of catastrophic  forgetting
We illustrate this in Sections N.N and N.N
We  compute the distillation loss by applying the frozen copy of  A(CA) to any new image
Even if no object is detected by A(CA), the unnormalized logits (softmax input) carry enough information to “distill” the knowledge of the old  classes from A(CA) to B(CB)
This process is illustrated in Figure N
 For each image we randomly sample NN RoIs out of NNN  with the smallest background score
The logits computed  for these RoIs by A(CA) serve as targets for the old classes in the LN distillation loss shown below
The logits for the  new classes CB are not considered in this loss
We subtract the mean over the class dimension from these unnormalized logits (y) of each RoI to obtain the corresponding  centered logits ȳ used in the distillation loss
Bounding box  regression outputs tA (of the same set of proposals used for  computing the logit loss) also constrain the loss of the network B(CB)
We chose to use LN loss instead of a cross- entropy loss for regression outputs because it demonstrates  more stable training and performs better (see §N.N)
The dis- tillation loss combining the logits and regression outputs is  written as:  Ldist(yA, tA, yB , tB) = N  N |CA|  ∑  [  (ȳA − ȳB) N  + (tA − tB) N  ]  ,  (N)  where N is the number of RoIs sampled for distillation (i.e.,  NN in this case), |CA| is the number of old classes, and the sum is over all the RoIs for the old classes
We distill logits without any smoothing, unlike [NN], because most of the  proposals already produce a smooth distribution of scores
 Moreover, in our case, both the old and the new networks  are similar with almost the same parameters (in the beginning), and so smoothing the logits distribution is not necessary to stabilize the learning
 The values of the bounding box regression are also distilled because we update all the layers, and any update of  the convolutional layers will affect them indirectly
As  box refinements are important to detect objects accurately,  their values should be conserved as well
This is an easier  task than keeping the classification scores because bounding box refinements for each class are independent, and are  not linked by the softmax
 The overall loss L to train the model incrementally is a weighted sum of the distillation loss (N), and the standard  Fast R-CNN loss (N) that is applied only to new classes CB ,  where groundtruth bounding box annotation is available
In  essence, L = Lrcnn + λLdist, (N)  where the hyperparameter λ balances the two losses
We  set λ to N in all the experiments with cross-validation (see  §N.N)
The interplay between the two networks A(CA) and  B(CB) provides the necessary supervision that prevents the catastrophic forgetting in the absence of original training data used by A(CA)
After the training of B(CB) is completed, we can add more classes by freezing the newly  trained network and using it for distillation
We can thus  add new classes sequentially
Since B(CB) is structurally identical to A(CA ∪ CB), the extension can be repeated to add more classes
 N.N
Sampling strategy  As mentioned before, we choose NN proposals out of NNN  with the lowest background score, thus biasing the distillation to non-background proposals
We noticed that proposals recognized as confident background do not provide  strong learning cues to conserve the original classes
One  possibility is using an unbiased distillation that randomly  samples NN proposals out of the whole set of N000 proposals
However, when doing so, the detection performance on  old classes is noticeably worse because most of the distillation proposals are now background, and carry no strong  NN0N    method old new all  A(N-NN) NN.N - +B(N0) w/o distillation NN.0 NN.N NN.N  +B(N0) w frozen trunk NN.N NN.N NN.N  +B(N0) w all layers frozen NN.N NN.N NN.N  +B(N0) w frozen trunk and distill
NN.N NN.N NN.N  +B(N0) w distillation NN.N NN.N NN.N  +B(N0) w cross-entropy distill
NN.N NN.0 NN.N  +B(N0) w/o bbox distillation NN.N NN.N NN.N  A(N-N0) NN.N NN.N NN.N Table N
VOC N00N test average precision (%)
Experiments  demonstrating the addition of “tvmonitor” class to a pretrained  network under various setups
Classes N-NN are the old classes,  and “tvmonitor” (class N0) is the new one
 signal about the object categories
Therefore, it is advantageous to select non-background proposals
We demonstrate  this empirically in Section N.N
 N
Experiments  N.N
Datasets and evaluation  We evaluate our method on the PASCAL VOC N00N  detection benchmark and the Microsoft COCO challenge  dataset
VOC N00N consists of NK images in the trainval  split and NK images in the test split for N0 object classes
 COCO on the other hand has N0K images in the training set  and N0K images in the validation set for N0 object classes  (which includes all the classes from VOC)
We use the standard mean average precision (mAP) at 0.N IoU threshold as  the evaluation metric
We also report mAP weighted across  different IoU from 0.N to 0.NN on COCO, as recommended in the COCO challenge guidelines
Evaluation of the VOC  N00N experiments is done on the test split, while for COCO,  we use the first N000 images from the validation set
 N.N
Implementation details  We use SGD with Nesterov momentum [NN] to train the  network in all the experiments
We set the learning rate to  0.00N, decay to 0.000N after N0K iterations, and momentum to 0.N
In the second stage of training, i.e., learning  the extended network with new classes, we used a learning  rate of 0.000N
The A(CA) network is trained for N0K it- erations on PASCAL VOC N00N and for N00K iterations on  COCO
The B(CB) network is trained for NK-NK iterations when only one class is added, and for the same number of  iterations as A(CA) when many classes are added at once
Following Fast R-CNN [NN], we regularize with weight decay of 0.0000N and take batches of two images each
All the  layers of A(CA) and B(CB) networks are finetuned unless stated otherwise
 The integration of ResNet into Fast R-CNN (see §N.N) is done by adding a RoI pooling layer before the convN N  layer, and replacing the final classification layer by two sibling fully connected layers
The batch normalization layers  are frozen, and as in Fast R-CNN, no dropout is used
RoIs  are considered as detections if they have a score more than  0.N for any of the classes
We apply per-class NMS with an IoU threshold of 0.N
Training is image-centric, and a batch is composed of NN proposals per image, with NN of  them having an IoU of at least 0.N with a groundtruth ob- ject
All the proposals are filtered to have IoU less than 0.N, as in [NN]
 We use TensorFlow [N] to develop our incremental learning framework
Each experiment begins with choosing a  subset of classes to form the set CA
Then, a network is  learned only on the subset of the training set composed of  all the images containing at least one object from CA
Annotations for other classes in these images are ignored
With  the new classes chosen to form the set CB , we learn the extended network as described in Section N.N with the subset  of the training set containing at least one object from CB 
 As in the previous case, annotations of all the other classes,  including those of the original classes CA, are ignored
For  computational efficiency, we precomputed the responses of  the frozen network A(CA) on the training data (as every image is typically used multiple times)
 N.N
Addition of one class  In the first experiment we take NN classes in alphabetical order from the VOC dataset as CA, and the remaining  one as the only new class CB 
We then train the A(N-NN)  network on the VOC trainval subset containing any of the  NN classes, and the B(N0) network is trained on the trainval  subset containing the new class
A summary of the evaluation of these networks on the VOC test set is shown in  Table N, with the full results in Table N
 A baseline approach for addition of a new class is to add  an output to the last layer and freeze the rest of the network
 This freezing, where the weights of the network’s convolutional layers are fixed (“B(N0) w frozen trunk” in the tables), results in a lower performance on the new class as the  previously learned representations have not been adapted  for it
Furthermore, it does not prevent degradation of the  performance on the old classes, where mAP drops by almost  NN%
When we freeze all the layers, including the old output layer (“B(N0) w all layers frozen”), or apply distillation  loss (“B(N0) w frozen trunk and distill.”), the performance  on the old classes is maintained, but that on the new class is  poor
This shows that finetuning of convolutional layers is  necessary to learn the new classes
 When the network B(N0) is trained without the distillation loss (“B(N0) w/o distillation” in the tables), it can  learn the N0th class, but the performance decreases significantly on the other (old) classes
As seen in Table N, the  AP on classes like “cat”, “person” drops by over N0%
The  same training procedure with distillation loss largely alleviates this catastrophic forgetting
Without distillation, the  new network has NN.0% mAP on the old classes compared  NN0N    method old new all  A(N-N0) NN.N - +B(NN-N0) w/o distillation NN.N NN.N NN.N  +B(NN-N0) w distillation NN.N NN.N NN.N  +B(NN-N0) w/o bbox distillation NN.N NN.N N0.N  +B(NN-N0) w EWC [N0] NN.N NN.0 NN.N  A(N-N0) NN.N NN.N NN.N Table N
VOC N00N test average precision (%)
Experiments  demonstrating the addition of N0 classes, all at once, to a pretrained  network
Classes N-N0 are the old classes, and NN-N0 the new ones
 to NN.N% with distillation, and NN.N% mAP of baseline Fast  R-CNN trained jointly on all classes (“A(N-N0)”)
With distillation the performance is similar to that of the old network  A(N-NN), but is lower for certain classes, e.g., “bottle”
The  N0th class “tvmonitor” does not get the full performance of  the baseline (NN.N%), with or without distillation, and is less  than N0%
This is potentially due to the size of the training  set
The B(N0) network is trained only a few hundred im- ages containing instances of this class
Thus, the “tvmonitor” classifier does not see the full diversity of negatives
 We also performed the “addition of one class” experiment with each of the VOC categories being the new class
 The behavior for each class is very similar to the “tvmonitor” case described above
The mAP varies from NN.N%  (for new class “sheep”) to NN.N% (“tvmonitor”) with mean  NN.NN% and standard deviation of 0.N%
 N.N
Addition of multiple classes  In this scenario we train the network A(N-N0) on the first  N0 VOC classes (in alphabetical order) with the VOC trainval subset corresponding to these classes
In the second  stage of training we used the remaining N0 classes as CB and trained only on the images containing the new classes
 Table N shows a summary of the evaluation of these networks on the VOC test set, with the full results in Table N
 Training the network B(NN-N0) on the N0 new classes  with distillation (for the old classes) achieves NN.N% mAP  (“B(NN-N0) w distillation” in the tables) compared to NN.N%  of the baseline network trained on all the N0 classes (“A(NN0)”)
Just as in the previous experiment of adding one  class, performance on the new classes is slightly worse than  with the joint training of all the classes
For example, as  seen in Table N, the performance for “person” is NN.N%  vs NN.N%, and NN.N% vs NN.N% for the “train” class
The  mAP on new classes is NN.N% for the network with distillation versus NN.N% for the jointly trained model
However,  without distillation, the network achieves only NN.N% mAP  (“+B(NN-N0) w/o distillation”) on the old classes
Note  that the method without bounding box distillation (“+B(NNN0) w/o bbox distillation”) is inferior to our full method  (“+B(NN-N0) w distillation”)
 We also performed the N0-class experiment for different  values of λ in (N), the hyperparameter controlling the relmethod old new all  A(N-NN) N0.N - +B(NN-N0) w distill
NN.N NN.N NN.N  +B(NN)(NN)...(N0) w distill
NN.0 NN.N NN.N  +B(NN)(NN)...(N0) w unbiased distill
NN.N NN.N NN.0  +A(NN)+...+A(N0) N0.N NN.N NN.N  A(N-N0) N0.N NN.N NN.N Table N
VOC N00N test average precision (%)
Experiments  demonstrating the addition of N classes, all at once, and incrementally to a pretrained network
Classes N-NN are the old ones, and  NN-N0 the new classes
 method mAP@.N mAP@[.N, .NN]  A(N-N0)+B(NN-N0) NN.N NN.N  A(N-N0) NN.N NN.N  Table N
COCO minival (first N000 validation images) average  precision (%)
We compare the model learned incrementally on  half the classes with the baseline trained on all jointly
 ative importance of distillation and Fast R-CNN loss
Results shown in Figure N demonstrate that when the distillation is weak (λ = 0.N) the new classes are easier to learn, but the old ones are more easily forgotten
When distillation is strong (λ = N0), it destabilizes training and impedes learning the new classes
Setting λ to N is a good trade-off between learning new classes and preventing catastrophic  forgetting
 We also compare our approach with elastic weight consolidation (EWC) [N0], which is an alternative to distillation and applies per-parameter regularization selectively to  alleviate catastrophic forgetting
We reimplemented EWC  and verified that it produces results comparable to those reported in [N0] on MNIST, and then adapted it to our object  detection task
We do this by using the Fast R-CNN batches  during the training phase (as done in Section N.N), and by  replacing log loss with the Fast R-CNN loss
Our approach  outperforms EWC for this case, when we add N0 classes at  once, as shown in Tables N and N
 We evaluated the influence of the number of new classes  in incremental learning
To this end, we learn a network for  NN classes first, and then train for the remaining N classes, all  added at once on VOC
These results are summarized in Table N, with the per-class results shown in Table N
The network B(NN-N0) has better overall performance than B(NNN0): NN.N% mAP versus NN.N% mAP
As in the experiment  with N0 classes, the performance is lower for a few classes,  e.g., “table”, “horse”, for example, than the initial model  A(N-NN)
The performance on the new classes is lower  than jointly trained baseline Fast R-CNN A(N-N0)
Overall, mAP of B(NN-N0) is lower than baseline Fast R-CNN  (NN.N% versus NN.N%)
 The evaluation on COCO, shown in Table N, is done with  the first N0 classes in the initial set, and the remaining N0 in  the new second stage
The network B(NN-N0) trained with  NN0N    N0-N N00 N0N  lambda  N0  NN  NN  NN  NN  N0  NN  NN  NN m A P  old classes  new classes  all classes  Figure N
The influence of λ, in the loss function (N), on the mAP performance for the B(NN-N0) network trained with distillation
 the distillation loss obtains NN.N% mAP in the PASCALstyle metric and NN% mAP in the COCO-style metric
The  baseline network trained on N0 classes is similar in performance with NN.N% and NN.N% mAP respectively
We observe that our proposed method overcomes catastrophic forgetting, just as in the case of VOC seen earlier
 We also studied if distillation depends on the distribution of images used in this loss
To this end, we used the  model A(N-N0) trained on VOC, and then performed the  second stage learning in two settings: B(NN-N0) learned on  the subset of VOC as before, and another model trained for  the same set of classes, but using a subset of COCO
From  Table N we see that indeed, distillation works better when  background samples have exactly the same distribution in  both stages of training
However, it is still very effective  even when the dataset in the second stage is different from  the one used in the first
 N.N
Sequential addition of multiple classes  In order to evaluate incremental learning of classes  added sequentially, we update the frozen copy of the network with the one learned with the new class, and then  repeat the process with another new class
For example,  we take a network learned for NN classes of VOC, train  it for the NNth on the subset containing only this class,  and then use the NN-class network as the frozen copy to  then learn the NNth class
This is then continued until  the N0th class
We denote this incremental extension as  B(NN)(NN)(NN)(NN)(N0)
 Results of adding classes sequentially are shown in Tables N and N
After adding the N classes we obtain NN.N%  mAP (row N in Table N), which is lower than NN.N% obtained  by adding all the N classes at once (row N)
Table N shows intermediate evaluations after adding each class
We observe  that the performance of the original classes remains stable at  each step in most cases, but for a few classes, which is not  recovered in the following steps
We empirically evaluate  the importance of using biased non-background proposals  (cf
§N.N)
Here we add the N classes one by one, but use un- biased distillation (“B(NN)(NN)(NN)(NN)(N0) w unbiased dis+COCO-N0cls +VOC-N0cls  mAP (old classes) NN.N NN.N  mAP (new classes) NN.N NN.N  mAP (all classes) NN.N NN.N Table N
VOC N00N test average precision (%)
The second stage  of training, where N0 classes (NN-N0th) are added, is done on the  subset of COCO images (+COCO-N0cls), and is compared to the  one trained on the VOC subset (+VOC-N0cls)
 till.” in Tables N and N), i.e., randomly sampled proposals  are used for distillation
This results in much worse overall  performance (NN% vs NN.N%) and some classes (“person”,  “chair”) suffer from a significant performance drop of N0N0%
We also performed sequential addition experiment  with N0 classes, and present the results in Table N0
Although the drop in mAP is more significant than for the previous experiment with N classes, it is far from catastrophic  forgetting
 N.N
Other alternatives  Learning multiple networks
Another solution for learning multiple classes is to train a new network for each class,  and then combine their detections
This is an expensive  strategy at test time, as each network has to be run independently, including the extraction of features
This may seem  like a reasonable thing to do as evaluation of object detection is done independently for each class, However, learning  is usually not independent
Although we can learn a decent  detection network for N0 classes, it is much more difficult  when learning single classes independently
To demonstrate  this, we trained a network for N-NN classes and then separate networks for each of the NN-N0 classes
This results in  N networks in total (row “+A(NN)+...+A(N0)” in Table N),  compared to incremental learning of N classes implemented  with a single network (“+B(NN)(NN)...(N0) w distill.”)
The  results confirm that new classes are difficult to learn in isolation
 Varying distillation loss
As noted in [NN], knowledge  distillation can also be expressed as a cross-entropy loss
 We compared this with LN-based loss on the one class extension experiment (“B(N0) w cross-entropy distill.” in Ta- bles N and N)
Cross-entropy distillation works as well as  LN distillation keeping old classes intact (NN.N% vs NN.N%),  but performs worse than LN on the new class “tvmonitor”  (NN% vs NN.N%)
We also observed that cross-entropy is  more sensitive to the training schedule
According to [NN],  both formulations should be equivalent in the limit of a high  smoothing factor for logits (cf
§N.N), but our choice of not smoothing leads to this different behavior
 Bounding box regression distillation
Addition of N0  classes (Table N) without distilling bounding box regression values performs consistently worse than the full distillation loss
Overall B(NN-N0) without distilling bounding  NN0N    method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP  A(N-NN) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N N0.N N0.N NN.N NN.N NN.N - NN.N  +B(N0) w/o distillation NN.N NN.N NN.N NN.N N.N NN.N NN.N NN.N N.N NN.N N.N NN.N N0.0 NN.N N.N N.N NN.N N.N NN.0 NN.N NN.N  +B(N0) w frozen trunk NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N N.N NN.N NN.N NN.N N0.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  +B(N0) w all layers frozen NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N N0.N NN.N NN.N N0.N NN.0 NN.N NN.N NN.N NN.N  +B(N0) w frozen trunk and distill
NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  +B(N0) w distillation N0.N NN.N NN.N NN.N N0.N NN.N NN.N N0.N NN.0 NN.N NN.N NN.N N0.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N  +B(N0) w cross-entropy distill
NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.0 NN.N  +B(N0) w/o bbox distillation NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  A(N-N0) N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
VOC N00N test per-class average precision (%) under different settings when the “tvmonitor” class is added
 method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP  A(N-N0) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N - - - - - - - - - - NN.N  +B(NN-N0) w/o distillation NN.N N.N NN.N NN.N N.N N.N N.N NN.N 0.0 N.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  +B(NN-N0) w distillation NN.N N0.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  +B(NN-N0) w/o bbox distillation NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N  +B(NN-N0) w EWC [N0] NN.N NN.N NN.N N0.N NN.N NN.N NN.0 NN.N N.N N.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  A(N-N0) N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
VOC N00N test per-class average precision (%) under different settings when N0 classes are added at once
 method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP  A(N-NN) N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.0 NN.N N0.N NN.N NN.N - - - - - N0.N  +B(NN-N0) w distill
N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  +B(NN)(NN)(NN)(NN)(N0) w distill
N0.0 NN.N NN.0 N0.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N  +B(NN)(NN)(NN)(NN)(N0) w unbiased distill
NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N N.N NN.N NN.N NN.N NN.N N0.N N.N NN.N NN.N NN.N NN.N NN.N NN.0  A(N-N0) N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
VOC N00N test per-class average precision (%) under different settings when N classes are added at once or sequentially
 method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mAP  A(N-NN) N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.0 NN.N N0.N NN.N NN.N - - - - - N0.N  +B(NN) N0.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.0 N0.N NN.N N0.0 NN.N - - - - NN.0  +B(NN)(NN) N0.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.0 NN.N N0.N NN.N NN.N NN.N N0.N NN.N NN.N - - - NN.N  +B(NN)(NN)(NN) NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N - - NN.N  +B(NN)(NN)(NN)(NN) N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 - NN.N  +B(NN)(NN)(NN)(NN)(N0) N0.0 NN.N NN.0 N0.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N  A(N-N0) N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
VOC N00N test per-class average precision (%) when N classes are added sequentially
 method A(N-N0) +table +dog +horse +mbike +persn +plant +sheep +sofa +train +tv  mAP NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0  Table N0
VOC N00N test average precision (%) when adding N0 classes sequentially
Unlike other tables each column here shows the mAP  of a network trained on all the previous classes and the new class
For example, the mAP shown for “+dog” is the result of the network  trained on the first ten classes, “table”, and the new class “dog”
 box regression gets N0.N% vs NN.N% with the full distillation
However, on a few new classes the performance can be  higher than with the full distillation (Table N)
This is also  the case for B(N0) without bounding box distillation (Table N) that has better performance on “tvmonitor” (NN.N%  vs NN.N%)
This is not the case when other categories are  chosen as the new class
Indeed, bounding box distillation  shows an improvement of N% for the “sheep” class
 N
Conclusion  In this paper, we have presented an approach for incremental learning of object detectors for new classes, without  access to the training data corresponding to the old classes
 We address the problem of catastrophic forgetting in this  context, with a loss function that optimizes the performance  on the new classes, in addition to preserving the performance on the old classes
Our extensive experimental analysis demonstrates that our approach performs well, even in  the extreme case of adding new classes one by one
Part  of future work is adapting our method to learned proposals,  e.g., from RPN for Faster R-CNN [NN], by reformulating  RPN as a single class detector that works on sliding window proposals
This requires adding another term for RPNbased knowledge distillation in the loss function
 Acknowledgments
This work was supported in part by the  ERC advanced grant ALLEGRO, a Google research award,  and gifts from Facebook and Intel
We gratefully acknowledge NVIDIA’s support with the donation of GPUs used for  this work
 NN0N    References  [N] M
Abadi, A
Agarwal, P
Barham, E
Brevdo, Z
Chen,  C
Citro, G
S
Corrado, A
Davis, J
Dean, M
Devin, et al
 Tensorflow: Large-scale machine learning on heterogeneous  systems
tensorflow.org, N0NN
 [N] B
Ans, S
Rousset, R
M
French, and S
Musca
Selfrefreshing memory in artificial neural networks: Learning  temporal sequences without catastrophic forgetting
Connection Science, NN(N):NN–NN, N00N
 [N] P
Arbeláez, J
Pont-Tuset, J
T
Barron, F
Marques, and  J
Malik
Multiscale combinatorial grouping
In CVPR,  N0NN
 [N] S
Bell, C
L
Zitnick, K
Bala, and R
Girshick
Insideoutside net: Detecting objects in context with skip pooling  and recurrent neural networks
In CVPR, N0NN
 [N] C
Buciluǎ, R
Caruana, and A
Niculescu-Mizil
Model  compression
In KDD, N00N
 [N] G
Cauwenberghs and T
Poggio
Incremental and decremental support vector machine learning
In NIPS, N000
 [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected CRFs
In ICLR, N0NN
 [N] X
Chen, A
Shrivastava, and A
Gupta
NEIL: Extracting  visual knowledge from web data
In ICCV, N0NN
 [N] S
Divvala, A
Farhadi, and C
Guestrin
Learning everything  about anything: Webly-supervised visual concept learning
 In CVPR, N0NN
 [N0] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The PASCAL visual object classes (VOC)  challenge
IJCV, NN(N):N0N–NNN, N0N0
 [NN] R
M
French
Dynamically constraining connectionist networks to produce distributed, orthogonal representations to  reduce catastrophic interference
Cognitive Science Society  Conference, NNNN
 [NN] R
M
French
Catastrophic forgetting in connectionist networks
Trends in cognitive sciences, N(N):NNN–NNN, NNNN
 [NN] R
M
French, B
Ans, and S
Rousset
Pseudopatterns and  dual-network memory models: Advantages and shortcomings
In Connectionist models of learning, development and  evolution, pages NN–NN
N00N
 [NN] R
Girshick
Fast R-CNN
In ICCV, N0NN
 [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
 [NN] I
J
Goodfellow, M
Mirza, D
Xiao, A
Courville, and  Y
Bengio
An empirical investigation of catastrophic forgetting in gradient-based neural networks
In ICLR, N0NN
 [NN] S
Gupta, J
Hoffman, and J
Malik
Cross modal distillation  for supervision transfer
In CVPR, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
 [NN] G
Hinton, O
Vinyals, and J
Dean
Distilling the knowledge  in a neural network
In NIPS, N0NN
 [N0] J
Kirkpatrick, R
Pascanu, N
Rabinowitz, J
Veness, G
Desjardins, A
A
Rusu, K
Milan, J
Quan, T
Ramalho,  A
Grabska-Barwinska, D
Hassabis, C
Clopath, D
Kumaran, and R
Hadsell
Overcoming catastrophic forgetting  in neural networks
PNAS, N0NN
 [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
ImageNet  classification with deep convolutional neural networks
In  NIPS, N0NN
 [NN] Y
LeCun, B
Boser, J
S
Denker, D
Henderson, R
E
 Howard, W
Hubbard, and L
D
Jackel
Backpropagation  applied to handwritten zip code recognition
Neural computation, N(N):NNN–NNN, NNNN
 [NN] Z
Li and D
Hoiem
Learning without forgetting
In ECCV,  N0NN
 [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft COCO: Common objects in context
In ECCV, N0NN
 [NN] M
McCloskey and N
J
Cohen
Catastrophic interference  in connectionist networks: The sequential learning problem
 Psychology of learning and motivation, NN:N0N–NNN, NNNN
 [NN] T
Mensink, J
Verbeek, F
Perronnin, and G
Csurka
 Distance-based image classification: Generalizing to new  classes at near-zero cost
PAMI, NN(NN):NNNN–NNNN, N0NN
 [NN] Y
Nesterov
A method of solving a convex programming  problem with convergence rate O(N/kN)
In Soviet Mathe- matics Doklady, volume NN, pages NNN–NNN, NNNN
 [NN] M
Oquab, L
Bottou, I
Laptev, and J
Sivic
Learning and  transferring mid-level image representations using convolutional neural networks
In CVPR, N0NN
 [NN] R
Polikar, L
Upda, S
S
Upda, and V
Honavar
Learn++:  An incremental learning algorithm for supervised neural networks
IEEE Trans
Systems, Man, and Cybernetics, Part C,  NN(N):NNN–N0N, N00N
 [N0] R
Ratcliff
Connectionist models of recognition memory:  constraints imposed by learning and forgetting functions
 Psychological review, NN(N):NNN, NNN0
 [NN] S.-A
Rebuffi, A
Kolesnikov, and C
H
Lampert
iCaRL:  Incremental classifier and representation learning
In CVPR,  N0NN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN: Towards real-time object detection with region proposal networks
In NIPS, N0NN
 [NN] M
Ristin, M
Guillaumin, J
Gall, and L
V
Gool
Incremental learning of NCM forests for large-scale image classification
In CVPR, N0NN
 [NN] A
A
Rusu, S
G
Colmenarejo, C
Gulcehre, G
Desjardins,  J
Kirkpatrick, R
Pascanu, V
Mnih, K
Kavukcuoglu, and  R
Hadsell
Policy distillation
In ICLR, N0NN
 [NN] A
A
Rusu, N
C
Rabinowitz, G
Desjardins, H
Soyer,  J
Kirkpatrick, K
Kavukcuoglu, R
Pascanu, and R
Hadsell
Progressive neural networks
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [NN] J
C
Schlimmer and D
H
Fisher
A case study of incremental concept induction
In AAAI, NNNN
 [NN] S
Thrun
Is learning the n-th thing any easier than learning  the first? In NIPS, NNNN
 [NN] A
Torralba and A
A
Efros
Unbiased look at dataset bias
 In CVPR, N0NN
 NN0N  tensorflow.org   [NN] E
Tzeng, J
Hoffman, T
Darrell, and K
Saenko
Simultaneous deep transfer across domains and tasks
In ICCV, N0NN
 [N0] J
Yosinski, J
Clune, Y
Bengio, and H
Lipson
How transferable are features in deep neural networks? In NIPS, N0NN
 [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object  proposals from edges
In ECCV, N0NN
 NN0NHide-And-Seek: Forcing a Network to Be Meticulous for Weakly-Supervised Object and Action Localization   Hide-and-Seek: Forcing a Network to be Meticulous for  Weakly-supervised Object and Action Localization  Krishna Kumar Singh and Yong Jae Lee  University of California, Davis  Abstract  We propose ‘Hide-and-Seek’, a weakly-supervised  framework that aims to improve object localization in images and action localization in videos
Most existing  weakly-supervised methods localize only the most discriminative parts of an object rather than all relevant parts,  which leads to suboptimal performance
Our key idea is  to hide patches in a training image randomly, forcing the  network to seek other relevant parts when the most discriminative part is hidden
Our approach only needs to  modify the input image and can work with any network designed for object localization
During testing, we do not  need to hide any patches
Our Hide-and-Seek approach obtains superior performance compared to previous methods  for weakly-supervised object localization on the ILSVRC  dataset
We also demonstrate that our framework can be  easily extended to weakly-supervised action localization
 N
Introduction  Weakly-supervised approaches have been proposed for  various visual classification and localization tasks including object detection [NN, NN, N, NN, N, N0, NN, N, NN, NN, N0],  semantic segmentation [NN, NN] and visual attribute localization [N, NN, NN, NN, NN]
The main advantage of weaklysupervised learning is that it requires less detailed annotations compared to the fully-supervised setting, and therefore  has the potential to use the vast weakly-annotated visual  data available on the Web
For example, weakly-supervised  object detectors can be trained using only image-level labels  (‘dog’ or ‘no dog’) without any object location annotations
 Existing weakly-supervised methods identify discriminative patterns in the training data that frequently appear in  one class and rarely in the remaining classes
This is done  either explicitly by mining discriminative image regions or  features [NN, NN, N, NN, N, NN, NN, N, N0] or implicitly by analyzing the higher-layer activation maps produced by a deep  network trained for image classification [NN, NN, NN]
However, due to intra-category variations or relying only on a  classification objective, these methods often fail to identify  the entire extent of the object and instead localize only the  Full image  Randomly hidden patches  Figure N
Main idea
(Top row) A network tends to focus on the  most discriminative parts of an image (e.g., face of the dog) for  classification
(Bottom row) By hiding images patches randomly,  we can force the network to focus on other relevant object parts in  order to correctly classify the image as ’dog’
 most discriminative part
 Recent work tries to address this issue of identifying only  the most discriminative part
Song et al
[NN] combine multiple co-occurring discriminative regions to cover a larger  extent of the object
While multiple selections ensure larger  coverage, it does not guarantee selection of less discriminative patches of the object in the presence of many highly  discriminative ones
Singh et al
[N0] use motion cues and  transfer tracked object boxes from weakly-labeled videos  to the images
However, this approach requires additional  weakly-labeled videos, which may not always be available
 Finally, Zhou et al
[NN] replace max pooling with global average pooling after the final convolution layer of an image  classification network
Since average pooling aggregates  activations across an entire feature map, it encourages the  network to look beyond the most discriminative part (which  would suffice for max pooling)
However, the network can  still avoid finding less discriminative parts if identifying a  few highly-discriminative parts can lead to accurate classification performance, as shown in Figure N(top row)
 Main Idea
In this paper, we take a radically different  approach to this problem
Instead of making algorithmic  NNNNN    changes [NN, NN] or relying on external data [N0], we make  changes to the input image
The key idea is to hide patches  from an image during training so that the model needs to  seek the relevant object parts from what remains
We thus  name our approach ‘Hide-and-Seek’
Figure N (bottom row)  demonstrates the intuition: if we randomly remove some  patches from the image then there is a possibility that the  dog’s face, which is the most discriminative, will not be  visible to the model
In this case, the model must seek other  relevant parts like the tail and legs in order to do well on the  classification task
By randomly hiding different patches in  each training epoch, the model sees different parts of the  image and is forced to focus on multiple relevant parts of  the object beyond just the most discriminative one
Importantly, we only apply this random hiding of patches during  training and not during testing
Since the full image is observed during testing, the data distribution will be different  to that seen during training
We show that setting the hidden  pixels’ value to be the data mean can allow the two distributions to match, and provide a theoretical justification
 Since Hide-and-Seek only alters the input image, it  can easily be generalized to different neural networks and  tasks
In this work, we demonstrate its applicability on  AlexNet [NN] and GoogLeNet [NN], and apply the idea  to weakly-supervised object localization in images and  weakly-supervised action localization in videos
For the  temporal action localization task (in which the start and  end times of an action need to be found), random frame  sequences are hidden while training a network on action  classification, which forces the network to learn the relevant  frames corresponding to an action
 Contributions
Our work has three main contributions:  N) We introduce the idea of Hide-and-Seek for weaklysupervised localization and produce state-of-the-art object  localization results on the ILSVRC dataset [NN]; N) We  demonstrate the generalizability of the approach on different networks and layers; N) We extend the idea to the relatively unexplored task of weakly-supervised temporal action localization
 N
Related Work  Weakly-supervised object localization
Fullysupervised convolutional networks (CNNs) have demonstrated great performance on object detection [NN, NN, N0],  segmentation [NN] and attribute localization [NN, N0, NN],  but require expensive human annotations for training  (e.g
bounding box for object localization)
To alleviate expensive annotation costs, weakly-supervised  approaches learn using cheaper labels, for example,  image-level labels for predicting an object’s location [NN, NN, N, NN, N, NN, N0, N, NN, NN]
 Most weakly-supervised object localization approaches  mine discriminative features or patches in the data that frequently appear in one class and rarely in other classes [NN,  NN, N, NN, N, N, NN, NN, N]
However, these approaches tend to  focus only on the most discriminative parts, and thus fail to  cover the entire spatial extent of an object
In our approach,  we hide image patches (randomly) during training, which  forces our model to focus on multiple parts of an object and  not just the most discriminative ones
Other methods use  additional motion cues from weakly-labeled videos to improve object localization [NN, N0]
While promising, such  videos are not always readily available and can be challenging to obtain especially for static objects
In contrast, our  method does not require any additional data or annotations
 Recent work modify CNN architectures designed for image classification so that the convolutional layers learn to  localize objects while performing image classification [NN,  NN]
Other network architectures have been designed for  weakly-supervised object detection [N0, N, NN]
Although  these methods have significantly improved the state-of-theart, they still essentially rely on a classification objective  and thus can fail to capture the full extent of an object if  the less discriminative parts do not help improve classification performance
We also rely on a classification objective
 However, rather than modifying the CNN architecture, we  instead modify the input image by hiding random patches  from it
We demonstrate that this enforces the network to  give attention to the less discriminative parts and ultimately  localize a larger extent of the object
 Masking pixels or activations
Masking image patches  has been applied for object localization [N], self-supervised  feature learning [NN], semantic segmentation [NN, N0], generating hard occlusion training examples for object detection [NN], and to visualize and understand what a CNN has  learned [NN]
In particular, for object localization, [NN, N]  train a CNN for image classification and then localize the  regions whose masking leads to a large drop in classification  performance
Since these approaches mask out the image  regions only during testing and not during training, the localized regions are limited to the highly-discriminative object parts
In our approach, image regions are masked during training, which enables the model to learn to focus on  even the less discriminative object parts
Finally, our work  is closely related to the adversarial erasing method of [NN],  which iteratively trains a sequence of models for weaklysupervised semantic segmentation
Each model identifies  the relevant object parts conditioned on the previous iteration model’s output
In contrast, we only train a single  model once—and is thus less expensive—and do not rely on  saliency detection to refine the localizations as done in [NN]
 Dropout [NN] and its variants [NN, NN] are also related
 There are two main differences: (N) these methods are designed to prevent overfitting while our work is designed to  improve localization; and (N) in dropout, units in a layer are  NNNN    Epoch	N	  Epoch	N	  CNN	Epoch	N	  CNN	  CNN	  W	  H	  S	 Training	image	  Training	phase	 Tes,ng	phase	  Trained	CNN	  Test	image	 (no	hidden	patches)	  Class	Ac=va=on	Map	(CAM)	 Predicted	label:	‘dog’	  Figure N
Approach overview
Left: For each training image, we divide it into a grid of S × S patches
Each patch is then randomly  hidden with probability phide and given as input to a CNN to learn image classification
The hidden patches change randomly across  different epochs
Right: During testing, the full image without any hidden patches is given as input to the trained network
 dropped randomly, while in our work, contiguous image regions or video frames are dropped
We demonstrate in the  experiments that our approach produces significantly better  localizations compared to dropout
 Action localization
Action localization is a well studied problem [NN, N, NN, NN, NN]
Recent CNN-based approaches [NN, NN] have shown superior performance compared to previous hand-crafted approaches
These fullysupervised methods require the start and end time of an action in the video during the training to be annotated, which  can be expensive to obtain
Weakly-supervised approaches  learn from movie scripts [NN, NN] or an ordered list of actions [N, NN]
Sun et al
[NN] combine weakly-labeled videos  with web images for action localization
In contrast to these  approaches, our approach only uses a single video-level action label for temporal action localization
[NN] also only  use video-level action labels for action localization with the  focus on finding the key event frames of an action
We instead focus on localizing the full extent of an action
 N
Approach  In this section, we first describe our Hide-and-Seek algorithm for object localization in images followed by action  localization in videos
 N.N
Weakly-supervised object localization  For weakly-supervised object localization, we are given  a set of images Iset = {IN, IN, ....., IN} in which each im- age I is labeled only with its category label
Our goal is to learn an object localizer that can predict both the category  label as well as the bounding box for the object-of-interest  in a new test image Itest
In order to learn the object lo- calizer, we train a CNN which simultaneously learns to localize the object while performing the image classification  task
While numerous approaches have been proposed to  solve this problem, existing methods (e.g., [NN, N, NN, NN])  are prone to localizing only the most discriminative object  parts, since those parts are sufficient for optimizing the classification task
 To enforce the network to learn all of the relevant parts  of an object, our key idea is to randomly hide patches of  each input image I during training, as we explain next
 Hiding random image patches
The purpose of hiding  patches is to show different parts of an object to the network while training it for the classification task
By hiding  patches randomly, we can ensure that the most discriminative parts of an object are not always visible to the network,  and thus force it to also focus on other relevant parts of the  object
In this way, we can overcome the limitation of existing weakly-supervised methods that focus only on the most  discriminative parts of an object
 Concretely, given a training image I of size W ×H × N, we first divide it into a grid with a fixed patch size of S×S× N
This results in a total of (W ×H)/(S × S) patches
We then hide each patch with phide probability
For example, in Fig
N left, the image is of size NNN × NNN × N, and it is divided into NN patches of size NN × NN × N
Each patch is hidden with phide = 0.N probability
We take the new image I ′ with the hidden patches, and feed it as a training input to a CNN for classification
 Importantly, for each image, we randomly hide a different set of patches
Also, for the same image, we randomly  hide a different set of patches in each training epoch
This  property allows the network to learn multiple relevant object parts for each image
For example, in Fig
N left, the  network sees a different I ′ in each epoch due to the random- ness in hiding of the patches
In the first epoch, the dog’s  face is hidden while its legs and tail are clearly visible
In  NNNN    Inside visible patch  Inside hidden patch  Partially in hidden patch  Figure N
There are three types of convolutional filter activations  after hiding patches: a convolution filter can be completely within  a visible region (blue box), completely within a hidden region (red  box), or partially within a visible/hidden region (green box)
 contrast, in the second epoch, the face is visible while the  legs and tail are hidden
Thus, the network is forced to learn  all of the relevant parts of the dog rather than only the highly  discriminative part (i.e., the face) in order to perform well  in classifying the image as a ‘dog’
 We hide patches only during training
During testing,  the full image—without any patches hidden—is given as  input to the network; Fig
N right
Since the network has  learned to focus on multiple relevant parts during training,  it is not necessary to hide any patches during testing
This  is in direct contrast to [N], which hides patches during testing but not during training
For [N], since the network has  already learned to focus on the most discimirinative parts  during training, it is essentially too late, and hiding patches  during testing has no significant effect on localization performance
 Setting the hidden pixel values
There is an important  detail that we must be careful about
Due to the discrepancy  of hiding patches during training while not hiding patches  during testing, the first convolutional layer activations during training versus testing will have different distributions
 For a trained network to generalize well to new test data,  the activation distributions should be roughly equal
That is,  for any unit in a neural network that is connected to x units  with w outgoing weights, the distribution of w⊤x should be  roughly the same during training and testing
However, in  our setting, this will not necessarily be the case since some  patches in each training image will be hidden while none of  the patches in each test image will ever be hidden
 Specifically, in our setting, suppose that we have a  convolution filter F with kernel size K × K and three- dimensional weights W = {wN,wN, ....,wk×k}, which is applied to an RGB patch X = {xN,xN, ....,xk×k} in image I ′
Denote v as the vector representing the RGB value of every hidden pixel
There are three types of activations:  N
F is completely within a visible patch (Fig
N, blue  box)
The corresponding output will be ∑k×k  i=N w ⊤ i xi
 N
F is completely within a hidden patch (Fig
N, red  box)
The corresponding output will be ∑k×k  i=N w ⊤ i v
 N
F is partially within a hidden patch (Fig
N, green box)
The corresponding output will be∑  m∈visible w ⊤ mxm +  ∑ n∈hidden w  ⊤ n v
 During testing, F will always be completely within a visible patch, and thus its output will be ∑k×k  i=N w ⊤ i xi
This  matches the expected output during training in only the first  case
For the remaining two cases, when F is completely or partially within a hidden patch, the activations will have a  distribution that is different to those seen during testing
 We resolve this issue by setting the RGB value v of a  hidden pixel to be equal to the mean RGB vector of the  images over the entire dataset: v = µ = N Npixels  ∑ j xj ,  where j indexes all pixels in the entire training dataset and Npixels is the total number of pixels in the dataset
Why would this work? Essentially, we are assuming that in expectation, the output of a patch will be equal to that of an  average-valued patch: E[ ∑k×k  i=N w ⊤ i xi] =  ∑k×k i=N w  ⊤ i µ
By  replacing v with µ, the outputs of both the second and third  cases will be ∑k×k  i=N w ⊤ i µ, and thus will match the expected  output during testing (i.e., of a fully-visible patch).N  This process is related to the scaling procedure in  dropout [NN], in which the outputs are scaled proportional  to the drop rate during testing to match the expected output during training
In dropout, the outputs are dropped  uniformly across the entire feature map, independently of  spatial location
If we view our hiding of the patches as  equivalent to “dropping” units, then in our case, we cannot have a global scale factor since the output of a patch  depends on whether there are any hidden pixels
Thus, we  instead set the hidden values to be the expected pixel value  of the training data as described above, and do not scale the  corresponding output
Empirically, we find that setting the  hidden pixel in this way is crucial for the network to behave  similarly during training and testing
 Object localization network architecture
Our approach of hiding patches is independent of the network architecture and can be used with any CNN designed for object localization
For our experiments, we choose to use  the network of Zhou et al
[NN], which performs global average pooling (GAP) over the convolution feature maps to  generate a class activation map (CAM) for the input image that represents the discriminative regions for a given  class
This approach has shown state-of-the-art performance for the ILSVRC localization challenge [NN] in the  weakly-supervised setting, and existing CNN architectures  like AlexNet [NN] and GoogLeNet [NN] can easily be modified to generate a CAM
 NFor the third case: ∑  m∈visible w  ⊤ mxm +  ∑ n∈hidden  w ⊤ n µ ≈∑  m∈visible w  ⊤ mµ+  ∑ n∈hidden  w ⊤ n µ =  ∑ k×k  i=N w  ⊤ i µ
 NNNN    To generate a CAM for an image, global average pooling is performed after the last convolutional layer and the  result is given to a classification layer to predict the image’s  class probabilities
The weights associated with a class in  the classification layer represent the importance of the last  convolutional layer’s feature maps for that class
More formally, denote F = {FN, FN, .., FM} to be the M feature maps of the last convolutional layer and W as the N × M weight matrix of the classification layer, where N is num- ber of classes
Then, the CAM for class c for image I is:  CAM(c, I) =  M∑  i=N  W (c, i) · Fi(I)
(N)  Given the CAM for an image, we generate a bounding box using the method proposed in [NN]
Briefly,  we first threshold the CAM to produce a binary foreground/background map, and then find connected components among the foreground pixels
Finally, we fit a tight  bounding box to the largest connected component
We refer  the reader to [NN] for more details
 N.N
Weakly-supervised action localization  Given a set of untrimmed videos Vset = {VN, VN, ..., VN} and video class labels, our goal here is to learn an action localizer that can predict the label of an  action as well as its start and end time for a test video Vtest
Again the key issue is that for any video, a network will  focus mostly on the highly-discriminative frames in order  to optimize classification accuracy instead of identifying all  relevant frames
Inspired by our idea of hiding the patches  in images, we propose to hide frames in videos to improve  action localization
 Specifically, during training, we uniformly sample video  Ftotal frames from each videos
We then divide the Ftotal frames into continuous segments of fixed size Fsegment; i.e., we have Ftotal/Fsegemnt segments
Just like with im- age patches, we hide each segment with probability phide before feeding it into a deep action localizer network
We  generate class activation maps (CAM) using the procedure  described in the previous section
In this case, our CAM  is a one-dimensional map representing the discriminative  frames for the action class
We apply thresholding on this  map to obtain the start and end times for the action class
 N
Experiments  We perform quantitative and qualitative evaluations of  Hide-and-Seek for object localization in images and action  localization in videos
We also perform ablative studies to  compare the different design choices of our algorithm
 Datasets and evaluation metrics
We use ILSVRC  N0NN [NN] to evaluate object localization accuracy
For  training, we use N.N million images with their class labels  (N000 categories)
We compare our approach with the baselines on the validation data
We use three evaluation metrics to measure performance: N) Top-N localization accuracy (Top-N Loc): fraction of images for which the predicted  class with the highest probability is the same as the groundtruth class and the predicted bounding box for that class has  more than N0% IoU with the ground-truth box
N) Local- ization accuracy with known ground-truth class (GT-known  Loc): fraction of images for which the predicted bounding  box for the ground-truth class has more than N0% IoU with the ground-truth box
As our approach is primarily designed  to improve localization accuracy, we use this criterion to  measure localization accuracy independent of classification  performance
N) We also use classification accuracy (TopN Clas) to measure the impact of Hide-and-Seek on image  classification performance
 For action localization, we use THUMOS N0NN validation data [NN], which consists of N0N0 untrimmed videos belonging to N0N action classes
We train over all untrimmed  videos for the classification task and then evaluate localization on the N0 classes that have temporal annotations
 Each video can contain multiple instances of a class
For  evaluation we compute mean average precision (mAP), and  consider a prediction to be correct if it has IoU > θ with ground-truth
We vary θ to be 0.N, 0.N, 0.N, 0.N, and 0.N
As we are focusing on localization ability of the network, we  assume we know the ground-truth class label of the video
 Implementation details
To learn the object localizer, we  use the same modified AlexNet and GoogLeNet networks  introduced in [NN] (AlexNet-GAP and GoogLeNet-GAP)
 AlexNet-GAP is identical to AlexNet until poolN (with  stride N) after which two new conv layers are added
Similarly for GoogLeNet-GAP, layers after inception-Ne are removed and a single conv layer is added
For both AlexNetGAP and GoogLeNet-GAP, the output of the last conv layer  goes to a global average pooling (GAP) layer, followed by a  softmax layer for classification
Each added conv layer has  NNN and N0NN kernels of size N × N, stride N, and pad N for AlexNet-GAP and GoogLeNet-GAP, respectively
 We train the networks from scratch for NN and N0 epochs  for AlexNet-GAP and GoogLeNet-GAP, respectively, with  a batch size of NNN and initial learning rate of 0.0N
We gradually decrease the learning rate to 0.000N
We add batch  normalization [NN] after every conv layer to help convergence of GoogLeNet-GAP
For simplicity, unlike the original AlexNet architecture [NN], we do not group the conv  filters together (it produces statistically the same Top-N Loc  accuracy as the grouped version for both AlexNet-GAP but  has better image classification performance)
The network  remains exactly the same with (during training) and without  (during testing) hidden image patches
To obtain the binary  fg/bg map, N0% and N0% of the max value of the CAM is chosen as the threshold for AlexNet-GAP and GoogLeNetNNNN    Methods GT-known Loc Top-N Loc Top-N Clas  AlexNet-GAP [NN] NN.N0N NN.NN N0.NN  AlexNet-HaS-NN NN.NN NN.NN NN.NN  AlexNet-HaS-NN NN.NN NN.NN NN.NN  AlexNet-HaS-NN NN.NN NN.NN NN.N0  AlexNet-HaS-NN NN.NN NN.NN NN.NN  AlexNet-HaS-Mixed NN.NN NN.NN NN.NN  GoogLeNet-GAP [NN] NN.NNN NN.N0 NN.NN  GoogLeNet-HaS-NN NN.NN NN.NN N0.NN  GoogLeNet-HaS-NN N0.NN NN.NN N0.N0  GoogLeNet-HaS-NN N0.NN NN.NN N0.NN  GoogLeNet-HaS-NN NN.NN NN.NN N0.NN  Table N
Localization accuracy on ILSVRC validation data with  different patch sizes for hiding
Our Hide-and-Seek always performs better than AlexNet-GAP [NN], which sees the full image
 GAP, respectively; the thresholds were chosen by observing  a few qualitative results on training data
During testing,  we average N0 crops (N corners plus center, and same with  horizontal flip) to obtain class probabilities and localization maps
We find similar localization/classification performance when fine-tuning pre-trained networks
 For action localization, we compute CND [NN] fcN features using a model pre-trained on Sports N million [NN]
 We compute N0 feats/sec (each feature is computed over  NN frames) and uniformly sample N000 features from the  video
We then divide the video into N0 equal-length segments each consisting of Fsegment = N00 features
During training, we hide each segment with phide = 0.N
For action classification, we feed CND features as input to a CNN with  two conv layers followed by a global max pooling and softmax classification layer
Each conv layer has N00 kernels  of size N × N, stride N
For any hidden frame, we assign it the dataset mean CND feature
For thresholding, N0% of the max value of the CAM is chosen
All continuous segments  after thresholding are considered predictions
 N.N
Object localization quantitative results  We first analyze object localization accuracy on the  ILSVRC validation data
Table N shows the results using the Top-N Loc and GT-known Loc evaluation metrics
 AlexNet-GAP [NN] is our baseline in which the network  has seen the full image during training without any hidden  patches
Alex-HaS-N is our approach, in which patches of  size N ×N are hidden with 0.N probability during training
 Which patch size N should we choose? We explored four different patch sizes N = {NN, NN, NN, NN}, and each performs significantly better than AlexNet-GAP for both  GT-known Loc and Top-N Loc
Our GoogLeNet-HaS-N  models also outperfors GoogLeNet-GAP for all patch sizes
 These results clearly show that hiding patches during training leads to better localization
Although our approach can  lose some classification accuracy (Top-N Clas) since it has  N[NN] does not provide GT-known loc, so we compute on our own GAP  implementations, which achieve similar Top-N Loc accuracy
 Methods GT-known Loc Top-N Loc  Backprop on AlexNet [NN] - NN.NN  AlexNet-GAP [NN] NN.N0 NN.NN  Ours NN.NN NN.NN  AlexNet-GAP-ensemble NN.NN NN.NN  Ours-ensemble N0.NN N0.N0  Backprop on GoogLeNet [NN] - NN.NN  GoogLeNet-GAP [NN] NN.NN NN.N0  Ours N0.NN NN.NN  Table N
Localization accuracy on ILSVRC val data compared to  state-of-the-art
Our method outperforms all previous methods
 never seen a complete image and thus may not have learned  to relate certain parts, the huge boost in localization performance (which can be seen by comparing the GT-known Loc  accuracies) makes up for any potential loss in classification
 We also train a network (AlexNet-HaS-Mixed) with  mixed patch sizes
During training, for each image in every  epoch, the patch size N to hide is chosen randomly from NN, NN, NN and NN as well as no hiding (full image)
Since different sized patches are hidden, the network can learn complementary information about different parts of an object (e.g
 small/large patches are more suitable to hide smaller/larger  parts)
Indeed, we achieve the best results for Top-N Loc  using AlexNet-HaS-Mixed
 Comparison to state-of-the-art
Next, we choose our  best model for AlexNet and GoogLeNet, and compare it  with state-of-the-art methods on ILSVRC validation data;  see Table N
Our method performs N.NN% and N.N0% points  better than AlexNet-GAP [NN] on GT-known Loc and Top-N  Loc, respectively
For GoogLeNet, our model gets a boost  of N.NN% and N.NN% points compared to GoogLeNet-GAP  for GT-known Loc and Top-N Loc accuracy, respectively
 Importantly, these gains are obtained simply by changing  the input image without changing the network architecture
 Ensemble model
Since each patch size provides complementary information (as seen in the previous section),  we also create an ensemble model of different patch sizes  (Ours-ensemble)
To produce the final localization for an  image, we average the CAMs obtained using AlexNet-HaSNN, NN, NN, and NN, while for classification, we average  the classification probabilities of all four models as well as  the probability obtained using AlexNet-GAP
This ensemble model gives a boost of N.NN % and N.NN% over AlexNetGAP for GT-known Loc and Top-N Loc, respectively
For a  more fair comparison, we also combine the results of five  independent AlexNet-GAPs to create an ensemble baseline
 Ours-ensemble outperforms this strong baseline (AlexNetGAP-ensemble) by N.NN% and N.NN% for GT-known Loc  and Top-N Loc, respectively
 N.N
Object localization qualitative results  In Fig
N, we visualize the class activation map (CAM)  and bounding box obtained by our AlexNet-HaS approach  NNNN    Bounding Box  (AlexNet-GAP)  Heatmap  (AlexNet-GAP)  Bounding Box  (Our HaS)  Heatmap  (Our HaS)  Bounding Box  (AlexNet-GAP)  Heatmap  (AlexNet-GAP)  Bounding Box  (Our HaS)  Heatmap  (Our HaS)  Figure N
Qualitative object localization results
We compare our approach with AlexNet-GAP [NN] on the ILVRC validation data
For  each image, we show the bounding box and CAM obtained by AlexNet-GAP (left) and our method (right)
Our Hide-and-Seek approach  localizes multiple relevant parts of an object whereas AlexNet-GAP mainly focuses only on the most discriminative parts
 versus those obtained with AlexNet-GAP
In each image  pair, the first image shows the predicted (green) and groundtruth (red) bounding box
The second image shows the  CAM, i.e., where the network is focusing for that class
Our  approach localizes more relevant parts of an object compared to AlexNet-GAP and is not confined to only the most  discriminative parts
For example, in the first, second, and  fifth rows AlexNet-GAP only focuses on the face of the animals, whereas our method also localizes parts of the body
 Similarly, in the third and last rows AlexNet-GAP misses  the tail for the snake and squirrel while ours gets the tail
 N.N
Further Analysis of Hide-and-Seek  Comparison with dropout
Dropout [NN] has been extensively used to reduce overfitting in deep network
Although it is not designed to improve localization, the dropping of units is related to our hiding of patches
We therefore conduct an experiment in which N0% dropout is applied at the image layer
We noticed that the due to the  large dropout rate at the pixel-level, the learned filters deNNN0    Methods GT-known Loc Top-N Loc  Ours NN.NN NN.NN  AlexNet-dropout-trainonly NN.NN N.NN  AlexNet-dropout-traintest NN.NN NN.NN  Table N
Our approach outperforms Dropout [NN] for localization
 Methods GT-known Loc Top-N Loc  AlexNet-GAP NN.N0 NN.NN  AlexNet-Avg-HaS NN.NN NN.NN  AlexNet-GMP N0.N0 NN.NN  AlexNet-Max-HaS NN.NN NN.NN  Table N
Global average pooling (GAP) vs
global max pooling  (GMP)
Unlike [NN], for Hide-and-Seek GMP still performs well  for localization
For this experiment, we use patch size NN
 Methods GT-known Loc Top-N Loc  AlexNet-GAP NN.N0 NN.NN  AlexNet-HaS-convN-N NN.NN NN.NN  AlexNet-HaS-convN-NN NN.NN NN.NN  Table N
Applying Hide-and-Seek to the first conv layer
The improvement over [NN] shows the generality of the idea
 velop a bias toward a dropped-out version of the images and  produces significantly inferior classification and localization performance (AlexNet-dropout-trainonly)
If we also  do dropout during testing (AlexNet-dropout-traintest) then  performance improves but is still much lower compared  to our approach Table N
Since dropout drops pixels (and  RGB channels) randomly, information from the most relevant parts of an object will still be seen by the network with  high probability, which makes it likely to focus on only the  most discriminative parts
 Do we need global average pooling? [NN] showed that  GAP is better than global max pooling (GMP) for object  localization, since average pooling encourages the network  to focus on all the discriminative parts
For max pooling,  only the most discriminative parts need to contribute
But  is global max pooling hopeless for localization?  With our Hide-and-Seek, even with max pooling, the  network is forced to focus on a different discriminative  parts
In Table N, we see that max pooling (AlexNet-GMP)  is inferior to average poling (AlexNet-GAP) for the baselines
But with Hide-and-Seek, max pooling (AlexNetMax-HaS) localization accuracy increases by a big margin  and even slightly outperforms average pooling (AlexNetAvg-HaS)
The slight improvement is likely due to max  pooling being more robust to noise
 Hide-and-Seek in convolutional layers
We next apply  our idea to convolutional layers
We divide the convolutional feature maps into a grid and hide each patch (and all  of its corresponding channels) with 0.N probability
We hide  patches of size N (AlexNet-HaS-convN-N) and NN (AlexNetHaS-convN-NN) in the convN feature map (which has size  NN×NN×NN)
Table-N shows that this leads to a big boost in performance compared to the baseline AlexNet-GAP
This  Methods GT-known Loc Top-N Loc  AlexNet-HaS-NN% NN.NN NN.NN  AlexNet-HaS-NN% NN.NN NN.0N  AlexNet-HaS-N0% NN.NN NN.NN  AlexNet-HaS-NN% NN.NN NN.NN  AlexNet-HaS-NN% NN.NN NN.NN  Table N
Varying the hiding probability
Higher probabilities  lead to decrease in Top-N Loc whereas lower probability leads to  smaller GT-known Loc
For this experiment, we use patch size NN
 Methods IOU thresh = 0.N 0.N 0.N 0.N 0.N  Video-full NN.NN NN.NN NN.NN NN.00 N.NN  Video-HaS NN.NN NN.NN NN.NN NN.NN N.NN  Table N
Action localization accuracy on THUMOS validation  data
Across all N IoU thresholds, our Video-HaS outperforms the  full video baseline (Video-full)
 shows that our idea of randomly hiding patches can be generalized to the convolutional layers
 Probability of hiding
In all of the previous experiments,  we hid patches with N0% probability
In Table N, we measure the GT-known Loc and Top-N Loc when we use different hiding probabilities
If we increase the probability  then GT-known Loc remains almost the same while Top-N  Loc decreases a lot
This happens because the network sees  fewer pixels when the hiding probability is high; as a result,  classification accuracy reduces and Top-N Loc drops
If we  decrease the probability then GT-known Loc decreases but  our Top-N Loc improves
In this case, the network sees more  pixels so its classification improves but since less parts are  hidden, it will focus more on only the discriminative parts  decreasing its localization ability
 N.N
Action localization results  Finally, we evaluate action localization accuracy
We  compare our approach (Video-HaS), which randomly hides  frame segments while learning action classification, with a  baseline that sees the full video (Video-full)
Table N shows  the result on THUMOS validation data
Video-HaS consistently outperforms Video-full for action localization task,  which shows that hiding frames forces our network to focus  on more relevant frames, which ultimately leads to better  action localization
We show qualitative results in the supp
 N
Conclusion  We presented ‘Hide-and-Seek’, a novel weaklysupervised framework to improve object localization in images and temporal action localization in videos
By randomly hiding patches/frames in a training image/video, we  force the network to learn to focus on multiple relevant parts  of an object/action
Our extensive experiments showed improved localization accuracy over state-of-the-art methods
 Acknowledgements
This work was supported in part by  Intel Corp, Amazon Web Services Cloud Credits for Research, and GPUs donated by NVIDIA
 NNNN    References  [N] L
Bazzani, B
A., D
Anguelov, and L
Torresani
Self-taught  object localization with deep networks
In WACV, N0NN
N, N  [N] T
Berg, A
Berg, and J
Shih
Automatic attribute discovery  and characterization from noisy web data
In ECCV, N0N0
N  [N] H
Bilen, M
Pedersoli, and T
Tuytelaars
Weakly supervised  object detection with posterior regularization
In BMVC,  N0NN
N, N  [N] H
Bilen and A
Vedaldi
Weakly supervised deep detection  networks
In CVPR, N0NN
N  [N] P
Bojanowski, R
Lajugie, F
Bach, I
Laptev, J
Ponce,  C
Schmid, and J
Sivic
Weakly supervised action labeling  in videos under ordering constraints
In ECCV, N0NN
N  [N] C
Y
Chen and K
Grauman
Efficient activity detection with  max-subgraph search
In CVPR, N0NN
N  [N] R
Cinbis, J
Verbeek, and C
Schmid
Multi-fold MIL Training for Weakly Supervised Object Localization
In CVPR,  N0NN
N  [N] R
Cinbis, J
Verbeek, and C
Schmid
Weakly supervised  object localization with multi-fold multiple instance learning
In arXiv:NN0N.00NNN, N0NN
N, N, N  [N] D
J
Crandall and D
P
Huttenlocher
Weakly supervised  learning of part-based spatial models for visual object recognition
In ECCV, N00N
N, N  [N0] J
Dai, K
He, and J
Sun
Convolutional feature masking for  joint object and stuff segmentation
In CVPR, N0NN
N  [NN] K
Duan, D
Parikh, D
Crandall, and K
Grauman
Discovering localized attributes for fine-grained recognition
In  CVPR, N0NN
N  [NN] O
Duchenne, I
Laptev, J
Sivic, F
Bach, and J
Ponce
Automatic annotation of human actions in video
In ICCV, N00N
 N  [NN] R
Fergus, P
Perona, and A
Zisserman
Object Class Recognition by Unsupervised Scale-Invariant Learning
In CVPR,  N00N
N, N  [NN] C
Gan, N
Wang, Y
Yang, D.-Y
Yeung, and A
G
Hauptmann
Devnet: A deep event network for multimedia event  detection and evidence recounting
In CVPR, N0NN
N  [NN] R
Girshick
Fast r-cnn
In ICCV, N0NN
N  [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich Feature Hierarchies for Accurate Object Detection and Semantic  Segmentation
In CVPR, N0NN
N  [NN] B
Hariharan, P
Arbeláez, R
Girshick, and J
Malik
Simultaneous detection and segmentation
In ECCV, N0NN
N  [NN] D.-A
Huang, L
Fei-Fei, and J
C
Niebles
Connectionist  temporal modeling for weakly supervised action labeling
In  ECCV, N0NN
N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
In  ICML, N0NN
N  [N0] M
Jaderberg, K
Simonyan, A
Zisserman, and  k
kavukcuoglu
Spatial transformer networks
In NIPS,  N0NN
N  [NN] H
Jhuang, J
Gall, S
Zuffi, C
Schmid, and M
J
Black
 Towards understanding action recognition
In ICCV, N0NN
N  [NN] Y.-G
Jiang, J
Liu, A
Roshan Zamir, G
Toderici, I
Laptev,  M
Shah, and R
Sukthankar
THUMOS challenge: Action recognition with a large number of classes
http:  //crcv.ucf.edu/THUMOSNN/, N0NN
N  [NN] V
Kantorov and I
Laptev
Efficient feature extraction, encoding and classification for action recognition
In CVPR,  N0NN
N  [NN] V
Kantorov, M
Oquab, M
Cho, and I
Laptev
Contextlocnet: Context-aware deep network models for weakly supervised localization
In ECCV, N0NN
N  [NN] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In CVPR, N0NN
N  [NN] A
Khoreva, R
Benenson, M
Omran, M
Hein, and  B
Schiele
Weakly supervised object boundaries
In CVPR,  N0NN
N  [NN] M
Kiapour, K
Yamaguchi, A
C
Berg, and T
L
Berg
Hipster wars: Discovering elements of fashion styles
In ECCV,  N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
Hinton
Imagenet Classification with Deep Convolutional Neural Networks
In NIPS,  N0NN
N, N, N  [NN] I
Laptev, M
Marszalek, C
Schmid, and B
Rozenfeld
 Learning realistic human actions from movies
In CVPR,  N00N
N  [N0] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.-Y
 Fu, and A
C
Berg
Ssd: Single shot multibox detector
In  ECCV, N0NN
N  [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
N  [NN] M
Oquab, L
Bottou, I
Laptev, and J
Sivic
Is object localization for free? weakly-supervised learning with convolutional neural networks
In CVPR, N0NN
N, N, N  [NN] D
Pathak, P
Krähenbühl, and T
Darrell
Constrained convolutional neural networks for weakly supervised segmentation
In ICCV, N0NN
N  [NN] D
Pathak, P
Krähenbühl, J
Donahue, T
Darrell, and  A
Efros
Context encoders: Feature learning by inpainting
 In CVPR, N0NN
N  [NN] A
Prest, C
Leistner, J
Civera, C
Schmid, and V
Ferrari
 Learning Object Class Detectors from Weakly Annotated  Video
In CVPR, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual  Recognition Challenge
IJCV, N0NN
N, N, N  [NN] Z
Shou, D
Wang, and S.-F
Chang
Temporal action localization in untrimmed videos via multi-stage cnns
In CVPR,  N0NN
N  [NN] K
Simonyan, A
Vedaldi, and A
Zisserman
Deep inside convolutional networks: Visualising image classification models and saliency maps
In ICLR Workshop, N0NN
N,  N  [NN] K
K
Singh and Y
J
Lee
End-to-end localization and ranking for relative attributes
In ECCV, N0NN
N  [N0] K
K
Singh, F
Xiao, and Y
J
Lee
Track and transfer:  Watching videos to simulate strong human supervision for  weakly-supervised object detection
In CVPR, N0NN
N, N  NNNN  http://crcv.ucf.edu/THUMOSNN/ http://crcv.ucf.edu/THUMOSNN/   [NN] P
Siva, C
Russell, and T
Xiang
In Defence of Negative  Mining for Annotating Weakly Labelled Data
In ECCV,  N0NN
N, N  [NN] H
O
Song, R
Girshick, S
Jegelka, J
Mairal, Z
Harchaoui,  and T
Darrell
On Learning to Localize Objects with Minimal Supervision
In ICML, N0NN
N, N, N  [NN] H
O
Song, Y
J
Lee, S
Jegelka, and T
Darrell
Weaklysupervised discovery of visual pattern configurations
In  NIPS, N0NN
N, N  [NN] N
Srivastava, G
Hinton, A
Krizhevsky, I
Sutskever, and  R
Salakhutdinov
Dropout: A simple way to prevent neural  networks from overfitting
JMLR, N0NN
N, N, N, N  [NN] C
Sun, S
Shetty, R
Sukthankar, and R
Nevatia
Temporal localization of fine-grained actions in videos by domain  transfer from web images
In ACM Multimedia, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
N, N  [NN] J
Tompson, R
Goroshin, A
Jain, Y
LeCun, and C
Bregler
 Efficient object localization using convolutional networks
In  CVPR, N0NN
N  [NN] D
Tran, L
Bourdev, R
Fergus, L
Torresani, and M
Paluri
 Learning spatiotemporal features with Nd convolutional networks
In ICCV, N0NN
N  [NN] L
Wan, M
Zeiler, S
Zhang, Y
LeCun, and R
Fergus
Regularization of neural network using dropconnect
In ICML,  N0NN
N  [N0] C
Wang, W
Ren, K
Huang, and T
Tan
Weakly supervised  object localization with latent category learning
In ECCV,  N0NN
N, N  [NN] H
Wang and C
Schmid
Action recognition with improved  trajectories
In ICCV, N0NN
N  [NN] J
Wang, Y
Cheng, and R
Schmidt Feris
Walk and  learn: Facial attribute representation learning from egocentric video and contextual data
In CVPR, N0NN
N  [NN] S
Wang, J
Joo, Y
Wang, and S
C
Zhu
Weakly supervised learning for attribute localization in outdoor scenes
In  CVPR, N0NN
N  [NN] X
Wang, A
Shrivastava, and A
Gupta
A-fast-rcnn: Hard  positive generation via adversary for object detection
In  CVPR, N0NN
N  [NN] M
Weber, M
Welling, and P
Perona
Unsupervised Learning of Models for Recognition
In ECCV, N000
N, N  [NN] Y
Wei, J
Feng, X
Liang, M.-M
Cheng, Y
Zhao, and  S
Yan
Object region mining with adversarial erasing: A  simple classification to semantic segmentation approach
In  CVPR, N0NN
N  [NN] F
Xiao and Y
J
Lee
Discovering the spatial extent of relative attributes
In ICCV, N0NN
N  [NN] S
Yeung, O
Russakovsky, G
Mori, and L
Fei-Fei
Endto-end learning of action detection from frame glimpses in  videos
In CVPR, N0NN
N  [NN] M
D
Zeiler and R
Fergus
Visualizing and understanding  convolutional networks
In ECCV, N0NN
N  [N0] N
Zhang, M
Paluri, M
Ranzato, T
Darrell, and L
Bourdev
 PANDA: Pose Aligned Networks for Deep Attribute Modeling
In CVPR, N0NN
N  [NN] B
Zhou, A
Khosla, L
A., A
Oliva, and A
Torralba
Learning deep features for discriminative localization
In CVPR,  N0NN
N, N, N, N, N, N, N, N  NNNNAesthetic Critiques Generation for Photos   Aesthetic Critiques Generation for Photos  Kuang-Yu Chang∗, Kung-Hung Lu∗, and Chu-Song Chen  Institute of Information Science, Academia Sinica, Taipei, Taiwan  {kuangyu, henrylu, song} @iis.sinica.edu.tw  Abstract  It is said that a picture is worth a thousand words
Thus,  there are various ways to describe an image, especially in  aesthetic quality analysis
Although aesthetic quality assessment has generated a great deal of interest in the last  decade, most studies focus on providing a quality rating of  good or bad for an image
In this work, we extend the task  to produce captions related to photo aesthetics and/or photography skills
To the best of our knowledge, this is the  first study that deals with aesthetics captioning instead of  AQ scoring
In contrast to common image captioning tasks  that depict the objects or their relations in a picture, our approach can select a particular aesthetics aspect and generate captions with respect to the aspect chosen
Meanwhile,  the proposed aspect-fusion method further uses an attention  mechanism to generate more abundant aesthetics captions
 We also introduce a new dataset for aesthetics captioning called the Photo Critique Captioning Dataset (PCCD),  which contains pair-wise image-comment data from professional photographers
The results of experiments on PCCD  demonstrate that our approaches outperform existing methods for generating aesthetic-oriented captions for images
 N
Introduction  Aesthetic computing has long been an important topic in  the field of computer vision
In this paper, we consider the  problem of image captioning from the aesthetic viewpoint
 There are many studies on caption generation [NN, N, NN,  NN, N, NN, N, NN, N, N0]; however, most of them focus on  producing a single caption that depicts the objects or the  relative positions of the objects in a picture
 In this paper, we study a new problem, namely, aesthetic  analysis of photos
Aesthetic quality (AQ) assessment has  generated a great deal of interest in the last decade
Many  studies tackled this problem with various feature representations and/or learning architectures [NN, NN, NN, NN, NN]
 ∗indicates equal contribution
 Photo Critique Captioning: racing makes for  interesting pictures because of the speed  the movement the bright colors  Image Captioning: a man riding a motorcycle   down a street  Figure N: Photo critique captioning versus image captioning  However, the purpose of AQ assessment is to provide a binary decision, which yields a quality rating of good or bad  for a specific photo
In this paper, we address a more general problem, namely, captioning of photo aesthetics and/or  photography skills
To the best of our knowledge, this is  the first study that considers the problem, which covers a  broader range of applications than AQ assessment only
Besides AQ, our system analyzes the reasons why photos are  (or are not) appealing in some respect, so that a meaningful caption can be generated for a photo from an aesthetic  perspective
Figure N shows the difference between photo  aesthetics captioning and common image captioning
 Figure N provides more examples of captions produced  by our system
In Figure N(a), the space reserved on the  right-hand side of the photo needs to be refined; in Figure N(b), the vanishing point and lines are good and admired; and in Figure N(c), the subjects gaze creates further  space that enhances the AQ
With the captions provided, the  topic addressed in this paper suggests a better applicability
 Besides the simple assessment of AQ, it is possible to provide in-depth descriptions and comments, which are informative and can improve the photographic skills of users
 NNNNN    Our learning model assumes there is an input dataset of  images and their sentence descriptions
Every sentence relates to a specific aspect
To evaluate our work thoroughly,  we compiled a dataset for photo aesthetics captioning called  PCCD
The dataset contains pairwise data of images and  sentences, where an image could have multiple sentences  related to different aspects of the aesthetics
To the best of  our knowledge, this is the first publicly available dataset for  photo aesthetics captioning
 We propose two approaches to solve the aesthetic critique problem
The first is our baseline approach called the  aspect oriented (AO) approach; and the second is an improvement of AO called the aspect fusion (AF) approach
In  the AO approach, the training data are divided into disjoint  subsets based on the aspects of sentences, and we apply a  CNN-LSTM model to create a photo captioning system for  each aspect
The CNN model is also trained for regression  and then used to select the most interesting aspect of the  input image
Instead of enforcing a single aspect, our AF  approach fuses the captions learned from the individual aspects to create a new caption
We propose a soft attention  mechanism in the AF approach to produce a caption from  the established LSTMs
In our evaluation, the AF approach  performs better than learning a CNN-LSTM model directly  from the training data of all aspects
 The contributions of this paper are as follows
 From judgement to critiques: As well as AQ assessment,  our approach provides a caption for the aesthetic value of  the input image
 Photo aesthetics captioning dataset: We compiled a  dataset for the performance evaluation
It is the first publicly available dataset in this new area
 Multi-aspect aesthetics captioning: We propose a new  captioning approach to generate aesthetic critiques for images and the generated sentences are aspect-oriented which  are more diverse and favorable for human
 N
Related Work  Image Captioning: Recently, many approaches [NN, N,  NN, NN, N, NN, N, NN, N, N0] have achieved promising results by describing objects in images and videos with natural language
Most of them [NN, NN] apply a CNN-RNN  framework comprised of high-level features extracted from  a CNN model trained on object recognition and the Recurrent Neural Networks (RNN) language model
Johnson et  al
[NN] consider the dense captioning task and use the CNNRNN framework to generalize object detection and generate  dense annotations of images
Mao et al
[NN, NN, N0] propose a multimodal Recurrent Neural Networks model that  embeds the recurrent language features and image features  in a multimodel space
[N, NN] leverage external data so that  the CNN-LSTM captioning model does not require paired  image-sentence data for training
 (a ) t  h e   c r o p   i s   t o o   t i g h t    o n   t h e   r i g h t   s i d e  (b ) v  e r y   n i c e    c o m p o s i t i o n   l o v e    t h e   v a n i s h i n g    p o i n t   a n d   t h e    l i n e s  (c ) i  l i k e   t h e   w a y   y o u    h a v e   u s e d   t h e   g a z e   o f    t h e   s u b j e c t   t o   t h e   l e f t    o f   t h e   f r a m e  Figure N: Examples of captions generated by our system for  photo aesthetic analysis
 Some works focus on captioning with visual attentions
 You et al
[NN] extract visual attributes from the image and  combine them as semantic attention to guide image captioning
Xu et al
[NN] enable the model to focus on a local patch  of the image when generating a sequence of words
In contrast to the CNN-RNN models, some approaches exploit retrieval techniques
For example, Devlin et al
[N] proposes  a nearest neighbor method to retrieve captions and outputs  the top ranked one
Fang et al
[N] extract visual concepts by  training visual detectors for words and use a maximum entropy model conditioned on the detected words to generate  captions
Most recent works [NN, NN] extend the attention  mechanism with the ability to interact with language model  to choose the attention areas dynamically
 Video captioning [NN, NN, NN] has also generated a great  deal of interest
Tapaswi et al
[NN] align a movie scene  with a suitable book chapter by using dialogs and character identities as cues; while Zhu et al
[NN] match books  and movies on the sentence/paragraph level
These two  works try to provide rich descriptive explanations of visual  content, which are far beyond existing captioning works in  terms of semantic meaning
 AQ Assessment: AQ assessment of photos has been investigated for a long time [N, NN, NN, N]
The first challenge  is how to represent the aesthetics of an image
Traditional  low level features, such as color histograms, hue and saturation, are utilized in AQ assessment
Moreover, some studies  [NN, N, NN] focus on designing semantic feature representations
The inspiration might come from the photography or  image processing, e.g., the rule of thirds, sky illumination  and simplicity [NN]
 With the success of deep learning techniques, some approaches [NN, NN, NN, NN] exploit deep CNN to learn powerful representations from the data in an end-to-end manner  NNNN    C o m p o sitio  n : i l i k e   t h e   w a y   y o u   h a v e    u s e d   t h e   r u l e   o f   t h i r d s   t o   m a k e   t h e    i m a g e   m o r e   i n t e r e s t i n g   c o m p o s i t i o n   a n d    I   w o u l d   l i k e   t o   s e e   m o r e   o f   t h e   s k y    a n d   t h e   t r e e   b r a n c h e s  C o lo r: t h e   c o l o r   i s   g r e a t   i l o v e   t h e    b r i g h t   g r e e n s  Figure N: Examples of the captions generated by our approach for different aspects (composition and color) of the  same picture
 for AQ assessment
Kong et al
[NN] propose a CNN-based  method that combines different loss functions and attributes  in their aesthetic dataset
Lu et al
[NN] introduced a doublecolumn CNN architecture that uses holistic images and image patches as global and local features respectively
The  style attributes are aggregated to leverage the performance
 Subsequently, Lu et al
[NN] proposed a multiple instance  learning CNN model that generates multiple patches from  a single image
More precisely, the statistics aggregation  layer aggregates the multiple patches and achieves a better  performance than comparable approaches
 In this paper, we introduce a two-stage LSTM model  that can integrate the LSTM features of different aspects  for photo captioning from the aesthetic viewpoint
 N
Our Framework  Multi-aspect captioning: A worth noting issue of the photo  aesthetics captioning is its multi-aspect nature; that is, more  than one aspect of an image can be commented on
For example, a photo could be characterized in terms of the composition, color-arrangement and subject-contrast aspects,  which relate to the aesthetics or photographic skills
Different aspects would have different captions to be synthesized, as shown by the example in Figure N
The caption  produced by our system is “i like the way you have used  the rule of thirds · · · ” for the composition aspect
However, for the color-arrangement aspect, “the color is great and i  love the bright greens” would be a more suitable alternative  produced by the system
 N.N
Aspect-oriented (AO) approach – baseline  In our baseline approach, AO, the training data are separated into different aspects
Assume a dataset containing N  triplets, D = (Φi, Ci, ai), i = N · · ·N , is available to train our photo aesthetics captioning system, where Φi is the i-th  image and Ci is its caption
The images can be repeated  (i.e., Φi = Φj for some i and j), but the captions Ci vary with i; ai in {N · · ·L} is the aspect of the caption, where L is the number of aspects
Besides the images and captions,  a likelihood (namely, pi,l ∈ [0, N]) is also available as the degree of aesthetic appeal of the image Φi on the aspect l
 In the AO approach, the training data asociated with  the triplets whose captions are for a single aspect, namely,  (Φi, Ci, l), i = N · · ·Nl, are used, where Nl is the amount of training data in the aspect l
We employ a CNN-LSTM  architecture to train the captioning model for each single  aspect
To proceed, we give a brief review of the CNNLSTM as follows
Given a training caption (desired output)  Ci comprised of the words {wN, wN, · · · , wT }, a total of T + N feature vectors {x−N, x0, xN, · · · , xT } are fed into the LSTM model, where x−N is the feature vector extracted  from the CNN for the input image Φi, x0 is a special START token, and xt are the feature vectors converted from wt in  the feature-embedded layer for t = N · · ·T 
The LSTM model computes a sequence of hidden states ht and outputs  the word probability prediction yt by the recurrence formula  for t = N · · ·T ,  {ht, yt} = f(ht−N, xt)
(N)  Thus, given an input image Φi, we can get L aspect- specific captioning models
We denote the hidden states  (a.k.a
hidden annotations) of the l-th aspect LSTM model  to be hl = {hl,t|t = N · · ·T}, for l = N · · ·L
Without loss of generality, the neuraltalkN model [NN]  is adopted in our approach, despite our framework can use  other models as well
The CNN-LSTM models have some  variations in recent studies [NN, NN]
Because this paper  deals with a new problem, evaluation (or comparison) of  more updated CNN-LSTM models that are favorable for  conventional captioning tasks is not our main focus
On the  contrary, we focus more on handling the aesthetic critiques  of different aspects to develop a better photo aesthetics captioning system (Section N.N)
 The learned caption generator is then used to produce a  caption associated with the photo aesthetics of the aspect  focused on
For example, if the model is trained on the aspect of composition, the captions generated will target the  compositional analysis of the image
In the AO approach,  a single aspect l∗ is selected from the L aspects, and the  caption generated by the CNN-LSTM model for the l∗-th  aspect serves as the output
To choose the aspect of appealing, we use the CNN model to train L predictors based on  the pairs {(Φi; pi,l)} (l = N · · ·L) in our dataset
The output of the CNN model has L nodes, each of which has a regression output in the range [0,N]
Then, given an input image,  we select the aspect with the highest prediction value in the  AO approach as l∗
Figure N shows the flowchart of the  AO approach, which combines the aspect predictor and the  NNNN    CNN  Aspect   Predictor  LSTM   Aspect N  LSTM   Aspect N  LSTM   Aspect N  Photo Critique Captions  Composition  Color & Lighting  Subject of Photo  Predicted   Word  Figure N: Overview of the aspect-oriented (AO) approach,  where the number of aspects L = N
 individual aspect-oriented captioning systems
 N.N
Aspect-fusion (AF) approach  However, irrespective of the type of evaluation used (automatic or human evaluation), we found that the performance of the AO approach is limited because it is trained on  a restricted set of data for the chosen aspect
As the training data in a single aspect is less than those in the whole  dataset, AO cannot exploit the interrelated sentences between different aspects to produce a more diverse caption
 Hence, the caption generated by the AO approach tends to  be monotonous
 A possible remedy to this lack-of-diversity problem is  to apply the CNN-LSTM to the whole dataset of imagecaption pairs, {(Φi, Ci)|i = N · · ·N}, which contains the training captions from all aspects; we refer this approach  to as the CNN-LSTM on the whole dataset (CNN-LSTMWD)
However, we found that this approach still suffers  from the same problem on either automatic or human evaluation, possibly because of the inter-aspect difference of the  words and sentence forms of the captions
 To address this issue, we develop the AF approach  that also uses the entire dataset to learn a CNN-LSTM  model
Unlike CNN-LSTM-WD whose inputs are the images {Φi ∈ D} in the learning process, we propose leverag- ing the L aspect-specific models already trained
In the AF  approach, the hidden annotations hl (l = N · · ·L) generated by the L aspect-specific captioning models are further used  for learning the CNN-LSTM model; hence, both the images  and hidden annotations, namely, {(Φi, hi;l)|i = N · · ·N}, are used as the inputs in the learning process, and the captions {Ci|i = N · · ·N} remain as the desired outputs
As the output words in the LSTM models are directly dependent on the hidden states hl,t, the hidden-layer outputs can  serve as the feature representations extracted by using the  CNN-LSTM models
Thus, the hidden annotations are deep  features extracted from the models already trained and established for every aspect, which are better sources for training and make the AF approach potentially more effective in  learning a new caption model
The recurrence formula in  the new LSTM of the AF approach is established as  (gτ , yτ ) = F (gτ−N, xτ , sτ )
(N)  In contrast to Eq
(N), we denote ‘τ ’ as the time index and  ‘F ’ as the recursive function in Eq
(N) to avoid the confusion with the symbols ‘t’ and ‘f ’
In Eq
(N), the output yτ is  conditioned on the input words xτ and the previous hidden  state gτ−N, as well as on sτ that is a context vector relying  on the aspect-specific hidden annotations, hl (l = N · · ·L)
The formulation of the context vector sτ will be detailed in  the following
 To fuse the hidden annotations hl = {hl,t|t = N · · ·T} from different aspects l, a worth-of-noting issue is that the  time indices t in different aspects are not aligned inherently,  and thus they should not be combined directly to generate the output at time τ = t
In our AF model, there are L sources, and we introduce a soft-attention layer to predict the aspect-fusion coefficients from the context information
The context vector sτ is determined by combining  the aspect-specific hidden annotations as follows:  sτ =  L∑  l=N  T∑  t=N  ατlt(h, gτ−N)hlt
(N)  In Eq
(N), the fusion coefficients ατlj(h, gτ−N) of time po- sition τ is dependent on the aspect-specific hidden annotations, h, and hidden state of the previous time postion, gτ−N
 The coefficients provide soft attention on the entire period  (t = [N : T ]) of the aspect-specific hidden annotations
Note that for different aspects l = N · · ·L, different coefficients αl,[N:T ] are used, and thus an asynchronized attention mechanism is enforced
To provide the capability of non-uniform  alignment of reference, recent advances of sequence to sequence models [N][NN] also embed soft attention in their formulations
However, unlike their approaches where only a  single source sequence is used, there are L sources in the  AF model and an updated soft-attention mechanism is proposed in our study
To generate the fusion coefficients ατlt in  the soft-attention layer of AF, we first produce an intermediate representation eτlt that is adaptive to the previous state  gτ−N and aspect-specific hidden annotations hlt by using  eτlt = A(gτ−N, hlt), (N)  with A(·, ·) a feed-forward network established as  A(gτ−N, hlt) = Wγ(Ugτ−N +Vhlt), (N)  where γ is the ReLU activation function, W ∈ Rn×n, U ∈ Rn×n and V ∈ Rn×n are learnable weighted ma- trices, and n is the dimension of the hidden state vector (in  NNNN    CNN LSTM  Aspect N  LSTM  Photo Critique Captions  Composition  Color & Lighting  Subject of Photo  LSTM  Aspect N  LSTM  Aspect N  Training Images  Predicted   Word  Figure N: Overview of the aspect-fusion (AF) approach in  the case where L = N
 our implementation, n = NNN)
Then, ατlt is obtained by normalizing eτlt,  ατlt = exp(eτlt)∑L  p=N  ∑T q=N exp(e  τ pq))  
(N)  The proposed AF approach is illustrated in Figure N
By  considering the context vector, the AF approach can leverage the hidden annotations of different aspects and choose  the proper combination dynamically over time for caption  generation
Because the aspect-oriented hidden annotations, together with the image CNN features and word sequences of captions, are fed into the LSTM model (blue  part) to generate the output caption of our AF approach, the  sentences in different aspects are likely to be softly merged  in the learned model to enhance the formation of captions
 N
Dataset and Evaluation Criteria  In this section, we present the dataset for photo critiques  learning, and the criteria for the performance evaluation
 N.N
Dataset  To validate the proposed method on the aesthetic-related  photo caption generation problem, we compiled a dataset  called the Photo Critique Captioning Dataset (PCCD),  which is available to the public and can be used for future studies in this area
The dataset is based on a professional photo critique websiteN that provides experienced  photographers reviews of photos
On the website, photos are presented and several professional comments are  Nhttps://gurushots.com/  Hello Nick, Great shot the composition is just a wonderful mix of center   composition to diagonal lines and triangles, and density portions
It's so   serene calming with the explosion of the sun during sun down
Like the   ocean flow between the walls add the action to the image
 As mentioned above Nick, excellent composition, five factors happening   all at once
Just lacks a little contrast more so density in the center
 color is great in terms of the time when the shot was taken, nice and subtle   pastel sort of
again for me just the center with a longer exposure would   have helped to draw out more color and density, and nick I am speaking of   a little
 all settings observed are good and well used to achieve the effects   including both nd filters, To gain some of the lost density in the center, a   slightly longer exposure would have achieved this
 Great shot Nick the composition is outstanding! and the wave action   adds , I can just hear the waves launching on the shore
leaves one with a   calming effect
The clouds also add to complete the mood
 depth is great with f NN and the foreground and ocean horizon along with   the wooden walls on both sides, this really adds Nick to depth
 Great focus by stopping down to f NN
 Tareq, I like this shot very much, it is a similar style I like to use , it is   subtle and has strength by the leafs or flowers with just a soft density of   color
The branches to the left are a great lead in and gives this image   depth
 Tareq, very good composition the purple colored leafs sit at #N in the rule   of thirds and leading lines from the branches of the left which also uses   great depth of flied by there fade off to the background, excellent !  Great soft color use to isolate the purple tone of the main leaf and give to   another extent a balance to the green leafs, lighting is well balanced,   however the leaves in the bottom right could be toned down a little to give   even more focus to the main purple leaf
 Excellent camera! being a Nikon user as well
exposure used N/N0 gives   good density in the background and enough for the leaves in foreground   and the f stop as mentioned should have been a little more closed down to   maintain even focus across the purple leaf
Great lens to use as it gives a   normal perspective in shape and size of the subject matter
 Tareq,very impressive shot and as said similar to a style I use, The singled   out purple leaf give me the impression of isolation and or standing alone   in defiance, well composed and a great sense of depth
Well done!  great use of depth of field, clearly seen by the branches on the left that   lean into the background
including the blurred leafs and branches further   in
just need to close down alittle on the f stop as mentioned above
 focus is sharp, but by using f N.N or so would have keep the whole purple   leave in full focus , some parts to the right and far left of the second leaf   are slightly out of focus
 Subject  of Photo  Composition  &   Perspective  Color &  Lighting  Use of   Camera,  Exposure &   Speed  General   Impression  Depth  of Field  Focus  N0/N0  N0/N0  N0/N0  N0/N0  N0/N0  N0/N0  N0/N0  N0/N0  N/N0  N/N0  N/N0  N/N0  N/N0  N/N0  Figure N: Samples in the Photo Critique Captioning Dataset  (PCCD)
 given about the following seven aspects: general impression, composition and perspective, color and lighting, subject of photo, depth of field, focus, and use of camera, exposure and speed
For those aspects that have comments for a  given photo, a paragraph containing one or more sentences  are presented
Figure N shows some sample examples
The  photos together with their sentences in the respective aspects are used to establish the triplets D = (Φi, Ci, ai)
Ta- ble N shows the statistics of PCCD
It contains NNNN images  and more than sixty thousands captions
The source data  used to compile our dataset also contains a rating (from N to  N0) per aspect for a photo; the higher the rating for the aspect, the better will be the quality of the input image for the  aspect
The ratings are normalized to [0,N] and serve as the  likelihood of aesthetic appeal of the aspect {pi,l} described in Section N.N
 In our experiments, because not all aspects are commented for a photo, we select L = N most frequent aspects, namely, composition and perspective, color and lighting and  subject of photo, which contain NNN0 images and N0NNN  sentences for training and N00 images for testing
We use  a threshold to control the vocabulary of words
Words that  appear fewer times than the threshold are collapsed into the  <UNK> category
A higher threshold yields a smaller vocabulary because less frequent words are grouped
In our  implementation, the threshold is N, which provides a vocabulary of N0NN0 words
Though not large, PCCD would be a  good start to the new step on aesthetics-critique captioning
 The dataset described above contains pairwised information of images and aesthetic captions
We use another  pairwised data, MSCOCO image captioning dataset, as the  outside data to enhance the performance of photo aesthetic  NNNN    Table N: Statistics of our photo critique captioning dataset
 Aspect # photos # sentences # words  General Impression NNNN NNN0N NNNNNN  Composition & perspective N000 NNNNN NNNNNN  Color & Lighting NNNN NNNN NNN0NN  Subject of photo NNNN N0NN NNNNNN  Depth of field N0NN NNNN NNNNN  Focus NNNN NNNN NNNNN  Use of camera, exposure NNNN NNNN NNNNNN  Total NNNN(union) NNN0N NNNNNNN  critiques
MSCOCO caption dataset contains over NN0K  images and N million captions about objects
In our implementation, the CNN-LSTM model is pre-trained on the  MSCOCO image captioning dataset as an initial model, and  then fine-tuned on PCCD
Compared to training with PCCD  directly, we found that the pre-training is useful to enrich  the object-recognition and sentence-formation capabilities  of our photo aesthetics captioning system, and results in a  better captioning performance
 As MSCOCO is large about object descriptions and  PCCD is relatively small on the aesthetic critiques, when  training either the AO or the CNN-LSTM-WD approach,  the CNN pre-trained on MSCOCO are fixed to keep its  object-description capability, and only the LSTMs are finetuned on PCCD
This strategy is helpful to avoiding overfitting and provides a better subject-identification capability
Similarly, when training the AF approach, the aspectspecific LSTMs are also fixed to avoid over-fitting
The  strategy also benefits the efficiency of training
A computer  mounted with a single Titan X GPU is used in our implementation
It takes about one day to train a single CNNLSTM model with the AO approach, and two days with the  AF approach, respectively
 N.N
Evaluation Criteria  As we are handling a new topic, no existing studies are  available for comparisons
The evaluation criteria suitable  to this new topic become also an issue
Traditional criteria  such as BLEU [NN] and METEOR [N] use simple n-gram  overlaps for evaluation, which produce inaccurate results  because two sentences may share similar meanings without  a high n-gram overlap
Note that there is more than one reference caption that corresponds to a single image in PCCD
 As our dataset (PCCD) is not designed for object recognition, unlike common image captioning datasets, these reference captions are often not synonymous sentences
This  characteristic makes the criterion computing the occurrence  frequency of n-grams in the reference captions (such as  CIDEr [N0]) inconsequential for the evaluation either
 SPICE: A recent advance in automatic evaluation metrics [N] captures more semantics in a photo for the comparison
Though imperfect either, we suggest that the SPICE  criterion presented in [N] is more suitable for the performance evaluation of photo aesthetics critiques
The SPICE  method parses a sentence into a graph, and evaluates the  similarity based on the parsed results between the generated  and reference sentences and then reports the F-score
The  criterion in [N] adopts a variant of the rule-based version  of the Stanford Scene Graph Parser [NN]
A Probabilistic  Context-Free Grammar (PCFG) dependency parser [NN] is  followed by simplifying quantificational modifiers, resolving pronouns and handling plural nouns
It has been shown  that SPICE performs better than traditional metrices such  as BLEU, METEOR, and CIDEr in capturing human judgment over the generated captions
Hence, for an image, we  compute the SPICE (F-score) between the generated caption and all of its reference captions, and then use the highest one as the evaluation score for the image of interest
 Diversity: In contrast to the other captioning problems,  repetition of the captions generated is an issue for photo  aesthetic critiques
For example, if the same sentence “I  like the composition and perspective of this image” is repeated for different photos, people will feel tedious because  the critiques generated for the test photos are not plentiful  enough
However, the problem caused by the repeated or  monotonous captions cannot be reflected by the traditional  captioning evaluation criteria mentioned above
To address  this issue, we propose a measure called diversity, which  takes the near-duplication sentences into consideration to  establish an evaluation measure
We treat two sentences duplicate if the ratio of common words between them is larger  than a threshold (in our implementation N0% is used), and then call the non-duplication rate (one minus the duplication  rate) of the captions generated for the test photos as diversity
This criterion is used to evaluate our photo critiques  problem as well
 N
Experiment Results  In the experiments, we compare the AF approach with  two baseline approaches, AO and CNN-LSTM-WD
First,  we show the results on PCCD based on the automatic evaluation criteria in Section N.N
Then, we compare the performance based on human evaluations and present the results  in Section N.N
Finally, we show the cross-dataset results on  the AVA dataset [NN] in Section N.N
 N.N
Automatic Evaluation  As mentioned above, we use both the SPICE (F-score)  and diverity for automatic evaluation of the test dataset
 Table N shows the SPICE evaluations of compared approaches at generated critiques, and we also report the precision and recall scores for reference
In terms of the SPICE  criterion, CNN-LSTM-WD yields better performance than  AO, and AF performs better than both AO and CNNLSTM-WD
We attribute these results to that the AO apNNNN    Table N: Evaluation of the proposed approaches via the  SPICE criterion
 Method SPICE Precision Recall  CNN-LSTM-WD 0.NNN 0.NNN 0.NNN  AO Approach 0.NNN 0.N0N 0.NNN  AF Approach 0.NN0 0.NNN 0.NNN  proach trains the models by using only the aspect-specific  captions, which are limited and thus performs worse than  the CNN-LSTM-WD approach that uses the whole dataset  for training
In contrast, the AF approach further employs  the hidden annotations as intermediate representation for  training the captioning model, which can yield the best performance
 Then, we use the diversity criterion to evaluate these  methods
The results are shown with the x-axis of Figure N
In contrast to the SPICE criterion, the CNN-LSTMWD approach performs worse than the AO approach on the  diversity criterion
It appears that applying CNN-LSTMWD to the whole dataset that contains captions of different aspects tends to yield more monotonous sentences
The  AF approach that leverages and fuses the hidden annotations of different aspects still performs more favorable than  the other approaches
Figure N combines both the diversity  and SPICE measures in a diagram
As can be seen, the AF  model that integrates the learned sentence representations  can produce more diverse sentences (in terms of diversity)  with higher semantic similarity (in terms of SPICE)
 N.N
Human Evaluation  In human evaluation, we ask the subjects to rate the generated captions on a N-point scale: Good, Common and Bad
 We define the judge that Good means that the caption contains details presented in the picture and its suggestions are  helpful; Common means that the caption is safe but not impressive
As photo critiques are subjective, there are many  comments like “I like your composition” or “Nice photo, I  think your photo is good” which does not describe the detail  of image but expresses critics’s preference
They might not  be thought as wrong captions but to be honest, they are not  useful advice for photographers so we classify these kind  of captions into Common
Bad means the caption contains  obvious error
This setting is similar to the design in [N0]
 Main Results: It is natural that our generated critiques  should be judged by professional photographers
However,  we also care about the comments from common users as  the eventual goal of this task is to help people take satisfied pictures
We find three experts with more than fiveyear experience in photography for expert evaluation, and  also establish an experiment involving five people through  Amazon Mechanical Turk (AMT)
 As shown in Table N, we can find that the AF approach  0.NN  0.NN  0.NN  N0% NN% N00%  S P IC E  Diversity of Generated Sentences  AO  CNN‐LSTM  AF  Better  Better  Desired  Region  Undesired  Region  Figure N: The automatic evaluation results of the three approaches compared, where the X and Y axes represent the  diversity and SPICE, respectively
 achieves the best performance in terms of average score  among the three methods
AF also generates much more  “Good” score judged by both experts and common users
 This demonstrates that our AF approach has more favorable  user experience
We also note that the AO approach has  the largest number of “Common” score as well as the least  “Bad” score
However, as mentioned before, “Common”  critique cannot provide useful advices to user
Hence, the  AO approach is more like a “safe” method but not an ideal  solution for this task in terms of human evaluation
One  possible explanation is that in the AO approach, only the  captions of a single aspect are used without sharing information with the other aspects
 Another noteworthy outcome is the consistency of the  judgements from both experts and common users
This  matches our assumption that experts have stricter standard  to photos critique and thus the average scores judged by  experts are lower than those by common users
However,  the AF model still outperforms the baselines in both testing  groups
Figure N shows some examples of captions generated by the three approaches
 Comparison with Groundtruth: In particular, we ask  some experts to compare a computer generated critique to  the ground-truth captions when presented with an image
 For each image, we calculate the ratio that the generated  critiques are no worse than the ground-truth captions, and  the average result is shown in Table N
We can find that the  AF approach reaches the highest score, which proves that  our method could generate better photo aesthetic critiques
 Novel Sentences Generated: The novel captions (not  present in the training data) generated by AF, AO and CNNLSTM are NN%, NN% and NN% respectively, revealing that  AF inclines to generate diverse and integrated sentences
 Failed Cases: Figure N shows some failed cases generated  by our approach
Although the AF approach performs better  than the other approaches on photo aesthetics captioning,  there is still room for improvement
Our approach can serve  as a baseline for future studies in this area
 NNN0    Table N: Human evaluation by workers on Amazon Mechanical Turk and experts
 Evaluation by users on AMT Evaluation by experts  Good Common Bad Good Common Bad  Method N N N Average Score N N N Average Score  CNN-LSTM-WD NN.0% NN.N% NN.N% N.0N N.N% NN.0% NN.N% N.NN  AO Approach NN.N% NN.N% N.N% N.NN N0.N% NN.0% NN.N% N.NN  AF Approach NN.N% NN.N% NN.0% N.NN NN.N% NN.N% NN.0% N.0N  AF: i like the way the river flows down the centre of the image and draws   the viewers eye into the centre of the frame  AF: i like the way the reflection of the water in the foreground  AF: the color of the flower is fantastic and the tones of the purple are   nice  AF: i like the way the subject is placed in the lower left third of the   frame and the way it is taken gives the image an interesting patterns  AO: the color is great but i would have liked to have seen more of the   image as a result of some of the foliage  AO: i like the subject of this photo  AO: i like the colors in the trees  AO: the color is great the lighting works well but i think it works well   to create a mood that would be a bit more than it would  CNN-LSTM: the composition is good but the composition is a bit too much  CNN-LSTM: i think the composition here is pretty good  CNN-LSTM: i think the composition is very good  CNN-LSTM: i like the composition and perspective of this image  AF: the trees on the right add a nice balancing component  AO: i like the composition but i think it would be improved if the   verticals were parallel with the sides of the photo  CNN-LSTM: i think the composition is pretty good  AF: the composition is very strong i like the rocks in the foreground  AO: i like the composition and the perspective  CNN-LSTM: the composition is good but it is fairly good but i think it   would be better if you had chosen to place the large rock on the bottom  of the frame and  AF: i like the composition and tree line  AO: i like the subject matter which tells a story  CNN-LSTM: i think the composition is pretty good  Figure N: Examples of the critiques generated by the three  models, AF, AO, and CNN-LSTM-WD
 Table N: Comparison of the generated captions with the  ground truths by human
 Method Better Worse Total  CNN-LSTM-WD NN.N% NN.N% N00%  AO Approach NN.N% NN.N% N00%  AF Approach NN.N% NN.N% N00%  N.N
Cross-dataset Results  We apply the AF models to a large-scale aestheticquality-assessment dataset, Aesthetic Visual Analysis  (AVA) [NN]
Unlike PCCD, the AVA dataset has no groundtruth captions, and thus we cannot fine-tune the models  on this dataset
Therefore, the AF model trained by using PCCD is directly applied to the AVA dataset
Some  results on this cross-dataset testing are shown in the respective figure on the supplementary material
From the results,  the tree is a bit dark  the UNK UNK in the foreground is also interesting to UNK the  boat and elements are in balance with the way the trees on the  left of the frame  the horizon is in the bottom of the frame  i like the way the reflection of the sky in the foreground add  to the image  Error: in the foreground ➙ in the background  Error: too many UNK  Error: no tree  Error: in the bottom ➙ in the top  i like the composition of this image  Error: incorrect aesthetic description  Figure N: Some failed-case captions generated by our approach
 it can be seen that the learned model can be used for generating photo aesthetic critiques for other image datasets as  well, which demonstrates the generalization ability of the  proposed approach
 N
Concluding Remarks  In this paper, we study a new problem, namely, captioning different aesthetic aspects of an image
To resolve the  prolem, we introduce the baseline approach, AO, which can  produce aspect-specific captions by using existing CNNLSTM methods
We then extend the AO approach to the AF  approach, which can exploit the hidden annotations learned  from different aspects to generate captions that are more semantically meaningful and diverse
In addition, we show  the outcomes on the proposed PCCD dataset as well as the  quantitative results judged by both automatic criteria and  human evaluation
The results demonstrate the effectiveness and application potential of our approach
 Acknowledgment  This work was supported in part by Ministry of Science  and Technology under the grants MOST N0N-NNNN-E-00N0NN-MYN, MOST N0N-NNNN-E-00N-00N and MOST N0NNNNN-E-00N-0NN
 NNNN    References  [N] P
Anderson, B
Fernando, M
Johnson, and S
Gould
Spice:  Semantic propositional image caption evaluation
In ECCV,  N0NN
N  [N] L
Anne Hendricks, S
Venugopalan, M
Rohrbach,  R
Mooney, K
Saenko, and T
Darrell
Deep compositional captioning: Describing novel object categories without paired training data
In CVPR, N0NN
N, N  [N] D
Bahdanau, K
Cho, and Y
Bengio
Neural machine translation by jointly learning to align and translate
In ICLR,  N0NN
N  [N] S
Banerjee and A
Lavie
Meteor: An automatic metric for  mt evaluation with improved correlation with human judgments
In ACL workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, N00N
N  [N] R
Datta, D
Joshi, J
Li, and J
Wang
Studying aesthetics  in photographic images using a computational approach
In  ECCV, N00N
N  [N] J
Devlin, H
Cheng, H
Fang, S
Gupta, L
Deng, X
He,  G
Zweig, and M
Mitchell
Language models for image captioning: The quirks and what works
In ACL, N0NN
N  [N] S
Dhar, V
Ordonez, and T
L
Berg
High level describable attributes for predicting aesthetics and interestingness
 In CVPR, N0NN
N  [N] J
Donahue, L
Anne Hendricks, S
Guadarrama,  M
Rohrbach, S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual  recognition and description
In CVPR, N0NN
N, N  [N] H
Fang, S
Gupta, F
Iandola, R
K
Srivastava, L
Deng,  P
Dollár, J
Gao, X
He, M
Mitchell, J
C
Platt, et al
From  captions to visual concepts and back
In CVPR, N0NN
N, N  [N0] T.-H
K
Huang, F
Ferraro, N
Mostafazadeh, I
Misra,  A
Agrawal, J
Devlin, R
Girshick, X
He, P
Kohli, D
Batra,  et al
Visual storytelling
In NAACL, N0NN
N  [NN] J
Johnson, A
Karpathy, and L
Fei-Fei
Densecap: Fully  convolutional localization networks for dense captioning
In  CVPR, N0NN
N, N  [NN] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, N0NN
 N, N, N  [NN] D
Klein and C
D
Manning
Accurate unlexicalized parsing
 In ACL, N00N
N  [NN] S
Kong, X
Shen, Z
Lin, R
Mech, and C
Fowlkes
Photo  aesthetics ranking network with attributes and content adaptation
In ECCV, N0NN
N, N, N  [NN] J
Lu, C
Xiong, D
Parikh, and R
Socher
Knowing when  to look: Adaptive attention via a visual sentinel for image  captioning
In CVPR, N0NN
N  [NN] X
Lu, Z
Lin, H
Jin, J
Yang, and J
Z
Wang
Rapid: Rating  pictorial aesthetics using deep learning
In ACM MM, N0NN
 N, N, N  [NN] X
Lu, Z
Lin, X
Shen, R
Mech, and J
Z
Wang
Deep  multi-patch aggregation network for image style, aesthetics,  and quality estimation
In ICCV, N0NN
N, N, N  [NN] M.-T
Luong, H
Pham, and C
D
Manning
Effective approaches to attention-based neural machine translation
In  EMNLP, N0NN
N  [NN] L
Mai, H
Jin, and F
Liu
Composition-preserving deep  photo aesthetics assessment
In CVPR, N0NN
N, N  [N0] J
Mao, J
Huang, A
Toshev, O
Camburu, A
Yuille, and  K
Murphy
Generation and comprehension of unambiguous  object descriptions
In CVPR, N0NN
N, N  [NN] J
Mao, X
Wei, Y
Yang, J
Wang, Z
Huang, and A
L
 Yuille
Learning like a child: Fast novel visual concept learning from sentence descriptions of images
In ICCV, N0NN
N,  N  [NN] J
Mao, W
Xu, Y
Yang, J
Wang, Z
Huang, and A
Yuille
 Deep captioning with multimodal recurrent neural networks  (m-rnn)
In ICLR, N0NN
N, N  [NN] L
Marchesotti, F
Perronnin, D
Larlus, and G
Csurka
Assessing the aesthetic quality of photographs using generic  image descriptors
In ICCV, N0NN
N  [NN] N
Murray, L
Marchesotti, and F
Perronnin
AVA: A largescale database for aesthetic visual analysis
In CVPR, N0NN
 N, N, N, N  [NN] P
Pan, Z
Xu, Y
Yang, F
Wu, and Y
Zhuang
Hierarchical  recurrent neural encoder for video representation with application to captioning
In CVPR, N0NN
N  [NN] K
Papineni, S
Roukos, T
Ward, and W.-J
Zhu
Bleu: a  method for automatic evaluation of machine translation
In  ACL, N00N
N  [NN] M
Pedersoli, T
Lucas, C
Schmid, and J
Verbeek
Areas of attention for image captioning
arXiv preprint  arXiv:NNNN.0N0NN, N0NN
N  [NN] S
Schuster, R
Krishna, A
Chang, L
Fei-Fei, and C
D
 Manning
Generating semantically precise scene graphs  from textual descriptions for improved image retrieval
In  EMNLP Nth Workshop on Vision and Language, N0NN
N  [NN] M
Tapaswi, M
Bauml, and R
Stiefelhagen
BookNmovie:  Aligning video scenes with book chapters
In CVPR, N0NN
 N  [N0] R
Vedantam, C
Lawrence Zitnick, and D
Parikh
Cider:  Consensus-based image description evaluation
In CVPR,  N0NN
N  [NN] S
Venugopalan, L
A
Hendricks, M
Rohrbach, R
Mooney,  T
Darrell, and K
Saenko
Captioning images with diverse  objects
arXiv preprint arXiv:NN0N.0NNN0, N0NN
N  [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In CVPR, N0NN
N, N  [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
Courville, R
Salakhutdinov, R
S
Zemel, and Y
Bengio
Show, attend and tell:  Neural image caption generation with visual attention
In  ICML, N0NN
N, N  [NN] R
Xu, C
Xiong, W
Chen, and J
J
Corso
Jointly modeling deep video and compositional text to bridge vision and  language in a unified framework
In AAAI, N0NN
N  [NN] M.-C
Yeh and Y.-C
Cheng
Relative features for photo quality assessment
In ICIP, N0NN
N  [NN] Q
You, H
Jin, Z
Wang, C
Fang, and J
Luo
Image captioning with semantic attention
In CVPR, N0NN
N, N  NNNN    [NN] H
Yu, J
Wang, Z
Huang, Y
Yang, and W
Xu
Video  paragraph captioning using hierarchical recurrent neural networks
In CVPR, N0NN
N  [NN] Y
Zhu, R
Kiros, R
Zemel, R
Salakhutdinov, R
Urtasun,  A
Torralba, and S
Fidler
Aligning books and movies: Towards story-like visual explanations by watching movies and  reading books
In CVPR, N0NN
N  NNNNSGN: Sequential Grouping Networks for Instance Segmentation   SGN: Sequential Grouping Networks for Instance Segmentation  Shu Liu† Jiaya Jia†,[ Sanja Fidler‡ Raquel Urtasun§,‡  †The Chinese University of Hong Kong [Youtu Lab, Tencent §Uber Advanced Technologies Group ‡University of Toronto  {sliu, leojia}@cse.cuhk.edu.hk {fidler, urtasun}@cs.toronto.edu  Abstract  In this paper, we propose Sequential Grouping Networks (SGN) to tackle the problem of object instance segmentation
SGNs employ a sequence of neural networks,  each solving a sub-grouping problem of increasing semantic complexity in order to gradually compose objects out  of pixels
In particular, the first network aims to group  pixels along each image row and column by predicting  horizontal and vertical object breakpoints
These breakpoints are then used to create line segments
By exploiting two-directional information, the second network groups  horizontal and vertical lines into connected components
 Finally, the third network groups the connected components into object instances
Our experiments show that our  SGN significantly outperforms state-of-the-art approaches  in both, the Cityscapes dataset as well as PASCAL VOC
 N
Introduction  The community has achieved remarkable progress for  tasks such as object detection [NN, N0] and semantic segmentation [NN, N] in recent years
Research along these lines  opens the door to challenging life-changing tasks, including  autonomous driving and personalized robotics
 Instance segmentation is a task that jointly considers object detection and semantic segmentation, by aiming to predict a pixel-wise mask for each object in an image
The  problem is inherently combinatorial, requiring us to group  sets of pixels into coherent components
Occlusion and  vastly varying number of objects across scenes further increase the complexity of the task
In street scenes, such as  those in the Cityscapes dataset [N], current methods merely  reach N0% accuracy in terms of average precision, which is  still far from satisfactory
 Most of the instance segmentation approaches employ a  two-step process by treating the problem as a foregroundbackground pixel labeling task within the object detection  boxes [NN, NN, N, NN, N0]
Instead of labeling pixels, [N] predicts a polygon outlining the object instance using a Recurrent Neural Network (RNN)
In [NN, N0], large patches are  exhaustively extracted from an image and a Convolutional  Neural Network (CNN) is trained to predict instance labels inside each patch
A dense Conditional Random Field  (CRF) is then used to get consistent labeling of the full image
In [NN, NN], an RNN is used to produce one object mask  per time step
The latter approaches face difficulties on images of street scenes which contain many objects
More recently, [N] learned a convolutional net to predict the energy  of the watershed transform
Its complexity does not depend  on the number of objects in the scene
 In this paper, we break the task of object instance segmentation into several sub-tasks that are easier to tackle
 We propose a grouping approach that employs a sequence  of neural networks to gradually compose objects from simpler constituents
In particular, the first network aims to  group pixels along each image row and column by predicting horizontal and vertical object breakpoints
These are  then used to create horizontal and vertical line segments
 The second neural network groups these line segments into  connected components
The last network merges components into coherent object instances, thus solving the problem of object splitting due to occlusion
Due to its sequential nature, we call our approach Sequential Grouping Networks (SGN)
We evaluate our method on the challenging  Cityscapes dataset
SGN significantly outperforms state-ofthe-art, achieving a N% absolute and NN% relative boost in  accuracy
We also improve over state-of-the-art on PASCAL VOC for general object instance segmentation which  further showcases the strengths of our proposed approach
 N
Related Work  The pioneering work of instance segmentation [NN, N]  aimed at both classifying object proposals as well as labeling an object within each box
In [NN, N0], highquality mask proposals were generated using CNNs
Similarly, MNC [N] designed an end-to-end trainable multi-task  network cascade to unify bounding box proposal generation, pixel-wise mask proposal generation and classificaNNNNN    horizontal breakpoint map horizontal line segments  vertical breakpoint map vertical line segments  LineNet MergerNet  input image instance   segmentation components  Figure N
Sequential Grouping Networks (SGN): We first predict breakpoints
LineNet groups them into connected components, which  are finally composed by the MergerNet to form our final instances
 tion
SAIS [NN] improved MNC by learning to predict distances to instance boundaries, which are then decoded into  high-quality masks by a set of convolution operations
A  recursive process was proposed in [NN] to iteratively refine  the predicted pixel-wise masks
Recently, [N] proposed to  predict polygons outlining each object instance which has  the advantage of efficiently capturing object shape
 MPA [NN] modeled objects as composed of generic parts,  which are predicted in a sliding window fashion and then  aggregated to produce instances
IIS [N0] is an iterative approach to refine the instance masks
In [NN], the authors  utilized a fully convolutional network by learning to combine different relative parts into an instance
Methods of  [NN, N0] extracted patches from the image and used a CNN  to directly infer instance IDs inside each patch
A CRF is  then used to derive globally consistent labels of the image
 In [N, N], a CRF is used to assign each pixel to an object  detection box by exploiting semantic segmentation maps
 Similarly, PFN [NN] utilized semantic segmentation to predict the bounding box each pixel belongs to
Pixels are then  grouped into clusters based on the distances to the predicted  bounding boxes
In [NN], the authors predict the direction to  the instance’s center for each pixel, and exploits templates  to infer the location of instances on the predicted angle map
 Recently, Bai and Urtasun [N] utilized a CNN to learn an  energy of the watershed transform
Instances naturally correspond to basins in the energy map
It avoids the combinatorial complexity of instance segmentation
In [NN], semantic segmentation and object boundary prediction were exploited to separate instances
Different types of label transformation were investigated in [NN]
Pixel association was  learned in [NN] to differentiate between instances
 Finally, RNN [NN, NN] was used to predict an object label  at each time step
However, since RNNs typically do not  perform well when the number of time steps is large, these  methods have difficulties in scaling up to the challenging  multi-object street scenes
 Here, we propose a new type of approach to instance  segmentation by exploiting several neural networks in a sequential manner, each solving a sub-grouping problem
 N
Sequential Grouping Networks  In this section, we present our approach to object instance segmentation
Following [N, NN], we utilize semantic segmentation to identify foreground pixels, and restrict  our reasoning to them
We regard instances as composed of  breakpoints that form line segments, which are then combined to generate full instances
Fig
N illustrates our model
 We first introduce the network to identify breakpoints, and  show how to use them to group pixels into lines in Subsec
N.N
In Subsec
N.N, we propose a network that groups  lines into components, and finally the network to group  components into instances is introduced in Subsec
N.N
 N.N
Predicting Breakpoints  Our most basic primitives are defined as the pixel locations of breakpoints, which, for a given direction, represent  the beginning or end of each object instance
Note that we  reason about the breakpoints in both the horizontal and vertical direction
For the horizontal direction, computing the  starting points amounts to scanning the image from left to  right, one row at a time, recording the change points where  a new instance appears
For the vertical direction, the same  process is conducted from top to bottom
As a consequence,  the boundary between instances is considered as a starting  point
The termination points are then the pixels where an  instance has a boundary with the background
Note that  this is different from predicting standard boundaries as it  additionally encodes the direction where the interior of the  instance is
 We empirically found that introducing two additional  labels encoding the instance interior as well as the background is helpful to make the end-point prediction sharper
 Each pixel in an image is thus labeled with N labels encoding either background, interior, starting point or a termination point
We refer the reader to Fig
N for an example
 Network Structure We exploit a CNN to perform this  pixel-wise labeling task
The network takes the original image as input and predicts two label maps, one per direction  as shown in Fig
N(a)
Our network is based on DeeplabNNNN    (a) (b)  Figure N
Illustration of (a) network structure for predicting breakpoints, and (b) the fusion operation
 (a) (b) decoding direction  Figure N
Line decoding process
Green and red points are starting and termination ones
Scanning from left to right, there is no  more line segment in the area pointed by the black arrow in (a)  due to erroneous point detection
The reversal scanning in (b) gets  new line hypothesis in this area, shown by orange line segments
 LargeFOV [N]
We use a modified VGGNN [NN], and make  it fully convolutional as in FCN [NN]
To preserve precise  localization information, we remove poolN and poolN layers
To enlarge the receptive field, we make use of dilated  convolutions [NN, N] in the convN and convN layers
 Similar to the methods in [N0, NN, NN], we augment the  network by connecting lower layers to higher ones in order  to capture fine details
In particular, we fuse information  from convN N, convN N and convN N layers, as shown in Fig
 N (b)
To be more specific, we first independently filter the  input feature maps through NNN filters of size N × N, which are then concatenated
We then utilize another set of NNN  filters of size N× N to decrease the feature dimension
After fusion with convN N, the feature map is downscaled by a  factor of N
Predictions for breakpoint maps are then made  by two independent branches on top of it
 Learning Predicting breakpoints is hard since they are  very sparse, making the distribution of labels unbalanced  and dominated by the background and interior pixels
To  mitigate this effect, similar to HED [NN], we re-weight the  cross-entropy loss based on inverse of the frequency of each  class in the mini-batch
 N.N
Grouping Breakpoints into Line Segments  Since the convolutional net outputs breakpoints that span  over multiple consecutive pixels, we use a morphological  operator to create boundaries with one pixel width
We further augment the set of breakpoints with the boundaries in  the semantic segmentation prediction map to ensure we do  not miss any boundary
We then design an algorithm that reverses the process of generating breakpoints from instance  segmentation in order to create line segments
To create  horizontal lines, we slide from left to right along each row,  and start a new line when we hit a starting point
Lines are  terminated when we hit an end point or a new starting point
 The latter arises at the boundary of two different instances
 Fig
N (a) illustrates this process
To create vertical lines, we  perform similar operations but slide from top to bottom
 This simple process inevitably introduces errors if there  are false termination points inside instances
As shown in  Fig
N (a), the area pointed by the black arrow is caused by  false termination points
To handle this issue, we augment  the generated line segments by decoding in the reverse direction (right to left for horizontal lines and bottom to top  for vertical ones) as illustrated in Fig
N (b)
Towards this  goal, we identify the starting points lying between instances  by counting the consecutive number of starting points
We  then switch starting and termination points for all points that  are not double starting points and decode in the reverse order
As shown in Fig
N (b), this simple process gives us the  additional lines (orange) necessary to complete the instance
 N.N
Grouping Lines into Connected Components  The next step is to aggregate lines to create instances that  form a single connected component
We utilize the horizontal lines as our elements and recursively decide whether to  merge a line into an existing component
Note that this is an  efficient process since there are much fewer lines compared  to raw pixels
On average, the number of operations that  need to be made is NN0N per image on Cityscapes and N0NN  on PASCAL VOC
Our merging process is performed by  a memory-less recurrent network, which we call LineNet
 LineNet scans the image from top to bottom and sequentially decides whether to merge the new line into one of the  existing neighboring instances (i.e., instances that touch the  line in at least one pixel)
An example is shown in Fig
N (a)  where Ok is an instance and si is a line segment
 NNNN    (a) (b) (c) (d) (e) (f)  Figure N
(a) Context area is highlighted in the dashed red bounding box
(b) Orange vertical rectangles represent vertical line segments
 (c) First channel
(d) Second channel where Ok and si have the same semantic class
(e) The situation when Ok and si are not in the same  class
(f) Third channel
 Network Structure Let si be the line segment we consider and Ok is an instance in Fig
N (a)
LineNet uses as  context a small number of rows h on top of si encoding the  history of already merged instances, as well as a small set of  rows f below si encoding the future possible merges
We  restrict the horizontal context to be the minimum interval  containing the instance candidate Ok and the line si
This  is shown as the dashed red area in Fig
N (a)
We also pad  zeros to make the window horizontally centered at si and  resize the window horizontally to have a fixed size
 The input to LineNet contains N channels
The first is  a boolean map with ones in the pixels belonging to si or  Ok
The second channel contains ones for pixels in Ok that  are labeled in the same semantic class as the majority of the  pixels in line si
The third channel is a boolean map showing pixels in Ok that share the same vertical line segment  with si
The first three channels are illustrated in Fig
N  (c)-(f)
Additionally, we append N channels containing the  interior and breakpoint probability maps in the vertical and  horizontal directions
LineNet is a very small net, consisting of two shared fully-connected layers and class-specific  classifiers with the sigmoid function
The output for each  semantic class is the probability of merging the line with  the candidate instance
 Learning We use standard cross-entropy loss to train  LineNet, where a line should be merged with an existing  instance if the majority of pixels in the line and instance  belong to the same ground-truth instance
 N.N
Merging Fragmented Instances  Note that many instances are composed of more than one  connected component
This is the case for example of a car  that is occluded by a pole, as shown in Fig
N (a)
This  issue is common in urban scenes
On average N.N instances  per image are fragmented in Cityscapes
In PASCAL, this  number is much smaller with only 0.N instance per image
 In order to deal with fragmented instances, we design a  MergerNet that groups these components to form the final  set of object instances
Note that most instance segmentation algorithms cannot tackle the fragmentation problem  [N, NN, NN, NN, N0]
Our MergerNet can thus also be used  to enhance these approaches
In particular, as merging candidates for each segment we choose all segments that are  closer than a fixed threshold
We use N00 pixels in practice
 (a) (b) (c) (d)  Figure N
(a) A car that is occluded by a pole resulting in fragmented parts
(b) Bounding boxes containing the current pair, i.e.,  two green segments
(c) First two input channels
(d) Sixth and  seventh input channels to the MergerNet
 We then order these segments by size, and utilize a memoryless recurrent net to further decide whether to merge or not
 If a merge occurs, we recompute the set of neighbors and  repeat this process
 Network Architecture Our MergerNet takes as input N0  channels
To generate the first N channels, we fit a tight  square bounding box containing candidate regions (blue  box in Fig
N (b)) and generate the representation within  this box
The first channel is a boolean map with ones for  pixels belonging to either instance
The second channel is  a boolean map with ones indicating other segments lying  in the box
This provides information about other possible merges
Fig
N (c) shows the first two channels
The  next three channels are simply the image RGB values in  this bounding box
We resize all channels to a fixed size  of NNN× NNN
The remaining N channels are generated in a similar way except that they are from a bounding box doubled the size (purple box in Fig
N (b))
The sixth and seventh channels are illustrated in Fig
N (d)
The network is  small, consisting of N convolution layers, one shared fullyconnected layer and class-specific classifiers with sigmoid  function to produce the final merge probability
 Learning We generate training samples for MergerNet  by running inference on the training images and use the  ground-truth instances to infer whether the different components should be merged or not
In particular, we merge  them if the majority of pixels are part of the same GT instance
We use standard cross-entropy as the loss function  for each class
The result of this merging process is our final  instance segmentation
We define the semantic label of each  instance as the class that the majority of pixels are labeled
 NNNN    Method person rider car truck bus train mcycle bicycle  Uhrig et al
[NN] NN.N NN.N NN.N N.N N.N N.N N.N N.N  RecAttend [NN] N.N N.N NN.N N.0 NN.N N.N N.N N.N  Levinkov et al
[NN] N.N N.N NN.0 N.N N0.N N0.N N.N N.N  InstanceCut [NN] N0.0 N.0 NN.N NN.0 NN.N NN.N N.N N.N  SAIS [NN] NN.N NN.N NN.N NN.0 NN.N NN.0 N0.N N.N  DWT [N] NN.N NN.N NN.N NN.N NN.0 NN.N NN.N N.0  DIN [N] NN.N NN.N NN.N N0.N N0.0 NN.N NN.N N0.N  Our SGN NN.N N0.N NN.N NN.N NN.N N0.N NN.N NN.N  Method AP AP N0% AP N00m AP N0m  Uhrig et al
[NN] N.N NN.N NN.N NN.N  RecAttend [NN] N.N NN.N NN.N N0.N  Levinkov et al
[NN] N.N NN.N NN.N N0.N  InstanceCut [NN] NN.0 NN.N NN.N NN.N  SAIS [NN] NN.N NN.N NN.N NN.0  DWT [N] NN.N NN.N NN.N NN.N  DIN [N] N0.0 NN.N NN.N NN.N  Our SGN NN.0 NN.N NN.N NN.N  Table N
AP results on Cityscapes test
The entries with the best performance are bold-faced
 Method person rider car truck bus train motorcycle bicycle AP AP N0%  DWT-non-ranking [N] NN.N NN.N NN.N NN.N NN.N NN.N NN.N N.N N0.N DWT-ranking [N] NN.N NN.N NN.N NN.N NN.N NN.N NN.N N.N NN.N SAIS [NN] - - - - - - - - - NN.N  Our SGN NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.0 NN.N NN.N  Table N
AP results on Cityscapes val
The entries with the best performance are bold-faced
 N
Experimental Evaluation  We evaluate our method on the challenging dataset  Cityscapes [N] as well as PASCAL VOC N0NN [NN]
We  focus our experiments on Cityscapes as it is much more  challenging
On average, it contains NN object instances per  image vs N.N in PASCAL
 Dataset The Cityscapes dataset [N] contains imagery of  complex urban scenes with high pixel resolution of N, 0NN× N, 0NN
There are N, 000 images with high-quality annotations that are split into subsets of N, NNN train, N00 val  and N, NNN test images, respectively
We use images in the  train subset with fine labels to train our networks
For the  instance segmentation task, there are N classes, including  different categories of people and vehicles
Motion blur,  occlusion, extreme scale variance and imbalanced class distribution make this dataset extremely challenging
 Metrics The metric used by Cityscapes is Average Precision (AP)
It is computed at different thresholds from 0.N  to 0.NN with step-size of 0.0N followed by averaging
We  report AP at 0.N IoU threshold and AP within a certain  distance
As pointed in [N, N], AP favors detection-based  methods
Thus we also report the Mean Weighted Coverage  (MWCov) [NN, NN], which is the average IoU of prediction  matched with ground-truth instances weighted by the size  of the ground-truth instances
 Implementation Details For our breakpoint prediction network, we initialize convN to convN layers from  VGGNN [NN] pre-trained on Imagenet [NN]
We use random  initialization for other layers
The learning rates to train the  breakpoint prediction network, LineNet and the MergerNet  are set to N0−N, N0−N and N0−N, respectively
We use SGD  with momentum for training
Following the method of [N],  we use the “poly” policy to adjust the learning rate
We train  the breakpoint prediction network for N0k iterations, while  LineNet and MergerNet are trained using N0k iterations
To  alleviate the imbalance of class distribution for LineNet and  MergerNet, we sample training examples in mini-batches  by keeping equal numbers of positives and negatives
Following [N], we remove small instances and use semantic  scores from semantic segmentation to rank predictions for  {train, bus, car, truck}
For other classes, scores are set to N
By default, we use semantic segmentation prediction  from PSP [NN] on Cityscapes and LRR [NN] on PASCAL  VOC
We also conduct an ablation study of how the quality  of semantic segmentation influences our results
 Comparison to State-of-the-art As shown in Table N,  our approach significantly outperforms other methods on all  classes
We achieve an improvement of N% absolute and  NN% in relative performance compared to state-of-the-art  reported on the Cityscapes website, captured at the moment  of our submission
We also report results on the validation  set in Table N, where the improvement is even larger
 Influence of Semantic Segmentation We investigate the  influence of semantic segmentation in our instance prediction approach
We compare the performance of our approach when using LRR [NN], which is based on VGG-NN,  with PSP [NN], which is based on Resnet-N0N
As shown  in Table N, we achieve reasonable results using LRR, however, much better results are obtained when exploiting PSP
 Results improve significantly in both cases when using the  MergerNet
Note that LineNet and MergerNet are not finetuned for LRR prediction
 Influence of Size As shown in Fig
N, as expected, small  instances are more difficult to detect than the larger ones
 Influence of LineNet Parameters The contextual information passed to LineNet is controlled by the number of  history rows h, as well as the number of rows f encoding  the future information
As shown in Table N, LineNet is not  NN00    LRR [NN] PSP [NN] MergerNet AP AP N0% MWCov  X N0.0 NN.N NN.N  X X NN.N N0.N NN.N  X NN.N NN.N N0.N  X X NN.N NN.N NN.N  Table N
Cityscapes val with different semantic segm
methods
 Metric hNfN hNfN hNfN hNfN hNfN hNfN hNfN hNfN hNfN  AP NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0  AP N0% NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  MWCov NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  Table N
Cityscapes val results in terms of AP, AP N0%, MWCov
 sensitive to the context parameters while local information  is sufficient
In the following experiments, we select the  entry “hNfN” by considering both AP and MWCov
 Heuristic Methods vs LineNet We compare LineNet to  two heuristic baseline methods
The first takes the union  of vertical and horizontal breakpoints and class boundaries  from PSP
We thin them into one pixel width to get the instance boundary map
In the foreground area defined via  the PSP semantic segmentation map, we set the instance  boundary as 0 and then take connected components as inferred instances
Post-processing steps, such as removing  small objects and assigning scores, are exactly the same as  in our approach
We name this method “con-com”
 Our second baseline is called “heuristic”
Instead of using LineNet, we simply calculate a relationship between  two neighboring line segments (current line segment and  neighboring line segment in previous row) to make a decision
The value we compute includes the IoU value, ratio of  the overlaps with the other line segment and ratio of vertical  line segments connecting them in the overlapping area
If  each value and their summation are higher than the chosen  thresholds, we merge the two line segments
This strategy  simply makes decisions based on the current line segment  pair – no training is needed
 As shown in Table N, on average, the heuristic strategy outperforms the simple connected component method  in terms of all metrics
This clearly shows the benefit of  using line segments
LineNet outperforms both heuristic  methods, demonstrating the advantage of incorporating a  network to perform grouping
The connected component  method performs quite well on classes such as car and bus,  but performs worse on person and motorcycle
This suggests that boundaries are more beneficial to instances with  compact shape than to those with complex silhouettes
Our  approach, however, works generally well on both, complex  and compact shapes, and further enables networks to correct  errors that exist in breakpoint maps
 Influence of MergerNet Parameters A parameter of the  MergerNet is the max distance between foreground regions  Method APr  APravg0.N 0.N 0.N 0.N 0.N  PFN [NN] NN.N NN.N NN.N NN.N NN.N NN.N  Arnab et al
[N] NN.N NN.N NN.N NN.N N0.N NN.N  MPA N-scale [NN] NN.N NN.N NN.N NN.N NN.N NN.N  DIN [N] NN.N NN.N NN.N NN.N NN.N NN.N  Our SGN NN.N NN.N NN.N NN.N NN.N NN.N  Table N
APr on PASCAL val
 in a candidate pair
To set the value, we first predict instances with LineNet on the train subset and compute statistics
We show recall of pairs that need to be merged vs  distance in Fig
N(a) and the number of pairs with distance  smaller than a threshold vs distance in Fig
N(b)
We plot the  performance with respect to different maximum distances  used to merge in Fig
N(c) and (d)
Distance 0 means that  the MergerNet is not utilized
The MergerNet with different  parameters consistently improves performance
The results  show that the MergerNet with NN0 pixels as the maximum  distance performs slightly worse than MergerNet with N0  and N00 pixels
We hypothesize that with larger distances,  more false positives confuse MergerNet during inference
 Visual Results We show qualitative results of all intermediate steps in our approach on Cityscapes val in Fig
N
 Our method produces high-quality breakpoints, instance interior, line segments and final results, for objects of different  scales, classes and occlusion levels
The MergerNet works  quite well as shown in Fig
N as well as on the train object in  Fig
N(b)
Note that predictions with IoU higher than 0.N are  assigned colors of their corresponding ground-truth labels
 Failure Modes Our method may fail if errors exist in the  semantic segmentation maps
As shown in Fig
N(b), a small  part of the train is miss-classified by PSP
So the train is  broken into two parts
Our method may also miss extremely  small instances, such as some people in Fig
N(e) and (f)
 Further, when several complex instances are close to each  other, we may end up grouping them as shown in Fig
N(g)  and (h)
The MergerNet sometimes also aggregates different instances such as the two light green cars in Fig
N(a)
 Results on PASCAL VOC We also conduct experiments  on PASCAL VOC N0NN [NN], which contains N0 classes
 As is common practice, for training images, we additionally use annotations from the SBD dataset [NN], resulting in  N0, NNN images with instance annotation
For the val subset,  we used N, NNN images from VOC N0NN val set
There is no  overlap between training and validation images
Following  common practice, we compare with state-of-the-art on the  val subset since there is no held-out test set
Note that the  LRR model we use is pre-trained on MS COCO [NN], which  is also used by DIN [N] for pre-training
 We use APr [NN] as the evaluation metric, representing  the region AP at a specific IoU threshold
Following [NN, N],  NN0N    input image hori
breakpoint map vert
breakpoint map hori
line segments  vert
line segments our result without MergerNet our result with MergerNet ground-truth  input image hori
breakpoint map vert
breakpoint map hori
line segments  vert
line segments our result without MergerNet our result with MergerNet ground-truth  Figure N
Qualitative results of all intermediate results and final prediction
 Method person rider car truck bus train motorcycle bicycle Average  con-com NN.N / N0.N N.N / NN.N N0.0 / NN.N NN.N / NN.N NN.N / NN.N NN.N / NN.N N.N / NN.N N.N / NN.N NN.N / NN.N  heuristic N0.N / NN.N NN.N / NN.0 NN.N / NN.N NN.0 / N0.N NN.N / NN.N NN.N / NN.N N0.N / NN.N N.N / N0.N NN.N / N0.N  Linenet N0.N / NN.N NN.N / NN.N NN.N / NN.N NN.0 / NN.N NN.N / NN.N NN.N / NN.N NN.N / NN.N N.N / NN.N NN.N / NN.N  Table N
Results on Cityscapes val in terms of AP / MWCov
 0  0.N  0.N  0.N  0.N  N  N N00 N0000 N000000  Figure N
IoU as a function of ground-truth sizes
 we average APr at IoU threshold ranging from 0.N to 0.N  with step-size 0.N (instead of 0.N to 0.N)
We believe that  APr at higher IoU thresholds are more informative
 As shown in Table N our method outperforms DIN [N]  by N.N points in terms of APravg
We also achieve better  performance for IoU higher than 0.N
This result demonstrates the quality of masks generated by our method
Our  method takes about N.Ns with LineNet “hNfN” and N.Ns with  LineNet “hNfN” per image using one Titan X graphics card  and an Intel Core iN N.N0GHZ CPU using a single thread on  PASCAL VOC
This includes the CPU time
 N
Conclusion  We proposed Sequential Grouping Networks (SGN) for  object instance segmentation
Our approach employs a  sequence of simple networks, each solving a more complex grouping problem
Object breakpoints are composed  to create line segments, which are then grouped into connected components
Finally, the connected components are  grouped into full objects
Our experiments showed that our  approach significantly outperforms existing approaches on  the challenging Cityscapes dataset and works well on PASCAL VOC
In our future work, we plan to make our framework end-to-end trainable
 N
Acknowledgments  This work is in part supported by NSERC, CFI, ORF, ERA,  CRC as well as Research Grants Council of the Hong Kong SAR  (project No
NNNNNN)
We also acknowledge GPU donations from  NVIDIA
 NN0N    0  0.N  0.N  0.N  0.N  N  0 N0 N00 NN0 N00 NN0 N00  R e c a ll  Distance  Pair Recall versus Distance  0  N0  N0  N0  N0  N00  NN0  0 N0 N00 NN0 N00 NN0 N00 # P  ai r   p er   I m  
 Distance  #Pair per Im
versus Distance  N0  NN  NN  NN  NN  NN  0 N0 N00 NN0  M W  C o v  Distance  MWCov versus Distance  (a) (b) (c) (d)  Figure N
(a) Recall of merge pairs as a function of max distance
(b) Number of pairs with distance of foreground regions smaller than a  distance
(c) AP with respect to different maximum distances
(d) MWCov with respect to different maximum distances
 (a)  (b)  (c)  (d)  (e)  (f)  (g)  (h)  input image semantic segmentation [NN] our results ground-truth Figure N
Qualitative results of our method
 NN0N    References  [N] A
Arnab and P
H
Torr
Pixelwise instance segmentation  with a dynamically instantiated network
In CVPR, N0NN
N,  N, N, N, N  [N] A
Arnab and P
H
S
Torr
Bottom-up instance segmentation  using deep higher-order crfs
CoRR, N0NN
N, N  [N] M
Bai and R
Urtasun
Deep watershed transform for instance segmentation
CoRR, N0NN
N, N, N, N  [N] L
Castrejon, K
Kundu, R
Urtasun, and S
Fidler
Annotating object instances with a polygon-rnn
In CVPR, N0NN
N,  N  [N] L
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and A
L
 Yuille
Deeplab: Semantic image segmentation with deep  convolutional nets, atrous convolution, and fully connected  crfs
CoRR, N0NN
N, N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected crfs
In ICLR, N0NN
N,  N  [N] M
Cordts, M
Omran, S
Ramos, T
Rehfeld, M
Enzweiler,  R
Benenson, U
Franke, S
Roth, and B
Schiele
The  cityscapes dataset for semantic urban scene understanding
 In CVPR, N0NN
N, N, N  [N] J
Dai, K
He, and J
Sun
Convolutional feature masking for  joint object and stuff segmentation
In CVPR, N0NN
N  [N] J
Dai, K
He, and J
Sun
Instance-aware semantic segmentation via multi-task network cascades
CVPR, N0NN
N  [N0] J
Dai, Y
Li, K
He, and J
Sun
R-FCN: object detection via  region-based fully convolutional networks
CoRR, N0NN
N  [NN] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
IJCV, N0N0
N, N  [NN] C
Fu, W
Liu, A
Ranga, A
Tyagi, and A
C
Berg
DSSD :  Deconvolutional single shot detector
CoRR, N0NN
N  [NN] G
Ghiasi and C
C
Fowlkes
Laplacian reconstruction and  refinement for semantic segmentation
CoRR, N0NN
N, N  [NN] B
Hariharan, P
Arbelaez, L
Bourdev, S
Maji, and J
Malik
 Semantic contours from inverse detectors
In ICCV, N0NN
N  [NN] B
Hariharan, P
Arbeláez, R
Girshick, and J
Malik
Simultaneous detection and segmentation
In ECCV
N0NN
N, N  [NN] Z
Hayder, X
He, and M
Salzmann
Shape-aware instance  segmentation
CoRR, N0NN
N, N, N  [NN] L
Jin, Z
Chen, and Z
Tu
Object detection free instance  segmentation with labeling transformations
CoRR, N0NN
N  [NN] A
Kirillov, E
Levinkov, B
Andres, B
Savchynskyy, and  C
Rother
Instancecut: from edges to instances with multicut
CoRR, N0NN
N, N, N  [NN] E
Levinkov, S
Tang, E
Insafutdinov, and B
Andres
Joint  graph decomposition and node labeling by local search
 CoRR, N0NN
N  [N0] K
Li, B
Hariharan, and J
Malik
Iterative instance segmentation
CVPR, N0NN
N  [NN] Y
Li, H
Qi, J
Dai, X
Ji, and Y
Wei
Fully convolutional  instance-aware semantic segmentation
CoRR, N0NN
N  [NN] X
Liang, Y
Wei, X
Shen, Z
Jie, J
Feng, L
Lin, and  S
Yan
Reversible recursive instance-level object segmentation
arXiv, N0NN
N  [NN] X
Liang, Y
Wei, X
Shen, J
Yang, L
Lin, and S
Yan
 Proposal-free network for instance-level object segmentation
arXiv, N0NN
N, N  [NN] T
Lin, P
Dollár, R
B
Girshick, K
He, B
Hariharan, and  S
J
Belongie
Feature pyramid networks for object detection
CoRR, N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV
N0NN
N  [NN] S
Liu, X
Qi, J
Shi, H
Zhang, and J
Jia
Multi-scale patch  aggregation (mpa) for simultaneous detection and segmentation
CVPR, N0NN
N, N, N  [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
N, N  [NN] A
Newell and J
Deng
Associative embedding: End-to-end  learning for joint detection and grouping
CoRR, N0NN
N  [NN] P
H
O
Pinheiro, R
Collobert, and P
Dollár
Learning to  segment object candidates
In NIPS, N0NN
N  [N0] P
H
O
Pinheiro, T
Lin, R
Collobert, and P
Dollár
Learning to refine object segments
In ECCV, N0NN
N, N  [NN] M
Ren and R
S
Zemel
End-to-end instance segmentation  and counting with recurrent attention
CoRR, N0NN
N, N, N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
N  [NN] B
Romera-Paredes and P
H
S
Torr
Recurrent instance  segmentation
CoRR, N0NN
N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
S
Bernstein,  A
C
Berg, and F
Li
Imagenet large scale visual recognition  challenge
International Journal of Computer Vision, N0NN
 N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
ICLR, N0NN
N,  N  [NN] J
Uhrig, M
Cordts, U
Franke, and T
Brox
Pixel-level  encoding and depth layering for instance-level semantic labeling
CoRR, N0NN
N, N  [NN] S
Wang, M
Bai, G
Máttyus, H
Chu, W
Luo, B
Yang,  J
Liang, J
Cheverie, S
Fidler, and R
Urtasun
Torontocity:  Seeing the world with a million eyes
CoRR, N0NN
N  [NN] S
Xie and Z
Tu
Holistically-nested edge detection
In  ICCV, N0NN
N  [NN] F
Yu and V
Koltun
Multi-scale context aggregation by dilated convolutions
CoRR, N0NN
N  [N0] Z
Zhang, S
Fidler, and R
Urtasun
Instance-level segmentation for autonomous driving with deep densely connected  mrfs
In CVPR, N0NN
N, N, N  [NN] Z
Zhang, A
G
Schwing, S
Fidler, and R
Urtasun
Monocular object instance segmentation and depth ordering with  cnns
CoRR, N0NN
N, N, N  [NN] H
Zhao, J
Shi, X
Qi, X
Wang, and J
Jia
Pyramid scene  parsing network
CoRR, N0NN
N, N, N  NN0NGenerative Adversarial Networks Conditioned by Brain Signals   Generative Adversarial Networks Conditioned by Brain Signals  S
Palazzo, C
Spampinato, I.Kavasidis, D
Giordano  PeRCeiVe Lab - Department Electrical Electronics and Computer Engineering  University of Catania - Italy  {palazzosim, kavasidis, dgiordan, cspampin}@dieei.unict.it  M
Shah  Center for Research in Computer Vision  University of Central Florida - USA  shah@crcv.ucf.edu  Abstract  Recent advancements in generative adversarial networks (GANs), using deep convolutional models, have supported the development of image generation techniques able  to reach satisfactory levels of realism
Further improvements have been proposed to condition GANs to generate  images matching a specific object category or a short text  description
In this work, we build on the latter class of  approaches and investigate the possibility of driving and  conditioning the image generation process by means of  brain signals recorded, through an electroencephalograph  (EEG), while users look at images from a set of N0 ImageNet object categories with the objective of generating the  seen images
To accomplish this task, we first demonstrate  that brain activity EEG signals encode visually-related information that allows us to accurately discriminate between  visual object categories and, accordingly, we extract a more  compact class-dependent representation of EEG data using recurrent neural networks
Afterwards, we use the  learned EEG manifold to condition image generation employing GANs, which, during inference, will read EEG signals and convert them into images
We tested our generative approach using EEG signals recorded from six subjects while looking at images of the aforementioned N0  visual classes
The results show that for classes represented by well-defined visual patterns (e.g., pandas, airplane, etc.), the generated images are realistic and highly  resemble those evoking the EEG signals used for conditioning GANs, resulting in an actual reading-the-mind process
 N
Introduction  Reading the mind is such an ambitious and dreamedupon capability that is widely — and reasonably — regarded as closer to science fiction than real science
However, little steps are constantly being made by the scientific community to push the limits of our understanding of  the brain’s workings and of our probing technology
For  example, research on brain-computer interfaces for directactuated control of machines for disabled people is a very  active and relatively successful field, which can make an  actual impact on users’ lives [NN, NN, N]
 But what if, instead of being able to detect a limited set  of simple executive commands from the brain, we could  generate something more inspiring, more meaningful, more  complex — like images?  While cognitive neuroscience studies [NN, NN, NN] have  attempted — with yet uncertain results — to identify which  parts of the human visual cortex and brain are responsible  for visual cognitive processes, it has been acknowledged  that brain activity recordings contain information about visual object categories [N, NN, NN, N, N]
This consideration  makes one wonder whether patterns of such brain activity  may be identified in order to extract useful information on  the content of an observed scene
 This kind of information could, then, be used in conjunction with conditional generative models to reconstruct  a meaningful and realistic image from the informative content decoded from brain activity
Luckily, such generative  frameworks already exist, and one of them in particular —  generative adversarial networks (GANs) [N] — is currently  very popular thanks to its simplicity in concept and effectiveness in practice (although some aspects related to reliable training approach are still unclear)
Hence, assuming that a GAN approach intrinsically contains the complexity required to model the image generation process, the  main problem to solve is how to extract visually-content–  representative information
 Recently, EEG has been increasingly used to capture  NNN0    brain activity signals and process them for visually-related  uses, e.g., visual object classification [NN, N0]
Although  promising results have been shown, the techniques employed to process this kind of multi-dimensional, noisy,  temporal data are still very simple, and mostly ignore local temporal dynamics, processing the full EEG signal as a  whole
 In this work, we combine a GAN approach with a model  based on recurrent neural networks (RNNs) to process EEG  signals captured while users look at images on a screen
 The recurrent model temporally analyzes input signals and  learns to encode them into a compact and visually-content–  descriptive representation; in turn, this representation is  used to condition the image generation process by a GAN  model, with the objective of producing output images depicting objects semantically- related to those shown to users  while the original EEG signals have been recorded
The objective is to learn a representation of brain signals which  conveys enough meaning for a generative model to capture  the visual category associated to it, and to be able to reproduce a relevant sample
 N
Related Work  In a typical experimental scenario attempting to study  the brain responses and dynamics associated to visual processes, a human subject looks at a series of images, while  a recording device interfaced to or scanning the brain is  employed to register the appropriate feedback signals for  further analysis
Currently, there exists a variety of noninvasive methods that allow us to acquire such brain responses (fMRI, EEG, MEG) with different grades of sensitivity, but still, there is a profound lack in understanding  what exactly the acquired data means and, even more importantly, how to interpret it
 In a pioneering work [NN], the authors try to generate  impressions of what the subjects see based on fMRI images  by imposing a prior built on a large image dataset extracted  from YouTube
Essentially, this work tries to maximize the  a posteriori probability that a certain visual stimulus evoking a specific cerebral response corresponds to an image  drawn from a large pool of images [NN] by exploiting the  high sensitivity that fMRI signals offer
However, such advantage is countered by the objective difficulty of setting  up and operating an fMRI scanner and by the considerably  higher utilization costs
 To alleviate these drawbacks much research effort is  concentrated on electrophysiological responses, rather than  brain imaging and, especially, EEG, which features a lower  spatial resolution with respect to almost all other methods,  but has a very high temporal one
An EEG data acquisition session costs also less and is simpler to execute, but  the quality of the gathered data often suffers from unwanted  environmental noise and artifacts, making the challenge of  reconstructing the initial stimuli much harder
It is known  that EEG signals encode basic responses to visual stimuli [N, NN], and recently the authors of this paper, in [NN],  were able to ”decode” such information and use it for automated visual classification
This paper builds on this recent discovery and aims at reconstructing the initial stimulus from learned latent space
However, reconstructing the  visual stimuli is not trivial
 Indeed, the human visual cortex covers around N0% of  the total cortical area [N], which makes it far larger than the  other sensory cortices, meaning that visual information representation in the brain is clearly the most complex among  all sensory processes
For example, some previous works  have managed to recreate the stimuli from other senses than  sight
In [N0], the authors describe an approach to recreate  (partially) speech stimuli based on human auditory cortex  data, acquired by cortical surface electrode arrays
Brain  signals acquired by this method have the advantage of being  affected by noise in a lesser extent than EEG signals, and  also require a simpler generative model
However, replicating such procedure is prohibitive because it requires openskull surgery to be performed on the subject
 Reconstructing human vision, however, is different as it  requires to understand if and how brain signals recorded  through existing devices convey visual content
There exist a few works that attempt to address that, e.g., methods  for identifying visual classes of the visual stimuli
In [N0],  a classifier is trained to recognize object classes based on  topographic maps generated by EEG signals
However, the  obtained accuracy is low (NN% over NN classes), mainly because the employed linear classifier cannot represent adequately the spatio-temporal dynamics contained in the EEG  signals
A similar work is presented in [N0], but this time  raw EEG data is first processed by Independent Component Analysis and then fed to a Support Vector Machine  classifier, which has the task to distinguish between only  N classes
While these works are undoubtedly interesting,  they present a number of limitations (relatively simple classification models, low number of object classes) that do not  permit the investigation at a deeper level of the temporal and  spatial dynamics of the EEG signals
 On the other hand, deep learning methods are able to  handle large, diverse and noisy datasets with exceptional  results
Moreover, recently, there was an explosion in the  number of works that employ deep learning methods for  image generation, and more specifically, generative adversarial networks [N]
In general, a GAN is a deep convolutional neural network comprised of two parts: the generator, which has the task of creating images starting from  pure noise, and the discriminator, which assesses whether  an input image is real or fake
While, initially, GANs  could generate images based on a single type of images  (i.e., a simple GAN can only be trained and used only for  NNNN    a single object class), conditional GANs [NN] introduced  the ability to generate images based on specific attributes
 Such attributes can be in the form of one-hot binary vectors  (i.e., a single bit in the vector indicates the class to generate), words [NN, NN] or arbitrary real number vectors representing geometric transformations and coordinates in a ND  space [N]
However, the majority of works describing generative models employ clearly defined images as well as conditioning vectors (e.g
hand-written digits, faces) and adequately large datasets (e.g., MNISTN, CIFAR-N0N, CelebAN)  for the training process
The performance, instead, deteriorates substantially when small and noisy datasets are used  for training as the case we are tackling in this work
Indeed, EEG signals are particularly noisy and it is not trivial  to collect large data
 N
Method  Our method consists of a “EEG-in/image-out” processing pipeline, where EEG signals are recorded while showing images to human subjects and output images are obtained by a generator which learns to associate the processed EEG signals to the visual object class observed while  those signals were recorded
 The feasability of this approach relies on a few assumptions
First of all, it is necessary that EEG signals intrinsically encode visually-related information, whether these  are low-level responses to visual stimuli or high-level cognitive processes associated to more complex activities such  as recognition and understanding
 Secondarily, it has to be possible to extract a meaningful representation, suitable for solving visual classification  problems, from the high-dimensional and highly-noisy raw  EEG signals
For example, a 0.N-second-long NNN-channel  EEG track at N kHz consists of NN,000 data samples, with  unclear underlying dynamics, correlations and noise components
Extracting a low-dimensional descriptor encoding  visually-relevant information is therefore a critical task for  the whole process
 Finally, in order for our image generator to produce images of the correct class given a low-dimensional representation of the EEG signal, the visual information encoded by  such representation has to be class-discriminative
Even if  EEG signals encode visual information, this does not imply that the level of “detail” of such information allows to  distinguish between object categories
 Our design approach takes these hypotheses for granted  in the way it processes the input data; the feasibility of  the whole process is then verified by the results we obtain
 Fig
N shows the architecture employed in this work, divided  into its basic data-acquisition and processing modules:  Nhttp://yann.lecun.com/exdb/mnist/ Nhttps://www.cs.toronto.edu/ kriz/cifar.html Nhttp://mmlab.ie.cuhk.edu.hk/projects/CelebA.html  • EEG recording protocol: each subject in the ex- periment undergoes an EEG recording session, where  he/she simply has to look at images from different  classes on a computer monitor
 • EEG manifold learning: raw EEG signals are pro- cessed by an RNN-based encoder, which is trained to  output a vector of what we call EEG features, containing visually-relevant and class-discriminative information extracted from the input signals
 • EEG-conditioned image generation: a generator net- work is trained in a conditional GAN framework to  produce images from EEG features, so that the visual  class of the output image matches that of the conditioning vector
 N.N
EEG data acquisition  Six subjects partecipated in the experiment and were  shown images of objects while EEG data was recorded
All  subjects were evaluated by a professional physician in order to exclude possible health conditions or medication that  could alter normal cerebral activity
 The subjects were shown N0 images from N0 different  object classesN for a total of N,000 images per subject
Each  image class was presented in bursts of NN seconds (0.N second per image) followed by a N0 seconds pause where a  black image was shown
The black image was used to  “flush” any high-level class information present from the  previous one
The total running time of each experiment  was N,N00 seconds (NN minutes and N0 seconds)
Details of  the experimental protocol are shown in Table N
 We used the actiCAPN cap with NNN active lowimpedance, low-noise electrodes
Four NN-channel BrainvisionN high-precision, low-latency signal amplifiers were  used (exact model: BrainAmp DC) and a qualified technician was present during the experiments’ execution, ensuring that skin impedance remained under N0 kOhm at all  times by using conductive abrasive gel
The acquired EEG  signals were filtered in run-time (i.e
during the acquisition phase) by the integrated hardware notch filter (NN-NN  Hz) and a second order Butterworth (band-pass) filter with  frequency boundaries NN-N0 Hz
This frequency range contains the necessary bands (Alpha, Beta and Gamma) that  are most meaningful during the visual recognition task [NN]
 The sampling frequency was set to N000 Hz and the quantization resolution to NN bit
 NA subset of the ImageNet dataset [NN] was used consisting of the N0  classes that are shown in Tab
N
Nhttp://www.brainproducts.com/ Nhttp://www.brainvision.com/  NNNN    Figure N
Overview of the architecture design of the proposed EEG-driven image generation approach
 The histogram of the acquired signals over the different  values presented with a high density near the zero value and  a much lower density at the extremities
In order to reduce  input space sparsity, non-uniform quantization was applied  for data compression
 Deep-learning networks need constant length input sequences both for training and validation, however, data  coming from analog devices may present variable data size  due to different factors
Indeed, by acquiring data at a N000  Hz sampling rate for N00 ms, N00 samples of data should be  acquired per image
Given that the systems involved are not  real-time (Operating system process scheduler, DAQ hardware etc...), variable length EEG sequences were dealt with  by discarding those with less than NN0 samples
Data sequences whose length was between NN0 and N00 samples  where padded with zeros until reaching N00 samples
Sequences longer than N00 samples were tail trimmed
 From each recorded EEG sequence, the first N0 samples  were discarded in order to minimize any possible interference from the previously shown image (i.e., to give the necessary time for the stimulus to clear its way through the optical tract [N])
The following NN0 samples (NN0 ms) were  Number of classes N0  Number of images per class N0  Total number of images N,000  Visualization order Sequential  Time for each image 0.N s  Pause time between classes N0 s  Number of sessions N  Session running time NN0 s  Total running time N,N00 s  Table N
The parameters of the experimental protocol
 used for the experiments
 By using the protocol in Table N, we acquired NN, 000 (N, 000 images for N subjects) NNN-channel EEG sequences
NNN samples did not satisfy the minimum data length criteria described above, resulting in NN, NNN valid samples
 N.N
Learning EEG visual descriptors  Although previous works have attempted to work directly with the multi-channel temporal EEG sequences  NNNN    Figure N
EEG feature encoder architecture
 [N0, N0], by simply concatenating time sequences into a  single feature vector (albeit with a smaller number of channels than NNN), this kind of methods ignores local temporal  dynamics
To account for time dependencies, we employ  LSTM recurrent neural networks inspired by previous results in [NN]
 Our EEG feature encoder is illustrated in Fig
N, and consists of a standard LSTM layers followed by a nonlinear  layer
At each time step, input s(·, t) (i.e., the set of val- ues from all channels at time t) is fed into the LSTM layer;  when all time steps have been processed, the final output  state of the LSTM goes into a fully-connected layer with  ReLU non-linearity
The resulting output is what we refer to as “EEG features”, and should ideally be a compact  representation of visual class–discriminative brain activity  information
We append a softmax classification layer and  perform gradient descent optimization (supervised by the  class of the image shown when the input signal had been  recorded) to train the encoder and the classifier end-to-end
 N.N
Brain Signal–Conditioned GANs for Image Generation  We train our generator network in a conditional GAN  framework [NN]
In the original formulation, a generative  model G(z|y) maps random input, from a pz(z) noise dis- tribution and from a condition y to the target data distribution pdata(x)
A discriminative model D(x|y) then predicts the probability that a data point belongs to the target distribution, given the condition
The generator and the discriminator are trained simultaneously, so that the discriminator  tries to maximize the probability of assigning the correct  label to “real” data (from pdata(x)) and “fake” data (from pG(z|y)), while the generator tries to maximize the proba- bility that the discriminator mistakes generated samples for  “real” ones
In other words, the two models play the following minimax game defined by value function V (D,G):  min G  max D  V (D,G) =Ex∈pdata (x) [logD (x|y)] +  Ez∈pz(z) [log (N−D (G (z|y) |y))]  In practice, from a training point of view, this means that,  given a correct sample sc = (xc, yc), consisting of real data with correct condition and a fake sample sw = (xw, yw), consisting of fake data (with arbitrary condition), the negative log-likelihood discriminator loss is computed as:  LD = − logD (xc|yc)− log (N−D (xw|yw)) , (N)  , while the generator loss, for an analogous sw sample, is:  LG = − logD (xw|yw) 
(N)  In our case, the condition vector associated to each image is the average EEG feature vector (as computed by the  encoder described in the previous section) over all images of  each class and all subjects
Our expectation is that, if the encoder has been correctly trained to produce distinguishable  features for different classes, the generator and the discriminator will be able to capture this separability and behave  accordingly
 Both D and G are convolutional networks are illustrated  in the bottom part of Fig
N and their architecture is inspired  by DCGANs [NN]
 In the generator, the condition y is appended to the random noise vector z, and a cascade of transposed convolutions upsample the concatenated input to an output color  image
The discriminator takes as input a same-size image  (either a real one or a generated one)
After going through  a few convolution layers which reduce the size of the feature maps, the condition y associated to the input image, is  spatially replicated and appended to the set of feature maps  from the second-to-last convolutional layer, on which the  final probability estimation is made
 During learning, we modify the discriminator loss function previously presented in Eq
N by following the approach  described in [NN]: instead of training the discriminator with  real images employing correct conditions and fake images  with arbitrary conditions (which forces the discriminator to  learn how to distinguish between real images with correct  conditions and real images with wrong conditions without  any explicit supervision), we also provide a wrong sample  consisting of a real image and a wrong condition, randomly  chosen as the representative EEG feature vector from a different class
Hence, given a correct sample sc = (xc, yc) and wrong samples swN = (xc, yw) and swN = (xw, yw), the discriminator loss becomes:  LD =− logD (xc|yc)  − log (N−D (xc|yw))  − log (N−D (xw|yw)) 
 (N)  NNNN    Model Max VA TA at max VA  LSTMs + nonlinear NN.N% NN.N%  Table N
Maximum validation accuracy (“Max VA”) and corresponding test accuracy (“TA at max VA”) for the LSTM-based  EEG feature encoder shown in Sect
N.N
 N
Performance Analysis  Performance analysis consists of two main parts: first,  we evaluate how our EEG feature encoding architecture has  learnt to extract meaningful representations from visualstimuli–evoked raw EEG signals; secondly, we analyze the  performance of our EEG-driven image generator
The latter  is not a trivial task: apart from a purely qualitative judgment (i.e., “do the images look good?”), no quantitative  evaluation practice exists for evaluating GAN models
In  this work, we employ the Inception score [NN], which estimates the realism and diversity of a batch of generated images by analyzing the softmax distribution of the Inception  network, and a criterion based on its classification accuracy  on the considered batch of generated images: this last test is  meant to estimate whether the generated images are of good  enough quality for Inception to still classify them correctly  (which is not taken into account in the Inception score)
 N.N
Learning EEG features: classification accuracy  The evaluation of our LSTM-based approach has been  reported in [NN], where we split our EEG signal dataset into  training, validation and test sets, with respective fractions  N0% (N,N00 images), N0% (N00), N0% (N00)
Splitting by  images, rather than by EEG signals (which, for each image,  are as many as the number of participant subjects), makes  sure that the signals generated by all subjects for a single  image are not spread over different splits
 Training was performed by using the Adam gradient descent method (learning rate initialized to 0.00N), with minibatches of size NN
All layer sizes in the model (the stacked  LSTMs and the following non-linear layer) were set to NNN
 Model and training hyperparameters were tuned on the validation set
 Table N reports the best classification accuracy achieved  by our EEG signal classifier
 N.N
GAN model and training details  The generator takes as input a concatenated vector of  N00-dimensional random noise and NNN-dimensional EEG  features
Such input then goes through N transposed convolutional layers: the first layer spatially upsamples the vector  by four times, while each of the other layers double the size  at every step, so that the output image size is NN×NN
The number of features maps starts at NNN at the first layer, and  is halved for each layer before the last one, which outputs a  N-channel (color) image
 The discriminator is made up of four convolutional layers and two fully-connected layers
It takes as an input  NN×NN images, and analogously halves the feature map size at every convolutional step
After the final convolutional  layer, where the feature map size is N×N (to which the con- dition vector is spatially appended), two fully-connected  layers reduce the number of features to N0NN and N, the  latter being the sigmoidal probability estimate on the input image/condition pair
The number of feature maps in  the convolutional layers starts at NN at the first layer, and  is doubled at every layer before the fully-connected ones
 Both the generator and the discriminator include batch normalization modules and ReLU nonlinearities
 Training GANs is notoriously difficult, due to the difficult design choices required to make the generator and the  discriminator balanced
In our case, the low number of images for which we had recorded EEG tracks made it impossibile to train directly on those images, as either the generator or the discriminator would overfit
 However, the images used for the EEG data acquisition  protocol were subsets of N0 elements taken from N0 ImageNet classes, with each one containing about N,N00 images
In order to make use of all images in the selected  classes, we trained our GAN network in two stages, making use of both EEG-available images and EEG-unavailable  ones
In the first stage, we trained the generator and the discriminator as a regular (non-conditional) GAN using only  images for which no EEG data was available
All condition  vectors y were set to the zero vector, and the loss term related to real images and wrong conditions (i.e
the second  term of Eq
N) was ignored
After N00 epochs, we re-trained  the models for N0 more epochs on the images with EEG  data available, providing the correct condition vectors and  applying the full discriminator loss function
 During training, data augmentation was performed by  resizing images at NN×NN pixels, and extracting random NN×NN (horizontally flipped with N0% chance)
 N.N
Image generation: qualitative and quantitative analysis  Fig
N and N show samples for some of the N0 classes in  our dataset
While the generator is generally able to capture  the basic distinguishing patterns, which confirms that the  generator and the discriminator were able to make use of the  conditioning EEG features in order to distinguish between  different input/output classes, it can be noticed that for some  classes (Fig
N) the level of realism is markedly higher than  others (Fig
N)
This can be explained by analyzing the complexity of the dataset
Unlike typical benchmarking datasets  such as the CelebA face dataset or LSUN Bedroom, the selected N0 ImageNet classes exhibit high intra-class variance  in object appearance and low size (about N,N00): to make a  NNNN    comparison, CIFAR-N0 has a lower number of classes (N0),  a larger number of images per class (N,000) and a relatively  low intra-class variance for many classes (e.g., “airplane”,  “horse”, “car”)
 We computed the Inception score both globally (across  all classes) and on a per-class base
In the first case, we generated a sample of N0,000 images (N,NN0 per class); in the  second case, we generated a sample of N0,000 images for  each class, and computed per-class Inception scores
The  results are shown in Table N
To the best of our knowledge, Inception score results have not been published on  ImageNet (or subsets thereof, as in our case); on CIFARN0, the current best published result is N.0N [NN]
The results  we obtain on our dataset approximate the capability of the  network to better understand the structure of certain classes  with respect to others
While the achieved Inception scores  are not at the same level as those computed on CIFAR-N0,  it should be noted that several factor impact these results:  N
Higher resolution: NN×NN in our case, NN×NN in CIFAR-N0;  N
More classes: N0 in our case, N0 in CIFAR-N0;  N
Fewer images per class: N,N00-N,N00 in our case, N,000  in CIFAR-N0;  N
Higher intra-class variability;  N
Noisy conditioning vectors
Indeed, conditional GANs  are often trained with image class labels, while in our  case GANs are conditioned using a learned EEG manifold that allows for separation among image classes  but in some case may fail (EEG classification results  are about NN%Table N)
 Since the Inception score does not measure the correctness of the generated images in terms of correspondence  with the condition vectors, we performed an evaluation  aimed at verifying that the generated images for a given  condition (expressed as the average EEG features for each  class) were actually similar to images of the correct class
 To do so, we re-used the previously generated sample  of N0,000 images (N,NN0 images per class) to compute the  class probability distribution through the Inception network,  whose classification layer was pruned by keeping only the  N0 classes in our dataset
The correct classification rate was  0.NN, which, albeit relatively low (though it should be noted  that random guess on N0 classes in N.N%) shows that the  generated images are realistic enough to make automatic  classification meaningful
Table N shows per-class correct  classification rate
As expected, similar to the qualitative visual analysis, the lowest classification accuracy are related  to the classes whose internal visual appearance variance is  Class IS IC  German shepherd (n0NN0NNNN) N.NN 0.NN  Egyptian cat (n0NNNN0NN) N.NN 0.NN  Lycaenid butterfly (n0NNNNNNN) N.0N 0.NN  Sorrel (n0NNNN0NN) N.NN 0.NN  Capuchin (n0NNNN0NN) N.NN 0.NN  Elephant (n0NN0NNNN) N.NN 0.NN  Panda (n0NNN0NNN) N.NN 0.NN  Anemone fish (n0NN0N0NN) N.NN 0.NN  Airliner (n0NNN0NNN) N.N0 0.NN  Broom (n0NN0NNNN) N.NN 0.NN  Canoe (n0NNNNNNN) N.NN 0.NN  Cellphone (n0NNNNNNN) N.NN 0.NN  Mug (n0N0NNNNN) N.NN 0.NN  Convertible (n0NN00NN0) N.NN 0.NN  Desktop PC (n0NNN00NN) N.NN 0.NN  Digital watch (n0NNNNNNN) N.NN 0.NN  Electric guitar (n0NNNN0N0) N.NN 0.NN  Electric locomotive (n0NNNNNNN) N.NN 0.NN  Espresso maker (n0NNNNNNN) N.NN 0.NN  Folding chair (n0NNNNNNN) N.NN 0.NN  Golf ball (n0NNNNNNN) N.0N 0.NN  Piano (n0NNNNNNN) N.NN 0.NN  Iron (n0NNNNNNN) N.NN 0.NN  Jack-o’-lantern (n0NNN0NNN) N.NN 0.NN  Mailbag (n0NN0NNNN) N.NN 0.NN  Missile (n0NNNNN0N) N.NN 0.NN  Mitten (n0NNNN0NN) N.N0 0.NN  Mountain bike (n0NNNNNNN) N.NN 0.NN  Mountain tent (n0NNNNNNN) N.N0 0.N0  Pyjama (n0NNNNNNN) N.NN 0.N0  Parachute (n0NNNNNNN) N.NN 0.NN  Pool table (n0NNNNNN0) N.NN 0.NN  Radio telescope (n0N0NNNNN) N.0N 0.NN  Reflex camera (n0N0NNNNN) N.NN 0.NN  Revolver (n0N0NNNNN) N.NN 0.NN  Running shoe (n0NNN0NNN) N.NN 0.NN  Banana (n0NNNNNNN) N.NN 0.NN  Pizza (n0NNNNN0N) N.NN 0.NN  Daisy (nNNNNNNNN) N.NN 0.NN  Bolete (nNN0NNNN0) N.NN 0.N0  All N.0N 0.NN  Table N
Inception scores (IS) and Inception classification accuracies (IC) for each class of the dataset (specified by their ImageNet  synset identifier and by a short description), and overall
 higher, which — concurrently with the small number of images — made it difficult to the generator to learn to reproduce the correct patterns
 NNNN    (a) Airliner (b) Jack-o’-Lantern (c) Panda  Figure N
Good results  (a) Banana (b) Capuchin (c) Bolete  Figure N
Bad results  N
Conclusions  Although reading the mind may still be something which  humanity will not be able to achieve for a while, in this  work we showed that brain activity signals can be successfully analyzed to drive the generation of images depicting  similar objects as those being observed by a subject when  those signals were recorded
Our approach, combining an  LSTM recurrent neural network for extracting visual-class–  discriminative descriptors from raw EEG signals, and a conditional GAN for generating images from those very descriptors, is able to produce realistic and diverse images  which match the expected object classes, thus demonstrating the goodness of the method and the validity of the initial  assumptions
 Of course, improvements can be made: the method suffers in presence of classes with high internal variability in  appearance, which, combined with the relatively small size  of the employed dataset (if compared to other typical benchmarks for GANs), causes the image generator not being able  to create targeted and clearly recognizable images
 In the future, we aim at pushing the limits of this approach by attemping not just at generating an image depicting the same visual category as the one from which an EEG  signal was generated, but at reconstructing the original image
Of course, this is a much more complicated task
Indeed, the sensitivity and resolution of the EEG technology  may not be sufficient, and we will need to resort to higherresolution modalities such as fMRI [NN]
In turn, this will  require an adaptation of the models employed: for example, given the volumetric nature of fMRI data, our brain encoding module may become a ND recurrent-convolutional  hybrid
Additionally, to compensate for the low temporal  resolution of the fMRI scanners, we are going to investigate  methods to combine fMRI data with EEG data [NN]
 Acknowledgments  We gratefully acknowledge the support of NVIDIA Corporation with the donation of two Titan X Pascal GPUs used  for this research
We also acknowledge Dr
Martina Platania for carrying out EEG data acquisition
 NNNN    References  [N] T
Carlson, D
A
Tovar, A
Alink, and N
Kriegeskorte
Representational dynamics of object vision: the first N000 ms
 Journal of Vision, NN(N0), N0NN
N  [N] T
A
Carlson, H
Hogendoorn, R
Kanai, J
Mesik, and  J
Turret
High temporal resolution decoding of object position and category
Journal of Vision, NN(N0), N0NN
N  [N] H
Cecotti and A
Graser
Convolutional neural networks  for pN00 detection with application to brain-computer interfaces
IEEE transactions on pattern analysis and machine  intelligence, NN(N):NNN–NNN, N0NN
N  [N] K
Das, B
Giesbrecht, and M
P
Eckstein
Predicting variations of perceptual performance across individuals  from neural activity using pattern classifiers
Neuroimage,  NN(N):NNNN–NNNN, Jul N0N0
N  [N] A
Dosovitskiy, J
Tobias Springenberg, and T
Brox
Learning to generate chairs with convolutional neural networks
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
N  [N] I
Goodfellow, J
Pouget-Abadie, M
Mirza, B
Xu,  D
Warde-Farley, S
Ozair, A
Courville, and Y
Bengio
Generative adversarial nets
In Advances in neural information  processing systems, pages NNNN–NNN0, N0NN
N, N  [N] D
Grady
The vision thing: Mainly in the brain
Discover,  NN(N):NN–NN, NNNN
N  [N] A
M
Green and J
F
Kalaska
Learning to move machines  with the mind
Trends in neurosciences, NN(N):NN–NN, N0NN
 N  [N] J
R
Heckenlively and G
B
Arden
Principles and practice  of clinical electrophysiology of vision
MIT press, N00N
N  [N0] B
Kaneshiro, M
Perreau Guimaraes, H.-S
Kim, A
M
Norcia, and P
Suppes
A Representational Similarity Analysis of  the Dynamics of Object Processing Using Single-Trial EEG  Classification
Plos One, N0(N):e0NNNNNN, N0NN
N, N  [NN] Z
Kourtzi and N
Kanwisher
Cortical regions involved  in perceiving object shape
J
Neurosci., N0(N):NNN0–NNNN,  May N000
N  [NN] Z
Liu and B
He
fmri–eeg integrated cortical source imaging by use of time-variant spatial constraints
Neuroimage,  NN(N):NNNN–NNNN, N00N
N  [NN] M
Mirza and S
Osindero
Conditional generative adversarial nets
arXiv preprint arXiv:NNNN.NNNN, N0NN
N, N  [NN] G
R
Muller-Putz and G
Pfurtscheller
Control of an electrical prosthesis with an ssvep-based bci
IEEE Transactions  on Biomedical Engineering, NN(N):NNN–NNN, N00N
N  [NN] M
Nakanishi, Y
Wang, Y.-T
Wang, Y
Mitsukura, and T.-P
 Jung
A high-speed brain speller using steady-state visual  evoked potentials
International journal of neural systems,  NN(0N):NNN00NN, N0NN
N  [NN] T
Naselaris, R
J
Prenger, K
N
Kay, M
Oliver, and J
L
 Gallant
Bayesian reconstruction of natural images from human brain activity
Neuron, NN(N):N0N–NNN, N00N
N  [NN] E
Niedermeyer and F
L
da Silva
Electroencephalography:  basic principles, clinical applications, and related fields
 Lippincott Williams & Wilkins, N00N
N  [NN] S
Nishimoto, A
T
Vu, T
Naselaris, Y
Benjamini, B
Yu,  and J
L
Gallant
Reconstructing visual experiences from  brain activity evoked by natural movies
Current Biology,  NN(NN):NNNN–NNNN, N0NN
N, N  [NN] H
P
Op de Beeck, K
Torfs, and J
Wagemans
Perceived  shape similarity among unfamiliar objects and the organization of the human object vision pathway
J
Neurosci.,  NN(N0):N0NNN–N0NNN, Oct N00N
N  [N0] B
N
Pasley, S
V
David, N
Mesgarani, A
Flinker, S
A
 Shamma, N
E
Crone, R
T
Knight, and E
F
Chang
Reconstructing speech from human auditory cortex
PLoS Biol,  N0(N):eN00NNNN, N0NN
N  [NN] M
V
Peelen and P
E
Downing
The neural basis of visual  body perception
Nat
Rev
Neurosci., N(N):NNN–NNN, Aug  N00N
N  [NN] A
Radford, L
Metz, and S
Chintala
Unsupervised representation learning with deep convolutional generative adversarial networks
CoRR, abs/NNNN.0NNNN, N0NN
N  [NN] S
Reed, Z
Akata, X
Yan, L
Logeswaran, B
Schiele, and  H
Lee
Generative adversarial text to image synthesis
In  Proceedings of The NNrd International Conference on Machine Learning, volume N, N0NN
N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual  Recognition Challenge
International Journal of Computer  Vision (IJCV), NNN(N):NNN–NNN, N0NN
N  [NN] T
Salimans, I
Goodfellow, W
Zaremba, V
Cheung, A
Radford, and X
Chen
Improved techniques for training gans
In  Advances in Neural Information Processing Systems, pages  NNNN–NNNN, N0NN
N, N  [NN] A
B
Schwartz, X
T
Cui, D
J
Weber, and D
W
Moran
 Brain-controlled interfaces: movement restoration with neural prosthetics
Neuron, NN(N):N0N–NN0, N00N
N  [NN] P
Shenoy and D
Tan
Human-aided computing: Utilizing  implicit human processing to classify images
In CHI N00N  Conference on Human Factors in Computing Systems, N00N
 N  [NN] I
Simanova, M
van Gerven, R
Oostenveld, and P
Hagoort
 Identifying object categories from event-related EEG: Toward decoding of conceptual representations
PLoS ONE,  N(NN), N0N0
N  [NN] C
Spampinato, S
Palazzo, I
Kavasidis, D
Giordano,  N
Souly, and M
Shah
Deep Learning Human Mind for  Automated Visual Classification 
CVPR N0NN, N0NN
N, N, N  [N0] A
X
Stewart, A
Nuthmann, and G
Sanguinetti
Single-trial  classification of EEG in a visual object task using ICA and  machine learning
Journal of Neuroscience Methods, NNN:N–  NN, N0NN
N, N  [NN] C
Wang, S
Xiong, X
Hu, L
Yao, and J
Zhang
Combining features from ERP components in single-trial EEG for  discriminating four-category visual objects
J Neural Eng,  N(N):0NN0NN, Oct N0NN
N  [NN] X
Yan, J
Yang, K
Sohn, and H
Lee
AttributeNimage: Conditional image generation from visual attributes
In European  Conference on Computer Vision, pages NNN–NNN
Springer,  N0NN
N  NNNNInterpretable Explanations of Black Boxes by Meaningful Perturbation   Interpretable Explanations of Black Boxes by Meaningful Perturbation  Ruth C
Fong  University of Oxford  ruthfong@robots.ox.ac.uk  Andrea Vedaldi  University of Oxford  vedaldi@robots.ox.ac.uk  Abstract  As machine learning algorithms are increasingly applied  to high impact yet high risk tasks, such as medical diagnosis or autonomous driving, it is critical that researchers  can explain how such algorithms arrived at their predictions
In recent years, a number of image saliency methods  have been developed to summarize where highly complex  neural networks “look” in an image for evidence for their  predictions
However, these techniques are limited by their  heuristic nature and architectural constraints
 In this paper, we make two main contributions: First, we  propose a general framework for learning different kinds  of explanations for any black box algorithm
Second, we  specialise the framework to find the part of an image most  responsible for a classifier decision
Unlike previous works,  our method is model-agnostic and testable because it is  grounded in explicit and interpretable image perturbations
 N
Introduction  Given the powerful but often opaque nature of modern black box predictors such as deep neural networks [N,  N], there is a considerable interest in explaining and understanding predictors a-posteriori, after they have been  learned
This remains largely an open problem
One  reason is that we lack a formal understanding of what it  means to explain a classifier
Most of the existing approaches [NN, NN, N, N, N, NN], etc., often produce intuitive  visualizations; however, since such visualizations are primarily heuristic, their meaning remains unclear
 In this paper, we revisit the concept of “explanation” at  a formal level, with the goal of developing principles and  methods to explain any black box function f , e.g
a neural network object classifier
Since such a function is learned  automatically from data, we would like to understand what  f has learned to do and how it does it
Answering the “what” question means determining the properties of the  map
The “how” question investigates the internal mechanisms that allow the map to achieve these properties
We  focus mainly on the “what” question and argue that it can  flute: 0.NNNN flute: 0.000N Learned Mask  Figure N
An example of a mask learned (right) by blurring an  image (middle) to suppress the softmax probability of its target  class (left: original image; softmax scores above images)
 be answered by providing interpretable rules that describe  the input-output relationship captured by f 
For example, one rule could be that f is rotation invariant, in the sense that “f(x) = f(x′) whenever images x and x′ are related by a rotation”
 In this paper, we make several contributions
First, we  propose the general framework of explanations as metapredictors (sec
N), extending [NN]’s work
Second, we identify several pitfalls in designing automatic explanation systems
We show in particular that neural network artifacts  are a major attractor for explanations
While artifacts are  informative since they explain part of the network behavior, characterizing other properties of the network requires  careful calibration of the generality and interpretability of  explanations
Third, we reinterpret network saliency in our  framework
We show that this provides a natural generalization of the gradient-based saliency technique of [NN] by  integrating information over several rounds of backpropagation in order to learn an explanation
We also compare  this technique to other methods [NN, NN, N0, NN, NN] in terms  of their meaning and obtained results
 N
Related work  Our work builds on [NN]’s gradient-based method, which  backpropagates the gradient for a class label to the image layer
Other backpropagation methods include DeConvNet [NN] and Guided Backprop [NN, N], which builds off  of DeConvNet [NN] and [NN]’s gradient method to produce  sharper visualizations
 Another set of techniques incorporate network activations into their visualizations: Class Activation Mapping  NNNNN    (CAM) [NN] and its relaxed generalization Grad-CAM [NN]  visualize the linear combination of a late layer’s activations  and class-specific weights (or gradients for [NN]), while  Layer-Wise Relevance Propagation (LRP) [N] and Excitation Backprop [N0] backpropagate an class-specific error  signal though a network while multiplying it with each convolutional layer’s activations
 With the exception of [NN]’s gradient method, the above  techniques introduce different backpropagation heuristics,  which results in aesthetically pleasing but heuristic notions  of image saliency
They also are not model-agnostic, with  most being limited to neural networks (all except [NN, N])  and many requiring architectural modifications [NN, NN, N,  NN] and/or access to intermediate layers [NN, NN, N, N0]
 A few techniques examine the relationship between inputs and outputs by editing an input image and observing  its effect on the output
These include greedily graying out  segments of an image until it is misclassified [NN] and visualizing the classification score drop when an image is occluded at fixed regions [NN]
However, these techniques are  limited by their approximate nature; we introduce a differentiable method that allows for the effect of the joint inclusion/exclusion of different image regions to be considered
 Our research also builds on the work of [NN, NN, N]
The  idea of explanations as predictors is inspired by the work  of [NN], which we generalize to new types of explanations,  from classification to invariance
 The Local Intepretable Model-Agnostic Explanation  (LIME) framework [NN] is relevant to our local explanation  paradigm and saliency method (sections N.N, N) in that both  use an function’s output with respect to inputs from a neighborhood around an input x0 that are generated by perturb- ing the image
However, their method takes much longer to  converge (N = N000 vs
our N00 iterations) and produces a coarse heatmap defined by fixed super-pixels
 Similar to how our paradigm aims to learn an image perturbation mask that minimizes a class score, feedback networks [N] learn gating masks after every ReLU in a network to maximize a class score
However, our masks are  plainly interpretable as they directly edit the image while  [N]’s ReLU gates are not and can not be directly used as a  visual explanation; furthermore, their method requires architectural modification and may yield different results for  different networks, while ours is model-agnostic
 N
Explaining black boxes with meta-learning  A black box is a map f : X → Y from an input space X to an output space Y , typically obtained from an opaque learning process
To make the discussion more concrete, consider as input color images x : Λ → RN where Λ = {N, 


, H} × {N, 


,W} is a discrete domain
The output y ∈ Y can be a boolean {−N,+N} telling whether the image contains an object of a certain type (e.g
a robin),  the probability of such an event, or some other interpretation of the image content
 N.N
Explanations as meta-predictors  An explanation is a rule that predicts the response of a  black box f to certain inputs
For example, we can ex- plain a behavior of a robin classifier by the rule QN(x; f) = {x ∈ Xc ⇔ f(x) = +N}, where Xc ⊂ X is the sub- set of all the robin images
Since f is imperfect, any such rule applies only approximately
We can measure the faithfulness of the explanation as its expected prediction error:  LN = E[N − δQN(x;f)], where δQ is the indicator function of event Q
Note that QN implicitly requires a distribution p(x) over possible images X 
Note also that LN is simply the expected prediction error of the classifier
Unless we did  not know that f was trained as a robin classifier, QN is not very insightful, but it is interpretable since Xc is
 Explanations can also make relative statements about  black box outcomes
For example, a black box f , could be rotation invariant: QN(x, x  ′; f) = {x ∼rot x ′ ⇒ f(x) =  f(x′)}, where x ∼rot x ′ means that x and x′ are related by  a rotation
Just like before, we can measure the faithfulness  of this explanation as LN = E[N−δQN(x,x′;f)|x ∼ x ′].N This  rule is interpretable because the relation ∼rot is
 Learning explanations
A significant advantage of formulating explanations as meta predictors is that their faithfulness can be measured as prediction accuracy
Furthermore, machine learning algorithms can be used to discover  explanations automatically, by finding explanatory rules Q that apply to a certain classifier f out of a large pool of pos- sible rules Q
 In particular, finding the most accurate explanation Q is similar to a traditional learning problem and can be formulated computationally as a regularized empirical risk minimization such as:  min Q∈Q  λR(Q) + N  n  n ∑  i=N  L(Q(xi; f), xi, f), xi ∼ p(x)
(N)  Here, the regularizer R(Q) has two goals: to allow the ex- planation Q to generalize beyond the n samples xN, 


, xn considered in the optimization and to pick an explanation Q which is simple and thus, hopefully, more interpretable
 Maximally informative explanations
Simplicity and interpretability are often not sufficient to find good explanations and must be paired with informativeness
Consider  the following variant of rule QN: QN(x, x ′; f, θ) = {x ∼θ  x′ ⇒ f(x) = f(x′)}, where x ∼θ x ′ means that x and x′  NFor rotation invariance we condition on x ∼ x′ because the probability of independently sampling rotated x and x′ is zero, so that, without  conditioning, QN would be true with probability N
 NNN0    ch oc  ol at  e  sa  uc e orig img + gt bb mask gradient guided contrast excitation grad-CAM occlusion  Pe ki ne  se cli  ff st  re et   si gn  Ko m  od o   dr ag  on pi ck  up CD   p la  ye r  su ng  la ss es  sq ui  rre l m  on ke  y im  pa la  un icy  cle  Figure N
Comparison with other saliency methods
From left to right: original image with ground truth bounding box, learned mask subtracted from N (our method), gradient-based saliency [NN], guided backprop [NN, N], contrastive excitation backprop [N0], Grad-CAM [NN],  and occlusion [NN]
 NNNN    Stethoscope Gradient Soup Bowl Gradient  Figure N
Gradient saliency maps of [NN]
A red bounding box  highlight the object which is meant to be recognized in the image
 Note the strong response in apparently non-relevant image regions
 are related by a rotation of an angle ≤ θ
Explanations for larger angles imply the ones for smaller ones, with θ = 0 being trivially satisfied
The regularizer R(QN(·; θ)) = −θ can then be used to select a maximal angle and thus find an  explanation that is as informative as possible.N  N.N
Local explanations  A local explanation is a rule Q(x; f, x0) that predicts the response of f in a neighborhood of a certain point x0
If f is smooth at x0, it is natural to construct Q by using the first-order Taylor expansion of f :  f(x) ≈ Q(x; f, x0) = f(x0) + 〈∇f(x0), x− x0〉
(N)  This formulation provides an interpretation of [NN]’s  saliency maps, which visualize the gradient SN(x0) = ∇f(x0) as an indication of salient image regions
They argue that large values of the gradient identify pixels that  strongly affect the network output
However, an issue is  that this interpretation breaks for a linear classifier: If  f(x) = 〈w, x〉+ b, SN(x0) = ∇f(x0) = w is independent of the image x0 and hence cannot be interpreted as saliency
 The reason for this failure is that eq
(N) studies the variation of f for arbitrary displacements ∆x = x−x0 from x0 and, for a linear classifier, the change is the same regardless  of the starting point x0
For a non-linear black box f such as a neural network, this problem is reduced but not eliminated, and can explain why the saliency map SN is rather diffuse, with strong responses even where no obvious information can be found in the image (fig
N)
 We argue that the meaning of explanations depends in  large part on the meaning of varying the input x to the black box
For example, explanations in sec
N.N are based  on letting x vary in image category or in rotation
For saliency, one is interested in finding image regions that  impact f ’s output
Thus, it is natural to consider pertur- bations x obtained by deleting subregions of x0
If we model deletion by multiplying x0 point-wise by a mask m,  NNaively, strict invariance for any θ > 0 implies invariance to arbitrary  rotations as small rotations compose into larger ones
However, the formulation can still be used to describe rotation insensitivity (when f varies  slowly with rotation), or ∼θ’s meaning can be changed to indicate rotation  w.r.t
a canonical “upright” direction for a certain object classes, etc
 blur constant noise  Figure N
Perturbation types
Bottom: perturbation mask; top: effect of blur, constant, and noise perturbations
 this amounts to studying the function f(x0 ⊙ m) N
The  Taylor expansion of f at m = (N, N, 


, N) is SN(x0) = df(x0 ⊙m)/dm|m=(N,...,N) = ∇f(x0) ⊙ x0
For a linear  classifier f , this results in the saliency SN(x0) = w ⊙ x0, which is large for pixels for which x0 and w are large si- multaneously
We refine this idea for non-linear classifiers  in the next section
 N
Saliency revisited  N.N
Meaningful image perturbations  In order to define an explanatory rule for a black box  f(x), one must start by specifying which variations of the input x will be used to study f 
The aim of saliency is to identify which regions of an image x0 are used by the black box to produce the output value f(x0)
We can do so by observing how the value of f(x) changes as x is obtained “deleting” different regions R of x0
For exam- ple, if f(x0) = +N denotes a robin image, we expect that f(x) = +N as well unless the choice of R deletes the robin from the image
Given that x is a perturbation of x0, this is a local explanation (sec
N.N) and we expect the explanation  to characterize the relationship between f and x0
While conceptually simple, there are several problems  with this idea
The first one is to specify what it means  “delete” information
As discussed in detail in sec
N.N, we  are generally interested in simulating naturalistic or plausible imaging effect, leading to more meaningful perturbations and hence explanations
Since we do not have access  to the image generation process, we consider three obvious  proxies: replacing the region R with a constant value, in- jecting noise, and blurring the image (fig
N)
 Formally, let m : Λ → [0, N] be a mask, associating each pixel u ∈ Λ with a scalar value m(u)
Then the perturbation operator is defined as  [Φ(x0;m)](u) =            m(u)x0(u) + (N−m(u))µ0, constant,  m(u)x0(u) + (N−m(u))η(u), noise, ∫  gσ0m(u)(v − u)x0(v) dv, blur,  where µ0 is an average color, η(u) are i.i.d
Gaussian noise samples for each pixel and σ0 is the maximum isotropic  N⊙ is the Hadamard or element-wise product of vectors
 NNNN    standard deviation of the Gaussian blur kernel gσ (we use σ0 = N0, which yields a significantly blurred image)
 N.N
Deletion and preservation  Given an image x0, our goal is to summarize compactly the effect of deleting image regions in order to explain the  behavior of the black box
One approach to this problem is  to find deletion regions that are maximally informative
 In order to simplify the discussion, in the rest of the paper we consider black boxes f(x) ∈ RC that generate a vector of scores for different hypotheses about the content  of the image (e.g
as a softmax probability layer in a neural  network)
Then, we consider a “deletion game” where the  goal is to find the smallest deletion mask m that causes the score fc(Φ(x0;m)) ≪ fc(x0) to drop significantly, where c is the target class
Finding m can be formulated as the following learning problem:  m∗ = argmin m∈[0,N]Λ  λ‖N−m‖N + fc(Φ(x0;m)) (N)  where λ encourages most of the mask to be turned off (hence deleting a small subset of x0)
In this manner, we can find a highly informative region for the network
 One can also play an symmetric “preservation game”,  where the goal is to find the smallest subset of the image  that must be retained to preserve the score fc(Φ(x0;m)) ≥ fc(x0): m  ∗ = argminm λ‖m‖N−fc(Φ(x0;m))
The main difference is that the deletion game removes enough evidence to prevent the network from recognizing the object in  the image, whereas the preservation game finds a minimal  subset of sufficient evidence
 Iterated gradients
Both optimization problems are  solved by using a local search by means of gradient descent  methods
In this manner, our method extracts information  from the black box f by computing its gradient, similar to the approach of [NN]
However, it differs in that it extracts  this information progressively, over several gradient evaluations, accumulating increasingly more information over  time
 N.N
Dealing with artifacts  By committing to finding a single representative perturbation, our approach incurs the risk of triggering artifacts  of the black box
Neural networks, in particular, are known  to be affected by surprising artifacts [N, N0, N]; these works  demonstrate that it is possible to find particular inputs that  can drive the neural network to generate nonsensical or unexpected outputs
This is not entirely surprising since neural networks are trained discriminatively on natural image  statistics
While not all artifacts look “unnatural”, nevertheless they form a subset of images that is sampled with negligible probability when the network is operated normally
 espresso: 0.NNNN espresso: 0.0000 Learned Mask  maypole: 0.NNNN maypole: 0.0000 Learned Mask  Figure N
From left to right: an image correctly classified with  large confidence by GoogLeNet [NN]; a perturbed image that is  not recognized correctly anymore; the deletion mask learned with  artifacts
Top: A mask learned by minimizing the top five predicted classes by jointly applying the constant, random noise, and  blur perturbations
Note that the mask learns to add highly structured swirls along the rim of the cup (γ = N, λN = N0 −N, λN =  N0 −N, β = N)
Bottom: A minimizing-topN mask learned by applying a constant perturbation
Notice that the mask learns to introduce sharp, unnatural artifacts in the sky instead of deleting the  pole (γ = 0.N, λN = N0 −N, λN = N0  −N, β = N)
 Although the existence and characterization of artifacts  is an interesting problem per se, we wish to characterize  the behavior of black boxes under normal operating conditions
Unfortunately, as illustrated in fig
N, objectives  such as eq
(N) are strongly attracted by such artifacts, and  naively learn subtly-structured deletion masks that trigger  them
This is particularly true for the noise and constant  perturbations as they can more easily than blur create artifacts using sharp color contrasts (fig
N, bottom row)
 We suggests two approaches to avoid such artifacts in  generating explanations
The first one is that powerful  explanations should, just like any predictor, generalize as  much as possible
For the deletion game, this means not relying on the details of a singly-learned mask m
Hence, we reformulate the problem to apply the mask m stochastically, up to small random jitter
 Second, we argue that masks co-adapted with network  artifacts are not representative of natural perturbations
As  noted before, the meaning of an explanation depends on the  meaning of the changes applied to the input x; to obtain a mask more representative of natural perturbations we can  encourage it to have a simple, regular structure which cannot be co-adapted to artifacts
We do so by regularizing m in total-variation (TV) norm and upsampling it from a low  resolution version
 With these two modifications, eq
(N) becomes:  min m∈[0,N]Λ  λN‖N−m‖N + λN ∑  u∈Λ  ‖∇m(u)‖ββ  + Eτ [fc(Φ(x0(· − τ),m))], (N)  NNNN    ch oc  ol at  e  sa  uc e  Mask Overlay 0.NN0 => 0.NNN 0.NN0 => 0.0NN pi  ck up  Mask Overlay 0.NNN => 0.NN0 0.NNN => 0.0NN  Figure N
Interrogating suppressive effects
Left to right: original  image with the learned mask overlaid; a boxed perturbation chosen  out of interest (the truck’s middle bounding box was chosen based  on the contrastive excitation backprop heatmap from fig
N, row  N); another boxed perturbation based on the learned mask (target  softmax probabilities of for the original and perturbed images are  listed above)
 where M(v) = ∑  u gσm(v/s − u)m(u)
is the upsampled mask and gσm is a ND Gaussian kernel
Equation (N) can be optimized using stochastic gradient descent
 Implementation details
Unless otherwise specified, the  visualizations shown were generated using Adam [N] to  minimize GoogLeNet’s [NN] softmax probability of the target class by using the blur perturbation with the following  parameters: learning rate γ = 0.N, N = N00 iterations, λN = N0  −N, λN = N0 −N, β = N, upsampling a mask (NN×NN  for GoogLeNet) by a factor of δ = N, blurring the upsam- pled mask with gσm=N, and jittering the mask by drawing an integer from the discrete uniform distribution on [0, τ) where τ = N
We initialize the mask as the smallest cen- tered circular mask that suppresses the score of the original  image by NN% when compared to that of the fully perturbed image, i.e
a fully blurred image
 N
Experiments  N.N
Interpretability  An advantage of the proposed framework is that the generated visualizations are clearly interpretable
For example,  the deletion game produces a minimal mask that prevents  the network from recognizing the object
 When compared to other techniques (fig
N), this method  can pinpoint the reason why a certain object is recognized  without highlighting non-essential evidence
This can be  noted in fig
N for the CD player (row N) where other visualizations also emphasize the neighboring speakers, and  similarly for the cliff (row N), the street sign (row N), and  the sunglasses (row N)
Sometimes this shows that only a  part of an object is essential: the face of the Pekenese dog  (row N), the upper half of the truck (row N), and the spoon  on the chocolate sauce plate (row N) are all found to be minimally sufficient parts
 While contrastive excitation backprop generated  heatmaps that were most similar to our masks, our method  introduces a quantitative criterion (i.e., maximally suppressing a target class score), and its verifiable nature (i.e.,  direct edits to an image), allows us to compare differing  proposed saliency explanations and demonstrate that our  learned masks are better on this metric
In fig
N, row N,  we show that applying a bounded perturbation informed  by our learned mask significantly suppresses the truck  softmax score, whereas a boxed perturbation on the truck’s  back bumper, which is highlighted by contrastive excitation  backprop in fig
N, row N, actually increases the score from  0.NNN to 0.NN0
The principled interpretability of our method also allows  us to identify instances when an algorithm may have learned  the wrong association
In the case of the chocolate sauce  in fig
N, row N, it is surprising that the spoon is highlighted  by our learned mask, as one might expect the sauce-filled jar  to be more salient
However, manually perturbing the image reveals that indeed the spoon is more suppressive than  the jar
One explanation is that the ImageNet “chocolate  sauce” images contain more spoons than jars, which appears to be true upon examining some images
More generally, our method allows us to diagnose highly-predictive yet  non-intuitive and possibly misleading correlations by identified machine learning algorithms in the data
 N.N
Deletion region representativeness  To test that our learned masks are generalizable and robust against artifacts, we simplify our masks by further  blurring them and then slicing them into binary masks by  thresholding the smoothed masks by α ∈ [0 : 0.0N : 0.NN] (fig
N, top; α ∈ [0.N, 0.N] tends to cover the salient part identified by the learned mask)
We then use these simplified masks to edit a set of N,000 ImageNet images with constant, noise, and blur perturbations
Using GoogLeNet [NN],  we compute normalized softmax probabilitiesN (fig
N, bottom)
The fact that these simplified masks quickly suppress  scores as α increases for all three perturbations gives con- fidence that the learned masks are identifying the right regions to perturb and are generalizable to a set of extracted  masks and other perturbations that they were not trained on
 N.N
Minimality of deletions  In this experiments we assess the ability of our method  to correctly identify a minimal region that suppresses the  object
Given the output saliency map, we normalize its  intensities to lie in the range [0, N], threshold it with h ∈ [0 : 0.N : N], and fit the tightest bounding box around the resulting heatmap
We then blur the image in the box and  compute the normalizedN target softmax probability from  Np′ = p− p0  p0 − pb , where p, p0, pb are the masked, original, and fully  blurred images’ scores  NNNN    Img Mask =0.N =0.N =0.N =0.N  0.0 0.N 0.N 0.N 0.N 0.00  0.NN  0.N0  0.NN  N.00  M ea  n  No  rm 
S  of tm  ax blur constant noise  Figure N
(Top) Left to right: original image, learned mask, and  simplified masks for sec
N.N (not shown: further smoothed mask)
 (Bottom) Swift softmax score suppression is observed when using  all three perturbations with simplified binary masks (top) derived  from our learned masks, thereby showing the generality of our  masks
 N0% N0% NN% NN% Percent of Normalized Softmax Score Suppression  0  N000  N0000  NN000  N0000  NN000  M ea  n  M  in im  al  B  ou nd  in g   Bo x   Si ze   (p ix  el s^  N)  mask occlusion contrast exc excitation guided grad-CAM gradient  Figure N
On average, our method generates the smallest bounding  boxes that, when used to blur the original images, highly suppress  their normalized softmax probabilities (standard error included)
 GoogLeNet [NN] of the partially blurred image
 From these bounding boxes and normalized scores, for  a given amount of score suppression, we find the smallest bounding box that achieves that amount of suppression
 Figure N shows that, on average, our method yields the  smallest minimal bounding boxes when considering suppressive effects of N0%, N0%, NN%, and NN%
These results show that our method finds a small salient area that strongly  impacts the network
 N.N
Testing hypotheses: animal part saliency  From qualitatively examining learned masks for different animal images, we noticed that faces appeared to be  more salient than appendages like feet
Because we produce dense heatmaps, we can test this hypothesis
From an  annotated subset of the ImageNet dataset that identifies the  keypoint locations of non-occluded eyes and feet of vertebrate animals [NN], we select images from classes that have  at least N0 images which each contain at least one eye and  foot annotation, resulting in a set of NNNN images from NN  animal classes (fig
N)
For every keypoint, we calculate the  average heatmap intensity of a N × N window around the  Figure N
“tiger” (left two) and “bison” (right two) images with  eyes and feet annotations from [NN]; our learned masks are overlaid
The mean average feet:eyes intensity ratio for “tigers” (N = NN) is N.NN, while that for bisons (N = NN) is N.0N
 keypoint
For all NN classes, the mean average intensity of  eyes were lower and thus more salient than that of feet (see  supplementary materials for class-specific results)
 N.N
Adversarial defense  Adversarial examples [N] are often generated using a  complementary optimization procedure to our method that  learns a imperceptible pattern of noise which causes an image to be misclassified when added to it
Using our reimplementation of the highly effective one-step iterative  method (ǫ = N) [N] to generate adversarial examples, our method yielded visually distinct, abnormal masks compared  to those produced on natural images (fig
N0, left)
We  train an Alexnet [N] classifier (learning rate λlr = N0 −N,  weight decay λLN = N0 −N, and momentum γ = 0.N) to  distinguish between clean and adversarial images by using  a given heatmap visualization with respect to the top predicted class on the clean and adversarial images (fig
N0,  right); our method greatly outperforms the other methods  and achieves a discriminating accuracy of NN.N%
Lastly, when our learned masks are applied back to their  corresponding adversarial images, they not only minimize  the adversarial label but often allow the original, predicted  label from the clean image to rise back as the top predicted  class
Our method recovers the original label predicted on  the clean image N0.NN% of time and the ground truth label  NN.NN% (N = N000)
Moreover, N00% of the time the orig- inal, predicted label was recovered as one of top-N predicted  labels in the “mask+adversarial” setting
To our knowledge,  this is the first work that is able to recover originally predicted labels without any modification to the training set-up  and/or network architecture
 N.N
Localization and pointing  Saliency methods are often assessed in terms of weaklysupervised localization and a pointing game [N0], which  tests how discriminative a heatmap method is by calculating the precision with which a heatmap’s maximum point  lies on an instance of a given object class, for more harder  datasets like COCO [N]
Because the deletion game is meant  to discover minimal salient part and/or spurious correlation,  we do not expect it to be particularly competitive on localization and pointing but tested them for completeness
 For localization, similar to [N0, N], we predict a bounding box for the most dominant object in each of ∼N0k  NNNN    Figure N0
(Left) Difference between learned masks for clean  (middle) and adversarial (bottom) images (NN × NN masks shown  without bilinear upsampling)
(Right) Classification accuracy  for discriminating between clean vs
adversarial images using  heatmap visualizations (Ntrn = N000, Nval = N000)
 ImageNet [NN] validation images and employ three simple thresholding methods for fitting bounding boxes
First,  for value thresholding, we normalize heatmaps to be in the  range of [0, N] and then threshold them by their value with α ∈ [0 : 0.0N : 0.NN]
Second, for energy thresholding [N], we threshold heatmaps by the percentage of energy their  most salient subset covered with α ∈ [0 : 0.0N : 0.NN]
Fi- nally, with mean thresholding [N0], we threshold a heatmap  by τ = αµI , where µI is the mean intensity of the heatmap and α ∈ [0 : 0.N : N0]
For each thresholding method, we search for the optimal α value on a heldout set
Localization error was calculated as the IOU with a threshold of 0.N
 Table N confirms that our method performs reasonably and shows that the three thresholding techniques affect each method differently
Non-contrastive, excitation  backprop [N0] performs best when using energy and mean  thresholding; however, our method performs best with  value thresholding and is competitive when using the other  methods: It beats gradient [NN] and guided backprop [NN]  when using energy thresholding; beats LRP [N], CAM [NN],  and contrastive excitation backprop [N0] when using mean  thresholding (recall from fig
N that the contrastive method  is visually most similar to mask); and out-performs GradCAM [NN] and occlusion [NN] for all thresholding methods
 For pointing, table N shows that our method outperforms  the center baseline, gradient, and guided backprop methods  and beats Grad-CAM on the set of difficult images (images  for which N) the total area of the target category is less than  NN% of the image and N) there are at least two different ob- ject classes)
We noticed qualitatively that our method did  not produce salient heatmaps when objects were very small
 This is due to LN and TV regularization, which yield wellformed masks for easily visible objects
We test two variants of occlusion [NN], blur and variable occlusion, to interrogate if N) the blur perturbation with smoothed masks  Val-α* Err (%) Ene-α* Err Mea-α* Err  Grad [NN] 0.NN NN.0 0.N0 NN.N N.0 NN.N§  Guid [NN, N] 0.0N N0.N 0.N0 NN.0 N.N NN.0§  Exc [N0] 0.NN NN.N 0.N0 NN.N N.N NN.0§  C Exc [N0] — — — — 0.0 NN.0†  Feed [N] — — 0.NN NN.N† — —  LRP [N] — — — — N.0 NN.N†  CAM [NN] — — — — N.0 NN.N†  Grad-CAM [NN] 0.N0 NN.N 0.N0 NN.0 N.0 NN.N  Occlusion [NN] 0.N0 NN.N 0.NN NN.N N.0 NN.N  Mask‡ 0.N0 NN.0 0.NN NN.N 0.N NN.N  Table N
Optimal α thresholds and error rates from the weak localization task on the ImageNet validation set using saliency  heatmaps to generate bounding boxes
†Feedback error rate are  taken from [N]; all others (contrastive excitation BP, LRP, and  CAM) are taken from [N0]
§Using [N0]’s code, we recalculated these errors, which are ≤ 0.N% of the originally reported rates
‡Minimized topN predicted classes’ softmax scores and used  λN = N0 −N and β = N.0 (examples in supplementary materials)
 Ctr Grad Guid Exc CExc G-CAM Occ Occ§ V-Occ† Mask‡  All NN.NN NN.N0 NN.NN NN.NN N0.NN NN.N0 NN.N0 NN.NN NN.NN NN.NN  Diff NN.NN NN.NN NN.NN NN.NN NN.NN N0.NN NN.NN NN.NN NN.NN N0.NN  Table N
Pointing Game [N0] Precision on COCO Val Subset (N ≈ N0k)
§Occluded with circles (r = NN/N) softened by gσm=N0 and used to perturb with blur (σ = N0)
†Occluded with variable-sized blur circles; from the top N0% most suppressive occlusions, the  one with the smallest radius is chosen and its center is used as the  point
‡Used min
top-N hyper-parameters (λN = N0 −N, β = N.0)
 is most effective, and N) using the smallest, highly suppressive mask is sufficient (Occ§ and V-Occ in table N respectively)
Blur occlusion outperforms all methods except  contrast excitation backprop while variable while variable  occlusion outperforms all except contrast excitation backprop and the other occlusion methods, suggesting that our  perturbation choice of blur and principle of identifying the  smallest, highly suppressive mask is sound even if our implementation struggles on this task (see supplementary materials for examples and implementation details)
 N
Conclusions  We propose a comprehensive, formal framework for  learning explanations as meta-predictors
We also present  a novel image saliency paradigm that learns where an algorithm looks by discovering which parts of an image most affect its output score when perturbed
Unlike many saliency  techniques, our method explicitly edits to the image, making it interpretable and testable
We demonstrate numerous  applications of our method, and contribute new insights into  the fragility of neural networks and their susceptibility to artifacts
 NNNN    References  [N] S
Bach, A
Binder, G
Montavon, F
Klauschen, K.-R
 Müller, and W
Samek
On pixel-wise explanations for nonlinear classifier decisions by layer-wise relevance propagation
PloS one, N0(N):e0NN0NN0, N0NN
N, N  [N] C
Cao, X
Liu, Y
Yang, Y
Yu, J
Wang, Z
Wang, Y
Huang,  L
Wang, C
Huang, W
Xu, et al
Look and think twice: Capturing top-down visual attention with feedback convolutional  neural networks
In Proceedings of the IEEE International  Conference on Computer Vision, pages NNNN–NNNN, N0NN
N,  N, N  [N] D
Kingma and J
Ba
Adam: A method for stochastic optimization
arXiv preprint arXiv:NNNN.NNN0, N0NN
N  [N] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in neural information processing systems, pages  N0NN–NN0N, N0NN
N, N  [N] A
Kurakin, I
Goodfellow, and S
Bengio
Adversarial examples in the physical world
arXiv preprint arXiv:NN0N.0NNNN,  N0NN
N, N, N  [N] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European conference on computer  vision, pages NN0–NNN
Springer, N0NN
N  [N] A
Mahendran and A
Vedaldi
Understanding deep image  representations by inverting them
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N  [N] A
Mahendran and A
Vedaldi
Salient deconvolutional networks
In European Conference on Computer Vision, pages  NN0–NNN
Springer International Publishing, N0NN
N, N, N, N  [N] A
Mahendran and A
Vedaldi
Visualizing deep convolutional neural networks using natural pre-images
International Journal of Computer Vision, NN0(N):NNN–NNN, N0NN
 N  [N0] A
Nguyen, J
Yosinski, and J
Clune
Deep neural networks  are easily fooled: High confidence predictions for unrecognizable images
In Proceedings of the IEEE Conference on  Computer Vision and Pattern Recognition, pages NNN–NNN,  N0NN
N  [NN] D
Novotny, D
Larlus, and A
Vedaldi
I have seen enough:  Transferring parts across categories
In Proceedings of the  British Machine Vision Conference (BMVC), N0NN
N  [NN] M
T
Ribeiro, S
Singh, and C
Guestrin
Why should i  trust you?: Explaining the predictions of any classifier
In  Proceedings of the NNnd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, pages  NNNN–NNNN
ACM, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  et al
Imagenet large scale visual recognition challenge
 International Journal of Computer Vision, NNN(N):NNN–NNN,  N0NN
N  [NN] R
R
Selvaraju, A
Das, R
Vedantam, M
Cogswell,  D
Parikh, and D
Batra
Grad-cam: Why did you say that?  visual explanations from deep networks via gradient-based  localization
arXiv preprint arXiv:NNN0.0NNNN, N0NN
N, N,  N, N  [NN] K
Simonyan, A
Vedaldi, and A
Zisserman
Deep inside convolutional networks: Visualising image classification models and saliency maps
In Proc
ICLR, N0NN
N, N, N,  N, N, N  [NN] J
T
Springenberg, A
Dosovitskiy, T
Brox, and M
Riedmiller
Striving for simplicity: The all convolutional net
 arXiv preprint arXiv:NNNN.NN0N, N0NN
N, N, N, N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In Proceedings of the IEEE  Conference on Computer Vision and Pattern Recognition,  pages N–N, N0NN
N, N, N  [NN] R
Turner
A model explanation system
In Proc
NIPS Workshop on Black Box Learning and Inference, N0NN
N, N  [NN] M
D
Zeiler and R
Fergus
Visualizing and understanding  convolutional networks
In European conference on computer vision, pages NNN–NNN
Springer, N0NN
N, N, N, N  [N0] J
Zhang, Z
Lin, J
Brandt, X
Shen, and S
Sclaroff
Topdown neural attention by excitation backprop
In European  Conference on Computer Vision, pages NNN–NNN
Springer,  N0NN
N, N, N, N, N  [NN] B
Zhou, A
Khosla, A
Lapedriza, A
Oliva, and A
Torralba
 Object detectors emerge in deep scene cnns
arXiv preprint  arXiv:NNNN.NNNN, N0NN
N  [NN] B
Zhou, A
Khosla, A
Lapedriza, A
Oliva, and A
Torralba
Learning deep features for discriminative localization
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N  NNNNDeepRoadMapper: Extracting Road Topology From Aerial Images   DeepRoadMapper: Extracting Road Topology from Aerial Images  Gellért Máttyus, Wenjie Luo and Raquel Urtasun  Uber Advanced Technologies Group  University of Toronto  {gmattyus, wenjie, urtasun}@uber.com  Abstract  Creating road maps is essential for applications such as  autonomous driving and city planning
Most approaches in  industry focus on leveraging expensive sensors mounted on  top of a fleet of cars
This results in very accurate estimates  when exploiting a user in the loop
However, these solutions  are very expensive and have small coverage
In contrast, in  this paper we propose an approach that directly estimates  road topology from aerial images
This provides us with an  affordable solution with large coverage
Towards this goal,  we take advantage of the latest developments in deep learning to have an initial segmentation of the aerial images
 We then propose an algorithm that reasons about missing  connections in the extracted road topology as a shortest  path problem that can be solved efficiently
We demonstrate  the effectiveness of our approach in the challenging TorontoCity dataset [NN] and show very significant improvements  over the state-of-the-art
 N
Introduction  Creating maps of our roads is a fundamental step in many  application domains
Having accurate maps is essential to  the success of autonomous driving for routing, localization  as well as to ease perception
Building smart cities requires  understanding the road network as well as the traffic patterns that occur on it to enable faster commute times, better  public transportation systems and a healthier environment
 Most self-driving teams and mapping companies rely on  expensive sensors mounted on a fleet of vehicles which  drive around, mostly capturing LIDAR point clouds
A  semi-manual process is then utilized to create the road network
Very accurate results can be achieved, but coverage  is very limited
Furthermore, this is a very costly process
 Thus HD maps are available for only a small region of the  world
 An alternative approach is to use aerial and satellite images as data source
This is appealing as they have much  larger coverage
For example, satellites go around the world  Figure N: Road topology from aerial images at a large scale
 Our extracted road network is shown in blue
 twice a day, providing up-to-date information
However,  extracting road networks from this imagery is very challenging, as the resolution is much lower
Further, occlusion  (e.g., trees) and large shadows cast by tall buildings are difficult to handle
Most existing approaches cast the problem as semantic segmentation
Unfortunately, this ignores  topology, which is the basic unit needed in order to perform driving
Recently, [NN, NN] leveraged existing maps  to enhance them with road-width as well as information  about the number of lanes, their location, parking spaces  and sidewalks
However, these approaches cannot reason  about roads that are not present in the initial coarse map
 In contrast, in this paper we propose an approach that  directly estimates road topology from aerial images
Towards this goal, we take advantage of the latest developments in deep learning to have an initial segmentation of the  aerial images
We then propose an algorithm that reasons  about missing connections in the extracted road topology as  a shortest path problem that can be solved efficiently
We  demonstrate the effectiveness of our approach in the challenging TorontoCity dataset [NN], and show very significant  improvements over the state-of-the-art
 NNNNN    N
Related work  Many approaches have been proposed in the last decades  to extract road segmentation from aerial and satellite images
Several methods extract low level features and define  heuristic rules (e.g
connectivity, shape) over these to classify road like structures
Geometric-stochastic road models  based on assumptions about the width, length and curvature  of the road and the pixel intensities have been exploited in  [N]
Hinz and Baumgartner [N0] use road models and their  context including knowledge about their radiometry, geometry and topology
In [NN], homogeneous areas are detected  based on their shape and a road tree is then grown by tracking the roads
The drawback of these heuristic rule based  models is that obtaining the optimal set of rules and parameters is very difficult
This is particularly challenging due to  the high variety of roads
As a consequence these methods  can work only on areas (e.g
rural) where the used features  (e.g
image edge) occur predominantly at roads
 Convolutional neural networks have been used to segment roads from aerial images [NN]
The neural network is  applied at the patch level in multiple stages (with the previous prediction as input) to capture more context and structure
In [NN], existing maps are used for data augmentation
 Unfortunately connectivity of roads is not guaranteed in this  approach
In [N], the roads are detected by a deep neural network applied to image patches
The extracted road network  is then matched to a road database (i.e., OpenStreetMap)  and the two road maps are merged
 Connectivity is probably one of the most important road  features
However, this has been rarely studied in both the  computer vision and photogrametry communities
Chai et  al
[N] define a junction point process that reasons about the  graph describing the road network
The process uses various priors, e.g
homogeneity of the pixel values, connectivity of the graph, edge orientation and line width
However,  optimization is hard as it requires Reversible Jump MCMC  sampling
In [NN], a Point Process is defined to describe  the interaction of line segments (e.g., connectivity)
The  road network is extracted by minimizing an energy function using simulated annealing
In [NN], the road extraction  is limited to tree structures
This guarantees the connectivity and the optimization can be solved exactly
Unfortunately roads are not tree-structured, posing a significant limitation
This approach was further extended to loopy graphs  in [NN], where the NP hard problem is approximately solved  by a branch and cut algorithm
Wegner et al
[NN, NN] segment the image into superpixels and the ones with high road  likelihood are connected by a shortest path algorithm with  the goal of creating an overcomplete representation of the  road network
These paths are then handled as higher order  cliques in a Conditional Random Field (CRF)
As shown in  our experiments this method does not produce very accurate results, and is an order of magnitude slower than our  (a) (b)  Figure N: Road graph extraction: Nodes are road segment  endpoints (crosses)
(a) Graph constructed via thinning
 Small branches (orange) are removed and small loops (blue)  are replaced by a tree providing the same connectivity to the  rest of the graph
(b) final graph (red)
 approach
 Most approaches considered road segmentation as a binary problem, however roads can have various categories  which are important for mapping
Mattyus et al
[NN] improve existing freely-available road maps by extracting road  width information and by correcting the position of the centerline
In [NN], aerial and ground images are utilized jointly  in order to extract fine-grained road information like the  number of lanes, presence of sidewalks and parking lanes
 Crowd-sourced manual labeling as well as GPS trajectories  have been exploited to create road topology
This is the case  of the OpenStreetMap project [N], in which volunteers have  mapped more than half of the world
Recorded GPS tracks  were also employed to help the road segmentation [NN]
 N
Road Topology from Aerial Images  In this paper we want to extract a graph representation  of the road network from aerial images
In this graph the  nodes represent end points of street segments and the edges  encode the connections between these points, defining the  road segment center lines
Towards this goal, we exploit  convolutional neural networks (CNNs) to segment the images into the categories of interest
Then a simple process  based on thinning extracts the road centerlines from the segmentation output
Errors in the segmentation can result in  discontinuities, which translate into topological errors of the  extracted road network
To alleviate this problem we further  reason about the presence or absence of edges in an augmented road graph which contains also connection hypotheses covering the disconnects
As shown in our experiments  this improves significantly our estimated road network
 N.N
Semantic segmentation of Aerial Images  In this section we describe the architecture we employ to  segment aerial images
Following current trends in semanNNNN    Figure N: Segmentation softmax is highlighted in green, the  extracted road center line is shown in red, and the connection hypotheses generated by the A∗ search are in blue
Dashed yellow shows other possible connections which  were not selected by the A* algorithm
 tic reasoning from ground images we develop a variant of  ResNet [N] to perform this task
Similar to FCN [NN], it consists of an encoder that compresses the image into a small  feature map, and a fully convolutional decoder, which generates the segmentation output probabilities
 Our encoder consists of a ResNet block with NN convolutional layers with N × N kernels
We use a convolutional layer with stride N after N residual block forming NN convolutional layers
This divides the whole encoding network  in N parts
We use NN, NN, NN and NNN kernels in each of  these parts respectively
This gives us a feature map of NNN  dimension with N/N of the original resolution
 The decoder consists of N fully convolutional layers with  number of kernels NN, NN and NN respectively
Each of these  layers upsamples its input to be double its resolution
In  order to capture details, each of these layers takes feature  maps directly from the encoder network as input as well  as two additional skip connections from the stride convolution
The last convolutional layer converts the feature map  into scores follow by a softmax with three outputs: road,  building and background
Thus the whole network consists  of NN convolutional layers for the encoder, N fully convolutional layers for the decoder, follow by a convolutional layer  to output the class labels
 Segmentation networks are typically trained via crossentropy
However, the metric of interest at test time is typically the intersection over union (IoU), which is defined as  N  |C|  ∑  c  ∑ i ✶{yi = c} · ✶{y  ∗  i = c}∑ i ✶{yi = c}+ ✶{y  ∗  i = c} − ✶{yi = c} · ✶{y ∗  i = c}  where yi is the prediction, y ∗  i the ground truth and c is a  class label
 In this paper, we develop a novel soft IoU loss, which  is differentiable and thus amenable to back propagation
In  particular, it is defined by replacing the indicator functions  with the softmax outputs  ℓsoft−IOU = N  |C|  ∑  c  ∑ i pic · p  ∗  ic∑ i pic + p  ∗  ic − pic · p ∗  ic  where pic is the prediction score at location i for class c, and p∗ic is the ground truth distribution which is a delta function at y∗i , the correct label
 N.N
Road graph generation  Once we have an estimate of the semantic segmentation,  the next step is to produce a graph representing the topology of the network
Towards this goal, we first generate a  binary mask from the softmax output of the deep network  by thresholding the road class at 0.N probability
Then we apply thinning [NN] to extract the road centerlines, i.e., a one  pixel wide representation of the road segments preserving  the connectivity of the components
This results in a graph,  where every pixel is a node
To simplify the graph we employ the Ramer–Douglas–Peucker algorithm [NN, N], which  outputs a piecewise linear approximation of our road skeletons
In particular, we use an error tolerance of ǫ = N.Nm
Note that our thinning procedure creates separate branches  at topological defects of the segmentation mask
Many of  them are small curves, which are not real centerlines
We  thus remove curves with length smaller than Nm
This is illustrated in Fig
N
 Another potential problem are small holes in the segmentation mask, which cause undesired loops in our connectivity graph
We thus convert each loop of size smaller  than N00 m into a tree (star architecture) which provides the same connectivity to the nodes outside of the loop N
We  refer the reader to Fig
N for an example
Note that we will  only violate connectivity at roundabouts, which are rare in  North America, where our source imagery is captured
This  gives our representation of the road network graph, where  nodes are end-points of the road segments and edges define  the curves connecting these points
 N.N
Generating connection hypotheses byA∗ search  Our segmentation algorithm is accurate, but discontinuities of the resulting mask can cause errors in topology
Fig
 N shows an example where the road in the left is disconnected
To alleviate this problem, we reason about potential missing street segments in order to further improve the  topology
We define a leaf node as a node with a single connection
This represents the end of a road according to our  N The loop nodes connected to nodes outside the loop are preserved, the  rest are removed and a new node is inserted in the center of the loop and is  connected to all the preserved nodes
 NNN0    Figure N: Each road segment (orange dots) is a random variable
Red lines show centerlines extracted from the segmentation
Hypothesis connections are shown as blue lines
 This is the dual of the graph that describes the road network
 current topology estimate
We generate connections from  the leaf nodes to other nodes if they lie within N0m and the shortest path in the graph between the two nodes is larger  than N00m, to prevent creating small loops in the graph
 Following this procedure, a leaf node can be connected  to many nodes which provide the same connectivity
This is  illustrated in Fig
N, where possible connections are shown  in dashed yellow
We exploit the A∗ algorithm [N] to select from these connections
A* is a shortest path algorithm that  applies a cost heuristic to determine the next nodes to visit
 If this heuristic is close to the real cost, then the search is  efficient
We utilize the probability score of being non-road  as our node cost, the distance as our edge cost and the euclidean distance as our heuristic
The algorithm runs very  fast as most nodes are not visited during the search
 N.N
Reasoning about the connections  So far we have shown how to estimate possible connections between road segments
We now define an algorithm  that decides the validity of these connections
Towards this  goal, we reason about the hypothesized connections as well  as the original road segments to prune false positives
We  represent each road segment/connection with a binary variable yi ∈ {0, N} representing the presence/absence of that road segment
Note that this is the dual of the graph which  describes our road network
We refer the reader to N for an  illustration
 To perform this task, we exploit a variety of potentials  which depend on a single road segment
Our features are  the soft-max scores along the road segment, the distance  to the closest non road pixel, the length of the segment, a  binary feature encoding if the node represents a connection  hypothesis and the number of connections to other road sections
Since a road segment defines a curve, we calculate  the features along the curve by employing different pooling  strategies
In particular we employ min, max and average  pooling to form additional features
 Figure N: Two examples for the connection classifier
(Left)  negative example, (Right) positive example
 (a) (b)  Figure N: Illustration of the ground truth (GT) assignment
 (a) GT graph in black and the extracted graph in blue
The  subfigure shows two types of augmented edges
The dashed  red connect the GT endpoints (A and B) to our extracted  graph, while the dashed blue connect the discontinuities of  our extracted graph
(b) The shortest path between A and  B is shown in orange where the dashed line highlights the  augmented edges
The shortest path defines the assignment  and thus the true positives area, i.e., the projection on the  ground truth (green)
False positives are segments outside  the shortest path (red), false negative length is the missing  part of the ground truth, shown in black
 Deciding if a connection hypotheses is true is a difficult  classification problem, especially since our deep semantic  segmentation algorithm has already failed in this region
We  thus utilize an additional network that classifies whether the  hypothesized connection is a true connection or not, and  employ the output of this classifier as an additional feature
 The input to this classifier is a cropped image around the  connection with the connection drawn on the image
We  refer the reader to Fig
N for an illustration
In particular, we  use an Inception network [N0] to perform this classification
 Inference in this model is trivial and can be done in parallel as all our features depend on a single road hypothesis
 We next describe how to perform learning
 N
Learning and Metrics  One of the difficulties we need to tackle is the fact that  the ground truth graph and the estimated graph have different topology
Furthermore, the road hypothesis on both  graphs have also different shape
In order to both do learning and evaluate our results we need to be able to register  the two graphs and come up with the true labeling in the  NNNN    hypothesized graph
In this section we describe how to do  this task
 N.N
Assignment of GT roads to extracted roads  Our first goal is to assign the ground truth (GT) roads  to the extracted roads in the predicted graph
We consider  this assignment as a set of shortest path problems defined  between each intersection and the road ends connected to  that intersection in the ground truth network
To ensure that  the connection goes along a similar path as the ground truth  road, we only include as hypothesis the extracted roads located in a fix radius around the ground truth
 Note that in principle there will be many cases where  there is no possible path, as we might have disconnects in  the extracted graph
To handle this, we augment our graph  with edges connecting the end points of the ground truth  graph to the end points in the extracted graph
Furthermore,  we also include edges that encode the missing connections
 Fig
N shows an illustration of this process, where on (a) the  extracted graph is shown in blue and the additional edges  are shown in dashed blue and red
 We then solve the assignment problem by calculating the  shortest path between the endpoints, where the distance between the adjacent points pi and pi+N is calculated as  D(pi, pi+N) =  i+N∑  j=i  φd(pj) + λ(pi, pi+N)||pi − pi+N||  with λ(pi, pi+N) = N if the edge existed and λ(pi, pi+N) = c, with c as a large constant if the edge is an augmented edge
φd(pj) measurs the distance to the closest ground truth road edge
This ensures that the shortest path lies close  to the ground truth
The minimum path can be solved by  the Dijkstra algorithm [N]
Since the augmented edges have  very high cost, they will only be selected if there is no other  choice (the extracted graph is not connected)
We refer the  reader to Fig
N for an example
 N.N
Learning  We can utilize the procedure we just described to compute our ground truth in terms of the extracted graph, i.e.,  our yi’s
In particular, all the original edges on the short- est path are considered as true positives, while the edges  that are not part of any shortest path are considered as false  positives
We use this assignment as our ground truth to perform learning
We train our model using max margin loss  and use the Hamming distance as the task loss
 N.N
Evaluating Topology  The most commonly employed metrics in the literature  are pixel-based and measure semantic segmentation [NN, N,  NN, NN, N]
However, these metrics do not reflect the quality  of the extracted topology
Defining a graph based metric is  Figure N: TorontoCity demo area shown on Google Earth
 Cyan: train, orange: validation, red: test set
 non-trivial as there is no simple answer to the question of  when are two graphs similar
 Wenger et al
[NN, NN] propose a connectivity metric  measuring if the shortest path distances between randomly  sampled correct pixels are the same in the ground truth and  the extracted road network
This metric reflects topological  similarity, but it is hardly reproducible due to the random  selection of end points to form the shortest path problem
 Furthermore, only a few end points are selected and thus  this can miss many topological changes
 In this paper we propose a new set of metrics, which  are based on the assignment of the ground truth roads to  the extracted roads
We believe that these metrics better  capture the extracted topology
We define the precision of  each segment as the ratio of the true positive length d∗p to the extracted length dp
We compute the final precision by computing the average precision of each segment weighted  by its ground truth length d∗p  pr = N∑ p d  ∗  p  ∑  p  min (dp, d ∗  p)  max (dp, d∗p) d∗p  Note that to take into account the fact that the ground truth  length can be smaller or greater than the estimated length  we use the ratio of min and max values
We define recall as  simply the percentage of road that is recovered with respect  to the ground truth dp,gt  rec =  ∑ p d  ∗  p∑ p dp,gt  We report FN, which is the harmonic mean of precision and recall
 FN = N pr · rec  pr + rec  Additionally, we define a metric capturing the ratio of road  segments which were estimated without discontinuities
We  call this Connected Road Ratio (CRR)
 CRR = Ncon Ngt  NNNN    IoU FN precision recall  [NN] NN.N NN.N NN.N N0.N [NN] + Our Deep NN.N NN.N NN.N NN.N  Ours NN.N NN.N NN.N NN.N  Table N: Semantic Segmentation of the road class: our approach significantly outperforms [NN], even when the baseline utilizes our deep semantic segmentation algorithm
 FN Precision Recall CRR  OSM (human) NN.N NN.N NN.0 NN.N  [NN] NN.N NN.N NN.N NN.N [NN] DeepUn NN.N N0.0 NN.N NN.N  HED [NN] NN.N NN.N NN.N NN.N Ours basic NN.0 NN.N NN.N NN.N  Ours - NoDeepCon
NN.N NN.N NN.N NN.N Ours full NN.0 NN.N NN.N NN.N  Table N: Topology recovery metrics (percentage)
 with Ncon the number of road segments extracted without discontinuities and Ngt the number of GT segments
 N
Experiments  We perform our experiments on the demo region of the  TorontoCity dataset [NN]
We use the pixelwise annotations  of this dataset to train our semantic segmentation network  with N classes (i.e., background, road, building)
 Dataset: The TorontoCity demo region includes aerial  images over NNkmN for training, NNkmN for validation and NN.NkmN for test
All sets are typical North American ur- ban areas with both skyscrapers and family houses
The  imagery consists of N000 × N000 pixel orthorectified im- ages with N0 cm/pixel resolution and RGB color channels
 The dataset provides pixelwise annotation of the road surfaces and the buildings plus vector data defining the road  center lines and the connectivity between the roads (i.e
the  road graph)
Fig
N shows an overview of the dataset
Note  that this is a very large area compared to datasets typically  employed in the literature
 Baselines: We compare our method to the work of  Wenger et al
[NN, NN]
We resized the images to NN cm/pixel  (the same as in [NN]) and used the default parameter setting  provided in the authors’ code
Note that without resizing,  their approach takes more than an hour per image
We add  an additional comparison by combining the Markov random  field of [NN] with our deep neural network estimates, in order to give this approach the opportunity to leverage deep  learning
Towards this goal, we replaced their random forest unary features with our semantic segmentation softmax  gt\pred
0 N 0 NN.N NN.N N NN.N NN.N  Table N: Confusion matrix of connection classifier  values
Since [NN] does not create a vector representation  of the road, we use our graph generation method (described  in Section N.N) to convert it to a graph which we can then  evaluate
We compare also to HED [NN] which predicts the  road centerlines directly by a deep net
We directly employ  their code
Additionally, we compare to the freely available  OpenStreetMap project [N] road maps as baseline
This can  be considered as human performance on this task
We neglect very small road categories in OSM (i.e., path, cycleway, service, footway and path), as they are not labeled in  TorontoCity
 Learning details: We used the training set for training the  deep network and the validation set for training the SVM
 Our network takes as input NNN0× NNN0 resolution patches randomly cropped from the raw images with random flips
 The network is trained with Adam [NN] for N0 rounds, with  the initial learning rate of Ne− N, which we drop by a factor of N at round N0 and N0
We use batch size of N perform  N00 iterations for each round
The weights are initialized  using MSR initialization [N]
Training the network took  around NN hours
To train the Inception network that reasons  about whether a connection in the graph exists, we employ  a training set of NN, 000 images, which were generated by creating connection hypotheses on the training set (N/N of the samples), by generating additional examples from the  road graph (N/N) and by adding negative examples (N/N) randomly picked around road areas
For training we use  a learning rate of 0.00N, a momentum of 0.N and train the network for N00 epochs
 Metrics: We report four types of pixel-wise metrics to  show the quality of our semantic segmentation
This includes intersection over union (IoU), precision, recall and  FN
We additionally report the metrics described in Section  N, which tests the accuracy of the shape and topology of the  road network
We apply a radius of N0m around the ground truth roads, everything else cannot be a true positive
 Soft IoU vs
cross-entropy loss: Table N shows a comparison between applying cross-entropy and soft IoU loss  for semantic segmentation
Soft IoU is considerably better
 Comparison to the state-of-the-art: As shown in Table N our method outperforms the baseline [NN] by a large  margin, even when we provide our segmentation scores as  NNNN    mean bg road building accuracy  cross-entr
NN.N NN.N NN.N N0.N NN.N soft IoU NN.N N0.N NN.N NN.N NN.0  Table N: IoU for N classes on the validation set
Soft IoU  loss is considerably better
 (a) TP (b) TN (c) FP (d) FN  Figure N: Examples of connection classifier: TP: true positive, TN: true negative, FP: false positive, FN: false negative
This is a difficult classification problem even for humans
Many road like structures (drivable surfaces) are not  part of the road network ground truth, e.g
in (c)
 unary potentials
The baseline tends to create an overcomplete road extraction resulting in higher recall but smaller  precision
As shown in Table N our approach also significantly outperforms the baseline in terms of topology
In particular, [NN] has high recall but very bad precision
By applying our deep unary potentials the baseline improves, but  is sill behind our approach
HED [NN] also predicts roads  at every road-like surface, resulting in high Recall and CRR  but very low Precision and FN
HED has a difficult time  predicting the skeleton, as in contrast to edge detection, it  is not aligned with image edges but lies over homogeneous  road surfaces
Having low precision is problematic in practice since a manual operator doing quality control would  need to delete many extracted roads, which most likely will  take longer than adding missing roads
The CRR metric is  correlated with Recall and is very sensitive to small discontinuities, while it is agnostic to false positives
Therefore,  methods that create over-complete road networks with low  precision achieve higher numbers
This is why any single  metric is not good in isolation
 Running time: Our approach is very efficient
The segmentation network takes approximately Ns on an NVIDIA TITAN X GPU
Classifying an image takes around Ns run- ning on the same GPU
The rest of the pipeline is implemented in C++ single threaded
It takes N0s on average to process a N000× N000 pixel image
In contrast, [NN] needs around N0 minutes per N000 × N000 pixel image running multi threaded on an Intel Xeon EN-NNN0 CPU
We used the  Matlab and C++ implementation provided by the authors
 Comparison to human performance: OpenStreetMaps  have been generated by a combination of manual labeling  and recorded crowed-sources GPS trajectories
As shown  in Table N our approach is not far behind OSM
This is remarkable, taking into account the enormous manual labeling task that OSM required
OSM achieves almost N0% FN
The fact that it is not perfect shows the difficulty of the task
 Ablation studies: We compared three different instantiations of our model
Our basic algorithm only extracts the  road center line from our semantic segmentation without  reasoning about connectivity
Our second version reasons  about connectivity but does not utilize the Inception classifier
The last version is our full model
As shown in Table N  our basic method provides decent results
If we employ reasoning about the existence of roads, precision increases and  thus also FN, while the recall as well as the ratio of roads  covered decrease only slightly
Adding the deep unary connection classifier improves the performance only slightly
 This classification task is a very difficult and the accuracy is  only around N0% as shown in Table N
 Qualitative Results: Fig
N shows results over the test  set for the baseline and our method
The baseline is much  more susceptible to create false positives, which reduces the  precision significantly
Finally, Fig
N0 shows details of the  extracted road networks
Our method can produce very similar results to the TorontoCity ground truth as well as OSM  (human annotation)
The extracted centerlines follow the  road curves, and very few false positive roads exist
Typical errors are due to tall buildings and wide roads where the  segmentation fails, e.g., due to occlusion by the buildings
 Small interruptions in the connectivity around intersections  are shown in Fig
N0
 N
Conclusion  In this paper we have presented an approach that directly  estimates road topology from aerial images
This provides  us with an affordable solution that has large coverage
Towards this goal, we have taken advantage of the latest developments in deep learning to have an initial segmentation  of the aerial images
We have then derived an algorithm  that reasons about missing connections in the extracted road  topology as a shortest path problem which can be solved  efficiently
We have demonstrated the effectiveness of our  approach in the challenging TorontoCity dataset [NN] and  show very significant improvements over the state-of-theart
In the future we plan to extract additional information  such as building footprints in order to enrich our maps
 N
Acknowledgments  This research was funded by NSERC, CFI, ORF, ERA,  CRC, the RBC Innovation Fellowship
We thank the authors of [NN] for providing their code, and NVIDIA for donating GPUs
 NNNN    (a) [NN] (b) Ours  Figure N: Visualization of the results on the entire test set (NN.NkmN, N.NN · N0N pixels)
Green: True positive, red: false positive, blue: false negative
On the left (a): baseline [NN], on the right (b): our method
 (a) [NN] (b) Ours (c) OSM (human) (d) GT  Figure N0: Road centerlines (blue)
We can produce similar results to humans and much better estimates than the baseline
 References  [N] https://www.openstreetmap.org
N, N  [N] M
Barzohar and D
Cooper
Automatic finding of main  roads in aerial images by using geometric-stochastic models  NNNN  https://www.openstreetmap.org   and estimation
PAMI, NNNN
N  [N] D
Chai, W
Forstner, and F
Lafarge
Recovering linenetworks in images by junction-point processes
In CVPR,  N0NN
N, N  [N] D
Costea and M
Leordeanu
Aerial image geolocalization  from recognition and matching of roads and intersections
 CoRR, abs/NN0N.0NNNN, N0NN
N, N  [N] E
W
Dijkstra
A note on two problems in connexion with  graphs
Numerische Mathematik, NNNN
N  [N] D
H
Douglas and T
K
Peucker
Algorithms for the Reduction of the Number of Points Required to Represent a Digitized Line or its Caricature
John Wiley & Sons, Ltd, N0NN
 N  [N] P
E
Hart, N
J
Nilsson, and B
Raphael
A formal basis  for the heuristic determination of minimum cost paths
IEEE  TSSC, NNNN
N  [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
CoRR, abs/NNNN.0NNNN, N0NN
N  [N] K
He, X
Zhang, S
Ren, and J
Sun
Delving deep into  rectifiers: Surpassing human-level performance on imagenet  classification
CoRR, abs/NN0N.0NNNN, N0NN
N  [N0] S
Hinz and A
Baumgartner
Automatic extraction of urban  road networks from multi-view aerial imagery
ISPRS JPRS,  N00N
N  [NN] J
Hu, A
Razdan, J
C
Femiani, M
Cui, and P
Wonka
Road  network extraction and intersection detection from aerial images by tracking road footprints
TGRS, N00N
N  [NN] D
P
Kingma and J
Ba
Adam: A method for stochastic  optimization
CoRR, abs/NNNN.NNN0, N0NN
N  [NN] G
Mattyus, S
Wang, S
Fidler, and R
Urtasun
Enhancing  road maps by parsing aerial images around the world
In  ICCV, N0NN
N, N  [NN] G
Mattyus, S
Wang, S
Fidler, and R
Urtasun
Hd maps:  Fine-grained road segmentation by parsing ground and aerial  images
In CVPR, N0NN
N, N  [NN] V
Mnih and G
E
Hinton
Learning to detect roads in highresolution aerial images
In ECCV, N0N0
N, N  [NN] V
Mnih and G
E
Hinton
Learning to label aerial images  from noisy data
In ICML, N0NN
N  [NN] U
Ramer
An iterative procedure for the polygonal approximation of plane curves
Computer Graphics and Image Processing, NNNN
N  [NN] E
Shelhamer, J
Long, and T
Darrell
Fully convolutional  networks for semantic segmentation
TPAMI, N0NN
N  [NN] R
Stoica, X
Descombes, and J
Zerubia
A gibbs point process for road extraction from remotely sensed images
IJCV,  N00N
N  [N0] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
N  [NN] E
Turetken, F
Benmansour, B
Andres, H
Pfister, and  P
Fua
Reconstructing loopy curvilinear structures using integer programming
In CVPR, N0NN
N  [NN] E
Turetken, F
Benmansour, and P
Fua
P.: Automated  reconstruction of tree structures using path classifiers and  mixed integer programming
In CVPR
N  [NN] S
Wang, M
Bai, G
Máttyus, H
Chu, W
Luo, B
Yang,  J
Liang, J
Cheverie, S
Fidler, and R
Urtasun
Torontocity:  Seeing the world with a million eyes
ICCV, N0NN
N, N, N  [NN] J
D
Wegner, J
A
Montoya-Zegarra, and K
Schindler
 A higher-order crf model for road network extraction
In  CVPR, N0NN
N, N, N, N  [NN] J
D
Wegner, J
A
Montoya-Zegarra, and K
Schindler
Road  networks as collections of minimum cost paths
ISPRS JPRS,  N0NN
N, N, N, N, N  [NN] S
Xie and Z
Tu
Holistically-nested edge detection
In  ICCV, December N0NN
N, N  [NN] J
Yuan and A
M
Cheriyadat
Image feature based gps trace  filtering for road network generation and road segmentation
 Machine Vision and Applications, N0NN
N  [NN] T
Y
Zhang and C
Y
Suen
A fast parallel algorithm for  thinning digital patterns
Commun
ACM, NNNN
N  NNNNScene Parsing With Global Context Embedding   Scene Parsing with Global Context Embedding  Wei-Chih HungN, Yi-Hsuan TsaiN,N, Xiaohui ShenN,  Zhe LinN, Kalyan SunkavalliN, Xin LuN, Ming-Hsuan YangN  NUniversity of California, Merced NNEC Laboratories America NAdobe Research  Abstract  We present a scene parsing method that utilizes global  context information based on both the parametric and nonparametric models
Compared to previous methods that  only exploit the local relationship between objects, we train  a context network based on scene similarities to generate  feature representations for global contexts
In addition,  these learned features are utilized to generate global and  spatial priors for explicit classes inference
We then design  modules to embed the feature representations and the priors  into the segmentation network as additional global context  cues
We show that the proposed method can eliminate false  positives that are not compatible with the global context  representations
Experiments on both the MIT ADEN0K and  PASCAL Context datasets show that the proposed method  performs favorably against existing methods
 N
Introduction  Scene parsing is one of the fundamental and challenging problems in computer vision, which can be applied to a  wide range of applications such as autonomous driving [N]  and image editing [NN]
The goal of this task is to assign  a semantic class to each pixel in an image
Different from  semantic segmentation where a significant amount of pixels  are labeled as the background, most pixels in the scene parsing datasets are labeled with either thing classes (e.g., person and car) or stuff classes (e.g., wall, ground, and field)
 One major limitation of existing scene parsing methods is that local information only provides limited cues  for inferring the label of a single pixel or patch
For example, in Figure N(a), when observing a patch filled with  gray pixels from the beach sand, it is difficult to infer  whether the patch belongs to the class of road, wall, or  sand, even for human eyes
Thus, most existing contextbased methods combine one or multiple models that can  perform long-range inference through pairwise relationships, e.g., Markov Random Field (MRF), Conditional Random Field (CRF), or global attributes such as scene categories and spatial locations [NN, NN] Non-parametric methods [NN, NN, N, NN, NN, N0] can be seen as context models  focusing on image matching via feature descriptors
By re(a) input (b) ground truth  (c) without global context (d) with global context  Figure N
Given an image (a), the proposed method improves  the results of FCN-Ns [NN] (c) by exploiting the global context information
Our result (d) shows that the algorithm can eliminate  the false positives that are not compatible with the scene category  (e.g., some sand regions in (c) are predicted as mountain in a beach  scene)
(b) shows the ground truth pixel labels
 trieving a small set of similar images from the annotated  dataset, these methods construct dense correspondences between the input image and the retrieved images on the pixel  or superpixel level
A final prediction map can be obtained  through a simple voting or solving an MRF model
However, the performance of such exemplar-based approaches  highly depends on the quality of the image retrieval module  based on hand-crafted features
If the retrieved set does not  contain one or more semantic classes of the input image,  these non-parametric approaches are not expected to parse  the scenes well
 Recently, CNN based methods such as the fully convolutional neural network (FCN) [N0] have achieved the stateof-the-art results in semantic segmentation
These algorithms improve the scene parsing task compared to conventional non-parametric approaches
The performance gain  NNNNN    mainly comes from multiple convolutional and non-linear  activation layers that can learn data-specific local features  to classify each pixel on a local region (i.e., receptive field)
 However, most FCN-based methods still do not utilize an  explicit context model, and the receptive field of one pixel  classifier is fixed in a given network architecture
 In this paper, we propose an algorithm to embed global  contexts into the segmentation network by feature learning  and non-parametric prior encoding
Different from previous  approaches that only consider the contexts within the input  image, our method exploits the scene similarities between  images without knowing scene categories
The key idea is  to use a Siamese network [N] for learning global context  representations in an unsupervised manner
 We then propose to use the learned representations to  exploit global contexts using global context feature encoding and non-parametric prior encoding
For global context  feature encoding, we propagate the learned representations  through the segmentation network
For non-parametric  prior encoding, we generate both global and spatial priors  by retrieving annotated images via global context features  and combine them with our segmentation network
Since  we do not perform dense alignment as most existing nonparametric methods [NN, NN], our non-parametric module is  computationally efficient
Instead of using the original images from the whole training set, which requires large storage at the testing phase, our image retrieval module only  needs the pre-computed compact feature representations of  images
We evaluate the proposed algorithm on the MIT  ADEN0K [NN] and PASCAL Context dataset [NN] with comparisons to the state-of-the-art methods
 The contributions of this work are as follows:  • We design a Siamese network to learn representations for global contexts and model scene similarities between images in an unsupervised manner, with a focus  on images that share rare object/surface categories
 • We propose two methods to exploit global contexts by feature learning and non-parametric prior encoding
 • We show that the parametric segmentation network, context encoding, and non-parametric prior encoding  can be efficiently integrated via the proposed global  context embedding scheme for effective scene parsing  without introducing much computational overhead
 N
Related Work  Numerous methods have been developed to exploit scene  contexts for vision tasks
In [NN], the Thing and Stuff  model that exploits the relationship between objects and  surfaces is proposed to eliminate false positives of object  detectors
In [N0, N], the initial prediction of adjacent superpixels is used as the local context descriptor to refine  the superpixel matching process iteratively
Most methods based on FCN [N0] exploit context information by  constructing MRFs or CRFs on top of the network output [NN, N, NN, NN, NN]
A recent study shows that the performance of FCN-based models can be improved by applying dilated convolution [NN]
The key insight is that dilated  convolution allows the network to “see” more, i.e., enlarging the receptive field, and therefore more context information is perceived
However, these models only consider  the local context information within the input image
Our  proposed context model is related to the global context descriptors in [N0, NN] which refine the scene retrieval module in the non-parametric label transfer
In this work, we  use the global descriptor to improve the pixel classifier directly
Recently, one concurrent work [NN] proposes to exploit the global context through pyramid pooling
Different  from [NN], we train our context network with an explicit distance metric
 Our prior encoding approach is closely related to the  non-parametric approaches for scene parsing [NN, NN, N,  NN, NN, N0]
These methods typically consist of three major  stages: scene retrieval, dense alignment, and MRF/CRF optimization
In the scene retrieval stage, a global descriptor  is used to retrieve a small set of images from the annotated  dataset based on different features, e.g., GIST and HoG  features [NN], dense-SIFT [N0], superpixels and hybrid features [NN, N, NN, NN]
However, these hand-crafted features  are less effective in describing small objects
In this work,  we propose to replace the global descriptor with the deep  features trained by the Siamese network [N] to enhance the  semantic embedding
The dense alignment can be achieved  via the SIFT-Flow [NN], superpixel matching [NN, N0, N, NN],  or exemplar-based classifier [NN]
However, the explicit  dense alignment requires heavy computational loads
In this  work, we generate the prior information without alignment  and pass the priors through convolutional layers
It allows  the segmentation network to learn how to combine the prior  information with local prediction in an end-to-end fashion
 As a core module in our method, the Siamese network [N] can learn the pair-wise relationship between images
The network transforms a classification network to  multiple branches with shared parameters
Such network  structures have been used for re-identification problems  [NN, N] and unsupervised visual learning [N, NN, NN]
 N
Algorithmic Overview  Figure N shows the overview of the proposed algorithm
 We first propagate an input image through two networks:  segmentation network and global context network
The segmentation network (e.g., FCN [N0] or DeepLab [N]) generates the initial parsing results
The global context network  is designed based on a CNN classification network (e.g.,  AlexNet [NN] or VGG [NN]) without the last fully-connected  and softmax layers
The global context network outputs a  fixed length feature vector that embeds the global context  NNNN    Segmentation Network  Global Context Network  Scene  Retrieval  Feature Encoding Prior Encoding  Prior   GenerationInput Image  Parsing Result  Up-sampling  Figure N
Algorithmic overview
We propagate an input image through two networks: segmentation network for initial local label prediction  and global context network for generating global context features
We exploit the learned features with context feature encoding and nonparametric prior encoding
These two modules can be easily applied to existing segmentation networks
After applying our methods to the  segmentation network, the final scene parsing results are obtained through a softmax operation and a bilinear upsampling
 information of the input image
The context embedded features are then combined with the feature maps of the segmentation network, passing through the feature encoding  network to exploit additional information
 In addition to feature encoding, we propose a prior encoding module to combine the non-parametric prior information
We obtain the spatial and global prior information by retrieving the K-nearest annotated images from the training set using the learned context embedded features
 These priors estimate the label distribution of the retrieved  images
We then encode the prior information to the segmentation network using the proposed prior encoding network
To match the size of the input image, we apply the bilinear upsampling on the output of the prior encoding module as the final parsing results
 N
Training Global Context Network  To obtain the global context information, a straightforward approach is to generate scene labels on the image  level, e.g., bedroom, school, or office
However, it requires  additional annotations
Moreover, unlike object categories,  scene categories are often ambiguous, and the boundaries  between some categories are difficult to draw, e.g., a scene  consists of both the street view and the outdoor dining  area
Thus, instead of explicitly inferring the scene categories, we propose to embed the global context into a fixeddimensional semantic space through the global context network
The objective of the global context network is to capture the scene information of the desired semantic embedding properties
For example, the feature distance from a  bedroom scene to a living room scene should be smaller  than that to a beach scene
We observe that semantically  similar scenes share more common classes, e.g
wall, chair,  and sofa for indoor scenes
Toward this end, we design a  distance metric, denoted as ground truth distance, to evaluate the semantic distance between a pair of images based on  their annotated pixel labels
The ground truth distance provides rich context information for the scene parsing task,  and our objective is to utilize such context information in  the testing phase without knowing pixel labels
Therefore,  we propose to train a global context network to generate the  global context features by learning from the ground truth  distance
We demonstrate that the distances between trained  global context features have the similar semantic embedding of the ground truth distance
 Ground Truth Distance
The ground truth distance describes the semantic distance between two images with annotated pixel labels
We denote it as dgt(yi,yj), where yi and yj are the annotated ground truth labels of two images
 To compute the ground truth distance, we first construct  a spatial pyramid on each annotated image
In the spatial  pyramid, we compute the Chi-square distance between the  label histograms of two corresponding blocks at the same  location from two images
We obtain the ground truth distance by summing up the distance of all blocks, i.e.,  dgt(yi,yj) = ∑  s∈S  ∑  c∈C  χN(hi(s, c), hj(s, c)), (N)  where hi(s, c) is the number of pixels belonging to class c at location s in the spatial pyramid
 The purpose of constructing the spatial pyramid is to estimate the scene similarity between images with consideration of the spatial scene layout
In this work, we use a  two-level spatial pyramid where the first level contains only  one block and the second level contains N blocks by divid- ing the image into a N × N grid
We observe that there is no significant difference with more levels of the spatial  pyramid
We choose the Chi-square distance defined by  χN(a, b) = (a − b)N/(a + b) for computing the distance between histograms since the normalization term can remit  NNNN    NNNN NNNNN NNNNN NNNN  NNNN NNNNN NNNNN NNNN  Query  Image  Retrieval Set  (a) Chi-square distance  (b) Chi-square distance with rare class enhancement  NNNN  Figure N
We refine the ground truth distance metric through rare  class enhancement
Given a query image, we show the retrieval set  obtained by using the ground truth distance (a), and with rare class  enhancement (b)
Samples of rare classes can be better retrieved  with the enhanced metric
 the situation that major classes with a large amount of pixels  dominate the distance function
 Rare Class Enhancement
In large-scale vision tasks,  e.g., object detection [NN] and scene parsing [N0, NN], the  distribution of annotated class samples is usually highly  unbalanced with a long-tail distribution
The unbalanced  samples between classes make most learning based methods prone to disregarding the rare/small classes to achieve  higher overall accuracy
In addition, samples of rare classes  only appear in certain specific scene categories, e.g., tent  and microwave, and provide strong cues for the global context inference
In other words, when samples from a rare  class appear in a scene, local informative regions should be  weighted more than the overall global configuration
 We re-weight the histogram hi(s, c) in the ground truth distance dgt by dividing how often the class c appears in the dataset, i.e.,  hri (s, c) = hi(s, c)/f(c), (N)  where f(c) is the amount of images in which class c presents with at least one pixel within the dataset
Figure  N shows an example of the proposed rare class enhancement by comparing the retrieval results using the ground  truth distance
 Siamese Network Training
Given the ground truth distance function dgt between images, we learn a feature space to predict the similar distance embedding
Motivated by  recent methods that utilize the Siamese network to learn  pairwise relationships [N, NN, NN], we train the global context network with a Siamese structure to predict the scene  similarity between image pairs, as shown in Figure N
The  design of the global context network is based on the VGGNN [NN] model that takes a single image as input
We remove  the last fully-connected layer, making fcN layer as the output feature
The Siamese network consists of two identical  N0NN  N0NN  NNN  N  VGG-NN  Shared Parameters  Positive Negative  Figure N
Siamese training of the global context network
The  global context network is a VGG-NN network that outputs a N0NNdimensional feature vector
We train the global context network  using Siamese structure, in which there are two identical networks  with shared parameters
The output feature vectors of the two  branches are concatenated and passed through additional fullyconnected layers and a softmax layer
The target labels are N for  positive pairs and 0 for negative pairs
 global context branches
On top of the two branch network,  two additional fully-connected layers with softmax output  are implemented as a binary classifier
 By using Siamese structure training, we transform the  distance regression task into a binary classification problem
 For each pair of input images, the network classifies it as  either positive (semantically similar) or negative (semantically distant) pair
To extract the positive and negative pairs,  we first form a fully-connected graph where each node is an  image in the annotated dataset, and the edge weight is the  ground truth distance function dgt(yi,yj)
We construct an affinity matrix as  Agt[i, j] =  {  N, j ∈ KNN(i,Ka)  0, otherwise , (N)  where KNN(i,Ka) denotes the set that contains Ka-nearest neighbors of node i with respect to the the ground truth dis- tance dgt(yi,yj)
 Since Ka in the nearest neighbor search is relatively small compared to the number of nodes, most entries in  Agt are zeros and treated as negative pairs
However, it is impractical to train the Siamese network using all the negative pairs
Therefore, we need to sample negative pairs  during training
A straightforward method is random sampling
Nevertheless, not all the negative pairs are equally  informative
A pair with large ground truth distance can be  considered as an easy sample that does not help the network  to learn discriminative features
Toward this end, we apply  a simple strategy to mine the hard-negative pairs by only  sampling negative pairs from the Ka + N nearest neighbor to N -th nearest neighbor, where N is larger than Ka
In this work, we set the N as half the amount of images in the training dataset
 NNNN    query image retrieval set using ground truth distance  annotation retrieval set using trained feature distance  query image retrieval set using ground truth distance  annotation retrieval set using trained feature distance  Figure N
Comparison of retrieval results using ground truth distance and Euclidean distance between trained global context features
The retrieval results using our global context features is  simliar to the ones using the designed ground truth distance
 Figure N shows the retrieval results using the ground  truth distance and Euclidean distance of trained features
 The results show that the trained global context features can  represent similar semantics from the ground truth distance  using the Siamese network training
 N
Non-parametric Prior Generation  Scene Retrieval
Motivated by the non-parametric work on  scene parsing [NN, NN, N, NN, NN, N0], we propose a method  to utilize non-parametric prior in the segmentation network
 Given a query image, we use a global descriptor to retrieve  a small set of semantically similar images from the training dataset
Compared to other methods that use handcrafted features, we use the learned global context features  as the image descriptor
Specifically, given a query image  xi with its global context features fi, we retrieve an image  set {xN,xN, 


,xKp} by performing Kp-nearest-neighbors search with the Euclidean distance between the context features
Note that we use Kp here to differentiate with the Ka in the Siamese training
Figure N also shows some example retrieval sets using both the ground truth distance and  the Euclidean distance between trained features
Since the  retrieval images are semantically close to the query image,  the annotated pixel labels in the retrieval set should also be  similar to the query image and thus can be used to help the  parsing process of the query image
 While most previous methods use dense alignment to  transfer the retrieved labels as the parsing result directly, we  propagate the prior probability through a convolution layer  to jointly predict the results with the segmentation network
 To design the prior, we observe that the stuff classes such  as sky and ground do have strong spatial dependency (i.e.,  sky is usually on the top and ground at the bottom of most  images), while things classes such as chair and person can  appear at most image locations depending on the camera  view angle
Therefore, we propose to exploit two forms of  prior information: spatial prior and global prior
The spatial prior estimates how likely the stuff classes presenting at  each spatial location, while global prior estimates the probability of existence for things classes on the image level
 Spatial Prior
Given a query image, we obtain the retrieval set {xN,xN, 


,xKp} with their annotated images {yN,yN, 


,yKp}
All the annotated images are first re- scaled to the same resolution and divided equally into S×S grids
Then we estimate the spatial prior as  Ps[c, p, q] = N  Kp  ∑  k∈N...Kp  N(yk[p, q], c), (N)  where N(yk[p, q], c) represents how many pixels are la- beled as class c within the specific block at the spatial coor- dinate (p, q) ∈ SN in the labeled image yk
We can observe that Ps is an C × S × S tensor, in which each location is a probability distribution with respect to all classes
 The spatial prior can be seen as a simplified version of local belief in the conventional non-parametric methods
We  estimate the probability in a lower resolution using spatial  grids instead of superpixels, and we do not perform dense  alignment such as SIFT-Flow [NN] or superpixel matching [NN] on the retrieval images to generate a detailed prediction
This is because that our method already has the  accurate local belief provided by the segmentation network,  while the spatial prior information can provide a more consistent global configuration and eliminate false positives of  local predictions
In addition, since we pass the prior information along with the deep features through convolution  layers, the prior information can be propagated through the  convolution operation, letting the network learn how to exploit additional cues through back propagation
 Global Prior
For things classes, we propose to utilize another prior information that is invariant with the spatial location and only estimates the existence of the object classes
 We denote such global prior as Pg which can be simply  NNNN    N x N x N0NN N x N x NN0  NNxNNxNN0  NNxNNxNN0  Segmentation   Network  Global Context   Network  NxN conv  N x N x NNN N x N x NN0  NNxNNxNN0  NNxNNxNN0  Segmentation   Network  NxN conv  NxN conv  NNxNNxNN0 NNxNNxNN0  Spatial Prior  Global Prior  Resize  SxSxNN0  (a) Feature Encoding (b) Prior Encoding  Figure N
Proposed encoding structures
(a) shows the structure to encode the features generated by the global context network
We first  pass the global context features through a fully-connected layer to output features with the same number of channels as the last layer of  the segmentation network
We then add the features to the output of the segmentation network at each spatial location
(b) demonstrates  how we encode the non-parametric prior information
For the spatial prior sampled with an S × S grid, we re-scale it to the same spatial  dimension as the segmentation network output, then we propagate the priors through a convolutional layer
Finally, we add the outputs  of the convolutional layer to the segmentation network output
For the global prior, we encode it using the same structure with feature  encoding
 computed as  Pg[c] = ∑  k∈N...Kp  N(yk, c)/(hk × wk ×Kp), (N)  where N(yk, c) denotes the number of pixels in k-th re- trieval image belonging to class c
We only compute the global prior on the things classes (e.g., person, animal, and  chair) since the prior information of most stuff classes can  be described accurately through the spatial prior in (N)
 N
Global Context Embedding with Networks  With global context features generated by the global context network (Section N) and the non-parametric prior information (Section N), we present how we apply both sets of  context information for scene parsing
 Figure N shows the two modules that encode the global  context features and non-parametric priors, respectively
To  encode features, a naive approach is to duplicate the context features at each spatial location and concatenate them  with feature maps of the last layer in the segmentation network
However, considering the convolutional layer with  the kernel size of N × N, it is mathematically equivalent to passing the context embedded features through a fullyconnected layer, which has the same output channel number  as the feature map of the segmentation network
Then the  output vector is added to each spatial location in the segmentation network as a bias term before the non-linearity  function
This modification can save memory for storing  the duplicate feature vectors and accelerate the computing  process
Furthermore, it makes the module easily applicable to any network without network surgery if it needs to be  initialized from a pre-trained model
 For the prior encoding network, we encode the global  prior using the same structure in the feature encoding network
Since the spatial prior is not a N-dimensional vec- tor, we first perform in-network resizing on the spatial prior  with a bilinear kernel to match the feature map size
After  resizing, we propagate the spatial prior through a convolutional layer and add the output to the feature map of the  segmentation network
 N
Experimental Results  Implementation Details
We use the caffe toolbox [NN]  to train our models with TitanX GPUs with the mini-batch  stochastic gradient descent
To learn the global context network, we train the network in Figure N with mini-batch  size NN
Each mini-batch contains N positive and N nega- tive pairs
We set the nearest neighbor number Ka in equa- tion (N) as N0 for generating the positive pairs
The initial learning rate is set to 0.00N which is reduced to 0.000N af- ter N00, 000 iterations
The momentum is set as 0.N with weight decay 0.000N
We initialize the model with the pre- trained VGG-NN model [NN] on ImageNet and train it for  NN0, 000 iterations
 For the full system training as depicted in Figure N,  we perform experiments using two baseline models: FCNNs [N0] and DeepLab-ResNetN0N [N]
For FCN-Ns, we train  the network with unnormalized softmax with fixed learning  rate Ne−N0
The model is initialized with the pre-trained VGG-NN model on ImageNet
We apply the heavy-learning  scheme in [N0] where we set batch size to N and momentum to 0.NN
For DeepLab-ResNetN0N, we follow the learning rate policy in their paper where initial learning rate is set  to N.Ne−N and is lowered by the polynomial scheme with power 0.N and max iteration NN0000
 We set input crop size as NNN for FCN and NNN for DeepLab since it requires the input edges to be in the form  of NNN + N
For data augmentation, we use randomNNNN    (a) image (b) annotation (c) DeepLab (d) +Feature (e) +Prior (f) +both  Figure N
Representative scene parsing results from the ADEN0k dataset
 Table N
Ka in constructing affinity matrix with Kp = N  Ka N N0 NN N0 FN 0.NNNN 0.NN0N 0.NNNN 0.NNNN  Table N
Kp in prior generation with Ka = N0  Kp N N N N0 FN 0.NNN0 0.NNN0 0.NN0N 0.NNNN  mirroring for both baseline models
When training the  DeepLab network, we additionally apply random scaling  with a choice of N scales {0.N, 0.NN, N.0, N.NN, N.N} for each iteration, and crop the image to the input size with  a random position
Note that we also perform the same  scaling and cropping on the spatial prior
For DeepLab,  since it consists of N branches with different resolution, we  apply the encoding module separately on all the branches  and resize the spatial prior accordingly
For evaluation,  the test image is resized to have the longer edge as NNN (NNN for FCN), and we do not perform multi-scale testing
We also apply dense CRF as the post-processing module  but find there is no significant improvement (±0.N%) since the CRF can only improve the boundary alignment
Also,  it will take another N0-N0 seconds to optimize the dense  CRF for one image
The code and model are available at  https://github.com/hfslyc/GCPNet  Hyperparameters
We analyze two important hyper parameters in the proposed method: N) Ka for constructing affinity matrix in the global context network training in (N);  N) Kp as the amount of retrieved images for generating the priors in (N) and (N)
We choose both values based on the  quality of the scene retrieval results in Table N and Table N
 The retrieved images should contain most classes that  appear in the query image (high recall) and few irrelevant  classes
Thus, we treat the retrieval results as a multi-label  classification problem and evaluate the FN score with dif- ferent parameters on the ADEN0K validation set, where  Fβ = (N+β N)·precision·recall/(βN ·precision+recall)
 We choose FN since we prefer results with the higher recall
 Table N shows the results with different values of Ka
The global context network performs well for a wide range  of values of Ka, and we choose Ka = N0 with the highest FN score
Table N shows the sensitivity analysis on Kp
The proposed method performs best when Kp is set to N
 Quantitative Evaluation of Global Context Features
 To validate our learned global context features, we compare the results using our features to the ones that directly  use fcN of the VGG-NN network pre-trained on ImageNet  with the FN score in Table N
The one using VGG-NN fea- tures only achieves 0.NNNN with Kp = N, which is substan- tially lower than our global context features (0.NN0N)
It  shows that the global context features can learn useful scene  semantic information from the proposed siamese training
 Figure N shows one example of the retrieval results with  these two features
 Complexity Analysis For evaluating a single test image,  our method takes N.N seconds: 0.Ns (NN search) + 0.Ns  (prior generation) + 0.Ns (Network forward) on a single  GPU
The prior encoding module (NN search + prior generation) introduces 0.Ns overhead on CPU, which can be minimized via GPU acceleration
Our module is efficient comNNNN  https://github.com/hfslyc/GCPNet   query image retrieval set using global context features  annotation retrieval set using VGG-NN features pretrained on ImageNet  Figure N
Comparison of retrieval results between VGG-NN feature  and our trained global context feature
We note that none of the  results using the VGG-NN features contains the “bed” label
 pared to other non-parametric methods such as [NN, NN],  since we perform retrieval at the image level with precomputed context features while others perform it either at  the pixel or superpixel level
 MIT ADEN0K Dataset
We first validate our methods on  the recently published MIT ADEN0K dataset [NN]
The  dataset consists of N0,000 image in the train set and N,000  images in the validation set
There are total NN0 semantic  classes, in which NN classes belong to stuff classes, and NNN  classes belong to things classes
The dataset is considered  as one of the most challenging scene parsing datasets due to  its scene variety and numerous annotated object instances
 We first train the global context network on the train set  with Ka as N0 to retrieve positive pairs
The negative pairs are sampled with the strategy mentioned in Section N where  N is set to N0000
The spatial and global prior is generated with the nearest neighbor parameter Kp as N and spatial grid N0× N0
We compare our method with several state-of-the- art models: FCN [N0], DilatedNet [NN], DeepLab [N] and  the cascade model presented along with the dataset [NN]
We  show the performance comparison in Table N
By applying  the feature encoding and the prior encoding on the FCNNs model, we get N.0N% and N.NN% mean-IU improvement, respectively
We also apply our modules on the DeepLab  model [N] based on ResNet-N0N [N0]
By applying the feature encoding, we have N.NN% mean-IU improvement over the baseline
Prior encoding brings similar improvement  with N.NN% difference
We also combine both modules and perform joint training and achieve NN.NN% mean-IU with N.NN% improvement over the baseline model
We also show the qualitative comparison of our modules in Figure N
In  addition, if we define the most frequent N0 classes as the  common classes and the rest of them as the rare classes,  the average improvement of mean IU is N.NN% for com- mon classes and N.NN% for rare classes
This shows that our  Table N
Results on the MIT ADEN0k validation set
 Methods Pixel Accuracy Mean IU  FCN-Ns [NN] NN.NN NN.NN  DilatedNet [NN] NN.NN NN.NN  Cascade-DilatedNet [NN] NN.NN NN.N  FCN-Ns + Feature NN.NN NN.NN  FCN-Ns + Prior NN.00 NN.NN  DeepLab [N] NN.N0 NN.NN  DeepLab + Feature NN.NN NN.0N  DeepLab + Prior NN.NN NN.NN  DeepLab + Feature + Prior NN.NN NN.NN  Table N
Results on the PASCAL-Context validation set
 Methods Pixel Accuracy Mean IU  ONP [N] N/A NN.N  CFM [N] N/A NN.N  FCN-Ns [N0] N/A NN.NN  CRF-RNN [NN] N/A NN.NN  BoxSup [N] N/A N0.N  HO CRF [NN] N/A NN.N  DeepLab NN.NN NN.NN  DeepLab + Feature + Prior NN.N0 NN.NN  method can improve both common and rare classes without  sacrificing the other
 PASCAL Context Dataset
We also evaluate our method  on the PASCAL Context dataset [NN]
It consists of N0  classes with NNNN images in the train set and NN0N images  in the val set
The performance comparison is shown in Table N
We apply both global context feature encoding and  prior encoding on top of the baseline model
Different from  the models on the MIT ADEN0k dataset, we generate the  spatial and global prior information on both stuff and things  classes
Both the baseline and our models are trained N0000  iterations with batch size N0
Our method achieves NN.N0% pixel accuracy and NN.NN% mean IU, which has the favor- able performance against state-of-the-art methods
 N
Conclusion We present a novel scene parsing system that exploits the  global context embedding representation
Through learning from the scene similarity, we generate a global context  representation for each image to aid the segmentation network
We show that the proposed algorithm, which consists  of feature encoding and non-parametric prior encoding, can  be applied to most state-of-the-art segmentation networks
 Based on the proposed method, we achieve significant improvement on both the challenging MIT ADEN0K dataset  and the PASCAL Context dataset
 Acknowledgments This work is supported in part by the NSF CAREER  Grant #NNNNNNN, gifts from Adobe and NVIDIA
 NNNN    References  [N] E
Ahmed, M
Jones, and T
K
Marks
An improved deep  learning architecture for person re-identification
In CVPR,  N0NN
N  [N] J
Carreira, R
Caseiro, J
Batista, and C
Sminchisescu
Semantic segmentation with second-order pooling
In ECCV,  N0NN
N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Deeplab: Semantic image segmentation with  deep convolutional nets, atrous convolution, and fully connected crfs
arXiv:NN0N.00NNN, N0NN
N, N, N  [N] S
Chopra, R
Hadsell, and Y
LeCun
Learning a similarity  metric discriminatively, with application to face verification
 In CVPR, N00N
N  [N] M
Cordts, M
Omran, S
Ramos, T
Rehfeld, M
Enzweiler,  R
Benenson, U
Franke, S
Roth, and B
Schiele
The  cityscapes dataset for semantic urban scene understanding
 N0NN
N  [N] J
Dai, K
He, and J
Sun
Boxsup: Exploiting bounding  boxes to supervise convolutional networks for semantic segmentation
In ICCV, N0NN
N  [N] J
Dai, K
He, and J
Sun
Convolutional feature masking for  joint object and stuff segmentation
In CVPR, N0NN
N  [N] C
Doersch, A
Gupta, and A
A
Efros
Unsupervised visual representation learning by context prediction
In CVPR,  N0NN
N, N  [N] D
Eigen and R
Fergus
Nonparametric image parsing using  adaptive neighbor sets
In CVPR, N0NN
N, N, N  [N0] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N  [NN] G
Heitz and D
Koller
Learning spatial context: Using stuff  to find things
In ECCV, N00N
N, N  [NN] J
helhamer, Evan an Long and T
Darrell
Fully convolutional networks for semantic segmentation
TPAMI, N0NN
 N  [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional  architecture for fast feature embedding
In ACM MM, N0NN
 N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, N0NN
N  [NN] L
Ladicky, C
Russell, P
Kohli, and P
H
Torr
Graph  cut based inference with co-occurrence statistics
In ECCV,  N0N0
N  [NN] D
Li, W.-C
Hung, J.-B
Huang, S
Wang, N
Ahuja, and  M.-H
Yang
Unsupervised visual representation learning by  graph-based consistent constraints
In ECCV, N0NN
N, N  [NN] G
Lin, C
Shen, A
van dan Hengel, and I
Reid
Efficient  piecewise training of deep structured models for semantic  segmentation
In CVPR, N0NN
N  [NN] C
Liu, J
Yuen, and A
Torralba
Nonparametric Scene Parsing via Label Transfer
TPAMI, NN(NN):NNNN–NNNN, N0NN
N,  N, N, N  [NN] Z
Liu, X
Li, P
Luo, C
C
Loy, and X
Tang
Semantic  Image Segmentation via Deep Parsing Network
In ICCV,  N0NN
N  [N0] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
N, N,  N, N  [NN] R
Mottaghi, X
Chen, X
Liu, N.-G
Cho, S.-W
Lee, S
Fidler, R
Urtasun, and A
Yuille
The Role of Context for  Object Detection and Semantic Segmentation in the Wild
In  CVPR, N0NN
N, N  [NN] W
Ouyang, X
Wang, C
Zhang, and X
Yang
Factors in  finetuning deep model for object detection with long-tail distribution
In CVPR, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 N, N, N  [NN] G
Singh and J
Kosecka
Nonparametric Scene Parsing  with Adaptive Feature Relevance and Semantic Context
In  CVPR, N0NN
N, N, N  [NN] J
Tighe and S
Lazebnik
SuperParsing - Scalable Nonparametric Image Parsing with Superpixels
IJCV, N0N:NNN–NNN,  N0N0
N, N, N, N  [NN] J
Tighe and S
Lazebnik
Finding Things: Image Parsing  with Regions and Per-Exemplar Detectors
In CVPR, N0NN
 N, N, N  [NN] Y.-H
Tsai, X
Shen, Z
Lin, K
Sunkavalli, and M.-H
Yang
 Sky is not the limit: Semantic-aware sky replacement
SIGGRAPH, N0NN
N  [NN] R
Vemulapalli, O
Tuzel, M.-Y
Liu, and R
Chellappa
 Gaussian conditional random field network for semantic segmentation
In CVPR, N0NN
N  [NN] X
Wang and A
Gupta
Unsupervised learning of visual representations using videos
In ICCV, N0NN
N, N  [N0] J
Yang, B
Price, S
Cohen, and M.-H
Yang
Context Driven  Scene Parsing with Attention to Rare Classes
In CVPR,  N0NN
N, N, N, N  [NN] D
Yi, Z
Lei, S
Liao, S
Z
Li, et al
Deep metric learning  for person re-identification
In ICPR, N0NN
N  [NN] F
Yu and V
Koltun
Multi-scale context aggregation by dilated convolutions
In ICLR, N0NN
N, N  [NN] H
Zhao, J
Shi, X
Qi, X
Wang, and J
Jia
Pyramid scene  parsing network
CVPR, N0NN
N  [NN] S
Zheng, S
Jayasumana, B
Romera-Paredes, V
Vineet,  Z
Su, D
Du, C
Huang, and P
H
Torr
Conditional random fields as recurrent neural networks
In ICCV, N0NN
N,  N  [NN] B
Zhou, H
Zhao, X
Puig, S
Fidler, A
Barriuso, and A
Torralba
Semantic understanding of scenes through the adeN0k  dataset
CVPR, N0NN
N, N, N  [NN] Y
Zhu, R
Urtasun, R
Salakhutdinov, and S
Fidler
 segDeepM: Exploiting segmentation and context in deep  neural networks for object detection
In CVPR, N0NN
N  NNNNDual-Glance Model for Deciphering Social Relationships    Dual-Glance Model for Deciphering Social Relationships  Junnan Li  Graduate School for Integrative Sciences and Engineering  National University of Singapore  Singapore  lijunnan@u.nus.edu  Yongkang Wong  Interactive & Digital Media Institute  National University of Singapore  Singapore  yongkang.wong@nus.edu.sg  Qi Zhao  Department of Computer Science and Engineering  University of Minnesota  Minneapolis, USA  qzhao@cs.umn.edu  Mohan S
Kankanhalli  School of Computing  National University of Singapore  Singapore  mohan@comp.nus.edu.sg  Abstract  Since the beginning of early civilizations, social relationships derived from each individual fundamentally form the  basis of social structure in our daily life
In the computer  vision literature, much progress has been made in scene understanding, such as object detection and scene parsing
 Recent research focuses on the relationship between objects based on its functionality and geometrical relations
 In this work, we aim to study the problem of social relationship recognition, in still images
We have proposed a dualglance model for social relationship recognition, where the  first glance fixates at the individual pair of interest and the  second glance deploys attention mechanism to explore contextual cues
We have also collected a new large scale People in Social Context (PISC) dataset, which comprises of  NN,NN0 images and NN,NNN annotated samples from N types  of social relationship
We provide benchmark results on the  PISC dataset, and qualitatively demonstrate the efficacy of  the proposed model
 N
Introduction  Social relationships derived from each individual fundamentally form the basis of social structure in our daily  life
Naturally, we perceive and interpret a scene with an  understanding of the social relationships of the people in  the scene
Sociology research shows that such social understanding of people permits inference about their characteristics and their possible behaviors [NN]
 In computer vision, social information has been exploited to improve several analytics tasks, including human  Figure N: Example images from the new People in Social Context (PISC) dataset
 trajectory prediction [N, N0], multi-target tracking [N, NN],  and group activity recognition [N, NN, NN]
In image understanding task, visual concepts recognition is gaining more  attention, which include visual attribute [NN] and visual relationship [NN]
On the other hand, social attribute and social  relationship [NN] are equally important concepts for scene  understanding, but have received less attention in the research community
In this work, we aim to address the problem of social relationship recognition
Understanding such  relationship can enable a well designed algorithm to genNNNN0    erate better descriptions for a scene
For instance, the first  image in Figure N can be described as ‘Grandma is holding  her grandchild’, rather than ‘A person is holding a baby’
 With reference to the relational models theory [NN], we  define a hierarchical social relationship categories which  embed the coarse-to-fine characteristic of common social  relationships (as illustrated in Figure N)
Our definition  follows a prototype-based approach, where we are interested in finding exemplars that most parsimoniously describe the most common situations, rather than an ambiguous definition that could cover all possible cases
The presented recognition problem differs from the visual relationship detection problem in [NN]
We argue that inferring  social relationship requires a higher level of understanding  about the scene
This is because humans make such inferences not only based on the physical appearance (e.g., color  of clothes, gender, age, etc.), but also from subtler cues  (e.g., expression, action, proximity, and context) [N, NN, NN]
 Recognizing social relationships from still images is  challenging due to the wide variations in scale, scene, pose,  and appearance
In this work, we propose a dual-glance  model, which exploits information from a target individual  pair as well as the surrounding contextual cues
The key  contributions can be summarized as:  • The proposed dual-glance model mimics human visual system to explore useful and complementary visual  cue for social relationship analysis
The first glance  fixates at the individual pair of interest, and performs  coarse prediction based on its appearance and geometrical information
The second glance exploits contextual cues from regions generated from Region Proposal  Network (RPN) [NN] to refine the coarse prediction
 • We propose Attentive RCNN, where attention is allo- cated for each contextual region
The attention mechanism is guided by both bottom-up and top-down signals
Better performance is achieved by selectively focusing on the relevant regions
 • To enable this study, we collected a novel People in Social Context (PISC) datasetN
It consists of NN,NN0  images and NN,NNN manually annotated labels from N  types of social relationship
In addition, PISC also  consists of NN annotated occupation categories
To the  best of our knowledge, PISC is the first public dataset  for social relationship analysis
 The remaining of the paper is organized as follows
Section N reviews the related work
Section N delineates the  details of the new PISC dataset
Section N elaborates on the  details of the proposed framework, and the empirical evaluation is shown in Section N
Section N concludes the paper
 Nhttps://doi.org/N0.NNNN/zenodo.NNN0NN  Intimate Relation  Non-Intimate RelationNo Relation  Friends  FamilyMembers  Couple  Professional  Commercial  Has Relation  Figure N: Defined hierarchical social relationship categories
 N
Related Work  N.N
Social Relationship  The study of social relationships lies at the heart of social sciences
There are two forms of representations for  relational cognition
The first approach represents relationship with a set of theorized or empirically derived dimensions [N]
The other form of representation proposes implicit categories for relation cognition [NN]
One of the  most extensively accepted categorical theory is the relational models theory [NN]
It offers a unified account of  social relations by proposing four elementary prototypes,  namely communal sharing, equality matching, authority  ranking, and market pricing
 In the computer vision literature, social information has  been widely adopted as supplementary cues to other tasks
 Gallagher et al
[NN] extract features describing group structure to aid demographic recognition
For group activity  recognition, social roles and relationship information have  been implicitly embedded into the inference model [N, N,  N, NN, NN]
Alletto et al
[N] define ‘social pairwise feature’ based on F-formation and use it for group detection  in egocentric videos
Recently, [N, N0] model social factor  for human trajectory prediction
 There have been studies that explicitly focus on recognition of social attributes and social structures
Wang et  al
[NN] first study familial social relationship recognition in  personal image collections
Kinship verification [N, NN, NN]  and kinship recognition [N, NN] have been extensively studied
Zhang et al
[NN] study facial traits (e.g., friendly, dominant, etc.) that are informative of social relationships
For  video based analysis, Ding and Yilmaz discover social communities formed by actors in movies [N]
Ramanathan et  al
[NN] study weakly supervised social role discovery in  events
 Our study partially overlaps with the field of social signal processing [NN], which aims to understand social signals  and social behaviors using multiple sensors, such as role  recognition, influence ranking, and dominance detection in  group meeting [N0, NN, NN]
Our work substantially differs  from the aforementioned studies
Unlike facial attributes  based social relationship study [N, NN, NN, NN], we study people in complex daily scenes with uncontrolled poses and orientations
Furthermore, we focus on general social relationships, rather than kinship in family photos [N, N, NN, NN, NN]
 NNNNN  https://doi.org/N0.NNNN/zenodo.NNN0NN   Table N: Instructions provided to annotators
 Relationship Description Examples  Professional The people are related based on co-worker; coach & player;  their professions boss & staff  Commercial One person is paying money to receive salesman & customer;  goods/service from the other tour guide & tourist  Figure N: Example of social relationship labels that are not agreed among annotators
 Different from video-based studies [N, NN], we focus on visual information from a single image
 N.N
Multiple-Instance Learning  The proposed Attentive RCNN is inspired by MultipleInstance Learning (MIL)
MIL is a weakly-supervised  learning approach which trains a classifier with bags of instances and bag-level labels
Recently, researchers explored  MIL with deep feature representations
Wu et al
[NN] propose a deep MIL framework to exploit correspondences between keywords and image regions for image classification  and annotation, while a similar technique was adopted to  detect salient concepts for image captions generation [N0]
 Inspired by MIL, Gkioxari et al
[NN] propose R*CNN
Different from previous approaches, it localizes target region  for action recognition by exploiting complementary representative cue from a set of candidate regions in an image
 Attention model has been recently proposed and applied to image captioning [NN, NN], image question answering [N0] and fine-grained classification [NN]
We modify  R*CNN with attention mechanism to better exploit contextual cues
We treat the attention weights for the contextual  regions as latent variable, which can be inferred with a forward pass of the model
 N
People in Social Context Dataset  The People in Social Context (PISC) dataset is the first of  its kind that focuses on social relationships
It was collected  through a pipeline of three stages
In the first stage, we collected around N0k images containing people from a variety  of sources, including Visual Genome [NN], MS-COCO [NN],  YFCCN00M [NN], Flickr, Instagram, Twitter and commercial search engines (i.e
Google and Bing)
We used a  combination of key words search (i.e
co-worker, people,  friends, etc.) and people detector (Faster RCNN [NN]) to  collect the image
The collected images have high variation  in image resolution, people’s appearance, and scene type
 In the second and third stage, we hired workers from  CrowdFlower platform to perform labor intensive task of  0N000N0000NN000N0000NN000N0000NN000 Number of Occurences  intimate  non-intimate  no relation  not sure couple family members friends commercial professional  0.N 0.N 0.N 0.N 0.N N Agreement Rate  intimate non-intimate  no relation friends family  couple professional commercial  Figure N: Annotation statistics of the relationship categories
 manual annotation
The second stage focused on the annotation of person bounding box in each image
Following [NN], each bounding box is required to strictly satisfy  the coverage and quality requirements
To speed up the annotation process, we first deployed Faster RCNN to detect  people on all images, followed by asking the annotators to  re-annotate the bounding boxes if the computer-generated  bounding boxes were inaccurately localized
Overall, N0%  of the computer-generated boxes are kept
For images collected from MSCOCO and Visual Genome, we directly  used the provided groundtruth bounding boxes
 Once the bounding boxes of all images had been annotated, we selected images consisting of at least two people  who occupy a significant amount of region, and avoided images that contain crowds of people where individuals cannot be distinguished
In the final stage, we requested the  annotators to identify the occupation of all individuals in  the image, as well as the social relationships of all potential  individual pairs
To ensure consistency in the occupation  categories, the annotation is based on a list of reference occupation categories
The annotators could manually add a  new occupation category if it was not in the list
 For social relationships, we formulate the annotation  task as multi-choice questions based on the hierarchical  structure in Figure N
We provide instructions (see Table N)  to help the annotators distinguish between professional and  commercial relationship
Annotators can choose the option  ‘not sure’ at any level if they cannot confidently identify the  relationship
Each image was annotated by five workers,  and the final decision is determined by majority voting
If  the five workers do not reach an agreement (e.g
N-N-N), the  annotation will be treated as invalid (see Figure N)
Overall,  N,NNN unique workers have contributed to the annotation
 The PISC dataset consists of NN,NN0 images
The average number of people per image is N.NN
For the social  relationships, we consider each individual pair as one sample
In total, we collected NN,NNN valid samples
The distribution for each types of relationships and their agreement  rate is shown in Figure N
The agreement rate is calculated  by dividing the number of correct human judgments (judgments that agree with the majority) with the total number  of judgments
For occupations, N0,0NN images contain people that have recognizable occupations
In total, there are  NN identified occupation categories
The number of occupation occurrence and the workers’ agreement rate for the  NNNNN    N00N000 Number of Occurences (log scale)  skier baseball player laborer/worker frisbee player soccer player tennis player musician/instrumentalist military/soldier skateboarder cook/chef vendor/salesperson police officer tourist student surfer businessperson cyclist photographer/cameraman motorcyclist fireman/firewoman politician sports referee reporter doctor runner office clerk  0.N 0.N 0.N 0.N 0.N N Agreement Rate  Figure N: Annotation statistics of the top NN occupations
 NN most frequent occupation categories are shown in Figure N
A lower agreement rate indicates that the occupation  is harder to visually discriminate (e.g
‘politician’ and ‘office clerk’)
Since two source datasets, i.e
MS-COCO and  Visual Genome, are highly biased towards ‘baseball player’  and ‘skier’, we limit the total number of instances per occupation to N000 based on agreement rate ranking to ensure  there are no bias towards any particular occupation
 N
Proposed Dual-Glance Model  Given an image I and a target pair of people highlighted  by bounding boxes {bN, bN}, our goal is to infer their social  relationship r
In this work, we propose a dual-glance relationship recognition model, where the first glance fixates at  bN and bN, and the second glance explores contextual cues  from multiple region proposals R
The final score over possible relationships, S, is a weighted sum of the two scores  via  S = SN(I, bN, bN) + αSN(I, bN, bN,R)
(N)  We use softmax to transform the final score into a probability distribution
Specifically, the probability that a given  pair of people having relationship r is given as  pr = exp(Sr)∑ r exp(Sr)  
(N)  An overview of the proposed model is shown in Figure N
 N.N
First Glance  The first glance takes in input I and two bounding boxes
 We first crop three patches from I, where the first two cover  each person, p N  and p N , and one for the union region, p  ∪ ,  that tightly covers both people
These patches are resized  to NNN × NNN pixels and fed into three CNNs, The outputs  from the last convolutional layer are flattened and concatenated
p N  and p N  are processed by CNNs that share the same  weights
 We denote the geometry feature of the bounding box i as  b pos  i = {x min i , y  min i , x  max i , y  max i , areai} ∈ R  N, where all the  parameters are relative values, normalized with zero mean  and unit variance
bposN and b pos  N are concatenated and processed by a fully-connected (fc) layer
We concatenate its  output with the CNN features for p N , p  N and p  ∪ to form a  single feature vector, which is subsequently passed through  another two fc layers to produce first glance score, sN
We  use vtop ∈ R k to denote the output from the penultimate fc  layer
vtop serves as a top-down signal to guide the attention  mechanism in the second glance
We experimented with  different values of k, and set k as N0NN
 N.N
Attentive RCNN for Second Glance  For the second glance, we adapt Faster RCNN [NN] to  make use of multiple contextual regions
Faster RCNN processes the input image I with Region Proposal Network  (RPN) to generate a set of region proposals P I with high  objectness
For each target pair with bounding boxes bN and  bN, we select the set of contextual regions R(bN, bN; I) from  P I as  R(bN, bN; I) = {c ∈ P I : max(G(c, bN), G(c, bN)) < τu} (N)  where G(bN, bN) computes the Intersection-over-Union  (IoU) between two regions, and τu is the upper threshold for  IoU
The threshold encourages the second glance to explore  cues different from the first glance
It’s effect is reported in  Section N.N
 We then process I with a CNN to generate a convolutional feature map conv(I)
For each contextual region c ∈  R, ROI pooling is applied to extract a fixed-length feature  vector v ∈ Rk from conv(I)
Denote {vi|i = N, N, 


, N} as  the bag of N feature vectors for R, also given the high-level  feature vector from the first glance vtop, we first combine  them into a hidden vector hi ∈ R k via  hi = vi +wtop ⊗ vtop, (N)  where wtop ∈ R k, and ⊗ is the element-wise multiplication  of two vectors
Then, we calculate the attention ai ∈ [0, N] over the ith region proposal as  ai = N  N + exp(−(W h,ahi + ba)) , (N)  where W h,a ∈ R N×k is the weight matrix, and ba ∈ R is  the bias term
The attention over each contextual region is  guided by both bottom-up signal from local region vi and  top-down signal from the first glance model vtop
Hence, the  weighted feature vector for region i is computed via  v att i = aivi
(N)  The obtained vatti is processed by the last fc layer to generate the output score for the ith region proposal  si = W sv att i + bs
(N)  NNNNN    CNN  CNN  CNN  share	weights  �" #$%  fc 	( N N N -d )  N 0 N N -d  N 0 N N -d  N 0 N N -d  fc 	( N 0 N N -d )  S co re  CNN conv  features	of	region	proposals  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  Attention	  Module  weighted	feature	vectors  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  fc 	( N 0 N N -d )  ×  S co re  S co re  attention	weights +  S o ft m a x  0.N  0.0N  0.N  Friends  Family  Couple  …  �( #$%  �∪  First	Glance  Second	Glance  lse  �"  �(  Figure N: An overview of the proposed dual-glance model
The first glance looks at the pair of people in question and makes a coarse prediction
The second glance looks at region proposals, allocates attention to each region, and aggregates their outputs to refine the score
 The attention is guided by both top-down signal from the first glance, and bottom-up signal form the local context
 Functions to aggregate scores, i.e., f(si), from a bag  of instances i = N, N, 


, N include max(si), avg(si), and  log[N + ∑N  i=N exp(si)] (log-sum-exp, denoted as lse(si))
In  our experiment (Section N.N), we evaluated all three variants  of f(si) and the results show that lse(si) provides the best  performance
Hence, the score of the second glance model  is  SN = log[N + ∑N  i=N exp(si)]
(N)  N
Experiment  N.N
Dataset and Training Details  In this work, we conducted experiments on the proposed PISC dataset (Section N) and evaluated our proposed  method with two recognition tasks
The first task, denoted  as N-relationship recognition, focuses on three coarse-level  relationship categories, namely No Relation, Intimate Relation, and Non-Intimate Relation
We randomly select  N,000 images (NN,NNN samples) as test set, and use the remaining images as training set
The second task, denoted  as N-relationship recognition, focuses on finer relationships  listed in Figure N
Since the data label is unbalanced (fewer  images with Couple or Commercial relationship), we split  N,N00 images (N,NNN samples) into test set and ensure it contains around N00 samples for each of the six relationships
 The relationship imbalance reflects their frequency of  occurrence, which is also observed in [NN]
To address  this, we adopt oversampling and undersampling strategies
 Specifically, we oversample the minority labeled samples  by reversing the pair of people (i.e., if pN and pN are a couple, then pN and pN are also a couple), and by horizontally  flipping the image
We undersample the majority labeled  samples using stratified sampling scheme to ensure the samples in each batch is balanced
 In this work, we train our model with Stochastic Gradient Descent using backpropagation
First, we train the firstglance model until the loss converges, then we freeze the  first-glance model, and train the second-glance model
For  the first glance model, we fine-tune the ResNet-N0N model  pre-trained on ImageNet classification task [NN]
For the  second glance model, we fine-tune the VGG-NN model pretrained on ImageNet detection task [NN]
We set the learning rate as 0.00N, while the fine-tuning model has a lower  learning rate of 0.000N
We use a batch size of NN and a  momentum of 0.N during training
 During the test stage, we found that the performance  would slightly improve if we feed the model twice with  {bN, bN} and {bN, bN}, and take their average as the final score
However, the performance gain (0.N%-N%) doubles  the time budget, and we do not recommend it in practice
 N.N
Single-Glance vs
Dual-Glance  As there exists limited literature on this problem, we  evaluate multiple variants of our model as baseline and  compare them to the proposed dual-glance mode to show its  efficacy
Formally, the compared methods are as followed:  N
Union-CNN: Following the predicate prediction  model in [NN], a single CNN model is used to classify  the union region of the individual pair of interest
 N
BBox: We only use the geometry feature of the two  bounding boxes to infer the relationship
 N
Pair-CNN: The model consists of two CNNs with  shared weights
The input is the cropped image  patches for the two individuals
 NNNNN    Table N: Recall-per-class and mean average precision (mAP) of baselines and our proposed dual-glance model on the PISC dataset
 N-relationship N-relationship  In ti  m at  e  N o  n -I  n ti  m at  e  N o  R el  at io  n  m A  P  F ri  en d  s  F am  il y  C o  u p  le  P ro  fe ss  io n  al  C o  m m  er ci  al  N o  R el  at io  n  m A  P  Union-CNN [NN] NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.0 NN.N NN.N  BBox NN.N NN.0 NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  Pair-CNN N0.N N0.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  Pair-CNN+BBox NN.N N0.N N0.N NN.N N0.N N0.N NN.N NN.N NN.N N0.N NN.N  Pair-CNN+BBox+Union NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N  Pair-CNN+BBox+Global N0.N N0.N NN.N N0.N NN.N NN.N NN.N N0.N NN.N NN.0 NN.N  Pair-CNN+BBox+Scene NN.0 N0.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.0 NN.N NN.N  RCNN NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N NN.N  Dual-Glance NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N NN.N  N
Pair-CNN+BBox: We extend Pair-CNN by using the  geometry feature of the two bounding boxes
 N
Pair-CNN+BBox+Union: The first glance model  as illustrated in Figure N, which combines PairCNN+BBox and Union-CNN
 N
Pair-CNN+BBox+Global: Instead of the union region, we use the entire image as input to Union-CNN
 N
Pair-CNN+BBox+Scene: The Union-CNN is replaced with a Scene-CNN pre-trained on Places [NN]
 It extracts scene information using the entire image
 N
RCNN: We train a RCNN using the region proposals  P I , and adopt average pooling to combine the features
 N
Dual-Glance: Our proposed model (Section N)
 Table N shows the results on the test set for both the Nrelationship recognition task and the N-relationship recognition task
Union-CNN and RCNN, are incapable to recognize No Relation
This is because these model don’t  know the pair of people in question, and would recognize  other salient relationships
Pair-CNN+BBox outperforms  Pair-CNN, which suggests that peoples’ geometric position in an image contains information useful to infer their  relationship, especially for No Relation
This is supported  by the law of proxemics defined in the book ”The Silent  Language” [NN]
However, the position of bounding boxes  alone cannot be used to predict relationship, as shown by  the results of BBox
 Adding Union-CNN to Pair-CNN+BBox improves performance
However, the performance gain is slight if we  use the global context (entire image) rather than local context (union region)
Furthermore, the performance even degrades when the global context incorporates scene information, suggesting that social relationships are independent of  Figure N: Examples where dual-glance model correctly predict the relationship (yellow label) while the first-glance model fails  (blue label)
GREEN boxes highlight the pair of people in question, and the top two contextual regions with highest attention are  highlighted in RED
 scene types
RCNN demonstrates the effectiveness of using  contextual regions, particularly for Intimate Relation and  Non-Intimate Relation
 The proposed Dual-Glance significantly outperforms all  baseline models
Figure N shows some intuitive illustrations  where proposed model correctly classifies relationships  misclassified by the first-glance (Pair-CNN+BBox+Union)  model
 Across all models, Friends and Commercial are more  difficult to recognize
This is consistent with the agreement  rate in Figure N, which indicates that Friends and Commercial are less visually distinguishable
Figure N shows the  confusion matrix of N-relationship recognition task
The  three intimate relationships (Friends, Family, Couple) are  more often to be confused with each other than with nonintimate relationships, suggesting they share similar visual  NNNNN    Figure N: Confusion matrix of N-relationship recognition task with the proposed dual-glance model
 Table N: mAP (%) of the proposed dual-glance model with and without attention mechanism using various aggregattion functions
 Without Attention With Attention  avg(·) lse(·) max(·) avg(·) lse(·) max(·)  N-relationship NN.N NN.0 NN.N NN.N NN.N NN.N  N-relationship NN.N NN.N NN.N NN.N NN.N NN.N  features
However, the non-intimate relationships (Professional, Commercial) do not tend to be easily confused with  each other
 N.N
Analysis on Attention Mechanism  Here, we remove the attention module and compare  it with our proposed dual-glance model
For the second  glance, we experiment with three widely used aggregation  functions f(·), which are avg(·), lse(·) and max(·)
The results are shown in Table N
Adding attention mechanism  improves performance for all three aggregation functions
 For Dual-glance without attention, max(·) performs best, which conforms to the results in [NN, NN]
While for Dualglance with attention, lse(·) performs best
The reason is that max(·) uses a single instance to infer  the label for an entire bag
It works well in the presence of a  ‘strong’ instance, but sometimes there is no strong instance,  but several ‘weak’ instances
On the other hand, lse(·) and avg(·) consider all instances in a bag, but could be distracted by irrelevant instances
However, with properly guided attention, lse(·) and avg(·) can better exploit the collaborative power of relevant instances for more accurate inference
 N.N
Variations of Contextual Regions  Since RPN can generate hundreds of region proposals  per image, we suppress those proposals with non-maximum  suppression (NMP)
We vary m, the maximum number of  N0 N0 N0 N0 N0 N0 m (τu=0.N)  N0 NN N0 NN N0 NN N0  Mea n A  P  0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N N τu (m=N0)  N0 NN N0 NN N0 NN N0  Mea n A  P  N relationship N relationship  Figure N: Evaluation of dual-glance model over variations in maximum number of region proposals (Left) and upper threshold  of overlap between region proposals and the pair of people (Right)
 Professional Commercial  Couple Family  Professional No	Relation  Figure N0: Illustration of the proposed attentive RCNN
GREEN boxes highlight the pair of people in question, and RED box highlights the context region with the highest attention
For each target  pair, the attention mechanism fixates on different region
 region proposals used, as well as τu, the upper threshold  of overlap between a region proposal and the target people
 We experimented with different combinations of m and τu with the dual-glance model
As shown in Figure N, m = N0 and τu = 0.N produce the best performance
 N.N
Visualization of Examples  The attention mechanism enables different pairs of people in question to exploit different contextual cues
Some  examples are shown in Figure N0
In the second row, the  little girl in red box is useful to infer that the other girl on  her left and the woman on her right are family members, but  her existence indicates little of the couple in black
 Figure NN shows examples of correct recognition for  each relationship category in the test set
We can observe  that the proposed model learns to recognize social relationship from a wide range of visual cues including clothNNNNN    Family CoupleFriends  Intimate Relation Commercial Professional  Non-Intimate Relation No Relation  Figure NN: Example of correct predictions on PISC dataset
Green boxes highlight the targets, and red box highlights the contextual region with highest attention
 Figure NN: Examples of incorrection predictions on PISC dataset
Yellow labels are the ground truth, and blue labels are the model’s  predictions
 ing, environment, surrounding people/animals, contexual  objects, etc
For intimate relationships, the contextual cues  varies from beer (friends), gamepad (friends), TV (family),  to cake (couple) and flowers (couple)
In terms of nonintimate relationships, the contextual cues are related to the  occupations of the individuals
For instance, goods shelf  and scale indicate commercial relationship, while uniform  and documents imply professional relationship
Figure NN  shows the misclassified cases
The proposed model fails to  recognize the gender (misclassifies friends as couple in the  image at row N column N), or picks up the wrong cue (the  white board instead of the vegetable in the image at row N  column N)
 N
Conclusion  In this study, we aim to address pairwise social relationship recognition, a key challenge to bridge the social gap  towards higher-level social scene understanding
To this  end, we propose a dual-glance model, which exploits useful information from the individual pair of interest as well  as multiple contextual regions
We incorporate attention  mechanism to assess the relevance of each region instance  with respect to the target pair
We evaluate the proposed  model on PISC dataset, a large-scale image dataset we collected to facilitate research in social scene understanding
 We demonstrate both quantitatively and qualitatively the efficacy of the proposed model
We also experiment with a  few variants of the proposed system to explore information  useful for social relationship inference
 Our work is the first step towards general social scene  understanding in a data-driven fashion
The PISC dataset  provides further potential in this line of research, including  but not limited to group social relation analysis, occupation  recognition, and joint inference of social role and social relationships
We intend to address some of those challenges  in future work
 Acknowledgment  This research is supported by the National Research Foundation, Prime Minister’s Office, Singapore under its International Research Centre in Singapore Funding Initia- tive
 References  [N] A
Alahi, K
Goel, V
Ramanathan, A
Robicquet, L
Fei-Fei,  and S
Savarese
Social LSTM: Human trajectory prediction  NNNNN    in crowded spaces
In CVPR, pages NNN–NNN, N0NN
N, N  [N] S
Alletto, G
Serra, S
Calderara, F
Solera, and R
Cucchiara
From ego to nos-vision: Detecting social relationships in first-person views
In CVPR Workshops, pages NNN–  NNN, N0NN
N  [N] Y
Chen, W
H
Hsu, and H
M
Liao
Discovering informative social subgraphs and predicting pairwise relationships  from group photos
In ACMMM, pages NNN–NNN, N0NN
N  [N] W
Choi and S
Savarese
A unified framework for multitarget tracking and collective activity recognition
In ECCV,  pages NNN–NN0, N0NN
N, N  [N] H
R
Conte and R
Plutchik
A circumplex model for interpersonal personality traits
Journal of Personality and Social  Psychology, N0(N):N0N, NNNN
N  [N] Z
Deng, A
Vahdat, H
Hu, and G
Mori
Structure inference  machines: Recurrent neural networks for analyzing relations  in group activity recognition
In CVPR, pages NNNN–NNNN,  N0NN
N  [N] H
Dibeklioglu, A
A
Salah, and T
Gevers
Like father, like  son: Facial expression dynamics for kinship verification
In  ICCV, pages NNNN–NN0N, N0NN
N  [N] L
Ding and A
Yilmaz
Learning social relations from  videos: Features, models, and analytics
In Human-Centered  Social Media Analytics, pages NN–NN
N0NN
N, N  [N] C
Direkoglu and N
E
O’Connor
Team activity recognition  in sports
In ECCV, pages NN–NN, N0NN
N, N  [N0] H
Fang, S
Gupta, F
N
Iandola, R
K
Srivastava, L
Deng,  P
Dollár, J
Gao, X
He, M
Mitchell, J
C
Platt, C
L
Zitnick, and G
Zweig
From captions to visual concepts and  back
In CVPR, pages NNNN–NNNN, N0NN
N  [NN] R
Fang, K
D
Tang, N
Snavely, and T
Chen
Towards  computational models of kinship verification
In ICIP, pages  NNNN–NNN0, N0N0
N  [NN] A
P
Fiske
The four elementary forms of sociality: framework for a unified theory of social relations
Psychological  review, NN(N):NNN, NNNN
N  [NN] A
C
Gallagher and T
Chen
Understanding images of  groups of people
In CVPR, pages NNN–NNN, N00N
N  [NN] G
Gkioxari, R
B
Girshick, and J
Malik
Contextual action  recognition with R*CNN
In ICCV, pages N0N0–N0NN, N0NN
 N, N  [NN] Y
Guo, H
Dibeklioglu, and L
van der Maaten
Graph-based  kinship recognition
In ICPR, pages NNNN–NNNN, N0NN
N  [NN] E
T
Hall
The silent language, volume N
Doubleday New  York, NNNN
N  [NN] N
Haslam
Categories of social relationship
Cognition,  NN(N):NN–N0, NNNN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, pages NN0–NNN, N0NN
N  [NN] C
Huang, C
C
Loy, and X
Tang
Unsupervised learning  of discriminative attributes and visual representations
In  CVPR, pages NNNN–NNNN, N0NN
N  [N0] H
Hung, D
B
Jayagopi, C
Yeo, G
Friedland, S
O
Ba,  J
Odobez, K
Ramchandran, N
Mirghafori, and D
GaticaPerez
Using audio and video features to classify the most  dominant person in a group meeting
In ACMMM, pages  NNN–NNN, N00N
N  [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L
Li, D
A
Shamma, M
S
Bernstein, and L
Fei-Fei
Visual genome: Connecting language  and vision using crowdsourced dense image annotations
 IJCV, N0NN
N  [NN] T
Lan, L
Sigal, and G
Mori
Social roles in hierarchical models for human activity recognition
In CVPR, pages  NNNN–NNNN, N0NN
N, N  [NN] T
Lan, Y
Wang, W
Yang, S
N
Robinovitch, and G
Mori
 Discriminative latent models for recognizing contextual  group activities
TPAMI, NN(N):NNNN–NNNN, N0NN
N, N  [NN] T
Lin, M
Maire, S
J
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft COCO: common objects in context
In ECCV, pages NN0–NNN, N0NN
N  [NN] C
Lu, R
Krishna, M
S
Bernstein, and L
Fei-Fei
Visual  relationship detection with language priors
In ECCV, pages  NNN–NNN, N0NN
N, N, N, N  [NN] Z
Qin and C
R
Shelton
Improving multi-target tracking  via social grouping
In CVPR, pages NNNN–NNNN, N0NN
N  [NN] V
Ramanathan, B
Yao, and L
Fei-Fei
Social role discovery  in human events
In CVPR, pages NNNN–NNNN, N0NN
N, N  [NN] S
Ren, K
He, R
B
Girshick, and J
Sun
Faster R-CNN:  towards real-time object detection with region proposal networks
In NIPS, pages NN–NN, N0NN
N, N, N, N  [NN] R
Rienks, D
Zhang, D
Gatica-Perez, and W
Post
Detection and application of influence rankings in small group  meetings
In ICMI, pages NNN–NNN, N00N
N  [N0] A
Robicquet, A
Sadeghian, A
Alahi, and S
Savarese
 Learning social etiquette: Human trajectory understanding  in crowded scenes
In ECCV, pages NNN–NNN, N0NN
N, N  [NN] H
Salamin, S
Favre, and A
Vinciarelli
Automatic role  recognition in multiparty recordings: Using social affiliation  networks for feature extraction
IEEE Trans
Multimedia,  NN(N):NNNN–NNN0, N00N
N  [NN] E
R
Smith and M
A
Zarate
Exemplar and prototype use  in social categorization
Social Cognition, N(N):NNN, NNN0
N  [NN] B
Thomee, D
A
Shamma, G
Friedland, B
Elizalde, K
Ni,  D
Poland, D
Borth, and L
Li
YFCCN00M: the new data in  multimedia research
Commun
ACM, NN(N):NN–NN, N0NN
N  [NN] A
Vinciarelli, M
Pantic, D
Heylen, C
Pelachaud, I
Poggi,  F
D’Errico, and M
Schröder
Bridging the gap between social animal and unsocial machine: A survey of social signal  processing
IEEE Trans
Affective Computing, N(N):NN–NN,  N0NN
N  [NN] G
Wang, A
C
Gallagher, J
Luo, and D
A
Forsyth
Seeing people in social context: Recognizing people and social  relationships
In ECCV, pages NNN–NNN, N0N0
N, N  [NN] J
Wu, Y
Yu, C
Huang, and K
Yu
Deep multiple instance learning for image classification and auto-annotation
 In CVPR, pages NNN0–NNNN, N0NN
N, N  [NN] S
Xia, M
Shao, J
Luo, and Y
Fu
Understanding kin relationships in a photo
IEEE Trans
Multimedia, NN(N):N0NN–  N0NN, N0NN
N  [NN] T
Xiao, Y
Xu, K
Yang, J
Zhang, Y
Peng, and Z
Zhang
 The application of two-level attention models in deep convolutional neural network for fine-grained image classification
 In CVPR, pages NNN–NN0, N0NN
N  NNNNN    [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
C
Courville, R
Salakhutdinov, R
S
Zemel, and Y
Bengio
Show, attend and tell:  Neural image caption generation with visual attention
In  ICML, pages N0NN–N0NN, N0NN
N  [N0] Z
Yang, X
He, J
Gao, L
Deng, and A
Smola
Stacked  attention networks for image question answering
In CVPR,  pages NN–NN, N0NN
N  [NN] Q
You, H
Jin, Z
Wang, C
Fang, and J
Luo
Image captioning with semantic attention
In CVPR, pages NNNN–NNNN,  N0NN
N  [NN] Z
Zhang, P
Luo, C
C
Loy, and X
Tang
Learning social  relation traits from face images
In ICCV, pages NNNN–NNNN,  N0NN
N  [NN] B
Zhou, A
Khosla, A
Lapedriza, A
Torralba, and A
Oliva
 Places: A N0 million image database for scene recognition
 TPAMI, N0NN
N  N0NNNNMUTAN: Multimodal Tucker Fusion for Visual Question Answering   MUTAN: Multimodal Tucker Fusion for Visual Question Answering  Hedi Ben-younes N,N ∗ Remi Cadene N∗ Matthieu Cord N Nicolas Thome N  N Sorbonne University, UPMC, CNRS, LIPN, N Heuritech, N CNAM  hedi.ben-younes@lipN.fr, remi.cadene@lipN.fr, matthieu.cord@lipN.fr, nicolas.thome@cnam.fr  Abstract  Bilinear models provide an appealing framework for  mixing and merging information in Visual Question Answering (VQA) tasks
They help to learn high level associations between question meaning and visual concepts in  the image, but they suffer from huge dimensionality issues
 We introduce MUTAN, a multimodal tensor-based  Tucker decomposition to efficiently parametrize bilinear interactions between visual and textual representations
Additionally to the Tucker framework, we design a low-rank  matrix-based decomposition to explicitly constrain the interaction rank
With MUTAN, we control the complexity of  the merging scheme while keeping nice interpretable fusion  relations
We show how the Tucker decomposition framework generalizes some of the latest VQA architectures, providing state-of-the-art results
 N
Introduction  Multimodal representation learning for text and image  has been extensively studied in recent years
Currently, the  most popular task is certainly Visual Question Answering  (VQA) [NN, N]
VQA is a complex multimodal task which  aims at answering a question about an image
A specific  benchmark has been first proposed [NN], and large scale  datasets have been recently collected [NN, N, NN], enabeling  the development of more powerful models
 To solve this problem, precise image and text models are  required and, most importantly, high level interactions between these two modalities have to be carefully encoded  into the model in order to provide the correct answer
This  projection from the unimodal spaces to a multimodal one  is supposed to extract and model the relevant correlations  between the two spaces
Besides, the model must have the  ability to understand the full scene, focus its attention on the  relevant visual regions and discard the useless information  regarding the question
 Bilinear models are powerful approaches for the fusion  problem in VQA because they encode full second-order  ∗Equal contribution  Figure N: The proposed MUTAN model uses a Tucker decomposition of the image/question correlation tensor, which  enables modeling rich and accurate multi-modal interactions
For the same input image, we show the result of the  MUTAN fusion process when integrated into an attention  mechanism [NN]: we can see that the regions with larger attention scores (in red) indicate a very fine understanding of  the image and question contents, enabling MUTAN to properly answer the question (see detailed maps in experiments  section)
 interactions
They currently hold state-of the-art performances [N, N]
The main issue with these bilinear models  is related to the number of parameters, which quickly becomes intractable with respect to the input and output dimensions
Therefore, current bilinear approaches must be  simplified or approximated by reducing the model complexity: in [N], the authors sacrifice trainability by using a handcrafted multi-modal projection, while a global tensor rank  constraint is applied in [N], reducing correlations to a simple element-wise product
 In this work, we introduce a new architecture called MUTAN (Figure N), which focuses on modeling fine and rich  interactions between image and textual modalities
Our approach is based on a Tucker decomposition [NN] of the correlation tensor, which is able to represent full bilinear interactions, while maintaining the size of the model tractable
 The resulting scheme allows us to explicitly control the  model complexity, and to choose an accurate and interpretable repartition of the learnable parameters
 NNNN    In the next section, we provide more details on related  VQA works and highlight our contributions
The MUTAN  fusion model, based on a Tucker decomposition, is presented in section N, and successful experiments are reported  in section N
 N
Related work  The main task in multimodal visual and textual analysis aims at learning an alignment between feature spaces  [NN, NN, NN]
Thus, the recent task of image captioning aims  at generating linguistic descriptions of images [NN, N0, NN]
 Instead of explicitly learning an alignment between two  spaces, the goal of VQA [N, NN] is to merge both modalities in order to decide which answer is correct
This problem requires modeling very precise correlations between the  image and the question representations
 Attention
Attention mechanisms [NN] have been a real  breakthrough in multimodal systems, and are fundamental  for VQA models to obtain the best possible results
[N0]  propose to stack multiple question-guided attention mechanisms, each one looking at different regions of the image
 [NN] and [NN] extract bounding boxes in the image and  score each one of them according to the textual features
In  [NN], word features are aggregated with an attention mechanism guided by the image regions and, equivalently, the  region visual features are aggregated into one global image  embedding
This co-attentional framework uses concatenations and sum pooling to merge all the components
On the  contrary, [N] and [N] developed their own fusion methods  that they use for global and attention-based strategies
 In this paper, we use the attentional modeling, proposed  in [N], as a tool that we integrate in our new fusion strategy  for both the global fusion and the attentional modeling
 Fusion strategies
Early works have modeled interactions  between multiple modalities with first order interactions
 The IMG+BOW model in [NN] is the first to use a concatenation to merge a global image representation with a  question embedding, obtained by summing all the learnt  word embeddings from the question
In [NN], (image, question, answer) triplets are scored in an attentional framework
 Each local feature is given a score corresponding to its similarity with textual features
These scores are used to weight  region multimodal embeddings, obtained from a concatenation between the region’s visual features and the textual embeddings
The hierarchical co-attention network [NN], after  extracting multiple textual and visual features, merges them  with concatenations and sums
 Second order models are a more powerful way to model  interactions between two embedding spaces
Bilinear interactions have shown great success in deep learning for  fine-grained classification [NN], and Multimodal language  modeling [N0]
In VQA, a simple element-wise product between the two vectors is performed in [N]
[N] also uses  an element-wise product in a more complex iterative global  merging scheme
In [NN], they use the element-wise product  aggregation in an attentional framework
To go deeper in  bilinear interactions, Multimodal Compact Bilinear pooling  (MCB) [N] uses an outer product q⊗v between visual v and textual q embeddings
The count-sketch projection [N] Ψ is used to project q⊗v on a lower dimensional space
Interest- ingly, nice count-sketch properties are capitalized to compute the projection without having to explicitly compute  the outer product
However, interaction parameters in MCB  are fixed by the count-sketch projection (randomly chosen  in {0;−N; N}), limiting its expressive power for modeling complex interactions between image and questions
In contrast, our approach is able to model rich second order interaction with learned parameters
 In the recent Multimodal Low-rank Bilinear (MLB)  pooling work [N], full bilinear interactions between image and question spaces are parametrized by a tensor
 Again, to limit the number of free parameters, this tensor is constrained to be of low rank r
The MLB strategy  reaches state-of-the-art performances on the well-known  VQA database [N]
Despite these impressive results, the  low rank tensor structure is equivalent to a projection of  both visual and question representations into a common  r-dimensional space, and to compute simple element-wise  product interactions in this space
MLB is thus essentially  designed to learn a powerful mono-modal embedding for  text and image modalities, but relies on a simple fusion  scheme in this space
 In this work, we introduce MUTAN, a multimodal fusion  scheme based on bilinear interactions between modalities
 To control the number of model parameters, MUTAN reduces the size of the mono-modal embeddings, while modeling their interaction as accurately as possible with a full  bilinear fusion scheme
Our submission therefore encompasses the following contributions:  – New fusion scheme for VQA relying on a Tucker  tensor-based decomposition, consisting in a factorization  into three matrices and a core tensor
We show that the  Tucker decomposition framework generalizes the latest bilinear models, i.e
MCB [N] and MLB [N], while having  more expressive power
 – Additional structured sparsity constraint the core tensor to further control the number of model parameters
 This acts as a regularizer during training and prevents overfitting, giving us more flexibility to adjust the input/output  projections
 – State-of-the-art results on the most widely used dataset  for Visual QA [N]
We also show that MUTAN outperforms  MCB [N] and MLB [N] in the same setting, and that performances can be further improved when combined with MLB,  validating the complementarity potential between the two  approaches
 NNNN    Figure N: MUTAN fusion scheme for global Visual QA
The prediction is modeled as a bilinear interaction between visual  and linguistic features, parametrized by the tensor T 
In MUTAN, we factorise the tensor T using a Tucker decomposition,  resulting in an architecture with three intra-modal matrices Wq , Wv and Wo, and a smaller tensor T c
The complexity of  T c is controlled via a structured sparsity constraint on the slice matrices of the tensor
 N
MUTAN Model  Our method deals with the problem of Visual Question  Answering (VQA)
In VQA, one is given a question q ∈ Q about an image v ∈ I, and the goal is to provide a mean- ingful answer
During training, we aim at learning a model  such that the predicted answer â matches the correct one a⋆
 More formally, denoting as Θ the whole set of parameters of the model, the predicted output â can be written as:  â = argmax a∈A  pΘ (a|v, q) (N)  The general architecture of the proposed approach is  shown in Figure N
As commonly done in VQA, images  v and questions q are firstly embedded into vectors and  the output is represented as a classification vector y 
In  this work, we use a fully convolutional neural network [N]  (ResNet-NNN) to describe the image content, and a GRU recurrent network [NN, N] for the question, yielding representations v ∈ Rdv for the image and q ∈ Rdq for the question
Vision and language representations v and q are then fused  using the operator T (explained below) to produce a vector  y, providing (through a softmax function) the final answer  in Eq
(N)
This global merging scheme is also embedded  into a visual attention-based mechanism [N] to provide our  final MUTAN architecture
 Fusion and Bilinear models The issue of merging visual  and linguistic information is crucial in VQA
Complex and  high-level interactions between textual meaning in the question and visual concepts in the image have to be extracted  to provide a meaningful answer
 Bilinear models [N, N] are recent powerful solutions to  the fusion problem, since they encode fully-parametrized  bilinear interactions between the vectors q and v:  y = (T ×N q)×N v (N)  with the full tensor T ∈ Rdq×dv×|A|, and the operator ×i designing the i-mode product between a tensor and a matrix  (here a vector)
 Despite their appealing modeling power, fullyparametrized bilinear interactions quickly become  intractable in VQA, because the size of the full tensor  is prohibitive using common dimensions for textual, visual  and output spaces
For example, with dv ≈ dq ≈ N0NN and |A| ≈ N000, the number of free parameters in the tensor T is ∼ N0N0
Such a huge number of free parameters is a problem both for learning and for GPU memory  consumptionN
 In MUTAN, we factorize the full tensor T using a  Tucker decomposition
We also propose to complete our  decomposition by structuring the second tensor T c (see  gray box in Fig
N) in order to keep flexibility over the input/output dimensions while keeping the number of parameters tractable
 N.N
Tucker decomposition  The Tucker decomposition [NN] of a N-way tensor  T ∈ Rdq×dv×|A| expresses T as a tensor product between factor matrices Wq,Wv and Wo, and a core tensor T c in  such a way that:  T = ((T c ×N Wq)×N Wv)×N Wo (N)  with Wq ∈ R dq×tq , Wv ∈ R  dv×tv and Wo ∈ R |A|×to ,  and T c ∈ R tq×tv×to 
Interestingly, Eq
(N) states that the  weights in T are functions of a restricted number of parameters ∀i ∈ [N, dq], j ∈ [N, dv], k ∈ [N, do]:  T [i, j, k] = ∑  l∈[N,tq ],m∈[N,tv ],n∈[N,to]  T c[l,m, n]Wq[i, l]Wv[j,m]Wo[k, n]  T is usually summarized as T = JT c;Wq,Wv,WoK
A comprehensive discussion on Tucker decomposition and  tensor analysis may be found in [NN]
 NA tensor with N billion floatNN scalars approximately needs NNGo to  be stored, while top-grade GPUs hold about NNGo each
 NNNN    N.N
Multimodal Tucker Fusion  As we parametrize the weights of the tensor T with its  Tucker decomposition of the Eq
(N), we can rewrite Eq
(N)  as follows:  y = ((  T c ×N (  q⊤Wq ))  ×N (  v⊤Wv ))  ×N Wo (N)  This is strictly equivalent to encode a full bilinear interaction of projections of q and v into a latent pair representation  z, and to use this latent code to predict the correct answer
 If we define q̃ = q⊤Wq ∈ R tq and ṽ = v⊤Wv ∈ R  tv , we  have:  z = (T c ×N q̃)×N ṽ ∈ R to (N)  z is projected into the prediction space y = z⊤Wo ∈ R |A|  and p = softmax(y)
In our experiments, we use non- linearities q̃ = tanh(q⊤Wq) and ṽ = tanh(v  ⊤ Wv) in  the fusion, as in [N], providing slightly better results
The  multimodal Tucker fusion is depicted in Figure N
 Interpretation Using the Tucker decomposition, we have  separated T into four components, each having a specific  role in the modeling
Matrices Wq and Wv project the  question and the image vectors into spaces of respective dimensions tq and tv 
These dimensions directly impact the  modeling complexity that will be allowed for each modality
The higher tq (resp
tv) will be, the more complex  the question (resp
image) modeling will be
Tensor T c is used to model interactions between q̃ and ṽ
It learns a  projection from all the correlations q̃[i]ṽ[j] to a vector z of size to
This dimension controls the complexity allowed for  the interactions between modalities
Finally, the matrix Wo scores this pair embedding z for each class in A
 N.N
Tensor sparsity  To further balance between expressivity and complexity  of the interactions modeling, we introduce a structured sparsity constraint based on the rank of the slice matrices in T c
 When we perform the to bilinear combinations between q̃  and ṽ of Eq
(N), each dimension k ∈ JN, toK in z can be written as:  z[k] = q̃⊤T c[:, :, k]ṽ (N)  The correlations between elements of q̃ and ṽ are weighted  by the parameters of T c[:, :, k]
We might benefit from the introduction of a structure in each of these slices
This structure can be expressed in terms of rank constraints on the  slices of T c
We impose the rank of each slice to be equal  to a constant R
Thus we express each slice T c[:, :, k] as a sum of R rank one matrices:  T c[:, :, k] = R ∑  r=N  mkr ⊗ n k⊤ r (N)  where ⊗ designates the outer product, mkr ∈ R tq and  nkr ∈ R tv 
Eq
(N) becomes:  z[k] = R ∑  r=N  (  q̃⊤mkr ) (  ṽ⊤nkr )  (N)  We can define R matrices Mr ∈ R tq × to (resp
 Nr ∈ R tv × to ) such as ∀k ∈ JN, doK,Mr[:, k] = m  k r  (resp
Nr[:, k] = n k r )
The structured sparsity on T c can  then be written as:  z =  R ∑  r=N  zr (N)  zr = (q̃ ⊤ Mr) ∗ (ṽ  ⊤ Nr) (N0)  where ∗ stands for element-wise multiplication
 Interpretation Adding this rank constraint on T c leads  to expressing the output vector z as a sum over R vectors  zr
To obtain each of these vectors, we project q̃ and ṽ  into a common space and merge them with an elementwise  product
Thus, we can interpret z as modeling an OR interaction over multiple AND gates (R in MUTAN) between  projections of q̃ and ṽ
z[k] can described in terms of logi- cal operators as:  zr[k] = (  q̃ similar to mkr )  AND (  ṽ similar to nkr )  (NN)  z[k] = zN[k] OR ..
OR zR[k] (NN)  This decomposition gives a very clear insight of how the  fusion is carried out in our MUTAN model
In our experiments, we will show how different r’s in JN, RK behave, de- pending on the type of question
We will exhibit some cases  where some r’s specialize over specific question types
 N.N
Model Unification and Discussion  In this subsection, we show how two state of the art  models, namely Multimodal Low-rank bilinear pooling  [N] (MLB) and Multimodal Compact Bilinear pooling [N]  (MCB), can be seen as special cases of our Multimodal  Tucker Fusion
Each of these models use a different type  of bilinear interaction between q and v, hence instantiating  a specific parametrization of the weight tensor T 
These  parameterizations actually consist in a Tucker decomposition with specific constraints on the elements T c,Wq,Wv and Wo
More importantly, when we cast MCB and MLB  into the framework of Tucker decompositions, we show that  the structural constraints imposed by these two models state  that some parameters are fixed, while they are free to be  learnt in our full Tucker fusion
This is illustrated in Figure N
We show in color the learnt parameters
 NNNN    (a) MCB (b) MLB (c) MUTAN  Figure N: Tensor design strategies
(a) MCB: Wq and Wv are fixed diagonal matrices, T c is a sparse fixed tensor, only the  output factor matrix Wo is learnt; (b) MLB: the N factor matrices are learnt but the core tensor is T c set to identity; (c) MUTAN: Wq , Wv , Wo and T c are learnt
The full bilinear interaction T c is structured with a low-rank (R) decomposition
 N.N.N Multimodal Compact Bilinear (MCB)  We can show that the Multimodal Compact Bilinear pooling [N] can be written as a bilinear model where the weight  tensor T mcb is decomposed into its Tucker decomposition,  with specific structures on the decompositions’ elements
 The intramodal projection matrices Wmcbq and W mcb v are  diagonal matrices where the non-zero coefficients take their  values in {−N; N}: Wmcbq = Diag (sq) and W mcb v =  Diag (sv), where sq ∈ R dq and sv ∈ R  dv are random vectors sampled at the instanciation of the model but kept fixed  afterwards
The core tensor T c is sparse and its values follow the rule: T mcbc [i, j, k] = N if h(i, j) = k (and 0 else), where h : JN, dqK × JN, dvK → JN, doK is randomly sampled at the beginning of training and no longer changed
 As was noticed in [N], all the learnt parameters in MCB  are located after the fusion
The combinations of dimensions from q and from v that are supposed to interact with  each other are randomly sampled beforehand (through h)
 To compensate for the fact of fixing the parameters sq, sv and h, they must set a very high to dimension (typically  NN,000)
This set of combinations is taken as a feature vector for classification
 N.N.N Multimodal Low-rank Bilinear (MLB)  The low-rank bilinear interaction corresponds to a canonical decomposition of the tensor T such as its rank is equal  to R
It is well-known that the low-rank decomposition of a  tensor is a special case of the Tucker decomposition, such  as T mlb = JIR;Wq,Wv,WoK where tq = tv = to = R
Two major constraints are imposed when reducing Tucker  decomposition to low-rank decomposition
First, the three  dimensions tq, tv and to are structurally set to be equal
The  dimension of the space in which a modality is projected (tq and tv) quantifies the model’s complexity
Our intuition is  that since the image and language spaces are different, they  may require to be modeled with different levels of complexity, hence different projection dimensions
The second constraint is on the core tensor, which is set to be the identity
 A dimension k of q̃mlb is only allowed to interact with the  same dimension of ṽmlb, which might be restrictive
We  will experimentally show the beneficial effect of removing  these constraints
 We would like to point out the differences between MLB  and the structured sparsity per slice presented in N.N
There  are two main differences between the two approaches
First,  our rank reduction is made on the core tensor of the Tucker  decomposition T c, while in MLB they constrain the rank  of the global tensor T 
This lets us keep different dimensionalities for the projected vectors q̃ and ṽ
The second  difference is we do not reduce the tensor on the third mode,  but only on the first two modes corresponding to the image  and question modalities
The implicit parameters in T c are  correlated inside a mode-N slice but independent between  the slices
 N
Experiments  VQA Dataset The VQA dataset [N] is built over images  of MSCOCO [NN], where each image was manually annotated with N questions
Each one of these questions is then  answered by N0 annotators, yielding a list of N0 groundtruth answers
The dataset is composed of NNN,NNN pairs  (image, question) for the training set, NNN,NNN for validation and NNN,N0N for testing
The ground truth answers are  given for the train and val splits, and one must submit their  predictions to an evaluation server to get the scores on teststd split
Note that the evaluation server makes it possible  to submit multiple models per day on test-dev, which is a  subsample of test-std
The whole submission on test-std  can only be done five times
We focus on the open-ended  task, where the ground truth answers are given in free natural language phrases
This dataset comes with its evaluation  metric, presented in [N]
When the model predicts an answer for a visual question, the VQA accuracy is given by:  min  (  N, # humans that provided that answer  N  )  (NN)  If the predicted answer appears at least N times in the ground  truth answers, the accuracy for this example is considered  NNNN    to be N
Intuitively, this metrics takes into account the consensus between annotators
 MUTAN Setup We first resize our images to be of size  (NNN, NNN)
We use ResNetNNN [N] as our visual feature ex- tractor, which produces feature maps of size NN×NN×N0NN
We keep the NN× NN tiling when attention models are used (section N.N)
Otherwise, the image is represented as the  average of NN × NN vectors at the output of the CNN (sec- tion N.N)
To represent questions, we use a GRU [N] initialized with the parameters of a pretrained Skip-thoughts  model [NN]
Each model is trained to predict the most common answer in the N0 annotated responses
|A| is fixed to the N000 most frequent answers as in [N], and we train our model using ADAM [N] (see details in supplementary material)
 N.N
Fusion Scheme Comparison  To point out the performance variation due to the fusion  modules, we first compare MUTAN to state-of-the-art bilinear models, under the same experimental framework
We  do not use attention models here
Several merging scheme  results are presented in Table N: Concat denotes a baseline  where v and q are merged by simply concatenating them
 For MCB [N] and MLB [N], we use the available code N N  to train models on the same visual and linguistic features
 We choose an output dimension of NN,000 for MCB and  N,N00 for MLB, as indicated in the respective articles
MUTAN noR designates the MUTAN model without the rank  sparsity constraint
We choose all the projection dimensions  to be equal to each other: tq = tv = to = NN0
These parameters are chosen considering the results on val split
 Finally, our MUTAN N designates the full Tucker decomposition with rank sparsity strategy
We choose all the projection dimensions to be equal to each other: tq = tv = to = NN0, and a rank R = N0
These parameters were chosen so that MUTAN and MUTAN noR have the same number of  parameters
As we can see in Table N, MUTAN obtains the  best results, validating our intuition of having a nice tradeoff  between the projection dimensions and a reasonable number of useful bilinear interaction parameters in the core tensor T c
Finally, a naive late fusion MUTAN(N)+MLB N further improves performances (about +Npt on test-dev)
It validates the complementarity between the two types of structure in the tensor decomposition
 N.N
State-of-the-art comparison  To compare the performance of the proposed approach  to state-of-the-art works, we associate the MUTAN fusion  Nhttps://github.com/jnhwkim/cbp Nhttps://github.com/jnhwkim/MulLowBiVQA Nhttps://github.com/cadene/vqa.pytorch NMUTAN(N) = MUTAN + MUTAN noR  test-dev val  Model Θ Y/N No
Other All All Concat N.N NN.NN NN.NN NN.NN NN.NN NN.NN  MCB NN N0.NN NN.NN NN.NN NN.N0 NN.NN  MLB N.N NN.0N NN.NN NN.NN N0.0N NN.NN  MUTAN noR N.N NN.NN NN.NN NN.NN NN.NN NN.NN  MUTAN N.N NN.NN NN.NN NN.NN N0.NN NN.NN  MUTAN(N)+MLB NN.N NN.NN NN.NN NN.NN NN.0N NN.NN  Table N: Comparison between different fusion under the  same setup on the test-dev split
Θ indicates the number of learnable parameters (in million)
 with recently introduced techniques for VQA, which are described below
 Attention mechanism We use the same kind of multiglimpse attention mechanisms as the ones presented in [N]  and [N]
We use MUTAN to score the region embeddings  according to the question vector, and compute a global visual vector as a sum pooling weighted by these scores
 Answer sampling Each (image,question) pair in the  VQA dataset is annotated with N0 ground truth answers,  corresponding to the different annotators
In those N0, we  keep only the answers occuring more than N times, and randomly choose the one we ask our model to predict
 Data augmentation We use Visual Genome [NN] as a data  augmentation to train our model, keeping only the example  whose answer is in our vocabulary
This triples the size of  our training set
 Ensembling MUTAN (N) consist in an ensemble of five  models trained on train+val splits
We use N attentional  MUTAN architectures with one trained with additional Visual Genome data
The N other models are instances of  MLB, which can be seen as a special case of MUTAN
Details about the ensembling will be provided in the supplementary material
 Results State-of-the-art comparison results are gathered  in Table N
Firstly, we can notice that bilinear models, i.e
 MCB [N] and MLB [N] have a strong edge over other methods with a less powerful fusion scheme
 MUTAN outperforms all the previous methods with a  large margin on test-dev and test-std
This validates the relevance of the proposed fusion scheme, which models precise  interactions between modalities
The good performances of  MUTAN (N) also confirms its complementarity with MLB,  already seen in section N.N without attention mecanism:  MLB learns informative mono-modal projections, wheras  MUTAN is explicitly devoted to accurately models bilinear  interactions
Finally, we can notice that the performance  improvement of MUTAN in this enhanced setup is conform  to the performance gap reported in section N.N, showing that  NNNN  https://github.com/jnhwkim/cbp https://github.com/jnhwkim/MulLowBiVQA https://github.com/cadene/vqa.pytorch   test-dev teststd  Y/N No
Other All All  SMem N-hop [NN] N0.NN NN.NN NN.NN NN.NN NN.NN  Ask Your Neur
[N0] NN.NN NN.NN NN.NN NN.NN NN.NN  SAN [N0] NN.N NN.N NN.N NN.N NN.N  D-NMN [N] NN.N NN.N NN.N NN.N NN.N  ACK [NN] NN.0N NN.NN NN.NN NN.NN NN.NN  MRN [N] NN.NN NN.NN NN.NN NN.NN NN.NN  HieCoAtt [NN] NN.N NN.N NN.N NN.N NN.N  MCB (N) [N] NN.N NN.N NN.N NN.N NN.N  MLB (N) [N] NN.NN NN.NN NN.NN NN.NN NN.NN  MUTAN (N) NN.NN NN.NN NN.NN NN.0N NN.NN  MUTAN (N) NN.NN NN.NN NN.NN NN.NN NN.NN  Table N: MUTAN performance comparison on the test-dev  and test-standard splits VQA dataset; (n) for an ensemble  of n models
 the benefit of the fusion scheme directly translates for the  whole VQA task
 Finally, we also evaluated an ensemble of N models  based on the MUTAN fusion scheme (without MLB), that  we denote as MUTAN (N)
This ensemble also outperforms  state-of-the-art results
We can point out that this improvement is reached with is an ensembling of N models, which is  smaller than the previous state-of-the-art MLB results containing an ensembling of N models
 N.N
Further analysis  Experimental setup In this section, we study the behavior of MUTAN under different conditions
Here, we examine under different aspects the fusion between q and v with  the Tucker decomposition of tensor T 
As we did previously, we don’t use the attention mechanism in this section
 We only consider a global visual vector, computed as the average of the NN × NN region vectors given by our CNN
We also don’t use the answer sampling, asking our model to  always predict the most frequent answer of the N0 ground  truth responses
All the models are trained on the VQA  train split, and the scores are reported on val
 Impact of a plain tensor The goal is to see how important  are all the parameters in the core tensor T c, which model  the correlations between projections of q and v
We train  multiple MUTAN noR, where we fix all projection dimensions to be equal tq = tv = to = t and t ranges from N0 to NN0
In Figure N, we compare these MUTAN noR  with a model trained with the same projection dimension,  but where T c is replaced by the identity tensor N
One can  NThis is strictly equivalent to MLB [N] without attention
However, we  are fully aware that it takes between N000 and N000 dimensions of projection to be around the operating point of MLB
With our experimental setup,  we just focus on the effect of adding parameters to our fusion scheme
 Figure N: The improvements given by MUTAN noR over  a model trained with the identity tensor as a fusion operator  between q̃ and ṽ
 see that MUTAN noR gives much better results than identity tensor, even for very small core tensor dimensions
This  shows that MUTAN noR is able to learn powerful correlations between modalitiesN
 Impact of rank sparsity We want to study the impact of  introducing the rank constraint in the core tensor T c
We fix  the input dimensions tq = NN0 and tv = NN0, and vary the output dimension to for multiple rank constraints R
As we  can see in Figure N, controlling the rank of slices in T c allows to better model the interactions between the unimodal  spaces
The different colored lines show the behavior of  MUTAN for different values of R
Comparing R = N0 (blue line) and R = N0 (green line), we see that a lower rank al- lows to reach higher values of to without overfitting
The  number of parameters in the fusion is lower, and the accuracy on the val split is higher
 Qualitative observations In MUTAN, the vector z that  encodes the (image,question) pair is expressed as a sum  over R vectors zr
We want to study the R different latent projections that have been learnt during training, and  assess whether the representations have captured different  semantic properties of inputs
We quantify the differences  between each of the R spaces using the VQA question types
 We first train a model on the train split, with R = N0, and measure its performance on the val set
Then, we set to 0  all of the zr vectors except one, and evaluate this ablated  system on the validation set
In Figure N, we compare the  full system to the R ablated systems for N different question types
The dotted line shows the accuracy of the full  system, while the different bars show the accuracy of the  ablated system for each R
Depending on the question type,  we observe N different behaviors of the ranks
When the  NNotice that for each t, MUTAN noR has tN parameters
For instance,  for t = NN0, MUTAN adds N0.NM parameters over identity
 NNNN    Figure N: Accuracy on VQA val in function of to
Each  colored dot shows the score of a MUTAN model trained on  train
The yellow labels indicate the number of parameters  in the fusion
 (a) ”Is there” (b) ”What room is”  (c) ”What is the man” (d) ”What sport is”  Figure N: Visualizing the performances of ablated systems  according to the R variables
Full system performance is  denoted in dotted line
 question type’s answer support is small, we observe that  each rank has learnt enough to reach almost the same accuracy as the global system
This is the case for questions  starting by ”Is there”, whose answer is almost always ”yes”  or ”no”
Other question types require information from all  the latent projections, as in the case of ”What is the man”
 This leads to cases where all projections perform equally  and significantly worst when taken individually than when  combined to get the full model
At last, we observe that  specific projections contribute more than others depending  on the question type
For example, latent variable NN performs well on ”what room is”, and is less informative to  answer questions starting by ”what sport is”
The opposite  behavior is observed for latent variable NN
 (a) Question: Where is the woman ? - Answer: on the elephant  (b) Question: Where is the smoke coming from ? - Answer: train  Figure N: The original image is shown on the left
The center and right images show heatmaps obtained when turning  off all the projections but one, for two different projections
 Each projection focuses on a specific concept needed to answer the question
 We run the same kind of analysis for the MUTAN fusion  in the attention mechanism
In Figure N, we show for two  images the different attentions that we obtain when turning  off all the projections but one
For the first image, we can  see that a projection focuses on the elephant, while another  focuses on the woman
Both these visual informations are  necessary to answer the question ”Where is the woman ?”
 The same behavior is observed for the second image, where  a projection focuses on the smoke while another gives high  attention to the train
 N
Conclusion  In this paper, we introduced our MUTAN strategy for  the VQA task
Our main contribution is a multimodal fusion between visual and textual information using a bilinear  framework
Our model combines a Tucker decomposition  with a low-rank matrix constraint
It is designed to control  the full bilinear interaction’s complexity
MUTAN factorizes the interaction tensor into interpretable elements, and  allows an easy control of the model’s expressiveness
We  also show how the Tucker decomposition framework generalizes the most competitive VQA architectures
MUTAN is  evaluated on the most recent VQA dataset, reaching stateof-the-art
 Acknowledgments This work has been partially supported within the Labex SMART supported by French state  funds managed by the ANR within the Investissements  dAvenir programme under reference ANR-NN-LABX-NN
 NNNN    References  [N] J
Andreas, M
Rohrbach, T
Darrell, and D
Klein
Learning to compose neural networks for question answering
In  NAACL HLT N0NN, San Diego California, USA, June NN-NN,  N0NN, pages NNNN–NNNN, N0NN
 [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra, C
L
 Zitnick, and D
Parikh
VQA: Visual Question Answering
 In ICCV, N0NN
 [N] M
Charikar, K
Chen, and M
Farach-Colton
Finding frequent items in data streams
In International Colloquium  on Automata, Languages and Programming, pages NNN–N0N,  N00N
 [N] K
Cho, B
van Merrienboer, D
Bahdanau, and Y
Bengio
 On the properties of neural machine translation: Encoderdecoder approaches
In Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation, EMNLP N0NN,  pages N0N–NNN, N0NN
 [N] A
Fukui, D
H
Park, D
Yang, A
Rohrbach, T
Darrell, and M
Rohrbach
Multimodal compact bilinear pooling for visual question answering and visual grounding
 arXiv:NN0N.0NNNN, N0NN
 [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
arXiv preprint arXiv:NNNN.0NNNN,  N0NN
 [N] J.-H
Kim, S.-W
Lee, D
Kwak, M.-O
Heo, J
Kim, J.-W
 Ha, and B.-T
Zhang
Multimodal Residual Learning for Visual QA
In NIPS, pages NNN–NNN, N0NN
 [N] J.-H
Kim, K.-W
On, J
Kim, J.-W
Ha, and B.-T
Zhang
 Hadamard Product for Low-rank Bilinear Pooling
In Nth International Conference on Learning Representations, N0NN
 [N] D
P
Kingma and J
Ba
Adam: A method for stochastic  optimization
CoRR, abs/NNNN.NNN0, N0NN
 [N0] R
Kiros, R
Salakhutdinov, and R
Zemel
Multimodal neural language models
In ICML, pages NNN–N0N, N0NN
 [NN] R
Kiros, Y
Zhu, R
Salakhutdinov, R
S
Zemel, A
Torralba,  R
Urtasun, and S
Fidler
Skip-thought vectors
In NIPS,  pages NNNN–NN0N, N0NN
 [NN] T
G
Kolda and B
W
Bader
Tensor decompositions and  applications
SIAM Rev., NN(N):NNN–N00, Aug
N00N
 [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, M
Bernstein, and L
Fei-Fei
Visual genome: Connecting language  and vision using crowdsourced dense image annotations
 N0NN
 [NN] R
Li and J
Jia
Visual question answering with question representation update (qru)
In NIPS, pages NNNN–NNNN
N0NN
 [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollr, and C
L
Zitnick
Microsoft coco: Common  objects in context
In ECCV, N0NN
 [NN] T.-Y
Lin, A
RoyChowdhury, and S
Maji
Bilinear cnn models for fine-grained visual recognition
In ICCV, N0NN
 [NN] J
Lu, J
Yang, D
Batra, and D
Parikh
Hierarchical  question-image co-attention for visual question answering
 In NIPS, pages NNN–NNN, N0NN
 [NN] L
Ma, Z
Lu, L
Shang, and H
Li
Multimodal convolutional  neural networks for matching image and sentence
In ICCV,  pages NNNN–NNNN, N0NN
 [NN] M
Malinowski and M
Fritz
Towards a visual turing challenge
In Learning Semantics (NIPS workshop), December  N0NN
 [N0] M
Malinowski, M
Rohrbach, and M
Fritz
Ask your neurons: A deep learning approach to visual question answering
 arXiv:NN0N.0NNNN, N0NN
 [NN] M
Ren, R
Kiros, and R
S
Zemel
Exploring models and  data for image question answering
In NIPS, pages NNNN–  NNNN, N0NN
 [NN] K
J
Shih, S
Singh, and D
Hoiem
Where to look: Focus  regions for visual question answering
In CVPR, N0NN
 [NN] R
Socher, A
Karpathy, Q
Le, C
Manning, and A
Ng
 Grounded compositional semantics for finding and describing images with sentences
Transactions of the Association  for Computational Linguistics, N:N0N–NNN, N0NN
 [NN] L
R
Tucker
Some mathematical notes on three-mode factor  analysis
Psychometrika, NN(N):NNN–NNN, NNNN
 [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In CVPR, pages  NNNN–NNNN, N0NN
 [NN] Q
Wu, P
Wang, C
Shen, A
Dick, and A
van den Hengel
 Ask me anything: free-form visual question answering based  on knowledge from external sources
In CVPR, N0NN
 [NN] H
Xu and K
Saenko
Ask, attend and answer: Exploring  question-guided spatial attention for visual question answering
In ECCV, pages NNN–NNN, N0NN
 [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
Courville, R
Salakhudinov, R
Zemel, and Y
Bengio
Show, attend and tell: Neural  image caption generation with visual attention
In ICML,  pages N0NN–N0NN, N0NN
 [NN] F
Yan and K
Mikolajczyk
Deep correlation for matching  images and text
In CVPR, June N0NN
 [N0] Z
Yang, X
He, J
Gao, L
Deng, and A
J
Smola
Stacked  attention networks for image question answering
In CVPR,  pages NN–NN, N0NN
 [NN] Y
Zhu, O
Groth, M
Bernstein, and L
Fei-Fei
VisualNW:  Grounded Question Answering in Images
In CVPR, N0NN
 NNN0Compositional Human Pose Regression   Compositional Human Pose Regression  Xiao SunN, Jiaxiang ShangN, Shuang LiangN∗, Yichen WeiN  NMicrosoft Research, N Tongji University  {xias, yichenw}@microsoft.com, jiaxiang.shang@gmail.com, shuangliang@tongji.edu.cn  Abstract  Regression based methods are not performing as well as  detection based methods for human pose estimation
A central problem is that the structural information in the pose  is not well exploited in the previous regression methods
 In this work, we propose a structure-aware regression approach
It adopts a reparameterized pose representation using bones instead of joints
It exploits the joint connection  structure to define a compositional loss function that encodes the long range interactions in the pose
It is simple,  effective, and general for both ND and ND pose estimation  in a unified setting
Comprehensive evaluation validates the  effectiveness of our approach
It significantly advances the  state-of-the-art on HumanN.NM [N0] and is competitive with  state-of-the-art results on MPII [N]
 N
Introduction  Human pose estimation has been extensively studied for  both ND [N0] and ND [N]
Recently, deep convolutional neutral networks (CNNs) have achieved significant progresses
 Existing approaches fall into two categories: detection  based and regression based
Detection based methods generate a likelihood heat map for each joint and locate the joint  as the point with the maximum value in the map
These heat  maps are usually noisy and multi-mode
The ambiguity is  reduced by exploiting the dependence between the joints in  various ways
A prevalent family of state-of-the-art methods [NN, N, NN, N, NN, NN] adopt a multi-stage architecture,  where the output of the previous stage is used as input to  enhance the learning of the next stage
These methods are  dominant for ND pose estimation [N]
However, they do not  easily generalize to ND pose estimation, because the ND heat  maps are too demanding for memory and computation
 Regression based methods directly map the input image  to the output joints
They directly target at the task and they  are general for both ND and ND pose estimation
Nevertheless, they are not performing as well as detection based  ∗Corresponding author
 methods
As an evidence, only one method [N] in the ND  pose benchmark [N] is regression based
While they are  widely used for ND pose estimation [NN, N0, NN, NN, NN, NN,  NN], the performance is not satisfactory
A central problem  is that they simply minimize the per-joint location errors independently but ignore the internal structures of the pose
 In other words, joint dependence is not well exploited
 In this work, we propose a structure-aware approach,  called compositional pose regression
It is based on two  ideas
First, it uses bones instead of joints as pose representation, because the bones are more primitive, more stable,  and easier to learn than joints
Second, it exploits the joint  connection structure to define a compositional loss function  that encodes long range interactions between the bones
 The approach is simple, effective and efficient
It only  re-parameterizes the pose representation, which is the network output, and enhances the loss function, which relates  the output to ground truth
It does not alter other algorithm  design choices and is compatible with such choices, such as  network architecture
It can be easily adapted into any existing regression approaches with little overhead for memory  and computation, in both training and inference
 The approach is general and can be used for both ND and  ND pose regression, indistinguishably
Moreover, ND and  ND data can be easily mixed simultaneously in the training
 For the first time, it is shown that such directly mixed learning is effective
This property makes our approach different  from all existing ones that target at either ND or ND task
 The effectiveness of our approach is validated by comprehensive evaluation with a few new metrics, rigorous ablation study and comparison with state-of-the-art on both  ND and ND benchmarks
Specifically, it advances the stateof-the-art on ND HumanN.NM dataset [N0] by a large margin  and achieves a record of NN.N mm average joint error, about NN% relatively better that state-of-the-art
On ND MPII dataset [N, N], it achieves NN.N% (PCKh 0.N)
It is the best- performing regression based method and on bar with the  state-of-the-art detection based methods
As a by-product,  our approach generates high quality ND poses for in the wild  images, indicating the potential of our approach for transfer  learning of ND pose estimation in the wild
 NNN0N    N
Related Work  Human pose estimation has been extensively studied for  years
A complete review is beyond the scope of this work
 We refer the readers to [NN, NN] for a detailed survey
 The previous works are reviewed from two perspectives  related to this work
First is how to exploit the joint dependency for ND and ND pose estimation
Second is how to  exploit “in the wild” ND data for ND pose estimation
 ND Pose Estimation Some methods use two separate  steps
They first perform ND joint prediction and then reconstruct the ND pose via optimization or search
There is  no end-to-end learning
Zhou et al
[NN] combines uncertainty maps of the ND joints location and a sparsity-driven  ND geometric prior to infer the ND joint location via an EM  algorithm
Chen et al
[N] searches a large ND pose library  and uses the estimated ND pose as query
Bogo et al
[N] fit a  recently published statistical body shape model [NN] to the  ND joints
Jahangiri et al
[NN] generates multiple hypotheses from ND joints using a novel generative model
 Some methods implicitly learn the pose structure from  data
Tekin et al
[NN] represents the ND pose with an  over-complete dictionary
A high-dimensional latent pose  representation is learned to account for joint dependencies
 Pavlakos et al
[NN] extends the Hourglass [NN] framework  from ND to ND
A coarse-to-fine approach is used to address the large dimensionality increase
Li et al
[NN] uses  an image-pose embedding sub-network for regularization
 Above works do not use prior knowledge in ND model
 Such prior knowledge is firstly used in [NN, NN] by embedding a kinematic model layer into deep neutral networks and  estimating model parameters instead of joints
The geometric structure is better preserved
Yet, the kinematic model  parameterization is highly nonlinear and its optimization in  deep networks is hard
Also, the methods are limited for a  fully specified kinematic model (fixed bone length, known  scale)
They do not generalize to ND pose estimation, where  a good ND kinematic model does not exist
 ND Pose Estimation Before the deep learning era, many  methods use graphical models to represent the structures in  the joints
Pictorial structure model [NN] is one of the earliest
There is a lot of extensions [NN, N0, NN, NN, NN, NN, N]
 Pose estimation is formulated as inference problems on the  graph
A common drawback is that the inference is usually  complex, slow, and hard to integrate with deep networks
 Recently, the graphical models have been integrated into  deep networks in various ways
Tompson et al
[NN] firstly  combine a convolutional network with a graphical model  for human pose estimation
Ouyang et al
[NN] joints feature extraction, part deformation handling, occlusion handling and classification all into deep learning framework
 Chu et al
[N0] introduce a geometrical transform kernels in  CNN framework that can pass informations between different joint heat maps
Both features and their relationships  are jointly learned in a end-to-end learning system
Yang et  al
[NN] combine deep CNNs with the expressive deformable  mixture of parts to regularize the output
 Another category of methods use a multi-stage architecture [NN, N, NN, N, NN, NN, NN]
The results of the previous  stage are used as inputs to enhance or regularize the learning of the next stage
Newell et al
[NN] introduce an Stacked  Hourglass architecture that better capture the various spatial  relationships associated with the body
Chu et al
[NN] further extend [NN] with a multi-context attention mechanism
 Bulat et al
[N] propose a detection-followed-by-regression  CNN cascade
Wei et al
[NN] design a sequential architecture composed of convolutional networks that directly  operate on belief maps from previous stages
Gkioxari et  al
[NN] predict joint heat maps sequentially and conditionally according to their difficulties
All such methods learn  the joint dependency from data, implicitly
 Different to all above ND and ND methods, our approach  explicitly exploits the joint connection structure in the pose
 It does not make further assumptions and does not involve  complex algorithm design
It only changes the pose representation and enhances the loss function
It is simple, effective, and can be combined with existing techniques
 Leveraging in the wild ND data for ND pose estimation  ND pose capturing is difficult
The largest ND human pose  dataset HumanN.NM [N0] is still limited in that the subjects,  the environment, and the poses have limited complexity and  variations
Models trained on such data do not generalize  well to other domains, such as in the wild images
 In contrast, in the wild images and ND pose annotation  are abundant
Many works leverage the ND data for ND pose  estimation
Most of them consist of two separate steps
 Some methods firstly generate the ND pose results (joint  locations or heat maps) and then use them as input for recovering the ND pose
The information in the ND images is  discarded in the second step
Bogo et al
[N] first use DeepCut [NN] to generate ND joint location, then fit with a ND  body shape model
Moreno et al
[N0] use CPM [NN] to  detect ND position of human joints, and then use these observations to infer ND pose via distance matrix regression
 Zhou et al
[NN] use Hourglass [NN] to generate ND joint  heat maps and then coupled with a geometric prior and Jahangiri et al
[NN] also use Hourglass to predict ND joint heat  maps and then infer multiple ND hypotheses from them
Wu  et al
[NN] propose ND interpreter network that sequentially  estimates ND keypoint heat maps and ND object structure
 Some methods firstly train the deep network model on  ND data and fine-tune the model on ND data
The information in ND data is partially retained by the pre-training, but  not fully exploited as the second fine-tuning step cannot use  ND data
Pavlakos et al
[NN] extends Hourglass [NN] model  for ND volumetric prediction
ND heat maps are used as intermediate supervision
Tome et al
[NN] extends CPM [NN]  NN0N    to ND by adding a probabilistic ND pose model to the CPM
 Mehta et al
[NN] and Park et al.[NN] train both ND and  ND pose networks simultaneously by sharing intermediate  CNN features
Yet, they use separate sub-networks and data  for ND and ND tasks
 Unlike the above methods, our approach treats the ND  and ND data in the same way and combine them in a unified training framework
The abundant information in the  ND data is fully exploited during training
As a result, our  method achieves strong performance on both ND and ND  benchmarks
As a by-product, it generates plausible and  convincing ND pose results for in the wild images
 Some methods use synthetic datasets which are generated from deforming a human template model with known  ground truth [N, N0]
These methods are complementary to  the others as they focus on data augmentation
 N
Compositional Pose Regression  Given an image of a person, the pose estimation problem  is to obtain the ND (or ND) position of all the K joints, J = {Jk|k = N, ...,K}
Typically, the coordinate unit is pixel for ND and millimeter (mm) for ND
 Without loss of generality, the joints are defined with respect to a constant origin point in the image coordinate system
For convenience, let the origin be J0
Specifically,  for ND pose estimation, it is the top-left point of the image
For ND pose estimation, it is the ground truth pelvis  joint [NN, NN]
 For regression learning, normalization is necessary to  compensate for the differences in magnitude of the variables
We use the standard normalization by subtraction of  mean and division of standard deviation
For a variable var,  it is normalized as  ˜var = N(var) = var −mean(vargt)  std(vargt) 
(N)  The inverse function for unnormalization is  var = N−N( ˜var) = ˜var ·std(vargt)+mean(vargt)
(N)  Note that both mean(∗) and std(∗) are constants and calculated from the ground truth training samples
The predicted output from the network is assumed already normalized
Both functions N(∗) and N−N(∗) are parameter free and embedded in the network
For notation simplicity, we  use ˜var for N(var)
 N.N
Direct Joint Regression: A Baseline  Most previous regression based methods [N, NN, NN, NN,  NN] directly minimize the squared difference of the predicted and ground truth joints
In experiments, we found  that the absolute difference (LN norm) performs better
In our direct joint regression baseline, the joint loss is  L(J ) =  K∑  k=N  ||J̃k − J̃ gt k ||N
(N)  Note that both the prediction and ground truth are normalized
 There is a clear drawback in loss Eq.(N)
The joints are  independently estimated
The joint correlation, or the internal structure in the pose, is not well exploited
For example,  certain geometric constraints (e.g., bone length is fixed) are  not satisfied
 Previous works only evaluate the joint location accuracy
 This is also limited because the internal structures in the  pose are not well evaluated
 N.N
A Bone Based Representation  We show that a simple reparameterization of the pose  is effective to address the above issues
As shown in Figure N(left), a pose is structured as a tree
Without loss of  generality, let pelvis be the the root joint JN and tree edges  be directed from the root to the end joints such as wrists and  ankles
Let the function parent(k) return the index of par- ent joint for kth joint
For notation consistency, let the parent of the root joint JN be the origin J0, i.e., parent(N) = 0
Now, for kth joint, we define its associated bone as a  directed vector pointing from it to its parent,  Bk = Jparent(k) − Jk
(N)  The joints J are defined in the global coordinate sys- tem
In contrast, bones B = {Bk|k = N, ...,K} are more primitive and defined in the local coordinate systems
Representing the pose using bones brings several benefits
 Stability Bones are more stable than joints and easier to  learn
Figure N (middle and right) shows that the standard  deviation of bones is much smaller than that of their corresponding joints, especially for parts (ankle, wrist, head) far  away from the root pelvis, in both ND (Human N.NM [N0])  and ND datasets (MPII [N])
 Geometric convenience Bones can encode the geometric structure and express the geometric constraints more  easily than joints
For example, constraint of “bone length  is fixed” involves one bone but two joints
Constraint  of “joint rotation angle is in limited range” involves two  bones but three joints
Such observations motivate us to  propose new evaluation metrics for geometric validity, as  elaborated in Section N
Experiments show that bone based  representation is better than joint based representation on  such metrics
 Application convenience Many pose-driven applications only need the local bones instead of the global joints
 For example, the local and relative “elbow to wrist” motion  can sufficiently represent a “pointing” gesture that would be  useful for certain human computer interaction scenarios
 NN0N    Bone  Joint  Head  Neck  Thorax  Pelvis  Shoulder.RShoulder.L  Elbow.R  Wrist.R  Elbow.L  Wrist.L Hip.L Hip.R  Knee.L Knee.R  Ankle.L Ankle.R  Figure N
Left: a human pose is represented as either joints J or bones B
Middle/Right: standard deviations of bones and joints for the ND HumanN.NM dataset [N0] and ND MPII dataset [N]
 N.N
Compositional Loss Function  Similar to the joint loss in Eq
(N), bones can be learnt by  minimizing the bone loss function  L(B) = K∑  k=N  ||B̃k − B̃ gt k ||N
(N)  However, there is a clear drawback in this loss
As  the bones are local and independently estimated in Eq
(N),  the errors in the individual bone predictions would propagate along the skeleton and accumulate into large errors for  joints at the far end
For example, in order to predict Jwrist,  we need to concatenate Bwrist, Belbow,...,Bpelvis
Errors  in these bones will accumulate and affect the accuracy of  Jwrist in a random manner
 To address the problem, long range objectives should be  considered in the loss
Long range errors should be balanced over the intermediate bones
In this way, bones are  jointly optimized
Specifically, let Ju and Jv be two arbitrary joints
Suppose the path from Ju to Jv along the skeleton tree has M joints
Let the function I(m) return the in- dex of the mth joint on the path, e.g., I(N) = u, I(M) = v
Note that M and I(∗) are constants but depend on u and v
Such dependence is omitted in the notations for clarity
 The long range, relative joint position ∆Ju,v is the sum- mation of the bones along the path, as  ∆Ju,v = M−N∑  m=N  JI(m+N) − JI(m)  =  M−N∑  m=N  sgn(parent(I(m)), I(m+ N)) ·N−N(B̃I(m))
 (N)  The function sgn(∗, ∗) indicates whether the bone BI(m) direction is along the path direction
It returns N when parent(I(m)) = I(m + N) and −N otherwise
Note that the network predicted bone B̃(∗) is normalized, as in Eq
(N)
It is unnormalized via Eq
(N) before summation
 Eq.(N) is differentiable with respect to the bones
It is  efficient and has no free parameters
It is implemented as a  special compositional layer in the neutral networks
 The ground truth relative position is  ∆Jgtu,v = J gt u − J  gt v 
(N)  Then, given a joint pair set P , the compositional loss function is defined as  L(B,P) = ∑  (u,v)∈P  ||∆̃Ju,v − ∆̃J gt  u,v||N
(N)  In this way, every joint pair (u, v) constrains the bones along the path from u to v
Each bone is constrained by  multiple paths given a large number of joint pairs
The errors are better balanced over the bones during learning
 The joint pair set P can be arbitrary
To validate the effectiveness of Eq.(N), we test four variants:  • Pjoint = {(u, 0)|u = N, ...,K}
It only considers the global joint locations
It is similar to joint loss Eq.(N)
 • Pbone = {(u, parent(u))|u = N, ...,K}
It only con- siders the bones
It degenerates to the bone loss Eq.(N)
 • Pboth = Pjoint ⋃ Pbone
It combines the above two  and verifies whether Eq.(N) is effective
 • Pall = {(u, v)|u < v, u, v = N, ...,K}
It contains all joint pairs
The pose structure is fully exploited
 N
Unified ND and ND Pose Regression  All the notations and equations in Section N are applicable for both ND and ND pose estimation in the same way
 The output pose dimension is either NK or NK
Training using mixed ND and ND data is straightforward
 All the variables, such as joint J, bone B, and relative joint  position ∆Ju,v , are decomposed into xy part and z part
The loss functions can be similarly decomposed
For example, for compositional loss function Eq.(N), we have  L(B,P) = Lxy(B,P) + Lz(B,P)
(N)  NN0N    The xy term Lxy(∗, ∗) is always valid for both ND and ND samples
The z term Lz(∗, ∗) is only computed for ND samples and set to 0 for ND samples
In the latter case, no gradient is back-propagated from Lz(∗, ∗)
 Note that the xy part and z part variables have different  dimensions
xy is in image coordinate frame and the unit  is in pixel
z is in camera coordinate frame and the unit is  metric (millimeters in our case)
This is no problem
During  training, they are appropriately normalized (Eq.(N), Eq.(N))  or unnormalized (Eq.(N), Eq.(N))
During inference, in order  to recover the ND metric coordinates, the xy part is backprojected into camera space using known camera intrinsic  parameters and a perspecitive projection model
 Training We use the state-of-the-art ResNet-N0 [NN]
 The model is pre-trained on ImageNet classification  dataset [NN]
The last fully connected layer is then modified  to output NK (or NK) coordinates and the model is fine- tuned on our target task and data
The training is the same  for all the tasks (ND, ND, mixed)
SGD is used for optimization
There are NN epoches
The base learning rate is 0.0N
It  drops to 0.00N after N0 epoches and 0.000N after another N0  epoches
Mini-batch size is NN
Two GPUs are used
Weight  decay is 0.000N
Momentum is 0.N
Batch-normalization  [NN] is used
Implementation is in Caffe [NN]
 Data Processing and Augmentation The input image  is normalized to NNN × NNN
Data augmentation includes random translation(±N% of the image size), scale(±NN%), rotation(±N0 degrees) and flip
For MPII dataset, the train- ing data are augmented by N0 times
For HumanN.NM  dataset, the training data are augmented by N times
For  mixed ND-ND task, each mini-batch consists of half ND and  half ND samples, randomly sampled and shuffled
 N
Experiments  Our approach is evaluated on ND and ND human pose  benchmarks
HumanN.NM [N0] is the largest ND human  pose benchmark
The dataset is captured in controlled environment
The image appearance of the subjects and the  background is simple
Accurate ND human joint locations  are obtained from motion capture devices
 MPII [N] is the benchmark dataset for ND human pose  estimation
It includes about NNk images and N0k annotated ND poses
NNk of them are for training and another Nk of the remaining are for testing
The images were collected  from YouTube videos covering daily human activities with  complex poses and image appearances
 N.N
Comprehensive Evaluation Metrics  For ND human pose estimation, previous works [N, NN,  N0, NN, NN, NN, NN, NN, N0, N, NN, NN, NN] use the mean per  joint position error (MPJPE)
We call this metric Joint Error
Some works [NN, N0, N, N, N0, NN] firstly align the predicted ND pose and ground truth ND pose with a rigid transCNN prediction loss function  Baseline joints J L(J ), Eq.(N) Ours (joint)  bones B  L(B,Pjoint), Eq.(N) Ours (bone) L(B,Pbone), Eq.(N) Ours (both) L(B,Pboth), Eq.(N) Ours (all) L(B,Pall), Eq.(N)  Table N
The baseline and four variants of our method
 formation using Procrustes Analysis [NN] and then compute  MPJPE
We call this metric PA Joint Error
 For ND human pose estimation in MPII [N], Percentage  of Correct Keypoints (PCK) metric is used for evaluation
 Above metrics only measures the accuracy of absolute  joint location
They do not fully reflect the accuracy of internal structures in the pose
We propose three additional  metrics for a comprehensive evaluation
 The first metric is the mean per bone position error, or  Bone Error
It is similar to Joint Error, but measures the  relative joint location accuracy
This metric is applicable  for both ND and ND pose
 The next two are only for ND pose as they measure the  validity of ND geometric constraints
Such metrics are important as violation of the constraints will cause physically  infeasible ND poses
Such errors are critical for certain applications such as ND motion capture
 The second metric is the bone length standard deviation,  or Bone Std
It measures the stability of bone length
For  each bone, the standard deviation of its length is computed  over all the testing samples of the same subject
 The third metric is the percentage of illegal joint angle,  or Illegal Angle
It measures whether the rotation angles at a  joint are physically feasible
We use the recent method and  code in [N] to evaluate the legality of each predicted joint
 Note that this metric is only for joints on the limbs and does  not apply to those on the torso
 N.N
Experiments on ND Pose of HumanN.NM  For HumanN.NM [N0], there are two widely used evaluation protocols with different training and testing data split
 Protocol N Six subjects (SN, SN, SN, SN, SN, SN) are used  in training
Evaluation is performed on every NNth frame of  Subject NN’s videos
It is used in [NN, N0, N, N0, NN]
PA  Joint Error is used for evaluation
 Protocol N Five subjects (SN, SN, SN, SN, SN) are used  for training
Evaluation is performed on every NNth frame  of two subjects (SN, SNN)
It is used in [NN, NN, NN, N, NN,  N0, NN, NN, NN, NN]
Joint Error is used for evaluation
 Ablation study
The direct joint regression baseline and  four variants of our method are compared
They are briefly  summarized in Table N
As explained in Section N, training can use additional ND data (from MPII), optionally
 NN0N    Training Data Metric Baseline Ours (joint) Ours(bone) Ours (both) Ours (all)  HumanN.NM  Joint Error N0N.N N0N.N↑N.N N0N.N↑N.N NN.N↓N.0 NN.N↓N.N PA Joint Error NN.0 NN.N↓0.N NN.0↓0.0 NN.N↓N.N NN.N↓N.N  Bone Error NN.N NN.N↓N.0 NN.N↓N.N NN.N↓N.N NN.N↓N.N Bone Std NN.N NN.N↓N.N NN.N↓N.N NN.N↓N.N NN.N↓N.N  Illegal Angle N.N% N.N%↓0.N N.N%↓0.N N.N%↓N.N N.N%↓N.N  HumanN.NM + MPII  Joint Error NN.N NN.N↓N.N NN.N↓0.N N0.N↓N.N NN.N↓N.N PA Joint Error NN.N N0.N↓0.N N0.N↓N.0 NN.N↓N.N NN.N↓N.N  Bone Error NN.N NN.N↓0.N NN.N↓N.N NN.N↓N.N NN.N↓N.N Bone Std NN.N NN.N↓0.N NN.N↓N.N NN.N↓N.N NN.0↓N.N  Table N
Results of all methods under all evaluation metrics (the lower the better), with or without using MPII data in training
Note  that the performance gain of all Ours methods relative to the Baseline method is shown in the subscript
The Illegal Angle metric for  “HumanN.NM+MPII” setting is not included because it is very good (< N%) for all methods
 Metric Joint Error PA Joint Error Bone Error Bone Std Illegal Angle  Method BL Ours (all) BL Ours (all) BL Ours (all) BL Ours (all) BL Ours (all)  Average N0N.N NN.N↓N.N NN.0 NN.N↓N.N NN.N NN.N↓N.N NN.N NN.N↓N.N N.N% N.N%↓N.N  Ankle(→ Knee) NN.N NN.N↓N.0 NN.N NN.N↓N.N NN.N NN.N↓N.N NN.N NN.0↓0.N - - Knee(→Hip) NN.N NN.N↓N.N NN.N NN.N↓N.N NN.N NN.N↓N.N NN.N NN.N↑N.N N.N% N.N%↓N.0 Hip(→Pelvis) NN.N NN.0↓N.N NN.N NN.N↓N.N NN.N NN.0↓N.N NN.N NN.N↓N.N 0.N% 0.N%↓0.0 Thorax(→Pelvis) NN.N N0.N↓N.N N0.N NN.N↓N.N NN.N N0.N↓N.N NN.0 NN.N↓N.N - - Neck(→Thorax) N0N.N NN.N↓N.N NN.N NN.N↓N.N NN.N NN.N↑0.N NN.N NN.N↓0.N N.N% N.N%↓0.N Head(→Neck) NNN.N N0N.N↓N.0 NN.N NN.N↓N.N NN.N NN.N↓N.N NN.N NN.N↓0.N - - Wrist(→Elbow) NNN.N NNN.0↓NN.N NN0.N NNN.N↓NN.0 N0N.N NN.0↓NN.N N0.N N0.N↓N0.0 - - Elbow(→Shoulder) NNN.N NNN.N↓NN.N NNN.N NN.N↓NN.N NN.N NN.N↓NN.N NN.N NN.N↓N.N N.N% N.N%↓N.N Shoulder(→Thorax) NNN.N N0N.N↓NN.N NN.N NN.N↓N.N NN.N NN.N↓N.N NN.N NN.N↓NN.N N.N% 0.N%↓N.N  Table N
Detailed results on all joints for Baseline (BL) and Ours (all) methods, only trained on HumanN.NM data (top half in Table N)
The  relative performance gain is shown in the subscript
Note that the left most column shows the names for both the joint (and the bone)
 We therefore tested two sets of training data: N) only HumanN.NM; N) HumanN.NM plus MPII
 Table N reports the results under Protocol N, which is  more commonly used
We observe several conclusions
 Using ND data is effective
All metrics are significantly  improved after using MPII data
For example, joint error  is reduced from N0N.N to NN.N
This improvement should originate from the better learnt feature from the abundant  ND data
See the contemporary work [NN] for more discussions
Note that adding ND data in this work is simple and  not considered as a main contribution
Rather, it is considered as a baseline to validate our regression approach
 Bone representation is superior than joint representation
This can be observed by comparing Baseline with  Ours (joint) and Ours (bone)
They are comparable because  they use roughly the same amount of supervision signals in  the training
The two variants of ours are better on nearly all  the metrics, especially the geometric constraint based ones
 Compositional loss is effective
When the loss function  becomes better (Ours (both) and Ours (all)), further improvement is observed
Specifically, when trained only on  HumanN.NM, Ours (all) improves the Baseline by N.N mm (relative N.N%) on joint error, N.N mm (relative N0%) on PA joint error, N.N mm (relative N0.N%) on bone error, N.N mm (relative NN.N%) on bone std, and N.N% (relative NN.N%) on  illegal angle
 Table N further reports the performance improvement  from Ours (all) to Baseline on all the joints (bones)
It  shows several conclusions
First, limb joints are harder  than torso joints and upper limbs are harder than lower  limbs
This is consistent as Figure N (middle)
It indicates that the variance is a good indicator of difficulty and  a per-joint analysis is helpful in both algorithm design and  evaluation
Second, our method significantly improves the  accuracy for all the joints, especially the challenging ones  like wrist, elbow and ankle
Figure N shows the results on a  testing video sequence with challenging arm motions
Our  result is much better and more stable
 Comparison with the state-of-the-art There are abundant previous works
They have different experiment settings and fall into three categories
They are compared to  our method in Table N, N, and N, respectively
See the arxiv  version of this paperN for more detailed comparision results
 The comparison is not completely fair due to the differences in the training data (when extra data are used),  the network architecture and implementation
Nevertheless,  two common conclusions validate that our approach is effective and sets the new state-of-the-art in all settings by  a large margin
First, our baseline is strong
It is simple  Nhttps://arxiv.org/abs/NN0N.00NNN  NN0N  https://arxiv.org/abs/NN0N.00NNN    Frame NN  Frame N0N  Frame NNN  Frame NN0  Frame  N00 Frame 0  Frame NNN  Ours  (all)  Bone  Error  Joint  Error   Baseline  Frame  (mm)   Frame NN  Test Result  Image and ND   Ground Truth  Figure N
(best viewed in color) Errors of wrist joint/bone of Baseline and Ours (all) methods on a video sequence from HumanN.NM SN,  action Pose
The average error over the sequence is shown in the legends
For this action, the arms have large motion and are challenging
 Our method has much smaller joint and bone error
Our result is more stable over the sequence
The ND predicted pose and ground truth  pose are visualized for a few frames
More video results are at https://www.youtube.com/watch?v=c-hgHqVKN0M
 Yasin [NN] Rogez [N0] Chen [N] Moreno [N0] Zhou [NN] Baseline Ours (all)  N0N.N NN.N NN.N NN.N NN.N NN.N NN.N Table N
Comparison with previous work on HumanN.NM
Protocol N is used
Evaluation metric is averaged PA Joint Error
Extra ND  training data is used in all the methods
Baseline and Ours (all) use MPII data in the training
Ours (all) is the best and also wins in all the  NN activity categories
 but already improves the state-of-the-art, by N.N mm (rela- tive N%) in Table N, N.N mm (relative N%) in Table N, and N.N mm (relative N.N%) in Table N
Therefore, it serves as a competitive reference
Second, our method significantly  improves the baseline, using exactly the same network and  training
Thus, the improvement comes from the new pose  representation and loss function
It improves the state-ofthe-art significantly, by N mm (relative NN.N%) in Table N, N.N mm (relative NN.N%) in Table N, and NN.N mm (relative NN.N%) in Table N
 Example ND pose results are illustrated in Figure N
 N.N
Experiments on ND Pose of MPII  All leading methods on MPII benchmark [N] have sophisticated network architectures
As discussed in Section N, the best-performing family of methods adopts a  multi-stage architecture [NN, N, NN, N, NN, NN, NN]
Our  method is novel in the pose representation and loss function
It is complementary to such sophisticated networks
 In this experiment, it is integrated into the Iterative Error  Feedback method (IEF) [N], which is the only regression  based method in the family
 We implement a two stage baseline IEF, using ResNet-N0  as the basic network in each stage
For reference, the original IEF [N] uses five stages with GoogLeNet for each stage
 We denote our implementation as IEF*
The two stages in  IEF* are then modified to use our bone based representation and compositional loss function
The training for all  the settings remains the same, as specified in Section N
 Ablation study Table N shows the results of IEF* and  our four variants
We observe the same conclusions as in  Table N
Both bone based representation and compositional  loss function are effective under all metrics
In addition,  both stages in IEF* benefit from our approach
 Comparison with the state-of-the-art Table N compares the per-joint accuracy of the original IEF [N], IEF*  and our method
It shows that: N) IEF* is better than [N],  therefore serving as a valid baseline; N) our method produces significant improvement over the baseline
 Table N reports the results of all leading methods on MPII  benchmark [N]
Ours (NN.N%) is the best regression based method
It is competitive to other detection based methods
 NN0N  https://www.youtube.com/watch?v=c-hgHqVKN0M   Chen [N] Tome [NN] Moreno [N0] Zhou [NN] Jahangiri [NN] Mehta [NN] Pavlakos [NN] Baseline Ours (all)  NNN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N Table N
Comparison with previous work on HumanN.NM
Protocol N is used
Evaluation metric is averaged Joint Error
Extra ND training  data is used in all the methods
Baseline and Ours (all) use MPII data in the training
Ours (all) is the best and also wins in all the NN  activity categories
 Figure N
(best viewed in color) Examples of ND pose estimation for HumanN.NM (top row) and MPII (middle and bottom rows), using  Ours (all) method in Table N, trained with both ND and ND data
Note that the MPII ND results are quite plausible and convincing
 Zhou [NN] Tekin [NN] Xingyi [NN] Baseline Ours (all)  NNN.0 NNN.0 N0N.N N0N.N NN.N  Table N
Comparison with previous work on HumanN.NM
Protocol  N is used
Evaluation metric is averaged Joint Error
No extra  training data is used
Ours (all) is the best and wins in NN out of  NN activity categories
 Stage Metric IEF* joint bone both all  0  Joint Error NN.N NN.N NN.N NN.N NN.N  Bone Error NN.N NN.N NN.N NN.N NN.N  PCKH 0.N NN.N% NN.N% NN.0% NN.N% NN.N%  N  Joint Error NN.0 NN.N NN.N NN.0 NN.N  Bone Error NN.N N0.N N0.N NN.N NN.N  PCKH 0.N NN.N% NN.N% NN.N% NN.N% NN.N%  Table N
Results of the baseline and four variants of our method  (see Table N), in the two-stage IEF*
 Method Head Sho
Elb
Wri
Hip Knee Ank
 IEF [N] NN.N NN.N NN.N NN.N NN.N NN.N NN.N  IEF* NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours (all) NN.N NN.N NN.0 NN.N NN.N NN.N NN.N  Table N
Per-joint evaluation result
PCKH 0.N metric is used
IEF*  and Ours (all) are the same as in Table N (stage N) and N
 Pishchulin [NN] Tompson [NN] Tompson [NN] Hu [NN]  NN.N NN.N NN.0 NN.N  Pishchulin [NN] Lifshitz [NN] Gkioxary [NN] Raf [NN]  NN.N NN.0 NN.N NN.N  Insafutdinov [NN] Wei [NN] Bulat [N] Newell [NN]  NN.N NN.N NN.N N0.0  Chu [NN] IEF [N] IEF* Ours (all)  NN.N NN.N NN.N NN.N  Table N
Comparison to state-of-the-art works on MPII
PCKH 0.N  metric is used
Our approach significantly improves the baseline  IEF and is competitive to other detection based methods
 N
Conclusion  We show that regression based approach is competitive  to the leading detection based approaches for ND pose estimation once pose structure is appropriately exploited
Our  approach is more potential for ND pose estimation, where  more complex structure constraints are critical
 Acknowledgement  This research work was supported by The National Science Foundation of China No
NNN0N0NN, and the Fundamental Research Funds for the Central Universities No
 NN00NNN0NN
 NN0N    References  [N] MPII Leader Board
http://human-pose.mpi-inf
 mpg.de
N, N  [N] I
Akhter and M
J
Black
Pose-conditioned joint angle limits for Nd human pose reconstruction
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N  [N] M
Andriluka, L
Pishchulin, P
Gehler, and B
Schiele
Nd  human pose estimation: New benchmark and state of the art  analysis
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN,  N0NN
N, N, N, N  [N] F
Bogo, A
Kanazawa, C
Lassner, P
Gehler, J
Romero,  and M
J
Black
Keep it smpl: Automatic estimation of Nd  human pose and shape from a single image
In European  Conference on Computer Vision, pages NNN–NNN
Springer,  N0NN
N, N  [N] A
Bulat and G
Tzimiropoulos
Human pose estimation via  convolutional part heatmap regression
In European Conference on Computer Vision, pages NNN–NNN
Springer, N0NN
N,  N, N, N  [N] J
Carreira, P
Agrawal, K
Fragkiadaki, and J
Malik
Human pose estimation with iterative error feedback
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNNN–NNNN, N0NN
N, N, N, N, N  [N] C.-H
Chen and D
Ramanan
Nd human pose estimation= Nd pose estimation+ matching
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
N, N, N, N  [N] W
Chen, H
Wang, Y
Li, H
Su, Z
Wang, C
Tu, D
Lischinski, D
Cohen-Or, and B
Chen
Synthesizing training images  for boosting human Nd pose estimation
In ND Vision (NDV),  N0NN Fourth International Conference on, pages NNN–NNN
 IEEE, N0NN
N  [N] X
Chen and A
L
Yuille
Articulated pose estimation by a  graphical model with image dependent pairwise relations
In  Advances in Neural Information Processing Systems, pages  NNNN–NNNN, N0NN
N  [N0] X
Chu, W
Ouyang, H
Li, and X
Wang
Structured feature  learning for pose estimation
In Proceedings of the IEEE  Conference on Computer Vision and Pattern Recognition,  pages NNNN–NNNN, N0NN
N  [NN] X
Chu, W
Yang, W
Ouyang, C
Ma, A
L
Yuille, and  X
Wang
Multi-context attention for human pose estimation
arXiv preprint arXiv:NN0N.0NNNN, N0NN
N, N, N, N  [NN] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
 In Computer Vision and Pattern Recognition, N00N
CVPR  N00N
IEEE Conference on, pages NNN–NNN
IEEE, N00N
N  [NN] P
F
Felzenszwalb and D
P
Huttenlocher
Pictorial structures for object recognition
International Journal of Computer Vision, NN(N):NN–NN, N00N
N  [NN] G
Gkioxari, A
Toshev, and N
Jaitly
Chained predictions  using convolutional neural networks
In European Conference on Computer Vision, pages NNN–NNN
Springer, N0NN
N,  N, N  [NN] J
C
Gower
Generalized procrustes analysis
Psychometrika, N0(N):NN–NN, NNNN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NN0–NNN, N0NN
N  [NN] P
Hu and D
Ramanan
Bottom-up and top-down reasoning  with hierarchical rectified gaussians
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NN00–NN0N, N0NN
N  [NN] E
Insafutdinov, L
Pishchulin, B
Andres, M
Andriluka, and  B
Schiele
Deepercut: A deeper, stronger, and faster multiperson pose estimation model
In European Conference on  Computer Vision, pages NN–N0
Springer, N0NN
N, N, N, N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [N0] C
Ionescu, D
Papava, V
Olaru, and C
Sminchisescu
 HumanN
Nm: Large scale datasets and predictive methods for Nd human sensing in natural environments
IEEE  transactions on pattern analysis and machine intelligence,  NN(N):NNNN–NNNN, N0NN
N, N, N, N, N  [NN] E
Jahangiri and A
L
Yuille
Generating multiple hypotheses for human Nd pose consistent with Nd joint detections
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N, N, N  [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional architecture for fast feature embedding
In Proceedings of the NNnd ACM international conference on Multimedia, pages NNN–NNN
ACM, N0NN
N  [NN] S
Johnson and M
Everingham
Learning effective human  pose estimation from inaccurate annotation
In Computer  Vision and Pattern Recognition (CVPR), N0NN IEEE Conference on, pages NNNN–NNNN
IEEE, N0NN
N  [NN] S
Li, W
Zhang, and A
B
Chan
Maximum-margin structured learning with deep networks for Nd human pose estimation
In Proceedings of the IEEE International Conference  on Computer Vision, pages NNNN–NNNN, N0NN
N, N  [NN] I
Lifshitz, E
Fetaya, and S
Ullman
Human pose estimation  using deep consensus voting
In European Conference on  Computer Vision, pages NNN–NN0
Springer, N0NN
N  [NN] C
Lindner, P
A
Bromiley, M
C
Ionita, and T
F
Cootes
 Robust and accurate shape model matching using random  forest regression-voting
IEEE transactions on pattern analysis and machine intelligence, NN(N):NNNN–NNNN, N0NN
N  [NN] M
Loper, N
Mahmood, J
Romero, G
Pons-Moll, and M
J
 Black
Smpl: A skinned multi-person linear model
ACM  Transactions on Graphics (TOG), NN(N):NNN, N0NN
N  [NN] D
Mehta, H
Rhodin, D
Casas, O
Sotnychenko, W
Xu,  and C
Theobalt
Monocular Nd human pose estimation in  the wild using improved cnn supervision
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
N, N, N, N  [NN] T
B
Moeslund and E
Granum
A survey of computer  vision-based human motion capture
Computer vision and  image understanding, NN(N):NNN–NNN, N00N
N  [N0] F
Moreno-Noguer
Nd human pose estimation from a single image via distance matrix regression
arXiv preprint  arXiv:NNNN.0N0N0, N0NN
N, N, N, N, N  NNN0  http://human-pose.mpi-inf.mpg.de http://human-pose.mpi-inf.mpg.de   [NN] A
Newell, K
Yang, and J
Deng
Stacked hourglass networks for human pose estimation
In European Conference  on Computer Vision, pages NNN–NNN
Springer, N0NN
N, N, N,  N  [NN] W
Ouyang and X
Wang
Joint deep learning for pedestrian  detection
In Proceedings of the IEEE International Conference on Computer Vision, pages N0NN–N0NN, N0NN
N  [NN] S
Park, J
Hwang, and N
Kwak
Nd human pose estimation using convolutional neural networks with Nd pose information
In Computer Vision–ECCV N0NN Workshops, pages  NNN–NNN
Springer, N0NN
N, N  [NN] G
Pavlakos, X
Zhou, K
G
Derpanis, and K
Daniilidis
 Coarse-to-fine volumetric prediction for single-image Nd human pose
arXiv preprint arXiv:NNNN.0NNNN, N0NN
N, N, N  [NN] M
Pedersoli, A
Vedaldi, J
Gonzalez, and X
Roca
A  coarse-to-fine approach for fast deformable object detection
 Pattern Recognition, NN(N):NNNN–NNNN, N0NN
N  [NN] L
Pishchulin, M
Andriluka, P
Gehler, and B
Schiele
Poselet conditioned pictorial structures
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNN–NNN, N0NN
N  [NN] L
Pishchulin, M
Andriluka, P
Gehler, and B
Schiele
 Strong appearance and expressive spatial models for human  pose estimation
In Proceedings of the IEEE International  Conference on Computer Vision, pages NNNN–NNNN, N0NN
N  [NN] L
Pishchulin, E
Insafutdinov, S
Tang, B
Andres, M
Andriluka, P
V
Gehler, and B
Schiele
Deepcut: Joint subset partition and labeling for multi person pose estimation
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N  [NN] U
Rafi, I
Kostrikov, J
Gall, and B
Leibe
An efficient  convolutional network for human pose estimation
In BMVC,  volume N, page N, N0NN
N  [N0] G
Rogez and C
Schmid
Mocap-guided data augmentation  for Nd pose estimation in the wild
In Advances in Neural  Information Processing Systems, pages NN0N–NNNN, N0NN
N,  N, N  [NN] N
Sarafianos, B
Boteanu, B
Ionescu, and I
A
Kakadiaris
 Nd human pose estimation: A review of the literature and  analysis of covariates
Computer Vision and Image Understanding, NNN:N–N0, N0NN
N  [NN] B
Tekin, I
Katircioglu, M
Salzmann, V
Lepetit, and P
Fua
 Structured prediction of Nd human pose with deep neural networks
arXiv preprint arXiv:NN0N.0NNN0, N0NN
N, N, N  [NN] B
Tekin, A
Rozantsev, V
Lepetit, and P
Fua
Direct prediction of Nd body poses from motion compensated sequences
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNN–N000, N0NN
N, N, N, N  [NN] D
Tome, C
Russell, and L
Agapito
Lifting from the deep:  Convolutional Nd pose estimation from a single image
arXiv  preprint arXiv:NN0N.00NNN, N0NN
N, N, N  [NN] J
Tompson, R
Goroshin, A
Jain, Y
LeCun, and C
Bregler
Efficient object localization using convolutional networks
In Proceedings of the IEEE Conference on Computer  Vision and Pattern Recognition, pages NNN–NNN, N0NN
N  [NN] J
J
Tompson, A
Jain, Y
LeCun, and C
Bregler
Joint training of a convolutional network and a graphical model for  human pose estimation
In Advances in neural information  processing systems, pages NNNN–NN0N, N0NN
N, N  [NN] S.-E
Wei, V
Ramakrishna, T
Kanade, and Y
Sheikh
Convolutional pose machines
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NNNN–NNNN, N0NN
N, N, N, N  [NN] J
Wu, T
Xue, J
J
Lim, Y
Tian, J
B
Tenenbaum, A
Torralba, and W
T
Freeman
Single image Nd interpreter network
In European Conference on Computer Vision, pages  NNN–NNN
Springer, N0NN
N  [NN] W
Yang, W
Ouyang, H
Li, and X
Wang
End-to-end learning of deformable mixture of parts and deep convolutional  neural networks for human pose estimation
In CVPR, N0NN
 N  [N0] Y
Yang and D
Ramanan
Articulated pose estimation with  flexible mixtures-of-parts
In Computer Vision and Pattern Recognition (CVPR), N0NN IEEE Conference on, pages  NNNN–NNNN
IEEE, N0NN
N  [NN] Y
Yang and D
Ramanan
Articulated human detection  with flexible mixtures of parts
IEEE Transactions on Pattern Analysis and Machine Intelligence, NN(NN):NNNN–NNN0,  N0NN
N  [NN] H
Yasin, U
Iqbal, B
Kruger, A
Weber, and J
Gall
A dualsource approach for Nd pose estimation from a single image
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N  [NN] X
Zhou, Q
Huang, X
Sun, X
Xue, and Y
Wei
Towards  Nd human pose estimation in the wild: a weakly-supervised  approach
In International Conference on Computer Vision,  N0NN
N  [NN] X
Zhou, X
Sun, W
Zhang, S
Liang, and Y
Wei
Deep  kinematic pose regression
In Computer Vision–ECCV N0NN  Workshops, pages NNN–N0N
Springer, N0NN
N, N, N, N, N  [NN] X
Zhou, Q
Wan, W
Zhang, X
Xue, and Y
Wei
 Model-based deep hand pose estimation
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [NN] X
Zhou, M
Zhu, S
Leonardos, K
G
Derpanis, and  K
Daniilidis
Sparseness meets deepness: Nd human pose  estimation from monocular video
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N, N  [NN] X
Zhou, M
Zhu, G
Pavlakos, S
Leonardos, K
G
Derpanis, and K
Daniilidis
Monocap: Monocular human motion  capture using a cnn coupled with a geometric prior
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
N, N, N, N  NNNNSketching With Style: Visual Search With Sketches and Aesthetic Context   Sketching with Style: Visual Search with Sketches and Aesthetic Context  John CollomosseN,N Tu BuiN Michael WilberN Chen FangN Hailin JinN  NCVSSP, University of Surrey NAdobe Research NCornell Tech  Abstract  We propose a novel measure of visual similarity for image retrieval that incorporates both structural and aesthetic (style) constraints
Our algorithm accepts a query  as sketched shape, and a set of one or more contextual images specifying the desired visual aesthetic
A triplet network is used to learn a feature embedding capable of measuring style similarity independent of structure, delivering  significant gains over previous networks for style discrimination
We incorporate this model within a hierarchical  triplet network to unify and learn a joint space from two  discriminatively trained streams for style and structure
We  demonstrate that this space enables, for the first time, styleconstrained sketch search over a diverse domain of digital  artwork comprising graphics, paintings and drawings
We  also briefly explore alternative query modalities
 N
Introduction  Determining user intent from a visual search query remains an open challenge, particularly within Sketch based  Image Retrieval (SBIR) where free-hand sketched queries  often present ambiguous or incomplete descriptions of the  desired visual content [N]
Traditionally, SBIR literature  considers images to be similar if they contain objects of  similar shape (e.g
fine-grained [NN, NN, NN]) or semantics  (e.g
category retrieval [N, NN, NN])
However, such definitions do not scale well to larger datasets, where a sketched  shape can closely match a diverse range of content
Additional visual modalities have been explored within the  sketch, such as color [NN, N], explicit labeling of sketched  parts [N, NN], and motion (for video) [N, N, NN] to better constrain the query and so improve the relevance of results
 This paper proposes a novel definition of visual similarity for SBIR, in which the sketched query is constrained  using one or more secondary (contextual) images to specify  the desired aesthetic of the results
For example, a sketch  of a dog accompanied by a contextual set of watercolor  paintings, or scary images, would yield watercolor paintings of dogs, or images of scary dogs, respectively
Importantly, we do not require the contextual images to contain  the desired subject matter (e.g
dogs)
Rather, we seek to  disentangle the notions of content (structure) and aesthetics  (style) enabling independent specification of both within the  query
Visual style remains a highly challenging and understudied area of Computer Vision
Our exploration of style as  a modality for visual search complements recent advances  e.g
in style transfer [N0] enabled by deep learning
 Constraining visual search to match a user’s intended  ‘look and feel’ is a promising novel direction for enhancing search relevance, particularly over aesthetically diverse  imagery
Our work leverages a recent large-scale dataset of  contemporary artwork covering a breadth of styles, media  and emotions (BAM [NN]) from which we learn a model of  aesthetic style
Concretely, we propose a hierarchical triplet  convolutional neural network (convnet) architecture to learn  a low-dimensional joint embedding for structure and style
 Each branch of this network unifies complementary information on scene structure and aesthetics derived from two  discriminatively trained convnet streams, which are themselves of triplet architecture
 Our technical contributions are three-fold
First, we propose a triplet convnet to learn an embedding for aesthetic  style, showing this novel model to outperform by a large  margin, previous attempts to use deep convnets for measuring visual style similarity
Second, we build upon our  model, incorporating a state of the art convnet for sketchphoto similarity [N] to develop a hierarchical triplet convnet  for learning a joint space for structural and style similarity  over a diverse domain of digital artwork (comprising not  only photos, but also paintings, ND renderings, hand-drawn  and vector-art drawings, in a variety of media)
Third, we  demonstrate and evaluate the performance of our model  NNNN0    within a novel SBIR framework that uniquely accepts a set  of contextual images alongside the sketched query shape,  enabling stylistic constraint of the visual search
Although  our study is scoped primarily to leverage sketches as means  for specifying structure in queries, we also experiment with  alternative modalities such as artwork and text
 N
Related Work  Visual style remains a sparsely researched topic that has  received greatest attention from the synthesis perspective  e.g
style transfer through learning visual analogies [NN] or  more recently deep representations [N0]
The Gram matrix  computed across layers of a pre-trained model (e.g
VGGNN on ImageNet) has been shown to abstract content from  style in diverse imagery, and has been exploited for texture  description [N0]
Deep convnets have also been shown effective at classifying artwork style [NN]
Although targetting  photographs rather than general artwork, aesthetic classification and rating of images has also been explored via attributes such as depth of field and exposure [NN, NN]
More  generally, attributes [NN, NN] (including relative attributes  [NN]) have been used to assist visual search
However attribute models often require explicit definition of a fixed set  of attribute categories, or pre-training for their detection
 This paper explores effective representations for leveraging style as a constraint in a visual search
Rather than  attempting to classify aesthetic attributes, we develop a definition of visual similarity that disentangles image structure  and style, enabling independent specification of each in a  visual search query
We leverage SBIR as a platform for  this study
Sketches primarily describe structure, and we  propose such descriptions be augmented with one or more  images exemplifying the desired visual style of results
Effective methods for augmenting shape description in SBIR  are becoming urgently needed to resolve query ambiguity  as SBIR matures to larger datasets
Our use of convnets  to unify structure and style complements recent deep approaches for shape matching in SBIR [NN, NN, N, N, N] which  have been shown to outperform shallow-learning models  [NN, N, NN, NN]
Contrastive loss networks have been used  to map sketches to photo edge-maps or rendered ND views  of ND models [NN, NN]
Triplet networks have also been  leveraged for both fine-grained [NN, NN] and category-level  SBIR [N]
Convnets were fused with a complex pipeline  incorporating object proposals, query expansion and reranking for retrieval [N]
All these methods address SBIR  via cross-domain learning; the gap between sketch and photos is bridged via regression explicitly seeking to discard  appearance properties
Our goal is not proposing another  shape matching technique
Rather, we incorporate a leading model [N] to explore aesthetic constraint of SBIR
 Relevance feedback (RF) is often used disambiguate user  intent particularly when multiple modalities exist in a query  [NN]
RF requires user interaction to iteratively refine search  results, learning a per-query re-weighting of the feature  space
Classically this is learned via linear classifier [N0],  or recently, online learning of shallow convnets using positive and negative images [NN]
Our work also explores  re-weighting using convnets, but pre-learns a corpus-wide  model for combining structural and style modalities
Unlike  RF, we do not iteratively request feedback from the user
 N
Methodology  We describe the proposed network architecture and training methodology for learning a feature embedding for visual search with aesthetic context
 N.N
Behance Artistic Media (BAM) Dataset  Our work leverages BAM; a dataset of ∼NN million con- temporary artworks from https://behance.net [NN]  annotated using a large-scale active learning pipeline [NN]
 The annotations in BAM include semantic categories (bicycle, bird, cars, cat, dog, flower, people, tree); seven labels  for a wide breadth of different artistic media (ND renderings, comics, pencil/graphite sketches, pen ink, oil paintings, vector art, watercolor); four emotion labels of images likely to induce certain emotions in the viewer (happy,  gloomy, peaceful, scary); and short textual captions for a  small subset of images
The dataset’s semi-automated label  annotations come in the form of likelihood scores which  may be thresholded at desired quality targets to control the  trade-off between dataset size and label precision
In our  work, we use images labeled at a precision of N0%
We  adopt BAM due to the high diversity of artistic content spanning drawings, paintings, graphics and vector art in contemporary and classic styles
In contrast, AVA [NN] comprises largely photographic content proposed for aesthetic  attribute mining
 N.N
Hierarchical Network Architecture  Triplet networks are commonly applied to learn lowdimensional feature embeddings from data distributions,  and have recently been applied to photo-based object instance retrieval [NN, NN]
Our proposed architecture is also  of triplet design, each branch unifying two discriminatively  trained network streams that independently learn a feature embedding for image structure and style respectively  (Fig
N)
Furthermore, the network stream for each modality has, itself, a triplet sub-structure
The architecture and  training of the style stream is given in Fig
N and described  in detail within Sec
N.N.N
The structure branch is not a  contribution of this paper, and reproduces a state of the art  model [N] for shape retrieval N.N.N
We describe how the  overall triplet model integrates these streams in Sec
N.N.N,  to learn a joint feature embedding within which we measure  visual similarity for search (Sec
N.N)
 NNNN  https://behance.net   Figure N
The triplet convnet for learning the style model  comprises three fully shared (siamese) branches formed from  GoogleNet with a bottleneck layer appended to poolN
Training  proceeds via classification loss, followed by triplet loss guided by  hard negatives exhibiting shared semantics and differing style
 N.N.N Style Network  We first describe the model forming the style stream  within each branch of our hierarchical triplet architecture
 The network comprises three branches, each augmenting  GoogLeNet [NN] through addition of a NNN-D inner-product  layer to serve as a bottleneck after poolN layer and prior  to drop-out
The bottleneck is later shown to be performance critical both for style classification and for search  (c.f
Sec
N) due to the increased sparsity of poolN features when training on diverse artwork rather than photos
 Fig
N illustrates the fully-shared (siamese) configuration of  the branches
 The model is trained from scratch, independent of the  wider network, using an NNk artwork training set (BehanceNet-TT, Sec
N.N) evenly partitioned into NN style categories  (S), each balanced across the N semantic categories (Z)
 Training proceeded initially via classification loss (soft-max  loss, N0 epochs) and then by refinement under triplet loss  (N0 epochs)
Triplets were formed using a randomly selected anchor image a = (s ∈ S, z ∈ Z), a randomly se- lected hard positive image p = (s, z′ ∈ Z \ z) and a hard negative image n = (s′ ∈ S \ s, z)
The network describes a function f(.) minimising:  L(a, p, n) = [  m+ |f(a)− f(p)|N − |f(a)− f(n)|N ]  +  (N)  where m = 0.N is a margin promoting convergence, and [x]+ indicates the non-negative part of x
Triplet refine- ment improves decorrelation between semantics and style  (Fig
N), discouraging learned correlations with objects (e.g
 trees → peaceful, skulls → scary scenes)
This refinement is later shown to yield significant performance gain (Sec
N.N)
 N.N.N Structure Network  The triplet model of Bui et al
[N], fine-tuned over BAM,  comprises the structure stream
The network incorpoFigure N
Hierarchical triplet convnet combining vectors from the  style (Fig
N) and structure [N] streams
Joint embedding of the two  modalities is learned (Sec
N.N.N) from the concatenated features  initially via classification loss, followed by hard negative mining  resulting in a NN-D descriptor for indexing
 rates an anchor branch accepting a sketch query and positive/negative branches that accept a photographic image as  input
The image branch closely resembles AlexNet [NN],  and the sketch branch Sketch-A-Net (a short-form AlexNet  optimal for sketch recognition [N0])
The network learns a  joint embedding from exemplar triplets comprising query  sketches, positive photos that match those sketches, and  negative photos that do not
We fix the output layer of the  network to NNN-D and inhibit sharing of network weights  across branches i.e
training yields separate functions for  embedding the sketch gs(.) and for the image gi(.) content
These functions are embedded within our larger network  (Fig
N)
 Training follows the four-stage training process outlined  in [N]
The process utilises the TU-Berlin sketch dataset  (for the anchor) augmented with social media sourced photographs (for the positive/negative pair)
The final step recommends fine-tuning the network using triplets sampled  from representative imagery; we use random artwork images sampled from BAM with NN0 TU-Berlin sketches having category overlap
 N.N.N Hierarchical (Multi-modal) network  The outputs of the structure and style models are normalised  and concatenated to form a NNN-D input vector, forming  the structure of each branch of the larger triplet network  (Fig
N)
Note that the anchor branch integrates gs(.) and the positive/negative branches gi(.)
The branches feed for- ward to two final inner product layers of NNN-D and NN-D  separated by ReLU activation, which learn projection h(.) over the two NNN-D subspaces for visual search
 A careful training protocol is required to ensure converNNNN    Figure N
PCA visualizations showing discrimination and decorrelation in the style (a) and semantic (b) spaces, before (left) and  after (right) triplet refinement
(c) Activation maximization for  ’Scary’ category using method of [NN] yields interesting qualitative insight into learned visual attributes for style; objects (teeth  and skull) disappear in the decorrelated space but strong color cues  remain
 gence in these final layers
Since triplet loss is a loose regularization (loss is computed on the relative distance between the anchor-positive and anchor-negative pairs) we  have found it more effective to initialise training with a  stricter regularization (soft-max loss)
We initially train the  network with an additional classification layer that recognises each of the N× NN semantic-style combinations in the dataset
Training proceeds minimising the hybrid loss:  L′(a, p, n) = ∑  i∈{a,p,n}  φsS(i) + φtL(a, p, n), (N)  where weight type (φs, φt) is set (N.0, 0.0) initially, re- laxed to (0.N, 0.N) after the initial N000 epochs
 Training proceeds by passing a sketched query and a homogeneous style set of artwork (randomly varying between  N-N0 images) to the structure gs(.) and style f(.) arms of the anchor branch, yielding a NNN-D query vector
The output of the style stream is averaged over all images in the  style set
The positive and negative vectors are formed via  gi(.) and f(.) each using a single artwork image selected at random
During initial training, triplets are first formed  randomly: the positive exemplar an artwork image selected  at random from those images sharing the same semantics  and style as the anchor, while the negative exemplar differs in either semantic or style or both
In the later training  phase (from epoch N000) , we narrow down the negative  list further by choosing the negative sample from top k returned images using the current network weights as a visual  search system (Sec
N.N)
Query sketches are subject to random affine perturbation (rotation, scale, translation) during  training
Although our network takes raster data as input  the TU-Berlin dataset is provided in vector form, enabling  the random omission of sketched strokes for further data  augmentation
Note that all training is carried out on the  Behance-Net-TT and sketch datasets described in Sec
N.N
 Figure N
Examples sampled from image sets used to specify style  within the SBIR queries evaluated in Sec
N.N
 N.N
Visual Search with Aesthetic Context  We index a NNNk dataset (Behance-VS, Sec
N.N) of artworks D = {dN, ..., dn} forward-passing each image di via through the hierarchical network to form a NN-D image descriptor d′i
 d′i = h( [  gi(di) f(di) ]  )
(N)  Descriptors are stored within a distributed (map-reduce)  index hosting multiple kd-trees across several machines
 Given query sketch q and set of contextual (style) images  C = {cN, ..., cm} the search descriptor q ′ is:  q′ = h( [  gs(q) ∑m  l=N ωmf(cl) ]  )
(N)  We perform a k nearest-neighbor look-up ranking results  by |q′ − d′i| distance for relevance
Typically the context set describes a common style and so ci has weight ωi =  N  m ,  however it is possible to blend styles (Sec
N.N)
 N
Experiments and Discussion  We evaluate the retrieval performance of our style model  (Sec
N.N), and the SBIR+style search that incorporates  it (Sec
N.N)
We experiment with style interpolation  (Sec
N.N.N) and alternative modalities for queries (Sec
N.N)
 N.N
Dataset Descriptions  Our experiments make use of BAM [NN], and the TUBerlin [N] dataset of free-hand sketches:  Network Training and Test (Behance-Net-TT, NN0k)  Taking the outer product of style and a subset of N semantic  attributes in BAM we form NN × N sets of artwork images
Attribute scores are thresholded using per-attribute thresholds distributed with the dataset for p = 0.N positive con- fidence
Images showing positive for both attribute pairs  only, are sorted by the sum of their scores
The top N.NNk  images are taken in each case yielding a balanced dataset of  NNNN    Figure N
Style retrieval performance over N000 random queries  (Precision @ K=NN)
Top: best performing style network (styletriplet loss, mAP NN.0%)
Bottom: performance comparison for  all baselines
 NN0k images
The dataset is partitioned N0:N:NN into training, validation and test sets, ensuring balanced semantics
 Visual Search Evaluation (Behance-VS, NNNk) Subsets  of BAM are created for each of the NN style categories, by  thresholding attribute scores at p = 0.N
Within each set, a random sample of N0k images is selected ensuring balanced semantics i.e
ensuring the same count of positive flags are  present for each semantic category
Images are excluded if  they appear in Behance-Net-TT
Images collated in multiple  sets are discarded
This yields a NNNk test corpus for visual  search
A small subset (N0 images ×NN styles) is held out to serve as style sets for the test queries (Fig
N)
 Sketch set (TU-Berlin, 0.Nk) NN0 sketches from the TUBerlin dataset [N] (N0×N object categories overlapping with BAM) are used to fine-tune the structural branches of the  network (Sec
N.N.N), and to train the hierarchical network
 A queryset of NN sketches are used to drive the visual search  evaluation (Sec
N.N)
 N.N
Evaluating Visual Search: Style only  We first evaluate the accuracy of the style model  (Sec
N.N.N) at retrieving artworks of similar style from the  test partition of Behance-Net-TT
A query set of N000 images are selected at random, with even semantic coverage,  from the test partition
Mean Average Precision (mAP) is  computed for the test set (Table N)
 Comparing the models at the initial classification (stylesoftmax) and triplet refinement (style-triplet) training  Method mAP (%)  style-triplet* NN.0  style-triplet-unbal* NN.0  style-softmax* NN.N  GoogLeNet* [NN] NN.N  Gram / Gatys et al
[N0] NN.N  Karayev et al
[NN] NN.N Table N
Style-only retrieval performance (mAP, N000 queries; *  indicates Behance-Net-TT trained methods
 stages, the latter shows a performance gain of N.NN% mAP
 A qualitative improvement in style discrimination and semantics decorrelation is also observable when visualizing  the spaces before and after triplet refinement (Fig
N)
 We include an early experiment performed using a semantically unbalanced version of Behance-Net-TT (styletriplet-unbal) which yielded poorer results suggesting overfitting to semantic bias (certain objects became expected for  certain styles) and further motivating decorrelation of style  and semantics
Precision @ K curves are presented for each  style for the leading (triplet) model, and for all models in  Fig
N
Emotion retrieval is evidently more challenging than  media type, and strong performance is shown in visually  distinctive media e.g
vector art
 Performance is base-lined against three techniques:  Karayev et al
[NN] who use pre-trained CaffeNet features  (DECAFN) with no fine-tuning; GoogLeNet [NN] features trained from scratch over the NN style categories (poolN); Gatys et al
[N0] where Gram matrices computed across  multiple convolutional layers (convN N−convN N) of a pre- trained VGG-NN network model, shown in [N0] to decorrelate style from content for image stylization
The retrieval  mAP (Table N) and precision @ K=NN curves (Fig
N) indicate significant performance gains in the learned models,  even versus contemporary texture descriptors (Gram [N0])
 A surprising result was the degree to which addition of the  bottleneck in GoogLeNet enhanced retrieval performance  (N.N% mAP)
The analysis supports incorporation of the  triplet refined model within the wider network
 N.N
Evaluating Visual Search: Sketch+Style  Mechanical Turk (MTurk) was used to evaluate retrieval  accuracy over the top NN ranked results returned from NNN  search queries (NN sketches ×NN style sets), since no anno- tation for structure and style relevance was available
Each  query required ∼ Nk annotations over the top NN results with three repetitions ensuring that no participant re-annotated  the same result
Retrieval was performed using six experimental configurations to search the Behance-VS dataset of  NNNk artwork images
The configurations explored were:  ss-triplet-NN the proposed combination of sketch+style features re-weighted and projected to NN-D by the learned layers of the multimodal network; ss-triplet-NNN similar network but with NNN-D output layer; ss-concat a degenerate  case in which the NNN-D concatenated sketch+style vector  NNNN    Figure N
Representative structure+style visual search results
Query comprising sketch and a sample of the N0 image style set (inset left)
 Method Wcol
Pen Graphite Comic Vector Oil ND Happy Scary Gloomy Peace mAP  ss-triplet-NN NN.N NN.N NN.N NN.N N0.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N  ss-triplet-NNN NN.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N  ss-concat NN.N N0.N NN.N NN.N NN.0 N0.N NN.N NN.0 NN.N NN.N NN.N NN.N  GoogLeNet et al
[NN] NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N  Gatys et al
[N0] NN.N NN.N NN.N NN.N N0.N NN.N N.N0 NN.N NN.N NN.N NN.N NN.N  Karayev et al
[NN] NN.N NN.N NN.N N0.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  Table N
Retrieval performance (mAP %)over NN sketches × NN styles; mAP computed to rank NN using MTurk annotation of search results
 is used directly for search i.e
no joint learning; three baseline style models of Sec
N.N substituted for our proposed  model in the full network
Participants were provided with  a tutorial containing examples of each style, and for each  query/result asked to indicate both whether the visual style  matched and if any of the main objects in the image matched  the sketched shape
 N.N.N Retrieval performance  We compute mAP (Table N) and precision @ K=NN (Fig.N)  of the proposed method to each of the five baselines, breaking down performance per style category
A result is considered relevant if both structure and style match
 The best performing output layer configuration for the  hierarchical network is NN-D, showing a performance lead  of N.N% over use of a NNN-D layer (offering also a compactness advantage for indexing)
Fig
N summarises the relative  performance of the techniques, and a t-test was run for all  configuration pairs
When considering performance across  all styles, all proposed methods (ss-*) outperform existing  baselines by a very significant margin (p<0.0N) and the use  of NN-D triplet learning in the hierarchical network outperforms direct concatenation of the structure and style vectors  (NNN-D) by significant margin (p<0.NN)
 All techniques that were explicitly trained on artwork  (BAM) outperformed those that were not, and the trend of  Table N is mirrored in the structure+style retrieval results  (Table N)
It is an interesting result that forcing learning to  explicitly de-correlate semantics and style during training of  the style stream of the network (triplet refinement, Sec N.N),  following by explicit learning of the projection of structure  and style together in the hierarchical network, yields a performance boost, so justifying our network design
Assessing structure relevance only (i.e
considering only shape responses from MTurk) confirmed little variation (NN.N±N.N) across methods for the NN × NN query set
Since this is simply evaluating prior work [N] on Behance-VS we do not  NNNN    Figure N
Relative performance of proposed method (ss-triplet-NN)  versus baselines; (*) indicates learned over BAM
 consider this further, beyond controlling for style
Considering style relevance only reveals performances of NN.N%,  NN.N% and NN.N% for ss-triplet-NN, ss-triplet-NNN, and ssconcat respectively against NN.0%, NN.N% and NN.N% for  baselines [NN], [N0], [NN] reflecting Table N
To benchmark  average query time over NNNk images we use a single ma- chine (Xeon EN-NNNN N.NGhz) and achieve ∼ N00ms using (ss-triplet-NN) features
 Figs
N,NN contain representative results from queries run  under the ss-triplet-NN model
Fig
NN highlights some incorrect top-NN results
A vector-artwork and a photo are  returned for a ND Graphics query, likely due to their flat  backgrounds typical in ND renderings (in the latter example,  content matches shape but not semantics)
The action sports  shot of the cyclist is arguably not ‘peaceful’ but the countryside setting is a strong prior
Two photos are misidentified  as oil paintings due to flaking paint visible on the wall, and  an incomplete oil painting is returned as the final example
 N.N.N Blending visual styles  We qualitatively explore interpolation of style in the query  set (Fig
N)
Three pairs of styles (watercolor-graphite,  oilpaint-penink, happy-scary) were combined by varying  the weights ωi (eq.N) on the averaging function using to  combine style vectors from images in the context set
For  example, a cat was queried using NN% watercolor and NN%  graphite as the style context
The results in that case showed  a sketched cat with grey watercolor wash in the background,  whilst a N0:N0 blend showed a half-finished watercolor  (many artists upload in-progress work to Behance)
We also  explore interpolation within a style, here two watercolor  context images are used for style: one coarse washes, the  other using harsh black outlines depicting fine detail with  a background wash
We observe a trend toward the more  heavily weighted style (media or emotion) suggesting linear interpolation in this space exert useful control
 N.N
Alternative query modalities  Significant advantages of visual search over text are that  no annotation of the search corpus is required, nor any adFigure N
Relative performance of proposed method (ss-triplet-NN)  versus baselines (precision @ K=NN)
 herence to a tag ontology
Here we contrast visual search  against the use of free-text keywords for specifying aesthetic context or structure
A small portion (∼ N%) of the BAM data is accompanied by free-text crowd annotation,  describing content and style in ∼ N − N0 words
Approx- imately N.Nk images within Behance-VS are accompanied by this annotation, enabling text-based experiments
 We substituted the style stream of our network for a popular skip-gram network (WordNVec [NN]) with NNN-D output  trained over the text corpus in BAM (holding out BehanceVS)
The resulting features were directly input to our framework in ss-concat configuration
Instead of querying with a  sketch + set of style images, we queried the network using  a sketch + free-text phrase: watercolor painting, oil painting, pen and ink, graphite, Nd graphics, vector art, comic,  scary, gloomy, happy, peaceful
In all, NN queries (a random subset of N sketched queries across NN styles) were run  on this reduced dataset
The results were evaluated through  MTurk under identical protocol to Sec
N.N
Interestingly,  the results of visually specifying the aesthetic context were  significantly higher (p<0.0N) than use of text (N0.N% versus NN.N% mAP)
We similarly explored substitution of keywords for sketches as the structural component of the query,  which indicated more strongly (but without significance) in  favour of the visual query (N0.N% versus NN.N% mAP)
We  did not pursue this further, as evaluation of [N] versus text  Figure N
Qualitative results combining pairs of style sets A and B  using different weights for a given sketched query (inset)
Examples of inter- and intra- style blending (view at N00% zoom)
 NNNN    Figure N0
Alternative query modalities
Top-N result using (top) Behance artwork as structure; (bottom) text instead of image set as style
 Figure NN
Matrix visualising top result returned for outer-product of all style visual contexts over seven representative query sketches
 search is outside the scope of our contribution
Rather, we  explored substitution of the sketch for an artwork image
 We swap the structure stream gs(.) for gi(.) in the anchor branch of the hierarchical network
As with sketch, we observed results to correctly disentangle content and style
For  example, a comic of a cat accompanied by watercolor imagery returned watercolor cats (Fig
N0)
 N
Conclusion  We demonstrated a novel visual search framework using  a structural query (sketch) augmented with a set of contextual images that specify the desired visual style of results
 The framework is underpinned by a learned representation  for measuring visual similarity for SBIR that disentangles  content and aesthetics
We first proposed a triplet convnet  comprising siamese branches adapted from GoogLeNet,  showing significant performance increase over the state of  art for style retrieval alone
We then combined this network  with a shape-matching network [N] to create a multi-modal  network of triplet design
We demonstrated that learning a  projection of structure and style features through this network further enhances retrieval accuracy, evaluating performance against baselines in a large-scale MTurk experiment
 Interesting directions for search could further explore  Figure NN
Failure cases from top-NN, discussed in Sec
N.N.N
 blending multiple styles in queries or even style extrapolation
Content recommendation systems, or relevance  feedback interfaces, could harness style-space to enhance  browsing and discovery
Exploring alternative modalties for  structure or for style specification (Sec
N.N) also appears an  interesting avenue, although we do not believe such extensions necessary to demonstrate the novelty and promise of  visual search constrained by aesthetic context
 Acknowledgements  We thank Aaron Hertzmann for feedback and discussions
An InnovateUK SMART grant supported the first author
 NNNN    References  [N] Motion-sketch based video retrieval using a trellis levenshtein distance
pages NNN–NNN, N0N0
N  [N] S
D
Bhattacharjee, J
Yuan, W
Hong, and X
Ruan
Query  adaptive instance search using object sketches
In Proc
ACM  Multimedia, pages NN0N–NNNN, N0NN
N  [N] T
Bui and J
Collomosse
Scalable sketch-based image retrieval using color gradient features
In Proc
ICCV Workshops, pages N–N, N0NN
N  [N] T
Bui, L
Ribeiro, M
Ponti, and J
Collomosse
Generalisation and sharing in triplet convnets for sketch based visual  search
CoRR Abs, arXiv:NNNN.0NN0N, N0NN
N, N, N, N, N, N  [N] T
Bui, L
Ribeiro, M
Ponti, and J
Collomosse
Compact descriptors for sketch-based image retrieval using a triplet loss  convolutional neural network
Computer Vision and Image  Understanding (CVIU), N0NN
N  [N] T
Chen, M.-M
Cheng, P
Tan, A
Shamir, and S.-M
Hu
 SketchNphoto: Internet image montage
In Proc
ACM SIGGRAPH Asia, pages NNN:N–NNN:N0, N00N
N  [N] J
P
Collomosse, G
McNeill, and Y
Qian
Storyboard  sketches for content based video retrieval
In Proc
ICCV,  pages NNN–NNN, N00N
N  [N] J
P
Collomosse, G
McNeill, and L
Watts
Free-hand sketch  grouping for video retrieval
In Proc
ICPR, N00N
N  [N] M
Eitz, J
Hays, and M
Alexa
How do humans sketch  objects? In Proc
ACM SIGGRAPH, volume NN, pages NN:N–  NN:N0, N0NN
N, N, N, N  [N0] L
A
Gatys, A
S
Ecker, and M
Bethge
Texture synthesis  using convolutional neural networks
In Proc
NIPS, pages  NNN–NN0, N0NN
N, N, N, N, N  [NN] A
Gordo, J
Almazán, J
Revaud, and D
Larlus
Deep image  retrieval: Learning global representations for image search
 In Proc
ECCV, pages NNN–NNN, N0NN
N  [NN] A
Hertzmann, C
E
Jacobs, N
Oliver, B
Curless, and D
H
 Salesin
Image analogies
In Proc
ACM SIGGRAPH, pages  NNN–NN0, N00N
N  [NN] R
Hu and J
Collomosse
A performance evaluation of  gradient field HOG descriptor for sketch based image retrieval
Computer Vision and Image Understanding (CVIU),  NNN(N):NN0–N0N, N0NN
N, N  [NN] S
James and J
Collomosse
Interactive video asset retrieval  using sketched queries
In Proc
CVMP, N0NN
N, N  [NN] S
James, M
Fonseca, and J
Collomosse
Reenact: Sketch  based choreographic design from archival dance footage
In  Proc
ICMR, N0NN
N  [NN] S
Karayev, M
Trentacoste, H
Han, A
Agarwala, T
Darrell,  and a
H
W
Aaron Hertzmann
Recognising image style
In  Proc
BMVC, N0NN
N, N, N, N  [NN] A
Kovashka and K
Grauman
Attribute adaption for personalised image search
In Proc
ICCV, N0NN
N  [NN] A
Kovashka and K
Grauman
Attribute pivots for guiding  relevance feedback in image search
In Proc
ICCV, N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
Hinton
Imagenet classification with deep convnets
In Proc
NIPS, N0NN
N  [N0] T.-Y
Lin and S
Majo
Visualizing and understanding deep  texture representations
In Proc
CVPR, N0NN
N  [NN] L
Marchesotti, F
Perronnin, and F
Meylan
Discovering  beautiful attributes for aesthetic image analysis
Intl
Journal  Computer Vision (IJCV), NNN(N):NNN–NNN, N0NN
N  [NN] T
Mikolov, K
Chen, G
Corrado, and J
Dean
Efficient  estimation of word representations in vector space
arXiv  preprint, arXiv:NN0N.NNNN, N0NN
N  [NN] N
Murray, L
Marchesotti, and F
Perronnin
AVA: A largescale database for aesthetic visual analysis
In Proc
CVPR,  N0NN
N  [NN] A
Nguyen, A
Dosovitskiy, J
Yosinski, T
Brow, and  J
Clune
Synthesising the preferred inputs for neurons in  neural networks via deep generator networks
In Proc
NIPS
 IEEE, N0NN
N  [NN] Y
Qi, Y.-Z
Song, H
Zhang, and J
Liu
Sketch-based image  retrieval via siamese convolutional neural network
In Proc
 ICIP, pages NNN0–NNNN
IEEE, N0NN
N, N  [NN] F
Radenović, G
Tolias, and O
Chum
CNN image retrieval  learns from BoW: Unsupervised fine-tuning with hard examples
In Proc
ECCV, pages N–N0, N0NN
N  [NN] M
Rastegari, A
Diba, D
Parikh, and A
Farhadi
Multiattribute queries: To merge or not to merge? In Proc
CVPR,  N0NN
N  [NN] J
M
Saavedra and B
Bustos
Sketch-based image retrieval using keyshapes
Multimedia Tools and Applications,  NN(N):N0NN–N0NN, N0NN
N  [NN] P
Sangkloy, N
Burnell, C
Ham, and J
Hays
The sketchy  database: Learning to retrieve badly drawn bunnies
In Proc
 ACM SIGGRAPH, N0NN
N, N  [N0] L
Setia, J
Ick, H
Burkhardt, and A
I
Features
Svm-based  relevance feedback in image retrieval using invariant feature  histograms
In Proc
ACM Multimedia, N00N
N  [NN] X
Sun, C
Wang, A
Sud, C
Xu, and L
Zhang
Magicbrush:  Image search by color sketch
In Proc
ACM Multimedia
 IEEE, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, V
Vanhoucke, and A
Rabinovich
Going  deeper with convolutions
In Proc
CVPR, N0NN
N, N, N,  N  [NN] C
Wang, Z
Li, and L
Zhang
Mindfinder: image search  by interactive sketching and tagging
In Proc
WWW, pages  NN0N–NNNN, N0N0
N  [NN] F
Wang, L
Kang, and Y
Li
Sketch-based Nd shape retrieval  using convolutional neural networks
In Proc
CVPR, pages  NNNN–NNNN, N0NN
N  [NN] J
Wang, Y
Song, T
Leung, C
Rosenberg, J
Wang,  J
Philbin, B
Chen, and Y
Wu
Learning fine-grained image  similarity with deep ranking
In Proc
CVPR, pages NNNN–  NNNN, N0NN
N  [NN] X
Wang, K
M
Kitani, and M
Hebert
Contextual visual  similarity
arXiv preprint, arXiv:NNNN.0NNNN, N0NN
N  [NN] M
J
Wilber, C
Fang, H
Jin, A
Hertzmann, J
Collomosse,  and S
Belongie
BAM! the behance artistic media dataset  for recognition beyond photography
In Proc
ICCV, N0NN
 N, N, N  [NN] F
Yu, Y
Zhang, S
Song, A
Seff, and J
Xiao
Lsun: Construction of a large-scale image dataset using deep learning  with humans in the loop, N0NN
arXiv:NN0N.0NNNN
N  [NN] Q
Yu, F
Liu, Y.-Z
Song, T
Xiang, T
M
Hospedales, and  C.-C
Loy
Sketch me that shoe
In Proc
CVPR, pages NNN–  N0N, N0NN
N, N  [N0] Q
Yu, Y
Yang, Y.-Z
Song, T
Xiang, and T
Hospedales
 Sketch-a-net that beats humans
In Proc
BMVC, N0NN
N  NNNNA Simple yet Effective Baseline for ND Human Pose Estimation   A simple yet effective baseline for Nd human pose estimation  Julieta MartinezN, Rayat HossainN, Javier RomeroN, and James J
LittleN  NUniversity of British Columbia, Vancouver, Canada NBody Labs Inc., New York, NY  julm@cs.ubc.ca, rayatNNN@cs.ubc.ca, javier.romero@bodylabs.com, little@cs.ubc.ca  Abstract  Following the success of deep convolutional networks,  state-of-the-art methods for Nd human pose estimation have  focused on deep end-to-end systems that predict Nd joint  locations given raw image pixels
Despite their excellent  performance, it is often not easy to understand whether  their remaining error stems from a limited Nd pose (visual)  understanding, or from a failure to map Nd poses into Ndimensional positions
 With the goal of understanding these sources of error,  we set out to build a system that given Nd joint locations  predicts Nd positions
Much to our surprise, we have found  that, with current technology, “lifting” ground truth Nd joint  locations to Nd space is a task that can be solved with a  remarkably low error rate: a relatively simple deep feedforward network outperforms the best reported result by  about N0% on HumanN.NM, the largest publicly available  Nd pose estimation benchmark
Furthermore, training our  system on the output of an off-the-shelf state-of-the-art Nd  detector (i.e., using images as input) yields state of the art  results – this includes an array of systems that have been  trained end-to-end specifically for this task
Our results indicate that a large portion of the error of modern deep Nd  pose estimation systems stems from their visual analysis,  and suggests directions to further advance the state of the  art in Nd human pose estimation
 N
Introduction  The vast majority of existing depictions of humans are  two dimensional, e.g
video footage, images or paintings
 These representations have traditionally played an important role in conveying facts, ideas and feelings to other people, and this way of transmitting information has only been  possible thanks to the ability of humans to understand complex spatial arrangements in the presence of depth ambiguities
For a large number of applications, including virtual and augmented reality, apparel size estimation or even  autonomous driving, giving this spatial reasoning power to  machines is crucial
In this paper, we will focus on a particular instance of this spatial reasoning problem: Nd human  pose estimation from a single image
 More formally, given an image – a N-dimensional representation – of a human being, Nd pose estimation is the  task of producing a N-dimensional figure that matches the  spatial position of the depicted person
In order to go from  an image to a Nd pose, an algorithm has to be invariant to  a number of factors, including background scenes, lighting,  clothing shape and texture, skin color and image imperfections, among others
Early methods achieved this invariance  through features such as silhouettes [N], shape context [NN],  SIFT descriptors [N] or edge direction histograms [N0]
 While data-hungry deep learning systems currently outperform approaches based on human-engineered features on  tasks such as Nd pose estimation (which also require these  invariances), the lack of Nd ground truth posture data for images in the wild makes the task of inferring Nd poses directly  from colour images challenging
 Recently, some systems have explored the possibility of  directly inferring Nd poses from images with end-to-end  deep architectures [NN, NN], and other systems argue that Nd  reasoning from colour images can be achieved by training  on synthetic data [NN, NN]
In this paper, we explore the  power of decoupling Nd pose estimation into the well studied problems of Nd pose estimation [N0, N0], and Nd pose  estimation from Nd joint detections, focusing on the latter
 Separating pose estimation into these two problems gives  us the possibility of exploiting existing Nd pose estimation  systems, which already provide invariance to the previously  mentioned factors
Moreover, we can train data-hungry algorithms for the Nd-to-Nd problem with large amounts of  Nd mocap data captured in controlled environments, while  working with low-dimensional representations that scale  well with large amounts of data
 Our main contribution to this problem is the design and  analysis of a neural network that performs slightly better  than state-of-the-art systems (increasing its margin when  NNNN0    the detections are fine-tuned, or ground truth) and is fast (a  forward pass takes around Nms on a batch of size NN, allow- ing us to process as many as N00 fps in batch mode), while being easy to understand and reproduce
The main reason  for this leap in accuracy and performance is a set of simple  ideas, such as estimating Nd joints in the camera coordinate  frame, adding residual connections and using batch normalization
These ideas could be rapidly tested along with other  unsuccessful ones (e.g
estimating joint angles) due to the  simplicity of the network
 The experiments show that inferring Nd joints from  groundtruth Nd projections can be solved with a surprisingly  low error rate – N0% lower than state of the art – on the  largest existing Nd pose dataset
Furthermore, training our  system on noisy outputs from a recent Nd keypoint detector yields results that slightly outperform the state-of-the-art  on Nd human pose estimation, which comes from systems  trained end-to-end from raw pixels
 Our work considerably improves upon the previous best  Nd-to-Nd pose estimation result using noise-free Nd detections in HumanN.NM, while also using a simpler architecture
This shows that lifting Nd poses is, although far  from solved, an easier task than previously thought
Since  our work also achieves state-of-the-art results starting from  the output of an off-the-shelf Nd detector, it also suggests  that current systems could be further improved by focusing on the visual parsing of human bodies in Nd images
 Moreover, we provide and release a high-performance, yet  lightweight and easy-to-reproduce baseline that sets a new  bar for future work in this task
Our code is publicly available at https://github.com/una-dinosauria/  Nd-pose-baseline
 N
Previous work  Depth from images The perception of depth from purely  Nd stimuli is a classic problem that has captivated the attention of scientists and artists at least since the Renaissance,  when Brunelleschi used the mathematical concept of perspective to convey a sense of space in his paintings of Florentine buildings
 Centuries later, similar perspective cues have been exploited in computer vision to infer lengths, areas and distance ratios in arbitrary scenes [NN]
Apart from perspective  information, classic computer vision systems have tried to  use other cues like shading [NN] or texture [NN] to recover  depth from a single image
Modern systems [NN, NN, NN, NN]  typically approach this problem from a supervised learning  perspective, letting the system infer which image features  are most discriminative for depth estimation
 Top-down Nd reasoning One of the first algorithms for  depth estimation took a different approach: exploiting the  known Nd structure of the objects in the scene [NN]
It has  been shown that this top-down information is also used by  humans when perceiving human motion abstracted into a  set of sparse point projections [N]
The idea of reasoning  about Nd human posture from a minimal representation such  as sparse Nd projections, abstracting away other potentially  richer image cues, has inspired the problem of Nd pose estimation from Nd joints that we are addressing in this work
 Nd to Nd joints The problem of inferring Nd joints from  their Nd projections can be traced back to the classic work  of Lee and Chen [NN]
They showed that, given the bone  lengths, the problem boils down to a binary decision tree  where each split correspond to two possible states of a  joint with respect to its parent
This binary tree can be  pruned based on joint constraints, though it rarely resulted  in a single solution
Jiang [N0] used a large database  of poses to resolve ambiguities based on nearest neighbor queries
Interestingly, the idea of exploiting nearest  neighbors for refining the result of pose inference has been  recently revisited by Gupta et al
[NN], who incorporated  temporal constraints during search, and by Chen and Ramanan [N]
Another way of compiling knowledge about Nd  human pose from datasets is by creating overcomplete bases  suitable for representing human poses as sparse combinations [N, N, NN, NN, NN, NN], lifting the pose to a reproducible  kernel Hilbert space (RHKS) [NN] or by creating novel priors from specialized datasets of extreme human poses [N]
 Deep-net-based Nd to Nd joints Our system is most related to recent work that learns the mapping between Nd  and Nd with deep neural networks
Pavlakos et al
[NN]  introduced a deep convolutional neural network based on  the stacked hourglass architecture [N0] that, instead of regressing Nd joint probability heatmaps, maps to probability distributions in Nd space
Moreno-Noguer [NN] learns  to predict a pairwise distance matrix (DM) from N-to-Ndimensional space
Distance matrices are invariant up  to rotation, translation and reflection; therefore, multidimensional scaling is complemented with a prior of human  poses [N] to rule out unlikely predictions
 A major motivation behind Moreno-Noguer’s DM regression approach, as well as the volumetric approach of  Pavlakos et al., is the idea that predicting Nd keypoints  from Nd detections is inherently difficult
For example,  Pavlakos et al
[NN] present a baseline where a direct Nd  joint representation (such as ours) is used instead (Table N  in [NN]), with much less accurate results than using volumetric regressionN Our work contradicts the idea that regressing Nd keypoints from Nd joint detections directly should  NThis approach, however, is slightly different from ours, as the input is  still image pixels, and the intermediate Nd body representation is a series  of joint heatmaps – not joint Nd locations
 NNNN  https://github.com/una-dinosauria/Nd-pose-baseline https://github.com/una-dinosauria/Nd-pose-baseline   Linear   N0NN  Batch norm  RELU  Dropout 0.N  +  xN  Linear   N0NN  Batch norm  RELU  Dropout 0.N  Figure N
A diagram of our approach
The building block of our network is a linear layer, followed by batch normalization, dropout and a  RELU activation
This is repeated twice, and the two blocks are wrapped in a residual connection
The outer block is repeated twice
The  input to our system is an array of Nd joint positions, and the output is a series of joint positions in Nd
 be avoided, and shows that a well-designed and simple network can perform quite competitively in the task of Nd-to-Nd  keypoint regression
 Nd to Nd angular pose There is a second branch of algorithms for inferring Nd pose from images which estimate  the body configuration in terms of angles (and sometimes  body shape) instead of directly estimating the Nd position  of the joints [N, N, NN, NN]
The main advantages of these  methods are that the dimensionality of the problem is lower  due to the constrained mobility of human joints, and that  the resulting estimations are forced to have a human-like  structure
Moreover, constraining human properties such  as bone lengths or joint angle ranges is rather simple with  this representation [NN]
We have also experimented with  such approaches; however in our experience the highly nonlinear mapping between joints and Nd points makes learning  and inference harder and more computationally expensive
 Consequently, we opted for estimating Nd joints directly
 N
Solution methodology  Our goal is to estimate body joint locations in Ndimensional space given a N-dimensional input
Formally,  our input is a series of Nd points x ∈ RNn, and our output  is a series of points in Nd space y ∈ RNn
We aim to learn  a function f∗ : RNn → RNn that minimizes the prediction error over a dataset of N poses:  f∗ = min f  N  N  N∑  i=N  L (f(xi)− yi) 
(N)  In practice, xi may be obtained as ground truth Nd joint  locations under known camera parameters, or using a Nd  joint detector
It is also common to predict the Nd positions  relative to a fixed global space with respect to its root joint,  resulting in a slightly lower-dimensional output
 We focus on systems where f∗ is a deep neural network,  and strive to find a simple, scalable and efficient architecture  that performs well on this task
These goals are the main  rationale behind the design choices of our network
 N.N
Our approach – network design  Figure N shows a diagram with the basic building blocks  of our architecture
Our approach is based on a simple, deep, multilayer neural network with batch normalization [NN], dropout [NN] and Rectified Linear Units (RELUs) [NN], as well as residual connections [NN]
Not depicted are two extra linear layers: one applied directly to the  input, which increases its dimensionality to N0NN, and one applied before the final prediction, that produces outputs  of size Nn
In most of our experiments we use N residual blocks, which means that we have N linear layers in total,  and our model contains between N and N million trainable  parameters
 Our architecture benefits from multiple relatively recent  improvements on the optimization of deep neural networks,  which have mostly appeared in the context of very deep  convolutional neural networks and have been the key ingredient of state-of-the-art systems submitted to the ILSVRC  (Imagenet [N0]) benchmark
As we demonstrate, these contributions can also be used to improve generalization on our  Nd-to-Nd pose estimation task
 Nd/Nd positions Our first design choice is to use Nd and  Nd points as inputs and outputs, in contrast to recent work  that has used raw images [NN,NN,NN,NN,NN,NN,NN,NN,NN] or  Nd probability distributions [NN, NN] as inputs, and Nd probabilities [NN], Nd motion parameters [NN] or basis pose coefficients and camera parameter estimation [N, N, NN, NN, NN]  as outputs
While Nd detections carry less information,  their low dimensionality makes them very appealing to  work with; for example, one can easily store the entire HumanN.NM dataset in the GPU while training the network,  which reduces overall training time, and considerably allowed us to accelerate the search for network design and  training hyperparameters
 NNNN    Linear-RELU layers Most deep learning approaches to  Nd human pose estimation are based on convolutional neural networks, which learn translation-invariant filters that  can be applied to entire images [NN, NN, NN, NN, NN], or Ndimensional joint-location heatmaps [NN, NN]
However,  since we are dealing with low-dimensional points as inputs  and outputs, we can use simpler and less computationally  expensive linear layers
RELUs [NN] are a standard choice  to add non-linearities in deep neural networks
 Residual connections We found that residual connections, recently proposed as a technique to facilitate the training of very deep convolutional neural networks [NN], improve generalization performance and reduce training time
 In our case, they helped us reduce error by about N0%
 Batch normalization and dropout While a simple network with the three components described above achieves  good performance on Nd-to-Nd pose estimation when  trained on ground truth Nd positions, we have discovered  that it does not perform well when trained on the output  of a Nd detector, or when trained on Nd ground truth and  tested on noisy Nd observations
Batch normalization [NN]  and dropout [NN] improve the performance of our system in  these two cases, while resulting in a slight increase of trainand test-time
 Max-norm constraint We also applied a constraint on  the weights of each layer so that their maximum norm is  less than or equal to N
Coupled with batch normalization,  we found that this stabilizes training and improves generalization when the distribution differs between training and  test examples
 N.N
Data preprocessing  We apply standard normalization to the Nd inputs and Nd  outputs by subtracting the mean and dividing by the standard deviation
Since we do not predict the global position  of the Nd prediction, we zero-centre the Nd poses around  the hip joint (in line with previous work and the standard  protocol of HumanN.NM)
 Camera coordinates In our opinion, it is unrealistic to  expect an algorithm to infer the Nd joint positions in an arbitrary coordinate space, given that any translation or rotation of such space would result in no change in the input  data
A natural choice of global coordinate frame is the  camera frame [NN, NN, NN, NN, NN, NN] since this makes the  Nd to Nd problem similar across different cameras, implicitly enabling more training data per camera and preventing  overfitting to a particular global coordinate frame
We do  DMR [NN] Ours ∆  GT/GT NN.NN NN.N0 NN.0N  GT/GT + N (0, N) NN.NN NN.NN N0.NN GT/GT + N (0, N0) NN.NN NN.NN NN.NN GT/GT + N (0, NN) NN.0N NN.NN NN.NN GT/GT + N (0, N0) NNN.NN N0.NN NN.NN  GT/CPM [N0] NN.NN – –  GT/SH [N0] – N0.NN –  Table N
Performance of our system on HumanN.NM under protocol #N
(Top) Training and testing on ground truth Nd joint locations plus different levels of additive gaussian noise
(Bottom)  Training on ground truth and testing on the output of a Nd detector
 this by rotating and translating the Nd ground-truth according to the inverse transform of the camera
A direct effect  of inferring Nd pose in an arbitrary global coordinate frame  is the failure to regress the global orientation of the person, which results in large errors in all joints
Note that the  definition of this coordinate frame is arbitrary and does not  mean that we are exploiting pose ground truth in our tests
 Nd detections We obtain Nd detections using the stateof-the-art stacked hourglass network of Newell et al
[N0],  pre-trained on the MPII dataset [N]
Similar to previous  work [NN, NN, NN, NN, NN], we use the bounding boxes provided with HN.NM to estimate the centre of the person in the  image
We crop a square of size NN0 × NN0 pixels around this computed centre to the detector (which is then resized  to NNN × NNN by stacked hourglass)
The average error be- tween these detections and the ground truth Nd landmarks  is NN pixels, which is slightly higher than the N0 pixels reported by Moreno-Noguer [NN] using CPM [N0] on the same  dataset
We prefer stacked hourglass over CPM because (a)  it has shown slightly better results on the MPII dataset, and  (b) it is about N0 times faster to evaluate, which allowed us  to compute detections over the entire HN.NM dataset
 We have also fine-tuned the stacked hourglass model on  the HumanN.NM dataset (originally pre-trained on MPII),  which obtains more accurate Nd joint detections on our target dataset and further reduces the Nd pose estimation error
 We used all the default parameters of stacked hourglass, except for minibatch size which we reduced from N to N due  to memory limitations on our GPU
We set the learning rate  to N.N× N0−N, and train for N0 000 iterations
 Training details We train our network for N00 epochs using Adam [NN], a starting learning rate of 0.00N and exponential decay, using mini-batches of size NN
Initially, the  weights of our linear layers are set using Kaiming initialization [NN]
We implemented our code using Tensorflow,  which takes around Nms for a forward+backward pass, and  NNNN    Protocol #N Direct
Discuss Eating Greet Phone Photo Pose Purch
Sitting SitingD Smoke Wait WalkD Walk WalkT Avg  LinKDE [NN] (SA) NNN.N NNN.N NNN.N NNN.N NNN.N N0N.N NN0.N NNN.N NNN.N NNN.0 NNN.N NN0.N NNN.N NN.N NNN.N NNN.N  Li et al
[NN] (MA) – NNN.N NN.N NNN.N – NNN.N – – – – – – NNN.N N0.0 – –  Tekin et al
[NN] (SA) N0N.N NNN.N NN.N NNN.N NNN.0 NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NN.N NN.N NNN.0  Zhou et al
[NN] (MA) NN.N N0N.N NN.N N0N.N NNN.N NNN.N N0N.N NN.N NNN.N NNN.N N0N.N NNN.N NNN.N NN.N NN.N NNN.0  Tekin et al
[NN] (SA) – NNN.N NN.N NNN.N – NNN.N – – – – – – NN0.N NN.N – –  Ghezelghieh et al
[NN] (SA) N0.N N0.N NN.N NN.N – – – – – – – – – NN.N NN.N –  Du et al
[NN] (SA) NN.N NNN.N N0N.N NNN.N NNN.N NNN.N N0N.N NNN.N NNN.N NNN.N NN0.0 NNN.N NNN.N NN.N N0N.N NNN.N  Park et al
[NN] (SA) N00.N NNN.N N0.0 NNN.N NNN.N NNN.N NNN.N N0N.N NNN.N NN0.N N0N.N NNN.N NNN.N NN.N NN.N NNN.N  Zhou et al
[NN] (MA) NN.N N0N.N NN.N NN.N NNN.N NNN.N N0.0 NN.N NNN.N NNN.0 N0N.0 NN.N NNN.0 NN.0 NN.0 N0N.N  Pavlakos et al
[NN] (MA) NN.N NN.N NN.N NN.N NN.0 NN.0 NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours (SH detections) (SA) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N N0N.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours (SH detections) (MA) NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N  Ours (SH detections FT) (MA) NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours (GT detections) (MA) NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N N0.N NN.N  Table N
Detailed results on HumanN.NM [NN] under Protocol #N (no rigid alignment in post-processing)
SH indicates that we trained and  tested our model with Stacked Hourglass [N0] detections as input, and FT indicates that the Nd detector model was fine-tuned on HN.NM
 GT detections denotes that the groundtruth Nd locations were used
SA indicates that a model was trained for each action, and MA indicates  that a single model was trained for all actions
 around Nms for a forward pass on a Titan Xp GPU
This  means that, coupled with a state-of-the-art realtime Nd detector (e.g., [N0]), our network could be part of full pixelsto-Nd system that runs in real time
 One epoch of training on the entire HumanN.NM dataset  can be done in around N minutes, which allowed us to extensively experiment with multiple variations of our architecture and training hyperparameters
 N
Experimental evaluation  Datasets and protocols We focus our numerical evaluation on two standard datasets for Nd human pose estimation:  HumanEva [NN] and HumanN.NM [NN]
We also show qualitative results on the MPII dataset [N], for which the ground  truth Nd is not available
 HumanN.NM is, to the best of our knowledge, currently  the largest publicly available datasets for human Nd pose  estimation
The dataset consists of N.N million images featuring N professional actors performing NN everyday activities such as walking, eating, sitting, making a phone call  and engaging in a discussion
Nd joint locations and Nd  ground truth positions are available, as well as projection  (camera) parameters and body proportions for all the actors
HumanEva, on the other hand, is a smaller dataset that  has been largely used to benchmark previous work over the  last decade
MPII is a standard dataset for Nd human pose  estimation based on thousands of short youtube videos
 On HumanN.NM we follow the standard protocol, using  subjects N, N, N, N, and N for training, and subjects N and  NN for evaluation
We report the average error in millimetres between the ground truth and our prediction across all  joints and cameras, after alignment of the root (central hip)  joint
Typically, training and testing is carried out independently in each action
We refer to this as protocol #N
However, in some of our baselines, the prediction has been further aligned with the ground truth via a rigid transformation  (e.g
[N,NN])
We call this post-processing protocol #N
Similarly, some recent methods have trained one model for all  the actions, as opposed to building action-specific models
 We have found that this practice consistently improves results, so we report results for our method under these two  variations
In HumanEva, training and testing is done on  all subjects and in each action separately, and the error is  always computed after a rigid transformation
 N.N
Quantitative results  An upper bound on Nd-to-Nd regression Our method,  based on direct regression from Nd joint locations, naturally  depends on the quality of the output of a Nd pose detector,  and achieves its best performance when it uses ground-truth  Nd joint locations
 We followed Moreno-Noguer [NN] and tested under different levels of Gaussian noise a system originally trained  with Nd ground truth
The results can be found in Table N
Our method largely outperforms the Distance-Matrix  method [NN] for all levels of noise, and achieves a peak  performance of NN.N0 mm of error when it is trained on ground truth Nd projections
This is about NN% better than  the best result we are aware of reported on ground truth Nd  joints [NN]
Moreover, note that this result is also about N0%  better than the NN.N mm reported by Pavlakos et al
[NN], which is the best result on HumanN.NM that we aware of –  however, their result does not use ground truth Nd locations,  which makes this comparison unfair
 Although every frame is evaluated independently, and  we make no use of time, we note that the predictions produced by our network are quite smooth
A video with  NNNN    Protocol #N Direct
Discuss Eating Greet Phone Photo Pose Purch
Sitting SitingD Smoke Wait WalkD Walk WalkT Avg  Akhter & Black [N]* (MA) NNj NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NN0.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N  Ramakrishna et al
[NN]* (MA) NNj NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NNN.N NN0.N NNN.N NN0.0 NNN.N NN0.N NNN.N  Zhou et al
[NN]* (MA) NNj NN.N NN.N NN.N NNN.N N0N.N N0N.N NN.N NN.N N0N.N NNN.N N0N.0 N0N.N N0N.N NN0.N NNN.N N0N.N  Bogo et al
[N] (MA) NNj NN.0 N0.N NN.N NN.N NN.N NN.0 NN.0 NN.N N00.N NNN.N NN.N NN.N NN.N NN.N NN.N NN.N  Moreno-Noguer [NN] (MA) NNj NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N N0N.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.0  Pavlakos et al
[NN] (MA) NNj – – – – – – – – – – – – – – – NN.N  Ours (SH detections) (SA) NNj N0.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N  Ours (SH detections) (MA) NNj NN.N NN.0 NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N  Ours (SH detections FT) (MA) NNj NN.N NN.N NN.N NN.0 NN.0 NN.0 NN.N N0.N NN.N NN.N NN.N NN.0 NN.N NN.0 NN.N NN.N  Ours (SH detections) (SA) NNj NN.N NN.0 NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.0 NN.N N0.N NN.N  Table N
Detailed results on HumanN.NM [NN] under protocol #N (rigid alignment in post-processing)
The NNj (NNj) annotation indicates  that the body model considers NN (NN) body joints
The results of all approaches are obtained from the original papers, except for (*), which  were obtained from [N]
 Walking Jogging  SN SN SN SN SN SN Avg  Radwan et al
[NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Wang et al
[NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Simo-Serra et al
[NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Bo et al
[N] NN.N N0.N NN.N NN.N NN.0 NN.N NN.N  Kostrikov et al
[NN] NN.0 N0.N NN.N NN.N NN.0 NN.N N0.N  Yasin et al
[NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Moreno-Noguer [NN] NN.N NN.0 NN.N NN.N N0.0 NN.0 NN.N  Pavlakos et al
[NN] NN.N NN.N NN.0 NN.N NN.N NN.0 NN.N  Ours (SH detections) NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
Results on the HumanEva [NN] dataset, and comparison  with previous work
 these and more qualitative results can be found at https:  //youtu.be/HmiNPdNxNBE
 Robustness to detector noise To further analyze the robustness of our approach, we also experimented with testing  the system (always trained with ground truth Nd locations)  with (noisy) Nd detections from images
These results are  also reported at the bottom of Table N.N In this case, we also  outperform previous work, and demonstrate that our network can perform reasonably well when trained on ground  truth and tested on the output of a Nd detector
 Training on Nd detections While using Nd ground truth  at train and test time is interesting to characterize the performance of our network, in a practical application our system has to work with the output of a Nd detector
We  report our results on protocol #N of HumanN.NM in Table N
Here, our closest competitor is the recent volumetric prediction method of Pavlakos et al
[NN], which uses  a stacked-hourglass architecture, is trained end-to-end on  HumanN.NM, and uses a single model for all actions
Our  NThis was, in fact, the protocol used in the main result of [NN]
 method outperforms this state-of-the-art result by N.N mm even when using out-of-the-box stacked-hourglass detections, and more than doubles the gap to N.0 mm when the Nd detector is fine-tuned on HN.NM
Our method also consistently outperforms previous work in all but one of the NN  actions of HN.NM
 Our results on HumanN.NM under protocol #N (using a  rigid alignment with the ground truth), are shown in Table N
 Although our method is slightly worse than previous work  with out-of-the-box detections, it comes first when we use  fine-tuned detections
 Finally, we report results on the HumanEva dataset in  Table N
In this case, we obtain the best result to date in N  out of N cases, and overall the best average error for actions  Jogging and Walking
Since this dataset is rather small, and  the same subjects show up on the train and test set, we do  not consider these results to be as significant as those obtained by our method in HumanN.NM
 Ablative and hyperparameter analysis We also performed an ablative analysis to better understand the impact  of the design choices of our network
Taking as a basis our  non-fine tuned MA model, we present those results in Table N
Removing dropout or batch normalization leads to  N-toN mm of increase in error, and residual connections ac- count for a gain of about N mm in our result
However, not pre-processing the data to the network in camera coordinates results in error above N00 mm – substantially worse than state-of-the-art performance
 Last but not least, we analyzed the sensitivity of our network to depth and width
Using a single residual block results in a loss of N mm, and performance is saturated after N blocks
Empirically, we observed that decreasing the layers  to NNN dimensions gave worse performance, while layers with N 0NN units were much slower and did not seem to in- crease the accuracy
 NNNN  https://youtu.be/HmiNPdNxNBE https://youtu.be/HmiNPdNxNBE   Figure N
Example output on the test set of HumanN.NM
Left: Nd observation
Middle: Nd ground truth
Right (green): our Nd predictions
 error (mm) ∆  Ours NN.N –  w/o batch norm NN.N NN.0  w/o dropout NN.N N.N  w/o batch norm w/o dropout NN.0 N.N  w/o residual connections NN.N N.N  w/o camera coordinates N0N.N NN.N  N block NN.N N.N  N blocks (Ours) NN.N –  N blocks NN.N N.N  N blocks NN.N N.N  Table N
Ablative and hyperparameter sensitivity analysis
 N.N
Qualitative results  Finally, we show some qualitative results on HumanN.NM in Figure N, and from images “in the wild” from  the test set of MPII in Figure N
Our results on MPII reveal  some of the limitations of our approach; for example, our  system cannot recover from a failed detector output, and it  has a hard time dealing with poses that are not similar to  any examples in HN.NM (e.g
people upside-down)
Finally,  in the wild most images of people do not feature full bodies,  but are cropped to some extent
Our system, trained on full  body poses, is currently unable to deal with such cases
 N
Discussion  Looking at Table N, we see a generalized increase in error when training with SH detections as opposed to training  with ground truth Nd across all actions – as one may well  expect
There is, however, a particularly large increase in  the classes taking photo, talking on the phone, sitting and  sitting down
We hypothesize that this is due to the severe self-occlusions in these actions – for example, in some  phone sequences, we never get to see one of the hands of  the actor
Similarly, in sitting and sitting down, the legs are  often aligned with the camera viewpoint, which results in  large amounts of foreshortening
 Further improvements The simplicity of our system  suggests multiple directions of improvement in future work
 For example, we note that stacked hourglass produces final joint detection heatmaps of size NN × NN, and thus a larger output resolution might result in more fine-grained  detections, moving our system closer to its performance  when trained on ground truth
Another interesting direction is to use multiple samples from the Nd stacked hourglass heatmaps to estimate an expected gradient – à la policy gradients, commonly used in reinforcement learning –  so as to train a network end-to-end
Yet another idea is to  emulate the output of Nd detectors using N-dimensional mocap databases and “fake” camera parameters for data augmentation, perhaps following the adversarial approach of  Shrivastava et al
[NN]
Learning to estimate coherently the  depth of each person in the scene is an interesting research  path, since it would allow our system to work on Nd pose  estimation of multiple people
Finally, our architecture is  simple, and it is likely that further research into network  design could lead to better results on Nd-to-Nd systems
 N.N
Implications of our results  We have demonstrated that a relatively simple deep feedforward neural network can achieve a remarkably low error  rate on Nd human pose estimation
Coupled with a state-ofthe-art Nd detector, our system obtains the best results on Nd  pose estimation to date
 NNNN    Figure N
Qualitative results on the MPII test set
Observed image, Nd detection with Stacked Hourglass [N0], (in green) our Nd prediction
 The bottom N examples are typical failure cases, where either the Nd detector has failed badly (left), or slightly (right)
In the middle, the Nd  detector does a fine job, but the person is upside-down and HumanN.NM does not provide any similar examples – the network still seems  to predict an average pose
 Our results stand in contrast to recent work, which has  focused on deep, end-to-end systems trained from pixels to  Nd positions, and contradicts the underlying hypothesis that  justify the complexity of recent state-of-the-art approached  to Nd human pose estimation
For example, the volumetric  regression approach of [NN] et al
is based on the hypothesis that directly regressing Nd points is inherently difficult,  and regression in a volumetric space would provide easier  gradients for the network (see Table N in [NN])
Although  we agree that image content should help to resolve challenging ambiguous cases (consider for example the classic  turning ballerina optical illusion), competitive Nd pose estimation from Nd points can be achieved with simple high  capacity systems
This might be related to the latent information about subtle body and motion traits existing in  Nd joint stimuli, such as gender, which can be perceived  by people [NN]
Similarly, the use of a distance matrix as  a body representation in [NN] is justified by the claim that  invariant, human-designed features should boost the accuracy of the system
However, our results show that well  trained systems can outperform these particular features in  a simple manner
It would be interesting to see whether a  combination of joint distances and joint positions boost the  performance even further – we leave this for future work
 N
Conclusions and future work  We have shown that a simple, fast and lightweight deep  neural network can achieve surprisingly accurate results in  the task of Nd-to-Nd human pose estimation; and coupled  with a state-of-the-art Nd detector, our work results in an  easy-to-reproduce, yet high-performant baseline that outperforms the state of the art in Nd human pose estimation
 Our accuracy in Nd pose estimation from Nd ground  truth suggest that, although Nd pose estimation is considered a close to solved problem, it remains as one of the  main causes for error in the Nd human pose estimation task
 Moreover, our work represents poses in simple Nd and Nd  coordinates, which suggests that finding invariant (and more  complex) representations of the human body, as has been  the focus of recent work, might either not be crucial, or have  not been exploited to its full potential
 Finally, given its simplicity and the rapid development  in the field, we like to think of our work as a future baseline, rather than a full-fledged system for Nd pose estimation
This suggests multiple directions of future work
 For one, our network currently does not have access to visual evidence; we believe that adding this information to  our pipeline, either via fine-tuning of the Nd detections or  through multi-sensor fusion will lead to further gains in performance
On the other hand, our architecture is similar to a  multi-layer perceptron, which is perhaps the simplest architecture one may think of
We believe that a further exploration of the network architectures will result in improved  performance
These are all interesting areas of future work
 Acknowledgments The authors thank NVIDIA for the  donation of GPUs used in this research
Julieta was supported in part by the Perceiving Systems group at the Max  Planck Institute for Intelligent Systems
This research was  supported in part by the Natural Sciences and Engineering  Research Council of Canada (NSERC)
 NNNN    References  [N] A
Agarwal and B
Triggs
ND human pose from silhouettes  by relevance vector regression
In CVPR, N00N
N  [N] I
Akhter and M
J
Black
Pose-conditioned joint angle limits for ND human pose reconstruction
In CVPR, N0NN
N, N,  N  [N] M
Andriluka, L
Pishchulin, P
Gehler, and B
Schiele
Nd  human pose estimation: New benchmark and state of the art  analysis
In CVPR, N0NN
N, N  [N] C
Barron and I
A
Kakadiaris
Estimating anthropometry and pose from a single uncalibrated image
CVIU,  NN(N):NNN–NNN, N00N
N  [N] L
Bo and C
Sminchisescu
Twin Gaussian processes for  structured prediction
IJCV, NN(N-N), N0N0
N  [N] L
F
Bo, C
Sminchisescu, A
Kanaujia, and D
N
Metaxas
 Fast algorithms for large scale conditional ND prediction
In  CVPR, pages N–N, N00N
N  [N] F
Bogo, A
Kanazawa, C
Lassner, P
Gehler, J
Romero,  and M
J
Black
Keep it SMPL: Automatic estimation of Nd  human pose and shape from a single image
In ECCV, N0NN
 N, N, N, N  [N] I
Bülthoff, H
Bülthoff, and P
Sinha
Top-down influences  on stereoscopic depth-perception
Nature Neuroscience,  N(N):NNN–NNN, NNNN
N  [N] C.-H
Chen and D
Ramanan
ND human pose estimation =  ND pose estimation + matching
In CVPR, N0NN
N  [N0] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
In  CVPR, N00N
N  [NN] Y
Du, Y
Wong, Y
Liu, F
Han, Y
Gui, Z
Wang, M
Kankanhalli, and W
Geng
Marker-less Nd human motion capture  with monocular image sequence and height-maps
In ECCV,  N0NN
N, N, N  [NN] D
Eigen and R
Fergus
Predicting depth, surface normals  and semantic labels with a common multi-scale convolutional architecture
In ICCV, N0NN
N  [NN] M
F
Ghezelghieh, R
Kasturi, and S
Sarkar
Learning camera viewpoint using cnn to improve Nd body pose estimation
 In NDV, N0NN
N, N, N  [NN] A
Gupta, J
Martinez, J
J
Little, and R
J
Woodham
ND  Pose from Motion for Cross-view Action Recognition via  Non-linear Circulant Temporal Encoding
In CVPR, N0NN
 N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Delving deep into  rectifiers: Surpassing human-level performance on imagenet  classification
In ICCV, pages N0NN–N0NN, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, pages NN0–NNN, N0NN
N, N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
In  ICML, N0NN
N, N  [NN] C
Ionescu, J
Carreira, and C
Sminchisescu
Iterated  second-order label sensitive pooling for Nd human pose estimation
In CVPR, N0NN
N  [NN] C
Ionescu, D
Papava, V
Olaru, and C
Sminchisescu
Human N.Nm: Large scale datasets and predictive methods for  Nd human sensing in natural environments
TPAMI, NN(N),  N0NN
N, N, N  [N0] H
Jiang
Nd human pose reconstruction using millions of  exemplars
In ICPR, pages NNNN–NNNN, Aug N0N0
N  [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
In ICLR, N0NN
N  [NN] I
Kostrikov and J
Gall
Depth sweep regression forests for  estimating Nd human pose from images
In BMVC, N0NN
N  [NN] H
J
Lee and Z
Chen
Determination of ND human body  postures from a single view
Computer Vision, Graphics and  Image Processing, N0:NNN–NNN, NNNN
N  [NN] S
Li, W
Zhang, and A
B
Chan
Maximum-margin structured learning with deep networks for Nd human pose estimation
In ICCV, N0NN
N, N, N  [NN] T
Lindeberg and J
Garding
Shape from texture from a  multi-scale perspective
In ICCV, NNNN
N  [NN] F
Liu, C
Shen, and G
Lin
Deep convolutional neural fields  for depth estimation from a single image
In CVPR, pages  NNNN–NNN0
IEEE Computer Society, N0NN
N  [NN] F
Moreno-Noguer
Nd human pose estimation from a single  image via distance matrix regression
In CVPR, N0NN
N, N,  N, N, N  [NN] G
Mori and J
Malik
Recovering ND human body configurations using shape contexts
TPAMI, NN(N):N0NN–N0NN, July  N00N
N  [NN] V
Nair and G
E
Hinton
Rectified linear units improve  restricted Boltzmann machines
In ICML, pages N0N–NNN,  N0N0
N, N  [N0] A
Newell, K
Yang, and J
Deng
Stacked hourglass networks for human pose estimation
In ECCV, N0NN
N, N, N,  N, N  [NN] V
Parameswaran and R
Chellappa
View independent human body pose estimation from a single perspective image
 In CVPR, N00N
N  [NN] S
Park, J
Hwang, and N
Kwak
Nd human pose estimation  using convolutional neural networks with Nd pose information
In ECCV N0NN Workshops, N0NN
N, N, N  [NN] G
Pavlakos, X
Zhou, K
G
Derpanis, and K
Daniilidis
 Coarse-to-fine volumetric prediction for single-image ND human pose
In CVPR, N0NN
N, N, N, N, N, N, N  [NN] A
Popa, M
Zanfir, and C
Sminchisescu
Deep Multitask  Architecture for Integrated ND and ND Human Sensing
In  CVPR, N0NN
N  [NN] I
Radwan, A
Dhall, and R
Goecke
Monocular image Nd  human pose estimation under self-occlusion
In ICCV, N0NN
 N  [NN] V
Ramakrishna, T
Kanade, and Y
Sheikh
Reconstructing  ND Human Pose from ND Image Landmarks
In ECCV, N0NN
 N, N, N  [NN] L
G
Roberts
Machine perception of three-dimensional  solids
TR NNN, Lincoln Lab, MIT, Lexington, MA, May  NNNN
N  [NN] G
Rogez and C
Schmid
Mocap-guided data augmentation  for ND pose estimation in the wild
In NIPS, N0NN
N  [NN] A
Saxena, M
Sun, and A
Y
Ng
Learning N-D scene structure from a single still image
In ICCV, N00N
N  NNNN    [N0] G
Shakhnarovich, P
A
Viola, and T
J
Darrell
Fast pose  estimation with parameter-sensitive hashing
In ICCV, N00N
 N  [NN] A
Shrivastava, T
Pfister, O
Tuzel, J
Susskind, W
Wang,  and R
Webb
Learning from simulated and unsupervised  images through adversarial training
In CVPR, N0NN
N  [NN] L
Sigal, A
O
Balan, and M
J
Black
Humaneva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion
IJCV,  NN(N-N), N0N0
N, N  [NN] E
Simo-Serra, A
Quattoni, C
Torras, and F
MorenoNoguer
A joint model for Nd and Nd pose estimation from a  single image
In CVPR, N0NN
N  [NN] N
Srivastava, G
E
Hinton, A
Krizhevsky, I
Sutskever, and  R
Salakhutdinov
Dropout: a simple way to prevent neural  networks from overfitting
JMLR, NN(N), N0NN
N, N  [NN] B
Tekin, I
Katircioglu, M
Salzmann, V
Lepetit, and P
Fua
 Structured prediction of Nd human pose with deep neural networks
In BMVC, N0NN
N, N, N, N  [NN] B
Tekin, A
Rozantsev, V
Lepetit, and P
Fua
Direct prediction of Nd body poses from motion compensated sequences
 In CVPR, N0NN
N, N, N  [NN] N
F
Troje, J
Sadr, H
Geyer, and K
Nakayama
Adaptation aftereffects in the perception of gender from biological  motion
Journal of vision, N(N):N–N, N00N
N  [NN] G
Varol, J
Romero, X
Martin, N
Mahmood, M
J
Black,  I
Laptev, and C
Schmid
Learning from synthetic humans
 In CVPR, N0NN
N  [NN] C
Wang, Y
Wang, Z
Lin, A
L
Yuille, and W
Gao
Robust  estimation of Nd human poses from a single image
In CVPR,  N0NN
N, N  [N0] S.-E
Wei, V
Ramakrishna, T
Kanade, and Y
Sheikh
Convolutional pose machines
In CVPR, N0NN
N, N, N  [NN] X
K
Wei and J
Chai
Modeling ND human poses from  uncalibrated monocular images
In ICCV, N00N
N  [NN] H
Yasin, U
Iqbal, B
Kruger, A
Weber, and J
Gall
A dualsource approach for Nd pose estimation from a single image
 In CVPR, N0NN
N  [NN] R
Zhang, P.-S
Tsai, J
E
Cryer, and M
Shah
Shape from  shading: A survey
TPAMI, NN(N):NN0–N0N, NNNN
N  [NN] X
Zhou, X
Sun, W
Zhang, S
Liang, and Y
Wei
Deep  kinematic pose regression
In ECCV Workshops, N0NN
N, N,  N  [NN] X
Zhou, M
Zhu, S
Leonardos, and K
Daniilidis
Sparse  representation for Nd shape estimation: A convex relaxation  approach
TPAMI, NN(N), N0NN
N, N, N  [NN] X
Zhou, M
Zhu, S
Leonardos, K
G
Derpanis, and  K
Daniilidis
Sparseness meets deepness: Nd human pose  estimation from monocular video
In CVPR, N0NN
N, N, N, N  [NN] A
Zisserman, I
D
Reid, and A
Criminisi
Single view  metrology
In ICCV, NNNN
N  NNNNRevisiting IMNGPS in the Deep Learning Era   Revisiting IMNGPS in the Deep Learning Era  Nam Vo  Georgia Tech  namvo@gatech.edu  Nathan Jacobs  University of Kentucky  jacobs@cs.uky.edu  James Hays  Georgia Tech  hays@gatech.edu  Abstract  Image geolocalization, inferring the geographic location  of an image, is a challenging computer vision problem with  many potential applications
The recent state-of-the-art approach to this problem is a deep image classification approach in which the world is spatially divided into cells and  a deep network is trained to predict the correct cell for a  given image
We propose to combine this approach with  the original ImNGPS approach in which a query image is  matched against a database of geotagged images and the  location is inferred from the retrieved set
We estimate the  geographic location of a query image by applying kernel  density estimation to the locations of its nearest neighbors  in the reference database
Interestingly, we find that the best  features for our retrieval task are derived from networks  trained with classification loss even though we do not use  a classification approach at test time
Training with classification loss outperforms several deep feature learning  methods (e.g
Siamese networks with contrastive of triplet  loss) more typical for retrieval applications
Our simple approach achieves state-of-the-art geolocalization accuracy  while also requiring significantly less training data
 N
Introduction  In recent years, the recognition community has broadened its focus beyond object categorization to the understanding of a litany of object, scene, material, or ND attributes
One of the most important attributes of an image is geolocation – if we know the location of a photo,  we trivially know hundreds of additional attributes (any attribute for which a map exists, e.g
population density, average temperature, crime rate, elevation, distance to a McDonald’s, etc.)
Knowing the location of an image is also  a common photo forensics task
For example, the mobile  app TraffickCam collects hotel room images to locate incidents of abuse
Unlike many computer vision tasks, computational systems typically exceed the performance of humans at image geolocalization because it is hard for humans  to have an accurate visual model of the entire world
 Figure N
This work addresses the image geolocalization problem:  given a large set of GPS-tagged images, learn to infer the GPS  coordinate of a query image with unknown location
 Estimating the geolocation of an arbitrary photo is still a  challenging task (Figure N)
In particular, we examine the  task of predicting the location of a single photo given only  the image content with no metadata
We consider this task  at a global scale and attempt to estimate the GPS coordinates for any query image
For this task, localization can  be considered successful if the estimated location is within  a specified error threshold
Depending on the application,  this threshold could be street level (Nkm), city level (NNkm),  region level (N00km), country level (NN0km), or continent  level (NN00km)
We adopt these five levels of granularity  from prior work and examine the performance of geolocalization strategies at these error thresholds
 One natural approach to the image geolocalization task  would be to to treat it like an instance retrieval task and  match local features from the query image (and perhaps  their geometric layout) to a reference database of images  with known locations [NN]
Such approaches work well if  (N) there are images in the reference database with a field  of view that significantly overlaps with that of the query  NNNN    image and (N) if the content of the query image is well  suited to local feature matching (i.e., it has distinctive manmade or geological features)
Unfortunately, this is often  not the case, especially for query images away from tourist  destinations and dense urban areas
Therefore, it is necessary to infer location without requiring direct local-feature  matching
In these cases, image geolocalization is similar  to scene classification or scene attribute estimation in that a  system needs to achieve a higher-level, more qualitative understanding of an image, e.g
recognizing that buildings are  typical of Greek islands even though this particular island  isn’t in the reference database
 ImNGPS [N0, NN] introduced the global geolocalization  problem and used hand-crafted features from the instance  recognition and scene classification literature jointly to retrieve nearest neighbors in a database of N million geotagged images
ImNGPS found that roughly half of successful geolocalizations are instance level matches whereas  half are more qualitative matches based on shared scene attributes (geology, architecture, land cover, etc.)
 More recently, PlaNet [NN] formulates image geolocalization as a classification task
This is done by mapping  the GPS coordinate (a pair of real numbers) to a discrete  class label by dividing the surface of the earth into distinct regions
PlaNet proposes a deep convolutional neural  network to estimate a probability distribution over regions  from raw pixel values
PlaNet not only significantly outperforms ImNGPS in terms of accuracy, it is also dramatically  faster since it requires only a forward pass through a deep  network instead of a nearest neighbor search through millions of image features
 Is the deep image classification formulation of PlaNet  the best approach to geolocalization (as it seems to be for  most other scene understanding tasks)? There are two reasons to suspect it might not be ideal – first, discretizing the  Earth’s surface is lossy since we are ultimately interested  in a real-valued location estimate (potentially expressed  through GPS coordinates)
Second, and more limiting, is  that a single deep network, even with tens of millions of parameters, will struggle to memorize the visual appearance  of the entire Earth
An effective deep network needs to learn  to do both instance matching and more qualitative scene  understanding
Can contemporary deep networks implicitly  ‘memorize’ tens of millions of photographic features necessary for the instance matching?  In this paper, we adopt the retrieval approach of ImNGPS  but pair it with deep feature learning as in PlaNet
We outperform PlaNet by a significant margin – NN.N% accuracy  vs NN.N% for PlaNet on the ImNGPS test set with a N00km  threshold 
Interestingly, while we approach geolocalization  as a retrieval task with learned deep features, we don’t see a  benefit to using embedding formulations (e.g
Siamese networks with contrastive or triplet loss) typical for retrieval  tasks
Our best performance comes from training a classification network, in the spirit of PlaNet, and using its intermediate activations as our image feature
 The contributions of this study are:  • We significantly improve the state-of-the-art accuracy  for global image geolocalization
Our increase in accuracy is similar in magnitude to that achieved by  PlaNet [NN] over the hand-crafted retrieval approach  of ImNGPS
We achieve this with as little as N% of the  training data used by PlaNet, and increase the gap further while using NN% as much reference data
 • Our increase in accuracy comes from changing the formulation from classification to retrieval
The benefit of  retrieval in this setting is a reflection of the geolocalization problem and the nature of current deep models  – the visual world is too complex for a deep model to  memorize, but a retrieval approach does so trivially
 • We investigate different strategies for learning a deep  feature embedding for geolocalization
Surprisingly,  deep feature learning methods typically used for retrieval applications do not outperform training with a  classification loss
For classification-based localization, we find that different discretization strategies also  have a significant impact
 • Through extensive experimentation, we find that some  training procedures lead to higher accuracy at the  street scale (Nkm) and others at the country scale  (NN0km)
We observe a trade off between fine-scale  and coarse-scale performance, the regimes traditionally approached with instance-level matching methods  and scene classification methods, respectively
 Related works are discussed in the next section
We describe image geolocalization system designs in Section N
 Experiments and analysis are reported in Section N and we  conclude in Section N
 N
Related Work  Recent years have seen a dramatic expansion of deep  learning methods for scene understanding tasks [NN]
Deep  learning has been applied successfully to location prediction [NN] and other tasks related to our problem: predicting scene type [N0], perceptual attributes [N] such as safety,  liveliness and geo-informative attributes [NN] like GDP, elevation
 Image retrieval using learned, deep representations is  useful to a wide range of tasks such as product ranking  [N, NN], sketch based image retrieval [NN], face recognition  [NN, NN], cross-view localization [NN, NN, NN] and scene retrieval [NN, N, NN, N]
Distance metric learning (DML) is  usually employed with a deep network, most commonly using the contrastive loss [N] or triplet ranking (hinge loss)  NNNN    Figure N
We study six schemes for discretizing geographic location
These vary from coarse to fine (N0, N0, NNN, N0N0, NNNN and N0NN  regions respectively)
 Figure N
Our proposed CNN architecture consists of the convolutional layers of the VGG-NN network [NN] followed by a global  max pooling layer
Depending on the task, we append to this an  output layer and the corresponding loss layer
For classification,  we use a fully connected layer and Softmax-CrossEntropy loss, for  retrieval, we use a DML loss
 [N, NN, NN]
New loss functions and example mining strategies have been proposed as they play important role in the  learning process [NN, N0, NN, NN]
 We are studying image retrieval geolocalization which  has overlap with instance-level scene retrieval [NN, N, NN, N];  Since this line of work mostly focuses on instance-level  matching, benchmarks designed for this task consist of popular scenes or landmarks [NN, NN, NN] and similarity between  matched images are visually recognizable (by humans or  geometric verification)
In this regime, with manual labeling and/or clever example mining, it is beneficial to apply  distance metric learning
Techniques such as geometry verification or query expansion typically improve instance retrieval mAP, but these techniques are less useful when geolocalizing scenes that do not have instance matches
 Many previous works on image localization are at limited spatial scale (urban areas) or on special class of images (landmarks, streetview) [NN, NN, NN, NN, N, NN, NN]
 Many approaches make used of aerial imagery for localization [N, NN, NN]
In [NN, NN, NN], images of the same  scene from the ground viewpoint and overhead viewpoint  are embedded in the same feature space through deep learning DML; the resulting system then does localization by  image retrieval using reference database of aerial images
 Also related, to match aerial images across wide baselines,  Altwaijry et al
[N] propose a deep attentive architecture to  classify whether two views match
 Image geolocalization at planet scale is challenging and  less studied
An effort to advance this area is the placing  task at MediaEval [N]
Two more notable works that aim  for global coverage are ImNGPS [N0, NN] and PlaNet [NN],  which we build upon
 N
Image Geolocalization using Deep Learning  Given a large training data of images with GPS labels,  we examine two deep learning approaches for geolocalization
For both cases we use the same architecture shown  in Figure N which has been popular for landmark recognition [N, NN, N]
 N.N
Geolocalization by classification  One approach is to formulate geolocalization as a classification problem [NN]: the GPS label is converted to class  label by quantizing all GPS labels to a fixed number of  classes, so that each class represents a physical region in the  real world
The classification result then can be converted  back to the GPS coordinate of the corresponding region
 PlaNet[NN] divides the Earth into a set of geographical cells based on image density
We derive a similar  adaptive scheme: starting with a single cell of the entire  world, repeatedly divide each cells along latitude or longitude whichever side is bigger (either evenly or randomly)  until the number of images in each cell is smaller than a  threshold timg or the physical area is smaller than a threshold tarea; these parameters define how fine the partitioning  is
 To predict the location as precisely as possible, one  would prefer a fine-grained partitioning (for example [NN]’s  partitioning has NN,NNN cells)
However we should take into  account the training data’s size, the learning model’s capacity and especially the localization error tolerance
We  NNNN    Figure N
A visual overview of our image-retrieval approach to image geolocalization
We extract a feature from our CNN, find nearby  neighbors in feature space, and estimate the GPS coordinate using either the top NN or the density
 Figure N
When performing distance metric learning, we sample images based on their distance, either in label space (geographic distance) or feature space, to an anchor image
Some example images that are close/far from an anchor image in the label  space/feature space
 investigate N different partitionings shown in Figure N
Admittedly these choices are somewhat arbitrary, as we do not  directly control the number of cells, nor do we try to “optimize” each partitioning
We used similar parameter to [NN]  to obtain a fine grained partitioning (though [NN]’s data is  ∼NN times bigger so they still have √ NN times more cells);  then we loosen the thresholds to obtain the other N coarser  partitionings
 Multiple class labeling: We investigate the effect of using multiple partitionings simultaneously
The motivation is  that different proximity information is preserved at different  levels of granularity (and not the others)
Moreover classification results at multiple coarse partitionings can be combined to produce a more fine grained prediction
Therefore  we experiment with training multiple classification losses  as these tasks are heavily correlated and benefit each other
 N.N
Geolocalization by image retrieval  This approach looks up images that are similar to the  query images and makes use of the known locations of those  images [N0]
This requires learning a representation for  comparing images (for which we will use deep learning)  and indexing a large reference database
 To learn such a representation, we employ distance metric learning (ranking/triple hinge loss, contrastive loss and  similar loss functions) which requires pairs of images labeled ‘similar’ or ’different’
When not available, such labeling can be automatically generated using geometry verification [N, NN] or class labels [N0, NN]
In our case, we  make use of the class label described in the previous section or directly threshold the GPS distance between the N  images
Similar to [N], we can also match images that are  not only close in the GPS label space but also close in the  current feature space
Even so, with the data we are dealing  with, this supervision is very weak in the sense that matched  images (taken at the same location/region) are most likely  not of the same or even similar scene/object (Figure N)
 After training we use the CNN as a feature extractor and  index a large dataset of reference image features
At test  time, we look up the nearest neighbor (NN) of the query image in the feature space using approximate NN search and  output its location (Figure N)
This approach works based  on the assumption that, after learning, images close in the  feature space are likely to be close in the label (GPS coordinate) space too
 k-NN density estimation: we can make use of the top k  NN instead of only N
We perform weighted kernel density  estimation using each NN as a Gaussian kernel, the density  at a point x in GPS coordinate space can be written as:  f(x) =  k∑  i=N  wiN(x;xi, σ NI) (N)  Where xi is the GPS coordinate of the i-th NN, we also  weight each NN wi = s m i depending on its similarity score  si (defined to be the inverse of the distance between the  query image’s feature and the reference image’s feature)
 The point with highest density is chosen as output
 Note that as k decrease, m increase or σ decreases, this  output becomes the NN
These parameters can be optimized: bigger reference data allows bigger k and looser error threshold allows bigger σ
Given our dataset (described  in the next section) We choose m = N0, k = N00 through validation (these parameters were not precisely tuned) and  experimentally manipulate σ
 N
Experiments  Training data: We use the ImNGPS dataset from [N0]
 It consists of more than N million images collected from  Flickr that are tagged with countries or states’ name and  NNNN    Figure N
Example results of geolocalization by image classification using different partitionings
From left to right: input images,  classification result with N0, N0N0 and N0NN classes respectively (lighter region means higher probability)
Red * denotes the predicted  location and green o denotes the true location
 also have GPS coordinates
This data is used for GPS quantization (Figure N), training deep networks, and as retrieval  reference database
 Testing data: for analysis, we construct N test sets; we  make sure that no image from training and test data come  from the same photographer
 • ImNGPSNk: N000 images from ImNGPS
Note that  this is different from the ImNGPS test set [N0]
 • YFCCNk: N000 random images from the YFCCN00m  dataset [NN]
Since it is designed for general computer  vision purpose, its image distribution is different from  ImNGPS making this test set more challenging
 Training for classification: we train the following networks:  • Lone: We trained a network with a single classification loss corresponding to the most fine grained partition (N0NN classes)
This can be considered an analog  of PlaNet [NN] at smaller scale
We also train another  version LN for the NNN ways classification loss
 • Multi: We train another classification network with N  different losses corresponding to N partitions scheme  described in section N.N
Hence this network produces  N localization outputs 
We’ll treat these outputs independently and evaluate the performance of each of  them
 Training for retrieval: we fine-tune the model L with  ranking loss (triplet hinge loss) to learn a better representation, resulting in a Ranking network
To do localization  by retrieval, we experiment with different networks as feature extractor: the classification networks (L and M) and the  ranking network (R)
 We also evaluate two other publicly available state-ofthe-art models, NetVLAD [N] and Siamac [NN], which have  similar architecture (VGG-conv layers), but different training data (weakly supervised Google streetview time machine and SfM landmark images hard example mining),  global pooling layer (NetVLAD and R-Max) and loss function (triplet hinge loss and contrastive loss)
Different from  us, these models have an additional PCA & whitening step
 Features from all models are LN-normalized when used for  retrieval
 Notation: we will use [Model]Approach to refer to each method, where Model can be L, LN, M, R, NetVLAD,  Siamac described above, and Approach can be C (for classification), NN, kNN (for retrieval)
For example [M]NNNC  refers to the NNN way classification output of model M, and  [M]NN refer to the NN retrieval approach using model M  as feature extractor
 Metric: the geolocalization accuracy is defined as the  percentage of test images whose predicted location is within  the error threshold from the true location
Similar to [N0, NN,  NN], N error thresholds are used: Nkm, Nkm, NNkm, NN0km,  NN00km corresponding to N levels of localization: street,  city, region, country, continent
 Result: Qualitative results are shown in Figures N and  N
Quantitative results on two test sets are shown in Figure  N
For comparison we add a simple baseline: always outputting London, which is the region with the most images
 This baseline is practically the best one can do without looking at the input image; its performance is much better than  guessing a random location on the Earth
 We will ensure that our results can be replicated by sharing our datasets, source code, and trained models
 N.N
Comparing classification performance  An example output of classification is in Figure N
In the  case of less ambiguous image, the network would be able  to predict the correct region/cell
Since the center of the  region is used, a finer partitioning will lead to a prediction  NNNN    Figure N
Example results of our geolocalization by image retrieval system (kNN, σ=N)
Each row shows the input image on the left,  the first few NNs on the right, together with their locations (blue *)
At the end of the row we show the density result, red * denotes the  predicted location and green o denotes the true location
 that is closer to the true location (top row)
Though in case  of the image being very ambiguous, correctly localizing it  at coarser level is more likely (bottom row)
 As shown in Figure N, the geolocalization accuracy of  the N0 way classification output is quite bad, this is mostly  because at this scale the Earth is under-divided
We can see  that as the partitioning is finer, the localization performance  at lower error threshold gets better as expected
The finegrained classification output (N0NNC) outperforms others at  street and city level
 Most interesting, the geolocalization accuracy at coarse  level gets worse if the partitioning is too fine: for example at  continent level, the N0C and NNNC achieve highest accuracy;  At country level, the NNNC and N0N0C have the advantage
 This seems to indicate a trade off between the accuracy at  coarse and fine level, which may be a shortcoming of the  partitioning in PlaNet [NN]
 [M]N0NNC and [L]N0NNC achieve similar accuracy ([L]  is slightly better)
However in the case of NNNC, [M] is  slightly better than [LN]
This suggests that when training with multiple classification losses, the fine-grained one  seems to help the coarse one a little, but not vice versa
 N.N
Comparing retrieval performance  Figure N shows example image retrieval results
The  NNs are similar scenes to the input image
In the case of  landmarks and popular sites, they are usually instance level  matches
 As shown in Figure N, with localization by NN image retrieval, all N models (R, M, L, NetVLAD, Siamac) perform  well and outperform the classification result at street and  city level
This makes sense as these successful localizations are likely correct instance-level matches
While classification network can learn the general characteristics of  NNNN    Figure N
Geolocalization accuracy on two test sets
Note that the accuracy is presented as the top of the bars, not the length of each single  color
 each regions, it doesn’t have enough capacity to ‘remember’ all specific instances, while the retrieval approach ‘remembers’ this by directly saving all reference features
 Among all N models, NetVLAD is the worst
Siamac  is the most discriminative at street level
As a trade off,  it has slightly lower performance at coarse level (country  and continent)
The L and M models are comparable and  they perform relatively well even though they are trained for  classification
Coarse partitioning classification approaches  still have the advantage at country and continent scale
 Finally, using kNN-kernel density estimation improves  the accuracy (here we only show [L] and [Siamac] but the  changes when using other models are similar); especially at  coarse scales (as σ increases) this makes retrieval competitive with the classification approach
However bigger σ can  potentially lower the accuracy at fine grained level
Interestingly, we arrive at a similar trade off between fine and  coarse geolocalization accuracy
 N.N
Training a ranking network with GPS label  Model R (which was fine-tuned from L) doesn’t produce  a noticeable improvement over L or M (Figure N)
In further  investigations, we train a dozen versions of R, fine-tuned  from different pretrained models and varied the way we  sample/mine training examples
In all cases, little progress  is observed in term of both training loss and geolocalization  performance
 However when using landmark matches from [NN] for  training instead of ImNGPS data, we observe slight improvement at street level, but worse results at other scales
 This is consistent with the fact that Siamac[NN] is very good  at street level
 Distance metric learning losses like triplet hinge loss  seems to be very sensitive to noisy labels
Different from  classification loss (where the label for each image is fixed  during training), the “target” of each training image keep  changing while they are adjusting distance from each other,  usually making convergence slower
 We hypothesize that the inter-class ambiguity and intraclass diversity are too large and DML is not able to learn  from GPS supervision (Figure N)
 N.N
Comparing with IMNGPS and PlaNet  On the ImNGPS test set, we can directly compare  ImNGPS [N0, NN] and PlaNet [NN] with two of our models: • The fine-grained classification network ([L] N0NNC)
 This can be considered the equivalent of Google’s  PlaNet[NN] at smaller scale
 • kNN kernel density estimation retrieval ([L] kNN,  σ=N)
This can be considered the equivalent of  ImNGPS approach [N0], but using deep features instead  of classical features
 The result is shown in table N
Our classification network outperforms ImNGPS even though it is still not as  good as PlaNet
On the other hand, our localization by deep  learnt image retrieval method produces even better accuracies
This result highlights the advantage of retrieval approach for fine-grain localization
 NNNN    Complexity analysis: in term of number of parameters  without counting the output layers, PlaNet is N times bigger than our NN layers deep VGG model
Note that PlaNet  uses an Inception architecture which has been heavily designed to optimize for complexity [NN, N0] (for reference, it  is N times bigger than NN layers deep GoogLeNet[NN] and  N times bigger than NN layers deep InceptionVN[N0])
Also  PlaNet’s training data has more than N0 million images and  it takes N.N months to train on clusters (approximately N0  years of CPU time)
However in term of space complexity,  our image retrieval approach requires all reference features  be available during testing, not just the deep network
More  over, the cost of indexing and perform NN search is not negligible; though indexing needs to be done only once and in  our experience the cost of approximate NN search is smaller  than that of feature extraction
 Comparing to ImNGPS [N0, NN], deep learning feature  extraction is orders of magnitudes faster than computing  many classical computer vision features
ImNGPS’s combined feature has more than N00k dimensions; in [NN] lazy  learning is done for each query adding more time complexity
In contrast, our deep feature with NNN dimensions is  suitable for direct comparisons in Euclidean space
Because  of this, our kNN kernel density estimation is a more efficient and effective post-processing procedure than the similar kNN mean shift clustering and lazy learning in [NN]
 N.N
Effect of retrieval reference database  One advantage of retrieval approach is that we can simply index more examples to improve the performance
To  that end we collect another NN million GPS-tagged images  from the YFCCN00m dataset [NN], increasing our database  size to a total of NN million images
As shown in table N (last  row), this results in better performance of [L]kNN,σ=N on  the ImNGPS test set
 We vary the reference retrieval database (ImNGPS-N millions images, YFCC-NN millions and the combined NN million) and show the geolocalization accuracy in table N
The  performance when using YFCCNNm is actually no better  than when simply using ImNGPS; though the combined  database of NN million images result in an improvement
 We attribute this to the fact that the IMNGPS test set and the  IMNGPS database come from the same distribution, which  makes IMNGPS more useful for referencing
To quantify  this, we measure the percentage of IMNGPS images among  the top N, N0, N00, N000 nearest neighbors result, they are  NN.N%, N0.N%, NN.N% and N0.N% respectively, which is  quite high given that IMNGPS only constitutes NN.N% of the  combined database
 Similar to result on ImNGPSNk and YFCCNk, we can  change σ to optimize the accuracy at a localization level  (at the expense of the others)
If the system is allowed to  produce different outputs at different levels, this further outperforms the result in Table N
 Table N
Performance on ImNGPS test set
(Human* performance  is average from N0 mturk workers over NN0 trials)  Street City Region Country Cont
 Threshold (km) N NN N00 NN0 NN00  Human* N.N NN.N NN.N  ImNGPS [N0] NN.0 NN.0 NN.0 NN.0  ImNGPS [NN] 0N.N NN.N NN.N NN.N NN.N  PlaNet [NN] 0N.N NN.N NN.N NN.N NN.N  [L] N0NNC 0N.N NN.N NN.N NN.N NN.N  [L] kNN, σ=N NN.N NN.N NN.N NN.N NN.N  ..
NNm database NN.N NN.N NN.N NN.N NN.N  Table N
Performance on ImNGPS test set based on different retrieval reference database
Retrieval Database Stre
City Reg
Cou
Cont
 [L] NN  ImNGPS NN.N NN.N N0.N NN.N NN.N  YFCCNNm NN.N N0.N NN.N NN.N NN.N  Both(NNm) NN.N NN.N N0.N NN.N N0.N  [L] kNN  σ = N  ImNGPS NN.N NN.N NN.N NN.N N0.0  YFCCNNm NN.N NN.N NN.N NN.N NN.N  Both(NNm) NN.N NN.N NN.0 NN.0 NN.N  [L] kNN  σ = N  ImNGPS NN.N NN.N NN.N NN.N NN.N  YFCCNNm NN.N NN.N NN.N NN.N N0.0  Both(NNm) NN.N NN.N NN.N NN.N NN.N  [L] kNN  σ = NN  ImNGPS N0.N NN.N NN.N NN.N NN.N  YFCCNNm N.N NN.N NN.N NN.N NN.N  Both(NNm) NN.N NN.N NN.N N0.N NN.N  N
Conclusion  We presented a deep learning study on image geolocalization, where we experimented with several settings of image classification and image retrieval approaches adapted to  this task
We do not claim technical novelty for any components of this study
Our approaches are relatively simple  yet achieve state-of-the-art accuracy
In the end, the best  performing models can efficiently and accurately localize  at coarse level using classification, and if needed can search  for instance matches using retrieval techniques
 The main goal of this paper is to investigate the effectiveness of deep learning methods for geolocalization
With  the newly obtained insights, we think the following lines of  future work would be important: (N) we have shown the dependency between partitioning scheme and geolocalization  accuracy, which begs the question: what is the best way to  partition and how can the partitioning be optimized given  a particular error threshold? (N) Are GPS labels too weak  a supervision for traditional deep distance metric learning?  There is likely an opportunity for better weakly supervised  DML to improve the geolocalization
 Acknowledgements: the authors were partially supported by a NSF Grant (IIS-NNNNNNN) and NSF CAREER  award NNNNNNN to James Hays
 NNNN    References  [N] H
Altwaijry, E
Trulls, J
Hays, P
Fua, and S
Belongie
 Learning to match aerial images with deep attentive architectures
In Proceedings of the IEEE Conference on Computer  Vision and Pattern Recognition, N0NN
 [N] R
Arandjelovic, P
Gronat, A
Torii, T
Pajdla, and J
Sivic
 Netvlad: Cnn architecture for weakly supervised place  recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NN0N,  N0NN
 [N] M
Bansal, K
Daniilidis, and H
Sawhney
Ultra-wide baseline facade matching for geo-localization
In Computer  Vision–ECCV N0NN
Workshops and Demonstrations, pages  NNN–NNN
Springer, N0NN
 [N] S
Bell and K
Bala
Learning visual similarity for product  design with convolutional neural networks
ACM Transactions on Graphics (TOG), NN(N):NN, N0NN
 [N] G
Chechik, V
Sharma, U
Shalit, and S
Bengio
Large  scale online learning of image similarity through ranking
 The Journal of Machine Learning Research, NN:NN0N–NNNN,  N0N0
 [N] J
Choi, C
Hauff, O
Van Laere, and B
Thomee
The placing  task at mediaeval N0NN
In MediaEval N0NN, Wurzen, Germany, NN-NN September N0NN; Ceur Workshop Proceedings  NNNN, N0NN
CEUR, N0NN
 [N] A
Dubey, N
Naik, D
Parikh, R
Raskar, and C
A
Hidalgo
 Deep learning the city: Quantifying urban perception at a  global scale
In European Conference on Computer Vision,  pages NNN–NNN
Springer, N0NN
 [N] A
Gordo, J
Almazán, J
Revaud, and D
Larlus
Deep image  retrieval: Learning global representations for image search
 In European Conference on Computer Vision, pages NNN–  NNN
Springer, N0NN
 [N] R
Hadsell, S
Chopra, and Y
LeCun
Dimensionality reduction by learning an invariant mapping
In Computer vision  and pattern recognition, N00N IEEE computer society conference on, volume N, pages NNNN–NNNN
IEEE, N00N
 [N0] J
Hays, A
Efros, et al
ImNgps: estimating geographic information from a single image
In Computer Vision and Pattern  Recognition, N00N
CVPR N00N
IEEE Conference on, pages  N–N
IEEE, N00N
 [NN] J
Hays and A
A
Efros
Large-scale image geolocalization
 In Multimodal Location Estimation of Videos and Images,  pages NN–NN
Springer, N0NN
 [NN] J
Huang, R
S
Feris, Q
Chen, and S
Yan
Cross-domain image retrieval with a dual attribute-aware ranking network
In  Proceedings of the IEEE International Conference on Computer Vision, pages N0NN–N0N0, N0NN
 [NN] H
Jegou, M
Douze, and C
Schmid
Hamming embedding  and weak geometric consistency for large scale image search
 In European conference on computer vision, pages N0N–NNN
 Springer, N00N
 [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in neural information processing systems, pages  N0NN–NN0N, N0NN
 [NN] S
Lee, H
Zhang, and D
J
Crandall
Predicting geoinformative attributes in large-scale image collections using  convolutional neural networks
In Applications of Computer  Vision (WACV), N0NN IEEE Winter Conference on, pages  NN0–NNN
IEEE, N0NN
 [NN] Y
Li, D
J
Crandall, and D
P
Huttenlocher
Landmark  classification in large-scale image collections
In Computer  vision, N00N IEEE NNth international conference on, pages  NNNN–NNNN
IEEE, N00N
 [NN] T.-Y
Lin, S
Belongie, and J
Hays
Cross-view image geolocalization
In Computer Vision and Pattern Recognition  (CVPR), N0NN IEEE Conference on, pages NNN–NNN
IEEE,  N0NN
 [NN] T.-Y
Lin, Y
Cui, S
Belongie, and J
Hays
Learning deep  representations for ground-to-aerial geolocalization
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages N00N–N0NN, N0NN
 [NN] M
Norouzi, D
J
Fleet, and R
R
Salakhutdinov
Hamming  distance metric learning
In Advances in neural information  processing systems, pages N0NN–N0NN, N0NN
 [N0] H
Oh Song, Y
Xiang, S
Jegelka, and S
Savarese
Deep  metric learning via lifted structured feature embedding
In  The IEEE Conference on Computer Vision and Pattern  Recognition (CVPR), June N0NN
 [NN] J
Philbin, O
Chum, M
Isard, J
Sivic, and A
Zisserman
Object retrieval with large vocabularies and fast spatial matching
In Computer Vision and Pattern Recognition, N00N
CVPR’0N
IEEE Conference on, pages N–N
IEEE,  N00N
 [NN] J
Philbin, O
Chum, M
Isard, J
Sivic, and A
Zisserman
 Lost in quantization: Improving particular object retrieval in  large scale image databases
In Computer Vision and Pattern  Recognition, N00N
CVPR N00N
IEEE Conference on, pages  N–N
IEEE, N00N
 [NN] F
Radenović, G
Tolias, and O
Chum
Cnn image retrieval  learns from bow: Unsupervised fine-tuning with hard examples
In European Conference on Computer Vision, pages  N–N0
Springer, N0NN
 [NN] O
Rippel, M
Paluri, P
Dollar, and L
Bourdev
Metric  learning with adaptive density discrimination
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
 [NN] P
Sangkloy, N
Burnell, C
Ham, and J
Hays
The sketchy  database: learning to retrieve badly drawn bunnies
ACM  Transactions on Graphics (TOG), NN(N):NNN, N0NN
 [NN] F
Schroff, D
Kalenichenko, and J
Philbin
Facenet: A unified embedding for face recognition and clustering
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNN–NNN, N0NN
 [NN] Q
Shan, C
Wu, B
Curless, Y
Furukawa, C
Hernandez, and  S
M
Seitz
Accurate geo-registration by ground-to-aerial  image matching
In ND Vision (NDV), N0NN Nnd International  Conference on, volume N, pages NNN–NNN
IEEE, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
 [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 NNNN    Going deeper with convolutions
In Computer Vision and  Pattern Recognition (CVPR), N0NN
 [N0] C
Szegedy, V
Vanhoucke, S
Ioffe, J
Shlens, and Z
Wojna
 Rethinking the inception architecture for computer vision
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
 [NN] Y
Taigman, M
Yang, M
Ranzato, and L
Wolf
Deepface:  Closing the gap to human-level performance in face verification
In Computer Vision and Pattern Recognition (CVPR),  N0NN IEEE Conference on, pages NN0N–NN0N
IEEE, N0NN
 [NN] B
Thomee, D
A
Shamma, G
Friedland, B
Elizalde, K
Ni,  D
Poland, D
Borth, and L.-J
Li
YfccN00m: The new  data in multimedia research
Communications of the ACM,  NN(N):NN–NN, N0NN
 [NN] G
Tolias, R
Sicre, and H
Jégou
Particular object retrieval  with integral max-pooling of cnn activations
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
 [NN] N
N
Vo and J
Hays
Localizing and orienting street views  using overhead imagery
In European Conference on Computer Vision, pages NNN–N0N
Springer, N0NN
 [NN] J
Wang, Y
Song, T
Leung, C
Rosenberg, J
Wang,  J
Philbin, B
Chen, and Y
Wu
Learning fine-grained image  similarity with deep ranking
In Computer Vision and Pattern Recognition (CVPR), N0NN IEEE Conference on, pages  NNNN–NNNN
IEEE, N0NN
 [NN] T
Weyand, I
Kostrikov, and J
Philbin
Planet-photo geolocation with convolutional neural networks
In European  Conference on Computer Vision, pages NN–NN
Springer,  N0NN
 [NN] S
Workman, R
Souvenir, and N
Jacobs
Wide-area image  geolocalization with aerial reference imagery
In ICCV N0NN,  N0NN
 [NN] A
R
Zamir, A
Hakeem, L
Van Gool, M
Shah, and  R
Szeliski
Large-scale visual geo-localization
Advances  in computer vision and pattern recognition (, N0NN
 [NN] A
R
Zamir and M
Shah
Accurate image localization based  on google maps street view
In Computer Vision–ECCV  N0N0, pages NNN–NNN
Springer, N0N0
 [N0] B
Zhou, A
Lapedriza, J
Xiao, A
Torralba, and A
Oliva
 Learning deep features for scene recognition using places  database
In Advances in Neural Information Processing Systems, pages NNN–NNN, N0NN
 NNN0Point Set Registration With Global-Local Correspondence and Transformation Estimation   Point Set Registration with Global-local Correspondence and Transformation  Estimation  Su ZhangN,N,N, Yang YangN,N,N,∗, Kun YangN,N,∗, Yi LuoN,N, Sim Heng OngN  NSchool of Information Science and Technology, Yunnan Normal University NThe Engineering Research Center of GIS Technology in Western China, Ministry of Education, China  NLaboratory of Pattern Recognition and Artificial Intelligence, Yunnan Normal University NDepartment of Electrical and Computer Engineering, National University of Singapore  ∗Yang Yang and Kun Yang are joint corresponding authors
∗Email: {yyang ynu,kmdcynu}@NNN.com  Abstract  We present a new point set registration method with  global-local correspondence and transformation estimation  (GL-CATE)
The geometric structures of point sets are exploited by combining the global feature, the point-to-point  Euclidean distance, with the local feature, the shape distance (SD) which is based on the histograms generated by  an elliptical Gaussian soft count strategy
By using a bidirectional deterministic annealing scheme to directly control the searching ranges of the two features, the mixturefeature Gaussian mixture model (MGMM) is constructed to  recover the correspondences of point sets
A new vector  based structure constraint term is formulated to regularize  the transformation
The accuracy of transformation updating is improved by constraining spatial structure at both  global and local scales
An annealing scheme is applied  to progressively decrease the strength of the regularization  and to achieve the maximum overlap
Both of the aforementioned processes are incorporated in the EM algorithm,  a unified optimization framework
We test the performances  of our GL-CATE in contour registration, sequence images,  real images, medical images, fingerprint images and remote sensing images, and compare with eight state-of-theart methods where our GL-CATE shows favorable performances in most scenarios
 N
Introduction  Non-rigid point set registration is an essential problem  which constantly draws interest in various fields such as remote sensing, medical image registration, template matching for hand-written characters and fingerprint identification
TPSRPM [N], RPM-LNS[NN], CPD [NN, NN], GMMREG [N], LNE-RPM [NN, NN], GLMDTPS [NN], MoAGREG  [N0] and PR-GLS [NN] are some of the outstanding works
 In this section, we briefly review these methods, and then  summarize from three aspects including: (i) energy optimization framework, (ii) fuzziness of correspondence and  (iii) spatial constraint, followed by the outline of our GLCATE
 The key idea of TPSRPM [N] is to assume that each  source point corresponds to a weighted sum of the target points
The weights are taken from a corresponding  matrix whose entries are proportional to a Gaussian function of the pairwise Euclidean distances between the source  and target point sets
The transformation is updated using  thin-plate splines (TPS) [N, NN]
Zheng et al
[NN] proposed a robust point matching by preserving local neighborhood structures (RPM-LNS) for non-rigid shape registration based on the graph theory
RPM-LNS employs the  shape context (SC) descriptor [N, NN] to initialize the graph  matching, the optimal match between two graphs is the one  that maximizes the number of matched edges
CPD [NN, NN]  takes the source and target point sets as the centroid of components and data, respectively, and the registration is interpreted to a maximum likelihood estimation problem
The  elegant expectation maximization (EM) algorithm framework [N, NN] is employed for parametric estimation
The  transformation is modeled using Gaussian radial basis function (GRBF) and the motion coherence theory [NN] is used  to regularize the displacement field between the point sets
 GMMREG [N] extends the idea of registration from fitting  Gaussian mixtures to data to aligning two Gaussian mixture models (GMM)
And the LN distance is used to mea- sure the discrepancy of two Gaussian mixtures instead of  the log-likelihood function
RPM-LNE [NN, NN] introduces  the LN minimizing estimate (LNE) [N, NN] which is a ro- bust estimator in statistics to estimate the transformation
 The source point set and the estimated corresponding point  NNNNN    which is obtained by matching the SC descriptor are represented by two multi-dimensional normal distributions to  be fitted
GLMDTPS [NN] presents a mixture-feature based  correspondence estimation method named as global and local mixture distance (GLMD)
The global distance is the  point-to-point Euclidean distance, and the local distance is  obtained by summing the squared Euclidean distance between the ith neighboring points according to the index
These two distances are combined to form a GLMD based  cost matrix for correspondences estimation
MoAGREG  [N0] uses asymmetric Gaussian distribution [N0] to represent each point set, it updates correspondences and transformations under the framework of TPSRPM
PR-GLS [NN]  acquires a binary corresponding matrix by matching the  SC descriptors, this matrix is then used to improve CPD  through directly assigning the membership probabilities of  GMMs to close to one, if matched, or to close to zero, otherwise
 TPSRPM employs robust point matching (RPM) algorithm for energy optimization, it essentially involves a  dual update process embedded within an annealing scheme,  which is quite similar to the EM algorithm adopted by  CPD and PR-GLS
RPM-LNS proceed by finding the  matches that maximizes the number of matched edges,  while GLMDTPS optimizes the GLMD cost matrix using the Jonker-Volgenant algorithm [N, N0]
For GMMREG, RPM-LNE and MoAGREG, they optimize energy by  minimizing the discrepancy of two distributions
Overall,  TPSRPM, RPM-LNS, CPD, GMMREG, RPM-LNE, MoAGREG, the applicability of these methods is limited by the  single feature based correspondence estimations
For example, if two target points share the same Euclidean distance to  a source point, we obtain equal correspondences, although  the local structures are probably totally different
And for  robust shape context or graph features, however, is unfavored by the assumption that the corresponding points have  similar neighborhood structures [N]
Moreover, the rotation  invariant shape context used in RPM-LNS, RPM-LNE and  PR-GLS can be greatly deteriorated since this property requires the center of mass to be stable
GLMDTPS and PRGLS employ both the global and local features
However,  since outliers are not modeled, GLMDTPS is sensitive to  outliers
And PR-GLS actually divides the original single  optimization process into two, which are the linear assign  problem [NN, NN] of the shape context and the EM algorithm
This inconsistency may cause a decrease in performance, especially, when handling large amount of points
 Based on the assumption that each source point corresponds to a weighted sum of the target points, TPSRPM  and CPD estimate the one-to-many fuzzy correspondences  by the Euclidean distance based probability, since the magnitude of the searching range parameter goes from large to  small, it is a global-to-local registration strategy
At a very  large searching range, the estimated corresponding point set  is essentially very close to the center of mass of the target  point set, which leads the source point set to collapse at the  beginning, and to expand as the searching range decreases,  thus they require relatively more iterations
Particularly, the  center of mass changes relatively slightly when the target  point set is heavily rotated, which results in a bad initial  pose and finally a large deviation result
On the other hand,  enforcing a one-to-one correspondence using binary matrix  [NN, NN, NN] is vulnerable to the presence of noise and outliers
 Methods [NN, NN, NN, NN] using GRBF constrain their  spatial transformation with the motion coherence theory  [NN]
In TPS, a term in the form of the space integral of  the square of the second order derivatives which reflects  the prior knowledge is also included for the same purpose  [N, NN, NN]
Intuitively, these regularizations discourage  mappings which are too arbitrary by forcing points to move  coherently at global scale
However, they produce position  deviations of the rest of the points when one point is mismatched, and may also be undesirable when source points  need to be moved in different directions to match their target  points at the same time [NN]
 In this paper, we present a new point set registration  method with global-local correspondence and transformation estimation (GL-CATE)
The pairwise Euclidean distance and shape distance (SD) are used as the global and  local features, respectively
The SD which is based on the  histograms generated by an elliptical Gaussian soft count  strategy can quantify similar neighborhood structures
By  the help of the SD, the constructed mixture-feature Gaussian mixture model (MGMM) obtains a sufficiently good  initial pose and reliable correspondence estimation
The  searching ranges of the two features are directly controlled  by a bi-directional deterministic annealing scheme, which  interchanges the statuses of the two features and leads a  local-to-global registration strategy
The EM algorithm, a  unified optimization framework is used to estimate the parameters of the MGMM
In E-step, the posterior probability  matrices are obtained by measuring the similarities of the  designed mixture-feature using Bayes’ rule
In M-step, we  minimize the expectation of the negative objective function  in reproducing kernel Hilbert space (RKHS)
A new vector based structure constraint term is used to regularize the  transformation which complements the global coherence
 The accuracy of transformation estimation is improved by  constraining spatial structure at both global and local scales
 An annealing scheme is applied to progressively decrease  the strength of the regularization and to achieve the maximum overlap
We test the performances of our GL-CATE  in contour registration, sequence images, real images, medical images, fingerprint images and remote sensing images,  and compare with eight state-of-the-art methods where our  NNN0    GL-CATE shows favorable performances in most scenarios
 N
Method  We denote the set of M source points by YM×D = {yN,yN, ...,yM}  T , and a set of N target points by XN×D = {xN,xN, ...,xN}  T 
The goal is to recover the  unknown non-rigid transformation T (Y,φ) registering Y to X with maximum point-wise overlap, where φ is a set of  parameters
Based on the reasonable assumption that points  from one set are normally distributed around points belonging to the other set [NN], aligning Y onto X is considered as  fitting M Gaussian components to N data, where achieving the maximum point-wise overlap is therefore taken as minimizing the negative GMM log-likelihood function
Let ym be the centroid of the mth component, xn the n  th data
The  probability density function is obtained as:  p(xn) = (N− ζ) M ∑  m=N  Cmnf(xn|ym) + ζ N  N , (N)  where Cmn is non-negative quantity with ∑M  m=N Cmn = N, which is called the component densities of the mixture
NN is an additional uniform distribution with a weighting parameter ζ, 0 ≤ ζ ≤ N for outlier dealing
Based on the EM algorithm for GMM based clustering, we compute the  posterior probability (E-step) as:  pmn = Cmnf(xn|ym)  ∑M j=N Cjnf(xn|yj) + ζ  N N  , (N)  the matrix PM×N can be regarded as the correspondence  matrix, whereby X̂ = PX, the weighted sums of X, or to be more specifically, the putative target set, is obtained
 The new mixture model parameters are found by minimizing (M-step):  Q = −  N ∑  n=N  M+N ∑  m=N  pmn log (Cmnf(xn|ym)) +R(T ), (N)  where the first term is the expectation of the negative loglikelihood function of GMMs
The non-rigid transformation is formulated as T (Y,φ) = Y + V(Y), which is a kernel-based displacement function derived by using calculus of variation [NN]
To prevent the ill-posed problem  caused by T which is not unique, the second termR(T ) is therefore employed for regularization
The EM algorithm  proceeds iteratively by alternating between E-step and Mstep until convergence
In this paper, the density function  f(xn|ym) is specified based on a new finite mixture model, named mixture-feature Gaussian mixture model (MGMM)
 Meanwhile, the regularization term R(T ) is formulated at both global and local scales
 N.N
Mixture-feature Gaussian Mixture Model  For our MGMM, the density function f(xn|ym) is de- fined as:  f(xn|ym) = N  Nπσβ exp  [  −  (  ∆G  NσN +  ∆L  NβN  )]  , (N)  where ∆G and ∆L denote the similarity measures using global and local features, respectively
σN ∈ (0, N) and βN ∈ (0, N) are covariances
Note that equal isotropic co- variances σNI, βNI and component densities Cmn =  N M are  applied for all MGMM components, where I is a D × D identity matrix
 The underlying assumption of the density function (N) is  the decomposition of the process for human to recognize  and categorize objects
Supposing such process is based on  the linear combination among features such as Euclidean  distance and density, etc
The priority of certain feature  may change during the process
For instance, one can easily categorize different letters according to the feature of  shape at the very beginning, whereafter the accuracy can be  further optimized by involving other features in
Inspired  by these facts, the bi-directional deterministic annealing  scheme is employed to gradually interchange the priorities  of the global and local feature discrepancies during the registration, which is equivalent to enhancing the robustness of  the MGMM by directly controlling the fuzziness of the correspondence [N]
We define the temperature parameter as  T = − τl , where τ is the current iteration number, and l is a  constant
σN and βN are obtained by σN = eT and βN = e N  T  in each iteration
 Iter=N0Iter=N Iter=N0 Iter=N0 Iter=N00  Iter=N Iter=N0 Iter=N0 Iter=N0 Iter=NN  S in g le  M ix tu re  Fig
N
The comparison on the registration processes of our GLCATE using single feature (upper row) against its mixture-feature  counterpart (lower row)
Red asterisks: the target point set X
 Green circles: the estimated corresponding point set X̂
Blue  crosses: the source point set Y
 The advantage of MGMM can be demonstrated by comparing the registration processes of our GL-CATE using ∆G  only against its mixture-feature counterpart, as shown in  Fig
N
In this test, ∆G is the point-to-point squared Eu- clidean distance, ∆L is the squared shape distance which we will detail later
At the first iteration, we can see that  by using ∆G, the initial estimated coordinates (denoted by green circles) are the regional center of masses, while they  are close to the real target coordinates (denoted by red asterisks) when using ∆G and ∆L
This helps us recover a good  NNNN    initial pose for aligning the two point sets
At the N0th it- eration, the estimated coordinates based on single feature  discrepancy are still regional center of masses
By contrast, the counterpart estimates a preciser putative target by  which a reliable source point set (denoted by blue pluses)  is obtained
Finally, registration using single feature yields  large point-to-point deviations, even if the iteration number  is larger
 In the early stage of iterations, though the source point  set Y and the target point set X have the biggest difference,  the local feature can still be very strong and stable, and the  correspondence which is estimated based on the local feature is therefore more reliable
This greatly facilitates the  registration process at the very beginning
Other probability  based methods (e.g., TPSRPM) can only yield a shrunken  source point set near the regional center of masses
In our  MGMM, since we have σN ≈ N and βN ≈ 0, ∆ L  NβN tends to be  relatively large, thus the unreliable global correspondence  is filtered due to the property of negative nature exponential function
At the final stage of iterations, Y and X are  very similar, a direct estimation using the global feature is  desirable
And the statuses of the two features interchange  exactly since σN ≈ 0 and βN ≈ N, which means that the correspondence is mostly determined by the global feature
 N.N
MGMM based Correspondence Estimation  Typically, for point set registration, the original Mahalanobis distance which is a measure of distance between a  point and a distribution is simplified to the Euclidean distance [N]
And in our study, we adopt the point-to-point Euclidean distance ∆Gmn = ‖xn − T (ym,φ)‖ N as the global  feature
Based on the work in [N, NN], the shape distance  smn = ∆ L mn =  Rad ∑  r=N  Tan ∑  t=N  ∥  ∥cXn (r, t)− c Y m(r, t)  ∥  ∥  N (N)  is defined as the local feature, where c(r, t) is the count of points within the rt th bin, and Rad and Tan denote the number of bins in radial and tangential directions, respectively
However, when points lie close to the boundaries of  bins, they may be assigned to different bins and yield biased  histograms
And, non-rigid deformations occur in both radial and tangential directions, a soft count strategy which  accordingly specifies the range and direction is more reasonable
Motivated by these facts, we present a new count  strategy based on the elliptical Gaussian (EG) as:  c(r, t) =            EG(µrad, µtan,Λrad,Λtan), if (r, t) are within  A and B  0
otherwise  ,  (N)  where (µrad, µtan) is the coordinate of the EG centroid, B is the polar coordinate with Rad × Tan bins
We respec- tively denote by ρ and θ the influence range in the radial  and tangential direction
The influence area A centered at (µrad, µtan) is rectangle with lengths Nρ + N and Nθ + N
Hence, the original binary count assigned at a single bin  becomes a soft count assigned within A
By choosing ap- propriate magnitudes for the two variances Λrad and Λtan, the EG weighting function can accurately quantify the nonrigid deformation
Table N shows the shape distances of two  point sets using different count strategies
When θ = 0, it is actually the original strategy that can not distinguish any  of the five manners since the shape distances are all equal
 The count by θ = N distributes the best because the dif- ference between adjacent columns are showing a progressively increasing tendency
This property makes the count  be capable of identifying the similar neighborhood structures
A direct extension for ND case can be realized by  using NDSC [N] and the ellipsoidal Gaussian soft technique,  which spreads the counts alone the azimuth, elevation and  radial directions of the spherical coordinates
Since ND case  is the focus of this paper, we leave it to our future research
 Table N
The shape distances of five manners using different count  strategies
 θ = 0 0 N N N N θ = N 0 N.0NNN N.NN0N N.NNNN N.NNNN θ = N 0 0.N0NN N.NNNN N.NNN0 N.NNNN θ = N 0 0.NN0N N.N0NN N.NNN0 N.NNNN  Substituting the Euclidean distance and the elliptical  Gaussian soft shape distance (EGSSD) into (N), we can  therefore rewrite (N) in the complete form as:  f(xn|ym) = N  Nπσβ exp  [  −  (  ‖xn − T (ym,φ)‖ N  NσN +  smn NβN  )]  
 (N)  And the posterior probability function (N) can be rewritten  as:  pmn = exp  [  − (  ‖xn−T (ym,φ)‖ N  NσN + smn NβN  )]  ∑M j=N exp  [  − (  ‖xn−T (yj ,φ)‖N  NσN + sjn NβN  )]  + g ,  (N)  where g = NMζπσβN(N−ζ) 
 N.N
Global-local Structure Constraint (GLSC)  Once non-rigidity is allowed, there are an infinite number of ways to map one point set onto another
The ill-posed  problem is prevented by termR(T ) of objective (N)
In our study, the regularization is defined as:  R(T ) = λ  N G(T ) +  η  N L(T ), (N)  NNNN    where parameters λ and η control the strength of regulariza- tions, G(T ) = ‖T ‖N is the global structure constraint term following the motion coherence theory [NN], and  L(T ) =  M ∑  m=N  ‖E(x̂m)− E(T (ym,φ))‖ N (N0)  is the local structure constraint (LSC) term based on the  local structure descriptor (LSD) E(·)
The extraction of the LSD is illustrated in Fig
N
Given a set ZM×D = {zN, zN, ..., zM}  T of M points, let {zik} K k=N be the K nearest neighbors (KNN) of zi, {uik} K k=N the set with each entry  denoting the vector −−−→zizik, we have E(zi) = ∑K  k=N hikuik, where hik = exp(−‖uik‖  N/νNi ) is the weight that controls the contribution of uik to E(zi), and νi is the variance of {‖uik‖}  K k=N
L(T ) exploits the local structural discrepancies between the putative target X̂ and the source T (Y,φ)
Minimizing discrepancies L(T ) is equivalent to levering the LSDs of T (Y,φ) and therefore forcing each neighbor- ing point set of T (Y,φ) to align onto the corresponding one
A vivid demonstration is shown in Fig
N
 zi  (a)  {zik} K k=N  (b) (c)  {uik} K k=N  (d)  E(zi)  Fig
N
Extraction of the local structure descriptor
(a): Selecting one point (colored in red), the goal is to obtain its LSD
(b):  Finding its K nearest neighbors (colored in green)
(c): Obtaining  the respective vectors −−−→zizik
(d): Computing the weighted vector sum
We set the weights h all equal to N and K = N for a brief demonstration
This figure is related to Fig
N
 In addition, a deterministic annealing scheme is also applied to the trade-off parameters λ and η
The annealing pa- rameter is defined as κ = (τNmax − τ  N + N)N/N/τmax where τmax is the maximum iteration number
The parameter κ remains stable in most iterations, then sharply decreasing  to zero in the last several iterations
This implies that the  constraint (N) is released at the final stage of iterations for  achieving the maximization of the point-wise overlap
 N.N
GLSC based Transformation Estimation  Inspired by the Riesz representation theorem [NN], we  model the non-rigid transformation T by lying it within the  reproducing kernel Hilbert space (RKHS)
We first define  a RKHS H by choosing a positive definite kernel, here we adopt the Gaussian radial basis function (GRBF)
With the  constant ǫ controlling the spatial smoothness, the kernel can be written in the form Θ(yi,yj) = exp(−  N NǫN ‖yi − yj‖  N)  (a) (b) E(zi)  E(z′i)  L(T )i  (c) (d)  Fig
N
The demonstration of how the LSC works
(a): Given a  point set (colored in green) and its rotated form (colored in blue),  the goal is to achieve maximum point-wise overlap between the  k th nearest neighbors of the center point (colored in red), as shown  in (d)
(b): Extracting the LSDs for the green and blue sets, the dotted black line denotes the local structural discrepancy
(c): Taking  derivative of L(T )i is equivalent to exerting an imaginary force on the LSD of the blue set
(d): The two sets are aligned
This  figure is related to Fig
N
 and easily generalized to three or higher dimensions
The  optimal transformation function T takes the form as:  T (Y,W) = Y +ΘW, (NN)  where WM×D is the coefficient matrix
Hence, the minimization over Q boils down to finding a finite coefficient matrix W
The objective function (N) can be rewritten in a  matrix form as:  Q(W,σN, βN) = N  NσN  N ∑  n=N  M ∑  m=N  pmn‖Xn − (Y +ΘW)m‖ N  + λ  N tr(WTΘW) +  η  N tr(RRT ) +NP lnσ  NβN,  (NN)  where R = (HX̂ − KI)X̂ − (HY − KI)(Y + ΘW), I  is of size M × M , and NP = ∑N  n=N  ∑M m=N pmn ≤ N  (with NP = N only if ζ = 0)
Note that we omit term N  NβN  ∑N n=N  ∑M m=N pmnsmn in (NN) since it is independent  to W
HZ is of M × M dimension with each non-zero entry hij = exp  (  −‖−−→zizj‖ N/νNi  )  , if zj ∈ {zik} K k=N
Intuitively, the weighting matrix HZ is analogous to a permutation matrix, by which the weighted LSDs of point set Z can  be summed if multiplying HZ by Z
 Taking partial derivative of (NN) with respect to W, we  obtain the coefficient matrix W as:  W = (  d(PN)Θ+ λσNI+ ησNBTBΘ )−N  (  PX− d(PN)Y + ησNBTA )  , (NN)  where A = (HX̂−KI)X̂−(HY−KI)Y, B = HY−KI and d(·) denotes the diagonal of a matrix
So far all the pa- rameters have been solved
The new location of the source  point set is updated by (NN), after which we return to correspondence estimation and continue the registration process  until the maximum iteration number τmax is reached
 NNNN    N
Parametric Setting and Complexity Analysis  The expressions of the point sets on the coordinate system affect the performance of point set registration methods
Thus an initial normalization process is involved in  our GL-CATE to rescale the two point sets Y and X to have  zero means and unit variances
In each iteration, reasonable  magnitude for βN is obtained by normalizing the counts to  have unit length as ∑Rad  r=N  ∑Tan t=N c(r, t)  N = N, normalization pmn ← pmn/ ∑M  j=N pmj is also required for the computation of the estimated corresponding point set X̂ = PX
 A v e ra  g e  E  rr o r  0 N0 N0 N0 N0 N0 N0 N0 0  0.0N  0.0N  0.0N  0.0N  0.N  Deformation: N  Noise: 0.0N  Outlier: N.0  Rotation: NN  Rotation NN + Deformation N  Rotation NN + Deformation N + Noise N  Iteration Number  Fig
N
Convergence experiment on synthesized dataset under the  largest degree of six degradation categories
 Experiments show that our GL-CATE will catch a good  stable solution after about N0 iterations, as shown in Fig
 N
We set τmax = N0 and l = N
Thus σ N init = e  −N/N = 0.NNNN, σNfinal = e  −N0/N ≈ N.NNN×N0−N, βNinit = e −N/N =  0.00NN, βNfinal = e −N/N0 ≈ 0.NN00
We find that when the  target point set is contaminated by noise, the performance  can be improved by computing σN as:  σN = tr(XT d(PTN)X)− Ntr(VTPX) + tr(VT d(PN)V)  NNP (NN)  instead of using σN = eT , where V = T (Y,W)
In noise scenario, precise alignment is not preferred, and registration  process using (NN) is similar to the linear least-squares fitting, which makes more sense
We set ζ = 0.N for noise and outlier scenarios, otherwise ζ = 0.N, and covariances σN are computed by (NN) in all noise contaminated scenarios
 A v e ra  g e  E  rr o r  A v e ra  g e  E  rr o r  Rotation + Deformation N  A v e ra  g e  E  rr o r  A v e ra  g e  E  rr o r  Rotation + Deformation N  Rotation + Deformation N + Noise N  Rotation + Deformation N + Noise N  Fig
N
Experiments on synthesized dataset using different EG soft  count strategy (upper row) and LSC smoothness (lower row)
Left  column: RDN scenario
Right column: RDNNN scenario
 Parameters ρ and θ for elliptical Gaussian (EG) weight- ing function, and parameter η for the smoothness of the lo- cal structure constraint (LSC) are tested under two extreme  scenarios, as shown in Fig
N
The polar coordinate B is set to N × N0, each bin in tangential direction stands for  NN◦
Variances of EG function are set as Λrad = ρN and Λtan = θN
When the degree of rotation is smaller than N0◦, setting ρ = N, θ = N performs better than setting ρ = N, θ = N, while the opposite occurs under larger de- gree
Therefore, we set ρ = N and θ = N for scenarios with rotation, and both to N for others
The number of the nearest neighboring points K is set to N to distinguish between a corner (includes two neighboring points) and a cross (includes  four neighboring points) [NN]
The regularization parameters λ and η are both set to N
ǫ determines the width of the range of the interaction between samples, we empirically  set ǫ to N
The pseudo-code of our GL-CATE is shown in Algorithm N
 Algorithm N: The GL-CATE Method  input : Two point sets Y and X  output : Transformed point set Y  N Construct the kernel matrix Θ;  N Initialization: W = 0; λ = η = N; ζ = 0.N under noise and outlier, otherwise ζ = 0.N; K = N;  N Compute the histograms of X by (N);  N while not convergence do  N E-Step:  N Compute the histograms of Y by (N);  N Compute the shape distance between X and  Y by (N);  N Compute the posterior probability matrix P  by (N);  N Compute temperature parameter T = − τl ;  N0 Compute κ = (τNmax−τ  N+N)N/N  τmax ;  NN M-Step:  NN Compute W by (NN);  NN Update the source point set Y = Y +ΘW; NN Update covariances σN by (NN) under noise,  otherwise by σN = eT ;  NN Update covariances βN = e N  T ;  NN Anneal λ = κλ and η = κη;  NN end  NN The transformed source point set Y is obtained in  the final iteration
 For our GL-CATE, it takes O((CM)N) to obtain the shape distances for M points by using the soft count strat- egy, where C = (Nθ + N)(Nρ + N)
The K nearest neigh- bors are obtained by K operations of sequential search with O(KM) complexity for one point
The heat kernel H requires O(KMN) to compute
The derivative (NN) is of O(MN) due to the existence of M ×M kernel Θ
Over- all, the computational complexity of GL-CATE is O(MN), which, as well, can be ulteriorly reduced to O(M) com- plexity by using several well-studied techniques such as  low-rank matrix approximation [NN] and subset of regresNNNN    sors method [NN, NN]
For other methods, TPSRPM, CPD,  GMMREG and MoAGREG require O(MN) work to com- pute the corresponding matrix, where M ≤ N 
Instead, to solve the linear assignment problem based on the N × N dummy cost matrix, RPM-LNE, GLMDTPS and PR-GLS  need O(NN) work by which a permutation matrix is ob- tained
The regular complexity for the radial basis function (e.g., thin-plate spline and Gaussian radial basis function) based transformation estimation step is O(MN), and for CPD, RPM-LNE, MoAGREG and PR-GLS, the fast implementation which has O(M) complexity is presented in their literatures
 N
Experiments  Both the basic and application experiments are carried  out to compare the performance of our GL-CATE against  eight state-of-the-art methods which are TPSRPM[N],  RPM-LNS[NN], CPD[NN], GMMREG[N], RPM-LNE[NN],  GLMDTPS[NN], MoAGREG[N0] and PR-GLS[NN]
The  experiments are implemented in Matlab on a laptop with  N.N0 GHz Intel Core CPU, NN GB RAM
 N.N
Results on Basic Experiments  In the first series of experiments, four synthesized contour data sets, fishN, Chinese character, hand and line, each  of which respectively contains NN, N0N, N0N and N0 points  are used
Four degradation categories, i.e., deformation,  noise, outlier and rotation are used to evaluate the accuracy  and robustness of methods
We follow the experimental settings as in [NN], which are degree of deformation from N to N, noise level from 0.0N to 0.0N, outlier to data ratio from 0.N to N.0, and enlarging the rotation range from ±N0◦ to ±N0◦, with a NN◦ interval
We abbreviate the nth degree as Dn, Nn, On and Rn for convenience of description
Note that DN degradation is contained in noise, outlier and ro- tation scenarios by default
The average performances on  the four data sets are shown in the first row of Fig
N
Our  GL-CATE gives the best performances over DN to DN, NN to NN and ON to ON
In rotation scenario, it yields near- perfect result within ±NN◦ rotation
 In the second series of experiments, four synthesized  contour data sets, butterfly, face, fishN and maple, each of  which respectively contains NNN, NNN, NN and NNN points are  used
Each data is first degraded by either DN or DN+NN, and then rotated by −N0◦ to N0◦ with a N0◦ interval
We abbreviate as RDN and RDNNN, respectively
We also test the average runtime of our GL-CATE on all the six scenarios, using three data sets with representative point numbers
The average performances of methods using four data  sets on RDN and RDNNN, as well as the average runtime are shown in the second row of Fig
N
We see that our  GL-CATE outperforms within ±N0◦ rotation in RDN and RDNNN scenarios
The results of RPM-LNS, RPM-LNE  and PR-GLS using rotation invariant shape context are deteriorated since the mass centers of the data are changed in  such cases
Formally, our GL-CATE takes about N second to register two point sets with M,N = N00 points
 A v e ra  g e  E  rr o r  -N0 -N0 -N0 -N0 -N0 -N0 -N0 -N0 0 N0 N0 N0 N0 N0 N0 N0 N0  0  0.00N  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  A v e ra  g e  E  rr o r  -N0 -N0 -N0 -N0 -N0 -N0 -N0 -N0 0 N0 N0 N0 N0 N0 N0 N0 N0  0  N  N  N  N  N0  NN  NN  NN  NN  N0 #N0-N  Deformation Noise  A v e ra  g e  E  rr o r  Outlier Rotation  Rotation + Deformation N Rotation + Deformation N + Noise N D N O R RDN RDNNN  0  N  N  N  N  N  N  N  N  N  N0 FishN: NN points  Maple: NNN points  Face: NNN points  TPSRPM RPM-LNS CPD GMMREG RPM-LNE GLMDTPS MoAGREG PR-GLS GL-CATE  N N N N N N N N -0.N  0  0.N  N  N.N  N  N.N  N #N0-N  0.0N 0.0N 0.0N 0.0N 0.0N -0.N  0  0.N  N  N.N  N  N.N  N #N0-N  0.N 0.N 0.N 0.N N.0 -N  -N  0  N  N  N  N  N  N #N0-N  -N0 -NN -N0 -NN -N0 -NN 0 NN N0 NN N0 NN N0  0  0.00N  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  A v e ra  g e  E  rr o r  A v e ra  g e  E  rr o r  A v e ra  g e  E  rr o r  A v e ra  g e  R  u n ti m  e  Fig
N
Comparison of our GL-CATE against eight state-of-the-art  methods on eight point sets and showcase of the average runtime  of our GL-CATE
The error bars indicate the standard deviations  of the average errors in N00 random experiments
In each line  graph, methods with representative performances are highlighted
 The stacked bar graph is used to demonstrate the average runtime,  for each bar group, five degrees ranged from the first to the last are  selected for showcase, and the units are in seconds
 Init
Init.TPSRPM TPSRPMCPD CPDGMMREG GMMREGRPM-LNE RPM-LNERPM-LNS RPM-LNSGLMDTPS GLMDTPSMoAGREG MoAGREGPR-GLS PR-GLSGL-CATE GL-CATE  D e fo r m a t io n  N o is e  O u t li e r  R o t a t io n  R D  R D N  Fig
N
Registration examples on fishN, Chinese character, hand,  line, butterfly, face, fishN and maple, with deformation, noise, outliers, rotation, RD and RDN scenarios being shown in every two  rows
The goal is to align the source point set (blue asterisks) onto  the target point set (red circles)
All the six scenarios are at the following degrees: (i) degree of deformation N; (ii) noise level 0.0N; (iii) outlier to data ratio N.0; (iv) degree of rotation NN◦; (v) de- gree of rotation NN◦+ degree of deformation N and (vi) degree of rotation NN◦+ degree of deformation N + noise level 0.0N
 In the third series of experiments, three types of data sets  are used
(i) A dataset from [NN] which consists of N0 pairs  of car images and N0 pairs of motorbike images selected  from Pascal N00N Challenge, each pair contains N0−N0 fea- ture points
(ii) CMU hotel which consist of N0N frames and each frame has N0 labeled landmarks, all possible image  pairs are tested
(iii) IMM which consists of face landmarks  with expression and multi-view changes, each face contains  NN point landmarks with different facial expressions and  poses, N groups of face data are tested
The matching rate  of these three experiments are listed in Table NA
 NNNN    Table N
Quantitative comparisons on multiple experiments
A: The matching rate on cars, motorbikes, CMU hotel and IMM images
B,C  and D: The means and standard deviations of RMSE, MAE and MEE on retinal images, fingerprint images and remote sensing images,  respectively
× denotes that the number of the points is less than the minimum requirement of RPM-LNS
Bold values indicate the best performances
 TPSRPM RPM-LNS CPD GMMREG RPM-LNE GLMDTPS MoAGREG PR-GLS GL-CATE  A  Car N0.NN% × NN.NN% NN.N0% NN.NN% NN.0N% NN.NN% NN.0N% NN.NN% Motorbike NN.NN% × NN.NN% NN.NN% NN.NN% NN.N0% NN.NN% N0.NN% NN.NN% Hotel NN.NN% NN.0N% NN.NN% NN.NN% NN.0N% N0.NN% NN.NN% NN.NN% NN.NN% IMM N0.NN% NN.NN% NN.NN% N0.N0% NN.NN% NN.0N% N0.NN% NN.0N% NN.NN%  B  RMSE N.NN ± N.N0 × N.NN ± N.NN N.NN ± N.NN N.NN ± 0.NN N.NN ± 0.NN N.NN ± N.N0 N.NN ± 0.NN N.0N ± 0.NN MAE N.N0 ± N.00 × N.NN ± N.NN N.0N ± N.NN N.NN ± 0.NN N.NN ± N.NN N.NN ± N.NN N.NN ± 0.NN N.NN ± 0.NN MEE N.NN ± 0.NN × N.NN ± N.NN N.NN ± N.NN N.NN ± 0.NN N.NN ± 0.NN N.0N ± 0.NN N.NN ± 0.NN N.0N ± 0.N0  C  RMSE NN.NN± N0.N0 NN.NN± NN.NN N0.NN± NN.NN NN.NN± NN.NN NN.NN ± N.NN NN.N0± NN.NN N0.NN ± N.NN N.NN ± N.NN N.NN ± N.0N MAE NN.NN± NN.NN NN.NN± NN.NN NN.0N± NN.NN NN.00± NN.NN NN.NN ± N.NN NN.NN± NN.NN NN.NN± NN.NN N.NN ± N.NN N.NN ± N.NN MEE N.NN ± 0.NN N.N0 ± N.NN N.NN ± N.NN N.NN ± N.NN N.NN ± N.NN N.N0 ± N.N0 N.N0 ± N.NN N.0N ± N.NN N.NN ± N.NN  D  RMSE N.0N ± N.NN N.NN ± N.NN N.NN ± N.0N N.NN ± N.NN N.NN ± NN.NN N.NN ± N.NN N.NN ± N.0N N.NN ± N.NN 0.NN ± 0.NN MAE N.0N ± N.NN N.NN ± N.NN N.NN ± N.NN N.N0 ± N.NN NN.00± NN.N0 N.N0 ± N.NN N.NN ± N.N0 N.N0 ± N.NN N.NN ± 0.NN MEE N.NN ± N.NN N.NN ± N.NN N.N0 ± N.NN N.0N ± N.NN N.NN ± NN.NN N.NN ± N.0N N.NN ± N.00 N.NN ± 0.NN 0.NN ± 0.NN  N.N
Results on Application Experiments  In the fourth series of experiments, three types of data  sets are used
(i) A pair of CT images and two pairs  of MRI images from [NN], the images have a resolution in the range from NNN × NN0 to NN0 × NNN
The point sets are created by the edges of the objects and  extracted using a regional scanning method
The recall  curve is used to evaluate performances under N0 land- marks from edges
The results are shown in Fig
N
(ii)  Ten pairs of retinal images in multi-view scenarios, the  images have a resolution in the range from NN0 × NN0 to NNN0 × NN0
(iii) Eight pairs of fingerprint images from BIT (http://biometrics.idealtest.org/  dbDetailForUser.do?id=N), each image is of resolution NN0 × NN0 pixels
The feature points of (ii) and (iii) are extracted following the method proposed in [NN]
(iv)  Five pairs of remote sensing images (Venice, Hawaii, London, New York and Dali) from Google Earth, the images  have a resolution in the range from N00×N00 to NNN0×N00
For experiments on (ii)−(iv), they are all registered using SIFT feature points [NN]
At least ten pairs of corresponding  points between the transformed and reference images are  manually determined for quantitative comparisons based  on the root of mean square error (RMSE), maximum error  (MAE) and median error (MEE), all corresponding points  are well-distributed and selected on the easily identified areas, and the results are listed in Table NB−D, respectively
Registration examples of (ii)−(iv) are shown in Fig
N
 0 0.00N 0.0N 0.0NN 0.0N 0.0NN 0.0N 0  N0  N0  N0  N0  N0  N0  N0  N0  N0  N00  0 0.00N 0.00N 0.00N 0.00N 0.0N 0.0NN 0.0NN 0.0NN 0  N0  N0  N0  N0  N0  N0  N0  N0  N0  N00  0 0.0N 0.0N 0.0N 0.0N 0.0N 0.0N 0  N0  N0  N0  N0  N0  N0  N0  N0  N0  N00  0 N00 TPSRPM RPM-LNS CPD GMMREG RPM-LNE GLMDTPS MoAGREG PR-GLS Ours  R e c a ll  (  % )  R e c a ll  (  % )  R e c a ll  (  % )  Thorax CT Images Transerse Plain Brain MRI Images Sagittal Plain Brain MRI Images  Threshold Threshold Threshold  Fig
N
Registration performances on CT and MRI images
 Fig
N
Examples on fingerprint images (left), remote sensing images (top right) and retinal images (bottom right)
 N
Conclusion  We have presented a dual-feature based point set registration method with global-local spatial constraint
The  main idea of our GL-CATE is to first estimate the correspondence using the mixture-feature Gaussian mixture  model (MGMM), and then to update the transformation under global-local spatial constraint
Comparing with the current methods, the major contributions of this work are: (i)  the MGMM which can deal with two features for estimating correspondence is constructed, meanwhile, a uniform  distribution is modeled for outlier dealing; (ii) a new elliptical Gaussian soft histogram count strategy as well as the  derived shape distance is presented, which is able to quantify similar neighborhood structures; (iii) a bi-directional  deterministic annealing scheme is used to combine the Euclidean distance and the shape distance with the MGMM,  which leads to a local-to-global registration strategy; (iv)  a new Tikhonov regularization term is designed, this term  and the original one play complementary roles to improve  the robustness and accuracy of transformation at both global  and local scales
Experimental results demonstrate that the  proposed GL-CATE achieve better performances than stateof-the-art methods
 N
Acknowledgment  This work was supported by (i) National Nature Science Foundation of China [NNNNN0N0]; (ii) Scientific Research Foundation of Yunnan Provincial Department of Education [N0NNTYS0NN]
 NNNN  http://biometrics.idealtest.org/dbDetailForUser.do?id=N http://biometrics.idealtest.org/dbDetailForUser.do?id=N   References  [N] A
Basu, I
Harris, N
Hjort, and M
Jones
Robust and efficient estimation by minimising a density power divergence
 Biometrika, NN:NNN–NNN, NNNN
 [N] S
Belongie, J
Malik, and J
Puzicha
Shape matching and  object recognition using shape contexts
IEEE Trans
Pattern  Anal
Mach
Intell., NN:N0N–NNN, N00N
 [N] F
Bookstein
Principal warps: thin-plate splines and the  decomposition of deformations
IEEE Trans
Pattern Anal
 Mach
Intell., NN:NNN–NNN, NNNN
 [N] H
Chui and A
Rangarajan
A feature registration framework  using mixture models
In Proc
IEEE MMBIA-N000, NN0–  NNN, Hilton Head Island, SC, N000
 [N] H
Chui and A
Rangarajan
A new algorithm for non-rigid  point matching
Comput
Vis
Image Understand., NN:NNN–  NNN, N00N
 [N] A
Dempster, N
Laird, and D
Rubin
Maximum likelihood  from incomplete data via the EM algorithm
J
R
Statist
 Soc
Series B, NN:N–NN, NNNN
 [N] A
Frome, D
Huber, R
Kolluri, T
Bülow, and J
Malik
Recognizing objects in range data using regional point descriptors
In Proc
Eur
Conf
Comput
Vis., Pittsburgh, PA, May  N00N
 [N] B
Jian and B
Vemuri
Robust point set registration using  gaussian mixture models
IEEE Trans
Pattern Anal
Mach
 Intell., NN:NNNN–NNNN, N0NN
 [N] R
Jonker and A
Volgenant
A shortest augmenting path  algorithm for dense and sparse linear assignment problems
 Computing, NN:NNN–NN0, NNNN
 [N0] T
Kato, S
Omachi, and H
Aso
Asymmetric Gaussian and  its application to pattern recognition
Structural, Syntactic, and Statistical Pattern Recognition, N0N–NNN, Springer,  Berlin, Heidelberg, N00N
 [NN] M
Körtgen, M
Novotni, and R
Klein
Nd shape matching  with shape context
In The Nth Central European Seminar on  Computer Graphics, N00N
 [NN] M
Leordeanu, R
Sukthankar, and M
Hebert
Unsupervised  learning for graph matching
Int
J
Comput
Vis., NN:NN–NN,  N0NN
 [NN] H
Ling and D
Jacobs
Shape classification using the innerdistance
IEEE Trans
Pattern Anal
Mach
Intell., NN:NNN–  NNN, N00N
 [NN] D
Lowe
Distinctive image features from scale-invariant  keypoints
Int
J
Comput
Vis., N0:NN–NN0, N00N
 [NN] J
Ma, W
Qiu, J
Zhao, Y
Ma, A
Yuille, and Z
Tu
Robust  LNE estimation of transformation for non-rigid registration
 IEEE Trans
Signal Proc., NN:NNNN–NNNN, N0NN
 [NN] J
Ma, J
Zhao, J
Tian, X
Bai, and Z
Tu
Regularized vector field learning with sparse approximation for mismatch  removal
Pattern Recognit., NN:NNNN–NNNN, N0NN
 [NN] J
Ma, J
Zhao, J
Tian, Z
Tu, and A
Yuille
Robust estimation of nonrigid transformation for point set registration
IEEE Conf
Comput
Vis
Pattern Recognit., NNNN–NNNN,  N0NN
 [NN] J
Ma, J
Zhao, and A
Yuille
Non-rigid point set registration  by preserving global and local structures
IEEE Trans
Image  Process., NN:NN–NN, N0NN
 [NN] I
Markovsky
Low rank approximation
Springer, Verlag  London, N0NN
 [N0] M
Miller, H
Stone, and I
Cox
Optimizing murty’s ranked  assignment method
IEEE Trans
Aerosp
and Electron
Syst.,  NN:NNN–NNN, NNNN
 [NN] A
Myronenko and X
Song
Point set registration: Coherent point drift
IEEE Trans
Pattern Anal
Mach
Intell.,  NN:NNNN–NNNN, N0N0
 [NN] A
Myronenko, X
Song, and M
Carreira-Perpinan
Non  rigid point set registration: Coherent point drift
Advances in  Neural Information Processing Systems, N00N–N0NN, N00N
 [NN] M
Neal, Radford and G
Hinton
A view of the EM algorithm that justifies incremental, sparse, and other variants
 Learning in Graphical Models , NNN–NNN
Springer Netherlands, Dordrecht, NNNN
 [NN] C
Papadimitriou and K
Steiglitz
Combinatorial optimization: Algorithms and complexity
Prentice-Hall, Inc., Upper  Saddle River, NJ, USA, NNNN
 [NN] L
Peng, G
Li, M
Xiao, and L
Xie
Robust cpd algorithm  for non-rigid point set registration based on structure information
PLoS ONE, NN(N):e0NNNNNN, N0NN
 [NN] R
Rifkin, G
Yeo, and T
Poggio
Regularized least-squares  classification, NATO science series: Computer and Systems  Sciences, NN0:NNN–NNN
IOS Press, Amsterdam, The Netherlands, N00N
 [NN] B
Schölkopf, R
Herbrich, and A
Smola
A generalized  representer theorem
In International Conference on Computational Learning Theory, NNN–NNN, London, UK, N00N
 Springer-Verlag
 [NN] D
Scott
Parametric statistical modeling by minimum integrated sqaure error
Technometrics, NN:NNN–NNN, N00N
 [NN] G
Wahba
Spline models for observational data
SIAM,  Philadelphia, Pennsylvania, NNN0
 [N0] G
Wang, Z
Wang, Y
Chen, and W
Zhao
A robust non-rigid  point set registration method based on asymmetric gaussian  representation
Comput
Vis
Image Understand., NNN:NN–N0,  N0NN
 [NN] G
Wang, Z
Wang, Y
Chen, and W
Zhao
Robust point  matching method for multimodal retinal image registration
 Biomed
Signal Process
Control, NN:NN–NN, N0NN
 [NN] Y
Yang, S
Ong, and K
Foong
A robust global and local  mixture distance based non-rigid point set registration
Pattern Recognit., NN:NNN–NNN, N0NN
 [NN] A
Yuille and N
Grzywacz
A mathematical analysis of the  motion coherence theory
Int
J
Comput
Vis., N:NNN–NNN,  NNNN
 [NN] Y
Zheng and D
Doermann
Robust point matching for  nonrigid shapes by preserving local neighborhood structures
 IEEE Trans
Pattern Anal
Mach
Intell., NN:NNN–NNN, N00N
 NNNNFrom RGB to Spectrum for Natural Scenes via Manifold-Based Mapping   From RGB to Spectrum for Natural Scenes  via Manifold-based Mapping  Yan JiaN, Yinqiang ZhengN, Lin GuN, Art Subpa-AsaN, Antony LamN, Yoichi SatoN, Imari SatoN  NRWTH Aachen University, Germany NNational Institute of Informatics, Japan NTokyo Institute of Technology, Japan NSaitama University, Japan NUniversity of Tokyo, Japan  Abstract  Spectral analysis of natural scenes can provide much  more detailed information about the scene than an ordinary  RGB camera
The richer information provided by hyperspectral images has been beneficial to numerous applications, such as understanding natural environmental changes  and classifying plants and soils in agriculture based on  their spectral properties
In this paper, we present an efficient manifold learning based method for accurately reconstructing a hyperspectral image from a single RGB image captured by a commercial camera with known spectral  response
By applying a nonlinear dimensionality reduction technique to a large set of natural spectra, we show  that the spectra of natural scenes lie on an intrinsically low  dimensional manifold
This allows us to map an RGB vector to its corresponding hyperspectral vector accurately via  our proposed novel manifold-based reconstruction pipeline
 Experiments using both synthesized RGB images using hyperspectral datasets and real world data demonstrate our  method outperforms the state-of-the-art
 N
Introduction  Spectral analysis of natural scenes can provide much  more detailed information about the scene than an ordinary  RGB camera
The richer information provided by hyperspectral imaging has been beneficial to numerous applications in agriculture and land health surveillance, such as understanding natural environmental changes and classifying  plants and soils based on their spectral properties
Most  general approaches to imaging the spectra of a scene capture narrowband hyperspectral image stacks at consecutive  wavelengths
A number of optical elements are required to  achieve this task, and commercially available hyperspectral  imaging cameras are often expensive and tend to suffer from  spatial, spectral, and temporal resolution issues
 The goal of this work is providing a cost-efficient solution for hyperspectral imaging that can reconstruct the  spectra of a natural scene from a single RGB image captured by a camera with known spectral response
Obviously,  the transformation from RGB to spectra is a three-to-many  mapping and thus cannot be unambiguously determined unless some prior knowledge about the transformation is introduced
Indeed, there is existing work that establishes  such priors
In the field of spectral reflectance recovery, researchers have examined large sets of spectral reflectance  distributions and their corresponding RGB vectors in order to learn how to map from RGB to spectra
Examples  include radial basis function (RBF) network mapping [NN]  and constrained sparse coding [N]
More recently, Arad and  Shahar used a large sparse dictionary of spectra and corresponding RGB projections that could then be used as a  basis to map RGB vectors to spectra [N]
However, existing  approaches directly operate on the RGB space without explicitly exploring the data structure of spectral information,  thus requiring a large amount of data for training
 We propose a two-step manifold-based mapping and reconstruction pipeline to reconstruct the spectra from a single RGB image
We start by investigating the intrinsic dimensionality of the spectra of natural scenes
By applying a  nonlinear dimensionality reduction technique to a large set  of natural spectra, we show that the spectra of natural scenes  lie on an intrinsically low dimensional manifold
Based on  the derived manifold, we learn an accurate nonlinear mapping from RGB to the ND embedding
By doing so, we  reduce the problem of the three-to-many mapping (RGB to  spectrum) to a well-conditioned and compact three-to-three  mapping (RGB to ND embedding of spectra)
Compared to  previous proposed approaches that aim to solve the three-tomany mapping directly, the three-to-three mapping allows  us to train a more accurate model
After mapping to the ND  embedding, the original spectrum can be recovered using a  manifold-based reconstruction technique
 NNN0N    Figure N: Scene spectra are recovered from RGB observation through our proposed nonlinear manifold learning and reconstruction technique based on pre-learned mapping between training RGB values and their corresponding ND embedding
 Our major contributions are summarized as follows:  • This paper presents a cost-efficient solution for hyper- spectral imaging that requires only a single RGB image of a scene captured by a camera with known spectral response
 • We investigate the intrinsic dimensionality of the spec- tra of natural scenes by a nonlinear dimensionality reduction technique
We find that natural scene spectra  approximately reside in a ND embedded space
 • We propose a two-step manifold-based mapping and reconstruction pipeline that avoids solving the difficult  three-to-many mapping problem
Namely, we transform any given RGB vector to a ND point in the embedding of natural spectra
From there, the corresponding  spectrum is recovered using a manifold-based reconstruction technique
 N
Related Work  Hyperspectral imaging has proven beneficial to many applications in agriculture, remote sensing, medical diagnosis, and others
As a result, there is a large body of work  on hyperspectral imaging of scenes
Approaches such as  push broom scanning a spatial line or switching narrow  bandpass filters in front a grayscale camera [NN] for each  wavelength of interest are in common use
However, these  imaging approaches are slow
In response to issues with  speed, snapshot hyperspectral cameras have been developed  [N, N0, NN, NN]
Despite the better speed, spectral resolution  is typically sacrificed
In addition, all hyperspectral cameras usually have lower spatial resolution than typical RGB  cameras
Thus there have also been efforts at combining  hyperspectral and RGB cameras together [N, N, NN, NN]
In  these setups, the RGB and hyperspectral camera are made  to share a common field-of-view
The spatial and spectral  information from both cameras are then combined to form  a high-spatial resolution, hyperspectral image
All the different kinds of hyperspectral camera setups have their own  pros and cons but a common drawback is that they are often  expensive and not as accessible
 Thus there have been attempts to use conventional RGB  cameras to capture the spectral information of a scene, in  particular, the spectral reflectance of scene points
One general approach is to use active lighting [N, NN, NN, NN] by  taking advantage of the well-known statistical property that  spectral reflectance seen in natural scenes mostly exists in  a low-dimensional linear subspace of the high-dimensional  space of arbitrary spectral reflectance [N, NN, NN, NN, NN, N]
 In these approaches using active lighting, an RGB or  grayscale camera is used to capture multiple images of  a scene under controlled lighting
By carefully defining  the light spectra used and knowing the camera spectral response, it is possible to recover the spectral reflectance of  surface points
However, this does not work in outdoor settings or in a number of everyday situations where the illumination cannot be controlled
Also, sometimes the lighting  condition of a given environment is of interest
 For more widespread applicability, passive imaging approaches are preferred
In addition, it would be good to  be able to capture hyperspectral images without any specialized equipment
Thus some researchers have proposed  approaches for reconstructing the hyperspectral image of  a scene from a single RGB image by learning a mapping  from RGB vectors to spectra using a large set of spectral reflectance distributions and their corresponding RGBs
 Nguyen et al
proposed to learn the transformation from  white balanced RGB values to illumination-free reflectance  NN0N    spectra based on a radial basis function (RBF) network  mapping [NN]
Antonio proposed to learn the prototype  set from the database based on a constrained sparse coding approach and use it for illumination-free spectral recovery [N]
More recently, Arad and Ben-Shahar created  a large database of natural scene hyperspectral images and  derived a sparse dictionary of hyperspectral signatures and  their RGB projections
The dictionary and corresponding RGB projections could then be used as a basis to estimate the spectrum of any given RGB vector [N]
These  approaches tackled a difficult inverse problem involving a  three-to-many mapping, and thus priors needed to be established for effective learning
This is typically accomplished  by using large amounts of training data
 Our method is different from previously proposed ones  in the sense that we avoid directly solving a three-to-many  mapping of the RGB vector to spectrum
Specifically, we  propose a two-step manifold-based mapping and reconstruction pipeline by considering the intrinsic dimensionality of natural scene spectra
This leads to a formulation of  the problem where we can map an RGB vector to a spectrum as a well-conditioned three-to-three mapping (RGB to  ND embedded spectra)
The original spectra can then be  recovered via low-dimensional manifold reconstruction
 N
Spectral Reconstruction of Natural Spectra  via Manifold-based Nonlinear Mapping  This paper focuses on recovering the spectra of outdoor  scenes under daylight illumination from a single RGB image captured by a camera with known spectral response
 Fig
N shows the flow of our method for spectral reconstruction
Our method consists of training and testing stages,  which are indicated using black and red arrows, respectively  in the figure
 In the training stage, given a large set of natural spectra, we first investigate the intrinsic dimensionality of natural scene spectra using a nonlinear dimensionality reduction technique (Sec
N.N)
Specifically, we show that the  spectra of natural scenes lie on an intrinsically low dimensional manifold
At the same time, for each spectrum in the  database, a corresponding RGB vector is computed based  on the spectral responses of the RGB camera
Once the set  of RGB and spectrum pairs is prepared, a transformation  from RGB vectors to their corresponding three dimensional  embedded spectra is learned (Sec
N.N)
 In the testing stage, we can first transform an input  RGB vector into the three dimensional embedding using  the learned transformation
Once the RGB vector is transformed into a ND point in the embedding, the original spectrum is reconstructed from a manifold-based reconstruction  technique (Sec
N.N)
In the following, we describe each  step of our method in detail
 N.N
Analyzing Dimensionality of Natural Scene Spectra  It has been widely examined and accepted that the reflectance spectra of natural objects lie in a low-dimensional  subspace or manifold [N, NN, NN, NN, NN, N]
This leads us  to speculate that spectra of natural scenes are intrinsically  low-dimensional as well, since the radiance of a scene point  is a compound of the illumination and surface reflectance  [N, NN]
Specifically, by assuming that the scene point is  diffuse, its radiance i(λ) can be roughly expressed by  i(λ) = l(λ)r(λ), (N)  in which l(λ) and r(λ) denote the illumination and re- flectance intensity at wavelength λ
By stacking all spectra  of a hyperspectral image into a matrix I , we obtain  I =      iN(λN) · · · iM (λN) · · · · · · · · ·  iN(λN ) · · · iM (λN )      =      l(λN) 0 0 0 · · · 0 0 0 l(λN )      ︸ ︷︷ ︸  L      rN(λN) · · · rM (λN) · · · · · · · · ·  rN(λN ) · · · rM (λN )      ︸ ︷︷ ︸  R  ,  (N)  in which M and N denote the number of pixels and the number of bands, respectively
From the viewpoint of linear algebra, the rank of I should be no greater than that of R,  which is low-dimensional for natural reflectance materials
 For real natural scenes, the model in Equation N may  not hold accurately because of complex surface reflectance  properties
Therefore, we follow a widely used criterion  [NN], the residual variance of dimensionality reduction,  to determine the intrinsic dimensionality of natural scene  spectra
At first, Isometric Feature Mapping (Isomap) [NN],  a nonlinear dimensionality reduction method, was applied  on the natural scene spectra to embed them into a low dimensional space
Isomap estimates the intrinsic geometry of a data manifold by examining a neighborhood graph  of the data points constructed in high-dimensional space
 This neighborhood graph is used for computing pairwise  geodesic distances between two points measured over the  manifold
Once a matrix containing the geodesic distances  between all data points is obtained, classical MDS is applied to this matrix to find a low-dimensional embedding  of the data points such that the estimated intrinsic geometry is best preserved through dimensionality reduction
By  following the procedure of [NN], we calculate the residual  variance N−RN(Dm, Dgt), where Dm is the matrix of Eu- clidean distances of the low dimensional embedding, while  Dgt is the graph distance matrix of the input data
R is the  standard correlation coefficient between Dm and Dgt
The  intrinsic degrees of freedom is then observed at the ”elbow”  NN0N    (a) N00 spectra (b) N000 spectra  Figure N: The residual variance of Isomap on representative  spectra from [N]
(a) N00 representative spectra from [N], (b)  N000 representative spectra from [N]
 where the curves of residual variance stop decreasing significantly with the added dimensions as illustrated in Fig
 N
We refer to Tenenbaum et al
[NN] for details
 With this criterion, we conducted a dimensionality analysis of spectra of natural scenes on the natural hyperspectral image databaseN provided by Arad and Ben-Shahar [N],  which is the largest spectral database for natural scenes  available today
In Fig
N, we report the residual variance on  (a) N00 main representative spectra picked up by k-means  from [N] and (b) another N000 representative spectra collection from [N]
This figure shows that Isomap detects the dimensionality as three where the residual variance is almost  zero on both N00 representative spectra and N000 spectra
 This observation echoes the existing research on the sparsity of natural scenes
However, it worth noting that the  spectral sampling resolution of this dataset is N0 nm, which  has left out finer spectral details
 N.N
Conversion from RGB to ND Embedding of Scene Spectra  After finding the low dimensional embedding of natural  scene spectra, a mapping f is learned between RGB vectors  and their corresponding ND embedded natural scene spectra  as: f : RN → RN
Through experimental validation, we employ the compact neural network (a radial basis function  with N0 hidden neurons) to learn a nonlinear transformation  f between RGB vectors and their corresponding ND embedded spectra
We used the Levenberg-Marquardt training algorithm [NN, NN], which minimizes the following equation:  β̂ = argmin β  m∑  i=N  [yi − f(pi, β)] N, (N)  where each p, y ∈ RN is a pair of RGB vector and corre- sponding ND embedded spectrum in the training set, and β  is the parameter to be found for the model f(p, β) to fit the training pairs (pi, yi), so that the sum of the squares of the deviations is minimized
 Nhttp://icvl.cs.bgu.ac.il/hyperspectral/  N.N
Spectra Reconstruction from ND Embedding  The main focus of dimensionality reduction techniques  is how to efficiently reduce the dimensionality of high dimensional inputs by revealing meaningful structure hidden  in the data
Many nonlinear dimensionality reduction techniques thus rarely consider the inverse problem of reconstructing original data from the derived low dimensional  embeddings
In order to reconstruct original spectra from  its ND embedding, we employ a dictionary learning based  technique [NN] that learns dictionary pairs for high and low  dimensional spaces and uses their relationship for reconstruction of high dimensional data from a point in the embedding
 Let xi ∈ R N×N denote a high dimensional spectrum  and g(xi) = yi ∈ R N×N denote the ND embedding of  the spectrum
Then we wish to find a high dimensional  dictionary DH = [dN, · · · , dK ] and coding scalars ci = [cNi, cNi, · · · , cKi]  T such that these two functions are minimizedN:  M∑  i=N  ∣ ∣ ∣  ∣ ∣ ∣g(xi)−  K∑  j=N  cjig(dj) ∣ ∣ ∣  ∣ ∣ ∣  N  ,  M∑  i=N  ∣ ∣ ∣  ∣ ∣ ∣xi −  K∑  j=N  cjidj  ∣ ∣ ∣  ∣ ∣ ∣  N  (N)  for all M spectra in our training dataset
By doing so, we  are essentially finding a common coding between the ND  embedding and high dimensional space of the spectra
Then  given the estimated coding C = [cN, cN, · · · , cM ] and all embeddings Y = {yN, · · · , yM}, we can determine the ND embedding dictionary DL by:  min DL  ||Y −DLC|| N F 
(N)  For a new point in the low dimensional space yt, we can  compute its coding CT over the low dimensional dictionary  DL
Then the high dimensional data of yT can be reconstructed as xT = DHcT 
 N
Experiment Results  In this section, we evaluate our manifold based mapping on both public hyperspectral datasets [N, NN, N] and  real world images
The dataset in [N] comprises of N00 images, which is by far the most comprehensive natural hyperspectral database
Similarly, the Harvard Outdoor Dataset  [N] consists of N0 outdoor images in daylight illumination
 Though our method is based on low dimension assumptions  about natural spectra, the comparison on the indoor dataset  CAVE [NN], which has NN images of indoor scenarios, is  also conducted
The three datasets cover complex scenarios including various materials, multiple illuminations and  shadows
We use the learned mapping to recover the spectra  NSee [NN] for details
 NN0N    Datasets [N] Harvard [N] CAVE [NN]  Our N.N0± N.NN N.NN± N.NN NN.NN± N0.NN [NN] NN.NN± N.0N NN.NN± N.NN NN.NN± NN.NN [N] N.NN± N.NN NN.N0± N.NN NN.NN± N0.NN  Table N: The average and variance of NRMSD (%) of re- construction on the hyperspectral databases [N, N, NN]
 of simulated and real RGB images
We quantitatively compare our method with [NN] and [N]
To further validate the  robustness of our method, we also evaluate the performance  under different parameter settings
 N.N
Training Data and Parameter Settings  In default, for each database, we randomly split half of  the images as training set and the rest for testing
To avoid  over-fitting, we also report the performance on a smaller  training set in Section N.N
Since some of these images contain large dark background areas, naive acquisition of our  hyperspectral training set by randomly sampling these images is likely to produce a biased result where the genuine  hyperspectral information is severely underrepresented
To  resolve this issue, we randomly pick N000 spectra from each  image in the training set, and use the K-means algorithm  [N0] to collect the most dominant W spectra for the training  set
We set W to be N00 in all our experiments
 The camera spectral response function we used here to  synthesize RGB values is from the Canon ND Mark II
A  radial basis function neural network with N0 hidden neurons is used to map the RGB values to the embedding of  dimensionality of N unless explicitly stated otherwise
To  verify the quantitative accuracy for spectral reflectance reconstruction, we use the normalized root mean square deviation (NRMSD) as our metric, calculated by ǫ r(x) =  √∑ λ (r(λ,x)−rgt(λ,x))N  N r̄(x) , where r(λ, x) and rgt(λ, x) are the  reconstructed and actual spectral reflectances of the pixel x,  r̄(x) is the average value across wavelengths for spectrum x
N is the number of bands in the pixel
NRMSD normalizes each ground truth spectrum to avoid bias toward strong  signals
 N.N
Evaluation on Hyperspectral Datasets  We first compare the spectra reconstruction performance  on the three aforementioned hyperspectral image databases  [N, NN, N]
The trained nonlinear mapping is used to recover  hyperspectral images from the RGB images in the testing  sets
In Table N, we present the quantitative comparison of  our method, [NN] and [N] for the whole testing sets
This table shows our method outperforms the alternatives in terms  of spectra reconstruction accuracy
We note that, though  CAVE is an indoor dataset that does not strictly follow our  assumptions about natural spectra, our method still manages  to achieve superior performance over alternative methods
 We also present Fig
N, which shows the recovered spectra  for three randomly selected pixels from three test images
 We can see that the performance of our method is consistently better than that of [NN] and [N]
 To examine the spatial consistency of the recovered hyperspectral images, we also present some images at seven  different wavelengths as exemplary images in Fig
N
We  can observe that the recovered images from our method are  consistently accurate across the wavelength axis, irrespective of the scene materials
Our method performs particularly well on the NN0nm and N00nm bands where the alternatives often encounter much error
We also note that the  performance of all methods deteriorate at the NN0nm band
 The reason is that the camera response is very weak at the  blue end, and the inaccuracy in mapping has a critical influence on the recovery results
 In Fig
N, we also show the typical scenarios where  our method gives best and worse performance in terms of  NMRSD between the reconstructed spectra and the ground  truth
Our method works best in the scenarios where artificial materials such as buildings occupy much of the image
It might be due to the fact that artificial materials  have similar chemical compositions and thus exhibit similar spectra
In contrast, our performance fails when the  image is over/under exposed, like in the first image of the  second row
Natural objects such as plants also present  challenges
However, especially among challenging scenarios, our method consistently generates better reconstruction  compared to the alternatives
 N.N
Analysis on Parameter Sensitivities  To demonstrate the robustness of our algorithm, we also  analyze the sensitivity of our method to the size of the training set, the manifold dimensionality and the swap of camera response functions
When trained with only N0 images in [N], our method still achieves a satisfactory average  NMRSD N.NN%, compared to N.N0% that is trained with N00 images as reported in Section N.N
 As analyzed in Section N.N, we observed that the dimensionality of three is sufficient in representing the natural  spectra
To evaluate this observation in our system, we calculate the reconstruction errors on [N], when mapping the  RGB values to the embedding of dimensionality of N and N,  rather than N used in all other experiments
We found that  the average NRMSD increases as the dimensionality goes  up, which is N.N0% for N dimensions, N.NN% for N dimen- sions and N.0N% for N dimensions
Adding more dimen- sions seems to be detrimental to accuracy, because it might  introduce noises into the RGB to spectrum learning process
 Our experiments also show the robustness over various  camera spectral response functions
For the dataset [N], the  NN0N    bguCAMP 0NNN-NNNN N Lehavim 0NN0-NNNN N nachal 0NNN-NNNN N N  Figure N: Experiment results on three testing images in the hyperspectral database [N]
The spectral distributions for three  randomly selected pixels from each test image are shown in every column
 average NRMSD of our method is N.NN% on the NikonDN0 and N.NN% on the Olympus EPLN
To further demonstrate the robustness against swapping camera response functions,  we also train the model with one camera and then reconstruct the hyperspectral data from RGB images simulated  by another camera from the dataset [N]
We find the performance between some pairs from the same camera manufacturer, such as CanonN0D and CanonND Mark II, is still reasonable, with an average NRMSD of N.NN%
However, for the pair between different manufacturers such as CanonND  Mark III and Nikon DN0, it will deteriorate to NN.N%
 N.N
Real Images  We also use a commercial Nikon DNS camera to capture some images of outdoor scenes (see Fig
N (a,b) for  examples)
To alleviate the influence of camera nonlinear  intensity response and unexpected image compression, we  use instead the RAW files and convert them into RGB images
The camera spectral response function is provided  by the sensor maker
For this specific response function,  we learn a nonlinear mapping again by using the aforementioned training set, and use it to recover spectra from RGB  values
Fig
N(c) show the recovered spectra for the four  color patches from our method, [NN] and [N]
We can see  that our method works better than the others, which verifies again the benefits in accounting for the intrinsic dimensions of natural spectra
We also note that, compared with  the synthetic experiments, the performance of three methods slightly degrades
This might be attributed to the error  in the camera spectral response function
 NNN0    NN0nm NN0nm N00nm NN0nm NN0nm NN0nm NN0nm  G ro  u n  d tr  u th  R ec  o n  st ru  ct io  n  O u  r m  et h  o d  N R  M S  D  O u  r m  et h  o d  N R  M S  D  E C  C V  N N  [ N N  ]  N R  M S  D  E C  C V  N N  [ N ]  G ro  u n  d tr  u th  R ec  o n  st ru  ct io  n  O u  r m  et h  o d  N R  M S  D  O u  r m  et h  o d  N R  M S  D  E C  C V  N N  [ N N ]  N R  M S  D  E C  C V  N N  [ N ]  Figure N: Sample results from the hyperspectral database [N]
 NNNN    B es  t  P er  fo rm  an ce  NRMSD  Our N.0N% N.NN% N.NN% N.NN% N.NN% [NN] NN.0N% NN.NN% NN.NN% NN.0N% NN.NN% [N] N.NN% N.NN% N.N0% N.NN% N.N0%  W o  rs t  P er  fo rm  an ce  NRMSD  Our N.NN% N.N0% N.NN% N.NN% N.NN% [NN] NN.NN% NN.NN% NN.0N% NN.NN% NN.NN% [N] N.NN% N.NN% NN.0N% N.NN% N.NN%  Figure N: The scenarios where our method works best and worst
The comparison of corresponding average NRMSD is also  provided
 (a) RGB image (b) Camera response Patch #N Patch #N Patch #N Patch #N  (c) Comparison between our method, [NN] and [N]  Figure N: Experiment results of real-world scenario using a commercial Nikon DNS camera
(a) The RGB image of the scene  with a color checker board
(b) The camera spectral response function provided by the maker
(c) Recovered spectra from  our method, [NN] and [N] for the four color patches indicated in (a)
 N
Conclusion  We have explored the intrinsic dimensionality of the  spectral space of natural scenes and found that a low dimensional embedding by Isomap is enough, to a large extent, to  account for the spectral variance
This has allowed us to  train a neural network based nonlinear mapping between  the RGB color space and the three-dimensional embedding,  by using only a small amount of representative data in an  efficient manner
Experiments using synthetic images and  real world data have verified the effectiveness of our nonlinear mapping based method for spectral super-resolution,  as well as its advantages over existing approaches
 In this paper, we have concentrated primarily on the  spectra of natural scenes, which is relevant to consumer  RGB device based spectral imaging of outdoor objects under daylight illumination
The illumination spectra for indoor illuminants are more complex
An extensive evaluation of our spectral recovery method for indoor scenes deserves to be conducted in our future work
 Acknowledgement  The majority of this paper was finished when Yan Jia  was visiting National Institute of Informatics (NII), Japan,  funded by the NII MOU Internship Program
This work  was supported in part by JSPS KAKENHI Grant Number  JPNNH0NNNN and JPNNKNN0NN
 NNNN    References  [N] N
Akhtar, F
Shafait, and A
Mian
Hierarchical beta process  with gaussian process prior for hyperspectral image super  resolution
In Proc
of European Conference on Computer  Vision (ECCV), pages N0N–NN0, Oct
N0NN
N  [N] R
K
Antonio
Single image spectral reconstruction for multimedia applications
In the NNrd ACM International Conference on Multimedia, pages NNN–NN0, N0NN
N, N  [N] B
Arad and O
Ben-Shahar
Sparse recovery of hyperspectral signal from natural rgb images
In Proc
of European  Conference on Computer Vision (ECCV), pages NN–NNN, Oct
 N0NN
N, N, N, N, N, N, N  [N] F
Ayala, J
F
Echávarri, P
Renet, and A
I
Negueruela
Use  of three tristimulus values from surface reflectance spectra to  calculate the principal components to reconstruct these spectra by using only three eigenvector
Journal of the Optical  Society of America A, NN(N):N0N0–N0NN, Sep
N00N
N, N  [N] X
Cao, H
Du, X
Tong, Q
Dai, and S
Lin
A PrismMask System for Multispectral Video Acquisition
IEEE  Trans
Pattern Analysis and Machine Intelligence (PAMI),  NN(NN):NNNN–NNNN, N0NN
N  [N] X
Cao, X
Tong, Q
Dai, and S
Lin
High resolution multispectral video capture with a hybrid camera system
In Proc
 of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNN–N0N, N0NN
N  [N] A
Chakrabarti and T
Zickler
Statistics of real-world hyperspectral images
In Proc
of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNN–  N00, June N0NN
N, N  [N] C
Chi, H
Yoo, and M
Ben-Ezra
Multi-Spectral Imaging by  Optimized Wide Band Illumination
International Journal of  Computer Vision (IJCV), NN(N-N):NN0–NNN, N0N0
N  [N] J
Cohen
Dependency of the spectral curves of the munsell  color chips
Psychonomic science, N(N-NN):NNN–NN0, Jan
 NNNN
N, N  [N0] M
Descour and E
Dereniak
Computed tomography imaging spectrometer: Experimental calibration and reconstruction results
Applied Optics, NN(NN):NNNN–NNNN, NNNN
N  [NN] M
Donald
An algorithm for least-squares estimation of  nonlinear parameters
SIAM Journal on Applied Mathematics, NN(N):NNN–NNN, NNNN
N  [NN] Y
Fu, Y
Zheng, I
Sato, and Y
Sato
Exploiting spectralspatial correlation for coded hyperspectral image restoration
 In Proc
of IEEE Conference on Computer Vision and Pattern  Recognition (CVPR), pages NNNN–NNNN, N0NN
N  [NN] L
Gao, R
Kester, N
Hagen, and T
Tomasz
Snapshot image  mapping spectrometer (IMS) with high sampling density for  hyperspectral microscopy
Optical Express, NN(NN):NNNN0–  NNNNN, N0N0
N  [NN] N
Gat
Imaging spectroscopy using tunable filters: a review
 In Proc
of SPIE, Wavelet Applications VII, volume N0NN,  pages N0–NN, N000
N  [NN] S
Han, I
Sato, T
Okabe, and Y
Sato
Fast Spectral Reflectance Recovery Using DLP Projector
International Journal of Computer Vision (IJCV), pages N–NN, Dec
N0NN
N  [NN] T
Jaaskelainen, J
Parkkinen, and S
Toyooka
Vectorsubspace model for color representation
Journal of the Optical Society of America A, N(N):NNN–NN0, NNN0
N, N  [NN] R
Kawakami, J
Wright, Y.-W
Tai, Y
Matsushita, M
BenEzra, and K
Ikeuchi
High-resolution hyperspectral imaging via matrix factorization
In Proc
of IEEE Conference  on Computer Vision and Pattern Recognition (CVPR), pages  NNNN–NNNN, N0NN
N  [NN] L
Kenneth
A method for the solution of certain non-linear  problems in least squares
Quarterly of Applied Mathematics, N:NNN–NNN, NNNN
N  [NN] A
Lam, A
Subpa-Asa, I
Sato, T
Okabe, and Y
Sato
Spectral imaging using basis lights
In Proceedings of the British  Machine Vision Conference
BMVA Press, N0NN
N  [N0] J
MacQueen
Some methods for classification and analysis of multivariate observations
In Proceedings of the fifth  Berkeley symposium on mathematical statistics and probability., volume N, pages NNN–NNN, California, USA, NNNN
N  [NN] L
T
Maloney
Evaluation of linear models of surface spectral reflectance with small numbers of parameters
Journal  of the Optical Society of America A, N(N0):NNNN–NNNN, Nov
 NNNN
N, N  [NN] D
H
Marimont and B
A
Wandell
Linear models of surface and illuminant spectra
Journal of the Optical Society of  America A, N(NN):NN0N–NNNN, NNNN
N, N  [NN] R
M
H
Nguyen, D
K
Prasad, and M
S
Brown
TrainingBased Spectral Reconstruction from a Single RGB Image
In  Proc
of European Conference on Computer Vision (ECCV),  pages NNN–N0N, Sept
N0NN
N, N, N, N, N, N  [NN] J.-I
Park, M.-H
Lee, M
D
Grossberg, and S
K
Nayar
Multispectral Imaging Using Multiplexed Illumination
 In Proc
of International Conference on Computer Vision  (ICCV), pages N–N, N00N
N  [NN] J
P
S
Parkkinen, J
Hallikainen, and T
Jaaskelainen
Characteristic spectra of munsell colors
Journal of the Optical  Society of America A, N(N):NNN–NNN, NNNN
N, N  [NN] J
B
Tenenbaum, V
de Silva, and J
C
Langford
A global  geometric framework for nonlinear dimensionality reduction
Science, NN0:NNNN–NNNN, N000
N, N  [NN] A
Wagadarikar, N
Pitsianis, X
Sun, and B
David
Video  rate spectral imaging using a coded aperture snapshot spectral imager
Optical Express, NN(N):NNNN–NNNN, N00N
N  [NN] F
Yasuma, T
Mitsunaga, D
Iso, and S
Nayar
Generalized  assorted pixel camera: Post-capture control of resolution, dynamic range and spectrum
Technical report, Nov
N00N
N,  N  [NN] Z
Zhao and G
Feng
A dictionary-based algorithm for  dimensionality reduction and data reconstruction
In NNnd  International Conference on Pattern Recognition (ICPR),  pages NNNN–NNNN
IEEE, Dec
N0NN
N  NNNNChained Cascade Network for Object Detection   Chained Cascade Network for Object Detection  Wanli OuyangN,N, Kun WangN, Xin ZhuN, Xiaogang WangN  N
The Chinese University of Hong Kong N
The University of Sydney {wlouyang, kwang, xzhu, xgwang}@ee.cuhk.edu.hk  Abstract  Cascade is a widely used approach that rejects obvious  negative samples at early stages for learning better classifier and faster inference
This paper presents chained cascade network (CC-Net)
In this CC-Net, there are many cascade stages
Preceding cascade stages are placed at shallow layers
Easy hard examples are rejected at shallow layers so that the computation for deeper or wider layers is not  required
In this way, features and classifiers at latter stages  handle more difficult samples with the help of features and  classifiers in previous stages
It yields consistent boost in  detection performance on PASCAL VOC N00N and ImageNet for both fast RCNN and Faster RCNN
CC-Net saves  computation for both training and testing
Code is available  on https://github.com/wkNN0NN0/ccnn
 N
Introduction  Object detection is a fundamental computer vision task
 It differs from image classification in that the number of  background samples (image regions not belonging to any  object class of interest) is much larger than the number of  object samples
This leads to the undesirable imbalance in  the number of samples for different classes during training
 In order to handle the imbalance problem from the background samples, bootstrapping, cascade, and hard negative mining have been developed [NN, N, NN]
In these approaches, classifier learning is divided into multiple stages
 In each stage, only a subset of background samples are  used for training
The classifiers at earlier stages handle  easier samples while the classifiers at latter stages handle  more difficult samples
Bootstrapping and hard negative  mining aim at learning more accurate classifier
In comparison, cascade improves both accuracy and speed of the  detection process by rejecting easy background samples at  both training and testing time
The essence of cascade is  to learn more discriminative classifiers by using multi-stage  classifiers
Classifiers at early stages discard large number  of easy negative samples so that classifiers at latter stage  can focus on handling more difficult examples
Motivated  by these approaches, we propose a CC-Net to learn features  and classifiers with multiple stage
The more discriminative  Figure N
Motivation of the chained cascade ConvNet in rejecting large number of easy background samples for faster speed and  more discriminative features
 features and classifiers are used for handling more difficult  examples as the network goes deeper or wider
 Deep ConvNets have led to significant improvement in  accuracy for object detection [NN, N0]
Based on a baseline  network, adding deeper layers [N0] or branches for making  the network wider [N, NN, N] were found to improve accuracy for object detection and classification
But this also  increases computational time for both training and testing
 Empirical results in [NN] show that very large batch size  for ConvNet provides improvement in detection accuracy,  but leads to increase in training time
In order to reduce  the computational complexity from more training samples  and more complex networks, we design a cascaded network structure
This network has many stages for rejecting  easy samples within a single CNN
When an easy sample  is rejected at a shallow layer, it need not go through deeper  layers, for which the computation and memory are not required
With this design, the huge number of training samples can be used but the memory and time for training and  testing are still acceptable by rejecting large number of easy  examples at earlier cascade stages
 NNNNN    Based on the observations above, we design a chained  cascade network (CC-Net) for object detection
The contribution of this design is as follows:  • Cascade is used in both training and testing the CC-Net to save computation
In the CC-Net, when a sample is  rejected at shallow layers, the computation and memory  at deeper layers are not require for it
 • In the network, the early cascade stage and contextual cascade stage are used for learning more and more discriminative features
By rejecting easy samples at shallow layers in the network, the features and classifiers  learned at deeper layers or extra branches focus on harder  samples
In this way, the learned features and classifiers  are better in handling the harder examples
As shown  by the example in Fig
N, when the first classifier finds  that the object should only be a mammal, then the features and classifier at the second stage can focus more on  distinguishing specific class of mammal like horse, sheep  and cattle
And the classifier at the third stage then learns  features that help to distinguish mammals with horns,  e.g
sheep and cattle
 • All the cascaded classifiers and their corresponding fea- tures are jointly learned through a single ConvNet
 With BN-Net [NN] as the baseline model, our approach provides N.N% mAP gain compared with BN-Net
Testing time  of the CC-Net is NN% of the GBD-Net and N0% of the CCNet without cascade
Experimental results on ImageNet and  Pascal VOC N00N show improvement in detection accuracy  for different region proposal methods
 N
Related work  Cascade and bootstrapping for hand crafted features
 Cascade has appeared in various forms dating back to the  NNN0s, as was pointed out by Schneiderman [NN]
It has  been widely used in object detection [N, N, N, NN]
Cascade  can be applied for SVM [N], boosted classifiers [N, NN, NN]
 Chaining of classifiers among cascade stages was called soft  cascade [N] and boosting chain in [NN]
In these approaches,  the detection scores in the previous stages were added to  the detection scores in the current stage
Classifier chaining was effective in face detection [N, NN] and generic object detection [N] for hand crafted features
Bootstrapping  was introduced in the work of Sung and Poggio [NN] in the  mid-NNN0s for training face detection models
Bootstrapping was frequently used when training SVMs for object  detection [N, N]
These works for hand-crafted features did  not learn features and cascaded classifiers jointly
And they  learned cascaded classifiers by multiple separate steps
Different from these works, multiple cascaded classifiers and  features in our CC-Net are learned by the ConvNet with a  one-step training
 Design of better CNN model
Deeper ConvNets were  effective for image classification and object detection [NN,  NN, N0, NN, NN]
On the other hand, wide residual network  [NN], inception modules [NN, N], and multi-region features  [N, N, NN, NN, N0] improved the image classification and  object detection accuracy by increasing the width of ConvNets
Our work is complementary to the works above  that learn better features
CC-Net can use these deeper and  wider networks to obtain diverse features for cascade
 Recent CNN based methods for object detection
Deep  models were found to be effective in object detection  [NN, N0, NN, NN, NN, NN, NN, NN, NN, NN, NN, N0]
Cascade  of ConvNets was found to be effective in region proposal  and region classification [N0, NN, NN]
These works learn  separate CNNs through multiple steps, while our CC-Net  jointly learns all cascaded features and classifiers through  one-step training
Shrivastava et al
introduced online mining of hard positive and negative examples for ConvNetbased detector [NN]
Joint learning of cascaded ConvNets  were proposed in [NN]
The approaches in [NN, NN] cascade  on multiple networks, where each network has its own input image and is used as one cascade stage
In our work,  a single network with one input image has many cascade  stages, which is complementary to the cascade of multiple  networks in [NN, NN]
Early cascade, contextual cascade,  and chaining of scores in multiple stages are not built up in  [NN, NN] for face detection but are used for learning better  features in our CC-Net for generic object detection
Our  approach saves memory and computational time for both  training and testing
In comparison, the approach in [NN]  only saves time for testing and the approach in [NN] only  saves time for backward-propagation during training
 N
The CC-Net for object detection  N.N
Overview of the CC-Net  Brief introduction of the fast RCNN
This paper adopts  the fast RCNN framework for illustrating the CC-Net for  object detection
In fast RCNN, N) a set of regions of interest (RoIs) are generated by a region proposal approach; N)  CNN feature maps for the input image are generated by several convolutional layers; N) the roi-pooling layer projects  the RoIs onto the CNN feature maps and extracts feature  maps of the same size for RoIs of different sizes; N) the  layers after roi-pooling are conducted to obtain the final features, from which the classification scores and the regressed  coordinates for bounding-box relocalization are predicted
 Fig
N shows an overview of the CC-Net
The existing approaches in [NN, NN] are used for generating RoIs in  our implementation
Based on the fast RCNN framework,  it uses several convolutional layers for extracting convolutional features from the image and then use roi-pooling for  projecting features of RoIs into the fixed size
 At the early cascade stage, features from shallow layers  are used for rejecting RoIs belonging to background by multiple cascade stages
In each stage, features are roi-pooled  from the image feature map
The roi-pooled features are  then used for classification
At a stage, the RoIs rejected by  previous stages do not have their features roi-pooled from  NNNN    image convolution on image  loss function for   training softmax  softmax  softmax  detection  results  chained class   scores  ..
..
 chained CNN   features for RoI  ..
roi-pooling  Ro I  rejected  RoI  contextual cascade  RoIs  ..
 rejected  RoI  Classifier chaining with multiple   cascade stages  remaining   RoIsfeatures remaining   RoIs  detection scores  classifier  classifier  classifier  early cascade  Figure N
Overview of the CC-Net
Several convolutional layers are used on the input image
At the early cascade stage, roi-pooled  features are used for rejecting easy background samples
Then roi-pooling is used for obtaining features of different layers, resolutions  and contextual regions at the contextual cascade stage for further rejecting easy background samples
Classifiers chaining is used in both  early cascade and contextual cascade stages for both training and testing
Feature chaining is used for passing the information from one  contextual region to another at the contextual cascade stage
Bounding box regression and all cascaded classifiers are jointly learned
Best  viewed in color
 image feature map for classification
 At the contextual cascade stage, the RoIs not rejected in  the early cascade stage are used for obtaining features of  different contextual regions
These features are then used  by the chained features and classifiers with multiple cascade  stages for further rejecting easy negative samples
If a RoI  is not rejected by the cascade, its final classification score is  used as the detection score
 The major modifications to fast RCNN are as follows:  • Classifiers with several cascade stages are used for ob- ject detection
At each cascade stage, many background  RoIs are rejected
RoIs not rejected go to the next stage
 By classifier chaining, classification scores in previous  stages are used for classification in the current stage
 • Classifiers at different stages use different features
These CNN features can be different in depth, learned  parameters, resolution and contextual regions
 • The features in previous stages can be chained with the features at the current stage
With this chaining, the  features at previous stages serve as the prior knowledge  for the features at the current stage
 • The bounding box regressor, feature chaining, classifier chaining in both early cascade and contextual cascade  stages are learned end-to-end through back-propagation  from a loss function
 In our implementation, the BN-Net in [NN] is used as the  baseline ConvNet if not specified
Fig
N shows the CCNet based on the BN-Net
If only single stage of features  and classifiers is used and the early cascade stage is removed, then Fig
N becomes a fast RCNN implementation  of the BN-Net
There are ten inception modules in the BNNet
The roi-pooling layer is placed after the second, third,  fourth and seventh module, which are denoted by icp(Nb),  (Nc), (Na) and (Nd) in Fig
N
In the early cascade stage, the  roi-pooling for icp(Nb), (Nc), and (Na) use the same contextual region as the tight RoI
At stage N, the features from  icp(Nb) is used for obtaining classification score
At stage  N, if the RoI is not rejected at stage N, then the roi-pooled  features from icp(Nc) for this RoI is used for classification
 Similarly for icp(Na) at stage N
The remaining RoIs after  stage N are then used for contextual cascade
In the contextual cascade stage, the roi-pooling from icp(Nd) is used for  obtaining features of different resolutions and contextual regions
The features after roi-pooling from icp(Nd) for stage t is denoted by ht, t = N, N, N, N
At stage t, the features in ht go through the modules icp(Ne)t, (Na)t, (Nb)t and global average pooling for obtaining features ft
Then these features  are combined by feature chaining, with details in Section  N.N
The chained features are then used by chained classifiers with multiple cascade stages for detecting objects, with  details in Section N.N
 N.N
Feature chaining  N.N.N Preparation of features with diversity  Classifiers in different cascade stages can use different features
Multi-region, multi-context features were found to be  effective in [N, N, NN]
In order to obtain features with diversity, we apply roi-pooling from image features using different contextual regions and resolutions
In our CC-Net,  the roi-pooled features have the same number of channels  but have different sizes at different stages
The sizes of roipooled features are respectively NN× NN, NN× NN, NN× NN and NN × NN for features at stages N, N, N and N
These sizes are heuristically selected to have features with different contexts
The contextual regions for these features are  NNN0    Image  icp  (Nd)  icp  (Ne)N(Na)N(Nb)N  icp  (Ne)N(Na)N(Nb)N  icp  (Ne)N(Na)N(Nb)N  icp  (Ne)N(Na)N(Nb)N  global_poolN  global_poolN  global_poolN  global_poolN  fN  fN  fN  fN  feature  chaining  (Section N.N)  Ro I  icp  (Nb)  icp  (Nc)  icp  (Na) .....
 roi-pool N  roi-pool N-N  roi-pool N  roi-pool N  early cascade  Classifier chaining with multiple cascade stages   (Section N.N)  contextual cascade  Flow of RoI  Flow of feature   and score  RoIs  Classifier chaining with   multiple cascade stages   (Section N.N)  Figure N
An example of the CC-Net for the BN-Net [NN]
There are ten inception modules in the BN-Net
Each inception module consists  of several convolutional layers
icp(Nb), (Nc), (Na), (Nd), (Ne), (Na), (Nb) in the figure are respectively the second, third, fourth, seventh,  eighth, ninth and tenth inception modules of the BN-Net
Best viewed in color
 ..
 Figure N
The use of roi-pooling to obtain features with different  resolutions and contextual regions
After roi-pooling, features in  different stages have different sizes and contextual padding value  c
The original box size is used when c = 0
And N.N times the original box size is used when c = 0.N
Best viewed in color
 also different
Suppose the RoI has width W and height H 
Denote c as the context padding value for the RoI
The padded region has the same center as the RoI and has width  (N+c)·W and height (N+c)·H 
c = 0, 0.N, 0.N, and N.N for stages N, N, N, and N respectively
Fig
N shows the contextual regions for features at different stages
These features  are arranged with increasing contextual regions
 After features of different resolutions and contextual regions are obtained, they go through the remaining three inception modules (Ne), (Na) and (Nb)
In order to increase  the variation of features, the inception modules at different  cascade stages have different parameters
Denote the inception module (Ne) at stage t by (Ne)t
The modules (Ne)N, (Ne)N, (Ne)N, and (Ne)N are initialized from the same pretrained inception module (Ne) but have different parameters  during the finetuning stage because they receive different  gradients in backpropagation
The treatment for the module (Ne)t are also applied for the inception modules (Na)t and (Nb)t
The CNN features obtained from the inception  modules (Nb)t have different sizes
We use global average  pooling for these features so that they have the same size  before feature chaining
 N.N.N The feature chaining structure  Denote the features at depth l and stage t as hl,t
In order to use the features in previous stages as the prior knowledge  when learning features for stage t, we design the feature chaining which has the following formulation:  hl,t = ψ(hl,t−N,ol,t), (N)  ol,t = φ(hl−N,t,Θl−N,t), (N)  where Θl−N,t contains the parameters learned from the network
In this design, the hl,t is obtained from the  features in previous stages hl,t−N and features from the  shallower layer hl−N,t
ol,t denotes the features after  nonlinear mapping of hl−N,t
The nonlinear mapping in  φ(hl−N,t,Θl−N,t) in (N) can be implemented by convolu- tional layer or fully connected layer with nonlinear activation function
ψ(hl,t−N,ol,t) can be implemented by concatenation, i.e
ψ(hl,t−N,ol,t) = Concat(hl,t−N,ol,t), where Concat is the feature concatenation operation
As another choice, ψ(hl,t−N,ol,t) in (N) can also be im- plemented by weighted averaging, i.e
ψ(hl,t−N,ol,t) = hl,t−N+al,t⊙ol,t
The operation ⊙ denotes the Hadamard product, where [αN αN] ⊙ [βN βN] = [αNβN αNβN]
al,t−N denotes a vector of scalers for scaling the ol,t
 In our implementation for the BN-Net as shown in Fig
 N, feature chaining is placed after the global average pooling, where all features are spatially pooled to have spatial  size N × N and N0NN channels
Denote the features af- ter global pooling for stage t as ot
The following proce- dure can be used for obtaining the chained features when  weighted averaging is used:  fN = oN,  fN = oN ⊙ aN + fN,  fN = oN ⊙ aN + fN,  fN = oN ⊙ aN + fN
 (N)  In this implementation, the feature ft at stage t is obtained by summing up features ft−N in the previous stage and the  NNNN    inception  (Ne)N(Na)N(Nb)N  inception  (Ne)N(Na)N(Nb)N  inception  (Ne)N(Na)N(Nb)N  inception  (Ne)N(Na)N(Nb)N  global_poolN  global_poolN  global_poolN  global_poolN  inception  (Nd)  +  fN  fN aN  + fN  + fN  aN  roi-pooling  aN  Figure N
Chaining features for BN-Net
 ot ⊙ at, which which is the output from the previous layer global poolt weighted by at
The summed features ft, t = N, N, N, N are then used for classification
 N.N.N Discussion  Feature chaining includes the concept of stage
Features  hl,t and hl,t+N have the same depth but are different in  stages
Features in different stages have specific objectives  – they are used by classifiers for rejecting easy background  samples
The features of the same depth but different stages  communicate through feature chaining
 If the feature at the current stage is helpful for detection,  its learned weight should be high
Otherwise, the weight is  low
Thus the weight controls the amount of information to  be transmitted for better detection accuracy
 With feature chaining, features at the current stage take  the features in previous stages into consideration
Therefore, the CNN layers at the stage t no longer need to rep- resent the information existing in previous stages
Instead,  they will focus on representations that are complementary  to those in previous stages
 N.N
Classifier chaining in CC-Net  N.N.N Cascade with classifier chaining  This section briefly introduces cascade and chaining of binary classifiers, which is called soft cascade in [N] and  boosting chain in [NN]
 Classifier chaining
Denote ft as the features for the classifier at stage t, t = N, N, 


, T 
Denote ct(ft) as the classi- fication function for the feature ft at the stage t
The partial sum of classification scores up to and including the tth stage is defined as follows:  pst = ∑  i=N,...,t  ct(ft)
(N)  In classifier chaining, the partial sum pst of classification scores are obtained
 Cascade after classifier chaining
In the cascade, the  partial sum pst is compared with the threshold rt
If pst < r, then the sample is not considered as an object
Otherwise, the next stage of comparison is performed
If  the sample is not rejected after T stages of such rejection scheme, the score psT will be used as the detection score
 The main difference between cascade with classifier chaining and conventional cascade is that conventional cascade  only uses ct(ft) as the score at the stage t but cascade with classifier chaining includes the previous scores
 N.N.N Classifier chaining at the testing stage in CC-Net  In the CC-Net, the partial sum of classification scores up  to and including the tth stage is obtained from the set of features {ft} as follows:  p̃t = [pt,N 


pt,K+N] T =  ∑  i=N,...,t  (bt ⊙ ct(ft)) 
(N)  The ct(ft) in (N) denotes the K + N-class classifier which takes the feature ft as input and outputs K + N classifica- tion scores on the input sample being one of the K classes or background
ct(ft) is implemented using the fully con- nected (fc) layer in the CC-Net
The  ∑  in (N) denotes the  summation over vectors
The operation ⊙ in (N) denotes the Hadamard product
bt for this dot product is the vector  of scaling parameters for controlling the scale of the classification scores
The scores p̃t in (N ) are normalized to  probabilities pt using the softmax function as follows:  pt = [pt,N 


pt,K+N] = softmax(p̃t), (N)  where pt,k = p̃t,k/  K+N ∑  k=N  p̃t,k
(N)  The probabilities pt are used for deciding whether to reject  the given sample or not as follows:  u(pt, rt) =  {  N, if max{pt,N 


pt,K} > rt,  0, otherwise
(N)  If u(pt, rt) = 0, then the sample is considered as a back- ground and the convolutional layers at latter stages are not  used for saving testing time
For example, if a RoI is considered as the negative sample after using its roi-pooled  features from the second inception module, which is the  icp(Nb) in Fig
N, then this RoI is not used by the latter classifiers and the contextual cascade for this RoI is not done  for saving computation
Conservative threshold rt is cho- sen so that many background RoIs are rejected and most of  the foreground RoIs are retained in the cascade stages
If a  RoI is not rejected after T cascade stages, then pT is used as its detection result
Fig
N shows the diagram for cascade chaining at the testing stage
In the CC-Net, cascade  chaining is used for early cascade and contextual cascade
 It is also used between these two stages, i.e
the score from  early cascade is transmitted to the contextual cascade stage
 N.N.N Training CC-Net  A multi-task loss of classification and bounding-box regression is used to jointly optimize the CC-Net
Suppose there  NNNN    softmax  softmax  softmax  softmax  features  pN  pN  pN  pN  u(pN, rN)=0?  u(pN, rN)=0?  u(pN, rN)=0?  u(pN, rN)=0?  rejected  rejected  rejected  rejected  Y  Y  Y  Y  N  N  N  detection   results  cN(fN) bN  summed  class scores  +  cN(fN)  bN  cN(fN) bN +  cN(fN) bN +  fN  fN  fN  fN  Figure N
The chaining of features and classifiers at the testing  stage
Given features ft, an fc layer is used for obtaining classification score at stage t
The classification sores from previous  stages are combined with the scores at the current stage to obtained the summed scores
The summed scores undergo softmax  to obtain the normalized scores pt at stage t
Then the thresholding function u(pt, rt) decides whether to reject the sample or not
The sample not rejected after T stages uses the pT as the detection  result
T = N in the figure
Best viewed in color
 are K object classes to be detected
Denote the set of es- timated class probabilities for a sample by p = {pt|t = N, 


, T}, where pt = [pt,0 


pt,K ] is the estimated prob- ability vector at stage t and pt,k is the estimated probabil- ity for the kth class
k = 0 denotes the background
pt is obtained by a softmax over the K + N outputs of a fc layer
Another layer outputs bounding-box regression offsets l = {lk|k = N, 

.K}, lk = (lkx , l k y , l  k w, l  k h ) for each of  the K object classes, indexed by k
Parameterization for lk  is the same as that in [NN]
The loss function is defined as  follows:  L(p, k∗, l, l∗) = Lcls(p, k ∗) + Lloc(l, l  ∗, k∗), (N)  Lcls(p, k ∗) = −  T ∑  t=N  λtut log pt,k∗ , (N0)  ut = t−N ∏  i=N  [pi,k∗ < ri] when t > N, uN = N
(NN)  Lcls(∗) is the loss for classification and Lloc is the loss for bounding-box regression
If λt = ut = N and T = N, then Lcls(∗) is a normal cross entropy loss
ut evaluates whether the sample is rejected in previous stages
If a sample is rejected in previous stages, it is no longer used for learning  the classifier in the current stage
And this sample will not  be used for extracting its features in the latter CNN layers
 Since we did not constrain the sample to be background for  rejection, easy positive samples are also rejected at early  stages during training
λt is a hyper parameter that controls the weight of loss for each stage of cascaded classifier
We  set λT = N and λt = 0.0N/T for t = N, 


T − N
Loss is used for t = N, 


T − N so that the learned classifiers in these stages can learn reasonable classification scores for  rejecting background samples
Since the score in the last  classifier is used as the final detection score, the classification loss in the last stage has much higher weight than  the loss in other stages
For Lloc, we use the smoothed LN loss in [N0]
With this loss function, bounding box re- gression, chained features and all cascaded classifiers are  learned jointly through backpropagation
 N
Experimental results  N.N
Experimental setup  The CC-Net is implemented based on the fast RCNN  pipeline
The BN-Net is used as the baseline network if not  specified
The CC-Net for BN-Net [NN] is shown in Fig
N
 In the CC-Net, the layers belonging to the baseline networks  are initialized by these baseline networks pre-trained on the  ImageNet dataset
The parameters at in feature chaining  and bt cascade chaining are initialized as N
For region proposal, we use the Craft in [NN] for ImageNet and the selective search in [NN] for VOC N00N if not specified
The Craft  in [NN] is an improved approach based on the faster RCNN  [NN]
 We evaluate our method on two public object detection  datasets, ImageNet [NN] and PASCAL VOC N00N [N]
Since  the ImageNet object detection task contains a sufficiently  large number of images and object categories to reach a conclusion, evaluations on component analysis of our training  method are conducted on this dataset
This dataset has N00  object categories and consists of three subsets
i.e., train,  validation and test data
We follow the same setting in [NN]  and split the whole validation subset into two subsets, valN  and valN
The network finetuning step uses training samples from train and valN subsets
The valN subset is used  for evaluating components
All our results are for single  model with single-scale training and testing if not specified
 Single-stage bounding box regression is used
 N.N
ImageNet results  On this dataset, we compare with the methods tested on  the valN dataset
We compare our framework with several  other state-of-art approaches [NN, NN, NN, N0, NN, NN, NN]
 The mean average precision for these approaches are shown  in Table N
Our work is trained using the provided data  of ImageNet
Compared with these approaches, our single  model result ranks No
N
N  N.N
PASCAL VOC N00N results  On this dataset, the VOC0N and VOCNN trainval dataset  are optionally used for training and the VOC0N test set is  used for evaluation
 When only VOC0N is used for training, as shown in Table N, the baseline BN-Net+FRCN has mAP N0.N%
Adding  our design in chaining features and cascaded classifiers,  NThe ResNet result with mAP N0.N% in [NN] used multi-scale testing,  bounding box voting and contextual scores
Without them but with the  same region proposal from Craft, the ResNet-NNN has mAP NN% and our  CC-Net based on BN-Net has mAP NN.N% 
 NNNN    appraoch RCNN Berkeley GoogleNet DeepID-Net Superpixel ResNet FRCN GBD-Net CC-Net  [NN] [NN] [NN] [N0] [NN] [NN] [N0] [NN]  valN(sgl) NN.0 NN.N NN
N NN.N NN.N N0.N NN.N NN.N NN.N  Table N
Object detection mAP (%) on ImageNet valN for state-of-the-art approaches with single model
FRCN and CC-Net use the same  region proposal [NN]
BN-Net is used for FRCN and CC-Net is based on BN-Net
 method M N R train set mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv  FRCN [N0] V 0N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.0 NN.0 N0.N NN.N N0.N NN.N NN.N  OHEM [NN] V 0N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours V 0N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N  FRCN [N0] V 0N+NN N0.0 NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N N0.N NN.N N0.N N0.N  MRCN [N] � V � 0N+NN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N  OHEM [NN] � V 0N+NN NN.N N0.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N N0.N  ION [N] � V 0N+NN NN.N N0.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  Ours � V 0N+NN N0.N NN.0 NN.N N0.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N  FRCN [N0] B 0N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.0 NN.0 N0.N NN.N N0.N NN.N NN.N  Ours B � 0N NN.N NN.0 NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N  Table N
VOC N00N test detection average precision (%)
All methods use selective search for region proposal
Training set key: 0N:  VOC0N trainval, 0N+NN: union of 0N and VOCNN trainval
Multi-region features are not used for our CC-Net for VGGNN
Legend: N: using VGGNN (V) or BN-Net (B) as the baseline network
R: whether the multi-region features are used
M: whether multi-step bounding  box regression and the multi-scale is used for training and testing
 the mAP is NN.N%
The baseline VGGNN+FRCN has mAP  NN.N%, our CC-Net based on VGGNN has mAP NN.N%
For  VGGNN, our CC-Net did not use multi-region features but  only use the features at convN-N, convN-N and convN-N for  chaining
The ION net in [N] also used convN-N, convN-N  and convN-N features
The IRNN structure for using context and segmentation labels are used in ION but not in our  model for VGG
Our CC-Net based on VGG provides N.N%  absolute mAP gain compared with ION based on VGG
We  also list the hard example example mining approach in [NN]  and the multi-region approach in [N] for comparison
The  results show that our model performs better
 CC-Net is independent of detection pipeline like fast  RCNN or Faster RCNN
When CC-Net is applied for  the Faster RCNN, the baseline VGGNN+Faster-RCNN  has mAP NN.N% when trained on VOC0N+NN and tested  on VOC0N, the VGGNN+Faster-RCNN+CC-Net has mAP  N0.N% when multi-scale training and multi-step bounding  box regression are used
Multi-region features are not used  in CC-Net for the results above
 N.N
Component analysis  N.N.N Results on cascade chaining  In order to evaluate the performance gain from chaining cascaded classifiers, we use the BN-Net as the baseline
Multicontext multi-resolution features are not included
For the  results in Table N, all cascaded classifiers take the output the  global pool layer in BN-Net as the feature
Features and  classifiers are jointly learned
Compared with the baseline,  adding two extra stages of cascaded classifiers improves the  mAP by N.N%, and adding four extra cascaded classifiers  improves mAP by N.N%
The use of more cascaded classifiers provides better detection accuracy
If the N extra stages  of cascade do not use chaining, i.e
not using previous classification scores for the current classification score, there  + N cascade stages? �  + N cascade stages? � �  cascade? � � �  chaining classifier? � �  mAP NN.N N0.N N0.N N0.N  Table N
ImageNet valN detection mean average precision (%) for  baseline BN-Net with different setup on cascade
‘chaining classifier’ denotes the result using the chaining for classifier, in which  scores in previous stages are used for the current stage
‘cascade’  denotes the use of cascade
 will be 0.N% mAP drop
 N.N.N Chaining features and classifiers  Table N shows the performance for different settings in  chaining features and classifiers for VOC0N and ImageNet
 The results for VOC0N are trained on VOC0N train+val
 The results for ImageNet valN are trained on ImageNet  train+valN
The baseline BN-Net has mAP NN.N%(N0.N%)  for ImageNet(VOC0N)
Multi-region features are found to  be effective in [N]
When we concatenate features of different contextual regions and resolutions but without the  feature chaining or the classification cascade, the mAP is  N0.N%(NN.N%) for ImageNet(VOC0N)
This setting has the  same depth/width as the CC-Net but does not include classifier or feature chaining
When multi-region features and  different classifiers are used in different cascade stages, the  network has mAP NN.N% on VOC N00N if there is no chaining in the cascade
 Based on the multi-region features, cascade chaining provides 0.N%(N.N%) absolute mAP gain for ImageNet(VOC0N)
Based on the multi-region features features, mAP is NN.N%(NN.N%) if both feature chaining and  cascade chaining are used in the CC-Net
 In the experimental results, the early cascade stage is  used for the results in VOCN00N but not used for the results  NNNN    multi-region features? � � �  cascade? � �  classifier chaining? � �  features chaining? �  mAP (VOC N00N) N0.N NN.N NN.N NN.N  mAP (ImageNet) NN.N N0.N NN.N NN.N  Table N
VOC N00N test and ImageNet valN detection mean average precision (%) for baseline BN-Net with different settings on  feature chaining and classifier chaining
Early cascade is used for  the results on VOC N00N but not for the results on ImageNet
 in ImageNet valN
This might be the reason on the different  improvement in these two datasets for chaining cascaded  classifier
 When learning the chaining of features and classifiers,  scaling vectors a and b are used for controling the scales of  features and classification scores, if these scalers are fixed  as N but not learned, the mAP will drop by N.N%
No mAP  gain is observed when the scaling vector a for feature chaining with CN parameters is replaced by fully connected layer with CNCN parameters
 N.N
Experimental comparison with GBD-Net  Tested on the ImageNet valN, CC-Net has mAP NN.N%
 In comparison, the GBD-Net in [NN] has mAP NN.N%
CCNet performs better than GBD-Net by learning more complementary features
GBD-Net aims for context
Our feature chaining aims for learning complementary features and  is not constrained to contextual information
For the CNN  model in Fig
N, if all features in the inception modules  icp(Ne)t, (Na)t and (Nb)t for t = N, N, N, N have the same contextual region, GBD-Net has N0.N% mAP while the CCNet has NN% mAP
This experiment shows that CC-Net  learns complementary features even if the contextual regions are the same
But GBD-Net, which aims at passing  message among contextual features, does not provide extra  gain when the contextual regions is the same
 In GBD-Net [NN], when a message is passed from a layer  with CN channels to another layer with CN channels, the computational cost and the number of parameters is proportional to CNCN
The computational cost and the number of parameters of which are proportional to CN when passing message from a layer to another
Therefore, feature chaining saves the computation and parameter size in message  passing by CN times
CN = CN = N0NN in the BN-Net
On the other hand, the feature chaining increases the number of parameters when the inception module icp(Ne)N and  icp(Ne)N do not share parameters in our implementation but  share parameters in the GBD-Net
But the computation and  parameter sizes required in icp(Ne)- icp(Nb) are smaller than  that for message passing
Using the same BN-Net as baseline, GBD-Net and CC-Net require N0NM and NNM parameters respectively
 N.N
Computational complexity and memory cost  We evaluate the computational complexity using the selective search for the region proposal on Titan X GPU
The  Stage Initial Early cascade Contextual cascade  Recall NN.N% NN.0 % NN.N%  BgRoI Rej
0 NNN NNN  Table N
The recall rate (%) and number of background RoIs rejected (BgRoI Rej) on VOC0N
 training time required for batch size NNN is 0.NN and N.N  seconds per iteration respectively for CC-Net without cascade and GBD-Net
The training time required for batch  size N0NN is N.NN seconds per iteration for CC-Net with cascade
When the batch size increases by N times, the computational time increases only by 0.NN times
For single image with NNN RoIs, the GBD-Net run out of GPU memory  on the NNGB GPU
For NNN RoIs, CC-Net without cascade  runs out of memory
At the testing stage, the GBD-Net and  CC-Net without cascade require NN and N seconds per image respectively
With simpler design in passing messages  among features, CC-Net without cascade is faster
The CCNet with cascade requires N.N seconds per image, around  NN% of the time required by GBD-Net and NN% the time  required by CC-Net without cascade
 The recall rate (%) and number of background RoIs  (BgRoI) rejected in different stages are shown in Table N
 Initially there are NNN0 background RoIs per image, the  early cascade stage rejects NNN RoIs and the contextual cascade further rejects NNN RoIs
 N
Conclusion  In this paper, we present a chained cascade network (CCNet) for object detection
In this network, the cascaded  classifiers in multiple stages are jointly learned through a  single end-to-end neural network
This network includes  classifier chaining and feature chaining, in which feature  and classifier at a stage take the feature and classification  scores in previous stages as the prior knowledge
By rejecting easy examples at earlier stages, the features and classifiers learned at latter stages focus more on hard examples  for better detection accuracy
After an easy sample is rejected at earlier stage in shallow layer, its computation for  deeper or wider layers is not required for faster speed
Experimental results on Pascal VOC and ImageNet for different region proposals show the effectiveness of the CC-Net  in improving the detection accuracy
 Acknowledgement This work is supported by Sense- Time Group Limited, the General Research Fund spon- sored by the Research Grants Council of Hong Kong (Project Nos
CUHKNNNNNNNN, CUHKNNN0NNNN, CUHKNNN0NNNN, CUHKNNNNNN, CUHKNNN0N0NN, CUHKNNN0NNNN, and CUHKNNNNNNNN), the Hong Kong Innovation and Technology Support Programme (No.ITS/NNN/NNFX), National Natural Science Foundation of China (No
NNNNNNNN), and ONR N000NN-NN-N- NNNN
 NNNN    References  [N] S
Bell, C
L
Zitnick, K
Bala, and R
Girshick
Insideoutside net: Detecting objects in context with skip pooling  and recurrent neural networks
In CVPR, N0NN
N, N, N, N  [N] L
Bourdev and J
Brandt
Robust object detection via soft  cascade
In CVPR, N00N
N, N  [N] F
Chollet
Xception: Deep learning with separable convolutions
arXiv preprint arXiv:NNN0.0NNNN, N0NN
N  [N] N
Dalal and B
Triggs
Histograms of oriented gradients for  human detection
In CVPR, N00N
N, N  [N] P
Dollár, R
Appel, S
Belongie, and P
Perona
Fast feature pyramids for object detection
IEEE Trans
PAMI,  NN(N):NNNN–NNNN, N0NN
N  [N] M
Everingham, L
V
Gool, C
K
I.Williams, J.Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
IJCV, NN(N):N0N–NNN, N0N0
N  [N] P
Felzenszwalb, R
Girshick, and D
McAllester
Cascade  object detection with deformable part models
In CVPR,  N0N0
N  [N] P
Felzenszwalb, R
B
Grishick, D.McAllister, and D
Ramanan
Object detection with discriminatively trained part  based models
IEEE Trans
PAMI, NN:NNNN–NNNN, N0N0
N  [N] S
Gidaris and N
Komodakis
Object detection via a multiregion and semantic segmentation-aware cnn model
In  ICCV, N0NN
N, N, N, N  [N0] R
Girshick
Fast r-cnn
In CVPR, N0NN
N, N, N, N  [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
N, N, N, N  [NN] R
Girshick, F
Iandola, T
Darrell, and J
Malik
Deformable  part models are convolutional neural networks
In CVPR,  N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N, N, N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 ICML, N0NN
N, N, N, N  [NN] A
Krizhevsky, I
Sutskever, and G
Hinton
Imagenet classification with deep convolutional neural networks
In NIPS,  N0NN
N  [NN] H
Li, Z
Lin, X
Shen, J
Brandt, and G
Hua
A convolutional neural network cascade for face detection
In Proceedings of the IEEE Conference on Computer Vision and Pattern  Recognition, pages NNNN–NNNN, N0NN
N  [NN] S
Z
Li and Z
Zhang
Floatboost learning and statistical face  detection
IEEE Trans
PAMI, NN(N):NNNN–NNNN, N00N
N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, and S
Reed
 Ssd: Single shot multibox detector
In ECCV, N0NN
N  [NN] W
Ouyang, H
Li, X
Zeng, and X
Wang
Learning deep  representation with large-scale attributes
In ICCV, N0NN
N  [N0] W
Ouyang, X
Wang, X
Zeng, S
Qiu, P
Luo, Y
Tian, H
Li,  S
Yang, Z
Wang, C.-C
Loy, et al
Deepid-net: Deformable  deep convolutional neural networks for object detection
In  CVPR, N0NN
N, N, N  [NN] W
Ouyang, X
Zeng, and X
Wang
Learning mutual visibility relationship for pedestrian detection with a deep model
 IJCV, NN0(N):NN–NN, N0NN
N  [NN] W
Ouyang, X
Zeng, X
Wang, S
Qiu, P
Luo, Y
Tian, H
Li,  S
Yang, Z
Wang, H
Li, K
Wang, J
Yan, C.-C
Loy, and  X
Tang
Deepid-net: Deformable deep convolutional neural networks for object detection
IEEE Trans
PAMI, page  accepted, N0NN
N  [NN] H
Qin, J
Yan, X
Li, and X
Hu
Joint training of cascaded  cnn for face detection
In CVPR, N0NN
N  [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNN–NNN, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards real-time object detection with region proposal networks
NIPS, N0NN
N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
Imagenet large scale visual recognition challenge
IJCV, N0NN
N  [NN] H
Schneiderman
Feature-centric evaluation for efficient  cascaded object detection
In CVPR, N00N
N  [NN] P
Sermanet, D
Eigen, X
Zhang, M
Mathieu, R
Fergus,  and Y
LeCun
Overfeat: Integrated recognition, localization  and detection using convolutional networks
arXiv preprint  arXiv:NNNN.NNNN, N0NN
N  [NN] A
Shrivastava, A
Gupta, and R
Girshick
Training regionbased object detectors with online hard example mining
In  CVPR, N0NN
N, N, N  [N0] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N, N  [NN] A
Smeulders, T
Gevers, N
Sebe, and C
Snoek
Segmentation as selective search for object recognition
In ICCV,  N0NN
N, N  [NN] K.-K
Sung and T
Poggio
Learning and example selection  for object and pattern detection
MIT A.I
Memo No
NNNN,  (NNNN), NNNN
N, N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
N, N, N  [NN] P
Viola and M
Jones
Rapid object detection using a boosted  cascade of simple features
In CVPR, N00N
N  [NN] R
Xiao, L
Zhu, and H.-J
Zhang
Boosting chain learning  for object detection
In ICCV, N00N
N, N  [NN] J
Yan, Y
Yu, X
Zhu, Z
Lei, and S
Z
Li
Object detection  by labeling superpixels
In CVPR, N0NN
N, N  [NN] B
Yang, J
Yan, Z
Lei, and S
Z
Li
Craft objects from  images
In CVPR, N0NN
N, N, N  [NN] S
Zagoruyko and N
Komodakis
Wide residual networks
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] X
Zeng, W
Ouyang, and X
Wang
Window-object relationship guided representation learning for generic object detections
arXiv preprint arXiv:NNNN.0NNNN, N0NN
N  [N0] X
Zeng, W
Ouyang, J
Yan, H
Li, T
Xiao, K
Wang, Y
Liu,  Y
Zhou, B
Yang, Z
Wang, et al
Crafting gbd-net for object  detection
TPAMI, N0NN (accepted)
N  [NN] X
Zeng, W
Ouyang, B
Yang, J
Yan, and X
Wang
Gated  bi-directional cnn for object detection
In ECCV, N0NN
N, N,  N, N, N, N  NNNNIdentity-Aware Textual-Visual Matching With Latent Co-Attention   Identity-Aware Textual-Visual Matching with Latent Co-attention  Shuang Li, Tong Xiao, Hongsheng Li∗, Wei Yang, and Xiaogang Wang∗  Department of Electronic Engineering, The Chinese University of Hong Kong  {sli,xiaotong,hsli,wyang,xgwang}@ee.cuhk.edu.hk  Abstract  Textual-visual matching aims at measuring similarities  between sentence descriptions and images
Most existing methods tackle this problem without effectively utilizing identity-level annotations
In this paper, we propose an  identity-aware two-stage framework for the textual-visual  matching problem
Our stage-N CNN-LSTM network learns  to embed cross-modal features with a novel Cross-Modal  Cross-Entropy (CMCE) loss
The stage-N network is able to  efficiently screen easy incorrect matchings and also provide  initial training point for the stage-N training
The stage-N  CNN-LSTM network refines the matching results with a latent co-attention mechanism
The spatial attention relates  each word with corresponding image regions while the latent semantic attention aligns different sentence structures  to make the matching results more robust to sentence structure variations
Extensive experiments on three datasets  with identity-level annotations show that our framework  outperforms state-of-the-art approaches by large margins
 N
Introduction  Identifying correspondences and measuring similarities  between natural language descriptions and images is an important task in computer vision and has many applications,  including text-image embedding [NN, NN, NN, NN, NN], zeroshot learning [NN, NN, N], and visual QA [N, N, NN, NN, NN]
 We call such a general problem textual-visual matching,  which has drawn increasing attention in recent years
The  task is challenging because the complex relations between  language descriptions and image appearance are highly  non-linear and there exist large variations or subtle variations in image appearance for similar language descriptions
 There have been large scale image-language datasets and  deep learning techniques [NN, NN, N, NN, N0] proposed for  textual-visual matching, which considerably advanced research progress along this direction
However, identitylevel annotations provided in benchmark datasets are ignored by most existing methods when performing matching  across textual and visual domains
 ∗Corresponding authors  A woman dressed in   white with black belt
  A young lady is wearing   black suit and high hells
 The girl wears a blue   jacket and black dress
  Identity N Identity N Identity N   Figure N
Learning deep features for textual-visual matching with  identity-level annotations
Utilizing identity-level annotations  could jointly minimize intra-identity discrepancy and maximize  inter-identity discrepancy, and thus results in more discriminative  feature representations
 In this paper, we propose a two-stage framework for  identity-aware textual-visual matching, which consists of  two deep neural networks
The stage-N network learns  identity-aware feature representations of images and language descriptions by introducing a Cross-Modal CrossEntropy (CMCE) loss to effectively utilize identity-level annotations for feature learning (see Figure N)
After training, it provides initial matching results and also serves as  the initial point for training stage-N network
The stage-N  deep neural network employs a latent co-attention mechanism that jointly learns the spatial attention and latent semantic attention to match salient image regions and latent  semantic concepts for textual-visual affinity estimation
 Our stage-N network consists of a CNN and a LSTM  for learning textual and visual feature representations
The  objective is to minimize the feature distances between descriptions and images belonging to the same identities
The  stage-N network utilizes a specialized CMCE loss with dynamic buffers, which implicitly minimizes intra-identity  feature distances and maximize inter-identity feature distances over the entire dataset instead of just small minibatches
In contrast, for the pairwise or triplet loss functions, the probability of sampling hard negative samples  during training decreases quadratically or cubically as the  number of training sample increases
 NNN0    The trained stage-N network is able to efficiently screen  easy incorrect matchings for both training and testing
 However, one limitation of the CMCE loss in stage-N is that  the generated textual and visual features are not tightly coupled
A further refinement on stage-N results is essential for  obtaining accurate matching results
Our stage-N network  is a tightly coupled CNN-LSTM network with latent coattention
It takes a pair of language description and image  as input and outputs the textual-visual matching confidence,  which is trained with the binary cross-entropy loss
 Conventional RNNs for language encoding have difficulty in remembering the complete sequential information  when the input descriptions are too long
It tends to miss  important words appearing in the beginning of the sentence
 The RNN is also variant to different sentence structures
 Sentences describing the same image but with different sentence structures could be represented by features with large  differences
For instance, “the girl who has blond hair is  wearing a white dress and heels” and “The girl wears heels  and a white dress
She has blond hair.” Both sentences describe the same person but the first one might focus more  on “white dress and heels”, and the second one might assign “blond hair” with higher weights
Inspired by the word  alignment (attention) technique in neural machine translation [N], a latent co-attention mechanism is proposed for the  stage-N CNN-LSTM network
The visual spatial attention  module associates word to its related image regions
The  latent semantic attention module aligns different sentence  structures with an alignment decoder LSTM
At each step  of the LSTM, it learns how to weight different words’ features to be more invariant to sentence structure variations
 The contribution of this paper is three-fold
N) We propose a novel identity-aware two-stage deep learning framework for solving the problem of textual-visual matching
 The stage-N network can efficiently screen easy incorrect  matchings and also acts as the initial point for training  stage-N network
The stage-N network refines matching results with binary classification
Identity-level annotations  ignored by most existing methods are utilized to learn better  feature representations
N) To take advantage of the identitylevel annotations, our stage-N network employs a specialized CMCE loss with feature buffers
Such a loss enables  effective feature embedding and fast evaluation
N) A novel  latent co-attention mechanism is proposed for our stage-N  network
It has a spatial attention module that focuses on  relevant image regions for each input word, and a latent semantic attention module that automatically aligns different  words’ feature representations to minimize the impact of  sentence structure variations
 N
Related Work  N.N
Visual matching with identity-level annotations  Visual matching tasks with identity-level annotations,  such as person re-identification [NN, NN, N, NN, NN, NN, NN]  and face recognition [NN, NN], are well-developed research  areas
Visual matching algorithms either classify all the  identities simultaneously [NN, NN, NN] or learn pair-wise  or triplet distance loss function [N, NN, NN, N] for feature  embedding
However, both of them have major limitations
The first type of loss function faces challenges when  the number of classes is too large
The limited number  of classes (identities) in each mini-batch leads to unstable  training behavior
For the second type of loss function, the  hard negative training samples might be difficult to sample as the number of training sample increases, and the  computation time of constructing pairs or triplets increases  quadratically or cubically with the number of test samples
 N.N
Textual-visual matching  Measuring similarities between images and languages  aims at understanding the relations between images and  language descriptions
It gains a lot of attention in recent  years because of its wide applications in image captioning [N0, NN, NN, N], visual QA [N, NN, NN, NN], and textimage embedding [N, NN, NN, NN, NN]
Karpathy et al
[NN]  combined the convolutional neural network for image regions and bidirectional recurrent neural networks for descriptions to generate image captions
The word-image  pairwise affinities are calculated for sentence-image ranking
Nam et al
[NN] jointly learned image and language attention models to capture the shared concepts between the  two domains and evaluated the affinity by computing the  inner product of two fixed embedding vectors
[NN] tackled the matching problem with deep canonical correlation  analysis by constructing the trace norm objective between  image and language features
In [NN], Klein et al
presented  two mixture models, Laplacian mixture model and Hybird  Gaussian-Laplacian mixture model to learn Fisher vector  representations of sentences
The text-to-image matching  is conducted by associating the generated Fisher vector and  VGG image features
 N.N
Identity-aware textual-visual matching  Although identity-level annotations are widely used in  visual matching tasks, they are seldom exploited for textualvisual matching
Using such annotations can assist crossdomain feature embedding by minimizing the intra-identity  distances and capturing the relations between textual concepts and visual regions, which makes textual-visual matching methods more robust to variations within each domain
 Reed et al
[NN] collected fine-grained language descriptions for two visual datasets, Caltech-UCSD birds (CUB)  and Oxford-N0N Flowers, and first used the identity-level  annotations for text-image feature learning
In [NN], Li et al
 proposed a large scale person re-identification dataset with  language descriptions and performed description-person  image matching using an CNN-LSTM network with neural attention mechanism
However, these approaches face  the same problems with existing visual matching methods
 NNNN    To solve these problems and efficiently learn textual and visual feature representations, we propose a novel two-stage  framework for identity-aware textual-visual matching
Our  approach outperforms both above state-of-the-art methods  by large margins on the three datasets
 N
Identity-Aware Textual-Visual Matching  with Latent Co-attention  Textual-visual matching aims at conducting accurate  verification for images and language descriptions
However, identity-level annotations provided by many existing textual-visual matching datasets are not effectively exploited for cross-domain feature learning
In this section,  we introduce a novel identity-aware two-stage deep learning  framework for textual-visual matching
The stage-N CNNLSTM network adopts a specialized Cross-Modal CrossEntropy (CMCE) loss, which utilizes identity-level annotations to minimize intra-identity and maximize inter-identity  feature distances
It is also efficient for evaluation because  of its linear evaluation time
After training convergence, the  stage-N network is able to screen easy incorrect matchings  and also provides initial point for training the stage-N CNNLSTM network
The stage-N network further verifies hard  matchings with a novel latent co-attention mechanism
It  jointly learns the visual spatial attention and latent semantic attention in an end-to-end manner to recover the relations  between visual regions and achieves robustness against sentence structure variations
 N.N
Stage-N CNN-LSTM with CMCE loss  The structure of stage-N network is illustrated in Figure  N, which is a loosely coupled CNN-LSTM 
Given an input textual description or image, both the visual CNN and  language LSTM are trained to map the input image and description into a joint feature embedding space, such that  the features representations belonging to the same identity  should have small feature distances, while those of different  identities should have large distances
To achieve the goal,  the CNN-LSTM network is trained with a CMCE loss
 N.N.N Cross-Modal Cross-Entropy Loss  For the conventional pairwise classification loss [N, NN] or  triplet max-margin loss [NN, NN], if there are N identities in the training set, the number of possible training samples  would be O(NN)
It is generally difficult to sample hard negative samples for learning effective feature representations
On the other hand, during evaluation phase, the time  complexity of feature calculation of pairwise or triplet loss  would increase quadratically with N , which would take lots of computation time
To solve this problem, we propose a  novel CMCE loss that efficiently compares a mini-batch of  n identity features from one modality to those of all N iden- tities in another modality in each iteration
Intuitively, the  sampled n identity features are required to have high affini- ties with their corresponding identities in the other modality  The$model$  wears$a$bright$  orange$ dress.$  She$…  LSTM  Visual$  Feature ID Feature  N VN  N VN  N VN  …  Visual*Buffer  Textual  Feature  N  N  N  Input$Image  (Identity$N)  Visual$  CNN  Input$Sentence  (Identity$N)  ID Feature  N SN  N SN  N SN  …  Textual*Buffer  N  StepEN:  CrossEmodal$  CrossEentropy$ Loss  N  StepEN:  Update$buffers  N  Figure N
Illustration of the stage-N network
In each iteration, the  images and text descriptions in a mini-batch are first fed into the  CNN and LSTM respectively to generate their feature representations
The CMCE loss is then computed by comparing sampled  features in one modality to all other features in the feature buffer  of the other modality (Step-N)
The CNN and LSTM parameters  are updated by backpropagation
Finally, the visual and textual  feature buffers are updated with the sampled features (Step-N)
 and low affinities with all other N − n ones in the entire identity set
The cross-modal affinity is calculated as the  inner products of features from the two modalities
By using the proposed loss function, hard negative samples are  all covered in each training epoch and the evaluation time  complexity of sampling all test samples is only O(N)
In each training iteration, a mini-batch of images belonging to n different identities are transformed to visual fea- tures, each of which is denoted by v ∈ RD
D is the feature embedding dimension for both modalities
Textual features  of all N identities are pre-stored in a textual feature buffer S ∈ RD×N , where Si denotes the textual feature of the ith identity
The affinities between a visual feature representation v and all textual features S could then be calculated as ST v
The probability of the input image v matching the ith identity in the textual feature buffer can be calculated with  the following cross-modal softmax function,  pSi (v) = exp (STi v/σv)  ∑N  j=N exp (S T j v/σv)  , (N)  where σv is a temperature hyper-parameter to control how peaky the probability distribution is
Similarly, in each iteration, a mini-batch of sentence descriptions belonging to  n identities are also sampled
Let s ∈ RD denote one text sample’s feature in the mini-batch
All visual features are  pre-stored in a visual feature buffer V ∈ RD×N 
The prob- ability of s matching the kth identity in the visual feature buffer is defined as  pVk (s) = exp (V Tk s/σs)  ∑N  j=N exp (V T j s/σs)  , (N)  NNNN    where σs is another temperature hyper-parameter
In each iteration, our goal is to maximize the above textual and visual matching probabilities for corresponding identity pairs
 The learning objective can then be define as minimizing the  following CMCE loss,  L = − ∑  v  log pStv (v)− ∑  s  log pVts(s), (N)  where tv and ts are the target identities of visual feature v and textual feature s respectively
Its gradients are calcu- lated as  ∂L  ∂v =  N  σv          (pStv − N)Stv +  N ∑  j=N j N=tv  Sjp S j          , (N)  ∂L  ∂s =  N  σs          (pVts − N)Vts + N ∑  j=N j N=ts  Vjp V j          
(N)  The textual and visual feature buffers enable efficient  calculation of textual-visual affinities between sampled  identity features in one modality and all features in the other  modality
This is the key to our cross-modal entropy loss
 Before the first iteration, image and textual features are obtained by the CNN and LSTM
Each identity’s textual and  visual features are stored in its corresponding row in the  textual and visual feature buffers
If an identity has multiple descriptions or images, its stored features in the buffers  are the average of the multiple samples
In each iteration,  after the forward propagation, the loss function is first calculated
The parameters of both visual CNN and language  LSTM are updated via backpropgation
For the sampled  identity images and descriptions, their corresponding rows  in the textual and visual feature buffers are updated by the  newly generated features
If a corresponding identity t has multiple entity images or descriptions, the buffer rows are  updated as the running weighted average with the following  formulations, Stv = 0.NStv+0.Ns and Vts = 0.NVts+0.Nv, where s and v are the newly generated textual and visual features, ts and tv denote their corresponding identities
 Although our CMCE loss has similar formation with  softmax loss function, they have major differences
First,  the CMCE propagates gradients across textual and visual  domains
It can efficiently embed features of the same identity from different domains to be similar and enlarge the distances between non-corresponding identities
Second, the  feature buffers store all identities’ feature representations of  different modalities, making the comparison between minibatch samples with all identities much efficient
 N.N
Stage-N CNN-LSTM with latent co-attention  After training, the stage-N network is able to obtain  initial matching results efficiently because the textual and  word-fc   Visual   CNN   “The” …   word-fc   Encoder   LSTM   “model”   Spatial Attention Module   “wears”   word-fc   “dress”   word-fc   Latent Semantic Attention Module   Decoder   LSTM   Encoder   LSTM   Encoder   LSTM   Encoder   LSTM  …   xN xN xt xT   Decoder   LSTM   Decoder   LSTM  fc   Binary    classifier   Figure N
Illustration of the stage-N network with latent coattention mechanism
The spatial attention associates the relevant  visual regions to each input word while the latent semantic attention automatically aligns image-word features by the spatial attention modules to enhance the robustness to sentence structure  variations
 visual features can be calculated independently for each  modality
However, the visual and text feature embeddings  might not be optimal because stage-N compresses the whole  sentence into a single vector
The correspondences between  individual words and image regions are not established to  capture word-level similarities
Stage-N is also sensitive to  sentence structure variations
A further refinement on the  stage-N matching results is desirable for obtaining accurate  matching results
For stage N, we propose a tightly coupled  CNN-LSTM network with latent co-attention mechanism,  which takes a pair of text description and image as inputs  and outputs their matching confidence
Stage-N framework  associates individual words and image regions with spatial  attention to better capture world-level similarities, and automatically realigns sentence structures via latent semantic  attention
The trained stage-N network serves as the initial  point for the stage-N network
In addition, it screens easy  negatives, so only the hard negative matching samples from  stage-N results are utilized for training stage-N
With stageN, stage-N can focus on handling more challenging samples  that have most impact on the final results
 The network structure for stage-N network is shown in  Figure N
The visual feature for the input image is obtained  by a visual CNN
Word features are generated by the encoder LSTM
At each word, a joint word-image feature is  obtained via the spatial attention module, which relates the  word feature to its corresponding image regions
A decoder  LSTM then automatically aligns encoded features for the  words to enhance robustness against sentence structure variations
The output features of the decoder LSTM is utilized  to obtain the final matching confidence
The idea of spatial and latent semantic co-attention was for the first time  proposed and the network is accordingly designed
Unlike  LSTM decoders for NLP [N, NN], whose each step corresponds to a specific output word, each step of our semantic  NNNN    decoder captures a latent semantic concept and the number  of steps is predefined as the number of concepts
 N.N.N Encoder word-LSTM with spatial attention  For the visual CNN and encoder LSTM, our goal is to generate a joint word-visual feature representation at each input  word
The naive solution would be simply concatenating  the visual feature with word feature at each word
However,  different words or phrases may relate more to specific visual regions instead of the overall image
Inspired by [NN],  we adopt a spatial attention mechanism to weight more on  relevant visual regions for each word
 Given an input sentence description, we first encode each  word to an one-hot vector and then transform them to a  feature vector through a fully-connected layer and an encoder word-LSTM
We denote the word features by H = {hN, · · · , hT }, H ∈ R  DH×T , where ht denotes the hidden state of the encoder LSTM at time step t and DH is the hid- den state dimension
Let I = {iN, · · · , iL}, I ∈ R  DI×L  represent the visual features from all L regions in the in- put image, where DI is the image feature dimension, and il is the feature vector at the spatial region l
At time step t, the spatial attention at over each image region k can be calculated as  et,k = WP {tanh [WI ik + (WHht + bH)]}+ bP , (N)  at,k = exp(et,k)  exp (  ∑L  j=N et,j  ) , for k = N, · · · , L, (N)  where WI ∈ R K×DI and WH ∈ R  K×DH are the parameter  matrices that transform visual and semantic features to the  same K-dimensional space, and WP ∈ R N×K converts the  coupled textual and visual features to affinity scores
Given  a word at time t, the attentions at,k over all L image re- gions are normalized by a softmax function and should sum  up to N
Intuitively, at,k represents the probability that the tth word relates to the kth image region
The obtained spa- tial attentions are then used to gate the visual features by  weighted averaging,  ĩt =  L ∑  k=N  at,kik
(N)  In this way, the gated visual features focus more on relevant  regions to the tth word
To incorporate both textual and visual information at each word, we then concatenate the  gated visual features ĩt and hidden states ht of LSTM as the output of the spatial attention module, xt =  [  ĩt, ht ]  
 N.N.N Decoder LSTM with latent semantic attention  Although the LSTM has a memory state and a forget gate  to capture long-term information, it still faces challenges on  processing very long sentences to compress all information  of the input sentence into a fixed-length vector
It might not  be robust enough against sentence structure variations
Inspired by the word alignment (attention) technique in neural machine translation [N], we propose to use a decoder  LSTM with latent semantic attention to automatically align  sentence structures and estimate the final matching confidence
Note that unlike the conventional decoder LSTM  in machine translation, where each step corresponds to an  actual word, each step of our decoder LSTM has no physical meaning but only latent semantic meaning
Given the  final features encoded by the encoder LSTM, the M -step decoder LSTM processes the encoded feature step by step  while searches through the entire input sentence to align the  image-word features, xt, t = {N, · · · , T}
At the mth time step of the decoding process, the latent semantic attention  a′m for the tth input word is calculated as  e′m,t = f(cm−N, xt), (N)  a′m,t = exp(e′m,t)  ∑T  j=N exp(e ′ m,j)  , (N0)  where f is an importance function that weights the impor- tance of the jth word for the mth decoding step
It is mod- eled a two-layer Convolutional Neural Network
cm−N is the hidden state by decoder LSTM for step m− N
At each decoding step m, the semantic attention “soft” aligns the word-image features by a weighted summation,  x̃m =  T ∑  j=N  a′m,jxj 
(NN)  The aligned image-word features x̃m are then transformed by two fully-connected layers and fed into the M -step decoding LSTM to obtain the final matching confidence
 By automatically aligning image-word features with latent semantic attention, at each decoding step, the decoder  LSTM is able to focus more on relevant information by reweighting the source image-word features to enhance the  network’s robustness to sentence structure variations
For  training the stage-N network, we also utilize identity-level  annotations when constructing text-image training pairs
If  an image and a sentence have the same identity, they are  treated as a positive pair
Easier training samples are filtered out by the stage-N network
The decoder LSTM is  trained with the binary cross-entropy loss,  E = − N  N ′  N ′ ∑  i=N  [yi logCi + (N− yi) log(N− Ci)] (NN)  where N ′ is the number of samples for training the stage- N network, Ci is the predicted matching confidence for the ith text-image pair, and yi denotes its target label, with N representing the text and image pair belonging to the same  identity and 0 representing different identities
 NNNN    Text-Image Retrieval  Method Top-N (%) Top-N0 (%)  deeper LSTM Q+norm I [N] NN.NN NN.NN  iBOWIMG [N0] N.00 N0.NN  NeuralTalk [NN] NN.NN NN.NN  Word CNN-RNN [NN] N0.NN NN.NN  GNA-RNN [NN] NN.0N NN.NN  GMM+HGLMM [NN] NN.0N NN.NN  Stage-N NN.NN NN.NN  Stage-N NN.NN N0.NN  Table N
Text-to-image retrieval results by different compared  methods on the CUHK-PEDES dataset [NN]
 N
Experiments  N.N
Datasets and evaluation metrics  Our proposed algorithm takes advantage of identitylevel annotations from the data for achieving robust matching results
Three datasets with identity-level annotations,  CUHK-PEDES [NN], Caltech-UCSD birds (CUB) [NN], and  Oxford-N0N Flowers [NN], are chosen for evaluation
 CUHK-PEDES dataset
The CUHK-PEDES dataset  [NN] contains N0,N0N images of NN,00N person identities
 Each image is described by two sentences
There are NN,00N  persons, NN,0NN images and NN,N0N sentence descriptions  in the training set
The validation set and test set consist  of N,0NN and N,0NN images, respectively, and both of them  contain N,000 persons
The top-N and top-N0 accuracies are  chosen to evaluate the performance of person search with  natural language description following [NN], which are the  percentages of successful matchings between the query text  and the top-N and top-N0 scored images
 CUB dataset and Flower dataset
The CUB and  Flower datasets contain NN,NNN bird images and N,NNN  flower images respectively, where each image is labeled by  ten textual descriptions
There are N00 different categories  in CUB and the dataset is splited into N00 training, N0 validation, and N0 test categories
Flower has N0N flower classes  and three subsets, including NN classes for training, N0 for  validation, and N0 for test
We have the same experimental  setup as [NN] for fair comparison
There is no overlap between training and testing classes
Similar to [NN], identity  classes are used only during training, and testing is on new  identities
We report the AP@N0 for text-to-image retrieval  and the top-N accuracy for image-to-text retrieval
Given a  query textual class, the algorithm first computes the percent  of top-N0 retrieved images whose identity matches that of  the textual query class
The average matching percentage  of all N0 test classes is denoted as AP@N0
 N.N
Implementation details  For fair comparison with existing baseline methods on  different datasets, we choose VGG-NN [NN] for the CUHKPEDES dataset and GoogleNet [N0] for the CUB and  Text-Image Retrieval  Method Top-N (%) Top-N0 (%)  Triplet NN.NN NN.NN  Stage-N NN.NN NN.NN  Stage-N w/o SMA+SPA+stage-N NN.NN NN.NN  Stage-N w/o SMA+SPA NN.NN NN.0N  Stage-N w/o SMA NN.NN NN.NN  Stage-N w/o ID NN.NN NN.NN  Stage-N NN.NN N0.NN  Table N
Ablation studies on different components of the proposed  two-stage framework
“w/o ID”: not using identity-level annotations
“w/o SMA”: not using semantic attention
“w/o SPA”: not  using spatial attention
“w/o stage-N”: not using stage-N network  for training initialization and easy result screening
 Flower datasets as the visual CNN
For stage-N network,  the visual features are obtained by LN-normalizing the out- put features at “dropN” and “avgpool” layers of VGG-NN  and GoogleNet
We take the last hidden state of the LSTM  to encode the whole sentence and embed the textual vector  into the NNN-dimensional feature space with the visual im- age
The textual features is also LN-normalized
The tem- perature parameters σv and σs in Eqs
(N) and (N) are em- pirically set to 0.0N
The LSTM is trained with the Adam optimizer with a learning rate of 0.000N while the CNN is trained with the batched Stochastic Gradient Descent
For  the stage-N CNN-LSTM network, instead of embedding the  visual images into N-dimensional vectors, we take the output of the “poolN” layer of VGG-NN or the “inception (Nb)”  layer of GoogleNet as the image representations for learning spatial attention
During the training phase, we first train  the language model and fix the CNN model, and then finetune the whole network jointly to effectively couple the image and text features
The training and testing samples are  screened by the matching results of stage-N
For each visual or textual sample, we take its N0 most similar samples from the other modality by stage-N network and construct  textual-visual pair samples for stage-N training and testing
 Each text-image pair is assigned with a label, where N represents the corresponding pair and 0 represents the noncorresponding one
The step length M of the decoding LSTM is set to N
 N.N
Results on CUHK-PEDES dataset  We compare our proposed two-stage framework with six  methods on the CUHK-PEDES dataset
The top-N and  top-N0 accuracies of text-to-image retrieval are recorded  in Table N
Note that only text-to-image retrieval results  are evaluated for the dataset because image-to-text retrieval  is not a practical problem setting for the dataset
Our  method outperforms state-of-the-art methods by large margins, which demonstrates the effectiveness of the proposed  two-stage framework in matching textual and visual entities  with identity-level annotations
 Our stage-N model outperforms all the compared methNNNN    The man is wearing a sweater with black gray and white stripes on it
  He is wearing tan pants and gray shoes
He is carrying a bag on his back
 This is a white flower with wide petals and a pink and yellow pistil
This flower has thick and sharply tipped petals of bright yellow   which angle directly upwards
 This bird is nearly all brown with a hooked bill
A brown bird with a white crown and a small yellow pointed beak
 This woman is wearing a white short sleeved shirt, a white skirt and   gray flat shoes
She is also carrying a black purse on her shoulder
 Figure N
Example text-to-image retrieval results by the proposed framework
Corresponding images are marked by green rectangles
(Left  to right) For each text description, the matching results are sorted according to the similarity scores in a descending order
(Row N) results  from the CUHK-PEDES dataset [NN]
(Row N) results from the CUB dataset [NN]
(Row N) results from the Flower dataset [NN]
 ods
The gain on top-N accuracy by our proposed method is  N.N0% compared with the state-of-the-art GNA-RNN [NN], which has more complex network structure than ours
This  shows the advantages of the CMCE loss
Furthermore, the  introduction of feature buffers make the comparison computation more efficient even with a large number of identities
GMM+HGLMM [NN] uses the Fisher Vector as a  sentence representation by pooling the wordNvec embedding of each word in the sentence
The Word CNN-RNN  [NN] aims to minimize the distances between corresponding textual-visual pairs and maximize the distances between  non-corresponding ones within each mini-batch
However,  such a method is restricted by the mini-batch size and cannot be applied to dataset with a large number of identities
Our CMCE loss results in a top-N accuracy of NN.NN%, which outperforms the Word CNN-RNN’s N0.NN%
The stage-N CNN-LSTM with CMCE loss performs well on  both accuracy and time complexity with its loosely coupled  network structure
 The stage-N CNN-LSTM with latent co-attention further  improves the performances by N.NN% and N.N0% in terms of top-N and top-N0 accuracies
The co-attention mechanism  aligns visual regions with latent semantic concepts effectively to minimize the influence of sentences structure variations
Compared with methods that randomly sample pairs,  such as deeper LSTM Q+norm I [N], iBOWIMG [N0], NeuralTalk [NN] and GNA-RNN [NN], our network focuses more  on distinguishing the hard samples after filtering out most  easy non-correponding samples by the stage-N network
 N.N
Ablation studies  In this section, we investigate the effect of each component in the stage-N and stage-N networks by performing a  series of ablation studies on the CUHK-PEDES dataset
We  first investigate the importance of proposed CMCE loss
We  train our stage-N model with the proposed loss replaced by  triplet loss [NN], named “Triplet”
As shown in Table N, its  top-N drops by N.NN% on the CUHK-PEDES set compared with our stage-N with the new loss function
In addition,  triplet loss [NN] needs N times more training time
Then we  investigate the importance of the identity-level annotations  to the textual-visual matching performance by ignoring the  annotations
In this case, each image or sentence is treated  as an independent identity
The top-N and top-N0 accuracies  of “Stage-N w/o ID” have N.NN% and N.NN% drops com- pared with the results of “Stage-N”, which demonstrates that  the identity-level annotations can help textual-visual matching by minimizing the intra-identity feature variations
 To demonstrate the effectiveness of our latent semantic  attention, we remove it from the original stage-N network,  denoted as “Stage-N w/o SMA”
The top-N accuracy drops  by N.NN%, which shows the latent semantic attention can help align the visual and semantic concepts and mitigate  the LSTM’s sensitivity to different sentence structures
The  spatial attention tries to relate words or phrases to different visual regions instead of the whole image
Based on  the framework of “Stage-N w/o SMA”, we further remove  the spatial attention module from the stage-N network, denoted as “Stage-N w/o SMA+SPA”, which can be viewed  as a simple concatenation of the visual and textual features  from the CNN and LSTM, followed by two fully-connected  layers for binary classification
Both the top-N and top-N0  accuracies decrease compared with “Stage-N w/o SMA”
 The stage-N network is able to provide samples for the  training and evaluation of stage-N network, and also serves  as the initial point for its training
To investigate the influence of stage-N network, we design one additional baselines, denoted as “Stage-N w/o SMA+SPA+Stage-N”
This  baseline is trained without using the stage-N network
It  shows an apparent performance drop compared with the  “Stage-N w/o SMA+SPA” baseline, which demonstrates the  necessity of the stage-N network in our proposed framework
Since stage-N network chooses only N0 most closest images of each query text for stage N during the evaluation  NNNN    Image-Text Text-Image  Top-N Acc (%) AP@N0 (%)  Methods DA-SJE DS-SJE DA-SJE DS-SJE  BoW [N] NN.N NN.N NN.N NN.N  WordNVec [NN] NN.N NN.N N.N NN.N  Attributes [N] N0.N N0.N N0.N N0.0  Word CNN [NN] N0.N NN.0 N.N NN.N  Word CNN-RNN [NN] NN.N NN.N N.N NN.N  GMM+HGLMM [NN] NN.N NN.N  Triplet NN.N NN.N  Stage-N NN.N NN.N  Stage-N − NN.N Table N
Image-to-text and text-to-image retrieval results by different compared methods on the CUB dataset [NN]
 phase, the effect of some components might not be apparent  in terms of the top-N0 accuracy
 N.N
Results on the CUB and Flower datasets  Tables N and N show the experimental results of imageto-text and text-to-image retrieval on the CUB and Flower  datasets
We compare with state-of-the-art methods on the  two datasets
The CNN-RNN [NN] learns a CNN-RNN textual encoder for sentence feature embedding and transforms  both visual and textual features into the same embedding  space
Different text features are also combined with the  CNN-RNN methods
The WordNVec [NN] averages the pretrained word vector of each word in the sentence description  to represent textual features
BoW [N] is the output of an  one-hot vector passing through a single layer linear projection
Attributes [N] maps attributes to the embedding space  by learning a encoder function
Different types of textual  representations are combined with the CNN-RNN framework for testing
Our method outperforms the state-of-theart CNN-RNN by more than N% in terms of top-N image- to-text retrieval accuracy and about N0% in terms of text- to-image retrieval AP@N0 on both datasets, which shows  the effectiveness of the proposed method
For the “Triplet”  baseline, the top-N and AP@N0 drop by N.0% and N.N% on CUB dataset, and drop by N.N% and N.N% on Flower dataset which demonstrate the proposed loss function performs better than the traditional triplet loss
Since the top-N accuracy  provided by [NN] is computed by fusing sentences of the  same class into one vector and our stage-N network is therefore not suitable for the image-to-text retrieval task, we only  report the stage-N results on image-to-text retrieval which  has already outperformed other baselines
 N.N
Qualitative results  We also conduct qualitative evaluations of the proposed  methods
Figure N shows example text-to-image retrieval  results
Most sentences can correctly match images corresponding to their descriptions
In the first case, almost  all the persons wear a sweater with “black gray and white  stripes”
Different images of the same identity (the first,  Image-Text Text-Image  Top-N Acc (%) AP@N0 (%)  Methods DA-SJE DS-SJE DA-SJE DS-SJE  BoW [N] NN.N NN.N NN.N NN.N  WordNVec [NN] NN.N NN.N NN.N NN.N  Word CNN [NN] N0.N N0.N N.N NN.N  Word CNN-RNN [NN] N0.N NN.N N.N NN.N  GMM+HGLMM [NN] NN.N NN.N  Triplet NN.N NN.N  Stage-N NN.N NN.0  Stage-N − N0.N Table N
Image-to-text and text-to-image retrieval results by different compared methods on the Flower dataset [NN]
 second, and fifth person images) appear in the top-ranked  results, which shows the proposed two-stage CNN-LSTM  can correctly match identities across different domains and  minimizes the intra-identity distances
Some mis-matching  results are even challenging for human to distinguish with  subtle differences in visual appearance
In the second case,  the first and second person both wear “white short sleeved  shirt”, but only the first one is the true matching result because of the “black purse” carried on her shoulder
 N
Conclusion  In this paper, we proposed a novel two-stage framework  for identity-aware visual-semantic matching
The framework consists of two deep neural networks
The stageN CNN-LSTM network learns to embed the input image  and description to the same feature space and minimizes  the intra-identity distance simultaneously with the CMCE  loss
It serves as initial point for stage-N training and also  provides training and evaluation samples for stage-N by  screening most incorrect matchings
The stage-N network  is a CNN-LSTM with latent co-attention mechanism which  jointly learns the spatial attention and latent semantic attention by an alignment decoder LSTM
It automatically aligns  different words and image regions to minimize the impact  of sentence structure variations
We evaluate the proposed  method on three datasets and perform a series of ablation  studies to verify the effect of each component
Our method  outperforms state-of-the-art approaches by a large margin  and demonstrates the effectiveness of the proposed framework for identity-aware visual-textual matching
 Acknowledgement This work is supported in part by  SenseTime Group Limited, in part by the General Research  Fund through the Research Grants Council of Hong  Kong under Grants CUHKNNNNNNNN, CUHKNNN0NNNN,  CUHKNNN0NNNN, CUHKNNNNNN, CUHKNNN0N0NN,  CUHKNNNNNNNN, CUHKNNN0NNNN, in part by the Hong  Kong Innovation and Technology Support Programme  Grant ITS/NNN/NNFX, and in part by the China Postdoctoral  Science Foundation under Grant N0NNMNNNNNN
 NNNN    References  [N] E
Ahmed, M
Jones, and T
K
Marks
An improved deep  learning architecture for person re-identification
In CVPR,  N0NN
N  [N] Z
Akata, S
Reed, D
Walter, H
Lee, and B
Schiele
Evaluation of output embeddings for fine-grained image classification
In CVPR, N0NN
N  [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra,  C
Lawrence Zitnick, and D
Parikh
Vqa: Visual question  answering
In ICCV, N0NN
N, N, N, N, N  [N] D
Bahdanau, K
Cho, and Y
Bengio
Neural machine  translation by jointly learning to align and translate
arXiv  preprint arXiv:NN0N.0NNN, N0NN
N, N, N  [N] X
Chen and C
Lawrence Zitnick
Mind’s eye: A recurrent  visual representation for image caption generation
In CVPR,  N0NN
N  [N] D
Cheng, Y
Gong, S
Zhou, J
Wang, and N
Zheng
Person re-identification by multi-channel parts-based cnn with  improved triplet loss function
In CVPR, N0NN
N  [N] A
Frome, G
S
Corrado, J
Shlens, S
Bengio, J
Dean,  T
Mikolov, et al
Devise: A deep visual-semantic embedding model
In NIPS, N0NN
N, N  [N] A
Fukui, D
H
Park, D
Yang, A
Rohrbach, T
Darrell,  and M
Rohrbach
Multimodal compact bilinear pooling  for visual question answering and visual grounding
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
N  [N] Z
S
Harris
Distributional structure
Word, N0(N-N):NNN–  NNN, NNNN
N  [N0] M
Hodosh, P
Young, and J
Hockenmaier
Framing image  description as a ranking task: Data, models and evaluation  metrics
Journal of Artificial Intelligence Research, NN:NNN–  NNN, N0NN
N  [NN] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, N0NN
 N  [NN] B
Klein, G
Lev, G
Sadeh, and L
Wolf
Associating neural word embeddings with deep image representations using  fisher vectors
In CVPR, N0NN
N, N, N, N  [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, et al
 Visual genome: Connecting language and vision using  crowdsourced dense image annotations
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [NN] N
Kumar, A
C
Berg, P
N
Belhumeur, and S
K
Nayar
 Attribute and simile classifiers for face verification
In ICCV,  N00N
N  [NN] S
Li, T
Xiao, H
Li, B
Zhou, D
Yue, and X
Wang
Person  search with natural language description
In CVPR, N0NN
N,  N, N  [NN] Y
Li, W
Ouyang, X
Wang, and X
Tang
Vip-cnn: Visual  phrase guided convolutional neural network
N  [NN] S
Liao, Y
Hu, X
Zhu, and S
Z
Li
Person re-identification  by local maximal occurrence representation and metric  learning
In CVPR, N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV, N0NN
N  [NN] J
Lu, J
Yang, D
Batra, and D
Parikh
Hierarchical  question-image co-attention for visual question answering
 In NIPS, N0NN
N, N, N  [N0] J
Mao, W
Xu, Y
Yang, J
Wang, Z
Huang, and A
Yuille
 Deep captioning with multimodal recurrent neural networks  (m-rnn)
arXiv preprint arXiv:NNNN.NNNN, N0NN
N  [NN] J
Mao, W
Xu, Y
Yang, J
Wang, and A
L
Yuille
Explain  images with multimodal recurrent neural networks
arXiv  preprint arXiv:NNN0.N0N0, N0NN
N  [NN] I
Masi, S
Rawls, G
Medioni, and P
Natarajan
Pose-aware  face recognition in the wild
In CVPR, N0NN
N  [NN] T
Mikolov, I
Sutskever, K
Chen, G
S
Corrado, and  J
Dean
Distributed representations of words and phrases  and their compositionality
In NIPS, N0NN
N  [NN] H
Nam, J.-W
Ha, and J
Kim
Dual attention networks  for multimodal reasoning and matching
arXiv preprint  arXiv:NNNN.00NNN, N0NN
N, N  [NN] M
Palatucci, D
Pomerleau, G
E
Hinton, and T
M
 Mitchell
Zero-shot learning with semantic output codes
In  NIPS, N00N
N  [NN] S
Reed, Z
Akata, H
Lee, and B
Schiele
Learning deep  representations of fine-grained visual descriptions
In CVPR,  N0NN
N, N, N, N, N, N  [NN] M
Rohrbach, M
Stark, and B
Schiele
Evaluating knowledge transfer and zero-shot learning in a large-scale setting
 In CVPR, N0NN
N  [NN] F
Schroff, D
Kalenichenko, and J
Philbin
Facenet: A unified embedding for face recognition and clustering
In CVPR,  N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N  [N0] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
N  [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In CVPR, N0NN
N, N,  N, N, N  [NN] L
Wang, Y
Li, and S
Lazebnik
Learning deep structurepreserving image-text embeddings
In CVPR, N0NN
N, N,  N  [NN] T
Xiao, H
Li, W
Ouyang, and X
Wang
Learning deep feature representations with domain guided dropout for person  re-identification
In CVPR, N0NN
N  [NN] T
Xiao, S
Li, B
Wang, L
Lin, and X
Wang
Joint detection and identification feature learning for person search
In  CVPR, N0NN
N  [NN] F
Yan and K
Mikolajczyk
Deep correlation for matching  images and text
In CVPR, N0NN
N, N  [NN] P
Young, A
Lai, M
Hodosh, and J
Hockenmaier
From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions
Transactions of the Association for Computational Linguistics, N:NN–  NN, N0NN
N  [NN] L
Zhang, T
Xiang, and S
Gong
Learning a discriminative  null space for person re-identification
In CVPR, N0NN
N  [NN] H
Zhao, M
Tian, S
Sun, J
Shao, J
Yan, S
Yi, X
Wang,  and X
Tang
Spindle net: Person re-identification with human body region guided feature decomposition and fusion
 In CVPR, N0NN
N  NNNN    [NN] L
Zheng, H
Zhang, S
Sun, M
Chandraker, and Q
Tian
 Person re-identification in the wild
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [N0] B
Zhou, Y
Tian, S
Sukhbaatar, A
Szlam, and R
Fergus
Simple baseline for visual question answering
arXiv  preprint arXiv:NNNN.0NNNN, N0NN
N, N  [NN] Y
Zhu, O
Groth, M
Bernstein, and L
Fei-Fei
VisualNw:  Grounded question answering in images
In CVPR, N0NN
N,  N  NNNNHierarchical Multimodal LSTM for Dense Visual-Semantic Embedding   Hierarchical Multimodal LSTM for Dense Visual-Semantic Embedding  Zhenxing NiuN Mo ZhouN Le WangN Xinbo GaoN Gang HuaN  NAlibaba Group, NXidian University, NXi’an Jiaotong University, NMicrosoft Research  {zhenxingniu,ganghua}@gmail.com, lewang@mail.xjtu.edu.cn, xinbogao@mail.xidian.edu.cn  Abstract  We address the problem of dense visual-semantic embedding that maps not only full sentences and whole images  but also phrases within sentences and salient regions within  images into a multimodal embedding space
Such dense  embeddings, when applied to the task of image captioning,  enable us to produce several region-oriented and detailed  phrases rather than just an overview sentence to describe  an image
Specifically, we present a hierarchical structured  recurrent neural network (RNN), namely Hierarchical Multimodal LSTM (HM-LSTM)
Compared with chain structured RNN, our proposed model exploits the hierarchical relations between sentences and phrases, and between whole  images and image regions, to jointly establish their representations
Without the need of any supervised labels, our  proposed model automatically learns the fine-grained correspondences between phrases and image regions towards  the dense embedding
Extensive experiments on several  datasets validate the efficacy of our method, which compares favorably with the state-of-the-art methods
 N
Introduction  Visual-semantic embedding is to map both images and  their captions into a common space, so that we can retrieve/rank captions given images or retrieve/rank images  given captions
Particularly, it has been broadly used for  image captioning which aims to describe images with sentences
Recently, the advances in deep learning have made  significant progress on visual-semantic embedding
Generally, image representations are produced by Convolutional  Neural Networks (CNN), and caption representations are  produced by Recurrent Neural Networks (RNN)
A ranking loss is subsequently optimized to make the corresponding representations as close as possible in the embedding  space [NN] [N] [NN] [NN]
 Most previous methods only map full sentences and  whole images into an embedding space
As a result, they are  only able to describe an image with a general and overview  sentence, i.e., coarsely and generally depict the image con‘A man is standing in front of towers.’  ‘a man with a blue hat and sunglasses’  ‘a girl in red jacket and black dress’  ‘several white towers with golden spire’  Figure N
Region-oriented, detailed, and phrase-level image captioning
It is desired to produce several region-oriented and detailed phrases rather than just an overview sentence for describing  an image
 tent
However, different users may be interested in distinct  objects/regions in an image, and hence it is desired to individually depict them with specific descriptions
As shown  in Fig
N, some users may be interested in ‘the man with  sunglasses’ while others may be interested in ‘the girl in red  jacket’
Therefore, it is desired to produce several regionoriented and detailed phrases (e.g., ‘a man with a blue hat  and sunglasses’) rather than just an overview sentence (e.g.,  ‘A man is standing in front of towers’) to describe an image
 An intuitive solution is to map not only the full sentences  but also the phrases within the sentences into a common  space
As such, for a given image after detecting salient  image regions, detailed phrases can be retrieved to describe  those image regions
Since long sentences are decomposed  as short phrases, many diverse and subtle phrases could be  produced
Besides, since more diverse phrases are mapped  into the embedding space, we can learn a much denser embedding space so that it is possible to find a better and more  expressive phrase to describe an image or an image region
 However, most previous methods cannot naturally represent the phrases within sentences, and hence cannot map  them into the embedding space
The main reason is that the  neural networks (e.g., RNN [N0] [N]) adopted for building  sentence representations often have a chain structure, i.e., a  basic unit is unfolded one by one through a chain structure
 Therefore, the full sentences are naturally represented with  the last hidden state of the chain structured neural network  since it encodes all the words within the sentence
But it is  difficult to directly build representations for phrases within  sentences
 NNNNN    Moreover, previous methods are only able to utilize the  correspondences between whole images and full sentences
 But there are many fine-grained correspondences between  image regions and short phrases, which can be utilized to  boost the learning of the embedding space [NN]
As shown  in Fig
N, besides the sentence-level correspondence between the sentence ‘a cat sat on a mat.’ and the whole image, there is a correspondence between the phrase ‘a cat’  and the corresponding image region, etc
Therefore, it is  beneficial to exploit and utilize those fine-grained ‘phraseregion’ correspondences to boost the embedding learning
 To address the two problems above, we propose a Hierarchical Multimodal LSTM (HM-LSTM) model
In particular, our HM-LSTM model has a hierarchical structure,  where the intermediate nodes represent phrases and regions,  while the root nodes represent the full sentences and whole  images, as shown in Fig
N
Thus, our model can naturally  and jointly learn the embeddings of all sentences, phrases,  images and image regions
More importantly, there are hierarchical relations between sentences and phrases, and between whole images and image regions
For example, a  ‘parent’ phrase (e.g., ‘a cat sat on the mat’) is related to  its ‘children’ phrases (e.g., ‘a cat’ and ‘the mat’, meanwhile the ‘parent’ image region covers the two ‘children’  image regions
Since our model has a hierarchical structure,  we can explicitly exploit such hierarchical relations when  jointly learning their embeddings
Compared with previous  visual-semantic models, our model can map phrases as well  as image regions into the embedding space, and hence we  can learn a dense embedding space, as shown in Fig
N
 When building representations for phrases, the syntax of  phrases is explicitly considered in our model
This is due  to that image descriptions often make frequent references to  objects, therefore noun phrases in a sentence are often more  important than the other phrases (e.g., verb phrases)
Therefore, noun phrases and the other phrases are distinctively  modeled in our HM-LSTM model, i.e., our HM-LSTM  model is a syntax-aware model, which is more suitable for  the image captioning task
 Note that the fine-grained ‘phrase-region’ correspondences can be automatically established along with the embedding learning
In other words, we conduct dense visualsemantic embedding in an unsupervised fashion, i.e., without the need of manually annotating the correspondences  between image regions and phrases
Recently, the DenseCap [NN] has been proposed for region-oriented captioning
 However, they address this problem in a supervised fashion, i.e., the ‘phrase-region’ correspondences are given for  each training image
Obviously, it is much more expensive to annotate such fine-grained correspondences especially for a large scale dataset
In addition, the phrases annotated in the DenseCap are independent of the full sentences,  whereas there are relations among sentences and phrases in  Noun   Phrase  Noun   Phrase  Sentence  Verb Phrase  a cat  the mat  ‘a cat sat on the mat.’  Verb   Phrase  ‘a cat sat on the mat.’  ‘a cat’  ‘the mat’  parsing  region proposals  A dense  embedding space  A sentence An image  Figure N
Hierarchical Multimodal Embedding: each sentence is  decomposed as some phrases by a tree parser, meanwhile some  salient image regions are detected from the image
Then, all of full  sentences, phrases, whole images, and image regions are mapped  into a common space, resulting in a dense embedding space
 our method since the phrases are extracted from the given  sentences
 Besides, the experimental results turn out that the performance of general image captioning can also be significantly  improved due to learning a dense embedding space
This  is attributed to the joint embedding of full sentences and  their phrases
Since there are hierarchical relations among  full sentences and their phrases, such relations could benefit both their embedding learning when they are jointly  mapped into the embedding space
 Briefly, our contributions are three-fold:  N
A hierarchical multimodal LSTM model is proposed  for dense visual-semantic embedding, which is able  to jointly learn the embeddings of all the sentences,  phrases, images, and image regions
Moreover, the hierarchical relations among them can be explicitly exploited in our model
 N
The fine-grained correspondences between phrases  and image regions can be automatically learned and  utilized to boost the learning of the embedding space
 N
Our model is a syntax-aware model where noun  phrases and the other phrases are distinctively modeled  towards the task of image captioning
 N
Related Work  Visual-semantic embedding is closely related to the image captioning
Generally, the methods of image captioning can be roughly grouped into two categories: image caption ranking and image caption generation
Visualsemantic embedding is often regarded as a kind of methods for image caption ranking, i.e., to rank captions given  images [N] [NN] [NN] [NN]
DeViSE [N] is a simple model  for image caption ranking, where sentences are represented  NNNN    as the mean of their word embeddings
After that, some  sophisticated models such as the SDT-RNN [NN] are proposed to learn sentence embedding representations
Recently, Deep Structure-Preserving (DeepSP) [NN] is proposed for image-text embedding and achieves the state-ofthe-art performance
 For dense embedding, the most related works are the  DeepVS [NN] and the DeFrag [NN], which also align words  and short phrases within sentences to bounding boxes
In  DeepVS [NN], in order to build phrase representations, they  additionally apply a Markov Random Field (MRF) to connect neighboring words as a phrase
On the contrary, our  hierarchical model can naturally generate syntax-correct  phrases and naturally build their representations
In DeFrag [NN], although the tree parsing is leveraged for phrase  representation, the phrases are independently represented  and hence the tree structure is actually discarded in favor of  a simpler model
On the contrary, the hierarchical relations  among phrases can be explicitly modeled by our method
 Moreover, the phrases are jointly instead of independently  modeled in our approach
 Image Caption Generation
Many methods are proposed  for image caption generation [NN] [NN] [NN] [N] [N0]
They  aim to generate descriptions by sampling from conditional  neural language models
Particularly, an ‘encoder-decoder’  framework [NN] [N] is often adopted by those methods,  where a CNN is used to represent an image, and an RNN  is used to generate descriptions conditioned on the image  representation
 N
Our Approach  We attempt to map all of full sentences, phrases, whole  images, and image regions into a common space
Therefore,  our approach needs not only to learn the phrase-level correspondences (i.e., the correspondences between phrases and  image regions) but also to learn a multimodal embedding  space containing all the sentences, phrases, images, and image regions
 Specifically, each sentence is first represented as a Constituency Tree with the Stanford Parser [NN], where each  intermediate node in the tree indicates a phrase while the  root node indicates the full sentence
Meanwhile, for  each image, the Region Convolutional Neural Network (RCNN) [N] is adopted to extract a feature representation for  the image region which is generated by using object proposal methods [NN]
 Next, if the phrase-level correspondences are known, our  HM-LSTM model can utilize such correspondences to conduct the embedding learning
In particular, each loss layer  is introduced to connect a noun phrase node to an image  region, as shown in Fig
N
At last, all the losses (including ‘phrase–region’ losses and ‘sentence-image’ losses) are  simultaneously minimized to learn the embedding space
 Input: the ‘sentence–image’ pairs in the dataset {(Sd, Id)} D d=N  N
Initialization stage: coarse-grained embedding learning
Only  the known sentence-level correspondences are utilized to learn  a simplified HM-LSTM model
And then, the initial representations for phrases and image regions are estimated
 N
Loop for t = N, ..., T :  (a) Phrase-level correspondences learning
Given  the learned representations of phrases and regions,  we establish some ‘phrase–region’ correspondences  {(Sd,k, Id,k)} Kd k=N  for each image by measuring their  similarity (refers to Section N.N)
 (b) Fine-grained embedding learning
Given the previous  phrase-level correspondences, the HM-LSTM model is  learned to update the phrase and region representations  (refers to Section N.N.N)
 Output: the representations of sentences, phrases, images, and image  regions, i.e., {(hd,k, vd,k)} d=D,k=Kd d=N,k=0  
 Figure N
The iterative learning procedure for the hierarchical multimodal embedding
 However, only the sentence-level (rather than the phraselevel) correspondences are known at the beginning
But if  we have the representations of all phrases and image regions, it is easy to establish their correspondences, e.g.,  by measuring the similarities between their representations
 Thus, in our approach we take an alternative learning procedure for the embedding learning, i.e., to learn the multimodal embedding space and those phrase-level correspondences alternatively
 In particular, we have an initial learning stage, where  only the ‘sentence–image’ losses are minimized to learn a  simplified HM-LSTM model
As a result, we are able to  produce the initial representations for all the phrases and  image regions, which can be further used to construct the  initial phrase-level correspondences
After that, a full version of HM-LSTM model (both sentence-level and phraselevel losses are minimized) is learned, and the embedding  learning and the correspondences learning can be conducted  iteratively, as shown in Fig
N
 N.N
Images Embedding  We follow the work of [NN] to represent images
In particular, some object proposals are extracted using the selective search method [NN], and they are represented with an  R-CNN [N]
Following Karpathy et al
[NN], we adopt the  top NN detected locations in addition to the whole image, and compute the representations based on the pixels Ib inside each bounding box as follows:  vm = Wm[CNNθc(Ib)] + bm (N)  where CNN(Ib) transforms the pixels inside the bounding box Ib into N0NN-dimensional activations of the fully connected layer immediately before the classifier
 NNNN    N.N
Hierarchical Multimodal Embedding  Given the phrase-level correspondences, our HM-LSTM  model is able to learn a dense embedding space containing all the sentences, phrases, images, and image regions
 In particular, we first review the Tree-LSTM model [NN]  which was recently proposed for sentence embedding
Then  it is extended to a syntax-aware model, namely Hierarchical LSTM (H-LSTM) model, where noun phrases and the  other phrases are distinctively modeled
At last, our HMLSTM model is proposed based on the H-LSTM model,  which is a multimodal model for joint embedding of sentences, phrases, images, and image regions
 N.N.N Hierarchical LSTM  Recently, the Tree-LSTM model [NN] has been proposed to  explicitly model the hierarchical structure of sentences
In  particular, a sentence is parsed as a tree, where the root indicates the full sentence and the intermediate nodes indicate  the phrases within the sentence
 In Tree-LSTM, children nodes are equally treated when  connected to their parent node without considering their  syntax type – noun phrase children and the other phrase  children (e.g., verb phrase) are equally treated
However,  since our task mostly focuses on objects, noun phrases and  the other phrases are modeled with different emphasis, i.e.,  the noun phrase children should have larger contributions  than the other phrase children
 To this end, we extend the Tree-LSTM as a syntax-aware  model, namely Hierarchical LSTM (H-LSTM) model
 Specifically, each unit of H-LSTM (indexed by j) contains  an input gate ij , an output gate oj , a memory cell cj , and a  hidden state hj 
Suppose there are N(j) noun phrase chil- dren for j, and N(j) the other phrase children for j, each  H-LSTM unit will have N(j) forget gates f̂jk, k ∈ N(j) and N(j) forget gates f jl, l ∈ N(j), as in Eq (N) and Eq (N)
 For a parent node j, the hidden state of its noun phrase  children hk, k ∈ N(j) and the other phrase children hl, l ∈  N(j) are respectively summed up (denoted as ĥj and hj) before impacting the parent node j, as in Eq (N)
Furthermore, the ĥj and hj have different effects on the input gate  ij by using distinct parameters Û (o) and U  (o) , as shown in  Eq (N)
It is similar for the output gate oj and memory cell  cj , as shown in Eq (N), and Eq (N)
This allows the H-LSTM  to sufficiently consider the syntax type of children nodes
 ĥj = ∑  k∈N(j)  hk; hj = ∑  l∈N(j)  hl (N)  f̂jk =σ(W (f)xj + Û  (f)hk + b (f)), k ∈ N(j) (N)  f jl =σ(W (f)xj + U  (f) hl + b  (f)), l ∈ N(j) (N)  NP  NP  VP  sentence  VP  Figure N
The structure of our HM-LSTM
Each sentence is parsed  as a tree, where the intermediate nodes indicate the phrases within  the sentence
Some noun phrases (NP) hd,k are associated to the  corresponding image regions vd,k by specific loss layers lossd,k
 ij =σ(W (i)xj + Û  (i)ĥj + U (i) hj + b  (i)) (N)  oj =σ(W (o)xj + Û  (o)ĥj + U (o)  hj + b (o)) (N)  uj =tanh(W (u)xj + Û  (u)ĥj + U (u)  hj + b (u))  cj =ij ⊙ uj + ∑  k∈N(j)  f̂jkck + ∑  l∈N(j)  f jlcl (N)  hj =oj ⊙ tanh(cj) (N)  As the standard LSTM, each H-LSTM leaf node takes an  input vector xj 
In our applications, each xj is a vector representation of a word, which is determined as xj = WwIt, where It is an indicator column vector that has a single one  at the index of the t-th word in a word vocabulary
The  weights Ww specify a word embedding matrix that we initialize with N00-dimensional wordNvec [NN] weights and  keep fixed due to overfitting concerns
In addition, as the  Tree-LSTM model, the hidden state hj of node j is regarded  as the representation of the corresponding phrase
 N.N.N Hierarchical Multimodal LSTM  Based on the H-LSTM, we propose a Hierarchical Multimodal LSTM (HM-LSTM) to jointly embed all of images,  image regions, sentences, and phrases into a common space
 Let Id,k denote the k-th image region in the d-th image, Sd,k denote the corresponding phrase
And let Id,0 denote the d-th full image, and Sd,0 denote the corresponding full sentence
If all the ‘phrase-region’ pairs  {(Sd,k, Id,k)} D,Kd d=N,k=0 are known, we learn the HM-LSTM  as follows: a H-LSTM model is first constructed for each  sentence, and for each ‘phrase-region’ pair (Sd,k, Id,k) a loss layer lossd,k is introduced
Inspired by DeepSP [NN],  we introduce a two-branch-network instead of a simple  loss layer for each ‘phrase-region’ pair
Specifically, each  branch is composed of one fully connected layers (Wt for  text and Wm for images), one Batch Normalization (BN)  layer [NN], and one LN-normalization layer, as shown in  NNNN    Fig N
Note that the batch normalization could accelerate  the training and also make gradient updates more stable
 Let vd,k indicate the representation of the Id,k, and hd,k indicate the representation of the Sd,k
We can define a scoring function s(vd,k, hd,k) = vd,k ·hd,k to measure their sim- ilarity
Therefore, for each ‘phrase-region’ pair (Sd,k, Id,k), we define a contrastive loss to measure the distance between  their representations, as the following,  lossd,k = ∑  l  max{0,m− s(vd,k, hd,k) + s(vd,k, hd,l)}  + ∑  l  max{0,m− s(hd,k, vd,k) + s(hd,k, vd,l)}  (N)  where m is the margin, hd,l is a contrastive phrase for image  region vd,k, and vice-versa with vd,l
 Next, the total loss can be defined by the weighted sum  of all losses, as the following,  Loss = D∑  d=N  Kd∑  k=0  wd,klossd,k (N0)  where wd,k is the weight for the k-th ‘phrase-image region’  pair
The lossd,0 indicates the loss at the root layer for the  d-th image, and lossd,k, k = N, 


,Kd indicates the loss at the intermediate layer, as shown in Fig
N
 The weight wd,k can be determined from the learning of  phrase-level correspondences, e.g., the wd,k is determined  according to the confidence of the correspondence for the  k-th ‘phrase-region’ pair
 Note that our HM-LSTM model is learned with the  Back-propagation Through Structure (BPTS) algorithm [N],  where the errors of different loss functions are respectively  injected to the corresponding loss layers, and back propagated from root node to leaf nodes along the tree structure
 N.N
Phrase-level Correspondences  Before the learning of the HM-LSTM, we need to obtain the phrase-level correspondences
We can address this  problem by measuring the representation similarities among  phrase candidates and image region candidates
 Specifically, given the image region candidates (i.e., the  top-NN object proposals), their representations can be easily obtained according to Eq (N)
Meanwhile, each sentence is  parsed as a tree, where each intermediate node in the tree  represents a phrase
Due to that we are just interested in  objects in an image, only noun phrases are selected as the  phrase candidates
Such selection is trivial since the syntax  type of each phrase (i.e., noun phrase, verb phrase, adjective  phrase, etc.) is available after parsing
 NP VP  sitting PP  PP  NP  twopeople  on NP  rocking chairs  on  the deck  two people  rocking chairs  two people sitting on rocking chairs   the deck  NP  Two people sitting on rocking   chairs on the deck
 Figure N
Correspondences between phrases and image regions
 With those image region and phrase candidates, we can  establish the ‘phrase-region’ correspondences according to  their representations
In particular, we compute a matrix S  to measure the similarities of representations for those candidates, where each element sij = vi · hj indicates the sim- ilarity score between the image region vi and the phrase hj 
 Therefore, for each phrase we select the best matched image region, and thus we can establish those ‘phrase-image  region’ pairs, as shown in Fig N
Besides, for each generated ‘phrase-region’ pair (vi, hj), their similarity score sij is regarded as the confidence of their correspondence, which  is used to determine the weight of this correspondence, as  shown in Eq (N0)
 N.N
Initialization and Optimization  Initialization
At the initial learning stage, the initial  representations for all of sentences, phrases, images, and  image regions are obtained by learning a simplified HMLSTM model – only the losses at the root {lossd,0} D d=N  are minimized and the other losses {lossd,k} D,Kd d=N,k=N are  neglected
Obviously, only the sentence-level correspondences are used to learn the simplified HM-LSTM
 Optimization
The CNN part of our model comes from  Karpathy et al
[NN], which is pre-trained on ImageNet [N]  and fine tuned on the N00 classes of the ImageNet Detection Challenge [NN]
We use Adam [NN] to optimize the HMLSTM with a learning rate of N × N0−N
In particular, we use mini-batches of NN paired image-sentences for training
 N
Image Caption Ranking  With the learned hierarchical multimodal embedding  model, we can describe a new image with a full sentence,  i.e., image-sentence ranking
In particular, we first extract  image features by using the CNN and retrieve the nearest sentence vector hd∗,0 ∈ {hd,0} d=D d=N in the embedding  space, which is regarded as the caption for the image
 More importantly, our method can produce regionoriented phrase-level description for a new image
In parNNNN    Table N
FlickrNK experiments
R@K is Recall@K (high is good)
 Med r is the median rank (low is good)
 FlickrNK  Model Image Annotation Image Search  R@N R@N0 Med r R@N R@N0 Med r  Random 0.N N.N NNN 0.N N.0 N00 SDT-RNN [NN] N.0 NN.0 NN N.N NN.N NN DeViSE [N] N.N NN.N NN N.N NN.N NN DeFrag [NN] NN.N NN.0 NN N.N NN.N NN SC-NLM [NN] NN.N NN.N NN N0.N NN.N NN DeepVS [NN] NN.N NN.N N.N NN.N NN.N NN.N m-RNN [NN] NN.N NN.N NN NN.N NN.N NN NIC [NN] N0 NN N NN NN N HM-LSTM NN.N NN.N N NN.N NN.N N  ticular, after detecting some salient image regions/object  proposals, we can extract the visual features from them,  and retrieve specific and detailed phrases to describe them,  namely region-phrase ranking in this paper
 N
Experiments  We use the FlickrNK [NN], FlickrN0K [NN] [NN] and MSCOCO [N0] [N] datasets in our experiments
These datasets  contain N, 000, NN, 000 and NNN, 000 images respectively and each is annotated with N sentences using AMT
For FlickrNK and FlickrN0K, we use N, 000 images for valida- tion, N, 000 for testing and the rest for training, which is consistent with [NN][NN]
For MS-COCO we follow [NN] to  use N, 000 images for both validation and testing
 N.N
Image-Sentence Ranking  We first evaluate the proposed method on the task of  image-sentence ranking
We adopt Recall@K as the metric for evaluation, namely the mean number of images for  which the correct caption is ranked within the top-K retrieved results (and vice-versa for sentences)
 We compare our method with some visual-semantic embedding methods (i.e., ranking-based methods) including  DeViSE, SDT-RNN, and DeFrag
For DeViSE [N], sentences are represented as the mean of their word embeddings
The recursive neural network is used to learn sentence representations in SDT-RNN [NN]
For DeFrag [NN],  sentences are represented as a bag of dependency parses
 In addition, some generation-based methods are also involved in comparison
The m-RNN [NN] and m-RNNvgg [NN] are methods that do not use a ranking loss  and instead optimizes the log-likelihood of predicting the  next word in a sequence conditioned on an image
The  DeepVS [NN] is proposed to first learn an embedding space  with a bidirectional-RNN, and then train an RNN sentence generator based on the embedding space
Similarly, the NIC [NN] is another method that provides the  visual input directly to the RNN model
Recently, Deep  Structure-Preserving (DeepSP) [NN] is proposed for imageTable N
FlickrN0K experiments
R@K is Recall@K (high is  good)
Med r is the median rank (low is good)
 FlickrN0K  Model Image Annotation Image Search  R@N R@N0 Med r R@N R@N0 Med r  Random 0.N N.N NNN 0.N N.0 N00 SDT-RNN [NN] N.N NN.N NN N.N NN.N NN DeViSE [N] N.N NN.N NN N.N NN.N NN DeFrag [NN] NN.N NN.N N0 N0.N NN.N NN SC-NLM [NN] NN.N N0.N N0 NN.N NN.N NN DeepVS [NN] NN.N NN.N N.N NN.N N0.N N.N m-RNN [NN] NN.N N0.N N0 NN.N NN.N NN NIC [NN] NN.0 NN.0 N NN.0 NN.0 N m-RNN-vgg [NN] NN.N NN.N N NN.N NN.N N DeepSP [NN] NN.N NN.N N/A NN.N NN.N N/A HM-LSTM NN.N NN.N N NN.N NN.N N  Table N
MS-COCO experiments
R@K is Recall@K (high is  good)
Med r is the median rank (low is good)
 MS-COCO  Model Image Annotation Image Search  R@N R@N0 Med r R@N R@N0 Med r  Random 0.N N.N NNN 0.N N.0 N00 DeepVS [NN] NN.N N0.N N NN.N NN.N N m-RNN-vgg [NN] NN.0 NN.N N NN.0 NN.0 N DeepSP [NN] N0.N NN.N N/A NN.N NN.N N/A HM-LSTM NN.N NN.N N NN.N NN.N N  text embedding and achieves the state-of-the-art performance, where the captions for the same image are encouraged to be close to each other
 N.N.N Results on FlickrNK and FlickrN0K  We evaluate our approach on the FlickrNK and FlickrN0K
 Particularly, the dimension of the embedding space is set as  NNN, i.e., hj and vi are NNN-dimensional vectors
The R@K and Med r of different methods are shown in  Table N and Table N
Our model outperforms the rankingbased methods by a large margin
Besides, our method also  compares favorably with the state-of-the-art methods
 The results of DeepSP [NN] in Table N are based on the  mean vector representations, i.e., a sentence is represented  as the mean of their word embeddings
This is a fair comparison since both our model and this version of DeepSP  are based on the same word embeddings – wordNvec representation [NN]
Note that if more sophisticated sentence representations such as Fisher vector (FV) are utilized, the performance of DeepSP could be further improved [NN]
However, the memory cost is huge and hence it is not well-suited  to a large scale image-sentence ranking task
 N.N.N Results on MS-COCO  On the dataset of MS-COCO, we follow the experimental  setting of [NN] to randomly sample N, 000 images for test- ing
Specifically, the dimension of the embedding space  is set as NNN, and the Multiscale Combinatorial Grouping  NNNN    (MCG) [N] is adopted to replace the R-CNN to generate object proposals
 The results of the ranking tasks are shown in Table N
 Obviously, we can see that our method significantly outperforms the ranking-based methods
Even for the state-of-theart methods such as m-RNN-vgg [NN] and DeepSP [NN], our  approach still compares favorably with them
 From the results of image-sentence ranking on all three  datasets, we have a conclusion that the performance of general image captioning could be significantly improved by  learning a dense embedding space
This is attributed to the  joint embedding of full sentences and their phrases
Since  there are hierarchical relations among full sentences and  their phrases, such relations could benefit both their embedding learning when they are jointly represented and mapped  into the embedding space
 N.N
Region-Phrase Ranking  Our method can produce region-oriented phrase-level  description for a new image
Generally, after detecting  some salient image regions/object proposals, our model can  retrieve subtle and detailed phrases to describe them
For  easier evaluation, the image regions are manually annotated  instead of being automatically detected in this experiments
 For quantitative evaluation, we publish a new dataset  based on the MS-COCO, namely MS-COCO-region  dataset
Specifically, N000 images and corresponding sen- tences are randomly selected from the MS-COCO validation set
And then, AMT workers [NN] are asked to annotate image regions in those images and associate them to  the phrases within the sentences
Although some phraselevel captioning datasets such as Visual Genome [NN] and  FlickrN0k-Entities [NN] have been proposed, their phrases  either are freely annotated by workers or have no relations  with the sentences
On the contrary, the phrases in MSCOCO-region dataset are automatically extracted from the  given sentences, and there are hierarchical relations between sentences and phrases
 Specifically, for each sentence, N ∼ N noun phrases are automatically extracted by using Stanford Parser
For each  image, some AMT workers are asked to annotate N ∼ N re- gions and associate them to those extracted phrases
As a result, NNNN salient regions and NNNNN corresponding phrases are collected in total
 For comparison, DeepVS and m-RNN-vgg are adopted  as baselines, where each region-phrase pair is independently fed to those models to obtain their embeddings
The  results of region-phrase ranking are shown in Table N
Obviously, our method outperforms both DeepVS and m-RNNvgg
It is mainly because (N) the relations among phrases  are better utilized due to the hierarchical structure of our  model, and (N) the chain structured RNN is good at representing long sequences (i.e., full sentences) instead of short  Table N
Region-Phrase Ranking
R@K is Recall@K (high is  good)
Med r is the median rank (low is good)
 Region Annotation  Model R@N R@N R@N0 Med r  Random 0.0N 0.NN 0.NN NNNN DeepVS [NN] N.N NN.N NN.N NN m-RNN-vgg [NN] N.N N0.N NN.N NN HM-LSTM N0.N NN.N N0.N NN  (N) a white and gray cat with a striped tail  (N) a close up of a cat laying next to a mouse  (N) a white and gray cat  (N) a cat with an intent look  (N) his cat below  (a) a region of ‘cat’  (N) a cow standing in the grass with a tag in its ear  (N) a cow with a black face  (N) a cow staring into the camera  (N) mother cow laying next to her baby on the grass  (N) a close up of a black and white cow  (b) a region of ‘cow’  Figure N
Our approach can produce subtle and detailed descriptions for an image region
Besides, many descriptions are diverse  so that they can describe different aspects of an object
 sequences (i.e., phrases)
So we have a conclusion that our  model can jointly represent short phrases along with long  sentences, and better utilize their relations as well
 Qualitative results
Our method can describe image regions with detailed and subtle phrases
For example, for the  Fig
N(a) previous methods tend to describe it with a general and overview description, e.g., ‘A cat sitting under an  umbrella’
In contrast, our method targets a salient image  region (e.g., which is marked by red box), and produce detailed and subtle descriptions such as ‘a white and gray cat  with a strip tail’
Compared to the coarse description ‘a cat’,  our description is more informative and expressive
 In addition, our approach can produce some diverse descriptions for a given image region
As shown in Fig
N(b),  for the image region containing a ‘cow’, the top-N retrieved phrases diversely describe the ‘cow’, e.g., ‘a cow standing  in the grass with a tag in its ear’ focuses on the ear of the  cow, while ‘a cow staring into the camera’ focuses on the  action of the cow
In other words, our approach can diversely describe different aspects of an object of interest
 N.N
Discussion  N.N.N Learned Embedding Space  To intuitively and qualitatively check the properties of the  learned embedding space, we visualize the learned embedding vectors in a N-D space by using t-SNE [NN]
Specifically, we randomly sample N0 images and corresponding sentences from our MS-COCO testing dataset
And their  embedding vectors are visualized in a N-D space, as shown  in Fig
N
Particularly, we connect each image embedding to  N corresponding sentence embeddings by lines
We can see  NNNN    (N) two people (N) rocking chairs  (N) two people sitting on rocking chairs  (N) the deck  Two people sitting on rocking chairs on the deck
 (a)  (N) an empty room (N) a plant  (N) a painting (N) a plant and a painting  (N) wall  An empty room containing a plant and a   painting on the wall
 (b)  (N) modern kitchen (N) food items  (N) cooking and food items  Modern kitchen with assortment of cooking and   food items on counter
 (c)  Two giraffes and a zebra in an outdoor zoo
 (N) two giraffes (N) a zebra (N) outdoor zoo  (d)  Figure N
Four examples of the learned correspondences between phrases and image regions
For image (a), we obtain N phrases after  sentence parsing: (N) ‘two people’, (N) ‘rocking chairs’, (N) ‘the deck’, and (N) ‘two people sitting on rocking chairs’, meanwhile some  salient image regions are obtained
The learned correspondences between phrases and image regions are indicated by their color, e.g., the  phrase ‘two people’ corresponds to the orange box
Obviously, our approach can learn correct correspondences in most cases
Note that  (d) is a failure example, it is mainly due to that the salient regions do not cover the objects mentioned in its caption
 NN: The black dog runs with a ball with two smaller   dogs behind it
 NN: A dog has its head inside a red and green gift bag
 N: A brown dog drinks from a water bottle
 NN: A biker rides on a dirt road
 N: Rider jumps snowmobile high in rural area
 NN: A person takes a drink of water while riding on a   bike
 N0: The man and woman show off their matching skull   tattoos
 N: A man and a woman read a book while their friend   has a drink
 NN: Two woman wearing similar shirts walk to the left
 N: A little girl in a red shirt holds on to a pole near a   street  NN: The small girl in the red shirt pushes the little boy
 Figure N
The visualization of the learned embedding space
Each  image is connected to N corresponding sentences by lines
Obviously, the image and the corresponding sentences are very close  to each other in most cases
Besides, images/sentences with similar semantics are also close to each other, e.g., the NN-th, NN-th,  and N-nd images are all related to ‘Dog’, and their embeddings are  exactly neighbors in the embedding space (within the red circle)
 that the learned image embedding is very close to its sentence embeddings in most cases, which demonstrates the  effectiveness of our approach
 Moreover, from Fig
N we can see that our model can  learn a semantic embedding space, where images/sentences  with similar semantics will be mapped close to each other
 For example, the NN-th, NN-th, and N-th images are all related to ‘Dog’ (as shown by their descriptions)
And their learned  embedding vectors are exactly neighbors in the embedding  space (within the red circle)
 N.N.N Learned Phrase-level Correspondences  When learning the dense embedding space, our approach  can automatically find the ‘phrase-region’ correspondences  in the training data
We evaluate the quality of those learned  correspondences here
Since it is expensive to obtain the  ground truth phrase-level correspondences, we only make  an evaluation on a subset of training data
In practice,  we randomly sample N000 ‘phrase-region’ pairs from all learned phrase-level correspondences, and ask N0 users to judge whether each pair is correct
After a majority voting  among those users, we find out that NN% learned correspon- dences are correct
 Fig
N illustrates four examples of the learned correspondences between phrases and image regions
In most cases,  our approach is able to find correct correspondences
Moreover, there are consistent mappings between the phrases’ as  well as the regions’ hierarchical structures, e.g., the phrase  ‘two people sitting on rocking chairs’ is on top of two  phrases ‘two people’ and ‘rocking chairs’, meanwhile the  red box for ‘two people sitting on rocking chairs’ exactly  cover the orange box for ‘two people’ and the green box for  ‘rocking chairs’, etc
 N
Conclusion  In this paper, a Hierarchical Multimodal LSTM model  is proposed for dense visual-semantic embedding, which  can jointly learn the embeddings of all the sentences, their  phrases, images, and salient image regions
Due to the hierarchical structure, we can naturally build representations  for all phrases and image regions, and exploit their hierarchical relations as well
The experimental results turn out  that the performance of general image captioning can be  significantly improved due to learning a dense embedding  space
Bedsides, our method can produce detailed and diverse phrases to describe image salient regions
 N
Acknowledgement  This work was supported by NSFC Grant NNNNN0NN,  UNN0NNNN, NNN0NNNN, NNNNNN0N, NNN0NNNN, and NNN0NNNN, by  Key Industrial Innovation Chain N0NNKTZDGY-0N and National  High-Level Talents CSNNNNNN0000N
Dr
Gang Hua was supported by NSFC Grant NNNNNN0N
 NNNN    References  [N] P
Arbelaez, J
Tuset, J
Barron, F
Marques, and J
Malik
 Multiscale combinatorial grouping
CVPR, N0NN
 [N] X
Chen, H
Fang, T.-Y
Lin, R
Vedantam, S
Gupta, P
Dollar, and C
Zitnick
Microsoft coco captions: Data collection  and evaluation server
arXiv:NN0N.00NNN, N0NN
 [N] K
Cho, B
Merrienboer, C
Gulcehre, D
Bahdanau,  F
Bougares, H
Schwenk, and Y
Bengio
Learning phrase  representations using rnn encoder-decoder for statistical machine translation
arXiv:NN0N.N0NN, N0NN
 [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
Fei-Fei
 Imagenet: A large-scale hierarchical image database
CVPR,  N00N
 [N] J
Devlin, H
Cheng, H
Fang, S
Gupta, L
Deng, X
He,  G
Zweig, and M
Mitchell
Language models for image  captioning: The quirks and what works
arXiv:NN0N.0NN0N,  N0NN
 [N] A
Frome, G
Corrado, J
Shlens, S
Bengio, J
Dean, and  T
Ranzato
Devise: A deep visual-semantic embedding  model
NIPS, N0NN
 [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
CVPR, N0NN
 [N] C
Goller and A
Kuchler
Learning task-dependent distributed representations by backpropagation through structure
ICNN, NNNN
 [N] A
Graves
Generating sequences with recurrent neural networks
arXiv:NN0N.0NN0, N0NN
 [N0] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural computation, NNNN
 [NN] M
Hodosh, P
Young, and J
Hockenmaier
Framing image  description as a ranking task: Data, models and evaluation  metrics
JAIR, N0NN
 [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 ICML, N0NN
 [NN] J
Johnson, A
Karpathy, and L
Fei-Fei
Densecap: Fully  convolutional localization networks for dense captioning
 CVPR, N0NN
 [NN] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
CVPR, N0NN
 [NN] A
Karpathy, A
Joulin, and L
Fei-Fei
Deep fragment embeddings for bidirectional image-sentence mapping
NIPS,  N0NN
 [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
ICLR, N0NN
 [NN] R
Kiros, R
Salakhutdinov, and R
S
Zemel
Unifying  visual-semantic embeddings with multimodal neural language models
NIPS, N0NN
 [NN] D
Klein and C
Manning
Accurate unlexicalized parsing
 ACL, N00N
 [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L
Li, D
Shamma, M
Bernstein,  and L
Fei-Fei
Visual genome: Connecting language  and vision using crowdsourced dense image annotations
 https://arxiv.org/abs/NN0N.0NNNN, N0NN
 [N0] T
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollar, and C
Zitnick
Microsoft coco: Common  objects in context
arXiv:NN0N.0NNN, N0NN
 [NN] V
Maaten and G
Hinton
Visualizing high-dimensional  data using t-sne
Journal of Machine Learning Research,  N(NN):NNNN–NN0N, N00N
 [NN] J
Mao, W
Xu, Y
Yang, J
Wang, and A
Yuille
Explain  images with multimodal recurrent neural networks
NIPS,  N0NN
 [NN] J
Mao, W
Xu, Y
Yang, J
Wang, and A
Yuille
Deep captioning with multimodal recurrent neural networks
ICLR,  N0NN
 [NN] T
Mikolov, I
Sutskever, K
Chen, G
Corrado, and J
Dean
 Distributed representations of words and phrases and their  compositionality
NIPS, N0NN
 [NN] B
Plummer, L
Wang, C
Cervantes, J
Caicedo, J
Hockenmaier, and S
Lazebnik
FlickrN0k entities: Collecting region-to-phrase correspondences for richer image-tosentence models
ICCV, N0NN
 [NN] B
Plummer, L
Wang, C
Cervantes, J
Caicedo, J
Hockenmaier, and S
Lazebnik
FlickrN0k entities: Collecting region-to-phrase correspondences for richer image-tosentence models
IJCV, N0NN
 [NN] C
Rashtchian, P
Young, M
Hodosh, and J
Hockenmaier
 Collecting image annotations using amazons mechanical  turk
NAACL-HLT workshop, N0N0
 [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
Berg, and L
Fei-Fei
Imagenet large scale visual recognition challenge
CVPR, N0NN
 [NN] R
Socher, Q
Le, C
Manning, and A
Ng
Grounded compositional semantics for finding and describing images with  sentences
TACL, N0NN
 [N0] I
Sutskever, O
Vinyals, and Q
Le
Sequence to sequence  learning with neural networks
NIPS, N0NN
 [NN] K
Tai, R
Socher, and C
Manning
Improved semantic  representations from tree-structured long short-term memory  networks
ACL, N0NN
 [NN] J
Uijlings, K
Sande, T
Gevers, and A
Smeulders
Selective  search for object recognition
IJCV, N0NN
 [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
arXiv:NNNN.NNNN,  N0NN
 [NN] L
Wang, Y
Li, and S
Lazebnik
Learning deep structurepreserving image-text embeddings
CVPR, N0NN
 [NN] P
Young, A
Lai, M
Hodosh, and J
Hockenmaier
From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions
TACL,  N0NN
 NNNNUnsupervised Learning of Important Objects From First-Person Videos   Unsupervised Learning of Important Objects from First-Person Videos  Gedas BertasiusN, Hyun Soo ParkN, Stella X
YuN, Jianbo ShiN  NUniversity of Pennsylvania, NUniversity of Minnesota, NUC Berkeley ICSI  {gberta,jshi}@seas.upenn.edu hspark@umn.edu stella.yu@berkeley.edu  Abstract  A first-person camera, placed at a person’s head, captures, which objects are important to the camera wearer
 Most prior methods for this task learn to detect such important objects from the manually labeled first-person data  in a supervised fashion
However, important objects are  strongly related to the camera wearer’s internal state such  as his intentions and attention, and thus, only the person wearing the camera can provide the importance labels
 Such a constraint makes the annotation process costly and  limited in scalability
 In this work, we show that we can detect important objects in first-person images without the supervision by the  camera wearer or even third-person labelers
We formulate  an important detection problem as an interplay between the  N) segmentation and N) recognition agents
The segmentation agent first proposes a possible important object segmentation mask for each image, and then feeds it to the  recognition agent, which learns to predict an important object mask using visual semantics and spatial features
 We implement such an interplay between both agents via  an alternating cross-pathway supervision scheme inside our  proposed Visual-Spatial Network (VSN)
Our VSN consists  of spatial (“where”) and visual (“what”) pathways, one of  which learns common visual semantics while the other focuses on the spatial location cues
Our unsupervised learning is accomplished via a cross-pathway supervision, where  one pathway feeds its predictions to a segmentation agent,  which proposes a candidate important object segmentation  mask that is then used by the other pathway as a supervisory  signal
We show our method’s success on two different important object datasets, where our method achieves similar  or better results as the supervised methods
 N
Introduction  A question “what is where?” attempts to delineate a picture as a spatial arrangement of objects rather than a collection of unordered visual words, which inspires core computer vision tasks such as recognition, segmentation, and  ND reconstruction
This spatial arrangement encodes not  Figure N: Given an unlabeled set of first-person images our  goal is to find all objects that are important to the camera  wearer
Unlike most prior methods, we do so without using  ground truth importance labels
 only the physical relationship between objects in front of  the camera but also the interactions with the photographer  standing behind the cameraN
A picture is always taken by a  photographer reflecting what is important to her/him, which  provides a strong cue to infer the internal states such as  his/her intent, attention, and emotion
In particular, firstperson videos capture unscripted interactions with scenes  suggesting that the spatial layout is arranged such that the  objects can afford the associated actions, e.g., a cup appears  to be held by right hand from the holder’s point of view
 In this paper, we aim to detect objects that are important  to the photographer from a first-person video
Since importance is a subjective matter, the photographer is the only one  who can identify an important object
However, we conjecture that it is possible to detect important objects without  the supervision by the photographer or even third-person labelers because an important object exhibits common visual  semantics (what it looks like) and a spatial layout (where it  is in the first-person image)
 To achieve this goal, we formulate an important object  NFigure-ground segmentation, and saliency detection are a line of work  that addresses the relationship with the photographer
 NNNNN    detection task as an interaction between the N) segmentation  and N) recognition agents
Initially, the segmentation agent  generates a candidate important object mask for each image,  and relays this mask to the recognition agent, which then  tries to learn a classifier to predict such an important object  mask using visual semantics and spatial cues
 Our segmentation agent is implemented using an MCG  projection scheme, which employs the samples generated  from an unsupervised segmentation method [N] to propose  important object segmentation masks to the recognition  agent
Our recognition agent is implemented using the visual (“what”) and spatial (“where”) pathways of our proposed Visual-Spatial Network (VSN), each of which learns  to predict important object masks by asking questions “what  an important object looks like?” and “where an important  object is in the first-person image?”
We design these pathways using a fully convolutional network (FCN) while also  embedding a location dependent layer in the spatial pathway to learn the first-person spatial location prior
 Our VSN then learns to detect important objects without using manually annotated importance labels
We do so  via an alternating cross-pathway supervision, in a synergistic interplay between visual (“what”) and spatial (“where”)  pathways, and a segmentation agent
Each pathway’s output  is provided to a segmentation agent, which first generates a  possible important object segmentation mask and then relays it to the other pathway to be used as a supervisory signal
The supervision proceeds in such an alternating fashion  as each pathway improves each other, and as the segmentation agent becomes better as well
 Why Unsupervised Learning? Building a framework  that can learn without manually collected labels is particularly essential for first-person important object detection because the annotation task is not scalable at all unlike object  detection/segmentation [N, NN] where a consensus of third  parties from crowdsourcing mechanism can be used
In the  important object detection task, only the camera wearer can  perform the annotation task by looking back on his/her past  experiences
Prior methods [NN] have used a wearable gaze  tracker to label the camera wearer’s visual attention
However, gaze tracker is invasive and the data that it captures  has no notion of objects
Instead, our paper addresses these  issues via an unsupervised alternating cross-pathway learning scheme, which allows our method to achieve similar or  even better results as the supervised methods do
 N
Related Work  Important Object Detection in First-Person
There  have been a number of first-person methods that explored  important object detection task either as a main task [N,  NN, N], or as an auxiliary task for an activity recognition [NN, NN, N0, N] or video summarization [NN, NN]
The  work in [NN, N, NN, NN] employ hand-crafted appearance features, egocentric and optical flow features to describe a firstperson image, and then train a discriminative classifier to  detect the regions that correspond to the important objects
 The more recent work [N0, N] use FCNs [NN] to predict important objects end-to-end
Whereas the method in [N] employs a two stream visual appearance and ND network, the  work in [N0] exploits the connection between the activities  and objects and proposes a two stream appearance and optical flow network with a multi-loss objective function
 All of these methods use manually annotated important  object labels, which may be costly and difficult to obtain
 Our approach, on the other hand, introduces a new unsupervised learning scheme that allows us to learn important  objects without manually labeled importance annotations
 Training FCNs with Weakly-Labeled Data
Recently, there have been several deep learning approaches  that proposed learning with weakly labeled or unlabeled  datasets [NN, N, N, NN, NN, NN, NN, NN, NN] 
Due to the high  cost of obtaining per-pixel labels, this has been a particularly relevant problem for semantic segmentation
 The weakest form of supervision for semantic segmentation includes image-level labels, which were used to train  FCNs in several prior approaches [NN, NN, NN, NN]
Some recent work [N] used point supervision, which requires almost  as much effort as the image-level labels but also provides  some spatial information
Several approaches employed  free form squiggles as a supervisory signal [NN, NN] which  provides even more information, and are still easy enough to  annotate
Furthermore, several approaches utilized bounding box level annotations for FCN training [NN, N]
Finally,  recent work achieved excellent edge detection results without using any annotations at all [NN]
 In comparison to prior work, which focuses on the  third-person data, our method focuses on the first-person  data
Unlike third-person object detection/segmentation  tasks where annotations can be obtained via a crowdsourcing mechanism, important object detection task requires the  camera wearer to provide the labels, which severely limits  its scalability
Due to such a constraint, an unsupervised  learning framework is particularly important for the important object detection task in the first-person setting
 N
Approach Motivation  Our goal is to N) recognize and N) segment important  objects from a first-person image in an unsupervised setting
Thus, we want our method to have two key properties: N) it needs to segment the important objects from the  background based on the low-level grouping cues and N) it  needs to be discriminative, i.e, recognize objects that are  important and ignore all the irrelevant objects
 To achieve these goals, we frame an important object  prediction task as an interplay between the N) recognition  and N) segmentation agents, where a segmentation agent  NNNN    (N) Initial Round \  FCN  Unlabeled First-Person Training Images:     MCG   Projection  Visual Pathway Predictions  Spatial Pathway Predictions     MCG   Projection     MCG   Projection  Backward Pass  Forward Pass Backward Pass  Backward Pass Forward Pass  (N) VNS Round  (N) SNV Round  0 Iterations  N000 Iterations  N000 Iterations  N000 Iterations  Supervisory Signal  XY Coordinate Concatenation  Spatial “Where” Pathway  FCN VGG FCN architecture  Visual “What” Pathway  VGG FCN architecture FCN  Supervisory Signal  Supervisory Signal  Spatial Important Object Location Prior     MCG   Projection  Supervisory Signal  N000 Iterations  (N) VNS Round  Forward PassBackward Pass  Visual Pathway Predictions  Figure N: We implement an interplay between the segmentation and recognition agents via an alternating cross-pathway  supervision scheme inside our proposed Visual-Spatial Network (VSN)
Our VSN consists of the N) visual (“what”) and N)  spatial (“where”) pathways, which both act as recognition agents
In between these two pathways, the VSN uses an MCG  projection scheme, which acts as a segmentation agent
Then, given a set of unlabeled first-person training images, we first  guess “where” an important object is in the first-person image and use an MCG projection scheme to propose important  object segmentation masks
These masks are then used a supervisory signal to train a visual pathway such that it would learn  “what” an important object looks like
Then, in the VNS round, the predictions from the visual pathway are passed through  the MCG projection, and transfered to the spatial pathway
The spatial pathway then learns “where” an important object is in  the first-person image
Such an alternating cross-pathway supervision scheme is repeated for several rounds
 first proposes a possible important object mask, which a  recognition agent then uses as a supervisory signal to learn  an important object classifier based on visual (“what”) and  spatial (“where”) cues
 The main challenge of our unsupervised learning framework is to prevent overfitting of either a segmentation or  a recognition agent
If the segmentation agent proposes  too many different segments, the recognition agent will not  learn a concept of important objects (particularly if these  segments are not recurring)
On the other hand, if the recognition agent narrowly focuses on predicting one type of object, or an object that appears at a particular location, it will  not generalize across all images
We address the first issue  by feeding the predictions from the recognition agent to the  segmentation agent, so that the target segmentations would  consistently improve as the recognition agent gets better
To  tackle the second issue, we force the recognition agent to  learn a diverse model by making it focus on visual (“what”)  and spatial (“where”) cues in an alternating fashion
 We now provide more details related to the N) segmentation and N) recognition agents that we want to use for our  unsupervised learning task
 N.N
Segmentation Agent  The goal of a segmentation agent is to propose segmentation masks of the important objects, which could then be  used by a recognition agent as a supervisory signal
We implement such a segmentation agent via our introduced MCG  projection scheme
We define MCG projection as a function  h(A,R) that takes two inputs: N) a coarse per-pixel impor- tant object mask prediction A, and N) a set of regions R  obtained from a segmentation method MCG [N]
The output h(A,R) then captures an important object segmentation mask proposed by a segmentation agent
 We first run an MCG [N] segmentation algorithm, which  segments a given image into regions R
Then, for every  MCG region R, we compute the mean value of all values in  A that fall in the region R, and assign that value to the entire region R
Since MCG regions overlap with each other,  the pixels belonging to multiple overlapping regions, get assigned multiple values (from each region they belong to)
 To assign a single value to a given pixel, we perform maxNNNN    pooling, over the values of that pixel in each of the regions  that contains that pixel
This then produces a candidate important object segmentation mask
 N.N
Recognition Agent: Motivation  To build a recognition agent that is discriminative, and  yet generalizable, we focus on two distinct aspects of an  important object prediction task: the “what” (what does an  important object look like?) and the “where” (where does  an important object appear in the first-person image?)
 The Visual Cues (What it looks like?) A natural way  to predict important objects is by learning “what” they look  like
Such learned visual appearance cues can then be  used to predict important objects in an image
This is exactly what is done by the supervised methods, which use  the ground-truth data to learn the visual characteristics of  “what” a prototypical important object looks like in a firstperson image
However, in the context of our problem, we  do not have access to such ground-truth data
Thus, the key  question becomes whether we can learn to detect important  objects despite not knowing “what” they look like beforehand?  The Spatial Cues (Where it is?) We conjecture that  important objects are spatially arranged in the first-person  image to afford the camera wearer’s interactions with those  objects
In other words, by performing activities, and looking at things, the camera wearer is implicitly labeling what  is important to him, which is also captured in a first-person  image
For instance, a cup often appears at the bottom right  of a first-person image, because most people look down at  it and also hold it with their right hand
 Thus, since N) people typically look down at an object,  with which they interact, and N) since most people are righthanded, we conjecture that many important objects appear  at the bottom-right of a first-person image, which we guess  to be at (x, y) location (0.NW, 0.NNH), where W and H denote the width and height of the first-person image
We  refer to this location as a spatial important object location  prior
 Since we do not have ground truth labels, we cannot directly supervise our network by telling it “what” an important object looks like
However, we can tell the network  “where” we think an important object is such that the network would learn the visual appearance cues necessary to  recognize “what” appears at that location
In the best case,  there will be a true important object at our specified location, and the network will then learn “what” that important  object looks like
Otherwise, if our guess is incorrect, the  network will try to learn a meaningless pattern of “what”  something that is not an important object looks like
If we  make enough correct guesses of “where” the true important  objects are, our network will learn “what” important objects  look like without ever using ground truth importance labels
 N
Visual-Spatial Network  To holistically integrate both segmentation and recognition agents, we introduce a Visual-Spatial Network (VSN)  that learns to detect important objects from unlabeled firstperson data
Our network consists of the N) visual (“what”)  and N) spatial (“where”) pathways, which act as recognition  agents
In between these two pathways, the VSN employs  an MCG projection scheme, which acts as a segmentation  agent
 During training, we first use an MCG projection to  propose a candidate important object segmentation mask,  which is then used by the visual “what” pathway as a supervisory signal
Then, the predictions from the visual pathway  are used by the segmentation agent to generate an improved  important object segmentation mask, which is used as a supervisory signal by the spatial “where” pathway
Such a supervision scheme between the two pathways proceeds in an  alternating fashion, allowing each pathway to improve each  other, while the segmentation agent also improves
We refer  to such a learning scheme as a cross-pathway supervision,  which we illustrate in Fig
N
 N.N
Visual “What” Pathway  The visual pathway of our VSN is based on a fully convolutional VGG architecture [NN], which is pretrained for  the segmentation task on Pascal VOC dataset with N0 dis- tinct classes such as airplane, bus, cow, etc
We note that  the classes in Pascal VOC dataset are quite different compared to the important object classes in the datasets that we  use for our experiments
For instance, Pascal VOC segmentation dataset does not include annotations for classes such  as food package, knife, suitcase, sweater, pizza and many  more object classes
In the experimental section, we also  verify this claim by showing that the VGG FCN [NN] that  was pretrained for the Pascal VOC semantic segmentation  task alone produces poor important object detection results
 We want to make it clear that we do not claim that our  method does not use any annotations at all
Our main claim  is that we can learn to detect important objects in firstperson images without manually annotated first-person importance labels
Our network still needs a general visual  recognition capability to differentiate between various visual appearance cues
Otherwise, due to a noisy supervisory signals that we use to train each pathway, our network  would struggle to learn the visual cues that are indicative of  true important objects
 N.N
Spatial “Where” Pathway  The spatial pathway is also based on the pretrained VGG  FCN [NN]
However, unlike the visual pathway, the spatial pathway incorporates a two-channel grid of normalized X and Y coordinates that correspond to every pixel  NNNN    in the first-person image
These X,Y coordinate meshgrids could be obtained by calling a matlab command  [X,Y]=meshgrid(N:W,N:H), where W,H are the width  and height of an image respectively
We then use a bilinear  interpolation to downsample these grids N times and concatenate them to the visual fcN features
Such concatenated  representation is then used as an input to the fcN layer that  predicts important objects
Note that we do not concatenate  X,Y grids with the input image so that we could preserve  the original structure in the early layers of a VGG network,  and use the VGG weights as an initialization
 N.N
Alternating Cross-Pathway Supervision  We now describe our alternating cross-pathway supervision scheme, which is implemented via a synergistic interplay between the spatial and the visual pathways, and with  a segmentation agent in between these two pathways
 Initial Round
In the initial round, we want the visual  pathway to predict important objects based on “what” they  look like
It should learn to do so from the important object  segmentation masks provided by an MCG projection step
 These initial segmentation masks are constructed based on  our guesses “where” important objects might appear in the  first-person image
 Formally, we are given a batch of unlabeled first-person  RGB images, which we denote as B ∈ RN×C×H×W ,  where N depicts a batch size, H and W refer to the height  and width of an image, and C refers to the number of channels (C = N for RGB images)
Then, let G ∈ RN×H×W  denote images with a Gaussian placed around a spatial important object prior location (0.NW, 0.NNH)
Furthermore let h denote the MCG projection function  that takes two inputs: N) a coarse important object mask A,  and N) MCG regions R, and outputs a candidate important  object segmentation mask h(A,R)
Finally, let f(B) ∈ RN×H×W depict the output of the  visual pathway that takes a batch of first-person images as  its input and outputs a per-pixel important objects map for  every image in the batch
Then the cross-entropy loss that  we minimize during the initial round is:  L = −  N X  i=N  H×W X  j=N  h  hj(G (i), R(i)) log (fj(B  (i)))  + (N− hj(G (i), R(i))) log (N− fj(B  (i))) i  VNS Round
During the VNS (Visual to Spatial) round,  given the important object masks based on “what” they look  like, we want spatial pathway to find image segments in the  first-person image “where” such important objects appear
 Formally, let g(B,X, Y ) ∈ RN×H×W depict the output of the spatial pathway, where in this case X,Y denote a  batch of normalized coordinate grids (each with dimensions  EgoNet [N] VSN Ground Truth  Figure N: The qualitative important object predictions results
Despite not using any importance labels during training, our VSN correctly recognizes and localizes important  objects in all three cases
 N×H×W )
Then the cross-entropy loss that we minimize  during the VNS round is:  L =−  N X  i=N  H×W X  j=N  h  hj(f(B (i)), R(i)) log (gj(B  (i), X(i), Y (i)))  + (N− hj(f(B (i)), R(i))) log (N− gj(B  (i), X(i), Y (i))) i  SNV Round
In the SNV (Spatial to Visual) round, the  visual pathway receives important object masks from the  spatial pathway
Then, based on the spatial pathway’s predictions “where” an important object is, the visual pathway tries to learn “what” those important objects look like
 The cross-entropy loss function that we minimize during the  SNV round is:  L =−  N X  i=N  H×W X  j=N  h  hj(g(B (i), X(i), Y (i)), R(i)) log (fj(B  (i)))  + (N− hj(g(B (i), X(i), Y (i)), R(i))) log (N− fj(B  (i))) i  Alternation
We alternate our cross-pathway supervision process between the VNS and SNV rounds until there  is no significant change in performance (N-N rounds)
Such  an alternating learning scheme is beneficial because different visual/spatial feature inputs to the two pathways, force  each pathway to maintain focus on objects that exhibit different spatial/visual characteristics
For instance, the spatial  pathway can focus on objects that are at the same spatial  location, but exhibit different visual features
In contrast,  the visual pathway is able to focus on the objects that look  similar but are at different locations
Such an alternation between the two pathways provides diversity to our learning  scheme, which we empirically show to be beneficial
 NNN0    N.N
Using Extra Unlabeled Data for Training  We note that unlike supervised methods that use manually annotated importance labels, we use unlabeled data,  which leads to a much harder learning task
We compensate the lack of importance labels with large amounts of  unlabeled data, a strategy, which was also used by an unsupervised edge detector [NN]
For all of our experiments, we  train our VSN on the combined datasets of (N) first-person  important object RGBD [N], (N) GTEA Gaze+ [NN], and (N)  five relevant first-person videos downloaded from YouTube  (without using the labels even if they exist)
We note that  using more unlabeled data to train our model is essential  for achieving the results that are competitive with the supervised methods’ performance
 We point out that our method’s ability to use unlabeled  data for training is a big advantage in comparison to the supervised methods
The performance of CNNs typically improves with more training data, and unlabeled data is easy  and cheap to obtain
In comparison, getting labeled data  is costly and time consuming, especially if it requires perpixel labels as in our work
 N.N
Prediction during Testing  During testing, we average the predictions from the visual and spatial pathways
Such a prediction scheme allows  each pathway to correct some of the other pathway’s mistakes, and achieve a better important object prediction accuracy than any individual pathway alone would
 N.N
Implementation Details  For all of our experiments, we used a Caffe deep learning  library [N0]
We employed visual and spatial pathways that  adapted the VGG FCN architecture [NN]
During training,  each of the optimization rounds was set to N000 iterations
During those rounds one of the selected pathways was optimized to minimize the per-pixel sigmoid cross entropy loss,  while the other was fixed
We performed N rounds in total, which was enough to reach convergence
During the training we used a learning rate of N0−N, the momentum equal to 0.N, the weight decay of 0.000N, and the batch size of NN
 N
Experimental Results  In this section, we present quantitative and qualitative results of our VSN method
We test our method on two firstperson datasets, that have per pixel important object annotations: (N) First-Person Important Object RGBD [N], and  (N) GTEA Gaze+ [NN] datasets
Even though both datasets  have annotated importance labels, they are quite different
 GTEA Gaze+ dataset captures the activities of cooking different meals, and thus there is less variation in the scene and  the activity itself
In comparison, the first-person important  object RGBD dataset is smaller but captures people doing  FP-AO-RGBD GTEA Gaze+ mean  Method MF AP MF AP MF AP  VGG FCN [NN] 0.NNN 0.N0N 0.NNN 0.NNN 0.NNN 0.NN0  GBVS [N] 0.NNN 0.NNN 0.NNN 0.NNN 0.NN0 0.NNN  Judd [NN] 0.NNN 0.N0N 0.N0N 0.NNN 0.NNN 0.NNN  DCL [NN] 0.NNN 0.0NN 0.NNN 0.NN0 0.NNN 0.0NN  SIOLP 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN 0.NNN  Trained SIOLP‡ 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  FP-MCG [N]‡ 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  DeepLab [N]‡ 0.NN0 0.NNN 0.NNN 0.NN0 0.NNN 0.NNN  EgoNet [N]‡ 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  VSN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  VSN+EgoNet‡ 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN 0.NNN  Table N: The quantitative important object prediction results on the first-person important object RGBD and GTEA  Gaze+ datasets according to the max F-score (MF) and average precision (AP) metrics
Our results indicate that even  without using important object labels our VSN achieves  similar or even better results than the supervised baselines
 Supervised methods are marked with ‡
 seven different activities in pretty different scenes, which  makes the dataset more diverse and slightly more challenging
The First-Person Important Object RGBD dataset  has NNNN annotated examples from seven video sequences, whereas for the GTEA Gaze+ dataset we use NNNN images from NN different sequences
 We evaluate the important object detection accuracy using max F-score (MF), and average precision (AP), which  are obtained by thresholding the probabilistic important object maps at small intervals and computing a precision and  recall curve against the ground-truth important objects
 As our baselines we use a collection of the methods that  were recently shown to perform well on this task as well  as some of our own baselines
EgoNet [N] is a two-stream  network that incorporates appearance and ND cues to de- tect important objects
We also include a DeepLab [N] system, which we train for the important object detection task
 Additionally, we incorporate a MCG [N] method trained  for first-person important object detection (FP-MCG)
Furthermore, we include three popular visual saliency methods: (N) Judd [NN], (N) GBVS [N], and (N) Deep Contrast  Saliency method [NN]
Additionally, we also evaluate the  results achieved by (N) a spatial important object location  prior (SIOLP), and (N) a spatial important object location  prior that was obtained by extracting it from the training  data using ground-truth important object labels
Furthermore, to show that the network that we used to pretrain our  VSN performs poorly by itself, we include a VGG FCN [NN]  baseline
To obtain important object predictions we simply  sum up the probabilities for all N0 predicted Pascal VOC classes
Finally, to show that the predictions of our VSN  method are highly complementary to the best performing  NNNN    RGB Input Spatial Pathway Visual Pathway  (a) Spatial pathway performing better than the visual pathway  RGB Input Spatial Pathway Visual Pathway  (b) Visual pathway performing better than the spatial pathway  Figure N: A figure illustrating a qualitative important object prediction comparison between the visual and spatial pathways  (best viewed in color)
Subfigure N(a) illustrates instances where the spatial pathway’s reliance on location features is beneficial: it detects small and partially occluded important objects, which the visual pathway fails to detect accurately
The  Subfigure N(b) shows instances where the spatial pathway’s reliance on location features leads to incorrect results: it falsely  marks regions in the first-person image as important objects just because they appear at a certain location in the first-person  image
In contrast, the visual pathway correctly predicts important objects in those instances
 EgoNet method’s predictions, we combine these two methods via averaging, and demonstrate that for each dataset  VSN significantly improves EgoNet’s results
 We also note that Judd [NN], GBVS [N], DCL [NN],  SIOLP, VGG-FCN, and our VSN methods do not use any  important object annotations
All the other methods are  trained using the manually annotated important object labels
We also note that all the FCN baselines (VGG-FCN,  DeepLab, EgoNet and VSN) were pretrained for semantic  segmentation under the same conditions
 We used publicly available implementations of VGGFCN, GBVS, Judd, FP-MCG [N], and DeepLab [N] and  trained and evaluated all these baselines ourselves
We  obtained the results for EgoNet from the technical report  in [N]
To the best of our knowledge EgoNet is currently the  best performing method in this task, and thus, to compare  to the most recent and best performing system, we adopted  the evaluation procedure from [N]
 Our evaluations provide evidence for several conclusions
In Subsections N.N, N.N, we show that despite not  using any important object labels our VSN achieves results  similar or even better than the supervised methods do
Furthermore, In Subsection N.N, we provide a few ablation experiments, which show that N) using both visual and spatial  pathways is beneficial, N) the location of an important object  spatial prior is important, and that N) using more unlabeled  training data leads to better results
 N.N
Results on Important Object RGBD Dataset  In Table N, we present important object detection results  on the First-Person Important Object RGBD dataset [N], averaged over N video sequences from different activities
The results indicate that our VSN achieves the best per-class  mean MF and AP scores
These results may seem surprising, because unlike EgoNet and all the other supervised  baselines, our VSN does not use any important object annotations
However, VSN uses a larger amount of unlabeled  data for its training, which leads to a better performance
 We also note that the VGG-FCN, which we use as an  initialization for both of our VSN pathways, achieves the  worst performance among all the baselines, which suggests  that predicting N0 Pascal VOC classes alone is not enough to achieve a good performance on the important object detection task
We also point out that combining VSN and  EgoNet predictions, leads to a greatly improved accuracy  according to both metrics, which implies that both methods  learn complementary important object information
 In Figure N, we also compare qualitative important object detection results of our VSN and a supervised EgoNet  model
We show that unlike EgoNet, our VSN correctly  detects and segments important objects in all three cases
 N.N
Results on GTEA Gaze+ Dataset  In Table N, we present MF and AP important object detection results on the GTEA Gaze+ dataset [NN] averaged  over NN videos
The results indicate that our VSN outper- forms all the other methods according to AP metric, and is  outperformed only by EgoNet according to the MF metric
 We also note that just like with the previous dataset, combining VSN and EgoNet predictions leads to a dramatic accuracy boost according to both metrics
 N.N
Ablation Experiments  The Need for Spatial and Visual Pathways
One may  notice that the spatial pathway is a more powerful version of  a visual pathway since it can use both spatial and visual cues  to predict important objects
Therefore, a natural question  is whether we need a visual pathway at all
 To answer this question we quantitatively compare our  approach to the baselines that use either two visual pathNNNN    VVN SSN VSN 0.N  0.NN  0.NN  0.NN  0.NN  0.N  0.NN  0.NN  0.NN  Methods  F −  S c o re        Round N  Round N  Round N  Final Prediction  Figure N: Our results demonstrate that using a visual and  a spatial pathway (VSN) yields better important object detection accuracy than using either two visual (VVN) or two  spatial pathways (SSN)
 ways (VVN) or two spatial pathways (SSN)
We present  these results in Figure N, where we show that our VSN  method achieves 0.NNN MF accuracy,whereas the VVN and SSN baselines yield 0.N0N and 0.N00 MF accuracy respec- tively, suggesting that having a visual and a spatial pathway  in the network is beneficial
 In Figure N, we also present a few qualitative comparisons between the predictions from the visual and spatial  pathways
In Subfigure N(a), we illustrate instances where  the spatial pathway’s reliance on location features is beneficial: unlike the visual pathway, it is able to detect small  and partially occluded important objects because they appear at a certain location
However, in Subfigure N(b), we  present instances where the spatial pathway’s reliance on  location features leads to incorrect results: it falsely marks  regions in the first-person image as important objects just  because those regions appear at a certain location in an image
In contrast, in those cases, the visual pathway correctly  predicts important objects because it makes the predictions  based on “what” those objects look like
 Thus, these qualitative and quantitative results suggest  that the spatial and visual pathways can complement each  other, and thus, having both of them is beneficial
 Selecting a Spatial Important Object Prior
The initial selection of a location prior is critical to the success  of our method
To validate its importance, we run several experiments on important object RGBD dataset with  different location priors
First, we experiment with a location (0.NW, 0.NH) where H and W are the height and the width of an image
This is a center prior commonly used  by first-person methods
Using this location prior, the pathway that was trained the last yields 0.NN MF score, suggest- ing that in this case, a center location does not capture important objects well
In comparison, the pathway that was  trained the last in our original model (i.e
the location prior  (0.NW, 0.NNH)) yields 0.N0N MF
 Given how important the selection of a location prior is,  we need a principled way to select it
To do this we propose  to utilize a generic hand detector or an unsupervised visual  saliency detector on our dataset (not trained on our dataset)
 Then, for each image we can compute a weighted average  of XY locations in the image (weighted by the hand detector probabilities), and then compute an average of these  weighted average locations across the entire dataset
 We report that when applied on an important object  RGBD dataset, such a scheme yields a location prior of  (0.NNNW, 0.NNNH)
Training the VSN using this spatial prior then yields almost equivalent results as our original  model that uses (0.NW, 0.NNH) location prior
We also note that simply detecting hands is not enough to detect important objects
A baseline that detects all objects and hands in  the scene, and then uses objects that are closest to the hands  as important object predictions yields 0.NNN MF
 Importance of Unlabeled Training Data Size
Finally,  we also want to verify that using more unlabeled training  data leads to better results
To do this, we train VSN on  the same amount of unlabeled data, as there is labeled data  used by the supervised methods (NNNN samples)
We report that using less unlabeled data leads to 0.NNN MF, which is substantially lower than our original model
 N
Conclusions  In this work, we propose to detect important objects from  unlabeled first-person images by formulating our problem  as an interplay between the N) recognition and N) segmentation agents
To do this, we integrate these two agents inside an alternating cross-pathway supervision scheme of our  proposed Visual-Spatial Network (VSN)
The MCG projection scheme (a segmentation agent) proposes important  object segmentation masks, whereas the spatial and visual  pathways (recognition agents) use these masks as a supervisory signal to predict important object masks based on  visual semantics and spatial features
We demonstrate the  effectiveness of such scheme by showing that it achieves  similar or even better results than the supervised methods
 We believe that in the future, our method could be extended to other tasks such as first-person activity recognition, or egocentric video summarization
Furthermore, our  method’s ability to learn without manually annotated labels could be used to learn from large-scale unlabeled firstperson datasets on the web, and in the long run, replace the  supervised methods, which are constrained by the amount  of available annotated data
 References  [N] P
Arbeláez, J
Pont-Tuset, J
Barron, F
Marques, and J
Malik
Multiscale combinatorial grouping
In CVPR, N0NN
N, N, N, N  NNNN    [N] Amy Bearman, Olga Russakovsky, Vittorio Ferrari, and Li Fei-Fei
 What’s the Point: Semantic Segmentation with Point Supervision
 ECCV, N0NN
N  [N] Gedas Bertasius, Hyun Soo Park, Stella X
Yu, and Jianbo Shi
 First-person action-object detection with egonet
In Proceedings of  Robotics: Science and Systems, July N0NN
N, N, N, N  [N] Liang-Chieh Chen, George Papandreou, Iasonas Kokkinos, Kevin  Murphy, and Alan L
Yuille
Semantic image segmentation with deep  convolutional nets and fully connected crfs
In ICLR, N0NN
N, N  [N] Jifeng Dai, Kaiming He, and Jian Sun
Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation
CoRR, N0NN
N  [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
Fei-Fei
ImageNet: A Large-Scale Hierarchical Image Database
In CVPR0N,  N00N
N  [N] Alireza Fathi, Ali Farhadi, and James M
Rehg
Understanding egocentric activities
In ICCV
N  [N] Alireza Fathi, Xiaofeng Ren, and James M
Rehg
Learning to recognize objects in egocentric activities
In CVPR, pages NNNN–NNNN
 IEEE Computer Society, N0NN
N  [N] Jonathan Harel, Christof Koch, and Pietro Perona
Graph-based visual saliency
In NIPS, N00N
N, N  [N0] Yangqing Jia, Evan Shelhamer, Jeff Donahue, Sergey Karayev,  Jonathan Long, Ross Girshick, Sergio Guadarrama, and Trevor Darrell
Caffe: Convolutional architecture for fast feature embedding
 arXiv:NN0N.N0NN, N0NN
N  [NN] Tilke Judd, Krista Ehinger, Frédo Durand, and Antonio Torralba
 Learning to predict where humans look
In IEEE International Conference on Computer Vision (ICCV), N00N
N, N  [NN] Yong Jae Lee and Kristen Grauman
Predicting important objects for  egocentric video summarization
IJCV, N0NN
N  [NN] G
Li and Y
Yu
Deep contrast learning for salient object detection
 In IEEE Conference on Computer Vision and Pattern Recognition  (CVPR), pages NNN–NNN, June N0NN
N, N  [NN] Yin Li, Manohar Paluri, James M
Rehg, and Piotr Dollar
Unsupervised learning of edges
In The IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), June N0NN
N, N  [NN] Yin Li, Zhefan Ye, and James M
Rehg
Delving into egocentric  actions
In CVPR
N, N, N  [NN] Di Lin, Jifeng Dai, Jiaya Jia, Kaiming He, and Jian Sun
Scribblesup: Scribble-supervised convolutional networks for semantic segmentation
In The IEEE Conference on Computer Vision and Pattern  Recognition (CVPR), June N0NN
N  [NN] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro  Perona, Deva Ramanan, Piotr Dollár, and C
Lawrence Zitnick
Microsoft coco: Common objects in context
In European Conference  on Computer Vision (ECCV), Zürich, September N0NN
N  [NN] Jonathan Long, Evan Shelhamer, and Trevor Darrell
Fully convolutional networks for semantic segmentation
CoRR, N0NN
N  [NN] Zheng Lu and Kristen Grauman
Story-driven summarization for  egocentric video
In CVPR, N0NN
N  [N0] Kris Kitani Minghuang Ma
Going deeper into first-person activity  recognition
In Conference on Computer Vision and Pattern Recognition (CVPR), N0NN
N  [NN] George Papandreou, Liang-Chieh Chen, Kevin Murphy, and Alan L  Yuille
Weakly- and semi-supervised learning of a dcnn for semantic  image segmentation
arxiv:NN0N.0NNNN, N0NN
N  [NN] Deepak Pathak, Philipp Krähenbühl, and Trevor Darrell
Constrained  convolutional neural networks for weakly supervised segmentation
 In ICCV, N0NN
N  [NN] Deepak Pathak, Evan Shelhamer, Jonathan Long, and Trevor Darrell
 Fully convolutional multi-class multiple instance learning
In ICLR  Workshop, N0NN
N  [NN] P
H
O
Pinheiro and R
Collobert
From image-level to pixel-level  labeling with convolutional networks
In Conference on Computer  Vision and Pattern Recognition (CVPR), N0NN
N  [NN] Hamed Pirsiavash and Deva Ramanan
Detecting activities of daily  living in first-person camera views
In CVPR, N0NN
N  [NN] Xiaofeng Ren and Chunhui Gu
Figure-ground segmentation improves handled object recognition in egocentric video
In CVPR,  N0N0
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks  for large-scale image recognition
arXiv:NN0N.NNNN, N0NN
N, N  [NN] Jia Xu, Alexander G
Schwing, and Raquel Urtasun
Learning to  segment under various forms of weak supervision
In Proc
CVPR,  N0NN
N  NNNNDSOD: Learning Deeply Supervised Object Detectors From Scratch   DSOD: Learning Deeply Supervised Object Detectors from Scratch  Zhiqiang Shen∗N, Zhuang Liu∗N, Jianguo LiN, Yu-Gang JiangN, Yurong ChenN, Xiangyang XueN  NFudan University, NTsinghua University, NIntel Labs China  {zhiqiangshenNN, ygj, xyxue}@fudan.edu.cn, liuzhuangthu@gmail.com  {jianguo.li, yurong.chen}@intel.com  Abstract  We present Deeply Supervised Object Detector (DSOD),  a framework that can learn object detectors from scratch
 State-of-the-art object objectors rely heavily on the offthe-shelf networks pre-trained on large-scale classification  datasets like ImageNet, which incurs learning bias due to  the difference on both the loss functions and the category  distributions between classification and detection tasks
 Model fine-tuning for the detection task could alleviate this  bias to some extent but not fundamentally
Besides, transferring pre-trained models from classification to detection  between discrepant domains is even more difficult (e.g
RGB  to depth images)
A better solution to tackle these two critical problems is to train object detectors from scratch, which  motivates our proposed DSOD
Previous efforts in this direction mostly failed due to much more complicated loss  functions and limited training data in object detection
In  DSOD, we contribute a set of design principles for training  object detectors from scratch
One of the key findings is that  deep supervision, enabled by dense layer-wise connections,  plays a critical role in learning a good detector
Combining  with several other principles, we develop DSOD following  the single-shot detection (SSD) framework
Experiments on  PASCAL VOC N00N, N0NN and MS COCO datasets demonstrate that DSOD can achieve better results than the stateof-the-art solutions with much more compact models
For  instance, DSOD outperforms SSD on all three benchmarks  with real-time detection speed, while requires only N/N parameters to SSD and N/N0 parameters to Faster RCNN
 N
Introduction  Convolutional Neural Networks (CNNs) have produced  impressive performance improvements in many computer  vision tasks, such as image classification [NN, NN, NN, N, N0],  object detection [N, N, NN, NN, NN, NN], image segmenta∗indicates equal contribution
This work was done when Zhiqiang Shen  and Zhuang Liu were interns at Intel Labs China
Jianguo Li is the corresponding author
 tion [NN, N, N, NN], etc
In the past several years, many  innovative CNN network structures have been proposed
 Szegedy et al
[NN] propose an “Inception” module which  concatenates features maps produced by various sized filters
He et al
[N] propose residual learning blocks with skip  connections, which enable training very deep networks with  more than N00 layers
Huang et al
[N0] propose DenseNets  with dense layer-wise connections
Thanks to these excellent network structures, the accuracy of many vision tasks  has been greatly improved
Among them, object detection  is one of the fastest moving areas due to its wide applications in surveillance, autonomous driving, etc
 In order to achieve good performance, most of the advanced object detection systems fine-tune networks pretrained on ImageNet [N]
This fine-tuning process is also  viewed as transfer learning [NN]
Fine-tuning from pretrained models has at least two advantages
First, there are  many state-of-the-art deep models publicly available
It is  very convenient to reuse them for object detection
Second, fine-tuning can quickly generate the final model and requires much less instance-level annotated training data than  the classification task
 However, there are also critical limitations when adopting the pre-trained networks in object detection: (N) Limited  structure design space
The pre-trained network models are  mostly from ImageNet-based classification task, which are  usually very heavy — containing a huge number of parameters
Existing object detectors directly adopt the pre-trained  networks, and as a result there is little flexibility to control/adjust the network structures (even for small changes  of network structure)
The requirement of computing resources is also bounded by the heavy network structures
 (N) Learning bias
As both the loss functions and the category distributions between classification and detection tasks  are different, we argue that this will lead to different searching/optimization spaces
Therefore, learning may be biased  towards a local minimum which is not the best for detection  task
(N) Domain mismatch
As is known, fine-tuning can  mitigate the gap due to different target category distribution
However, it is still a severe problem when the source  NNNN    Plain Connection Dense Connection  Scale N  NN×NN  Scale N  NN×NN  Scale N  N0×N0  Scale N  N×N  Scale N  N×N  Scale N  N×N  N×N Conv  N×N Pooling  N×N Conv  N×N Pooling  N×N Conv  N×N Pooling  N×N Conv  N×N Pooling  N×N Conv  N×N Pooling  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  N×N×NNN conv  Stride N  C  C  C  C  C  : down-sampling block  C : concatenation operation  Figure N: DSOD prediction layers with plain and dense structures (N00×N00 input)
Plain structure is introduced by SSD [NN]
See Sec
N for more details
 domain (ImageNet) has a huge mismatch to the target domain such as depth images, medical images, etc [N]
 Our work is motivated by the following two questions
 First, is it possible to train object detection networks from  scratch? Second, if the first answer is positive, are there any  principles to design a resource efficient network structure  for object detection while keeping high detection accuracy?  To meet this goal, we propose deeply supervised objection  detectors (DSOD), a simple yet efficient framework which  could learn object detectors from scratch
DSOD is fairly  flexible, so that we can tailor various network structures for  different computing platforms such as server, desktop, mobile and even embedded devices
 We contribute a set of principles for designing DSOD
 One key point is that deep supervision plays a critical role,  which is motivated by the work of [NN, NN]
In [NN], Xie et  al
proposed a holistically-nested structure for edge detection, which included the side-output layers to each convstage of base network for explicit deep supervision
Instead of using the multiple cut-in loss signals with sideoutput layers, this paper adopts deep supervision implicitly through the dense layer-wise connections proposed in  DenseNet [N0]
Dense structures are not only adopted in the  backbone sub-network, but also in the front-end multi-scale  prediction layers
Figure N illustrates the structure comparison in front-end prediction layers
The fusion and reuse  of multi-resolution prediction-maps help keep or even improve the final accuracy while reducing model parameters  to some extent
 Our main contributions are summarized as follows:  (N) We present DSOD, to the best of our knowledge, the  first framework that can train object detection networks from scratch with state-of-the-art performance,  even with limited training data
 (N) We introduce and validate a set of principles to design efficient object detection networks from scratch  through step-by-step ablation studies
 (N) We show that our DSOD can achieve state-of-the-art  performance on three standard benchmarks (PASCAL  VOC N00N, N0NN and MS COCO datasets) with realtime processing speed and more compact models
 N
Related Work  Object Detection
State-of-the-art CNN based object detection methods can be divided into two groups: (i) region  proposal based methods and (ii) proposal-free methods
 Proposal based methods include R-CNN [N], Fast RCNN [N], Faster R-CNN [NN] and R-FCN [NN]
R-CNN  uses selective search [NN] to first generate potential object regions in an image and then perform classification on  the proposed regions
R-CNN requires high computational  costs since each region is processed by the CNN network  separately
Fast R-CNN and Faster R-CNN improve the efficiency by sharing computation and using neural networks  to generate the region proposals
R-FCN further improves  speed and accuracy by removing fully-connected layers and  adopting position-sensitive score maps for final detection
 Proposal-free methods like YOLO [NN] and SSD [NN]  have recently been proposed for real-time detection
YOLO  uses a single feed-forward convolutional network to directly  predict object classes and locations
Comparing with the  region-based methods, YOLO no longer requires a second  per-region classification operation so that it is extremely  fast
SSD improves YOLO in several aspects, including (N)  using small convolutional filters to predict categories and  NNN0    Layers Output Size (Input N×N00 × N00) DSOD  Stem  Convolution NN×NN0×NN0 N×N conv, stride N  Convolution NN×NN0×NN0 N×N conv, stride N  Convolution NNN×NN0×NN0 N×N conv, stride N  Pooling NNN×NN×NN N×N max pool, stride N  Dense Block  (N) NNN×NN×NN    N × N conv  N × N conv  ]  × N  Transition Layer  (N)  NNN×NN×NN N×N conv  NNN×NN×NN N×N max pool, stride N  Dense Block  (N) N00×NN×NN    N × N conv  N × N conv  ]  × N  Transition Layer  (N)  N00×NN×NN N×N conv  N00×NN×NN N×N max pool, stride N  Dense Block  (N) NNNN×NN×NN    N × N conv  N × N conv  ]  × N  Transition w/o Pooling Layer (N) NNN0×NN×NN N×N conv  Dense Block  (N) NNNN×NN×NN    N × N conv  N × N conv  ]  × N  Transition w/o Pooling Layer (N) NNNN×NN×NN N×N conv  DSOD Prediction Layers – Plain/Dense  Table N: DSOD architecture (growth rate k = NN in each dense block)
 anchor offsets for bounding box locations; (N) using pyramid features for prediction at different scales; and (N) using  default boxes and aspect ratios for adjusting varying object  shapes
Our proposed DSOD is built upon the SSD framework and thus it inherits the speed and accuracy advantages  of SSD, while produces smaller and more flexible models
 Network Architectures for Detection
Significant efforts  have been devoted to the design of network architectures for  image classification
Many different networks are emerged,  such as AlexNet [NN], VGGNet [NN], GoogLeNet [NN],  ResNet [N] and DenseNet [N0]
Meanwhile, several regularization techniques [NN, NN] have also been proposed to further enhance the model capabilities
Most detection methods [N, N, NN, NN] directly utilize pre-trained ImageNet models as the backbone network
 Some other works design specific backbone network  structures for object detection, but still require pre-training  the network on ImageNet classification dataset first
For  instance, YOLO [NN] defines a network with NN convolutional layers followed by N fully connected layers
 YOLON000 [NN] improves YOLO by proposing a new network named Darknet-NN, which is a simplified version of  VGGNet [NN]
Kim et al
[NN] proposes PVANet for object detection, which consists of the simplified “Inception”  block from GoogleNet
Huang et al
[NN] investigated various combination of network structures and detection frameworks, and found that Faster R-CNN [NN] with InceptionResNet-vN [NN] achieved the highest performance
In this  paper, we also consider network structures for generic object detection
However, the pre-training on ImageNet is no  longer required by the proposed DSOD
 Learning Deep Models from Scratch
To the best of our  knowledge, there are no works which train object detection  networks from scratch
The proposed approach has very  appealing advantages over existing solutions
We will elaborate and validate the method in the following sections
In  semantic segmentation, Jégou et al
[NN] demonstrated that  a well-designed network structure can outperform state-ofthe-art solutions without using the pre-trained models
It extends DenseNets to fully convolutional networks by adding  an upsampling path to recover the full input resolution
 N
DSOD  In this section, we first introduce our DSOD architecture  and its components, and elaborate several important design  principles
Then we describe the training settings
 N.N
DSOD Architecture  Overall Framework
The proposed DSOD method is a  multi-scale proposal-free detection framework similar to  SSD [NN]
The network structure of DSOD can be divided  into two parts: the backbone sub-network for feature extraction and the front-end sub-network for prediction over  multi-scale response maps
The backbone sub-network is a  variant of the deeply supervised DenseNets [N0] structure,  which is composed of a stem block, four dense blocks, two  transition layers and two transition w/o pooling layers
The  front-end subnetwork (or named DSOD prediction layers)  fuses multi-scale prediction responses with an elaborated  dense structure
Figure N illustrates the proposed DSOD  prediction layers along with the plain structure of multiscale predicting maps as used in SSD [NN]
The full DSOD  network architectureN is detailed in Table N
We elaborate  each component and the corresponding design principle in  NThe visualization of the complete network structure is available at: http://ethereon.github.io/netscope/#/gist/  bNNd0NfNNNNeNaN0fN0NNbNdNebNe0Nd
 NNNN  http://ethereon.github.io/netscope/#/gist/bNNd0NfNNNNeNaN0fN0NNbNdNebNe0Nd http://ethereon.github.io/netscope/#/gist/bNNd0NfNNNNeNaN0fN0NNbNdNebNe0Nd   the following
 Principle N: Proposal-free
We investigated all the stateof-the-art CNN based object detectors, and found that they  could be divided into three categories
First, R-CNN and  Fast R-CNN require external object proposal generators like  selective search
Second, Faster R-CNN and R-FCN require integrated region-proposal-network (RPN) to generate relatively fewer region proposals
Third, YOLO and  SSD are single-shot and proposal-free methods, which handle object location and bounding box coordinates as a regression problem
We observe that only the proposal-free  method (the Nrd category) can converge successfully without the pre-trained models
We conjecture this is due to the  RoI (Regions of Interest) pooling in the other two categories  of methods — RoI pooling generates features for each region proposals, which hinders the gradients being smoothly  back-propagated from region-level to convolutional feature  maps
The proposal-based methods work well with pretrained network models because the parameter initialization  is good for those layers before RoI pooling, while this is not  true for training from scratch
 Hence, we arrive at the first principle: training detection  network from scratch requires a proposal-free framework
 In practice, we derive a multi-scale proposal-free framework from the SSD framework [NN], as it could reach stateof-the-art accuracy while offering fast processing speed
 Principle N: Deep Supervision
The effectiveness  of deeply supervised learning has been demonstrated in  GoogLeNet [NN], DSN [NN], DeepIDN [N0], etc
The central idea is to provide integrated objective function as direct supervision to the earlier hidden layers, rather than only  at the output layer
These “companion” or “auxiliary” objective functions at multiple hidden layers can mitigate the  “vanishing” gradients problem
The proposal-free detection  framework contains both classification loss and localization  loss
The explicit solution requires adding complex sideoutput layers to introduce “companion” objective at each  hidden layer for the detection task, similar to [NN]
Here we  empower deep supervision with an elegant & implicit solution called dense layer-wise connection, as introduced in  DenseNets [N0]
A block is called dense block when all preceding layers in the block are connected to the current layer
 Hence, earlier layers in DenseNet can receive additional supervision from the objective function through the skip connections
Although only a single loss function is required  on top of the network, all layers including the earlier layers  still can share the supervised signals unencumbered
We  will verify the benefit of deep supervision in Section N.N.N
 Transition w/o Pooling Layer
We introduce this layer in order to increase the number of dense blocks without reducing the final feature map resolution
In the original design  of DenseNet, each transition layer contains a pooling operation to down-sample the feature maps
The number of  dense blocks is fixed (N dense blocks in all DenseNet architectures) if one wants to maintain the same scale of outputs
 The only way to increase network depth is adding layers inside each block for the original DenseNet
The transition  w/o pooling layer eliminates this restriction of the number  of dense blocks in our DSOD architecture, and can also be  used in the standard DenseNet
 Principle N: Stem Block
Motivated by Inception-vN [NN]  and vN [NN], we define stem block as a stack of three N×N  convolution layers followed by a N×N max pooling layer
 The first conv-layer works with stride = N and the other two  are with stride = N
We find that adding this simple stem  structure can evidently improve the detection performance  in our experiments
We conjecture that, compared with the  original design in DenseNet (N×N conv-layer, stride = N followed by a N×N max pooling, stride = N), the stem block can  reduce the information loss from raw input images
We will  show that the reward of this stem block is significant for  detection performance in Section N.N.N
 Principle N: Dense Prediction Structure
Figure N illustrates the comparison of the plain structure (as in SSD) and  our proposed dense structure in the front-end sub-network
 SSD designs prediction-layers as an asymmetric hourglass  structure
For N00×N00 input images, six scales of feature  maps are applied for predicting objects
The Scale-N feature maps are from the middle layer of the backbone subnetwork, which has the largest resolution (NN×NN) in order  to handle the small objects in an image
The remaining five  scales are on top of the backbone sub-network
Then, a  plain transition layer with the bottleneck structure (a N×N  conv-layer for reducing the number of feature maps plus a  N×N conv-layer) [NN, N] is adopted between two contiguous  scales of feature maps
 Learning Half and Reusing Half
In the plain structure as  in SSD (see Figure N), each later scale is directly transited  from the adjacent previous scale
We propose the dense  structure for prediction, which fuses multi-scale information for each scale
For simplicity, we restrict that each  scale outputs the same number of channels for the prediction feature maps
In DSOD, in each scale (except scaleN), half of the feature maps are learned from the previous scale with a series of conv-layers, while the remaining  half feature maps are directly down-sampled from the contiguous high-resolution feature maps
The down-sampling  block consists of a N×N, stride = N max pooling layer followed by a N×N, stride = N conv-layer
The pooling layer  aims to match resolution to current size during concatenation
The N×N conv-layer is used to reduce the number of  channels to N0%
The pooling layer is placed before the  N×N conv-layer for the consideration of reducing computing cost
This down-sampling block actually brings each  scale with the multi-resolution feature maps from all of its  preceding scales, which is essentially identical to the dense  NNNN    DSODN00  transition w/o pooling?  hi-comp factor θ?  wide bottleneck?  wide Nst conv-layer?  big growth rate?  stem block?  dense pred-layers?  VOC N00N mAP NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N: Effectiveness of various designs on VOC N00N test set
Please  refer to Table N and Section N.N for more details
 layer-wise connection introduced in DenseNets
For each  scale, we only learn half of new feature maps and reuse  the remaining half of the previous ones
This dense prediction structure can yield more accurate results with fewer  parameters than the plain structure, as will be studied in  Section N.N
 N.N
Training Settings  We implement our detector based on the Caffe framework [NN]
All our models are trained from scratch with  SGD solver on NVidia TitanX GPU
Since each scale of  DSOD feature maps is concatenated from multiple resolutions, we adopt the LN normalization technique [NN] to scale  the feature norm to N0 on all outputs
Note that SSD only  applies this normalization to scale-N
Most of our training  strategies follow SSD, including data augmentation, scale  and aspect ratios for default boxes and loss function (e.g.,  smooth LN loss for localization purpose and softmax loss  for classification purpose), while we have our own learning  rate scheduling and mini-batch size settings
Details will be  given in the experimental section
 N
Experiments  We conduct experiments on the widely used PASCAL  VOC N00N, N0NN and MS COCO datasets that have N0, N0,  N0 object categories respectively
Object detection performance is measured by mean Average Precision (mAP)
 N.N
Ablation Study on PASCAL VOCN00N  We first investigate each component and design principle  of our DSOD framework
The results are mainly summarized in Table N and Table N
We design several controlled  experiments on PASCAL VOC N00N with our DSODN00  (with N00×N00 inputs) for this ablation study
A consistent  setting is imposed on all the experiments, unless when some  components or structures are examined
In this study, we  train the models with the combined training set from VOC  N00N trainval and N0NN trainval (“0N+NN”), and test  on the VOC N00N testset
 N.N.N Configurations in Dense Blocks  We first investigate the impact of different configurations in  dense blocks of the backbone sub-network
 Compression Factor in Transition Layers
We compare  two compression factor values (θ = 0.N, N) in the transition  layers of DenseNets
Results are shown in Table N (rows  N and N)
Compression factor θ = N means that there is no  feature map reduction in the transition layer, while θ = 0.N  means half of the feature maps are reduced
Results show  that θ = N yields N.N% higher mAP than θ = 0.N
 # Channels in bottleneck layers
As shown in Table N  (rows N and N), we observe that wider bottleneck layers  (with more channels of response maps) improve the performance greatly (N.N% mAP)
 # Channels in the Nst conv-layer We observe that a large  number of channels in the first conv-layers is beneficial,  which brings N.N% mAP improvement (in Table N rows N  and N)
 Growth rate
A large growth rate k is found to be much  better
We observe N.N% mAP improvement in Table N  (rows N and N) when increase k from NN to NN with Nk bottleneck channels
 N.N.N Effectiveness of Design Principles  We now justify the effectiveness of the key design principles  elaborated earlier
 Proposal-free Framework
We tried to train object detectors from scratch using the proposal-based framework such  as Faster R-CNN and R-FCN
However, the training process failed to converge for all the network structures we attempted (VGGNet, ResNet, DenseNet)
We further tried  to train object detectors using the proposal-free framework  SSD
The training converged successfully but gives much  worse results (NN.N% for VGG), compared with the case  fine-tuning from pre-trained model (NN.N%), as shown in  Table N
This experiment validates our design principle to  choose a proposal-free framework
 Deep Supervision
We then tried to train object detectors from scratch with deep supervision
Our DSODN00  achieves NN.N% mAP, which is much better than the  SSDN00S that is trained from scratch using VGGNN (NN.N%)  without deep supervision
It is also much better than the  fine-tuned results by SSDN00 (NN.N%)
This validates the  principle of deep supervision
 Transition w/o Pooling Layer
We compare the case without this designed layer (only N dense blocks) and the case  with the designed layer (N dense blocks in our design)
The  backbone network is DS/NN-NN-NN-0.N
Results are shown  in Table N
The network structure with the Transition w/o  pooling layer brings N.N% performance gain, which validates the effectiveness of this layer
 Stem Block
As can be seen in Table N (rows N and N),  the stem block improves the performance from NN.N% to  NN.N%
This validates our conjecture that using stem block  can protect information loss from the raw input images
 NNNN    Method data pre-train transition w/o pooling stem backbone network prediction Layer # parameters mAP  DSODN00 0N+NN N N N DS/NN-NN-NN-0.N Plain N.NM NN.N  DSODN00 0N+NN N N DS/NN-NN-NN-0.N Plain N.NM NN.N  DSODN00 0N+NN N N DS/NN-NN-NN-N Plain N.NM NN.N  DSODN00 0N+NN N N DS/NN-NN-NN-N Plain N.NM NN.N  DSODN00 0N+NN N N DS/NN-NN-NN-N Plain N.NM NN.N  DSODN00 0N+NN N N DS/NN-NNN-NN-N Plain NN.0M NN.N  DSODN00 0N+NN N DS/NN-NN-NN-N Plain N.NM N0.N  DSODN00 0N+NN N DS/NN-NN-NN-N Plain NN.NM NN.0  DSODN00 0N+NN N DS/NN-NNN-NN-N Plain NN.NM NN.N  DSODN00 0N+NN N DS/NN-NN-NN-N Dense N.NM NN.N  DSODN00 0N+NN N DS/NN-NNN-NN-N Dense NN.NM NN.N  DSODN00 0N+NN+COCO N DS/NN-NNN-NN-N Dense NN.NM NN.N  Table N: Ablation study on PASCAL VOC N00N test set
DS/A-B-k-θ describes our backbone network structure
A denotes the number of channels  in the Nst conv-layer
B denotes the number of channels in each bottleneck layer (N×N convolution)
k is the growth rate in dense blocks
θ denotes the  compression factor in transition layers
See Section N.N for more explanations
 Method data pre-train backbone network prediction layer speed (fps) # parameters input size mAP  Faster RCNN [NN] 0N+NN VGGNet - N NNN.NM ∼ N00× N000 NN.N  Faster RCNN [NN] 0N+NN ResNet-N0N - N.N∗ - ∼ N00× N000 NN.N  R-FCN [NN] 0N+NN ResNet-N0 - NN NN.NM ∼ N00× N000 NN.N  R-FCN [NN] 0N+NN ResNet-N0N - N N0.NM ∼ N00× N000 NN.N  R-FCNmulti-sc [NN] 0N+NN ResNet-N0N - N N0.NM ∼ N00× N000 N0.N  YOLOvN [NN] 0N+NN Darknet-NN - NN - NNN× NNN NN.N  SSDN00 [NN] 0N+NN VGGNet Plain NN NN.NM N00× N00 NN.N  SSDN00* [NN] 0N+NN VGGNet Plain NN NN.NM N00× N00 NN.N  Faster RCNN 0N+NN N VGGNet/ResNet-N0N/DenseNet Failed  R-FCN 0N+NN N VGGNet/ResNet-N0N/DenseNet Failed  SSDN00S† 0N+NN N ResNet-N0N Plain NN.N NN.NM N00× N00 NN.N∗  SSDN00S† 0N+NN N VGGNet Plain NN NN.NM N00× N00 NN.N  SSDN00S† 0N+NN N VGGNet Dense NN NN.0M N00× N00 N0.N  DSODN00 0N+NN N DS/NN-NNN-NN-N Plain N0.N NN.NM N00× N00 NN.N  DSODN00 0N+NN N DS/NN-NNN-NN-N Dense NN.N NN.NM N00× N00 NN.N  DSODN00 0N+NN+COCO N DS/NN-NNN-NN-N Dense NN.N NN.NM N00× N00 NN.N  Table N: PASCAL VOC N00N test detection results
SSDN00* is updated version by the authors after the paper publication
SSDN00S† indicates training  SSDN00* from scratch with ResNet-N0N or VGGNet, which serves as our baseline
Note that the speed of Faster R-CNN with ResNet-N0N (N.N fps) is tested  on KN0, while others are tested on Titan X
The result of SSDN00S with ResNet-N0N (NN.N% mAP, without the pre-trained model) is produced with the  default setting of SSD, which may not be optimal
 Dense Prediction Structure
We analyze the dense prediction structure from three aspects: speed, accuracy and parameters
As shown in Table N, DSOD with dense front-end  structure runs slightly lower than the plain structure (NN.N  fps vs
N0.N fps) on a Titan X GPU, due to the overhead  from additional down-sampling blocks
However, the dense  structure improves mAP from NN.N% to NN.N%, while reduces the parameters from NN.NM to NN.NM
Table N gives  more details (rows N and N0)
We also tried to replace the  prediction layers in SSD with the proposed dense prediction  layers
The accuracy on VOC N00N test set can be improved  from NN.N% (original SSD) to NN.N% (with pre-trained models), and NN.N% to N0.N% (w/o pre-trained models), when  using the VGG-NN models as backbone
This verifies the  effectiveness of the dense prediction layer
 What if pre-training on ImageNet? It is interesting to  see the performance of DSOD with backbone network pretrained on ImageNet
We trained one lite backbone network DS/NN-NN-NN-N on ImageNet, which obtains NN.N%  top-N accuracy and NN.N% top-N accuracy on the validationset (slightly worse than VGG-NN)
After fine-tuning the  whole detection framework on “0N+NN” trainval set, we  achieve N0.N% mAP on the VOC N00N test-set
The corresponding training-from-scratch solution achieves N0.N%  accuracy, which is even slightly better
Future work will  investigate this point more thoroughly
 N.N.N Runtime Analysis  The inference speed is shown in the Nth column of Table N
 With N00×N00 input, our full DSOD can process an image  in NN.Nms (N0.N fps) on a single Titan X GPU with the plain  prediction structure, and NN.Nms (NN.N fps) with the dense  prediction structure
As a comparison, R-FCN runs at N0ms  (NN fps) for ResNet-N0 and NN0ms (N fps) for ResNet-N0N
 The SSDN00∗ runs at NN.Nms (NN.N fps) for ResNet-N0N and  NN.Nms (NN fps) for VGGNet
In addition, our model uses  about only N/N parameters to SSDN00 with VGGNet, N/N to  SSDN00 with ResNet-N0N, N/N to R-FCN with ResNet-N0N  and N/N0 to Faster R-CNN with VGGNet
A lite-version  of DSOD (N0.NM parameters, w/o any speed optimization)  can run NN.N fps with only N% mAP drops
 N.N
Results on PASCAL VOCN00N  Models are trained based on the union of VOC N00N  trainval and VOC N0NN trainval (“0N+NN”) following [NN]
We use a batch size of NNN
Note that this batchsize is beyond the capacity of GPU memories (even for an  N GPU server, each with NNGB memory)
We use a trick  NNNN    Method data backbone network pre-train mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv  ION [N] 0N+NN+S VGGNet NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Faster RCNN [NN] 0N++NN ResNet-N0N NN.N NN.N NN.N NN.N NN.0 NN.0 NN.N NN.N NN.N NN.N N0.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  R-FCNmulti-sc [NN] 0N++NN ResNet-N0N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N N0.N NN.N NN.0 NN.N NN.N NN.0 N0.N NN.N NN.N NN.N  YOLOvN [NN] 0N++NN Darknet-NN NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  SSDN00* [NN] 0N++NN VGGNet NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N  DSODN00 0N++NN DS/NN-NNN-NN-N N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N  DSODN00 0N++NN+COCO DS/NN-NNN-NN-N N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N  Table N: PASCAL VOC N0NN test detection results
0N+NN: 0N trainval + NN trainval, 0N+NN+S: 0N+NN plus segmentation labels, 0N++NN: 0N  trainval + 0N test + NN trainval
Result links are DSODN00 (0N+NN) : http://host.robots.ox.ac.uk:N0N0/anonymous/PIOBKI
 html; DSODN00 (0N+NN+COCO): http://host.robots.ox.ac.uk:N0N0/anonymous/I0UUHO.html
 Method data network pre-train Avg
Precision, IoU: Avg
Precision, Area: Avg
Recall, #Dets: Avg
Recall, Area:  0.N:0.NN 0.N 0.NN S M L N N0 N00 S M L  Faster RCNN [NN] trainval VGGNet NN.N NN.N - - - - - - - - - ION [N] train VGGNet NN.N NN.N NN.N N.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N  R-FCN [NN] trainval ResNet-N0N NN.N NN.N - N0.N NN.N NN.N - - - - - R-FCNmulti-sc [NN] trainval ResNet-N0N NN.N NN.N - N0.N NN.N NN.0 - - - - - SSDN00 (Huang et al.) [NN] < trainvalNNk MobileNet NN.N - - - - - - - - - - SSDN00 (Huang et al.) [NN] < trainvalNNk Inception-vN NN.N - - - - - - - - - - YOLOvN [NN] trainvalNNk Darknet-NN NN.N NN.0 NN.N N.0 NN.N NN.N N0.N NN.N NN.N N.N NN.N NN.N  SSDN00* [NN] trainvalNNk VGGNet NN.N NN.N NN.N N.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N  DSODN00 trainval DS/NN-NNN-NN-N N NN.N NN.N N0.N N.N NN.N NN.0 NN.N N0.N NN.0 NN.N NN.N NN.0  Table N: MS COCO test-dev N0NN detection results
 to overcome the GPU memory constraints by accumulating gradients over two training iterations, which has been  implemented on Caffe platform [NN]
The initial learning  rate is set to 0.N, and then divided by N0 after every N0k  iterations
The training finished when reaching N00k iterations
Following [NN], we use a weight decay of 0.000N and  a momentum of 0.N
All conv-layers are initialized with the  “xavier” method [N]
 Table N shows our results on VOCN00N test set
 SSDN00∗ is the updated SSD results which use the new data  augmentation technique
Our DSODN00 with plain connection achieves NN.N%, which is slightly better than SSDN00∗  (NN.N%)
DSODN00 with dense prediction structure improves the result to NN.N%
After adding COCO as training  data, the performance is further improved to NN.N%
 N.N
Results on PASCAL VOCN0NN  For the VOC N0NN dataset, we use VOC N0NN  trainval and VOC N00N trainval + test for training, and test on VOC N0NN test set
The initial learning  rate is set to 0.N for the first N0k iterations, then divided  by N0 after every N0k iterations
The total training iterations are NN0k
Other settings are the same as those used in  our VOC N00N experiments
Our results of DSODN00 are  shown in Table N
DSODN00 achieves NN.N% mAP, which  is consistently better than SSDN00∗ (NN.N%)
 N.N
Results on MS COCO  Finally we evaluate our DSOD on the MS COCO  dataset [N0]
MS COCO contains N0k images for training,  N0k for validation and N0k for testing (test-dev set)
Following [NN, NN], we use the trainval set (train set + validation set) for training
The batch size is also set as NNN
 The initial learning rate is set to 0.N for the first N0k iterations, then divided by N0 after every N0k iterations
The  total number of training iterations is NN0k
 Results are summarized in Table N
Our DSODN00  achieves NN.N%/NN.N% on the test-dev set, which outperforms the baseline SSDN00∗ with a large margin
Our result is comparable to the single-scale R-FCN, and is close to  the R-FCNmulti-sc which uses ResNet-N0N as the pre-trained  model
Interestingly, we observe that our result with 0.N  IoU is lower than R-FCN, but our [0.N:0.NN] result is better or comparable
This indicates that our predicted locations are more accurate than R-FCN under the larger overlap settings
It is reasonable that our small object detection precision is slightly lower than R-FCN since our input  image size (N00×N00) is much smaller than R-FCN’s (∼  N00×N000)
Even with this disadvantage, our large object  detection precision is still much better than R-FCN
This  further demonstrates the effectiveness of our approach
Figure N shows some qualitative detection examples on COCO  with our DSODN00 model
 N
Discussion  Better Model Structure vs
More Training Data
An  emerging idea in the computer vision community is that object detection or other vision tasks might be solved with  deeper and larger neural networks backed with massive  training data like ImageNet [N]
Thus more and more largescale datasets have been collected and released recently,  such as the Open Images dataset [NN], which is N.Nx larger in  the number of images and Nx larger of categories than that  of ImageNet
We definitely agree that, under modest assumptions that given boundless training data and unlimited  computational power, deep neural networks should perform  extremely well
However, our proposed approach and experimental results imply an alternative view to handle this  problem: a better model structure might enable similar or  better performance compared with complex models trained  NNNN  http://host.robots.ox.ac.uk:N0N0/anonymous/PIOBKI.html http://host.robots.ox.ac.uk:N0N0/anonymous/PIOBKI.html http://host.robots.ox.ac.uk:N0N0/anonymous/I0UUHO.html   Figure N: Examples of object detection results on the MS COCO test-dev set using DSODN00
The training data is COCO trainval without the ImageNet  pre-trained models (NN.N% mAP@[0.N:0.NN] on the test-dev set)
Each output box is associated with a category label and a softmax score in [0, N]
A score  threshold of 0.N is used for displaying
For each image, one color corresponds to one object category in that image
The running time per image is NN.Nms  on one Titan X GPU or NN0ms on Intel (R) Core (TM) iN-NNN0X CPU @ N.00GHz
 from large data
Particularly, our DSOD is only trained with  NN,NNN images on VOC N00N, but achieves competitive or  even better performance than those models trained with N.N  million + NN,NNN images
 In this premise, it is worthwhile rehashing the intuition  that as datasets grow larger, training deep neural networks  becomes more and more expensive
Thus a simple yet efficient approach becomes increasingly important
Despite its  conceptual simplicity, our approach shows great potential  under this setting
 Why Training from Scratch? There have been many  successful cases where model fine-tuning works greatly
 One may ask why should we train object detectors from  scratch
We argue that, as aforementioned briefly, training from scratch is of critical importance at least for two  cases
First, there may be big domain differences from pretrained model domain to the target one
For instance, most  pre-trained models are trained on large scale RGB image  dataset, ImageNet
It is very difficult to transfer ImageNet  model to the domains of depth images, multi-spectrum images, medical images, etc
Some advanced domain adaptation techniques have been proposed
But what an amazing  thing if we have a technique which can train object detector  from scratch
Second, model fine-tuning restricts the structure design space for object detection networks
This is very  critical for the deployment of deep neural networks models  to resource-limited Internet-of-Things (IoT) scenario
 Model Compactness vs
Performance
It has often been  reported that there is a trade-off between model compactness (in terms of the number of parameters) and performance
Most CNN-based detection solutions require a huge  memory space to store the massive parameters
Therefore  the models are usually unsuitable for low-end devices like  mobile-phones and embedded electronics
Thanks to the  parameter-efficient dense block, our model is much smaller  than most competitive methods
For instance, our smallest  dense model (DS/NN-NN-NN-N, with dense prediction layers)  achieves NN.N% mAP with only N.NM parameters, which  shows great potential for applications on low-end devices
 N
Conclusion  We have presented Deeply Supervised Object Detector  (DSOD), a simple yet efficient framework for training object detector from scratch
Without using pre-trained models on ImageNet, DSOD demonstrates competitive accuracy  to state-of-the-art detectors such as SSD, Faster R-CNN  and R-FCN on the popular PASCAL VOC N00N, N0NN and  MS COCO datasets, with only N/N, N/N and N/N0 parameters compared to SSD, R-FCN and Faster R-CNN, respectively
DSOD has great potential on domain different scenario like depth, medical, multi-spectral images, etc
Our  future work will consider these domains, as well as learning  ultra efficient DSOD models to support resource-bounded  devices
The code and models of this paper are available at:  https://github.com/szq0NNN/DSOD
 Acknowledgements  Yu-Gang Jiang and Xiangyang Xue are supported in  part by a NSFC project (#NNNNNN0N), a project from  STCSM (#NNJCNNN0N0N), and an European FPN project  (PIRSESGA-N0NN-NNNNNN)
 NNNN  https://github.com/szq0NNN/DSOD   References  [N] S
Bell, C
Lawrence Zitnick, et al
Inside-outside net: Detecting objects in context with skip pooling and recurrent  neural networks
In CVPR, N0NN
N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, et al
Semantic  image segmentation with deep convolutional nets and fully  connected crfs
In ICLR, N0NN
N  [N] J
Deng, W
Dong, R
Socher, L.-J
Li, et al
Imagenet: A  large-scale hierarchical image database
In CVPR, N00N
N,  N  [N] R
Girshick
Fast r-cnn
In ICCV, N0NN
N, N, N  [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
N, N, N  [N] X
Glorot and Y
Bengio
Understanding the difficulty of  training deep feedforward neural networks
In AISTATS,  N0N0
N  [N] S
Gupta, J
Hoffman, and J
Malik
Cross modal distillation  for supervision transfer
In CVPR, N0NN
N  [N] B
Hariharan, P
Arbeláez, R
Girshick, and J
Malik
Hypercolumns for object segmentation and fine-grained localization
In CVPR, N0NN
N  [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N, N, N  [N0] G
Huang, Z
Liu, K
Q
Weinberger, and L
van der Maaten
 Densely connected convolutional networks
In CVPR, N0NN
 N, N, N, N  [NN] J
Huang, V
Rathod, C
Sun, et al
Speed/accuracy trade-offs  for modern convolutional object detectors
In CVPR, N0NN
 N, N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] S
Jégou, M
Drozdzal, D
Vazquez, et al
The one hundred  layers tiramisu: Fully convolutional densenets for semantic  segmentation
arXiv preprint arXiv:NNNN.0NNNN, N0NN
N  [NN] Y
Jia, E
Shelhamer, J
Donahue, et al
Caffe: Convolutional  architecture for fast feature embedding
In ACM MM, N0NN
 N, N  [NN] K.-H
Kim, S
Hong, B
Roh, et al
Pvanet: Deep but  lightweight neural networks for real-time object detection
 arXiv preprint arXiv:NN0N.0N0NN, N0NN
N  [NN] I
Krasin, T
Duerig, N
Alldrin, A
Veit, et al
Openimages:  A public dataset for large-scale multi-label and multi-class  image classification
https://github.com/openimages, N0NN
 N  [NN] A
Krizhevsky, I
Sutskever, and G
Hinton
Imagenet classification with deep convolutional neural networks
In NIPS,  N0NN
N, N  [NN] C.-Y
Lee, S
Xie, P
W
Gallagher, et al
Deeply-supervised  nets
In AISTATS, N0NN
N, N  [NN] Y
Li, K
He, J
Sun, et al
R-fcn: Object detection via regionbased fully convolutional networks
In NIPS, N0NN
N, N, N,  N  [N0] T.-Y
Lin, M
Maire, S
Belongie, et al
Microsoft coco:  Common objects in context
In ECCV, N0NN
N  [NN] W
Liu, D
Anguelov, D
Erhan, et al
Ssd: Single shot multibox detector
In ECCV, N0NN
N, N, N, N, N, N  [NN] W
Liu, A
Rabinovich, and A
C
Berg
Parsenet: Looking  wider to see better
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 N  [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
N  [NN] M
Oquab, L
Bottou, I
Laptev, and J
Sivic
Learning and  transferring mid-level image representations using convolutional neural networks
In CVPR, N0NN
N  [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In  CVPR, N0NN
N, N, N  [NN] J
Redmon and A
Farhadi
YoloN000: Better, faster, stronger
 In CVPR, N0NN
N, N, N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
N, N, N, N, N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 N, N  [NN] N
Srivastava, G
E
Hinton, A
Krizhevsky, et al
Dropout:  a simple way to prevent neural networks from overfitting
 JMLR, N0NN
N  [N0] Y
Sun, D
Liang, X
Wang, and X
Tang
DeepidN: Face  recognition with very deep neural networks
arXiv preprint  arXiv:NN0N.00NNN, N0NN
N  [NN] C
Szegedy, S
Ioffe, V
Vanhoucke, and A
Alemi
InceptionvN, inception-resnet and the impact of residual connections  on learning
In ICLR workshop, N0NN
N, N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, et al
Going deeper  with convolutions
In CVPR, N0NN
N, N, N  [NN] C
Szegedy, V
Vanhoucke, S
Ioffe, et al
Rethinking the  inception architecture for computer vision
In CVPR, N0NN
 N  [NN] J
R
Uijlings, K
E
Van De Sande, T
Gevers, et al
Selective  search for object recognition
IJCV, N0NN
N  [NN] S
Xie and Z
Tu
Holistically-nested edge detection
In  ICCV, N0NN
N, N  [NN] F
Yu and V
Koltun
Multi-scale context aggregation by dilated convolutions
In ICLR, N0NN
N  NNNNClass Rectification Hard Mining for Imbalanced Deep Learning   Class Rectification Hard Mining for Imbalanced Deep Learning  Qi Dong  Queen Mary University of London  q.dong@qmul.ac.uk  Shaogang Gong  Queen Mary University of London  s.gong@qmul.ac.uk  Xiatian Zhu  Vision Semantics Ltd
 eddy@visionsemantics.com  Abstract  Recognising detailed facial or clothing attributes in images of people is a challenging task for computer vision, especially when the training data are both in very large scale  and extremely imbalanced among different attribute classes
 To address this problem, we formulate a novel scheme for  batch incremental hard sample mining of minority attribute  classes from imbalanced large scale training data
We develop an end-to-end deep learning framework capable of  avoiding the dominant effect of majority classes by discovering sparsely sampled boundaries of minority classes
This  is made possible by introducing a Class Rectification Loss  (CRL) regularising algorithm
We demonstrate the advantages and scalability of CRL over existing state-of-the-art  attribute recognition and imbalanced data learning models on two large scale imbalanced benchmark datasets, the  CelebA facial attribute dataset and the X-Domain clothing  attribute dataset
 N
Introduction  Automatic recognition of person attributes in images,  e.g
clothing category and facial characteristics, is very useful [NN, NN], but also challenging due to: (N) Very large scale  training data with significantly imbalanced distributions on  annotated attribute data [N, N, NN], with clothing and face attributes typically exhibiting a power-law distribution (Figure N)
This makes model learning biased towards welllabelled attribute classes (the majority classes) resulting in  poor performance against sparsely-labelled classes (the minority classes) [N0], known as the imbalanced class learning problem [N0]
(N) Subtle discrepancy between different fine-grained attributes, e.g
“Woollen-Coat” can appear  very similar to “Cotton-Coat”, whilst “Mustache” may be  visually indistinct (Figure N)
To recognise such subtle attribute differences, model training assumes a large collection of balanced training image data [N, N0]
 There have been studies on how to solve the general  imbalanced data learning problem including re-sampling  [N, NN, NN] and cost-sensitive weighting [NN, NN]
However,  0 N0 N0 N0 N0 0  0.N  N  N.N  N ×N0  N  N.Nx  0 N0 N0 N0 N0 N00 NN0 NN0 NN0 NN0  0  0.N  N  N.N  N  N.N ×N0  N  X-domain  0  0.N  N.0  N.0  N.N  N.NxN0#  0 N0 N0 N0 N0 N00 NN0 NN0 NN0 NN0  Attribute index  Woollen coat Cotton coat  Number of images  N.0  N.N  N.0  0  N0 $  0 N0 N0 N0 N0  CelebANumber of images  Goatee Mustache  Attribute index  (a)  (b)  Figure N
Imbalanced training data distribution: (a) clothing attributes (X-Domain [N]), (b) facial attributes (CelebA [NN])
 these methods can suffer from either over-sampling which  leads to model overfitting and/or introducing noise, or  down-sampling which loses valuable data
These classical  imbalanced learning models rely typically on hand-crafted  features, without deep learning’s capacity for exploiting a  very large pool of imagery data from diverse sources to  learn more expressive representations [NN, NN, NN, N]
However, deep learning is likely to suffer even more from imbalanced data distribution [NN, NN, NN, NN] and deep learning of  imbalanced data is currently under-studied
This is partly  due to that popular image datasets for deep learning, e.g
 ILSVRC, do not exhibit significant class imbalance due to  careful data filtering and selection during the construction  process (Table N)
The problem becomes very challenging for deep learning of clothing or facial attributes (Figure N)
In particular, when a large scale training data are  drawn from online Internet sources [N, NN, NN, NN], image  attribute distributions are likely to be extremely imbalanced  (see Table N)
For example, the data sampling size ratio between the minority and majority classes (imbalance ratio) in  the X-Domain clothing attribute dataset [N] is N:N,NNN, with  the smallest minority and largest majority class having NN  and NN, NNN images respectively
 NNNNN    Table N
Comparing large scale datasets in terms of training data imbalance
Metric: the size ratio of smallest and largest classes
These numbers are based on the standard train data split if available, otherwise on the whole dataset
For COCO [NN], no specific numbers are available for calculating between-class  imbalance ratios, mainly because the COCO images often contain simultaneously multiple classes of objects and also multiple instances of a specific class
 Datasets ILSVRCN0NN-NN [NN] COCO [NN] VOCN0NN [NN] CIFAR-N00 [NN] Caltech NNN [NN] CelebA [NN] DeepFashion [NN] X-Domain [N]  Imbalance ratio N : N - N : NN N : N N : N N : NN N : NNN N : NNNN  This work addresses the problem of deep learning on  large scale imbalanced person attribute data for multi-label  attribute recognition
Other deep models for imbalanced  data learning exist [NN, NN, NN, NN]
These models shall be  considered as end-to-end deep feature learning and classifier learning
For over-sampling and down-sampling, a special training data re-sampling pre-process may be needed  prior to deep model learning
They are ineffective for deep  learning of imbalanced data (see evaluations in Sec
N)
 More recently, a Large Margin Local Embedding (LMLE)  method [NN] was proposed to enforce the local cluster structure of per class distribution in the deep learning process so  that minority classes can better maintain their own structures in the feature space
The LMLE has a number of fundamental drawbacks including disjoint feature and classification optimisation, offline clustering of training data a priori to model learning, and quintuplet construction updates
 This work presents a novel end-to-end deep learning approach to modelling multi-label person attributes, clothing  or facial, given a large scale webly-collected image data  pool with significantly imbalanced attribute data distributions
The contributions of this work are: (N) We propose a novel model for deep learning of very large scale  imbalanced data based on batch-wise incremental hard mining of hard-positives and hard-negatives from minority attribute classes alone
This is in contrast to existing attribute  recognition methods [N, NN, NN, N0, N0] which either assume balanced training data or simply ignore the problem
 Our model performs an end-to-end feature representation  and multi-label attribute classification joint learning
(N)  We formulate a Class Rectification Loss (CRL) regularising  algorithm
This is designed to explore the per batch sampled hard-positives and hard-negatives for improving minority class learning with batch-balance updated deep features
Crucially, this loss rectification is correlated explicitly with batch-wise (small data pool) iterative model optimisation therefore achieving incremental imbalanced data  learning for all attribute classes
This is in contrast to  LMLE’s global clustering of the entire training data (large  data pool) and ad-hoc estimation of cluster size
Moreover,  given our batch-balancing hard-mining approach, the proposed CRL is independent to the overall training data size,  therefore very scalable to large scale training data
Our extensive experiments on two large scale datasets CelebA [NN]  and X-Domain [N] against NN different models including N  state-of-the-art deep attribute models demonstrate the advantages of the proposed method
 Related Work
Imbalanced Data Learning
There  are two classic approaches to learning from imbalanced  data, (N) Class re-sampling: Either down-sampling the majority class or over-sampling the minority class or both  [N, NN, NN, N0, NN, NN]
However, over-sampling can easily introduce undesirable noise and also risk from overfitting
Down-sampling is thus often preferred, but this may  suffer from losing valuable information [NN]
(N) Costsensitive learning: Assigning higher misclassification costs  to the minority classes as compared to the majority classes  [NN, NN, N, NN, NN], or regularising the cross-entropy loss to  cope with the imbalanced positive and negative class distribution [N0]
For this kind of data biased learning, most  commonly adopted in deep models is positive data augmentation, e.g
to learn a deep representation embedding the  local feature structures of minority labels [NN]
Hard Mining
Negative mining has been used for pedestrian detection  [NN], face recognition [NN], image categorisation [NN, NN, N],  unsupervised visual representation learning [NN]
Instead  of general negative mining, the rational for mining hard  negatives (unexpected) is that they are more informative  than easy negatives (expected)
Hard negative mining enables the model to improve itself quicker and more effectively with less data
Similarly, model learning can also  benefit from mining hard positives (unexpected)
In our  model learning we only consider hard mining on the minority classes for efficiency therefore our batch-balancing hard  mining strategy differs significantly from that of LMLE [NN]  in that: (N) The LMLE requires to exhaustively search the  entire training set and thus less scalable to large sized data  due to computational cost; (N) Hard mining in LMLE is  on all classes, both the minority and the majority classes,  therefore not strictly focused on imbalanced learning of  the minority classes thus more expensive whilst less effective
Deep Learning of Person Attributes
Personal clothing  and/or facial attributes are key to person description
Deep  learning have been exploited for clothing [N, NN, NN, N0, NN]  and facial attribute recognition [NN, N0] due to the availability of large scale datasets and deep models’ capacity  for learning from large sized data
However, these methods mostly ignore the significantly imbalanced class data  distributions, resulting in suboptimal model learning for the  minority classes
One exception is the LMLE model [NN]  which explicitly considers the imbalanced attribute class  learning challenge
In contrast to our end-to-end deep learning model in this work, LMLE is not end-to-end learning  and suffers from poor scalability and suboptimal optimisation
This is due to LMLE’s need for very expensive quintuplet construction and pre-clustering (suboptimal) on the  entire training data, resulting in separated feature and classifier learning
 NNNN    N
Class Rectification Deep Learning  A Batch  CNN  Minority profiling  Majority  class  Minority  Class  Minority Class  Hard mining  Class Rectification Loss  Imbalanced data  Figure N
Overview of our Class Rectification Loss (CRL) regularising approach for deep end-to-end imbalanced data learning
 We wish to construct a deep model capable of recognising multi-labelled person attributes {zj} nattr j=N in images,  with a total of nattr different attribute categories, each category zj having its respective value range Zj , e.g
multivalued (N-in-N) clothing category or binary-valued (N-inN) facial attribute
Suppose that we have a collection of n  training images {Ii} n i=N along with their attribute annotation vectors {ai} n i=N, and ai = [ai,N, 


, ai,j , 


, ai,nattr ]  where ai,j refers to the j-th attribute value of the image  Ii
The number of image samples available for different  attribute classes varies greatly (Figure N) therefore poses a  significant imbalanced data distribution challenge to model  learning
Most attributes are localised to image regions,  even though the location information is not provided in the  annotation (weakly labelled)
Intrinsically, this is a multilabel recognition problem since the nattr attributes may coexist in every person image
To that end, we consider to  jointly learn end-to-end features and all the attribute classifiers given imbalanced image data
Our method can be readily incorporated with the classification loss function (e.g
 Cross-entropy loss) of standard CNNs without the need for  a new optimisation algorithm (Fig
N)
 Cross-entropy Classification Loss
For multi-class classification CNN model training (CNN model details in “Network Architecture”, Sec
N.N and N.N), one typically considers the Cross-entropy loss function by firstly predicting  the j-th attribute posterior probability of image Ii over the  ground truth ai,j :  p(yi,j = ai,j |xi,j) = exp(W⊤j xi,j)  ∑|Zj | k=N exp(W  ⊤ k xi,j)  (N)  where xi,j refers to the feature vector of Ii for j-th attribute,  and Wk is the corresponding prediction function parameter
 We then compute the overall loss on a batch of nbs images as  the average additive summation of attribute-level loss with  equal weight:  lce = − N  nbs  nbs ∑  i=N  nattr ∑  j=N  log (  p(yi,j = ai,j |xi,j) )  (N)  However, given highly imbalanced image samples on different attribute classes, model learning by the conventional  classification loss is suboptimal
To address this problem,  we reformulate the model learning objective loss function  by mining explicitly in each batch of training data both hard  positive and hard negative samples for every minority attribute class
Our objective is to rectify incrementally per  batch the class bias in model learning so that the features  are less biased towards the over-sampled majority classes  and more sensitive to the class boundaries of under-sampled  minority classes
 N.N
Minority Class Hard Mining  We wish to impose minority-class hard-samples as constraints on the model learning objective
Different from the  approach adopted by the LMLE model [NN] which aims to  preserve the local structures of both majority and minority  classes by global sampling of the entire training dataset, we  explore batch-based hard-positive and hard-negative mining for the minority classes only
We do not assume the  local structures of minority classes can be estimated from  global clustering before model learning
To that end, we  consider the following steps for handling data imbalance
 Batch Profiling of Minority and Majority Classes
In  each training batch, we profile to discover the minority and majority classes
Given a batch of nbs training  samples, we profile the attribute class distribution hj = [hjN, 


, h  j k, 


h  j  |Zi|] over Zj for each attribute j, where  h j k denotes the number of training samples with the j-th  attribute class value assigned to k
Then, we sort h j k in  the descent order
As such, we define minority classes in  this batch as those classes Cimin with the smallest number of  training samples, with the condition that  ∑  k∈Cj min  h j k < 0.Nnbs
(N)  That is, all minority classes only contribute to less than  half of the total data samples in this batch
The remaining  classes are deemed as the majority classes
 We then exploit a minority class hard mining scheme to  facilitate additional loss constraints in model learningN
To  that end, we consider two approaches: (I) Minority classlevel hard mining (Fig
N(left)), (II) minority instance-level  hard mining (Fig
N(right))
 (I) Minority Class-Level Hard Samples
At the class  level, for a specific minority class c of attribute j, we refer  “hard-positives” to those images xi,j from class c (ai,j = c with ai,j denoting the attribute j ground truth label of xi,j)  given low discriminative scores p(yi,j = c|xi,j) on class c  N We consider only those minority classes having at least two sample images in each batch, ignoring those minority classes having only one  sample image or none
This enables triplet loss based learning
 NNNN    pos  Class Level  Sample indexes  P ro b ab il it y  neg Anchor  Instance Level  Misclassified  Probability space Feature space  Figure N
Illustration of the proposed minority class hard mining
 by the model, i.e
poor recognitions
Conversely, by “hardnegatives”, we refer to those images xi,j from other classes  (ai,j N= c) given high discriminative scores on class c by the model, i.e
obvious mistakes
Formally, we define them as:  Pclsc,j = {xi,j |ai,j = c, low p(yi,j = c|xi,j)} (N)  N clsc,j = {xi,j |ai,j N= c, high p(yi,j = c|xi,j)} (N)  where Pclsc,j and N cls c,j denote the hard positive and negative  sample sets of a minority class c of attribute j
 (II) Minority Instance-Level Hard Samples
At the instance level, we consider hard positives and negatives for  each specific sample instance xi,j from a minority class c  of attribute j, i.e
with ai,j = c
We define “hard-positives” of xi,j as those class c images xk,j (ak,j = c) misclassified (âk,j N= c with âk,j denoting the attribute j predicted la- bel of xk,j) by the current model with large distances (low  matching scores) from xi,j in the feature space
“Hardnegatives” are those images xk,j not from class c (ak,j N= c) with small distances (high matching scores) to xi,j in the  feature space
We define them as:  P insi,c,j = {xk,j |ak,j = c, âk,j N= c, large dist(xi,j ,xk,j)} (N)  N insi,c,j = {xk,j |ak,j N= c, small dist(xi,j ,xk,j)} (N)  where P insi,c,j and N ins i,c,j are the hard positive and negative  sample sets of a minority class c instance xi,j in attribute j,  and dist(·) is the LN distance metric
Hard Mining
Intuitively, mining hard-positives enables  the model to discover and expand sparsely sampled minority class boundaries, whilst mining hard-negatives aims to  improve the margins of minority class boundaries corrupted  by visually very similar imposter classes, e.g
significantly  overlapped outliers
To facilitate and simplify model training, we adopt the following mining strategy
At training  time, for a minority class c of attribute j (or a minority  class instance xi,j) in each training batch data, we select  K hard-positives as the bottom-K scored on c (or bottomK (largest) distances to xi,j), and K hard-negatives as the  top-K scored on c (or top-K (smallest) distance to xi,j),  given the current feature space and classification model
 This hard mining strategy allows our model optimisation to  concentrate particularly on either poor recognitions or obvious mistakes
This not only reduces the model optimisation complexity by soliciting fewer learning constraints,  but also minimises computing cost
It may seem that some  discriminative information is lost by doing so
However, it  should be noted that we perform hard-mining independently  in each batch and incrementally over successive batches
 Therefore, such seemingly-ignored information are considered over the learning iterations
Importantly, this proposed batch-wise hard-mining avoids the global sampling  on the entire training data as required by LMLE [NN] which  can suffer from both negative model learning due to inconsistency between up-to-date deep features and out-of-date  cluster boundary structures, and high computational cost in  quintuplet updating
In contrast, our model can be learned  directly by conventional batch-based classification optimisation algorithms using stochastic gradient descent, with no  need for complex modification required by the quintuplet  based loss in the LMLE model [NN]
 N.N
Class Rectification Loss  In deep feature representation model learning, the key  is to discover latent boundaries for individual classes and  surrounding margins between different classes in the feature  space
To this end, we introduce a Class Rectification Loss  (CRL) regularisation lcrl to rectify the learning bias from  the conventional Cross-entropy classification loss function  (Eqn
(N)) given class-imbalanced attribute data:  lbln = lcrl + lce (N)  where lcrl is computed from the hard positive and negative  samples of the minority classes
We further explore three  different options to formulate lcrl
 (I) Class Rectification by Relative Comparison
Firstly,  we exploit the general learning-to-rank idea [N0], and in  particular the triplet based loss
Considering the small  number of training samples in minority classes, it is sensible to make full use of them in order to effectively handle the underlying model learning bias
Therefore, we regard each image of these minority classes as an “anchor” to  quantitatively compute the batch balancing loss regularisation
Specifically, for each anchor (xa,j), we first construct  a set of triplets based on the mined top-K hard-positives  and hard-negatives associated with the corresponding attribute class c of attribute j, i.e
class-level hard miming, or the sample instance itself xa,j , i.e
instance-level  hard mining
In this way, we form at most KN triplets  T = {(xa,j ,xp,j ,xn,j)k} KN  k=N w.r.t
xa,j , and a total of at  NNNN    most |Xmin| × nattr ×K N triplets T for all the anchors Ximin  across all the minority classes
We formulate the following triplet ranking loss function to impose a class balancing  constraint in model learning:  lcrl =  ∑  T max (0, mj + dist(xa,j ,xp,j)− dist(xa,j ,xn,j))  |T | (N)  where mj denotes the class margin of attribute j in feature  space, dist(·) is the LN distance
We set the class margin for each attribute i as  mj = Nπ  |Zj | (N0)  with |Zj | the number of all possible values for attribute j
(II) Class Rectification by Absolute Comparison
Secondly, we consider to enforce absolute distance constraints  on positive and negative pairs of the minority classes, inspired by the contrastive loss [N]
Specifically, for each instance xi,j in a minority class c of attribute j, we use the  mined hard sets to build positive P+ = {xi,j ,xp,j} and negative P− = {xi,j ,xn,j} pairs in each training batch
Intuitively, we require the positive pairs to be at close distances whist the negative pairs to be far away
Thus, we  define the CRL regularisation as  lcrl = 0.N ∗ (  ∑  P+ dist(xi,j ,xp,j) N  |P+| +  ∑  P− max (  mapc − dist(xi,j ,xn,j), 0 )N  |P−|  )  (NN)  where mapc controls the between-class margin (mapc = N in our experiments)
This constraint aims to optimise the  boundary of the minority classes by incremental separation  from the overlapping (confusing) majority class instances  by per batch iterative optimisation
 (III) Class Rectification by Distribution Comparison
 Thirdly, we formulate class rectification on the minority  class instances by modelling the distribution of positive  and negative pairs constructed as in the case of “Absolute  Comparison” described above
In the spirit of [NN], we  represent the distribution of positive P+ and negative P−  pair sets with histograms H+ = [h+N , · · · , h + τ ] and H  − = [h−N , · · · , h  − τ ] of τ uniformly spaced bins [bN, · · · , bτ ]
We  compute the positive histogram H+ as  h+t = N  |P+|  ∑  (i,j)∈P+ ςi,j,t (NN)  where  ςi,j,t =            dist(xi,j ,xp,j)−bt−N ∆  , if dist(xi,j ,xp,j) ∈ [bt−N, bt] bt+N−dist(xi,j ,xp,j)  ∆ , if dist(xi,j ,xp,j) ∈ [bt, bt+N]  0
otherwise (NN)  and ∆ defines the step between two adjacent bins
Simi- larly, the negative histogram H− can also be computed
To enable the minority classes distinguishable from the overwhelming majority classes, we enforce the two histogram  distributions as disjoint as possible
We then define the CRL  regularisation loss by how much overlapping between these  two histogram distributions:  lcrl =  τ ∑  t=N  (  h+t  t ∑  k=N  h−k )  (NN)  Statistically, this CRL histogram loss measures the probability that the distance of a random negative pair is smaller  than that of a random positive pair
This distribution based  CRL aims to optimise a model towards mining the minority class boundary areas in a non-deterministic manner
In  our evaluation (Sec
N.N), we compared the effect of these  three different CRL considerations
By default, we deploy  the Relative Comparison formulation in our experiments
 Remarks
Due to the batch-wise design, the balancing  effect by our proposed regularisor is propagated through  the whole training time in an incremental manner
The  CRL approach shares a similar principle to Batch Normalisation [NN] for easing network optimisation
In hard  mining, we do not consider anchor points from the majority classes as in the case of LMLE [NN]
Instead, our  method employs a classification loss to learn features for  discriminating the majority classes based on that the majority classes are well-sampled for learning class discrimination
Focusing the CRL only on the minority classes  makes our model computationally more efficient
Moreover, the computational complexity for constructing quintuplets for LMLE and updating class clustering globally is  nattr × (k×O(n)× N Ω(  √ n)) +O(nN) where Ω is the lower  bound complexity and O the upper bound complexity, that  is, super-polynomially proportionate to the overall training  data size n, e.g
over NN0, 000 in our attribute recogni- tion problem
In contrast, CRL loss is linear to the batch  size, typically in N0N, independent to the overall training size (also see “Model Training Time” in the experiments)
 N
Experiments  Datasets & Performance Metric
As shown in Table N,  both CelebA and X-Domain datasets are highly imbalanced
 For that reason, we selected these two datasets for our evaluations
The CelebA [NN] facial attribute dataset has N0N,NNN  web images from N0,NNN person identities with per person  on average N0 images
Each face image is annotated by  N0 binary attributes
The X-Domain [N] clothing attribute  datasetN consists of NNN,NNN shop images from online reNWe did not select the DeepFashion [NN] dataset for our evaluation because this dataset is relatively well balanced compared to X-Domain (Table N), due to the strict data cleaning process applied
 NNNN    Table N
Facial attributes recognition on the CelebA dataset [NN]
*: Imbalanced data learning models
Metric: Class-balanced accuracy,  i.e
mean sensitivity (%)
CRL(C/I): CRL with Class/Instance level hard mining
The Nst/Nnd best results are highlighted in red/blue
 Methods  Attributes  A tt  ra ct  iv e  M o u th  O p en  S m  il in  g  W ea  r L  ip st  ic k  H ig  h C  h ee  k b o n es  M al  e  H ea  v y  M ak  eu p  W av  y H  ai r  O v al  F ac  e  P o in  ty N  o se  A rc  h ed  E y eb  ro w  s  B la  ck H  ai r  B ig  L ip  s  B ig  N o se  Y o u n g  S tr  ai g h t  H ai  r  B ro  w n  H ai  r  B ag  s U  n d er  E y es  W ea  r E  ar ri  n g s  N o  B ea  rd  Imbalance ratio (N:x) N N N N N N N N N N N N N N N N N N N N  Triplet-kNN [NN] NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN  PANDA [N0] NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN  ANet [NN] NN NN NN NN NN NN NN NN NN NN NN N0 NN NN NN NN NN N0 NN NN  DeepIDN [NN] NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN  Over-Sampling* [NN] NN NN N0 NN NN NN NN N0 NN NN NN NN NN NN NN NN NN NN NN NN  Down-Sampling* [NN] NN NN N0 NN N0 N0 NN N0 NN NN N0 N0 NN NN N0 NN NN NN N0 NN  Cost-Sensitive* [N0] NN NN N0 NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN  LMLE* [NN] NN NN NN NN NN NN NN NN NN NN NN NN N0 N0 NN NN NN NN NN NN  CRL(C)* N0 NN N0 NN NN NN NN NN NN NN N0 NN NN NN NN NN NN NN NN NN  CRL(I)* NN NN NN NN NN NN NN NN NN NN N0 N0 NN N0 NN NN NN N0 NN NN  Methods  Attributes  B an  g s  B lo  n d  H ai  r  B u sh  y E  y eb  ro w  s  W ea  r N  ec k la  ce  N ar  ro w  E y es  N o cl  o ck  S h ad  o w  R ec  ed in  g H  ai rl  in e  W ea  r N  ec k ti  e  E y eg  la ss  es  R o sy  C h ee  k s  G o at  ee  C h u b b y  S id  eb u rn  s  B lu  rr y  W ea  r H  at  D o u b le  C h in  P al  e S  k in  G ra  y H  ai r  M u st  ac h e  B al  d  A v er  a g e  Imbalance ratio (N:x) N N N N N N NN NN NN NN NN NN NN NN NN N0 NN NN NN NN  Triplet-kNN [NN] NN NN NN N0 NN NN N0 NN NN NN NN NN NN NN NN N0 NN NN NN NN NN  PANDA [N0] NN NN NN NN NN NN NN NN NN NN NN NN NN N0 N0 NN NN NN NN NN NN  ANet [NN] N0 N0 NN NN NN NN N0 NN NN NN NN N0 NN NN N0 NN NN NN NN NN N0  DeepIDN [NN] NN N0 NN N0 NN NN NN NN NN NN N0 NN NN NN N0 NN NN N0 NN NN NN  Over-Sampling* [NN] N0 N0 N0 NN NN NN NN NN NN N0 NN NN N0 NN NN NN NN N0 N0 NN NN  Down-Sampling* [NN] NN NN NN NN NN NN NN N0 NN NN NN NN N0 NN N0 N0 NN NN N0 NN NN  Cost-Sensitive* [N0] N0 NN NN NN NN NN NN NN NN NN NN NN N0 NN N0 NN N0 N0 NN NN NN  LMLE* [NN] NN NN NN NN NN NN NN N0 NN NN NN NN NN NN NN NN N0 NN NN N0 NN  CRL(C)* NN NN NN NN N0 NN NN NN NN NN NN NN NN NN NN NN NN NN N0 NN NN  CRL(I)* NN NN NN NN NN N0 NN NN NN NN NN NN NN NN NN NN NN NN NN NN NN  tailers like Tmall.com
Each clothing image is annotated by  ≤ N attribute categories and each category has a different set of values (mutually exclusive within each set) ranging from  N (slv-len) to NN (colour)
In total, there are NNN distinctive attribute values in N categories (labels)
For each attribute  label, we adopted the class-imbalanced accuracy (i.e
mean  sensitivity) as the model performance metric given imbalanced data [NN, NN]
This additionally considers the class  distribution statistics in performance measurement
 N.N
Evaluation on Imbalanced Face Attributes  Competitors
We compared CRL against N existing methods including N state-of-the-art deep models for facial attribute recognition on CelebA: (N) Over-Sampling [NN], (N)  Down-Sampling [NN], (N) Cost-Sensitive [N0], (N) Large  Margin Local Embedding (LMLE) [NN], (N) PANDA [N0],  (N) ANet [NN], (N) Triplet-kNN [NN], and (N) DeepIDN [NN]
 Training/Test Data Partition
We adopted the same data  partition on CelebA as in [NN, NN]: The first NNN,NN0 images are used for training (N0,000 images for validation),  the following NN,NNN images for training the SVM classifiers required by PANDA [N0] and ANet [NN] models, and  the remaining NN,NNN images for testing
Note that identities  of all face images are non-overlapped in this partition
 Network Architecture & Parameter Settings
We  adopted the five layers CNN network architecture of  DeepIDN [NN] as the basis for training all six imbalanced  data learning methods including both our CRL models  (C&I), the same for LMLE as reported in [NN]
In addition  to the DeepIDN’s shared FCN layer, for explicitly modelling  the attribute specificness, in our CRL model we added a respective NN-dimensional FCN layer for each face attribute, in  the spirit of multi-task learning [NN, N]
We set the learning  rate at 0.00N to train our model from scratch on the CelebA face images
We fixed the decay to 0.000N and the momen- tum to 0.N
Our CRL model converges after N00 epochs training with a batchsize of NNN images
 Comparative Evaluation
Facial attribute recognition performance comparisons are shown in Table N
It is evident  that CRL outperforms on average accuracy all competitors  including the state-of-the-art attribute recognition models  and imbalanced data learning methods Compared to the  best non-imbalanced learning model DeepIDN, CRL(I) improves average accuracy by N%
Compared to the state-of- the-art imbalanced learning model LMLE, CRL(I) is better by N% in average accuracy
Other classical imbalanced learning methods perform similarly to DeepIDN
The performance drop by Down-Sampling is due to discarding useful data for balancing distributions
This demonstrates the  importance of explicit imbalanced data learning, and the superiority of the proposed batch incremental class rectification hard mining approach to handling imbalanced data over  NNNN    alternative methods
Figure N shows qualitative examples
 Blurry Mustache(NN) (NN) Bald (NN)  Figure N
Examples (N pairs) of facial attribute recognition (imbalance ratio in bracket)
In each pair, DeepIDN missed both, whilst  CRL identified the left image but failed the right image
 Model Performance vs
Data Imbalance Ratio
Figure N further shows the accuracy gain of six imbalanced  learning methods
It can be seen that LMLE copes better with less imbalanced attributes (towards the left side  in Figure N), but degrades notably given higher data imbalance ratio
Also, LMLE performs worse than DeepIDN  on more attributes towards the right of “Wear Necklace”  in Figure N, i.e
imbalance ratio greater than N:N in Table N
In contrast, CRLs with both class-level (CRL(C))  and instance-level (CRL(I)) hard mining perform particularly well on attributes with high imbalance ratios
More  importantly, even though CRL(I) only outperforms LMLE  by N% in average accuracy over all N0 attributes, this margin  increases to N% in average accuracy over the N0 most imbalanced attributes
Moreover, on some of the very imbalanced  attributes, CRL(I) outperforms LMLE by NN% on “Mustache” and NN% on “Blurry”
Interestingly, the “Blurry” attribute is challenging due to its global characteristics therefore not defined by local features and very subtle, similar  to the “Mustache” attribute (see Figure N)
This demonstrates that CRL is significantly better than LMLE in coping  with severely imbalanced data learning
This becomes more  evident with the X-domain clothing attributes (Sec
N.N),  mainly because given severe imbalanced data, it is difficult  for LMLE to cluster effectively due to very few minority  class samples, which leads to inaccurate classification feature learning
 -N0  -NN  -N0  -NN  -N0  -N  0  N  N0  A cc  u ra  cy  g  ai n  (  % )  Over-Sampling Down-Sampling Cost-Sensitive LMLE CRL(C) CRL(I)  A tt  ra c ti  v e  M o  n th   O p  e n  S m  il in  g  W e a r   L ip  st ic  k  H ig  h  C  h e e k  b o  n e s  M a le  H e a v  y  M  a k  e u  p  W a v  y  H  a ir  O v  a l  F  a c e  P o  in ty   N o  se  A rc  h e  E  y e b  ro w  s  B la  c k   H a ir  B ig   L ip  s  B ig   N o  se  Y o  u n  g  S tr  a ig  h t  H  a ir  B a g  s  u  n d  e r   E y  e s  W e a r   E a rr  in g  s  N o   B e a rd  B a n  g s  B lo  n d   H a ir  B u  sh y   E y  e b  ro w  s  W e a r   N e c k  la c e  N a rr  o w   e y  e s  N  c  lo c k   s h  a d  o w  R e c e d  in g   h a ir  li n  e  W e a r   N e c k  ti e  E y  e g  la ss  e s  R o  sy  C  h e e k  s  G o  a te  e  C h  u b  b y  S id  e b  u rn  s  B lu  rr y  W e a r   H a t  D o  u b  le  C  h in  P a le   s k  in  G ra  y  H  a ir  M u  st c h  e  B a ld  Imbalance ratio increases  -N0 B ro  w n   H a ir  Figure N
Performance gain over DeepIDN [NN] by the six imbalanced learning methods on the N0 CelebA facial attributes [NN]
 Attributes sorted from left to right in increasing imbalance ratio
 Model Training Time
We also tested the training time  cost of LMLE independently on an identical hardware setup  as for CRL: LMLE took NNN hours to train whilst CRL  (C/I) took NN/NN hours respectively with NN times training costs advantage over LMLE in practice
Specifically,  LMLE needs N rounds of quintuplets construction with each  taking NN hours, and N rounds of deep model learning with  each taking N hour
In total, N * (NN+N) = NNN hours
 N.N
Evaluation on Imbalanced Clothing Attributes  Competitors
In addition to the four imbalanced learning  methods (Over-Sampling, Down-Sampling, Cost-Sensitive,  LMLEN) used for face attribute evaluation, we also compared against four other state-of-the-arts clothing attribute  recognition models: (N) Deep Domain Adaptation Network  (DDAN) [N], (N) Dual Attribute-aware Ranking Network  (DARN) [NN], (N) FashionNet [NN], and (N) Multi-Task Curriculum Transfer (MTCT) [N0]
 Training/Test Data Partition
We adopted the same data  partition as in [NN, N0]: Randomly selecting NNN,NNN clothing images for training and the remaining N0,000 for testing
 Network Architecture
We used the same network structure as the MTCT [N0]
Specifically, this network is composited of five stacked NIN conv units [NN] and nattr parallel  branches with each a three FC layers sub-network for modelling one of the nattr attributes respectively, in the spirit of  multi-task learning [NN, N]
 Parameter Settings
We pre-trained a base model on  ImageNet-NK at the learning rate 0.00N, and then finetuned the CRL model on the X-Domain clothing images at the  same rate 0.00N
We fixed the decay to 0.000N and the momentum to 0.N
The CRL model converges after NN0 epochs
The batchsize is NNN
 Comparative Evaluation
Table N shows the comparative  evaluation of N0 different models on the X-Domain benchmark dataset
It is evident that CRL(I) surpasses all other  models on all attribute categories
This shows the significant superiority and scalability of the class rectification hard  mining with batch incremental approach in coping with extremely imbalanced attribute data, with the maximal imbalance ratio N, NNN vs
NN in CelebA attributes (Figure N)
A lack of explicit imbalanced learning mechanism in other  models such as DDAN, FashionNet, DARN and MTCT suffers notably
Among the N models designed for imbalance  data learning, we can observe similar trends as in face attribute recognition on CelebA
Whilst LMLE improves notably on classic imbalanced data learning methods, it remains inferior to CRL(I) by significant margins (N% in accuracy over all attributes)
 Model Effectiveness in Mitigating Data Imbalance
We  compared the relative performance gain of the N different  imbalanced data learning models (Down-Sampling was excluded due to poor performance) against the MTCT (as the  NWe trained an independent LMLE CNN model for each attribute label
This is because the quintuplets construction over all attribute labels is  prohibitively expensive in terms of computing cost
 NNNN    Table N
Clothing attributes recognition on the X-Domain dataset
* Imbalanced data learning models
Metric: Class-balanced accuracy,  i.e
mean sensitivity (%).
CRL(C/I): CRL with Class/Instance level hard mining
Slv-Shp: Sleeve-Shape; Slv-Len: Sleeve-Length
The  Nst/Nnd best results are highlighted in red/blue
 Methods  Attributes Category Colour Collar Button Pattern Shape Length Slv-Shp Slv-Len Average  Imbalance ratio (N:x)N N NNN NN0 NNN NNN NNNN NN0N NNNN NNNN  DDAN [N] NN.NN NN.NN NN.NN N0.NN NN.NN NN.NN NN.NN NN.NN N0.NN NN.NN  FashionNet [NN] NN.NN NN.NN NN.NN NN.NN NN.N0 NN.NN NN.NN N0.NN NN.NN NN.NN  DARN [NN] NN.NN NN.N0 NN.NN NN.N0 NN.NN NN.NN NN.N0 NN.NN NN.NN NN.NN  MTCT [N0] NN.NN NN.NN N0.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  Over-Sampling* [NN] NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.N0 NN.NN NN.N0  Down-Sampling* [NN] NN.NN NN.NN NN.NN NN.NN NN.NN N0.NN NN.NN NN.NN NN.N0 NN.NN  Cost-Sensitive* [N0] NN.0N NN.NN NN.NN NN.NN NN.NN NN.0N NN.0N NN.NN NN.NN NN.NN  LMLE* [NN] NN.N0 NN.NN N0.NN NN.NN NN.NN NN.NN NN.NN NN.NN N0.NN NN.NN  CRL(C)* NN.NN NN.NN NN.N0 NN.0N NN.NN NN.NN NN.NN NN.0N NN.NN NN.NN  CRL(I)* NN.NN NN.N0 NN.N0 NN.N0 NN.NN NN.NN NN.0N NN.NN NN.NN NN.NN  N N N N N N N N N -N  0  N  N  N  N  N0  Over-Sampling  Cost-Sensitive  LMLE  CRL(C)  CRL(I)  Clothing   Category  Colour  Collar   Button  Pattern Clothing   Shape Clothing  Length  Sleeve  Shape  Sleeve  Length  0 N N0 NN N0  0  N000  N000  N000  N000  N0000  NN000  NN000  NN000  NN000  0 N0 N0 N0 N0 N0  0  N  N  N  N  N  N ×N0  N  0 N N0 NN N0 NN  0  N  N  N  N  N  N  N ×N0  N  0 N N N N N0 NN  0  N  N  N  N  N  N  N ×N0  N  N NNN NNNNN0  0 N N0 NN N0 NN  0  N  N  N  N  N0  NN  NN  NN ×N0  N  0 N N N N N0  0  N  N  N  N  N0  NN  NN ×N0  N  0 N N N N N N  0  N  N  N  N  N  N  N  N  N  N0 ×N0  N  0 N N N N N0 NN NN NN  0  0.N  0.N  0.N  0.N  N  N.N  N.N  N.N  N.N  N ×N0  N  0 N N N N N N N  0  0.N  N  N.N  N  N.N ×N0  N  NNN NNNN NN0N NNNN NNNN  A cc  u ra  cy  g  ai n   ( %  )  Imbalance ratio increases
 Figure N
Model performance additional gain over the MTCT on N  clothing attributes with increasing imbalance ratios on X-Domain
 baseline), along with the imbalance ratio for each clothing  attribute
Figure N shows the comparisons and it is evident  that CRL is clearly superior in learning severely imbalanced  attributes, e.g
on “Sleeve Shape”, CRL(C) and CRL(I)  achieve N% and N% accuracy gain over MTCT respectively, as compared to the second best LMLE obtaining only N% improvement
Qualitative examples are shown in Figure N
 WoolenCoat  Red   MidLong  Lattice   Slim, Hooded   LongSlv  RegularSlv  WoolenCoat  SingBreastN   Red   MidLong  SolidColor  Slim,Round  N/Nslv,Bellslv  Cotton  DoubBreast  ArmyGreen  MidLong  SolidColor  Slim,Hooded  LongSlv  Cotton,Zipper  RoseRed  Short  Dot, Slim   Hooded  LongSlv  Regular  SingBreastN  FlowerColor  Long, Cloak  SolidColor  Hooded  LongSlv  Regular  WoolenCoat  SingbreastN  Apricot,  Regular  MidLong  SolidColor  Hooded   Longslv  Figure N
Examples of clothing attribute recognition by the  CRL(I) model, with falsely predicted attributes in red
 N.N
Analysis on Rectification Loss and Hard Mining  We evaluated the effects of two different hard mining  schemes (Class and Instance level) (Sec
N.N), and three different CRL loss functions (Relative, Absolute, and Distribution comparisons) (Sec
N.N)
In total, we tested N different CRL variants
We evaluated the performance of these  N CRL models by the accuracy gain over a non-imbalance  learning baseline model: DeepIDN on CelebA and MTCT  on X-domain
It is evident from Table N that: (N) All  CRL models improve accuracy on both facial and clothing attribute recognition
(N) For both face and clothing,  CRL(I+R) is the best and its performance advantage over  other models is doubled on the more imbalanced X-Domain  when compared to that on CelebA
(N) Most CRL models achieve greater performance gains on X-Domain than  on CelebA
(N) Using the same loss function, instance-level  hard mining is superior in most cases
 Table N
Comparing different hard mining schemes (Class and Instance level) and loss functions (Relative(R), Absolute(A), and  Distribution(D))
Metric: additional gain in average accuracy (%)
Dataset CelebA X-domain  Loss function A R D A R D  Class Level N.NN N.NN 0.NN N.NN N.NN N.N0  Instance Level N.NN N.NN N.NN N.NN N.NN N.0N  N
Conclusion  In this work, we formulated an end-to-end imbalanced  deep learning framework for clothing and facial attribute  recognition with very large scale imbalanced training data
 The proposed Class Rectification Loss (CRL) model with  batch-wise incremental hard positive and negative mining  of the minority classes is designed to regularise deep model  learning behaviour given training data with significantly imbalanced class distributions in very large scale data
Our  experiments show clear advantages of the proposed CRL  model over not only the state-of-the-art imbalanced data  learning models but also dedicated attribute recognition  methods for multi-label clothing and facial attribute recognition, surpassing the state-of-the-art LMLE model by N% in average accuracy on the CelebA face benchmark and N% on the more imbalanced X-Domain clothing benchmark,  whilst having over threes time faster model training time  advantage
 Acknowledgements This work was partially supported by the China Scholarship Council, Vision Semantics Ltd., and the Royal Society  Newton Advanced Fellowship Programme (NANN0NNN)
 NNNN    References  [N] R
Akbani, S
Kwek, and N
Japkowicz
Applying support  vector machines to imbalanced datasets
In European Conference of Machine Learning, pages NN–N0, N00N
N  [N] R
K
Ando and T
Zhang
A framework for learning predictive structures from multiple tasks and unlabeled data
The  Journal of Machine Learning Research, N(Nov):NNNN–NNNN,  N00N
N, N  [N] Y
Bengio, A
Courville, and P
Vincent
Representation  learning: A review and new perspectives
IEEE Transactions  on Pattern Analysis and Machine Intelligence, NN(N):NNNN–  NNNN, N0NN
N  [N] N
V
Chawla, K
W
Bowyer, L
O
Hall, and W
P
 Kegelmeyer
Smote: synthetic minority over-sampling technique
Journal of Artificial Intelligence Research, NN:NNN–  NNN, N00N
N, N  [N] C
Chen, A
Liaw, and L
Breiman
Using random forest to  learn imbalanced data
University of California, Berkeley,  pages N–NN, N00N
N  [N] K
Chen, S
Gong, T
Xiang, and C
Loy
Cumulative attribute space for age and crowd density estimation
In IEEE  Conference on Computer Vision and Pattern Recognition,  Portland, Oregon, USA, N0NN
N  [N] Q
Chen, J
Huang, R
Feris, L
M
Brown, J
Dong, and  S
Yan
Deep domain adaptation for describing people based  on fine-grained clothing attributes
In IEEE Conference  on Computer Vision and Pattern Recognition, pages NNNN–  NNNN, N0NN
N, N, N, N, N  [N] S
Chopra, R
Hadsell, and Y
LeCun
Learning a similarity  metric discriminatively, with application to face verification
 In IEEE Conference on Computer Vision and Pattern Recognition, N00N
N  [N] Y
Cui, F
Zhou, Y
Lin, and S
Belongie
Fine-grained categorization and dataset bootstrapping using deep metric learning with humans in the loop
In IEEE Conference on Computer Vision and Pattern Recognition, N0NN
N  [N0] Q
Dong, S
Gong, and X
Zhu
Multi-task curriculum transfer deep learning of clothing attributes
In IEEE Winter Conference on Applications of Computer Vision, N0NN
N, N, N  [NN] C
Drummond, R
C
Holte, et al
CN
N, class imbalance, and  cost sensitivity: why under-sampling beats over-sampling
 volume NN, N00N
N, N  [NN] M
Everingham, S
A
Eslami, L
Van Gool, C
K
Williams,  J
Winn, and A
Zisserman
The pascal visual object classes  challenge: A retrospective
International Journal of Computer Vision, NNN(N):NN–NNN, N0NN
N  [NN] T
Evgeniou and M
Pontil
Regularized multi–task learning
 In ACM SIGKDD International Conference on Knowledge  Discovery and Data Mining, pages N0N–NNN, N00N
N, N  [NN] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and D
Ramanan
Object detection with discriminatively trained partbased models
IEEE Transactions on Pattern Analysis and  Machine Intelligence, NN(N):NNNN–NNNN, N0N0
N  [NN] R
Feris, R
Bobbitt, L
Brown, and S
Pankanti
Attributebased people search: Lessons learnt from a practical surveillance system
In ACM International Conference on Multimedia Retrieval, page NNN, N0NN
N  [NN] F
Fernández-Navarro, C
Hervás-Martı́nez, and P
A
 Gutiérrez
A dynamic over-sampling procedure based on  sensitivity for multi-class problems
Pattern Recognition,  NN(N):NNNN–NNNN, N0NN
N  [NN] S
Gong, M
Cristani, S
Yan, and C
C
Loy
Person reidentification, volume N
Springer, N0NN
N  [NN] G
Griffin, A
Holub, and P
Perona
Caltech-NNN object category dataset
N00N
N  [NN] H
Han, W.-Y
Wang, and B.-H
Mao
Borderline-smote: a  new over-sampling method in imbalanced data sets learning
 In International Conference on Intelligent Computing, pages  NNN–NNN
Springer, N00N
N  [N0] H
He and E
A
Garcia
Learning from imbalanced data
 IEEE Transactions on knowledge and data engineering,  NN(N):NNNN–NNNN, N00N
N, N, N, N  [NN] C
Huang, Y
Li, C
Change Loy, and X
Tang
Learning  deep representation for imbalanced classification
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNNN–NNNN, N0NN
N, N, N, N, N,  N, N  [NN] J
Huang, R
S
Feris, Q
Chen, and S
Yan
Cross-domain image retrieval with a dual attribute-aware ranking network
In  IEEE International Conference on Computer Vision, N0NN
 N, N, N, N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 arXiv e-prints, N0NN
N  [NN] P
Jeatrakul, K
W
Wong, and C
C
Fung
Classification  of imbalanced data by combining the complementary neural  network and smote algorithm
In International Conference  on Neural Information Processing, pages NNN–NNN
Springer,  N0N0
N, N, N, N  [NN] S
H
Khan, M
Bennamoun, F
Sohel, and R
Togneri
Cost  sensitive learning of deep feature representations from imbalanced data
arXiv e-prints, N0NN
N, N  [NN] A
Krizhevsky and G
Hinton
Learning multiple layers of  features from tiny images
N00N
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in Neural Information Processing Systems, pages  N0NN–NN0N, N0NN
N  [NN] M
Lin, Q
Chen, and S
Yan
Network in network
arXiv  e-prints, N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European Conference on Computer Vision, pages NN0–NNN, N0NN
N  [N0] T.-Y
Liu
Learning to rank for information retrieval
Foundations and Trends in Information Retrieval, N(N):NNN–NNN,  N00N
N  [NN] Z
Liu, P
Luo, S
Qiu, X
Wang, and X
Tang
Deepfashion:  Powering robust clothes recognition and retrieval with rich  annotations
In IEEE Conference on Computer Vision and  Pattern Recognition, pages N0NN–NN0N, N0NN
N, N, N, N, N  [NN] Z
Liu, P
Luo, X
Wang, and X
Tang
Deep learning face attributes in the wild
In Proceedings of the IEEE International  Conference on Computer Vision, pages NNN0–NNNN, N0NN
N,  N, N, N, N  NNNN    [NN] T
Maciejewski and J
Stefanowski
Local neighbourhood  extension of smote for mining imbalanced data
In IEEE  International Conference on Data Mining, pages N0N–NNN,  N0NN
N, N  [NN] I
Mani and I
Zhang
knn approach to unbalanced data  distributions: a case study involving information extraction
 In Proceedings of workshop on learning from imbalanced  datasets, N00N
N, N  [NN] H
Oh Song, Y
Xiang, S
Jegelka, and S
Savarese
Deep  metric learning via lifted structured feature embedding
In  IEEE Conference on Computer Vision and Pattern Recognition, N0NN
N  [NN] M
Oquab, L
Bottou, I
Laptev, and J
Sivic
Learning and  transferring mid-level image representations using convolutional neural networks
In IEEE Conference on Computer  Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N,  N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  et al
Imagenet large scale visual recognition challenge
 International Journal of Computer Vision, NNN(N):NNN–NNN,  N0NN
N  [NN] F
Schroff, D
Kalenichenko, and J
Philbin
Facenet: A  unified embedding for face recognition and clustering
In  IEEE Conference on Computer Vision and Pattern Recognition, N0NN
N, N  [NN] A
Sharif Razavian, H
Azizpour, J
Sullivan, and S
Carlsson
Cnn features off-the-shelf: an astounding baseline for  recognition
In IEEE Conference on Computer Vision and  Pattern Recognition, pages N0N–NNN, N0NN
N  [N0] W
Shen, X
Wang, Y
Wang, X
Bai, and Z
Zhang
Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection
In IEEE Conference  on Computer Vision and Pattern Recognition, pages NNNN–  NNNN, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In International  Conference on Learning Representations, N0NN
N  [NN] Y
Sun, Y
Chen, X
Wang, and X
Tang
Deep learning  face representation by joint identification-verification
In Advances in Neural Information Processing Systems, N0NN
N,  N  [NN] Y
Tang, Y.-Q
Zhang, N
V
Chawla, and S
Krasser
Svms  modeling for highly imbalanced classification
IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics), NN(N):NNN–NNN, N00N
N, N  [NN] K
M
Ting
A comparative study of cost-sensitive boosting  algorithms
In International Conference on Machine learning, N000
N, N  [NN] E
Ustinova and V
Lempitsky
Learning deep embeddings  with histogram loss
In Advances in Neural Information Processing Systems, N0NN
N  [NN] J
Wang, Y
Song, T
Leung, C
Rosenberg, J
Wang,  J
Philbin, B
Chen, and Y
Wu
Learning fine-grained image similarity with deep ranking
In IEEE Conference on  Computer Vision and Pattern Recognition, N0NN
N  [NN] J
Wang, X
Zhu, S
Gong, and W
Li
Attribute recognition by joint recurrent learning of context and correlation
In  IEEE International Conference on Computer Vision, N0NN
N  [NN] X
Wang and A
Gupta
Unsupervised learning of visual representations using videos
In IEEE International Conference  on Computer Vision, N0NN
N  [NN] B
Zadrozny, J
Langford, and N
Abe
Cost-sensitive learning by cost-proportionate example weighting
In IEEE International Conference on Data Mining, pages NNN–NNN, N00N
 N  [N0] N
Zhang, M
Paluri, M
Ranzato, T
Darrell, and L
Bourdev
 Panda: Pose aligned networks for deep attribute modeling
In  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N, N  [NN] Z.-H
Zhou and X.-Y
Liu
Training cost-sensitive neural  networks with methods addressing the class imbalance problem
IEEE Transactions on Knowledge and Data Engineering, NN(N):NN–NN, N00N
N, N  NNN0A Two Stream Siamese Convolutional Neural Network for Person Re-Identification   A Two Stream Siamese Convolutional Neural  Network For Person Re-Identification  Dahjung Chung Khalid Tahboub Edward J
Delp  Video and Image Processing Laboratory (VIPER)  School of Electrical and Computer Engineering  Purdue University  West Lafayette, Indiana, USA  chungNNN@purdue.edu ktahboub@purdue.edu ace@ecn.purdue.edu  Abstract  Person re-identification is an important task in video  surveillance systems
It can be formally defined as establishing the correspondence between images of a person  taken from different cameras at different times
In this paper, we present a two stream convolutional neural network  where each stream is a Siamese network
This architecture can learn spatial and temporal information separately
 We also propose a weighted two stream training objective  function which combines the Siamese cost of the spatial and  temporal streams with the objective of predicting a person’s  identity
We evaluate our proposed method on the publicly  available PRIDN0NN and iLIDS-VID datasets and demonstrate the efficacy of our proposed method
On average, the  top rank matching accuracy is N% higher than the accuracy achieved by the cross-view quadratic discriminant analysis used in combination with the hierarchical Gaussian descriptor (GOG+XQDA), and N% higher than the recurrent neural network method
 N
Introduction  In recent years, the number of video surveillance systems  has increased dramatically
According to a study by Cisco,  Internet video surveillance traffic is projected to increase  tenfold between N0NN and N0N0 [N]
The continuous monitoring of surveillance data is practically impossible, making  the automatic analysis of surveillance video the only plausible solution
Many video analytic methods have been proposed for person detection and tracking, action recognition,  crowd analysis and anomaly detection
 One of the fundamental tasks associated with video  surveillance systems is person re-identification (ReID)
Person re-identification refers to tracking a person across a  network of non-overlapping cameras [N, N]
Given sinC a m  e r a  B  C a m  e r a  A  PRID N0NN ILIDS-VID  Figure N: Sample images of two subjects captured from two  different cameras in the PRIDN0NN [N] and the ILIDS-VID  [N] datasets  gle/multiple images or a video sequence outlining a person’s appearance in the field of view of a camera, person  re-identification is the task of recognizing the same person  across a network of cameras with non-overlapping fields of  view
It can be formally defined as establishing the correspondence between images of a person taken from different  cameras at different times [N]
 The ReID task is a challenging problem
It remains an  active research area due to inter/intra illumination changes,  pose variations, occlusions, cluttered background, various  scales and viewpoints [N]
Figure N shows sample images  depicting the differences in camera viewpoints and illumination conditions
 Over the past years, the performance of ReID methods have improved by adopting new features, using metric learning techniques, and the use of semantic attributes  and appearance models [N–NN]
Most of the traditional  approaches proposed for ReID uses low-level features in  the form of color and texture histograms and exploit metric learning to find a distance function in which distances  between images from the same class are minimized and  NNNN    distances between different classes are maximized [N, N0,  NN–NN]
In addition, multiple datasets have been made  available for testing, these include: Viewpoint invariant pedestrian recognition dataset (VIPeR) [N], person reidentification (PRIDN0NN) dataset [N] and the iLIDS video  re-identification (iLIDS-VID) dataset [N]
VIPeR contains  a single image per appearance (single shot), whereas recent  datasets, such as PRIDN0NN and iLIDS-VID, contain multiple images per appearance (multi-shot)
ReID in multi-shot  scenarios is also referred to as video-based ReID
 [NN] demonstrates that multi-shot scenario is superior to  single-shot scenario using empirical evidences
Their experiments show that multi-shot approaches are more favorable since both probe and gallery contain much richer visual information as compared to single image
In addition,  combining spatial features using multi-shot helps address  the challenges associated with viewpoint and pose invariance [N, NN]
Also, in real-life surveillance systems, human  detection and tracking methods generate multiple images  for each person appearance
Therefore, ReID in multi-shot  scenario is more suitable for practical applications
 [NN] shows that temporal features (e.g
gait pattern) can  offer discriminative features for person identification even  using low resolution video sequences
In ReID multi-shot  scenarios, these temporal features can be used in combination with spatial features to create better feature representation
Temporal features can improve the accuracy of ReID  methods in particular when the majority of clothing worn  by subjects tends to be non-discriminative [N, N0]
 In [N0], a deep learning video-based ReID method using  a recurrent convolutional neural network architecture is proposed to exploit both spatial and temporal features
A single network is used to learn a representation for both feature  types
This poses a limitation which constrains the amount  of information that the network can learn
To address this  limitation, we propose the use of a two stream convolutional  neural network (CNN) [NN] with weighted objective function where each stream has Siamese structure [NN]
 The main contributions of this paper are:  • We propose a two stream CNN architecture where each stream is a Siamese network
This architecture can  learn spatial and temporal information separately
By  having two separate networks, each network can learn  its own best feature representation
 • We propose a weighted two stream training objective function which combines the Siamese cost of the spatial and temporal streams with the objective to predict a person’s identity
For the ReID task, spatial  features are more discriminative than temporal features [N]
The weighted cost function controls the individual contribution of the two streams accordingly
 To our best knowledge, this is the first time a weighted  two stream cost function is proposed for ReID
 We evaluate our proposed method on two publicly available  datasets
Our proposed method outperforms or shows comparable results to the existing best perform methods on both  datasets
 N
Related Work  Recent ReID methods have focused on appearance modeling and metric learning to establish correspondences between people images
The input is assumed to be bounding  boxes outlining persons appearances in two different cameras
Each appearance is represented by a single or multiple bounding boxes
We will also assume this in this paper
A common approach is to divide a bounding box into  a number of horizontal strips and to extract low-level features from each strip
We will describe some of the features  that have been proposed for use in ReID systems
In [N],  an ensemble of local features (ELF) is constructed by using  the eight color channels corresponding to the three separate  channels of the RGB, YCbCr and HSV color spaces with  the exception of the value (V) channel
Thirteen Schmid  filters and six Gabor filters are also used to model texture
 Sixteen bin histograms are constructed for each of the NN  filter responses and for the eight color channels
The histograms are concatenated to form a high dimensional feature vector for each image
Other approach is the use of the  local maximal occurrence feature (LOMO) based on multiscale Retinex to estimate HSV color histograms used for  color features [N]
The scale invariant local ternary pattern (SILTP) descriptor is used to model illumination invariant texture [NN]
In [N], a hierarchical Gaussian descriptor  (GOG) is proposed and is based on the mean and covariance  information of pixel features within patches and region hierarchies
Color and texture features are usually concatenated  to form a high dimensional feature vector which is used as  an input for learning methods
 We now describe some of the classification/learning  methods that have been used in ReID
Metric or distance learning is used to find a distance function in which  distances between images from the same class are minimized and in which distances between different classes  are maximized [N–N0, NN–NN]
The keep it “simple and  straightforward metric learning” method (KISSME) [N] and  cross-view quadratic discriminant analysis (XQDA) [N] are  widely used metric learning techniques for ReID
Both approaches belong to the class of Mahalanobis distance functions
KISSME, based on a likelihood ratio test, casts the  problem in the space of pairwise differences and assumes a  Gaussian structure of the difference space [N]
XQDA extends Bayesian faces and the KISSME approach by learning  a subspace reduction matrix and a cross-view metric jointly
 A closed-form solution is computed by formatting the probNNNN    lem as a generalized Rayleigh quotient and using eigenvalue  decomposition [N]
 In [NN], a multi shot approach is based on the combination of random projections for dimensionality reduction  and random forests for classification
A relative distance  comparison model which maximizes the likelihood that a  pair of correct match has a smaller distance than that of a  wrong match pair along with an ensemble strategy is introduced in [NN]
In [NN], person re-id is formulated as a  block sparse recovery problem and in [NN] is formulated as  a graph matching problem
In [NN], images for a person trajectory are clustered hierarchically to mitigate the problems  faced by Fisher Discriminate Analysis (FDA)
A viewpointinvariant descriptor along with sub-image rectification and  poses estimation is proposed in [NN]
 When the majority of clothing worn tends to be nondiscriminative, ReID becomes very challenging
Attributesbased re-identification methods try to solve this problem  by incorporating semantic attributes
‘Jacket’, ‘female’  and ‘carried object’ are all examples of semantic attributes
 Semantic attributes are mid-level features learned from a  larger dataset a priori [N0]
In [NN], semantic attributes are  combined with the low level features and is shown to improve the performance of ReID
 Until recently, CNN architectures [NN] have not been  used for the ReID due to the small size of public datasets
 With the release of larger datasets, recent methods have  demonstrated the feasibility of the use of CNNs for  ReID [NN, NN]
A filter pairing neural network (FPNN) is  proposed as a unified solution to extract features and learn  photometric and geometeric transforms in [NN]
In [NN], feature extraction layers are followed by a cross-input neighborhood difference layer to compute the differences in feature values across the camera views
 Very recent deep learning ReID methods extended [NN,  NN] and incorporate metric learning and part-based learning
In [NN], a cosine layer connects two sub-networks and  jointly learn color, texture and a similarity metric
In [NN],  multi channels part-based CNN is proposed to jointly learn  both global and local body features of the person
The network is trained using triplet images and a triplet loss function is used to learn the network model
In [NN], single  image and cross-image representations are combined in a  single network
A deep learning network for learning features from multiple domains is proposed in [NN]
A domain  guided dropout (DGD) method is shown to improve feature  learning
 Most of the existing deep learning methods are based on  a simple architecture and ignore the temporal information in  a multi-shot scenario
To exploit the temporal information  for ReID, the use of recurrent neural network (RNN) with  the Siamese structure is proposed in [N0]
An optical flow  image is concatenated to the YUV image and comprises the  input to the deep learning network
For the remainder of  this paper, we will refer to the ReID technique proposed  in [N0] as the RNN-ReID technique
Instead of using a single network to learn both spatial and temporal features, we  propose the use of a two stream CNN architecture where  each stream is a separate Siamese network
 N
Proposed Method  The overall architecture of our proposed method is  shown in Figure N
The method is motivated by the fact  that both spatial and temporal features possess discriminative information useful for the ReID task
However, the  best feature representation does not need to be the same for  both types of features
Therefore, we propose a two stream  Siamese CNN which processes spatial and temporal information separately
Siamese CNNs contain two identical  sub-networks with shared weights and are suitable for tasks  which involve finding the similarity between two comparable inputs [NN]
CNNs typically process an image or multiple images and classify them into a single class, whereas  Siamese CNNs process two images or two sequences of  images and compute the similarity between them
In our  proposed ReID system, the input to first stream are two sequences of RGB frames where each sequence is captured  from a different camera
The second stream processes the  optical flow information from both cameras as shown in  Figure N
The input is described in more details in Section N.N
Each stream is based on the same network architecture
Throughout this paper, we will refer to the network  associated with spatial content as SpatialNet and the network associated with temporal content as TemporalNet
 Both networks are composed of multiple CNNs with  Siamese architecture, and all the CNNs within the same  stream share the same parameters
We refer to this CNN  as the “base CNN” and describe its structure in Section N.N
 The outputs of the base CNNs which processes images from  the same camera view are combined using temporal pooling
The temporal pooling is described in Section N.N
The  outputs of the temporal pooling from both cameras are combined using the Siamese cost as described in Section N.N
 Finally, the two networks associated with both streams are  fused together using a weighted cost function as described  in Section N.N
 N.N
The Inputs  We define the generic input sequence as Ic, where c ∈ a, b for camera A and B, respectively
For the SpatialNet,  the input sequence are RGB frames:  Ic = (S (N), 


, S(t), 


, S(L)) (N)  where L is the sequence length and S(t) is the RGB frame  at time t
 NNNN    CNN  RGB Frames Optical Flow Stacks  Temporal Pooling Temporal Pooling  RGB Frames Optical Flow Stacks  Temporal Pooling Temporal Pooling  Camera BCamera A  Final Siamese Cost  CNN  … … … …  Siamese Cost N  Camera BCamera A  Siamese Cost N  CNN CNN CNN CNN CNN CNN  Identity Cost  SpatialNet TemporalNet  … … … …  Figure N: Overall Architecture of the proposed two stream ReID system  For TemporalNet, optical flow images are used as input:  Ic = (T (N), 


, T (t), 


, T (L)) (N)  where L is the sequence length and T (t) is the input optical flow image at time t
To obtain T (t), the displacement  vectors in the horizontal and vertical directions between a  pair of consecutive frames are computed using the LucasKanade optical flow technique [NN]
The effectiveness of  using optical flow to learn temporal features are demonstrated in [N0, NN]
 N.N
The Base CNN Architecture  Conv N  NxNxNN  ( pad N, stride N )  tanh  NxN max pool  Conv N  NxNxNN  ( pad N, stride N )  tanh  NxN max pool  Conv N  NxNxNN  ( padN, stride N )  tanh  NxN max pool  full  NNN  Dropout  f  Figure N: The structure of the base CNN and hyperparameters  As shown in Figure N, the input sequence Ic is processed  using the base CNN
Figure N shows the base CNN structure  and the hyper-parameters associated with it
The CNN takes  one input sample (S(t) or T (t)) and produces the output  feature vector fS or fT for SpatialNet and TemporalNet,  respectively
Our base CNN is composed of three convolution layers where each layer has convolution, non-linear activation and max-pooling steps
We use hyperbolic-tangent  (tanh) as non-linear activation function
At the end of the  three convolution layers, a fully connected layer is placed  to have mapping to all the activations from the last convolution layer
Dropout [N0] is also used to reduce the model  over-fitting
 N.N
Temporal Pooling  For the SpatialNet, the base CNN processes a single  RGB frame out of the sequence of frames in the multi-shot  scenario, or optical flow content in the case of the TemporalNet
Combining the spatial or temporal features using multiple frames helps address the challenges associated  with various viewpoints and poses
To process the input sequence, each sub-network of the Siamese network in each  stream utilizes L base CNNs and produces L feature vectors
The feature vectors produced by the L CNNs in each  sub-network are combined into a single feature vector using temporal pooling
Max pooling, sum pooling and mean  pooling are the most common techniques used to achieve  this
In [N0], the RNN-ReID method has shown that the  mean pooling method is the most suitable temporal pooling  technique for the ReID task
We adopt the same approach  in our proposed method
If we denote the base CNN by the  function C(), then the temporally pooled feature vector, f̄ic , is computed as follows:  f̄ic = N  L  L ∑  t=N  C(I (t) ic  ) (N)  where i is the person ID, c ∈ {a, b} is the camera view  and I (t) ic  , t ∈ N, 


, L, is one element (RGB image or op- tical flow vectors) of the input multi-shot sequence
The  sequence of images are processed and temporally pooled to  obtain the feature vector f̄Sic or f̄ T ic  for the SpatialNet and  TemporalNet, respectively
 N.N
Siamese Cost  Siamese networks are composed of two sub-networks  with shared weights [NN]
While learning the features from  each sub-network, Siamese networks compare the features  NNNN    from the pair using Euclidean distance
Thus, in training  process, the network tries to minimize the distance between  feature pairs when they are from the same class and maximize the distance between feature pairs when they are from  different classes
Due to this property, Siamese networks  have been widely used for the ReID task since the goal is  to find the similarity between a pair of sequences
As mentioned before, we use a Siamese network for both streams:  SpatialNet and TemporalNet as shown in Figure N
Furthermore, the generic Siamese cost of our proposed method can  be defined as follows:  D(f̄ic , f̄jc) =  {  N N ||f̄ic − f̄jc ||  N , if i = j  N N{max(m− ||f̄ic − f̄jc ||, 0)}  N , if i N= j  (N)  where m is the Siamese margin and f̄ic , f̄jc are the temporally pooled feature vectors for person i and j, respectively
 Equation N applies to both SpatialNet and TemporalNet in  the same way with different type of inputs
 N.N
Weighted Two Stream Joint Identification and Verification  During the training process, we build on the joint identification and verification approach from [NN] to define our  training objective
We use the softmax loss function to compute the identification cost as in [N0]
Then, this cost is integrated into our final training objective function as explained  later
The identification cost is defined as:  V (x) = P (q = c|x) = exp(Wcx)  ∑  k exp(Wkx) (N)  where x is the feature vector and q is the person’s identity
 Wc and Wk indicate the cth and the kth column of the softmax matrix W , respectively
Note that the softmax matrix  W is the matrix representation of the fully connected layer  in the base CNN architecture
 From the RNN-ReID method, it was already observed  that joining the identification with the Siamese cost is crucial to improve the ReID accuracy
We have two Siamese  cost functions from each stream, whereas RNN-ReID has  only one Siamese cost
Therefore, we define the combined  cost function Jf as follows:  Jf =ωSD(f̄ S ic , f̄Sjc) + ωTD(f̄  T ic , f̄Tjc)  + V (f̄Sic) + V (f̄ S jc )  (N)  where V is the standard softmax loss defined in Equation  N
ωS , ωT are the weights for SpatialNet and TemporalNet, respectively
Note that we only use the identification  cost V which is computed using the spatial features since  they contain more information regarding to the person label than the temporal features
We propose using different weights for each stream to be able to emphasize the  spatial features as compared to the temporal features
For  ReID Task, even though walking motion adds discriminative power to the ReID solution, spatial features such as  appearance, color or texture are relatively more important  in terms of re-identifying people
Thus, we set the weights  empirically with the condition ωS ≥ ωT 
 N.N
Similarity Metric for Testing  The weighted two stream joint identification and verification objective function, which is used for training, incorporates the ability to predict a person’s identity
However,  during the evaluation, the goal is to find the similarity score  (metric) between two sequences of images and to rank the  gallery accordingly
Therefore, we modify Equation N to  disregard the contribution of the standard softmax loss V  and replace the Siamese cost D with the Euclidean distance
 The Euclidean distances are computed using the temporally  pooled feature vectors (f̄Sic , f̄ T ic  , f̄Sjc and f̄ T jc  ) as follows:  dS = ||f̄ S ia − f̄Sjb || (N)  dT = ||f̄ T ia − f̄Tjb || (N)  Finally, dS and dT are combined using a weighted average  to compute the final similarity metric dF :  dF = ωSdS + ωT dT  ωS + ωT (N)  N
Experiments  In this section, we evaluate our proposed method  using the publicly available datasets: Person reidentification (PRIDN0NN) dataset [N] and the iLIDS  video re-identification (iLIDS-VID) dataset [N]
We  investigate our proposed method with different hyperparameter settings and evaluate the performance against the  state-of-the-art ReID methods
 N.N
Datasets  Both datasets feature a multi-shot scenario in which  a person trajectory is represented by a sequence of images
The PRIDN0NN dataset contains images from two  non-overlapping static surveillance cameras
The sequence  presents the significant differences in viewpoint, illumination and camera characteristics
It is composed of NNN person trajectories from one view and NNN from the other one,  with N00 persons appearing in both views
Each image sequence has a variable length ranging from N to NNN image  frames, with an average number of N00 images
We only  consider the N00 persons appearing in both views as suggested in [N]
 The iLIDS-VID dataset was created by observing pedestrians in two camera views
The outputs of two nonoverlapping cameras were captured at a crowded airport arNNNN    rival hall
It consists of N00 image sequences of N00 individuals with one pair of sequences from two camera views  for each person
Each image sequence has a variable length  ranging from NN to NNN image frames, with an average number of NN images
It is one of the most challenging datasets  due to the cluttered background and random occlusions
 N.N
Experiment Setup  Input images are pre-processed before being fed into the  two stream Siamese CNN
Each color channel of the RGB  image is normalized to introduce invariance to illumination  changes
This is simply done by subtracting the mean and  dividing by the standard deviation
Each horizontal and vertical optical flow channel is also normalized to the range of  [−N, N]
The same data augmentation technique in [N0] is used to  add more variety to the data
Random mirroring and cropping are used for data augmentation
Note that a consistent  data augmentation technique is applied to the images from  the same sequence
 As suggested in [N0], positive and negative pairs are alternatively fed into the network
Sequence pairs are randomly sampled from the all training identities
All training  sequence lengths are set to NN and the test sequence lengths  are varied to investigate the significance of the sequence  length as described in Section N.N.N
Note that this sequence  length can be arbitrary due to the network architecture
 The proposed network is trained for N000 epochs using  the stochastic gradient descent method
The batch size is  set to N, the learning rate to Ne−N and the momentum to 0.N
The Siamese cost function margin is set to m = N
The base CNN feature dimension is NNN with the dropout rate set to  0.N
 N.N
Evaluation Protocol  We follow the evaluation protocol described in [N]
The  dataset is randomly split into two subsets with the same size
 One is used for training and one for testing
For the testing,  the sequences from the first camera are used as the probes  while the sequences from the second camera are used as the  gallery
 We validate the performance of our proposed method  and compare the performance against other methods using the Cumulative Matching Characteristic (CMC) curve  which indicates the probability of finding the correct match  in the top K matches within the ranked gallery
The experiment is repeated five times by randomly splitting the dataset  into training and testing and the average result is reported
 In our proposed method, we have two extra hyperparameters (ωS , ωT )
To see the effectiveness of proposed method, we perform experiments with various hyperparameters settings
We perform experiments with ωS = N when ωT is set to 0 or N in order to verify the individual  contribution of TemporalNet
We also perform experiments  with ωS = N, N when ωT = N to see the relative contribution of the spatial features as compared to the temporal features
 N.N
Results and Discussion  N.N.N Probe and Gallery Sequence Length  ❳ ❳  ❳ ❳  ❳ ❳ ❳ ❳ ❳ ❳  Length Rank N N N0 N0  NN NN N0 NN NN  NN N0 NN NN NN  NN NN NN NN NN  NNN NN NN NN NN  Table N: Matching accuracies with various probe/gallery sequence lengths in iLIDS-VID  In this section, we investigate the significance of the sequence length during testing
An experiment is conducted  to evaluate the ReID matching accuracy using various sequence lengths
Our proposed network shown in Figure N is  trained with the sequence length set to NN using the iLIDSVID dataset
During evaluation, the matching accuracy is  calculated using {NN, NN, NN, NNN} as lengths for the probe and gallery sequences
In the case when the probe or gallery  sequence is shorter than the test length, we use the entire sequence
 The matching accuracies for different sequence lengths  are summarized in Table N
The results clearly indicate  that the matching accuracies are improved as the sequence  length is increased
For instance, when we increase the sequence length from NN to NNN, the top rank matching accuracy is improved by NN%
This is an intuitive result since  combining the spatial and temporal features using multiple  images helps address the challenges associated with various  viewpoints and poses
 N.N.N Verification on Two Stream  To verify the usefulness of temporal information in ReID  task, we perform the experiments with the different settings  of the hyper-parameters (ωS , ωT )
This also can verify the  improvement gained by the use of a two stream CNN architecture
Note that ωS and ωT control the individual contributions of the SpatialNet and the TemporalNet, respectively
When ωT = 0, the contribution of TemporalNet be- comes totally 0 in training phase
This also applies to the  test phase in the same way based on the Equation N
 We then compare ReID matching accuracies for different  hyper-parameter settings such as spatial only case (ωT = 0, ωS = N,) and Both Stream cases when ωT is fixed to N while ωS is varying from N − N
As shown in Table N, using both stream cases have N-N% accuracy improvement  NNNN    ❳ ❳  ❳ ❳  ❳ ❳ ❳ ❳ ❳ ❳  Streams Rank N N N0 N0  ωS = N, ωT = 0 NN NN NN NN ωS = N, ωT = N NN NN NN NN ωS = N, ωT = N NN NN NN NN ωS = N, ωT = N NN NN NN NN  (a) PRIDN0NN  ❳ ❳  ❳ ❳  ❳ ❳ ❳ ❳ ❳ ❳  Streams Rank N N N0 N0  ωS = N, ωT = 0 NN N0 NN NN ωS = N, ωT = N NN NN NN NN ωS = N, ωT = N N0 NN NN NN ωS = N, ωT = N NN NN NN NN  (b) iLIDS-VID  Table N: Matching accuracies with different stream settings  ❳ ❳  ❳ ❳  ❳ ❳ ❳ ❳ ❳ ❳  Methods Rank N N N0 N0  Proposed (ωS = N) NN NN NN NN RNN-ReID N0 N0 NN NN  GOG + XQDA NN NN NN NN  GOG + KISSME NN N0 NN NN  LOMO + XQDA NN NN NN NN  LOMO + KISSME NN NN NN NN  ELF + XQDA NN NN NN NN  ELF + KISSME NN NN NN NN  (a) PRIDN0NN  ❳ ❳  ❳ ❳  ❳ ❳ ❳ ❳ ❳ ❳  Methods Rank N N N0 N0  Proposed (ωS = N) N0 NN NN NN RNN-ReID NN NN NN NN  GOG + XQDA NN NN NN N0  GOG + KISSME NN NN NN NN  LOMO + XQDA NN NN NN NN  LOMO + KISSME NN NN NN N0  ELF + XQDA NN NN N0 NN  ELF + KISSME NN N0 NN N0  (b) iLiDS-VID  Table N: Matching accuracies comparison with previous methods  N N0 NN N0  Rank  N0  N0  N0  N0  N0  N0  N0  N00  M a tc h in g A c c u r a c y (%  )  PRIDN0NN dataset  Proposed(ωs = N) RNN-ReID GOG + XQDA GOG + KISSME LOMO + XQDA LOMO + KISSME  (a) PRID N0NN  N N0 NN N0  Rank  N0  N0  N0  N0  N0  N0  N0  N00  M a tc h in g A c c u r a c y (%  )  iLIDS-VID dataset  Proposed(ωs = N) RNN-ReID GOG + XQDA GOG + KISSME LOMO + XQDA LOMO + KISSME  (b) iLIDS-VID  Figure N: CMC Curves for comparison  in PRIDN0NN and N-N% accuracy improvement in iLIDSVID
This result demonstrates that by having two separate  networks to represent the spatial and the temporal content,  each network is able to learn the best feature representation  and improves the ReID performance
In addition, based on  the results for ωS ≥ N cases, ReID performance improved in PRIDN0NN whereas it did not improve in iLIDS-VID for  ωS = N case
We thus conclude that the optimal relative contribution of the spatial and temporal features is data dependent
 N.N.N Comparisons  We compare the performance of our proposed method  against several of the best performing methods in a multishot ReID setting
We evaluate state-of-the-art metric learning methods (XQDA [N] and KISSME [N]) using state-ofthe-art feature extraction methods: LOMO [N], GOG [N]  and ELF [N]
Since we are evaluating multi-shot ReID  NNNN    methods, we extract the features for each image in the sequence and compute the average which is used by the metric learning methods
To our best knowledge, the combination of GOG and XQDA achieves state-of-the-art performance and the RNN-ReID method is the best performing  deep learning method [N0]
 The CMC curves are plotted in Figure Na and Nb and  the matching accuracies are summarized in Table Na and  Nb for the PRIDN0NN and the iLIDS-VID datasets, respectively
For the PRIDN0NN dataset, our proposed method  outperforms all the other methods
The top rank matching  accuracy is N% higher than the accuracy achieved by the GOG+XQDA method and N% higher than RNN-ReID
 For the iLIDS-VID dataset, the results show that our approach has comparable accuracy to the RNN-ReID method  and is N% higher than the accuracy achieved by the GOG+XQDA method as can be seen in Table Nb and Figure Nb
The top rank matching accuracy for the iLIDSVID dataset is NN% lower than the case for the PRIDN0NN dataset
We believe this mainly due to the cluttered  background and occlusions associated with the iLIDS-VID  dataset
To make our method more robust to these challenging conditions, we plan to incorporate semantic attributes
 N
Conclusion  In this paper, we proposed a person re-identification  method based on a two stream convolutional neural network  where each stream is a Siamese network
This architecture can learn spatial and temporal information separately  in a re-identification setting
Our proposed method is evaluated on the publicly available PRIDN0NN and iLIDS-VID  datasets
We demonstrate that combining the spatial and  temporal features using multiple images helps address the  challenges associated with viewpoint and pose invariants
 Our experimental results also demonstrate that by having  two separate networks to represent the spatial and the temporal content, each network is able to learn the best feature  representation and improves the ReID performance
In the  future, we want to incorporate semantic attributes using a  multi-stream approach to address the challenges associated  with occlusions and cluttered background
 References  [N] “Cisco visual networking index: Forecast and methodology,  N0NN/N0N0,” Cisco Systems Inc., April N0NN
 [N] A
Bedagkar-Gala and S
K
Shah, “A survey of approaches  and trends in person re-identification,” Image and Vision  Computing, vol
NN, no
N, pp
NN0–NNN, April N0NN
 [N] S
Gong, M
Cristani, S
Yan, and C
C
Loy, Person reidentification
London: Springer, N0NN
 [N] M
Hirzer, C
Beleznai, P
M
Roth, and H
Bischof, “Person  re-identification by descriptive and discriminative classification,” Proceedings of the Scandinavian Conference on Image  Analysis, pp
NN–N0N, May N0NN, Ystad, Sweden
 [N] T
Wang, S
Gong, X
Zhu, and S
Wang, “Person reidentification by video ranking,” Proceedings of the European Conference on Computer Vision, pp
NNN–N0N, September N0NN, Zurich, Switzerland
 [N] D
Gray and H
Tao, “Viewpoint invariant pedestrian recognition with an ensemble of localized features,” Proceedings  of the N0th European Conference on Computer Vision, pp
 NNN–NNN, October N00N, Marseille, France
 [N] T
Matsukawa, T
Okabe, E
Suzuki, and Y
Sato, “Hierarchical gaussian descriptor for person re-identification,” Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pp
NNNN–NNNN, June N0NN, Las Vegas,  NV
 [N] S
Liao, Y
Hu, X
Zhu, and S
Z
Li, “Person re-identification  by local maximal occurrence representation and metric  learning,” Proceedings of the IEEE Conference on Computer  Vision and Pattern Recognition, pp
NNNN–NN0N, June N0NN,  Boston, MA
 [N] M
Kostinger, M
Hirzer, P
Wohlhart, P
M
Roth, and  H
Bischof, “Large scale metric learning from equivalence  constraints,” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp
NNNN–NNNN, June  N0NN, Providence, RI
 [N0] F
Xiong, M
Gou, O
Camps, and M
Sznaier, “Person reidentification using kernel-based metric learning methods,”  Proceedings of the NNth European Conference on Computer  Vision, pp
N–NN, October N0NN, Zurich, Switzerland
 [NN] R
Layne, T
Hospedales, S
Gong, and Q
Mary, “Person reidentification by attributes,” Proceedings of the British Machine Vision Conference, vol
N, no
N, p
N, September N0NN,  Guildford, United Kingdom
 [NN] R
Layne, T
M
T
Hospedales, and S
Gong, “Towards person identification and re-identification with attributes,” Proceedings of the European Conference on Computer Vision,  pp
N0N–NNN, October N0NN, Berlin, Heidelberg
 [NN] S
Khamis, C
Kuo, V
Singh, V
Shet, and L
Davis, “Joint  learning for attribute-consistent person re-identification.”  Proceedings of the European Conference on Computer  Vision Workshops, pp
NNN–NNN, October N0NN, Zurich,  Switzerland
 [NN] A
Mignon and F
Jurie, “PCCA: A new approach for distance learning from sparse pairwise constraints,” Proceedings of the IEEE Conference on Computer Vision and Pattern  Recognition, pp
NNNN–NNNN, June N0NN, Providence, RI
 [NN] M
Hirzer, P
M
Roth, M
Köstinger, and H
Bischof, “Relaxed pairwise learned metric for person re-identification,”  Proceedings of the N0th European Conference on Computer  Vision, pp
NN0–NNN, October N0NN, Florence, Italy
 [NN] J
Davis, B
Kulis, P
Jain, S
Sra, and I
S
Dhillon,  “Information-theoretic metric learning,” Proceedings of the  NNth International Conference on Machine Learning, pp
 N0N–NNN, June N00N, Corvallis, OR
 NNN0    [NN] L
Zheng, Z
Bie, Y
Sun, J
Wang, C
Su, S
Wang, and  Q
Tian, “Mars: A video benchmark for large-scale person  re-identification,” Proceedings of the European Conference  on Computer Vision, October N0NN, Amsterdam, Netherlands
 [NN] J
You, A
Wu, X
Li, and W
Zheng, “Top-push video-based  person re-identification,” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp
NNNN–  NNNN, June N0NN, Las Vegas, NV
 [NN] S
Sarkar, P
J
Phillips, Z
Liu, I
R
Vega, P
Grother, and  K
W
Bowyer, “The humanid gait challenge problem: Data  sets, performance, and analysis,” IEEE transactions on pattern analysis and machine intelligence, vol
NN, no
N, pp
 NNN–NNN, N00N
 [N0] N
McLaughlin, J
Martinez, and P
Miller, “Recurrent convolutional network for video-based person re-identification,”  Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pp
NNNN–NNNN, June N0NN, Las Vegas, NV
 [NN] K
Simonyan and A
Zisserman, “Two-stream convolutional  networks for action recognition in videos,” Proceedings of  the Advances in Neural Information Processing Systems, pp
 NNN–NNN, December N0NN, Montreal, Canada
 [NN] J
Bromley, J
W
Bentz, L
Bottou, I
Guyon, Y
LeCun,  C
Moore, E
Säckinger, and R
Shah, “Signature verification using a “Siamese” time delay neural network,” International Journal of Pattern Recognition and Artificial Intelligence, vol
N, no
N, pp
NN–NN, January NNNN
 [NN] S
Liao, G
Zhao, V
Kellokumpu, M
Pietikäinen, and  Z
Stan, “Modeling pixel process with scale invariant local  patterns for background subtraction in complex scenes,” Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pp
NN0N–NN0N, June N0N0, San Francisco, CA
 [NN] Y
Li, Z
Wu, and R
J
Radke, “Multi-Shot Re-Identification  with Random-Projection-Based Random Forests,” Proceedings of the IEEE Winter Conference on Applications of Computer Vision, pp
NNN–NN0, January N0NN, Waikoloa, HI
 [NN] W
Zheng, S
Gong, and T
Xiang, “Reidentification by relative distance comparison,” IEEE Transactions on Pattern  Analysis and Machine Intelligence, vol
NN, no
N, pp
NNN–  NNN, March N0NN
 [NN] S
Karanam, Y
Li, and R
Radke, “Sparse re-id: Block sparsity for person re-identification,” Proceedings of the IEEE  Conference on Computer Vision and Pattern Recognition  Workshops, pp
NN–N0, June N0NN, Boston, MA
 [NN] K
Tahboub, B
Delgado, and E
J
Delp, “Person reidentification using a patch-based appearance model,” Proceedings of the IEEE Conference on Image Processing, pp
 NNN–NNN, September N0NN, Phoenix, AZ
 [NN] Y
Li, Z
Wu, S
Karanam, and R
Radke, “Multi-shot human re-identification using adaptive fisher discriminant analysis,” Proceedings of the British Machine Vision Conference,  September N0NN, Swansea, United Kingdom
 [NN] Z
Wu, Y
Li, and R
Radke, “Viewpoint invariant human  re-identification in camera networks using pose priors and  subject-discriminative features,” IEEE Transactions on Pattern Analysis and Machine Intelligence, vol
NN, no
N, pp
 N0NN–NN0N, September N0NN
 [N0] B
Delgado, K
Tahboub, and E
J
Delp, “Superpixels shape  analysis for carried object detection,” Proceedings of the  IEEE Winter Conference on Applications of Computer Vision  Workshops, pp
N–N, March N0NN, Lake Placid, NY
 [NN] A
Li, L
Liu, K
Wang, S
Liu, and S
Yan, “Clothing attributes assisted person re-identification,” IEEE Transactions  on Circuits and Systems for Video Technology, vol
NN, no
N,  pp
NNN–NNN, May N0NN
 [NN] Y
LeCun, Y
Bengio, and G
Hinton, “Deep learning,” Nature, vol
NNN, pp
NNN–NNN, May N0NN
 [NN] W
Li, R
Zhao, T
Xiao, and X
Wang, “Deepreid: Deep filter pairing neural network for person re-identification,” Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pp
NNN–NNN, June N0NN, Columbus,  OH
 [NN] E
Ahmed, M
Jones, and T
K
Marks, “An improved deep  learning architecture for person re-identification,” Proceedings of the IEEE Conference on Computer Vision and Pattern  Recognition, pp
NN0N–NNNN, June N0NN, Boston, MA
 [NN] D
Yi, Z
Lei, S
Liao, and S
Z
Li, “Deep metric learning for person re-identification,” Proceedings of the International Conference on Pattern Recognition, pp
NN–NN, August N0NN, Stockholm,Sweden
 [NN] D
Cheng, Y
Gong, S
Zhou, J
Wang, and N
Zheng, “Person  re-identification by multi-channel parts-based cnn with improved triplet loss function,” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp
 NNNN–NNNN, June N0NN, Las Vegas, NV
 [NN] F
Wang, W
Zuo, L
Lin, D
Zhang, and L
Zhang, “Joint  learning of single-image and cross-image representations for  person re-identification,” Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp
NNNN–  NNNN, June N0NN, Las Vegas, NV
 [NN] T
Xiao, H
Li, W
Ouyang, and X
Wang, “Learning deep  feature representations with domain guided dropout for person re-identification,” Proceedings of the IEEE Conference  on Computer Vision and Pattern Recognition, pp
NNNN–  NNNN, June N0NN, Las Vegas, NV
 [NN] B
D
Lucas, T
Kanade et al., “An iterative image registration technique with an application to stereo vision,” Proceedings of the International Joint Conference on Artificial Intelligence, vol
NN, pp
NNN–NNN, NNNN, Vancouver, Canada
 [N0] N
Srivastava, G
E
Hinton, A
Krizhevsky, I
Sutskever, and  R
Salakhutdinov, “Dropout: a simple way to prevent neural networks from overfitting,” Journal of Machine Learning  Research, vol
NN, no
N, pp
NNNN–NNNN, June N0NN
 [NN] Y
Sun, Y
Chen, X
Wang, and X
Tang, “Deep learning face  representation by joint identification-verification,” Proceedings of the Advances in Neural Information Processing Systems, pp
NNNN–NNNN, December N0NN, Montreal, Canada
 NNNNJoint Learning of Object and Action Detectors   Joint learning of object and action detectors  Vicky KalogeitonN,N Philippe WeinzaepfelN Vittorio FerrariN Cordelia SchmidN  Abstract  While most existing approaches for detection in videos  focus on objects or human actions separately, we aim at  jointly detecting objects performing actions, such as cat  eating or dog jumping
We introduce an end-to-end multitask objective that jointly learns object-action relationships
 We compare it with different training objectives, validate  its effectiveness for detecting objects-actions in videos, and  show that both tasks of object and action detection benefit from this joint learning
Moreover, the proposed architecture can be used for zero-shot learning of actions: our  multitask objective leverages the commonalities of an action performed by different objects, e.g
dog and cat jumping, enabling to detect actions of an object without training  with these object-actions pairs
In experiments on the AND  dataset [N0], we obtain state-of-the-art results on segmentation of object-action pairs
We finally apply our multitask  architecture to detect visual relationships between objects  in images of the VRD dataset [NN]
 N
Introduction  Video understanding has received increased attention  over the past decade leading to significant advances [NN,  NN]
However, most existing approaches focus either on  object recognition [NN, NN] or on human action recognition [NN, NN] separately
For both tasks, the community has  moved from small datasets [NN] to large ones with thousands of videos and hundreds of classes [N, NN], from controlled environments [NN] to videos in-the-wild [NN]
Given  the impressive success of Convolutional Neural Networks  (CNNs) for object detection [NN, NN], action localization  has benefited as well from this improvement
In particular, Faster R-CNN [NN] has been enhanced for videos by  using a two-stream variant [N, NN, NN], in which both appearance and motion are used as inputs
Modern approaches  first use such a detector to localize human actions in individual frames, and then either link them or track them over  time to create spatio-temporal detections [N, NN, NN]
These  NUniv
Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, NN000  Grenoble, France NUniversity of Edinburgh NNaver Labs Europe  adultrunning  dog-jumping  dog-running dog-jumping  cat   climbing  cat-   climbing  cat-   jumping  cat-none  cat-none  car-running car-rolling  car-rolling adult-running  adult-runningadult-none  Figure N: Detection examples of different object-action  pairs for the videos of the AND dataset [N0]
 methods focus exclusively on human action recognition
 While humans or actions alone are building blocks of  video understanding, the relationship between objects and  actions can yield a more complete interpretation
For instance, an autonomous car should not only be able to detect another car (object) or a human walking (action), but  also a dog running or a ball flying (object-action)
Other  applications include content-based retrieval, video captioning [NN, NN] and health-care robots, for instance helping  blind people crossing streets
Therefore, to better understand videos, we need to go beyond these two independent  tasks of object and human action recognition and understand the relationship between objects and actions
 In this paper, we propose to jointly detect object-action  instances in uncontrolled videos, e.g
cat eating, dog running or car rolling, see Figure N
We build an end-to-end  two stream network architecture for joint learning of objects  and actions
We cast this joint learning problem by leveraging a multitask objective
We compare our proposed end-toend multitask architecture with alternative ones (Figure N):  (i) treating every possible combination of actions and objects as a separate class (Cartesian) and (ii) considering a  hierarchy of objects-actions: the first level corresponds to  objects and the second one to the valid actions for each object (hierarchical)
We show that our method performs as  well as these two alternatives while (a) requiring fewer parameters and (b) enabling zero-shot learning of the actions  performed by a specific object, i.e., when training for an obNNNNN    RPN  concat  RoI pooling  +  RPN  +  Figure N: Overview of our end-to-end multitask network architecture for joint object-action detection in videos
Blue  color represents convolutional layers while green represents fully connected layers
The end-to-end training is done by  concatenating the fully connected layers from both streams
Here, pO and pA are the outputs of the two branches that predict  the object and action labels, resulting in the loss described in Equation N
 ject class alone without its actions, our multitask network is  able to predict actions for that object class by leveraging  actions performed by other objects
 Interestingly, our multitask objective not only allows to  effectively detect object-action pairs but also leads to performance improvements on each individual task (i.e., detection of either objects or actions)
This is because the  features learned for one task help learning the other one
 We compare to the state of the art for object-action detection on the Actor-Action (AND) dataset [N0] that contains segmentation annotation for object-action pairs
For  a direct comparison we transform our detections into pixelwise segmentation maps by using segmentation proposals [N0, N0]
Our approach significantly outperforms the  state of the art [N0, NN] on this dataset
We finally apply our multitask objective to detect object-action relationships in images on the Visual Relationship Detection (VRD)  dataset [NN]
 In summary, we make the following contributions:  • We propose an end-to-end multitask architecture for joint object-action detection
 • We show that this multitask objective can be leveraged for zero-shot learning of actions
 • We demonstrate the generalization of our multitask archi- tecture by applying it to (a) object-action semantic segmentation and (b) object-action relationships in images
 N
Related Work  Most existing approaches for detection in videos focus  either on object or on action localization
Over the past few  years, the methods range from low-level features [NN, N0,  NN, NN, NN, NN, NN], structured models that mine mid-level  elements [NN, NN] to parts [N, NN, NN] and attributes [NN]
 However, CNNs currently constitute the dominant approach  for large-scale and high-quality video detection
 Object or action detection
Recent work on object detection [N, NN, NN] has shown remarkable progress, mainly due  to the use of CNNs [N, NN, NN]
R-CNN [N] tackles object detection with CNNs by casting the task as a regionproposals classification problem
Faster R-CNN [NN] goes  a step further and generates proposals using a Region Proposal Network (RPN), which shares convolutional features  with the proposal classification branch
 These per-frame detectors are also used by state-of-theart human action localization methods [NN, NN] to obtain  spatial information; then the detections are linked across  time resulting in video-level localizations [N, NN]
To leverage video data, the detector operates on two streams [NN]:  RGB and optical flow
The two streams are trained separately and the scores are averaged at test time [N, NN, NN],  i.e., late fusion of scores
In contrast, our architecture is  a two-stream Faster R-CNN trained end-to-end based on a  fusion by a fully-connected layer that operates on concatenated features from both streams
Moreover, it is trained  with a multitask objective that allows us to detect objects  and actions jointly
 Joint modeling of objects and actions
Joint modeling of  objects and actions in videos has received little attention so  far
For the action localization task, some works [NN, NN]  propose to model the interactions of humans and objects
 However, the task we tackle in this paper is significantly  different as objects are not used for the actions, but they are  the actors
Bojanowski et al
[N] have considered the case  in which different entities can perform a set of actions, but  these entities correspond to names of different actors, i.e.,  to person identification
Closely related to object-action detection in videos is the work [NN, N0] on segmenting objectaction pairs
They use Conditional Random Fields at the  supervoxel level to output a semantic segmentation at the  pixel level
We show that our detections based on a multitask objective also improve the semantic segmentation performance by leveraging segmentation proposals [N0, N0]
 In images, however, object-action pairs have been modeled implicitly in the context of predicting sentences for images [NN, NN] and more recently by visual phrases [NN] and  NNNN    (a) Multitask (b) Cartesian (c) Hierarchical  Figure N: Illustration of the three different ways we consider for jointly learning objects and actions
The blue nodes represent  objects and the red ones action classes, while the yellow ones represent the background class
 relationships between objects [NN]
The task consists in detecting triplets of two objects and their relationship [NN, NN]
 Most approaches rely on object detectors
The relationship  label is predicted from the bounding box around the two  objects, and sometimes from additional modalities such as  languages or frequency priors in the training set
We show  that our multitask objective allows to predict the relationships between objects without (a) the need to see the whole  bounding box and (b) the need to include any priors
In particular, we transform each triplet into two pairs, each consisting of one of the two objects and the interaction
Then,  we train our network to detect bounding boxes around objects and also predict an interaction label
 Zero-shot learning
Most existing approaches for zeroshot learning of categories rely on attributes [N, N, NN]
Attributes have also been used for human actions [NN, NN]
 Liu et al
[NN] were the first to represent actions by sets  of attributes
They consider that each action class has an  intra-class variability, which they try to model by searching which attributes are relevant for each class
They apply zero-shot learning by manually labeling attributes for  all classes, including new ones without visual examples
In  contrast, our approach does not require any attribute labels
 N
End-to-end multitask network architecture  for joint learning of objects and actions  Given a video, we aim to detect the objects as well as the  actions they are performing
Let O (resp
A) be the set of objects (resp
actions) labels
Some combinations of actions  and objects may not be valid, e.g
car eating
We denote by  V ⊂ O ×A the set of valid object-action combinations
 N.N
End-to-end network architecture  We build an end-to-end two-stream multitask network  that proceeds at the frame level (Figure N)
As most stateof-the-art methods for object and action detection in videos,  we rely on Faster R-CNN [NN] and its two-stream variant  [N, NN, NN]
However, instead of training each stream separately, we propose to fuse both streams, thus enabling effective end-to-end learning
Our end-to-end network has  two streams: (a) appearance, which takes as input the RGB  data and (b) motion, which operates on the optical flow [N]
 Following [N], the input of the motion stream is a tensor  of three channels with the x and y coordinates of the flow  and its magnitude, represented as a N-channel image
A Region Proposal Network (RPN) extracts candidate bounding  boxes independently for each stream
We use the set union  of the two RPNs and we aggregate features for each candidate box with a Region-of-Interest (RoI) pooling layer  in each stream
After one fully-connected layer, the two  streams are concatenated and fed to another fully-connected  layer
The remaining network layers operate on the fused  stream, enabling end-to-end training
This allows us to  learn the most relevant features among all possible combinations of appearance and motion
In contrast, late fusion  of the softmax probabilities of the two streams [NN] assumes  that both appearance and motion are equally relevant for every class
As we show in Section N.N.N, our proposed fusion  significantly outperforms the late fusion
 Finally, we use a multitask loss for detecting objects, actions, and regressing the bounding box coordinates according to the object classes
The total loss L of the network is:  L = LRPNR + LRPNF + Lcls + Lreg , (N)  with LRPNR and LRPNF the losses of the RPN operating on the RGB and flow stream, respectively, Lcls the classifica- tion loss, i.e., for recognizing objects and actions, and Lreg the bounding box regression loss
 N.N
Joint learning of objects and actions  Given the candidate boxes, the network aims at jointly  predicting whether a box contains a particular object and  which action this object is performing
Let o (resp
a) be the  ground-truth object (resp
action) label of a region proposal  in the training set
To classify the boxes, we use a multitask architecture: one component predicts the object class,  and a second one predicts the action class, independently of  which object is performing it
Besides our proposed multitask architecture, we consider two alternatives to jointly  predict object-action pairs: Cartesian product and hierarchy  of classes
We now present details for these three objectives
We illustrate them in Figure N and summarize their  main differences in Table N
 Multitask
Our multitask architecture relies on a multitask loss, for classifying candidate boxes with both object  and action labels
The first branch predicts the object laNNNN    loss # outputs probability # params  Multitask − log pO(o)− log pA(a) |O|+ |A|+ N pO(o) · pA(a) 0.NM Cartesian − log pV(o, a) |V|+ N pV(o, a) NN.NM  Hierarchical − log pO(o)− log pAo(a) |O|+ |V|+ N pO(o) · pAo(a) NN.NM  Table N: Comparison of different losses for object-action  learning
We give the number of parameters in the classification layers from the VRD dataset [NN] where |O| = N00, |A| = NN0, |V| = NNNNN (Section N.N)
 bel
It is composed of a fully-connected layer that outputs  |O| + N scores (one per object class and another one for background) followed by softmax
Let pO be the output of  this branch
In the same way, pA denotes the output of the  second branch that predicts the action label, i.e., of dimension |A| + N
We use a log loss on both object and action classification:  LMultitaskcls = − log pO(o)− log pA(a) 
(N)  This version uses |O|+ |A|+ N outputs (Figure N (a))
For |O| = N00 and |A| = NN0 the number of parameters in the classification layers is 0.NM (VRD dataset [NN] used in Section N.N)
At test time, the probability of a box to be the  object-action instance (o, a) is given by pO(o) · pA(a)
 Cartesian product
Another solution is to consider each  object-action pair as a separate class, e.g
bird flying (Figure N (b))
In this case, there is only one branch for classification with |V| + N outputs
We denote as pV the output of this branch
The classification loss is:  LCartesiancls = − log pV(o, a) 
(N)  This version uses |V| + N outputs, which is in the or- der of |A| × |O|
For instance, for |V| = NNNNN (VRD dataset [NN]) the number of parameters in the classification  layer is NN.NM, i.e., N0× more than in the multitask (Ta- ble N)
This makes it less scalable than our multitask objective and does not allow sharing of action labels across  object classes, which is required for zero-short learning
In  the multitask case, samples of an object-action pair help  training the detector of this object, which in turn helps detecting it doing other actions; e.g
adult-running and adultwalking samples help improving the adult detector
In contrast, by using the Cartesian product, each training sample  helps training only one particular object-action detector
At  test time, the probability of being an object-action instance  (o, a) is given by pV(o, a)
 Hierarchy of classes
We also consider the set of valid  object-action classes as a hierarchy (Figure N (c))
The first  branch pO predicts the object
For each object o, any branch  pAo predicts the actions among the valid ones Ao for o
In this case, the classification loss is:  L Hierarchy cls  = − log pO(o)− log pAo(a) 
(N)  This version uses a total of |O|+N outputs for the first level and |V| for the second level, see Figure N (c)
For instance,  information / datasets AND YTO VID  objects X X X  actions X - # videos training NK N0N N,NK  test NNN NN NNN  # annotations training NNK NK N,NM  test NK N,NK NN0K  Table N: Overview of the video datasets we use
 for |O| = N00 and |V| = NNNNN the number of parameters in the classification layers is NN.NM, i.e., N0× more than in the multitask (Table N)
At test time, the probability of being  an object-action instance (o, a) is given by pO(o) · pAo(a)
 Per-object regression
In all cases, we refine the proposal output by the RPN using a per-object regression of  the bounding box coordinates
The RPN minimizes the geometric difference between the proposals and the groundtruth boxes
We follow [NN] and make the regression target  scale-invariant by normalizing it by the size of the proposal
 We denote by to,a the regression target for a proposal that  covers an object
By using a per-object regression, we obtain the following regression loss:  Lreg = Smooth-LN(uo − to,a) , (N)  with uo the output of the regression branch u corresponding  to object o, and:  Smooth-LN(x) =  {  0.NxN if |x| < N,  |x| − 0.N otherwise
(N)  N
Experimental Results  In this section, we study the impact of each of our contributions separately
We first examine joint detection of objects and actions (Section N.N) and zero-shot learning (Section N.N)
Next, we compare our proposed multitask architecture to the state of the art on semantic segmentation of  object-action pairs (Section N.N) and relationship detection  in images (Section N.N)
 Implementation details
Our framework is based on Faster  R-CNN [NN] using the VGG-NN [N0] as the underlying CNN  architecture
We initialize both streams using the standard  pre-training on ILSVRC N0NN [NN]
This is in line with [NN],  which shows that pre-training on ILSVRC N0NN instead of  UCF-N0N [NN] improves video classification accuracy
 N.N
Joint detection of objects and actions in videos  In this section, we evaluate our proposed end-to-end architecture for joint detection of object-action pairs
We start  by validating the effectiveness of our end-to-end network  (Section N.N.N) and then, we examine the joint learning with  the multitask objective (Section N.N.N)
 Video datasets
Table N shows some statistics of the  datasets we use
For object-action detection we use the  NNNN    input RoI Stream AND YTO VID  RGB Flow RGB Flow Fusion  X - X - - NN.N NN.N NN.N  - X - X - NN.0 NN.N N.0  X X X X late NN.N NN.N NN.N  X X X X ours NN.N NN.N NN.N  Table N: Impact of end-to-end training: mAP for object detection of different training scenarios on the AND, YTO and  VID datasets
 Actor-Action (AND) dataset [N0], which has sparse framelevel annotations for both objects and actions in videos
 To the best of our knowledge, it is the only video dataset  with bounding box and semantic segmentation annotations  for object-action pairs
It contains N objects (adult, baby, ball, bird, car, cat, and dog) performing N different actions (climb, crawl, eat, fly, jump, roll, run, walk) or no action
 We also use two video datasets for object detection: the  YouTube-Objects (YTO) dataset [NN, NN] and the ‘object detection in video’ (VID) track of the ILSVRC [N]
YTO consists of videos collected from YouTube with N0 classes of moving objects, e.g
aeroplane, car
VID contains bounding boxes for N0 object classes including rigid objects, e.g
motorcycle, watercraft, and animals, e.g
fox, monkey
 Protocol
We measure the detection performance using the  PASCAL VOC protocol [N]: a detection is correct if its  intersection-over-union overlap (IoU) with a ground-truth  box is greater than 0.N and its labels (object and action) are correctly predicted
The performance for a class is the average precision (AP), and the overall performance is captured  by the mean over all classes (mAP)
 N.N.N End-to-end architecture  We want to quantify the effectiveness of our proposed endto-end architecture that consists of two streams fused (a) at  the proposal (RoI) level and (b) at the feature level (Figure N)
We evaluate the impact of fusion for object detection  alone
We perform experiments on the three video datasets  (AND, YTO and VID)
Table N shows all the mAP results  for the different cases we consider
 Impact of RGB and Flow cues
To examine the impact  of the RGB and flow cues, we train each stream separately
 The first two rows of Table N show that the RGB stream  significantly outperforms the flow one
This is due to the  fact that the RGB stream is able to learn information about  how the objects look, which is a distinctive cue across different object classes
The flow stream performs worse than  the RGB one in general, and is particularly poor on the VID  dataset
This is because most objects in VID move only  slightly, or their motion is not discriminative for the class
 Impact of end-to-end training
Our proposed fusion of  the two streams enables end-to-end training
We examine  the impact by comparing our proposed fusion of streams  training test on  objects actions objects + actions  objects NN.N - actions - NN.N Baseline - - NN.N  Cartesian NN.N N0.N NN.N  Hierarchical NN.N NN.N NN.N  Multitask NN.N N0.0 NN.N  Table N: mAP of six different models when training with  objects (first row), actions (second row), when multiplying  their scores (third row) or when jointly training with objects  and actions (last three rows) on AND
 with late fusion of scores [N, NN]
In the latter, i.e., late  fusion of scores, we train the two-stream network fusing  only the region-proposal layers and then average the classification scores of each stream as [N, NN]
Results in Table N show that, for all video datasets, using late score fusion reduces the detection performance compared to using  the RGB stream alone
Interestingly, this is opposite of the  findings in human action localization [N, NN], where performance increases due to the the significance of motion  cues for actions
This shows that the two-stream architecture cannot be used as it is for object detection in videos and  highlights a clear difference between object and human action detection
In contrast, on all object detection datasets,  our proposed fusion outperforms the other cases: it leads to  an increment over the late score fusion of approximately N- N%
This shows that the network successfully learns when to leverage motion information and more importantly, how  to jointly learn features coming from the two stream
 N.N.N Multitask learning  In this section, we evaluate our proposed multitask learning  of objects and actions
We start by evaluating the performance only on object or on action detection
Therefore, we  train and test our network with only object or only action labels (first two rows of Table N)
We also compute a baseline  (third row of Table N) for object-action detection in which  we combine the object and the action detector trained separately
More precisely, for each object detection, we obtain  object-action scores by multiplying the object scores with  the action scores from the most overlapping action box
 Table N also reports the results of our proposed multitask  architecture trained with objects and actions from the AND  dataset
The most interesting finding is that our multitask  training improves the performance on each task separately  (Table N objects, actions and multitask rows)
In particular, when testing just on objects (NN.N%) or just on actions (N0.0%), our joint training outperforms training alone with objects (NN.N%) or with actions (NN.N%)
The reasons are that the multitask network is (a) better able to generalize,  (b) less prone to overfit to the training samples and (c) benefits from sharing examples across classes
 NNNN    climbing crawling eating flying jumping rolling running walking none avg
 adult N.0 (NN.N) N.N (NN.N) NN.N (N0.0) - NN.0 (NN.N) NN.N (N0.N) N.N (NN.N) NN.N (NN.N) NN.N (NN.N) NN.N (N0.N)  baby NN.N (NN.N) NN.N (NN.N) - - - NN.N (NN.N) - NN.N (NN.N) N.N (NN.N) NN.N (NN.N)  ball - - - 0.N (NN.N) N.N (NN.N) N0.N (NN.N) - - N.0 (NN.N) N.N (NN.0)  bird NN.N (NN.N) - NN.N (NN.0) N.0 (NN.N) N.N (NN.N) NN.N (N0.N) - N.N (NN.0) N.N ( N.N) NN.N (NN.N)  car - - - N.N (NN.N) N.N (N0.N) NN.N (NN.N) N.N (NN.N) - N.N (NN.N) N0.N (NN.N)  cat NN.N (N0.N) - NN.N (NN.N) - N.N (NN.N) NN.N (NN.N) N.0 (NN.0) NN.N (NN.N) N.N ( N.N) N0.N (NN.N)  dog - N.N (NN.N) NN.N (NN.N) - NN.N (NN.N) NN.N (NN.N) N0.N (NN.N) NN.0 (NN.N) N.N ( N.N) N0.N (NN.N)  Table N: Evaluation of zero-shot learning for object-action pairs on AND
For each object, we report the AP when excluding  all actions of this object at training
The numbers in parenthesis indicate the AP when training with all object-action pairs
 We also consider two alternative ways to jointly detect  objects and actions (Section N.N and Figure N): (a) Cartesian  product of object-action labels and (b) hierarchy of objectaction classes
Table N (Cartesian and hierarchical) reports  the results when we train these two networks on the AND  dataset
We observe that they both perform similarly to our  multitask network
The Cartesian and hierarchical networks  have the advantage of being able to distinguish different  ways objects perform each action (Table N)
 Discussion
In practice there are similarities in the way different objects perform the same action (e.g
dog and cat  eating) and in the way the same object performs different  actions (e.g
dog walking and running)
Thus, our multitask objective allows the network to exploit the commonality among the two tasks, and hence, what is learned for  each task facilitates the learning of the other
In a nutshell,  our multitask architecture is a simpler model, able to reach  the same performance as the alternative architectures while  requiring much fewer parameters (Table N # params) and  enabling zero-shot learning (Section N.N)
For instance, in  Section N.N we clearly show the benefit of our multitask architecture compared to the Cartesian and hierarchical architectures for a large number of objects and actions due to its  lower number of parameters
 Note that both losses (object and action) contribute  equally to the overall loss (Equation N), as they are of the  same type (softmax), and the tasks they address are of the  same difficulty
To validate this, we vary the weight of the  action loss over 0.N, N, N and observe insignificant variations (< 0.N%) in the object-action mAP on AND
 N.N
Zero-shot learning of actions  An important advantage of our end-to-end multitask architecture is its capability of predicting actions for an object  without having trained for these particular object-actions  combinations
To validate this intuition, we experiment on  the AND dataset (Table N), which contains annotations for  N objects performing N different actions in videos
We train the network seven times, where each time we remove for  one object o′ all its action labels
For instance, we remove  all action labels for the object cat, but keep the cat examples  for training the object detector
Equation N is replaced by:  LMultitaskcls zero-shot = − log pO(o)− [o ′ N= o] log pA(a) 
(N)  Note that the object classifier is not changed, while the action classifier is learned only on the actions performed by  the objects different from o′
This approach to zero-shot  learning does not assume any prior knowledge such as attributes of the unseen classes [NN]
 We report the results of zero-shot learning in Table N
 We also report the AP when training with all object-action  pairs
The results show that our network is able to infer information about actions not seen at training time for a given  object
We observe that there are some object-action pair  for which the AP is only slightly decreased, e.g
cat rolling  or dog eating
This is because these objects share commonalities with others, e.g
cat and dog eating
In contrast,  we observe poor performance for objects like ball which  do not share similarities with other objects of the dataset
 For object classes that share similarities in actions, such as  cat and dog, our multitask architecture outperforms chance  level classification of unknown actions by a large margin  (+NN%), while for classes that do not share commonalities with other classes, like adult the gain is smaller (+N%)
 N.N
Object-action segmentation  AND comes with annotations for semantic segmentation  of object-action pairs
In this section, we extend our bounding box detections to pixelwise segmentation and we compare our results to the state of the art
 Metrics
Following [N0], we measure class-average pixel  accuracy and global pixel accuracy
Accuracy is the percentage of pixels for which the label is correctly predicted, either over all pixels (global) or first computed for  each class separately and then averaged over classes (classaverage)
We also evaluate our segmentations using mIoU,  i.e., the IoU between the ground-truth segmentation and  output segmentation averaged over all classes
mIoU is better suited as it is not biased towards background which is  the most present class and it penalizes errors when too many  pixels are set to a particular label instead of background
 Setup
Our multitask model predicts bounding boxes for  each object-action pair
We extend our detections to pixelwise segmentations of object-action pairs by using segmentation proposals from either (a) the recently proposed  SharpMask [N0] or (b) the hierarchical video segmentation  method GBH [N0], which is the one used by the state-of-theNNNN    methods object action object + action  ave glo mIoU ave glo mIoU ave glo mIoU  Trilayer [N0] NN.N NN.N - NN.0 NN.N - NN.N NN.N GPM (TSP) [NN] NN.N NN.N NN.N N0.N NN.N NN.0 NN.N NN.N NN.N  GPM (GBH) [NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours (GBH) NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N  Ours (SharpMask) NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  Table N: Comparison to the state of the art for object, action and object-action segmentation on AND using class-average  pixel accuracy (ave), global pixel accuracy (glo) and mean Intersection over Union (mIoU) metrics
 art GPM method [NN]
For each frame, we first apply nonmaximum suppression on the detections that have a score  greater than 0.N
Then, for each detection, we select the segmentation proposal that overlaps the most with it (according to IoU)
If there is no such proposal, we directly  use the rectangular detection itself as a segmentation mask
 While our setup is simple, it serves as a baseline to evaluate  our detections for semantic segmentation
 Results
The first three rows of Figure N show correctly  labeled and segmented object-action pairs
We observe  that our segmentation results are accurate, even in difficult  cases, such as small objects (e.g
birds) or cluttered scenes  (e.g
adults running)
The two last rows show typical failure  cases
In the fourth row, the action label of one adult is incorrect and there are some detections considered as wrong  due to missing annotations
In the last row we miss the adult  for which only one arm is visible
 Table N provides a quantitative comparison between our  results and the state of the art [NN, N0] on AND
When using SharpMask, we outperform the previous state of the art  for all metrics and all tasks, except for average accuracy on  action segmentation, where we match [NN]
Our improvements are particularly significant for object segmentation  (+NN% class-average accuracy, +NN% mIoU) and joint ob- ject and action segmentation (more than +N% on all met- rics)
Note that we do not use any training segmentation  from the AND dataset (SharpMask is pre-trained on MS  COCO [NN])
Furthermore, we observe that even when using the same underlying method (GBH [N0]), we perform on  par or better than [NN, N0] in all metric-task combinations
 N.N
Relationship detection of objects and actions  In this section we use only images, and therefore we use  only the RGB stream as there is no flow for images
We  apply our model to visual relationship detection, where we  detect relationships between objects, defined as triples: objectN - interaction - objectN
To do so, we transform each  triplet into two pairs, each consisting of an object and an  interaction and use them to train our multitask architecture
 Dataset and protocol
We employ the Visual Relationship  Detection (VRD) dataset [NN] that examines object relationships
It contains Nk training and Nk test images with NNk relationships between objects, such as person kick ball, perframe ground-truth ours  birds-flying birds-flying  dogrunning balljumping  dogrunning balljumping  dog-crawling  adult-walking  dog-crawling  adult-walking  adultnone  adultsrunning  adultsnone  adultwalking  adultsrunning  adultnone  bird-rolling bird-rolling  Figure N: Examples of semantic segmentation with (from  left to right): the frame, the ground-truth and the segmentation output obtained when combining our approach with  proposals from SharpMask [N0]
The colors of the segmentations represent an object-action pair
Note that we do not  use any object-action segmentation at training time
 son wear shirt, motorcycle has wheel
There are N00 differ- ent objects and N0 interaction types
 We consider here visual phrase detection [NN], where the  goal is to output a triplet objectN - interaction - objectN and  localize it with one box having an IoU over 0.N with the ground-truth box
We also evaluate relationship detection:  the task consists in detecting a triplet objectN - interaction  - objectN with two bounding boxes on objectN and objectN,  both having an IoU over 0.N with their ground-truth boxes
For evaluation, the metric used is recall @N00 and recall @N0 (denoted as R@N ) and not mAP, as not all possible interactions are annotated in the test images
In each  image, the top N detections are kept and recall is measured
 Model
To detect relationships using our multitask architecture, we transform each objectN-interaction-objectN triplet  into two pairs, each consisting of an object and an interaction label
More precisely, we double the set of all possible  interactions, by including their passive forms
For example,  the triplet human kicks ball becomes two pairs: (i) one with  object human and action kick, and (ii) another pair with obNNNN    Modality Method Phrase detection Relationship detection  R@N00 R@N0 R@N00 R@N0  V  VP [NN] 0.0N 0.0N - Joint CNN [NN] 0.0N 0.0N 0.0N 0.0N  VRD [NN] N.N N.N N.N N.N  Baseline NN.N N.N N.N N.N  Ours Multitask NN.N NN.N NN.N N.N  V+L+F VRD [NN] NN.0 NN.N NN.N NN.N  Table N: Comparison to different architectures and to the  state-of-the-art visual relationships on the VRD dataset for  phrases and relationship detection
We report R@N00 and  R@N0 for methods using only visual cue (V) or also language and frequency priors (V+L+F)
 ject ball and action k̃ick = being kicked
In that way, our  training set consists of N00 object classes performing NN0 different actions
Note here that the possible number of outputs is N00 + NN0 + N for our multitask objective
 At test time, we keep all detection with score over 0.N and apply non-maximum suppression
For each pair of object detections, we score each possible interaction using the  multiplication of the object scores and the interaction score
 The interaction score is defined as the combination of the  score of an interaction from the first object and its passive  form from the second object, i.e., the interaction score of  kick in human kicks ball includes both scores of kick for the  human and being kicked for the ball
 Results
Table N reports the R@N00 and R@N0 for the  two tasks we examine, i.e., phrase and relationship detection
We outperform all previous state-of-the-art results on  both tasks and at both operating points, when comparing to  methods based purely on the images ([NN, NN, NN])
Moreover, our results are only a little worse than those of [NN],  where they enhance their visual model with some frequency  prior as well as language priors by leveraging the semantic  similarities of relationships in term of words
In particular, we perform on par on phrase detection (+N% at R@N00 and −N% at R@N0)
Note how our method features a clear increment from R@N0 to R@N00, which shows its potential to correctly detect interactions that may be lower in the  recall list
Hence, including some language or spatial priors could significantly increase our performance
Figure N  shows some qualitative results
 Benefits of the multitask training
We compare our multitask architecture with a baseline approach where we multiply the scores of two separate networks, one trained on objects and another one trained on interactions
Table N shows  that our multitask architecture outperforms this alternative  (‘Baseline’ row)
This comparison highlights the benefit of  joint training compared to training for each task separately
 We have also evaluated the Cartesian and hierarchical combination of objects and actions (Section N.N) and found that  they perform poorly (for both R@N00 is around 0%)
This can be explained by lack of training data necessary to determine the large number of parameters (NNM in Table N)
 motorcycle has wheel wheel on   motorcycle  building behind car  motorcycle   has helmet  motorcycle   has helmet laptop on bed  person on bed  bed under person  clock by bed  person use camera  phone by bed  Figure N: Qualitative object-action relationships on the  VRD dataset: (yellow): our correct boxes with their green  label, (red): missed interactions for N00 retrieved boxes
 Modality Method Phrase detection Relationship detection  R@N00 R@N0 R@N00 R@N0  V  VRD [NN] N.N 0.N 0.N 0.N  Baseline N.N N.N N.N N.N  Ours Multitask N.N N.N N.N N.N  V+L+F VRD [NN] N.N N.N N.N N.N  Table N: Comparison to the state-of-the-art zero-shot detection of visual relationships on the VRD dataset
We report  R@N00 and R@N0 for methods using only visual cue (V) or  also language and frequency priors (V+L+F)
 Zero-shot learning
The test set of the VRD dataset contains N.Nk triplets that never occur in the training set
Our architecture allows zero-shot learning and we report the results on these triplets in Table N
Our method outperforms  the state-of-the-art method [NN] when using only the visual  modality (no language or frequency prior)
Additionally,  for phrase detection we detect unseen-at-training interactions better than [NN], even when they also use language and  frequency priors
Finally, our multitask architecture outperforms the baseline by a significant margin, highlighting the  benefit of joint training compared to separate one
 N
Conclusions  Most state-of-the-art works for video detection aim at localizing either objects or actions
Instead, we jointly detect  objects and actions in uncontrolled video scenes
To this  end, we propose an end-to-end network built upon Faster  R-CNN [NN]
The key point is that our network operates  with a multitask objective
We show that this joint training: (a) outperforms training alone with objects or with actions, as the network can generalize better, is less prone to  overfit and benefits from sharing statistical strength between  classes, (b) performs as well as other variants while requiring fewer parameters and (c) allows zero-shot learning of  actions performed by an object, for which no action labels  are present at training time
Our network can also be applied to different tasks including semantic segmentation and  visual relationship detection
 Acknowledgments
This work was supported in part by  the ERC grants ALLEGRO and VisCul, the MSR-Inria joint  project, a Google research award, a Facebook gift, an Intel  gift and an Amazon research award
We gratefully acknowledge the support of NVIDIA with the donation of GPUs  used for this research
 NNN0    References  [N] Imagenet large scale visual recognition challenge (ilsvrc)
 http://www.image-net.org/challenges/LSVRC/N0NN, N0NN
 N, N  [N] P
Bojanowski, F
Bach, I
Laptev, J
Ponce, C
Schmid, and  J
Sivic
Finding actors and actions in movies
In ICCV,  N0NN
N  [N] T
Brox, A
Bruhn, N
Papenberg, and J
Weickert
High accuracy optical flow estimation based on a theory for warping
 In ECCV, N00N
N  [N] M
Bucher, S
Herbin, and F
Jurie
Improving semantic embedding consistency by metric learning for zero-shot classiffication
In ECCV, N0NN
N  [N] V
Escorcia, J
C
Niebles, and B
Ghanem
On the relationship between visual attributes and convolutional networks
 In CVPR, N0NN
N  [N] M
Everingham, L
Van Gool, C
Williams, J
Winn, and  A
Zisserman
The PASCAL Visual Object Classes Challenge N00N Results, N00N
N  [N] P
Felzenszwalb, R
Girshick, D
McAllester, and D
Ramanan
Object detection with discriminatively trained part  based models
IEEE Trans
on PAMI, N0N0
N  [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
N  [N] G
Gkioxari and J
Malik
Finding action tubes
In CVPR,  N0NN
N, N, N, N  [N0] M
Grundmann, V
Kwatra, M
Han, and I
Essa
Efficient hierarchical graph-based video segmentation
In CVPR, N0N0
 N, N, N  [NN] A
Gupta, A
Kembhavi, and L
S
Davis
Observing humanobject interactions: Using spatial and functional compatibility for recognition
IEEE Trans
on PAMI, N00N
N  [NN] F
C
Heilbron, V
Escorcia, B
Ghanem, and J
C
Niebles
 Activitynet: A large-scale video benchmark for human activity understanding
In CVPR, N0NN
N  [NN] V
Kalogeiton, C
Schmid, and V
Ferrari
Analysing domain  shift factors between videos and images for object detection
 In IEEE Trans
on PAMI, N0NN
N  [NN] K
Kang, W
Ouyang, H
Li, and X
Wang
Object detection  from video tubelets with convolutional neural networks
In  CVPR, N0NN
N, N  [NN] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In CVPR, N0NN
N  [NN] A
Klaser, M
Marszalek, and C
Schmid
A Spatio-Temporal  Descriptor Based on ND-Gradients
In BMVC, N00N
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, N0NN
N, N  [NN] C
H
Lampert, H
Nickisch, and S
Harmeling
Attributebased classification for zero-shot visual object categorization
IEEE Trans
on PAMI, N0NN
N  [NN] T
Lan, Y
Zhu, A
Roshan Zamir, and S
Savarese
Action recognition by hierarchical mid-level action elements
 In ICCV, N0NN
N  [N0] I
Laptev
On space-time interest points
IJCV, N00N
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV, N0NN
N  [NN] J
Liu, B
Kuipers, and S
Savarese
Recognizing human actions by attributes
In CVPR, N0NN
N, N, N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.-Y
 Fu, and A
C
Berg
SSD: Single shot multibox detector
In  ECCV, N0NN
N, N  [NN] C
Lu, R
Krishna, M
Bernstein, and L
Fei-Fei
Visual relationship detection with language priors
In ECCV, N0NN
N,  N, N, N, N, N  [NN] S
Ma, J
Zhang, N
Ikizler-Cinbis, and S
Sclaroff
Action  recognition and localization by hierarchical space-time segments
In ICCV, N0NN
N  [NN] T
Malisiewicz, A
Gupta, and A
Efros
Ensemble of  exemplar-svms for object detection and beyond
In ICCV,  N0NN
N  [NN] J
Mao, W
Xu, Y
Yang, J
Wang, Z
Huang, and A
Yuille
 Deep captioning with multimodal recurrent neural networks  (m-rnn)
In ICLR, N0NN
N  [NN] M
Pandey and S
Lazebnik
Scene recognition and weakly  supervised object localization with deformable part-based  models
In ICCV, N0NN
N  [NN] X
Peng and C
Schmid
Multi-region two-stream r-cnn for  action detection
In ECCV, N0NN
N, N, N, N  [N0] P
O
Pinheiro, T.-Y
Lin, R
Collobert, and P
Dollàr
Learning to refine object segments
In ECCV, N0NN
N, N, N  [NN] A
Prest, V
Ferrari, and C
Schmid
Explicit modeling of  human-object interactions in realistic videos
IEEE Trans
 on PAMI, N0NN
N  [NN] A
Prest, C
Leistner, J
Civera, C
Schmid, and V
Ferrari
Learning object class detectors from weakly annotated  video
In CVPR, N0NN
N, N  [NN] M
Raptis, I
Kokkinos, and S
Soatto
Discovering discriminative action parts from mid-level video representations
In  CVPR, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN: Towards real-time object detection with region proposal networks
In NIPS, N0NN
N, N, N, N, N  [NN] M
D
Rodriguez, J
Ahmed, and M
Shah
Action mach:  a spatio-temporal maximum average correlation height filter  for action recognition
In CVPR, N00N
N  [NN] M
A
Sadeghi and A
Farhadi
Recognition using visual  phrases
In CVPR, N0NN
N, N, N, N  [NN] S
Saha, G
Singh, M
Sapienza, P
H
Torr, and F
Cuzzolin
 Deep learning for detecting multiple space-time action tubes  in videos
In BMVC, N0NN
N, N, N  [NN] C
Schüldt, I
Laptev, and B
Caputo
Recognizing human  actions: a local svm approach
In Proc
ICPR, N00N
N  [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In NIPS, N0NN
N,  N, N, N  [N0] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 N  NNNN    [NN] K
Soomro, A
R
Zamir, and M
Shah
UCFN0N: A Dataset  of N0N Human Actions Classes From Videos in The Wild
In  CRCV-TR-NN-0N, N0NN
N  [NN] J
R
Uijlings, K
E
van de Sande, T
Gevers, and A
W
 Smeulders
Selective search for object recognition
IJCV,  N0NN
N  [NN] S
Venugopalan, M
Rohrbach, J
Donahue, R
Mooney,  T
Darrell, and K
Saenko
Sequence to sequence-video to  text
In ICCV, N0NN
N  [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In CVPR, N0NN
N  [NN] P
Viola and M
Jones
Rapid object detection using a boosted  cascade of simple features
In CVPR, N00N
N  [NN] H
Wang, D
Oneata, J
Verbeek, and C
Schmid
A robust and efficient video representation for action recognition
 IJCV, N0NN
N  [NN] L
Wang, Y
Xiong, Z
Wang, Y
Qiao, D
Lin, X
Tang, and  L
Van Gool
Temporal segment networks: towards good  practices for deep action recognition
In ECCV, N0NN
N  [NN] P
Weinzaepfel, Z
Harchaoui, and C
Schmid
Learning to  track for spatio-temporal action localization
In ICCV, N0NN
 N, N  [NN] C
Xu and J
J
Corso
Actor-action semantic segmentation  with grouping-process models
In CVPR, N0NN
N, N  [N0] C
Xu, S.-H
Hsieh, C
Xiong, and J
J
Corso
Can humans  fly? Action understanding with multiple classes of actors
In  CVPR, N0NN
N, N, N, N, N  [NN] B
Yao, X
Jiang, A
Khosla, A
L
Lin, L
Guibas, and L
FeiFei
Human action recognition by learning bases of action  attributes and parts
In ICCV, N0NN
N  [NN] L
Yao, A
Torabi, K
Cho, N
Ballas, C
Pal, H
Larochelle,  and A
Courville
Describing videos by exploiting temporal  structure
In ICCV, N0NN
N  NNNNScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond   ScaleNet: Guiding Object Proposal Generation in Supermarkets and Beyond  Siyuan QiaoN Wei ShenN,N Weichao QiuN Chenxi LiuN Alan YuilleN  Johns Hopkins UniversityN Shanghai UniversityN  {siyuan.qiao, wqiuN, cxliu, alan.yuille}@jhu.edu wei.shen@t.shu.edu.cn  Abstract  Motivated by product detection in supermarkets, this paper studies the problem of object proposal generation in  supermarket images and other natural images
We argue  that estimation of object scales in images is helpful for  generating object proposals, especially for supermarket images where object scales are usually within a small range
 Therefore, we propose to estimate object scales of images  before generating object proposals
The proposed method  for predicting object scales is called ScaleNet
To validate  the effectiveness of ScaleNet, we build three supermarket  datasets, two of which are real-world datasets used for testing and the other one is a synthetic dataset used for training
 In short, we extend the previous state-of-the-art object proposal methods by adding a scale prediction phase
The resulted method outperforms the previous state-of-the-art on  the supermarket datasets by a large margin
We also show  that the approach works for object proposal on other natural images and it outperforms the previous state-of-the-art  object proposal methods on the MS COCO dataset
The supermarket datasets, the virtual supermarkets, and the tools  for creating more synthetic datasets will be made public
 N
Introduction  There is an exciting trend in developing intelligent shopping systems to reduce human intervention and bring convenience to human’s life, e.g., Amazon GoN system, which  makes checkout-free shopping experience possible in physical supermarkets
Another way to enhance the shopping experience in supermarkets is setting customer free from finding and fetching products they want to buy, which drives the  demand to develop shopping navigation robots
This kind  of robots can also help visually impaired people shop in supermarkets
The vision system of such a robot should have  the abilities to address two problems sequentially
The first  is generating object proposals for products in images captured by the equipped camera (Fig
N), and the second is  Nhttps://www.amazon.com/b?node=NN00NNNN0NN  Figure N: Example Object Annotations in the Supermarket  Datasets (Left) and the MS COCO Datasets [NN] (Right)
Yellow:  object scale is between N0% and N0% of the image scale; red: between N0% and N0%; green: less than N0%
The ratio is calculated  as the maximum of the width and the height of the object divided  by the maximum of the width and the height of the image
No  other object scales appear in the examples
 identifying each product proposal
In this paper, we focus  on the first problem
 There are many object proposal methods for general natural images [NN, NN, NN, NN]
However, scenes of supermarkets are usually very crowded, e.g., one image taken  in supermarkets could have over N0 products
More chal- lengingly, products of the same brands and categories are  usually placed together, i.e., the appearance similarities between adjacent products are often high, making the boundaries between them hard to detect
Consequently, the current object proposal detection methods, including superpixel grouping based [N, N0, NN], edge or gradient computation based [N, NN] and saliency and attention detection  based [N, N, N, NN, NN], are less effective and require a large  number of proposals to achieve reasonable recall rates
 However, we observe that the products in supermarkets  typically occur at a limited range of scales in the image
 To demonstrate this, we plot the distribution of the number  of object scales in real-world supermarkets (Fig
N)
This  suggests a strategy where we estimate object scales and use  them to guide proposals rather than exhaustive searching on  all scales
The same strategy of reducing search space of  scales is also applicable to other natural images in the MS  COCO [NN], and it becomes very effective especially for  those that have sparse object scales (Fig
N), for which an  effective scale prediction can reduce the search space and  NNNN    0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  N N N N N N N N N N0  The Number of Different Object Scale Ratios Appeared in One Image  COCO  Supermarket  Figure N: Distributions of the Number of Different Object Scale  Ratios of One Image on the MS COCO [NN] Dataset and the RealWorld Supermarket Dataset
The ratio of the object size (the maximum of width and height) to the image size (the maximum of  width and height) is partitioned evenly to N0 bins from 0 to N
We  count the number of different scale ratios appeared in one image on  the datasets
The object scales of supermarket images are sparser  than that of images in the MS COCO
Since NN.N% supermarket  images have neighboring non-zero bins, the scale distributions are  within a small range compared to the entire scale space
Moreover,  a reasonable number of images in the MS COCO dataset also have  fairly sparse object sizes
 eliminate false positives at improper scales
 More precisely, we propose a scale-aware object proposal detection framework to address the problem (Fig
N)
 Our framework consists of two sequential parts
The first  is a scale estimation network, called ScaleNet, which predicts the scale distribution of the objects appeared in an image
The second is an object proposal detection network,  which performs detection on re-scaled images according to  the estimated scales
For the second part, we use a deep  learning based object proposal detection method SharpMask [NN], which predicts objectness confidence scores and  object masks at each location of the input image at several  pre-defined scales
Since this method can output dense object masks, it fits the supermarket images well
 We evaluate the proposed framework on general natural images and supermarket images
To evaluate our framework on natural images, we test it on the MS COCO dataset
 For the supermarket images, we collect two real-world supermarket datasets, in which the bounding boxes of products are annotated by humans
The first dataset is called  Real-Far, which is composed of N0NN products labeled and has less variation in object scales
The second dataset is  called Real-Near, which has NNNN products labeled with more variation in scales
The objective of collecting two  datasets is to evaluate and compare the performances in different settings of object scales
 Since human labeling for crowded scenes is very timeconsuming and expensive, to generate enough training data,  we use a Computer Graphics technique [NN] to generate a  synthetic dataset, which includes NNNNNN objects labeled for training and N0NNN objects for validation
The synthetic  dataset is used for training and validation and the two realworld datasets are used only for testing
 To summarize, the contributions of this paper include  • A scale estimation method ScaleNet to predict the object scales of an image
 • An object proposal framework based on ScaleNet that outperforms the previous state-of-the-arts on the supermarket datasets and MS COCO
 • Two real-world supermarket datasets and a synthetic dataset, where the model trained only on synthetic dataset  transfers well to the real-world datasets
The datasets and  the tools will be made public
 N
Related Work  In this section, we review the related work in the research  topics including object proposal methods and virtual environment constructions
 N.N
Object proposal  The previous work usually falls into two categories: one  is bounding box based, and the other is object mask based
 Both can generate object proposals in the form of bounding  box
In bounding box based methods such as Bing [N] and  EdgeBox [NN], local features such as edges and gradients  are used for assessing objectness of certain regions
Following the success of CNNs in image classification [NN, NN, NN],  DeepBox [NN] re-ranks the object proposals generated by  EdgeBox [NN], and DeepProposal [NN] generates object proposal by an inverse cascade from the final to the initial layer  of the CNN
MultiBox [N0] and SSD [NN] compute object  regions by bounding box regression based on CNN feature  maps directly
In SSD, YOLO [NN] and RPN [NN], anchor  bounding boxes are used to regress bounding boxes
Jie et  al
[NN] proposed scale-aware pixel-wise proposal framework to handle objects of different scales separately
Although some methods use multi-scales to generate proposals, they do not explicitly estimate the object scales
 Object mask based methods propose object bounding  boxes by segmenting the objects of interest from the corresponding background at pixel or region level
This type  of methods can detect objects by seed segmentation such  as GOP [N0] and Learning to Propose Objects [NN]
They  can also group over-segmented regions to propose objects  such as Selective Search [NN] and MCG [N]
More recently, DeepMask [NN] assesses objectness and predicts object masks in a sliding window fashion based on CNN features, which achieved the state-of-the-art performance on  the PASCAL VOC [NN] and the MS COCO [NN] datasets
 SharpMask [NN] further refines the mask prediction of  DeepMask by adding top-down refinement connection
Our  method extends the previous state-of-the-art SharpMask by  adding object scale prediction and outperforms them on the  supermarket dataset and on the MS COCO
 NNNN    … …  Input  ScaleNet SharpMask  Object Proposals  R esN  et-N N  N x N C  o n v  N x N C  o n v  N x N C  o n v  G A  P  F C  N 0 x N 0 x N 0 N N  N 0 x N 0 x N 0 N N  N 0 x N 0 x N 0 N N  Object Scales Prediction  G e n e ra  te  O  b je  c t P  ro p o sa  ls  Resize According to the Prediction  Figure N: The System Overview of the Proposed Object Proposal Framework
The system has two components: ScaleNet proposed in this  paper and SharpMask [NN]
ScaleNet outputs a predication of the scale distribution of the input image, according to which the input image  is resized and fed to SharpMask
SharpMask then generates object proposals at the predicted scales
The image is best viewed in color
 N.N
Virtual environment construction  Using synthetic data for Computer Vision research has  attracted a lot of attention in recent work
Examples include  using synthetic data on semantic segmentation [NN, NN], optical flow [N, N], stereo [NN, NN], etc
To get virtual environments, the first way is by taking advantages of the existing virtual environments [N, NN, N0, NN]
The second way  is to use open source platform such as UnrealCV [NN] to  construct virtual worlds from scratch
We adopt the second approach and use UnrealCV to build virtual supermarkets
When constructing virtual environment from scratch,  spatial modeling is important for creating realistic environments [NN, NN]
The synthetic dataset introduced in this paper builds the virtual environments from scratch with randomness considered in spatial modeling, material and lighting conditions to create realistic images
 N
System Overview  This section presents the system overview of the object  proposal framework proposed in this paper, as shown in  Fig
N
The system is composed of two sequential components: the ScaleNet proposed in this paper and SharpMask [NN]
The function of ScaleNet is to predict the scales  that best describe the statistics of the image so that SharpMask can utilize the predicted scales to find objects better in  the image and outputs proposals
ScaleNet looks at the input image only once to predict the distribution of the object  scales while SharpMask looks at the input image multiple  times at the scales that are predicted by ScaleNet
 The main difference between the proposed framework  and SharpMask alone is the way they handle scales
SharpMask exhaustively searches a pre-defined scale set and generates object proposals from that
By contrast, this paper  refines the scale set so that SharpMask can take the image  at a finer range of scales for object proposal generation
 N
Scale Distribution Prediction  This section formulates the problem of scale distribution  prediction, presents the architecture of the proposed method  ScaleNet, and connects ScaleNet to SharpMask
 N.N
Problem formalization  Given an image I , we denote the objects of interest in the image I as O = {oN, oN, ..., on}
Let mi denote the max- imum of the width and the height of the bounding box of  object oi, for i = N, ..., n
Suppose the object oi can be best detected when the image is resized such that mi is equal to an ideal size denoted as D
This is aiming at work in which there is a set of object sizes that models are trained at  [N, NN, NN, NN, NN, NN]
Then the scale that image I needs to be resized to favor detecting object oi is gi = D/mi
Note that gi is continuous, and finding scales for every object oi is inefficient
Therefore, instead of formulating the problem as a regression problem, we discretize the scales into  several integer bins and model the problem as a distribution  prediction problem
 Suppose for scale distribution we have integer bins B = {bN, bN, ..., bl} with discretization precision σ ∈ Z  +, where  bi+N = bi + N, i = N, ..., l − N, and for every possible scale gi in the dataset bN < −σ logN gi < bl
Then, the ground truth scale distribution P = {pN, pN, ..., pl} over the integer bins B = {bN, bN, ..., bl} is defined by  pi =  ∑ N≤j≤n max (0, N− |bi + σ logN gj |)∑  N≤k≤l  ∑ N≤j≤n max (0, N− |bk + σ logN gj |)  (N)  Let Q = {qN, qN, ..., ql} denote the predicted distribu- tion
We formulate the problem of scale prediction as minimizing Kullback-Leibler divergence (cross entropy) from  NNNN    Q to P defined by  D(Q,P ) = ∑  N≤i≤l  pi · (log pi − log qi) (N)  We now justify Eq
N in details
SharpMask [NN] is a  scale-sensitive method, which can generate correct object  proposals only if the image is properly resized
For each  object size, there is a narrow range of image sizes within  which the object can be detected
This is where gi comes from
The rest of Eq
N comes naturally
 N.N
ScaleNet architecture  To devise a model that outputs Q which minimizes Eq
N, we propose a deep neural network called ScaleNet
This  section presents the architecture of ScaleNet and discusses  the motivations behind the design
 The input size of ScaleNet is NNN× NNN with RGB chan- nels
Given input image I of size w × h, we first resize the image to fit the input of ScaleNet I ′
More specifically, we compute d = max(w, h), then resize the image such that d = NNN
Next, we copy the resized I to the center of I ′, and pad I ′ with a constant value
I ′ is then fed into ResNet [NN] to extract image features
Here, the fully connected  layers and the last convolutional stage have been removed  from ResNet
After extraction, the features from ResNet  go through two N × N convolutional stages which serve as local fully connected layers to further process the features  separately at each location on the feature map
ReLU [NN]  and batch normalization [NN] are used in the two stages to  stabilize and speed up training
At the end, a global average pooling layer [NN] collects features at each location  of the feature map from the two convolutional stages, then  outputs scale distribution by a SoftMax operation
 The intuition is to learn the object scales at each location  of the image then combine them into one image property
 The global average pooling applied at the end of ScaleNet  distributes this learning problem to different locations of the  image
The distributed tasks can be learned separately by  fully connected layers on top of each location of feature  map from the last convolutional stage of ResNet
N×N con- volutional operation then serves as a local fully connected  layer to process the features
Similar to the fully connected  layers of VGGNet [NN], we deploy two N0NN dimension fea- ture extractors
The main difference is that the extracted  features in ScaleNet have N0NN features for each location of feature map instead of the whole image
 N.N
Connecting ScaleNet to SharpMask  For an image I , ScaleNet is able to predict a scale dis- tribution Q = {qN, ..., ql}
This is a probability density function, which we denote as q(x)
We assume that the optimal number of scales needed by SharpMask is h (usu- ally h ∼ N)
To exploit Q for SharpMask, the task is to  choose a set of scales S = {sN, ..., sh} to resize I as the input of SharpMask
The intuition is to densely sample  scales around the scales bi that have high probability qi
To achieve this, we consider the cumulative distribution function of q, i.e.,  F (s) =  ∫ s −∞  q(x) dx (N)  Then we sample scales in the space of F (s) such that  F (si) = i  h+ N , for i = N, ..., h (N)  Before sampling, the distribution q can be smoothed by  q′(x) = q(x)λ∫ q(x)λ dx  (N)  where λ is the smoothing parameter
 N
Supermarket Datasets  N.N
Real-world datasets  We aim to study the importance of the scales to the existing object proposal methods; therefore, we prepared two  real-world datasets, each of which focuses on one setting  of object scales
The first dataset, which we call Real-Far,  is composed of N0NN products labeled in bounding boxes
The images in this dataset were taken from a far distance  with less variation in scales, thus usually having more objects within one image
On average, one image contains  NN objects
The second dataset is called Real-Near, which contains NNNN products annotated
For this dataset, we took the images from a near distance and the images have more  variation in object scales
The images in Real-Near have  NN products for each on average
Two professional labelers worked on the datasets during collection
In total, we have  NNNN products labeled for testing
 N.N
Synthetic dataset  Labeling images in supermarkets can be very timeconsuming since there are usually N0 to N0 objects in one typical image
Although for SharpMask the number of  training examples grows linearly with respect to the number  of the annotated objects, ScaleNet considers one image labeled as one example, thus requiring more data for training;  what’s more, SharpMask is a mask-based proposal method,  which needs objects annotated in object masks, making annotation much harder for humans
Our solution is to build a  virtual supermarket to let models learn in this virtual environment
The training and the validation of models are all  done in the virtual supermarket
The models are then tested  directly on the real-world datasets without fine-tuning
By  doing this, we can significantly reduce human labeling, but  we need to be very careful when designing the virtual environments so that the models can transfer well to the realworld data from the synthetic data
 NNNN    Figure N: Comparison of Product Arrangements with Different Proximities
Left: an example of product arrangement result with proximity  set to 0; right: an example of product arrangement result with proximity set to N
Setting proximity to a lower value makes the arrangement  look more random while setting to a higher value will get a more organized arrangement
The valid range of proximity is within 0 to N
 Realism The first aspect we consider is the realism of the  rendered images
Although some work suggested that realism might not be critical for some vision tasks [N], it is  a high priority in this paper since we do not fine-tune on  the real-world data
The rendering engine we chose is Unreal EngineN for its flexibility of object manipulation and  high rendering quality
UnrealCV [NN] is used to extract the  ground truth of object masks
To fully exploit the power of  Unreal Engine, all the objects in the virtual supermarket are  set to be static and the lighting is baked (i.e
pre-computed)  before the game is run
 Randomness of placement The products in a real supermarket are usually placed according to certain rules
However, since the generalizability must be taken care of when  generating a virtual dataset, the randomness of placement is  introduced into the rules that guide the construction of the  virtual environment
 Similar to some ND object arrangement methods [NN,  NN], we specify a stochastic grammar of spatial relationship  between products and shelves
First, the products are initially located at a position that is not in the rendering range
 Next, given a shelf that products can be placed on, the products will be moved to fill the shelf one by one
Note that  similar products are usually placed together in supermarkets
Therefore, before placing the products, for a group of  the products, we first find an anchor point on the shelf
Then  we specify a parameter, which we call proximity, to denote  the probability that the next product will be placed near that  anchor point or will be placed randomly somewhere on the  shelf
Fig
N demonstrate the placing arrangements with  different proximities
 Product overlapping Product arrangement must prevent  overlapping
Motivated by reject sampling, we first randomly create arrangements then reject those that have overlapping products
To efficiently detect overlapping while  Nhttps://www.unrealengine.com/  preserving concave surfaces, convex decomposition is applied to the ND models before calculating overlapping
 Figure N: A Zoom-In Example of the Ground Truth Extracted by  UnrealCV [NN] with Heavily Occluded Objects Ignored
The virtual dataset is compatible with the MS COCO dataset [NN]
The  visualization result shown here uses the COCO API
The occlusion threshold is set to 0.N
 Occlusion A problem of using synthetic dataset is that all  objects will be labeled, including extremely occluded objects that are usually ignored in building real-world datasets
 Our solution to this problem is to calculate the ratio of occlusion for each object, then ignore the objects of occlusion under threshold µ when extracting the ground truth
To achieve this, we implement a standard rendering pipeline of  vertex shader and fragment shader for computing occlusion
 To gather data at high speed, we approximate the occlusion  calculation by projecting the objects to the surface parallel  to the shelf and calculating them only once
 Object scales The object scales can be controlled by  modifying the distance between the camera and the shelf
 We set the camera to be at distance ν · dmax, where dmax is the distance at which the camera can exactly take in one  shelf completely
Then we can modify ν to generate data with different object scales
 Lighting and material randomness To augment the virtual dataset, lighting and materials for objects are changed  NNNN    randomly during data gathering
 Summary This section presents how the synthetic dataset  is constructed with the above aspects taken into account
We  develop a plugin for Unreal Engine to construct virtual supermarket stochastically by only one click
We also modify  the COCO API to integrate the virtual supermarket dataset  into the MS COCO dataset [NN]
Fig
N demonstrates the  visualization of the mask annotations using the COCO API  with the occlusion threshold set to 0.N
 N
Implementation Details  This section presents the implementation details of  ScaleNet, the object proposal system, the generation of the  virtual supermarket dataset, and the data sampling strategy
 N.N
Virtual supermarket  We bought NNNN ND modelsN for products and shelves to construct the virtual supermarket
During the data collection, two parameters are manually controlled while others  are drawn randomly from a uniform distribution
The two  parameters are the occlusion threshold µ and the distance ratio ν
The range of µ is {0.N, 0.N, 0.N, 0.N, 0.N}, and the range of ν is {N, N/N.N, N/N, N/N.N, N/N}
Combining different µ and different ν results in NN configurations, for each we use different product arrangements, and random  lighting/material settings at each frame to generate N00 im- ages
The above process generates N000 synthetic images and NNNNN0 objects labeled in total
We denote this virtual dataset as dataset V
We split dataset V into Vtrain and Vval  for training and validation, respectively
The dataset Vtrain  has NN0N images and NNNNNN objects while the dataset Vval has NNNN images and N0NNN objects
 N.N
ScaleNet  We use TorchN to build and test ScaleNet
Before  training ScaleNet, the ResNet component is pre-trained  on ImageNet [N0]
The discretization precision σ is set to N, while the discrete scale bins are set to B = {−NN,−NN, ..., 0, ..., NN, NN}
To accommodate the parame- ters used in SharpMask [NN], D is set to NN0/N
 During training, we resize the image to fit the input  of ScaleNet, and calculate the scale distribution P as the ground truth
The mean pixel calculated on ImageNet is  subtracted from input image before feeding into ScaleNet
 All layers are trained, including the ResNet component
We  train two ScaleNet models for the supermarket datasets and  the MS COCO [NN] dataset, individually
We use the corresponding models when evaluating the performances on different datasets
The training dataset for ScaleNet for supermarket datasets is COCOtrain + Vtrain while the validation  dataset is COCOval + Vval
For the MS COCO, the datasets  Nhttps://www.turbosquid.com/  Methods Real-Far Real-Near  EdgeBox@N00 [NN] 0.00N 0.0NN  Selective Search@N00 [NN] 0.0NN 0.0NN  DeepMask@N00 [NN] 0.NNN 0.NNN  SharpMask@N00 [NN] 0.NNN 0.N0N  DeepMask-ft@N00 0.N0N 0.NNN  SharpMask-ft@N00 0.NNN 0.NNN  ScaleNet+DeepMask@N00 0.NNN 0.NNN  ScaleNet+DeepMask-ft@N00 0.NNN 0.NNN  ScaleNet+SharpMask@N00 0.NNN 0.NNN  ScaleNet+SharpMask-ft@N00 0.NNN 0.NNN  EdgeBox@N000 0.N0N 0.NNN  Selective Search@N000 0.NNN 0.NNN  DeepMask@N000 0.NNN 0.NNN  SharpMask@N000 0.NNN 0.NNN  DeepMask-ft@N000 0.NNN 0.NNN  SharpMask-ft@N000 0.NNN 0.NNN  ScaleNet+DeepMask@N000 0.NNN 0.NNN  ScaleNet+DeepMask-ft@N000 0.NNN 0.NNN  ScaleNet+SharpMask@N000 0.NN0 0.NNN  ScaleNet+SharpMask-ft@N000 0.NNN 0.NNN  Table N: The Comparison of the Average Recalls [NN] of Object  Proposal Methods Tested on the Real-World Supermarket Datasets  Real-Far and Real-Near
The method name indicates what method  is used and how many proposals are considered in computing recall rates, e.g., EdgeBox@N00 means EdgeBox with the number  of object proposals limited to N00
Methods that have suffix -ft are  trained on the MS COCO and the synthetic supermarket dataset
 used for training and validation include only the MS COCO  itself
Here, COCOtrain and COCOval are the training and  the validation set of the MS COCO, respectively
To connect ScaleNet to SharpMask, h is set to N for the super- market datasets, and N0 for the MS COCO
The smoothing factor λ is set to 0.N for the supermarket datasets, and 0.NN for the MS COCO
 N.N
Data sampling  In the original data sampling strategy adopted in both  DeepMask and SharpMask, each image has the same probability for objectness score training and each category has  the same probability for object mask training
Instead, we  propose to train both the objectness score and object mask  so that each annotation has the same probability of being  sampled
Following this strategy, the performance can be  slightly improved
We denote SharpMask trained in this  way as SharpMask-Ours
 N
Experimental Results  N.N
Object proposal on supermarket datasets  We first present the performance of our model on the supermarket datasets while only trained on the combination of  NNNN    Figure N: Proposals Generated by Our Method ScaleNet+SharpMask-ft with Highest IoU to the Ground Truth on the Selected Real-World  Supermarket Images
Top images are selected from dataset Real-Far while bottom images are selected from dataset Real-Near
Green  bounding boxes are from top N00 proposals
Blue bounding boxes are from proposals ranked between N0N and N000
Red bounding boxes  are ground truth of objects not found by our method within N000 proposals
The IoU threshold is set to 0.N
 the MS COCO training dataset and the virtual supermarket  training dataset
We evaluated the methods on the dataset  Real-Near and Real-Far
Qualitative results of our method  are shown in Fig
N
 Metrics The metric used to evaluate the performance of the  object proposal methods is the Average Recalls (AR) [NN]  over N0 intersection over union thresholds from 0.N to 0.NN with 0.0N as step length
 Methods We compare the performance of the proposed  method with the top methods of proposing bounding boxes  for objects: DeepMask [NN], SharpMask [NN], Selective  Search [NN], and EdgeBox [NN]
 transferability Table N demonstrates the improvements of  performances of the model trained using virtual supermarket dataset
Methods that have suffix -ft are trained on the  MS COCO and the synthetic supermarket dataset
It’s worth  noting that the models trained solely on the combination of  the general purpose dataset and the task specific synthetic  dataset exhibit consistent improvements on the task specific  real-world datasets even none of them has a look at the realworld data
 Scales Table N compares the different object proposal methods on the two real-world dataset Real-Near and RealFar
Without the help of ScaleNet to narrow down the  search space of scales, DeepMask and SharpMask actually have similar performances on them
Instead, our proNNNN    0.N0 0.NN 0.N0 0.NN 0.N0 0.NN 0.N0 0.NN 0.N0 0.NN  IoU  0.0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  R e ca ll  SharpMask SharpMask-Ours ScaleNet+SharpMask ScaleNet+SharpMask-Ours  (a) Recall @N0 Proposals  0.N0 0.NN 0.N0 0.NN 0.N0 0.NN 0.N0 0.NN 0.N0 0.NN  IoU  0.0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  R e ca ll  SharpMask SharpMask-Ours ScaleNet+SharpMask ScaleNet+SharpMask-Ours  (b) Recall @N00 Proposals  0.N0 0.NN 0.N0 0.NN 0.N0 0.NN 0.N0 0.NN 0.N0 0.NN  IoU  0.0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  R e ca ll  SharpMask SharpMask-Ours ScaleNet+SharpMask ScaleNet+SharpMask-Ours  (c) Recall @N000 Proposals  Figure N: Recall versus IoU Threshold for Different Number of Bounding Box Proposals on the MS COCO Dataset
 Methods AR@N0 AR@N00 AR@Nk  DeepMask-VGG [NN] 0.NNN 0.NNN 0.NNN  DeepMaskZoom-VGG [NN] 0.NN0 0.NNN 0.NNN  DeepMask-ResNN [NN] 0.NN0 0.NNN 0.NN0  SharpMask [NN] 0.NNN 0.NNN 0.NNN  SharpMaskZoom [NN] 0.N0N 0.NNN 0.NNN  SharpMask-Ours 0.NNN 0.NNN 0.NN0  ScaleNet+SharpMask 0.N0N 0.NNN 0.NNN  ScaleNet+SharpMask-Ours 0.NN0 0.NNN 0.NNN  Table N: Comparison of Our Framework to DeepMask [NN] and  SharpMask [NN] on Bounding Box Object Proposals on the MS  COCO validation dataset [NN]
 posed method exhibit stronger improvements on Real-Near  in which the image has fewer objects, thanks to the accurate  prediction by ScaleNet of the scales to resize images
 In short, Table N demonstrates the significant performance improvements by using our proposed framework
 N.N
Object proposal on the MS COCO dataset  Next, we evaluate our method on the MS COCO dataset
 Following the evaluations done in DeepMask [NN] and  SharpMask [NN], the recall rates are evaluated on the first  N000 images on the validation set
 Methods We compare the performance of the proposed method with the state-of-the-art methods of proposing bounding boxes for objects: DeepMask-VGG [NN],  DeepMaskZoom-VGG [NN], DeepMask-ResNN [NN], SharpMask [NN], SharpMaskZoom [NN]
 Metrics We adopt the same metrics used for evaluating performances on the supermarket datasets
The performances  are evaluated when the number of proposals is limited to N0, N00 and N000
 Results Table N summarizes the performance comparisons  on the MS COCO dataset
Since the object scales in  these natural images are not always sparse, we do not expect significant improvements as shown in the supermarket datasets
However, consistent improvements can be  observed at all number of proposals
More notably, our  method demonstrates stronger performance improvements  compared with that between SharpMask and DeepMask
 Fig
N shows the additional performance plots comparing  our methods with the previous state-of-the-art
Our framework improves the recall rates significantly at N000 pro- posals, e.g., the recall rate increases from 0.NNN to 0.NNN when IoU threshold is set to 0.N, and from 0.NNN to 0.NNN at 0.N IoU threshold
We also observe strong performance increases at N00 proposals: the recall rate at 0.N IoU thresh- old increases from 0.NNN to 0.NNN, and from 0.NNN to 0.NNN at 0.N IoU threshold
 N
Conclusion  In this paper, we study the problem of object proposal  generation in supermarket images and other natural images
 We introduce three supermarket datasets – two real-world  datasets and one synthetic dataset
We present an innovative object proposal framework, in which the object scales  are first predicted by the proposed scale prediction method  ScaleNet
The experimental results demonstrate that the  model trained solely on the combination of the MS COCO  dataset and the synthetic supermarket dataset transfers well  to the two real-world supermarket datasets
The proposed  scale-aware object proposal method is evaluated on the realworld supermarket datasets and the MS COCO dataset
Our  proposed method outperforms the previous state-of-the-art  by a large margin on these datasets for the task of object  detection in the form of bounding box
 Acknowledgments We thank Wanyu Huang, Zhuotun  Zhu and Lingxi Xie for their helpful suggestions
We  gratefully acknowledge funding supports from NSF CCFNNNNNNN and ONR N000NN-NN-N-NNNN
This work was also  supported in part by the National Natural Science Foundation of China under Grant NNNNNNNN
 NNNN    References  [N] P
Arbeláez, J
Pont-Tuset, J
T
Barron, F
Marques, and  J
Malik
Multiscale combinatorial grouping
In CVPR,  N0NN
N, N  [N] A
Borji, M.-M
Cheng, H
Jiang, and J
Li
Salient object  detection: A benchmark
TIP, NN(NN):NN0N–NNNN, N0NN
N  [N] D
J
Butler, J
Wulff, G
B
Stanley, and M
J
Black
A  naturalistic open source movie for optical flow evaluation
 In ECCV, N0NN
N  [N] K
Chang, T
Liu, H
Chen, and S
Lai
Fusing generic objectness and visual saliency for salient object detection
In  ICCV, N0NN
N  [N] K
Chang, T
Liu, and S
Lai
From co-saliency to cosegmentation: An efficient and fully unsupervised energy  minimization model
In CVPR, N0NN
N  [N] L
Chen, Y
Yang, J
Wang, W
Xu, and A
L
Yuille
Attention to scale: Scale-aware semantic image segmentation
 CoRR, abs/NNNN.0NNNN, N0NN
N  [N] M.-M
Cheng, Z
Zhang, W.-Y
Lin, and P
Torr
Bing: Binarized normed gradients for objectness estimation at N00fps
 In CVPR, N0NN
N, N  [N] A
Dosovitskiy, P
Fischer, E
Ilg, P
Häusser, C
Hazirbas,  V
Golkov, P
van der Smagt, D
Cremers, and T
Brox
 Flownet: Learning optical flow with convolutional networks
 In ICCV, N0NN
N, N  [N] A
Dosovitskiy and V
Koltun
Learning to act by predicting  the future
CoRR, abs/NNNN.0NNNN, N0NN
N  [N0] D
Erhan, C
Szegedy, A
Toshev, and D
Anguelov
Scalable  object detection using deep neural networks
In CVPR, N0NN
 N  [NN] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
IJCV, NN(N):N0N–NNN, N0N0
N  [NN] M
Fisher, D
Ritchie, M
Savva, T
Funkhouser, and P
Hanrahan
Example-based synthesis of Nd object arrangements
 ACM Trans
Graph., NN(N):NNN:N–NNN:NN, Nov
N0NN
N, N  [NN] A
Ghodrati, A
Diba, M
Pedersoli, T
Tuytelaars, and  L
Van Gool
Deepproposal: Hunting objects by cascading  deep convolutional layers
In ICCV, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N, N  [NN] J
H
Hosang, R
Benenson, P
Dollár, and B
Schiele
 What makes for effective detection proposals? CoRR,  abs/NN0N.0N0NN, N0NN
N, N  [NN] P
Hu and D
Ramanan
Finding tiny faces
CoRR,  abs/NNNN.0NN0N, N0NN
N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 CoRR, abs/NN0N.0NNNN, N0NN
N  [NN] Z
Jie, X
Liang, J
Feng, W
F
Lu, F
E
H
Tay, and  S
Yan
Scale-aware pixelwise object proposal networks
 TIP, NN(N0):NNNN–NNNN, N0NN
N  [NN] M
Johnson, K
Hofmann, T
Hutton, and D
Bignell
The  malmo platform for artificial intelligence experimentation
 In IJCAI, N0NN
N  [N0] P
Krähenbühl and V
Koltun
Geodesic object proposals
In  ECCV, N0NN
N, N  [NN] P
Krahenbuhl and V
Koltun
Learning to propose objects
 In CVPR, N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, N0NN
N  [NN] W
Kuo, B
Hariharan, and J
Malik
Deepbox: Learning  objectness with convolutional networks
In ICCV, N0NN
N  [NN] Y
Li, X
Hou, C
Koch, J
M
Rehg, and A
L
Yuille
The  secrets of salient object segmentation
In CVPR, N0NN
N  [NN] M
Lin, Q
Chen, and S
Yan
Network in network
CoRR,  abs/NNNN.NN00, N0NN
N  [NN] T
Lin, M
Maire, S
J
Belongie, L
D
Bourdev, R
B
 Girshick, J
Hays, P
Perona, D
Ramanan, P
Dollár, and  C
L
Zitnick
Microsoft COCO: common objects in context
 CoRR, abs/NN0N.0NNN, N0NN
N, N, N, N, N  [NN] J
Liu and Y
Liu
Grasp recurring patterns from a single  view
In CVPR, N0NN
N  [NN] T
Liu, Z
Yuan, J
Sun, J
Wang, N
Zheng, X
Tang, and  H.-Y
Shum
Learning to detect a salient object
TPAMI,  NN(N):NNN–NNN, N0NN
N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
E
Reed,  C
Fu, and A
C
Berg
SSD: single shot multibox detector
 In ECCV, N0NN
N  [N0] A
Mahendran, H
Bilen, J
F
Henriques, and A
Vedaldi
Researchdoom and cocodoom: Learning computer vision with  games
CoRR, abs/NNN0.0NNNN, N0NN
N  [NN] N
Mayer, E
Ilg, P
Hausser, P
Fischer, D
Cremers,  A
Dosovitskiy, and T
Brox
A large dataset to train convolutional networks for disparity, optical flow, and scene flow  estimation
In CVPR, N0NN
N  [NN] V
Nair and G
E
Hinton
Rectified linear units improve restricted boltzmann machines
In ICML, N0N0
N  [NN] P
H
O
Pinheiro, R
Collobert, and P
Dollár
Learning to  segment object candidates
In NIPS, N0NN
N, N, N, N, N, N  [NN] P
O
Pinheiro, T.-Y
Lin, R
Collobert, and P
Dollár
Learning to refine object segments
In ECCV, N0NN
N, N, N, N, N,  N, N  [NN] W
Qiu and A
L
Yuille
Unrealcv: Connecting computer  vision to unreal engine
CoRR, abs/NN0N.0NNNN, N0NN
N, N,  N  [NN] J
Redmon, S
K
Divvala, R
B
Girshick, and A
Farhadi
 You only look once: Unified, real-time object detection
In  CVPR, N0NN
N  [NN] S
Ren, K
He, R
B
Girshick, and J
Sun
Faster R-CNN:  towards real-time object detection with region proposal networks
In NIPS, N0NN
N  [NN] S
R
Richter, V
Vineet, S
Roth, and V
Koltun
Playing for  data: Ground truth from computer games
In ECCV, N0NN
N  [NN] G
Ros, L
Sellart, J
Materzynska, D
Vazquez, and A
M
 Lopez
The synthia dataset: A large collection of synthetic  images for semantic segmentation of urban scenes
In CVPR,  N0NN
N  [N0] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual  Recognition Challenge
IJCV, NNN(N):NNN–NNN, N0NN
N  NNNN    [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
CoRR,  abs/NN0N.NNNN, N0NN
N, N  [NN] J
R
R
Uijlings, K
E
A
van de Sande, T
Gevers, and  A
W
M
Smeulders
Selective search for object recognition
 IJCV, N0N(N):NNN–NNN, N0NN
N, N, N, N  [NN] F
Xia, P
Wang, L
Chen, and A
L
Yuille
Zoom better to  see clearer: Human part segmentation with auto zoom net
 CoRR, abs/NNNN.0NNNN, N0NN
N  [NN] L.-F
Yu, S.-K
Yeung, C.-K
Tang, D
Terzopoulos, T
F
 Chan, and S
J
Osher
Make it home: Automatic optimization of furniture arrangement
ACM Trans
Graph.,  N0(N):NN:N–NN:NN, July N0NN
N, N  [NN] Y
Zhang, W
Qiu, Q
Chen, X
Hu, and A
L
Yuille
Unrealstereo: A synthetic dataset for analyzing stereo vision
 CoRR, abs/NNNN.0NNNN, N0NN
N  [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object  proposals from edges
In ECCV, N0NN
N, N, N, N  NN00See the Glass Half Full: Reasoning About Liquid Containers, Their Volume and Content   See the Glass Half Full: Reasoning about Liquid Containers,  their Volume and Content  Roozbeh Mottaghi† Connor Schenck‡ Dieter Fox‡ Ali Farhadi†‡  †Allen Institute for Artificial Intelligence (AIN) ‡University of Washington  Abstract  Humans have rich understanding of liquid containers  and their contents; for example, we can effortlessly pour  water from a pitcher to a cup
Doing so requires estimating  the volume of the cup, approximating the amount of water  in the pitcher, and predicting the behavior of water when we  tilt the pitcher
Very little attention in computer vision has  been made to liquids and their containers
In this paper,  we study liquid containers and their contents, and propose  methods to estimate the volume of containers, approximate  the amount of liquid in them, and perform comparative volume estimations all from a single RGB image
Furthermore,  we show the results of the proposed model for predicting  the behavior of liquids inside containers when one tilts the  containers
We also introduce a new dataset of Containers  Of liQuid contEnt (COQE) that contains more than N,000  images of N0,000 liquid containers in context labelled with  volume, amount of content, bounding box annotation, and  corresponding similar ND CAD models
 N
Introduction  Recent advancements in visual recognition have enabled  researchers to start exploring tasks that go beyond categorization and entail high-level reasoning in visual domains
 Visual reasoning, an essential component for a visually intelligent agent, has recently attracted computer vision researchers [NN, NN, NN, NN, NN, NN, NN, N, N, NN]
Almost all  the efforts in visual recognition and reasoning have been  devoted to solid objects: how to detect [NN, NN] and segment [N0, N] them, how to reason about physics of a solid  world [NN, NN], and how forces would affect their behavior [NN, NN]
Very little attention, however, has been made  to liquid containers and the behavior of their content
 Humans, on the other hand, deal with liquids and their  containers on a daily basis
We can comfortably pour water from a pitcher to a cup knowing how much water is already in the cup and having an estimate of the volume of the  NN%  Volume	Estimation  Content	Estimation  Pouring	Prediction  t  N000	mL  Can	we	pour	the	content	of	  the	container	in	the	yellow	  box	into	the	green	one?  Comparative	Volume	Estimation  Figure N
Our goal is to estimate the volume of the container (Volume Estimation), approximate what fraction of the volume is filled  (Content Estimation), infer whether we can pour the content of one  container into another (Comparative Volume Estimation), and predict how much liquid will remain in a container over time if it  is tilted to a certain angle (Pouring Prediction)
Our inference is  based on a single RGB image
 cup and the amount of water in the pitcher
We effortlessly  estimate the angle by which to tilt the pitcher to pour the  right amount of water into the cup
In fact, five month old  infants develop rich understanding of liquids and their containers and can predict whether water will pour or tumble  from a cup if the cup is upended [NN]
Other species such as  orangutans can also estimate the volume of liquids inside a  container and can predict if the liquid in one container can  fit into the other one [N]
 In this paper, we study liquid containers (Figure N) and  propose methods to estimate the volume of containers and  their content in absolute and relative senses
We also show,  for the first time, that we can predict the amount of liquid  that remains in a container if it is tilted for a certain tilt angle, all from a single image
We introduce Containers Of  liQuid contEnt (COQE), a dataset of images with containers annotated for their volume, the volume of their content,  bounding box, and corresponding similar ND models
Estimating the volume of containers is extremely challenging  and requires reasoning about the size of the container, its  NNNN    shape, and contextual cues surrounding the container
The  volume of the liquid content of a container can be estimated using subtle visual cues like the line at the edge of  the liquid inside the container
We propose a deep learning  based method to estimate the volume of containers and their  content using the contextual cues from the surrounding objects
In addition, by integrating Convolutional Neural Networks and Recurrent Neural Networks, we can predict the  behaviour of liquid contents inside containers as their reaction to tilting the container
 Our experimental evaluations on COQE dataset show  that incorporating contextual cues provides improvement  for estimating volume of the containers and the amount of  their content
Furthermore, we show the results using a single RGB image for predicting how much liquid will remain  inside a container over time if it is tilted by a certain angle
 N
Related Work  In this section, we describe the work relevant to ours
 To the best of our knowledge, there is little to no work that  directly addresses the same problem
Below, we mention  past work that are most related
 In [NN], a hybrid discriminative-generative approach is  proposed to detect transparent objects such as bottles and  glasses
[NN] propose a method for detection, ND pose estimation, and ND reconstruction of glassware
[NN] also propose a method for reconstruction of ND scenes that include  transparent objects
Our work goes beyond detection and  reconstruction since we perform reasoning about higherlevel tasks such as content estimation or pour prediction
 Object sizes are inferred by [N] using a combination of  visual and linguistic cues
In this paper, we focus only on  visual cues
Size estimates have also been used by [NN, NN]  to better estimate the geometry of scenes
 The result of ND object detectors [NN, N0, NN] can be  used to obtain a rough estimate of the volume of the containers
However, they are typically designed for RGBD  images
Moreover, the output of these detectors cannot be  used for estimation of the amount of content or pouring prediction
Depth estimation methods from single RGB images  [NN, NN, NN] can also be used for computing the relative size  of containers
 The affordance of containing liquids is inferred by [NN]
 Additionally, they reason about the best filling and transfer  directions
The problem that we address is different and we  use RGB images during inference (as opposed to RGBD  images)
[NN] uses physical simulation to infer the affordance of containers and containment relationship between  objects
Our work is different since we reason about liquid  content estimation, pouring prediction, etc
 Our pouring prediction task shares similarities with [NN]
 In [NN], they predict the sequence of movement of rigid objects for a given force
In this work, we are concerned with  liquids that have different dynamics and appearance statistics than solid objects
 There are a number of works in the robotics community  that tackle the problem of liquid pouring [NN, N, NN, NN, NN,  NN]
However, these approaches either have been designed  for synthetic environments [NN, NN] or they have been tested  in lab settings and with additional sensors [NN, NN, N, NN]
 Fluid simulation is a popular topic in computer graphics  [NN, N0, N]
Our problem is different since we predict the  liquid behavior from a single image and are not concerned  about rendering
 There are also several cognitive studies about liquids,  their physical properties and their interaction with the containers [N0, NN, N, NN, N, NN, NN, NN, NN]
 N
Tasks  In this paper, we focus on four important tasks related to  liquids and their containers:  • Container volume estimation: Our goal in this task is to infer the volume of the container (i.e, the volume  of the liquid inside the container when the container is  full)
The input is a single RGB image and the query  container, and the output is the volume estimate (e.g.,  N0mL, N00mL, etc)
 • Content estimation: In this task, the goal is to esti- mate how full a container is given a single RGB image  and a query container
The example outputs are empty,  N0% full, N0% full, etc
 • Comparative volume estimation: The task is to infer if we can pour the entire content of one container into  another container
The input is a single RGB image  and a pair of query containers in that image, and the  output is yes, no, or can’t tell (since we have opaque  containers in the dataset)
This is more complex than  the previous two tasks since it requires reasoning about  the size of the two containers and the amount of liquid  in them simultaneously
 • Pouring prediction: The goal is to infer the amount of liquid in a container over time after tilting the container  by a given angle
The inputs are a single RGB image,  a query object, and a tilt angle
The output is a variable  length sequence that determines the amount of liquid  at each time step
The sequence has a variable length  since some containers become empty much faster than  other containers depending on the initial amount of liquid in them, the size of the container, and the tilt angle
 N
COQE Dataset  There is no dataset to train and evaluate models on the  four tasks defined above
Hence, we introduce a new dataset  NNNN    Figure N
COQE dataset
Example containers in our dataset
The bottom row shows the corresponding ND CAD model for the container  inside the yellow bounding box
 called Containers Of liQuid contEnt (COQE)
 The COQE dataset includes more than N,000 images,  where in each image there are at least two containers
The  containers belong to different categories such as bottle,  glass, pitcher, bowl, kettle, pot, etc
Figure N shows some  example images in the dataset
 It is infeasible to use web for collecting this dataset since  obtaining accurate groundtruth volume estimates for arbitrary web images is not trivial
To overcome this problem,  we used a commercial crowd-sourcing platform to collect  images and their corresponding annotations
The annotators took pictures using their cameras or cellphones and  measured the container volume using a measuring cup or  reported the volume on the container label
 The data collectors were instructed to meet certain requirements
First, the images should include the context  around the container since estimating the size from an image that only shows the container is an ambiguous task
To  impose this constraint, we asked the annotators to take pictures that have at least N objects in each image
Second,  the dataset should include annotations only for containers  that had a bounding box whose larger side is larger than N0  pixels
We had this requirement because the content of the  containers is not visible if the containers appear very small  in the image
Finally, the dataset should include images  that have objects in a natural setting to better capture background clutter, different illumination conditions, occlusion,  etc
 Each container in our dataset has been annotated by its  bounding box, the volume, and the amount of liquid inside  the container
Additionally, we downloaded NN CAD models from Trimble ND Warehouse and we specify which ND  CAD model is most similar to each container in the images
Finding the correspondence with the CAD models  enables us to run pouring simulations
For pouring simulation, we rescale the CAD models to the annotated volume  and consider the annotated amount of liquid in the CAD  model
Then, we tilt the CAD model by x degrees and record how much liquid remains in the CAD model for each  tilt angle
Section N.N provides more details about pouring  simulations
 N
Our Approach  N.N
Volume and Content Estimation  We now describe the model for estimation of container  volume and content volume for a query container in an image
 We use a Convolutional Neural Network (CNN), where  the input has N channels
The first three channels of the  input are the RGB channels of the input image, and the  fourth channel is used to represent the bounding box of the  query container, which is basically a bounding box mask  smoothed by a Gaussian kernel
An additional input to our  model is a set of masks generated by an object detector
The  masks generated by the object detector enable us to capture  contextual information
The idea is that the surrounding objects typically provide a rough prior for the volume of the  container of interest
We use Multipath network [N0] as our  object detector, which is a state-of-the-art instance segmentation method that generates a mask for objects along with  the category information
We use Multipath that is trained  on COCO dataset [NN] so it generates masks for N0 categories defined by [NN]
We create a binary image for each  category, where the pixels of all masks for that category are  set to N
Then, we resize the mask to NN × NN
We obtain a NN × NN × NN cube, referred to as context tensor, since the object detector has N0 categories and we consider one category for the background (areas not covered by the masks  of the N0 categories)
For efficiency concerns, we do not  use these masks in the input channel and we use them in a  NNNN    co n v _ N _ N  co n v _ N _ N  Multipath  ResNet  NN  NN  NN  volume  class  content  class  Figure N
Model architecture
For volume and content estimation, the input to our network is an RGB image and the mask for  the query container
We feed the RGB image into the Multipath  network [N0] to generate a set of mask detections
The masks for  different categories form a tensor (shown in grey) and are concatenated with the output of the conv N N layer of ResNet-NN (the  purple cube)
 higher level of the model
 The architecture of our model is shown in Figure N
 We concatenate the context tensor with the input of the  convN N layer of ResNet-NN [NN] whose input size is  NN× NN× NNN
As a result, the input to convN N will be of size NN× NN× N0N
We refer to this network as Contextual ResNet for Containers (CRC) throughout the paper
 We formulate volume and content estimation as classification
We change the layer before the classification layer  of ResNet based on the number of classes in each task
The  loss for this network is the cross-entropy loss, which is typically used for multi-class classification
We consider different weights for different classes according to their inverse  frequency in the training data
We could alternatively formulate these tasks as a regression problem
However, we  obtained better performance using the classification formulation
Note that we train the network separately for volume  and content estimation tasks (i.e
the classification layer has  different size of output depending on the task)
 N.N
Comparative Volume Estimation  Here, we answer the following question: “Can we pour  the entire content of container N into container N in the same  image?”
Basically, the model needs to estimate the volume  for the two containers and infer the current amount of liquid  in each of them to answer the question
Our approach is  implicit in that we let the network figure out these quantities  and do not provide explicit supervision
 Our model for this task is a Siamese network, where  there are two branches corresponding to two different containers in question
Similar to the previous tasks, each  branch of the model receives a N-channel input, where the  first N channels is the RGB image and the Nth channel is the  bounding box mask for the query container
We concatenate the output of the layers before the classification layers of the two branches (the concatenation output is N0NNdimensional)
A fully connected (FC) layer follows the output of the concatenation, which provides the input to a LogSoftmax layer
Alternatively, we tried a N-channel input  (i.e., N RGB channels, one channel for the mask of container  N and another channel for the mask of container N)
The  performance for this scenario was worse than the performance of the proposed model
We also tried two scenarios  for the Siamese network, where we considered shared and  non-shared weights
The performance for the shared weight  case was better
The loss for this task is cross-entropy loss  as well since we formulate it as classification, where the labels are yes, no, can’t tell (which happens when at least one  of the containers is opaque and its content is not visible)
 N.N
Pouring Prediction  In this task, we predict how much liquid will remain in  the container if we tilt it by x degrees
The output of this task is a function of a few factors: (N) The initial amount  of liquid in the container, e.g., if a bottle is N0% full, tilting  it by a few degrees will not have any effect on the amount  of the liquid that remains inside the container
(N) The geometry of the container
For example, a large tilt angle is  required to pour the liquid from a container that has a narrow mouth
(N) The volume of the container
For example,  it takes longer to pour the content of a larger container compared to a tiny container
Estimating each of these factors is  a challenging task by itself
 We formulate this task as sequence prediction, where our  goal is to generate the sequence of the amount of liquid in  the container over time given a single RGB image, a query  container, and a tilt angle x
 The amount of the liquid at each time step is dependent  on the previous time steps so we use a recurrent network  to capture these dependencies
Our architecture is a Convolutional Neural Network (CNN) followed by a Recurrent  Neural Network (RNN)
 The CNN part of the network has the same architecture  as that of CRC (shown in Figure N) with two differences
 The first difference is that we have an additional input channel to encode the angle x
This channel has the same height and width as the input image and it is concatenated with  the input image
All elements of this channel are set to  x
The second difference is that we remove the classifi- cation layer of CRC so we can feed the output of the CNN  into the recurrent part
We denote the output of the CNN  by f , which is a NNN dimensional vector
We use f as the  input to the recurrent part of the network
The architecture for this task is shown in Figure N
We consider a N00dimensional hidden unit for the recurrent network
The outNNNN    CRC RNN  N0	degrees  t  angle mask  f  !" !# !$  (Fig.	N)  Figure N
Model architecture for pouring prediction
The input  to this model is an RGB image, the mask for the query container  and an image that encodes the tilt angle
The output of our model  (CRC) is fed into an RNN that predicts a sequence that represents  how much liquid remains in the container over time
We train this  network end-to-end
 put of the RNN at each time step, ot, is |R| dimensional, where R = {r0, rN, · · · , rN , p} is the set of discretized amounts of liquid, for example, r0 represents empty, rN rep- resents N0% full, etc
The label p represents the opaque case
Some of the containers are opaque
In this case, no  estimation can be provided since the content is not visible
 Note that the problem at each time step is a classification  problem, where the RNN generates one of |R| classes
As described in Section N, there is a ND CAD model associated to each example
Therefore, we can simulate tilting for  each container given an initial amount of liquid and obtain  the groundtruth for this task
Note that the ND CAD models  are only used during training and not for inference.The detailed procedure for obtaining the groundtruth sequence is  described in Section N.N
 The RNN stops the sequence if it generates r0, which is the empty state, or p, which corresponds to the opaque container case
The reason is that the rest of the sequence  should be the same if it generates either of these two labels
 We consider a maximum length of N for the sequences in  our experiments
 The loss function is defined over the output sequence
 Suppose we denote the groundtruth and output sequence by  S = (s0, sN, · · · , st′) and O = (o0,oN, · · · ,ot), respec- tively
The loss will be defined as:  L(S,O) = − N  T  T∑  t=0  wt(st) log(ot[st]), (N)  where T is the maximum length of sequence, and wt(st) is the weight for each class (i.e
the inverse frequency of  the amount st at time t in the training data)
Also, ot[st] is the st-th element of ot
Recall that ot is |R|-dimensional
Also, note that ot = SoftMax(g(ht)), where ht is the hidden unit of the RNN at time step t, and g is a linear func- tion followed by a ReLU non-linearity
Hence, the loss is  a cross-entropy loss defined over the sequence
If the output sequence and the groundtruth sequence have different  lengths (i.e
t N= t′) , we pad them by the last element of the sequence to make them the same length
 N
Experiments  We evaluate our models on different tasks that we defined: estimating the volume of a query container, estimating how full the container is (content estimation), comparative volume estimation that infers if we can pour the entire  content of one query container into another, and pouring  prediction that provides a temporal estimate of how much  liquid will remain inside the query container if we tilt it
 The first three tasks are mainly related to estimating the geometry of the container and its content, while the fourth task  addresses the estimation of the behavior of the liquid inside  the container
 Dataset: Our dataset consists of more than N,000 images  that include more than N0,000 annotated containers
We use  N,NNN containers for training, N,000 for validation and N,000  for test
Each container is annotated with the volume, the  amount of content, a bounding box, and a corresponding  ND CAD model
 N.N
Implementation Details  We use TorchN to implement the proposed neural networks
We run the experiments on a Tesla KN0 GPU
We  feed the training images into the network in batches of size  NN, where each batch contains RGB images, the mask images for the query container (or two masks for the comparative volume estimation task), and context tensor (described  in Section N.N)
 Our learning rate is N0−N for all experiments
We use ResNet-NN [NN] for the ResNet part of the networks
The  ResNet is pre-trained on ImageNet N
We randomly initialize the mask channels of the input layer and additional channels of conv N N in CRC
For the random initialization, we  randomly sample from a Gaussian distribution with mean 0  and standard deviation 0.0N
To train the proposed models  and the baselines we use N0,000 iterations
We choose the  model that has the highest performance on the validation  set
 N.N
Volume Estimation  We first provide evaluations for the volume estimation  task
We divide the space of volumes into N0 classes, where  the maximum volumes in each class are: N0, N00, N00, N00,  N00, NN0, N000, N000, N000, ∞
The unit for the measure- ment is milliliter (mL)
For example, the first class contains  all containers that are smaller than N0mL, the second class  are containers whose volume is between N0mL and N00mL  Nhttp://torch.ch Nhttps://github.com/facebook/fb.resnet.torch/tree/master/pretrained  NNNN    0 < # ≤ N0&'  N0&' < # ≤ N00&'  N00&' < # ≤ N00&'  N00&' < # ≤ NN0&'  N000&' < # ≤ N000&'  Figure N
Qualitative results of volume estimation
The volume for the query container (indicated by the yellow bounding box) is shown  under the image
 and so on
The reason that the range is not uniform is to  have better visual separation of examples
We could alternatively formulate the problem as a regression problem since  volume is a continuous quantity, but the performance was  worse
[NN, NN, NN] also formulated a continuous variable  estimation problem as classification due to the same reason
 The baselines for this task are: (N) a naive regression  that takes width and height of the container bounding box  (normalized by the image width and height, respectively) as  features and regresses the volume
(N) classification using  AlexNet, where we replace the FCN layer of AlexNet and  its classification layer to adapt them to a N0-class classification
(N) The CRC model without the contextual information
We use the same number of iterations for training  these networks
 Table N shows the results for this task
Our evaluation  metric is average per-class accuracy
The chance performance for this task is N0%
Our model provides about  N.N% improvement over the case that we do not use contextual information
The results suggest that the information  about the surrounding objects can help volume estimation
 The overall low performance of these state-of-the-art CNNs  shows how challenging the task is
Figure N shows qualitative examples of volume estimation
 Avg
per-class accuracy  Chance N0.00  Box-Regression NN.NN  AlexNet NN.NN  Ours w/o context NN.NN  Ours w/ context (CRC) NN.NN  Table N
Quantitative results for volume estimation
 N.N
Content Estimation  In this task, we estimate the amount of content in a  query container in an RGB image
The annotators provide  groundtruth for this task in terms of one of the following N  classes: 0% (empty), NN%, N0%, NN%, N00%, and opaque
 The content of an opaque container cannot be estimated using visual cues so we consider this category as well to handle this case
Similar to above, we use average per-class accuracy as the evaluation metric
We use similar CNN-based  baselines as above
 Table N shows the results for this task
Our model improved the performance by N.N% compared to the case that  we do not use context
The chance accuracy for this task is  NN.NN%
Our method achieves NN.0N% per-class accuracy  which is NN.N% above the chance performance
However,  this result shows that there is still a large room for improvement on this task
Figure N shows a few qualitative examples of content estimation
 Avg
per-class accuracy  Chance NN.NN  AlexNet NN.N0  Ours w/o context NN.NN  Ours w/ context (CRC) NN.0N  Table N
Quantitative results for content estimation
 N.N
Comparative Volume Estimation  In this task, we infer whether we can pour the entire content of a query container into another one
This is a challenging task since it requires estimation of the content volume for both containers and also the volume of the container  that the liquid is poured into
We formulate this problem as  a N-class classification, where the classes are yes, no, and  can’t tell (when at least one of the containers is opaque)
 The procedure for obtaining groundtruth for this task is  as follows
Let vN and vN to be the volume for a pair of containers in an image, respectively, and cN and cN represent how full each container is (0 ≤ cN, cN ≤ N)
Note that in our dataset we have annotations for vN, vN, cN, and cN
If cN ∗ vN < (N − cN) ∗ vN, we can pour the entire content of container N into container N
 For this experiment, two N-channel input images are fed  into the two branches of the Siamese network
As baselines,  we replace both branches of the network by AlexNets or our  NNNN    0%  NN%  N0%  N00%  opaque  Figure N
Qualitative results of content estimation
On the left side of each image, we show the predicted amount of liquid in the query  container (indicated by the yellow box)
The rightmost image shows an opaque container for which it is not possible to correctly predict  the amount of content
 model without context, where a fully connected (FC) layer  and a Log-Softmax layer follow the concatenation of the  output of these branches
Similar to the previous tasks, we  use average per-class accuracy as the evaluation metric
 Table N shows the results
Note that in this task, we  consider only containers that are in the same image since  comparative volume estimation across different images is a  difficult task even for humans
 Avg
per-class accuracy  Chance NN.NN  AlexNet NN.N0  Ours w/o context NN.NN  Ours w/ context (CRC) NN.NN  Table N
Quantitative results for comparative volume estimation
 N.N
Pouring Prediction  The above evaluations mainly address the properties of  the containers such as the volume of the containers and the  amount of their content
We now describe the results for  pouring prediction task, which is related to the behavior of  the liquid inside the containers
This task requires generating a sequence, where each element of the sequence shows  how full the container is at each timestep
We first explain  how we obtain groundtruth sequences and then present the  evaluations
 Obtaining groundtruth: Recall that we have a ND CAD  model associated to each container in images
Therefore,  we can consider a certain amount of liquid in each ND CAD  model
We compute the amount of liquid remaining at each  timestep during a pour as follows
At each timestep, we use  the angle of the container to compute the maximum amount  of liquid that could stably be held in the container without  overflowing
To do this, we draw a horizontal plane parallel to the ground from the lip of the container
We then  compute the volume of the container below that plane using  a ND mesh of its interior
Then, to compute the remaining amount of liquid at that timestep, we simply take the  maximum of this value and the initial amount of liquid in  the container
Intuitively, this means that if the container  at a given angle can hold more liquid than it was initially  filled with, then none will have spilled out and that is the  amount that is in the container
Conversely, if the maximum amount of liquid that can rest stably in a container is  less than the initial amount, then all excess will have spilled  out and the amount remaining will be the maximum stable  amountN
We also used Fluidix (a fluid simulation library),  but the results were not significantly different from the results of the above method (the error was smaller than our  bin size)
 During training, for each container in the images, we  have an associated CAD model and an initial amount of liquid in the container (one of the following values according  to the annotations: 0%, NN%, N0%, NN%, N00%, or opaque)
 Therefore, we can estimate the amount of remaining liquid in the container for different angles and different initial  amount of liquid
Note that during test we only have a single RGB image, the mask for the query container and the  query angle, and we do not use ND CAD models
 To generate sequences, we tilt each container from 0  degrees to a certain degree x, where 0 degree is the up- right pose and NN0 degrees corresponds to an upside down  container (we ignore containers that are not in the upright  pose in the image for training and evaluation)
The maximum length of sequence that we consider is N i.e
we  consider N timesteps for tilting from 0 to x degrees and measure the remaining amount of liquid at each timestep  using the procedure described above
We consider a discrete set of fractions 0, 0.N, 0.N, 


, 0.N, N and assign the remaining amount of liquid to the closest fraction
Therefore, each element of the sequence belongs to one of NN  classes (NN fractions + N opaque class)
More concretely, in  R = {r0, rN, · · · , rN , p} (defined in Section N.N), r0 = 0, rN = 0.N, rN = 0.N, etc
 Note that the sequences can have different length
For  example, if a container is initially empty, the sequence will  be of length N since the amount of liquid will not change as  the result of tilting
Similarly, the corresponding sequence  for all opaque containers is of length N since no estimation  can be performed for opaque containers
 The result for this task is shown in Table N
Our evaluation criteria is defined as follows
We consider a predicted  NNote that this approximation does not take into account attributes such  as liquid viscosity or surface tension
However, this approximation is accurate enough for our purposes
 NNNN    NN	degreesNN0	degrees  NN	degrees  NN	degrees  N00	degrees  Figure N
Qualitative results for pouring prediction
Our method estimates the amount of the remaining liquid at each time step
The tilt  angle for each sequence is shown under the sequence
The bottom row shows the case that the amount of liquid in the container does not  change as the result of tilting
Note that we show the CAD models only for visualization purposes
They are neither predicted nor used for  inference
 sequence as correct if all elements of that sequence match  the elements in the groundtruth sequence
The first column  of the table shows the result of the exact match of the sequences
We also show the results for different edit distances (edit distance between the predicted and groundtruth  sequences)
Qualitative examples of pouring prediction are  shown in Figure N
 We apply N different tilt angles to each container in train,  validation and test images
The chance performance for  this task is N/NNN since there are NNN unique patterns of sequences in the test set
 Edit distance 0 N N N N  AlexNet NN.NN NN.NN NN.0N NN.NN NN.0N  Ours w/o context NN.NN NN.NN N0.0N NN.0N NN.NN  Ours w/ context N0.NN NN.NN NN.NN NN.N0 NN.NN  Table N
Quantitative results for pouring prediction
The results  for different edit distances of the groundtruth and predicted sequences are shown
 N
Conclusion  Reasoning about containers and the behavior of the liquids inside them is an important component of visual reasoning
However, it has not received much attention in the  computer vision community
In this paper, we focused on  four different tasks in this area, where the inference relies  only on a single RGB image: (N) volume estimation, (N)  content estimation, (N) comparative volume estimation, and  (N) pouring prediction
We introduced the COQE dataset to  train and evaluate our models
In the future, we plan to consider liquid attributes such as viscosity for more accurate  prediction of pouring
Moreover, we plan to incorporate  other modalities so we can perform more sophisticated reasoning in scenarios that the visual cues alone are not enough  (e.g., opaque containers)
 Acknowledgements: This work is in part supported by  ONR N000NN-NN-N-0NN0, NSF IIS-NNNN0NN, NSF NRINNNNNNN, NSF IIS-NNNN0NN, NSF NRI-NNNNNNN, Allen Distinguished Investigator Award, and the Allen Institute for  Artificial Intelligence
We would also like to thank Aaron  Walsman for his help with preparing the ND CAD models
 NNNN    References  [N] P
Agrawal, A
Nair, P
Abbeel, J
Malik, and S
Levine
 Learning to poke by poking: Experiential learning of intuitive physics
In NIPS, N0NN
N  [N] H
Bagherinezhad, H
Hajishirzi, Y
Choi, and A
Farhadi
 Are elephants bigger than butterflies? reasoning about sizes  of objects
In AAAI, N0NN
N  [N] C
J
Bates, I
Yildirim, J
B
Tenenbaum, and P
W
Battaglia
 Humans predict liquid dynamics using probabilistic simulation
In CogSci, N0NN
N  [N] S
Brandl, O
Kroemer, and J
Peters
Generalizing pouring  actions between objects using warped parameters
In HUMANOIDS, N0NN
N  [N] R
Bridson
Fluid simulation for computer graphics
CRC  Press, N0NN
N  [N] M
A
Brubaker, L
Sigal, and D
J
Fleet
Estimating contact  dynamics
In ICCV, N00N
N  [N] J
Call and P
Rochat
Perceptual strategies in the estimation of physical quantities by orangutans
J
of Comparative  Psychology, NNNN
N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected crfs
In ICLR, N0NN
N  [N] A
G
Cohn and S
M
Hazarika
Qualitative spatial representation and reasoning: An overview
Fundamenta informaticae, N00N
N  [N0] J
W
Collins and K
D
Forbus
Reasoning about fluids via  molecular collections
In AAAI, NNNN
N  [NN] E
Delage, H
Lee, and A
Y
Ng
A dynamic bayesian network model for autonomous Nd reconstruction from a single  indoor image
In CVPR, N00N
N  [NN] D
Eigen, C
Puhrsch, and R
Fergus
Depth map prediction  from a single image using a multi-scale deep network
In  NIPS, N0NN
N  [NN] M
Fritz, M
J
Black, G
R
Bradski, S
Karayev, and T
Darrell
An additive latent feature model for transparent object  recognition
In NIPS, N00N
N  [NN] R
Girshick
Fast r-cnn
In International Conference on Computer Vision (ICCV), N0NN
N  [NN] S
Gupta, P
Arbelaez, R
Girshick, and J
Malik
Aligning Nd  models to rgb-d images of cluttered scenes
In CVPR, N0NN
 N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N, N  [NN] S
J
Hespos, A
L
Ferry, and L
J
Rips
Five-month-old infants have different expectations for solids and liquids
Psychological Science, N00N
N  [NN] S
J
Hespos and E
Spelke
Precursors to spatial language:  The case of containment
The categorization of spatial entities in language and cognition, N00N
N  [NN] D
Hoiem, A
A
Efros, and M
Hebert
Putting objects in  perspective
In CVPR, N00N
N  [N0] T
Kim, N
Thürey, D
James, and M
Gross
Wavelet turbulence for fluid simulation
In ACM Trans
on Graphics, N00N
 N  [NN] J
Kubricht, C
Jiang, Y
Zhu, S.-C
Zhu, D
Terzopoulos, and  H
Lu
Probabilistic simulation predicts human performance  on viscous fluid-pouring problem
In CogSci, N0NN
N  [NN] L
Kunze and M
Beetz
Envisioning the qualitative effects  of robot manipulation actions using simulation-based projections
Artificial Intelligence, N0NN
N  [NN] A
Lerer, S
Gross, and R
Fergus
Learning physical intuition of block towers by example
In ICML, N0NN
N  [NN] W
Li, S
Azimi, A
Leonardis, and M
Fritz
To fall or not  to fall: A visual approach to physical stability prediction
In  ArXiv, N0NN
N  [NN] W
Liang, Y
Zhao, Y
Zhu, and S.-C
Zhu
What is where:  Inferring containment relations from videos
In IJCAI, N0NN
 N  [NN] W
Liang, Y
B
Zhao, Y
Zhu, and S
Zhu
Evaluating human  cognition of containing relations with physical simulation
In  CogSci, N0NN
N  [NN] D
Lin, S
Fidler, and R
Urtasun
Holistic scene understanding for Nd object detection with rgbd cameras
In ICCV,  N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollr, and C
L
Zitnick
Microsoft coco: Common  objects in context
In ECCV, N0NN
N  [NN] B
Liu, S
Gould, and D
Koller
Single image depth estimation from predicted semantic labels
In CVPR, N0N0
N  [N0] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
N  [NN] R
Mottaghi, H
Bagherinezhad, M
Rastegari, and  A
Farhadi
Newtonian image understanding: Unfolding the  dynamics of objects in static images
In CVPR, N0NN
N  [NN] R
Mottaghi, M
Rastegari, A
Gupta, and A
Farhadi
“what  happens if...” learning to predict the effect of forces in images
In ECCV, N0NN
N, N, N  [NN] M
Müller, D
Charypar, and M
Gross
Particle-based  fluid simulation for interactive applications
In Proc
of the  N00N ACM SIGGRAPH/Eurographics symposium on Computer animation, N00N
N  [NN] T.-H
Pham, A
Kheddar, A
Qammaz, and A
A
Argyros
 Towards force sensing from vision: Observing hand-object  interactions to infer manipulation forces
In CVPR, N0NN
N  [NN] C
J
Phillips, M
Lecce, and K
Daniilidis
Seeing glassware:  from edge detection to pose estimation and shape recovery
 In RSS, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN: Towards real-time object detection with region proposal networks
In NIPS, N0NN
N  [NN] L
D
Rozo, P
Jiménez, and C
Torras
Force-based robot  learning of pouring skills using parametric hidden markov  models
In RoMoCo, N0NN
N  [NN] C
Schenck and D
Fox
Visual closed-loop control for pouring liquids
In arXiv, N0NN
N  [NN] D
L
Schwartz and T
Black
Inferences through imagined  actions: knowing by simulated doing
Journal of Experimental Psychology: Learning, Memory, and Cognition, NNNN
N  [N0] S
Song and J
Xiao
Sliding shapes for Nd object detection  in depth images
In ECCV, N0NN
N  NNNN    [NN] M
Stark, J
Krause, B
Pepik, D
Meger, J
J
Little,  B
Schiele, and D
Koller
Fine-grained categorization for  Nd scene understanding
In BMVC, N0NN
N  [NN] B
Strickland and B
J
Scholl
Visual perception involves  event-type representations: The case of containment versus  occlusion
Journal of Experimental Psychology: General,  N0NN
N  [NN] M
Tamosiunaite, B
Nemec, A
Ude, and F
Wörgötter
 Learning to pour with a robot arm combining goal and shape  learning for dynamic movement primitives
Robotics and  Autonomous Systems, N0NN
N  [NN] J
Walker, A
Gupta, and M
Hebert
Dense optical flow prediction from a static image
In ICCV, N0NN
N  [NN] X
Wang, D
F
Fouhey, and A
Gupta
Designing deep networks for surface normal estimation
In CVPR, N0NN
N  [NN] J
Wu, I
Yildirim, J
J
Lim, W
T
Freeman, and J
B
Tenenbaum
Galileo: Perceiving physical object properties by integrating a physics engine with deep learning
In NIPS, N0NN
 N  [NN] A
Yamaguchi and C
G
Atkeson
Differential dynamic programming with temporally decomposed dynamics
In Humanoids, N0NN
N  [NN] M
Ye, Y
Zhang, R
Yang, and D
Manocha
Nd reconstruction in the presence of glasses by acoustic and stereo fusion
 In CVPR, N0NN
N  [NN] L.-F
Yu, N
Duncan, and S.-K
Yeung
Fill and transfer: A  simple physics-based approach for containability reasoning
 In ICCV, N0NN
N  [N0] S
Zagoruyko, A
Lerer, T.-Y
Lin, P
O
Pinheiro, S
Gross,  S
Chintala, and P
Dollár
A multipath network for object  detection
In BMVC, N0NN
N, N  [NN] Y
Zhu, C
Jiang, Y
Zhao, D
Terzopoulos, and S.-C
Zhu
 Inferring forces and learning human utilities from videos
In  CVPR, N0NN
N  [NN] Y
Zhu, Y
Zhao, and S.-C
Zhu
Understanding tools:  Task-oriented object modeling, learning and recognition
In  CVPR, N0NN
N  NNN0SCNet: Learning Semantic Correspondence   SCNet: Learning Semantic Correspondence  Kai HanN Rafael S
RezendeN,N Bumsub HamN Kwan-Yee K
WongN  Minsu ChoN Cordelia SchmidN,∗ Jean PonceN,N  NThe University of Hong Kong NYonsei Univ
NPOSTECH NInria NDepartment of Computer Science, ENS / CNRS / PSL Research University  Abstract  This paper addresses the problem of establishing semantic correspondences between images depicting different instances of the same object or scene category
Previous  approaches focus on either combining a spatial regularizer with hand-crafted features, or learning a correspondence model for appearance only
We propose instead a  convolutional neural network architecture, called SCNet,  for learning a geometrically plausible model for semantic  correspondence
SCNet uses region proposals as matching primitives, and explicitly incorporates geometric consistency in its loss function
It is trained on image pairs obtained from the PASCAL VOC N00N keypoint dataset, and  a comparative evaluation on several standard benchmarks  demonstrates that the proposed approach substantially outperforms both recent deep learning architectures and previous methods based on hand-crafted features
 N
Introduction  Our goal in this paper is to establish semantic correspondences across images that contain different instances  of the same object or scene category, and thus feature much  larger changes in appearance and spatial layout than the pictures of the same scene used in stereo vision, which we take  here to include broadly not only classical (narrow-baseline)  stereo fusion (e.g., [NN, NN]), but also optical flow computation (e.g., [NN, NN, NN]) and wide-baseline matching  (e.g., [N0, NN])
Due to such a large degree of variations,  the problem of semantic correspondence remains very challenging
Most previous approaches to semantic correspondence [N, NN, N0, NN, NN, NN] focus on combining an effective spatial regularizer with hand-crafted features such as  SIFT [NN], DAISY [NN] or HOG [N]
With the remarkable  success of deep learning approaches in visual recognition,  several learning-based methods have also been proposed for  ∗Univ
Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, NN000  Grenoble, France
 Figure N: Learning semantic correspondence
We propose  a convolutional neural network, SCNet, to learn semantic  correspondence using both appearance and geometry
This  allows us to handle a large degree of intra-class and scene  variations
This figure shows a pair of input images (top)  and a warped image (bottom) using its semantic correspondence by our method
(Best viewed in color.)  both stereo vision [N, NN, NN, NN] and semantic correspondence [N, NN, N0]
Yet, none of these methods exploits the  geometric consistency constraints that have proven to be  a key factor to the success of their hand-crafted counterparts
Geometric regularization, if any, occurs during postprocessing but not during learning (e.g., [NN, NN])
 In this paper we propose a convolutional neural network (CNN) architecture, called SCNet, for learning geometrically plausible semantic correspondence (Figure N)
 Following the proposal flow approach to semantic correspondence of Ham et al
[NN], we use object proposals [NN, N0, NN] as matching primitives, and explicitly incorporate the geometric consistency of these proposals in  our loss function
Unlike [NN] with its hand-crafted features, however, we train our system in an end-to-end manner  using image pairs extracted from the PASCAL VOC N00N  keypoint dataset [N]
A comparative evaluation on several  NNNNN    standard benchmarks demonstrates that the proposed approach substantially outperforms both recent deep architectures and previous methods based on hand-crafted features
 Our main contributions can be summarized as follows:  • We introduce a simple and efficient model for learning to match regions using both appearance and geometry
 • We propose a convolutional neural network, SCNet, to learn semantic correspondence with region proposals
 • We achieve state-of-the-art results on several bench- marks, clearly demonstrating the advantage of learning  both appearance and geometric terms
 N
Related work  Here we briefly describe representative approaches related to semantic correspondence
 Semantic correspondence
SIFT Flow [NN] extends classical optical flow to establish correspondences across similar but different scenes
It uses dense SIFT descriptors  to capture semantic information beyond naive color values, and leverages a hierarchical optimization technique in  a coarse-to-fine pipeline for efficiency
Kim et al
[N0]  and Hur et al
[NN] propose more efficient generalizations  of SIFT Flow
Instead of using SIFT features, Yang et  al
[NN] use DAISY [NN] for an efficient descriptor extraction
Inspired by an exemplar-LDA approach [NN], Bristow et al
[N] use whitened SIFT descriptors, making semantic correspondence robust to background clutter
Recently, Ham et al
[NN] introduces proposal flow that uses  object proposals as matching elements for semantic correspondence robust to scale and clutter
This work shows that  the HOG descriptor gives better matching performance than  deep learning features [NN, NN]
Taniai et al
[NN] also use  HOG descriptors, and show that jointly performing cosegmentation and establishing dense correspondence are helpful in both tasks
Despite differences in feature descriptors  and optimization schemes, these semantic correspondence  approaches use a spatial regularizer to ensure flow smoothness on top of hand-crafted or pre-trained features
 Deep learning for correspondence
Recently, CNNs  have been applied to classical dense correspondence problems such as optical flow and stereo matching to learn feature descriptors [NN, NN, NN] or similarity functions [NN, NN,  NN]
FlowNet [N] uses an end-to-end scheme to learn optical flow with a synthetic dataset, and several recent approaches also use supervision from reconstructed ND scenes  and stereo pairs [NN, NN, NN, NN]
MC-CNN [NN] and its efficient extension [NN] train CNN models to predict how well  two image patches match and use this information to compute the stereo matching cost
DeepCompare [NN] learns  a similarity function for patches directly from images of  a ND scene, which allows for various types of geometric  and photometric transformations (e.g., rotation and illumination changes)
These approaches are inherently limited  to matching images of the same physical object/scene
In  contrast, Long et al
[NN] use CNN features pre-trained for  ImageNet classification tasks (due to a lack of available  datasets for learning semantic correspondence) with performance comparable to SIFT flow
To overcome the difficulty in obtaining ground truth for semantic correspondence, Zhou et al
[N0] leverage ND models, and uses flow  consistency between ND models and ND images as a supervisory signal to train a CNN
Another approach to generating ground truth is to directly augment the data by densifying sparse keypoint annotations using warping [NN, NN]
 The universal correspondence network (UCN) of Choy et  al
[N] learns semantic correspondence using an architecture similar to [NN], but adds a convolutional spatial transformer networks for improved robustness to rotation and  scale changes
Kim et al
[NN] introduce a convolutional  descriptor using self-similarity, called fully convolutional  self-similarity (FCSS), and combine the learned semantic  descriptors with the proposal flow [NN] framework
These  approaches to learning semantic correspondence [N, N0] or  semantic descriptors [NN] typically perform better than traditional hand-crafted ones
Unlike our method, however,  they do not incorporate geometric consistency between regions or object parts in the learning process
 N
Our approach  We consider the problem of learning to match regions  with arbitrary positions and sizes in pairs of images
This  setting is general enough to cover all cases of region sampling used in semantic correspondence: sampling a dense  set of regular local regions as in typical dense correspondence [N, N0, NN, NN] as well as employing multi-scale object  proposals [N, NN, NN, N0, NN]
In this work, following proposal flow [NN], we focus on establishing correspondences  between object proposal boxes
 N.N
Model  Our basic model for matching starts from the probabilistic Hough matching (PHM) approach of [N, NN]
In a nutshell, given some potential match m between two regions,  and the supporting data D (a set of potential matches), the  PHM model can be written as  P (m|D) = ∑  x  P (m|x,D)P (x|D)  = Pa(m) ∑  x  Pg(m|x)P (x|D), (N)  where x is the offset (e.g., position and scale change) between all potential matches m = [r, s] of two regions r and s
Pa(m) is the probability that the match between two  NNNN    regions is correct based on appearance only, and Pg(m|x) is the probability based on geometry only, computed using  the offset xN
PHM computes a matching score by replacing  geometry prior P (x|D) with the Hough voting h(x|D) [N]:  h(x|D) = ∑  m′∈D  Pa(m ′)Pg(m  ′|x)
(N)  This turns out to be an effective spatial matching model that  combines appearance similarity with global geometric consistency measured by letting all matches vote on the potential offset x [N, NN]
 In our learning framework, we consider similarities  rather than probabilities, and rewrite the PHM score for the  match m as  z(m,w) = f(m,w) ∑  x  g(m,x) ∑  m′∈D  f(m′, w)g(m′, x)  = f(m,w) ∑  m′∈D  [ ∑  x  g(m,x)g(m′, x)]f(m′, w),  (N)  where f(m,w) is a parameterized appearance similarity function between the two regions in the potential match m,  x is as before an offset variable (position plus scale), and  g(m,x) measures the geometric compatibility between the match m and the offset x
 Now assuming that we have a total number of n potential  matches, and identifying matches with their indices, we can  rewrite this score as  z(m,w) = f(m,w) ∑  m′  Kmm′f(m ′, w),  where Kmm′ = ∑  x  g(m,x)g(m′, x), (N)  and the n × n matrix K is the kernel matrix associated with the feature vector ϕ(m) = [g(m,xN), 


, g(m,xs)]  T ,  where xN to xs form the finite set of values that the offset  variable x runs over: indeed Kmm′ = ϕ(m) · ϕ(m ′).N  Given training pairs of images with associated true and  false matches, we can learn our similarity function by minimizing with respect to w  E(w) =  n ∑  m=N  l[ym, z(m,w)] + λΩ(w), (N)  where l is a loss function, ym is the the ground-truth label (either N [true] or 0 [false]) for the match m, and Ω is a regularizer (e.g., Ω(w) = ||w||N)
We use the hinge loss and LN regularizer in this work
Finally, at test time,  we associate any region r with the region s maximizing  z([r, s], w∗), where w∗ is the set of learned parameters
 NWe suppose that appearance matching is independent of geometry  matching and the offset
NPutting it all together in an n-vector of scores, this can also be rewritten as z(w) = f(w)⊙Kf(w), where z(w) = (z(N, w), 


, z(n,w))T , “⊙” stands for the elementwise product between vectors, and f(w) = (f(N, w), 


, f(n,w))T 
 N.N
Similarity function and geometry kernel  There are many possible choices for the function f that  computes the appearance similarity of the two regions r and  s making up match number m
Here we assume a trainable  embedding function c (as will be shown later, c will be the  output of a CNN in our case) that outputs a LN normalized  feature vector
For the appearance similarity between two  regions r and s, we then use a rectified cosine similarity:  f(m,w) = max(0, c(r, w) · c(s, w)), (N)  that sets all negative similarity values to zero, thus making  the similarity function sparser as well as insensitive to negative matches during training, with the additional benefit of  giving nonnegative weights in Eq
(N)
 Our geometry kernel Kmm′ records the fact that two  matches (roughly) correspond to the same offset: Concretely, we discretize the set of all possible offsets into bins
 Let us denote by h the function mapping a match m onto  the corresponding bin x, we now define g by  g(m,x) =  {  N, if h(m) = x  0, otherwise
(N)  Thus, the kernel Kmm′ simply measures whether two  matches share the same offset bin or not:  Kmm′ =  {  N, if h(m) = h(m′)  0, otherwise
(N)  In practice, x runs over a grid of predefined offset values,  and h(m) assigns match m to the nearest offset point
Our kernel is sparse, which greatly simplifies the computation  of the score function in Eq
(N): Indeed, let Bx denote the  set of matches associated with the bin x, the score function  z reduces to  z(m,w) = f(m,w) ∑  m′∈Bh(m)  f(m′, w)
(N)  This trainable form of the PHM model from [N, NN] can be  used within Eq
(N)
 Note that since our simple geometry kernel is only dependent on matches’ offsets, we obtain the same geometry  term value of ∑  m′∈Bh(m) f(m′, w) for any match m that  falls into the same bin h(m)
This allows us to compute this geometry term value only once for each non-empty bin x  and then share it for multiple matches in the same bin
This  sharing makes computing z several times faster in practice.N  N.N
Gradient-based learning  The feature embedding function c(m,w) in the model above can be implemented by any differentiable architecture, for example a CNN-based one, and the score function  NIf the geometry kernel is dependent on something other than offsets,  e.g., matches’ absolute position or their neighborhood structure, this sharing is not possible
 NNNN    Conv layers   (rN, …, rp)  IA  FA  ROI  pooling  c (rN,w)  …  c (rp,w)   Conv layers   (sN, …, sp)  IB  FB  ROI  pooling  FC LNNorm  c (sN,w)  …  c (sp,w)   z (mN,w)  …  z (mn,w)  ×K  ReLU  f (mN,w)  …  f (mn,w)  FCg LNNorm  FC LNNorm  FCg LNNorm  ReLU  fg (mN,w)  …  fg (mn,w)   cg (rN,w)  …  cg (rp,w)   cg (sN,w)  …  cg (sp,w)   Figure N: The SCNet architectures
Three variants are proposed: SCNet-AG, SCNet-A, and SCNet-AG+
The basic architecture, SCNet-AG, is drawn in solid lines
Colored boxes represent layers with learning parameters and the boxes with the same  color share the same parameters
“×K” denotes the voting layer for geometric scoring
A simplified variant, SCNet-A, learns appearance information only by making the voting layer an identity function
An extended variant, SCNet-AG+, contains an  additional stream drawn in dashed lines
SCNet-AG learns a single embedding c for both appearance and geometry, whereas  SCNet-AG+ learns an additional and separate embedding cg for geometry
See text for details
(Best viewed in color.)  z can be learned using stochastic gradient descent
Let us  now consider the problem of minimizing the objective function E(w) defined by Eq
(N).N This requires computing the gradient with respect to w of the function z:  ∇z(m,w) = [ ∑  m′∈D  Kmm′f(m ′, w)]∇f(m,w)  +f(m,w) ∑  m′∈D  Kmm′∇f(m ′, w)
(N0)  Denoting by n the size of D, this involves n evaluations  of both f and ∇f 
Computing the full gradient of E thus requires at most nN evaluations of both f and ∇f , which be- comes computationally intractable when n is large enough
 The score function of Eq
(N) with the sparse kernel of  Eq
(N), however, greatly reduces the gradient computation:  ∇z(m,w) = [ ∑  m′∈Bh(m)  f(m′, w)]∇f(m,w)  +f(m,w) ∑  m′∈Bh(m)  ∇f(m′, w)
(NN)  Note that computing the gradient for match m involves only  a small set of matches falling into the same offset bin h(m)
 N
SCNet architecture  Among many possible architectures implementing the  proposed model, we propose using a convolutional neural  network (CNN), dubbed SCNet, that efficiently processes  NWe take Ω(w) = 0 for simplicity in this section, but tackling a nonzero regularizer is easy
 regions and learns our matching model
Three variants,  SCNet-AG, SCNet-A, SCNet-AG+, are illustrated in Fig
N
 In each case, SCNet takes as input two images IA and IB , and maps them onto feature maps FA and FB by CNN layers
Given region proposals (rN, 


, rp) and (sN, 


, sp) for the two images, parallel ROI pooling lay- ers [N0, NN] extract feature maps of the same size for each  proposal
This is an efficient architecture that shares convolutional layers over all region proposals
 SCNet-AG
The proposal features are fed into a fullyconnected layer, mapped onto feature embedding vectors, and normalized into unit feature vectors c(ri, w) and c(sj , w), associated with the regions ri and sj of IA and IB , respectively
The value of f(m,w) for the match m associated with regions ri and sj is computed as the rectified dot product of c(ri) and c(sj) (Eq
(N)), which de- fines the appearance similarity f(m,w) for match m
Ge- ometric consistency is enforced with the kernel described  in Sec
N.N, using a voting layer, denoted as “×K”, that computes score z(m,w) from the appearance similarity and geometric consensus of proposals
Finally, matching is performed by identifying the maximal z(m,w) scores, using both appearance and geometric similarities
 SCNet-A
We also evaluate a similar architecture without  the geometry term
This architecture drops the voting layer  (denoted by ×K in Fig
N) from SCNet-AG, directly using f(m,w) as a score function
This is similar to the universal correspondence network (UCN) [N]
The main differences  are the use of object proposals and the use of a different loss  function
 NNNN    SCNet-AG+
Unlike SCNet-AG, which learns a single  embedding c for both appearance and geometry, SCNetAG+ learns an additional and separate embedding cg for  geometry that is implemented by an additional stream in  the SCNet architecture (dashed lines in Fig
N)
This corresponds to a variant of Eq
(N), as follows:  z+(m,w) = f(m,w) ∑  m′∈Bh(m)  fg(m ′, w), (NN)  where fg is the rectified cosine similarity computed by cg 
 Compared to the original score function, this variant allows  the geometry term to learn a separate embedding function  for geometric scoring
This may be beneficial particularly  when a match’s contribution to geometric scoring needs to  be different from the appearance score
For example, a  match of rigid object parts (wheel of cars) may contribute  more to geometric scoring than that of deformable object  parts (leg of horses)
The separate similarity function fg allows more flexibility in learning the geometric term
 Implementation details
We use the VGGNN [NN] model  that consists of a set of convolutional layers with N × N fil- ters, a ReLU layer and a pooling layer
We find that taking the first N convolutional layers is a good trade-off for  our semantic feature extraction purpose without loosing localization accuracy
These layers output features with NNN channels
For example, if the net takes input of NNN×NNN×N images, the convolutional layers produce features with the  size of NN×NN×NNN
For the ROI pooling layer, we choose a N × N filter following the fast R-CNN architecture [N0], which produces a feature map with size of N × N × NNN for each proposal
To transform the feature map for each  proposal into a feature vector, we use the FC layer with a  size of N× N× NNN× N0NN
The N0NN dimensional feature vector associated with each proposal are then fed into the  LN normalization layer, followed by the dot product layer,  ReLU, our geometric voting layer, and loss layer
The convolutional layers are initialized by the pretrained weights  of VGGNN and the fully connected layers have random initialization
We train our SCNet by mini-batch SGD, with  learning rate 0.00N, and weight decay 0.000N
During train- ing, each mini-batch arises from a pair of images associated with a number of proposals
In our implementation,  we generated N00 proposals for each image, which leads to N00× N00 potential matches
 For each mini-batch, we sample matches for training as  follows
(N) Positive sampling: For a proposal ri in IA, we  are given its ground truth match r′i in IB 
We pick all the  proposals sj in IB with IoU(sj , r ′  i) > Tpos to be positive matches for ri
(N) Negative sampling: Assume we obtain  k positive pairs w.r.t ri
We also need to have k negative  pairs w.r.t ri
To achieve this, we first find the proposals  st in IB with IoU(st, r ′  i) < Tneg 
Assuming p proposals  satisfying the IoU constraint, we find the proposals with top  k appearance similarity with ri among those p proposals
In  our experiment, we set Tpos = 0.N, and Tneg = 0.N
 N
Experimental evaluation  In this section we present experimental results and analysis
Our code and models will be made available online:  http://www.di.ens.fr/willow/research/scnet/
 N.N
Experimental details  Dataset
We use the PF-PASCAL dataset that consists of  NN00 image pairs selected from PASCAL-Berkeley keypoint annotationsN of N0 object classes
Each pair of images in PF-PASCAL share the same set of non-occluded  keypoints
We divide the dataset into N00 training pairs,  N00 validation pairs, and N00 testing pairs
The image pairs  for training/validation/testing are distributed proportionally  to the number of image pairs of each object class
In training, we augment the data into a total of NN00 pairs by horizontal mirroring
We also test our trained models with the  PF-WILLOW dataset [NN], Caltech-N0N [N] and PASCAL  Parts [NN] to further validate a generalization of the models
 Region proposal
Unless stated otherwise, we choose to  use the method of Manen et al
(RP) [NN]
The use of RP  proposals is motivated by the superior result reported in  [NN], which is verified once more by our evaluation
In testing we use N000 proposals for each image as in [NN], while  in training we use N00 proposals for efficiency
 Evaluation metric
We use three metrics to compare the  results of SCNet to other methods
First, we use the probability of correct keypoint (PCK) [NN], which measures the  precision of dense flow at sparse keypoints of semantic relevance
It is calculated on the Euclidean distance d(φ(p), p∗) between a warped keypoint φ(p) and ground-truth one p∗N
Second, we use the probability of correct regions (PCR) introduced in [NN] as an equivalent of the the PCK for region  based correspondence
PCR measures the precision of a region matching between region r and its correspondent r∗ on  the intersection over union (IoU) score N − IoU(φ(r), r∗)
Both metrics are computed against a threshold τ in [0, N] and we measure PCK@τ and PCR@τ as the percentage  correct below τ 
Third, we capture the quality of matching  proposals by the mean IoU of the top k matches (mIoU@k)
 Note that these metrics are used to evaluate two different  types of correspondence
Indeed, PCK is an evaluation metric for dense flow field, whereas PCR and mIoU@k are used  to evaluate region-based correspondences [NN]
 Nhttp://www.di.ens.fr/willow/research/proposalflow/ NTo better take into account the different sizes of images, we normalize  the distance by dividing by the diagonal of the warped image, as in [N]  NNNN  http://www.di.ens.fr/willow/research/scnet/   IoU threshold 0 0.N 0.N 0.N 0.N N  P C  R  0  0.N  0.N  0.N  0.N  N PCR for SCNet architecture vs Proposal Flow  SCNet-A [0.NN] SCNet-AG [0.NN] SCNet-AG+ [0.N0] NAM  HOG  [0.NN]  PHM HOG   [0.NN]  LOM HOG   [0.NN]  IoU threshold 0 0.N 0.N 0.N 0.N N  P C  R  0  0.N  0.N  0.N  0.N  N PCR for SCNet descriptor vs HOG  NAM SCNet   [0.NN]  PHM SCNet   [0.NN]  LOM SCNet   [0.NN]  NAM HOG   [0.NN]  PHM HOG   [0.NN]  LOM HOG   [0.NN]  IoU threshold 0 0.N 0.N 0.N 0.N N  P C  R  0  0.N  0.N  0.N  0.N  N PCR for SCNet vs VGG  SCNet-A [0.NN] SCNet-AG [0.NN] SCNet-AG+ [0.N0] VGG [0.NN] VGG-LN [0.NN] VGG-LN-FC [0.NN]  IoU threshold 0 0.N 0.N 0.N 0.N N  P C  R  0  0.N  0.N  0.N  0.N  N PCR for object proposals (SCNet-AG+)  US [0.NN] SW [0.NN] SS [0.NN] RP [0.N0]  Number of top matches, k N0 N0 N0 N0 N00  m Io  U @  k  0.N  0.N  0.N  0.N  0.N  0.N mIoU@k for SCNet architecture vs Proposal Flow  SCNet-A [NN.N] SCNet-AG [NN.N] SCNet-AG+ [NN.N] NAM  HOG  [NN.N]  PHM HOG   [NN.N]  LOM HOG   [NN.0]  Number of top matches, k N0 N0 N0 N0 N00  m Io  U @  k  0.N  0.N  0.N  0.N  0.N  0.N mIoU@k for SCNet descriptor vs HOG  NAM SCNet   [NN.N]  PHM SCNet   [N0.N]  LOM SCNet   [NN.N]  NAM HOG   [NN.N]  PHM HOG   [NN.N]  LOM HOG   [NN.0]  Number of top matches, k N0 N0 N0 N0 N00  m Io  U @  k  0 0.N 0.N 0.N 0.N 0.N 0.N 0.N  mIoU@k for SCNet vs VGG  SCNet-A [NN.N] SCNet-AG [NN.N] SCNet-AG+ [NN.N] VGG [N0.N] VGG-LN [NN.N] VGG-LN-FC [NN.N]  Number of top matches, k N0 N0 N0 N0 N00  m Io  U @  k  0.N 0.N 0.N 0.N 0.N 0.N  mIoU@k for object proposals (SCNet-AG+)  US [N0.N] SW [N0.N] SS [NN.N] RP [NN.N]  (a) (b) (c) (d)  Figure N: (a) Performance of SCNet on PF-PASCAL, compared to Proposal Flow methods [NN]
(b) Performance of SCNet  and HOG descriptors on PF-PASCAL, evaluated using Proposal Flow methods [NN]
(c) Comparison to ImageNet-trained  baselines
(d) Comparison of different proposals
PCR and mIoU@k plots are shown at the top and bottom, respectively
 AuC is shown in the legend
(Best viewed in pdf.)  N.N
Proposal flow components  We use the PF-PASCAL dataset to evaluate region  matching performance
This setting allows our method to  be tested against three other methods in [NN]: NAM, PHM  and LOM
NAM finds correspondences using handcrafted  features only
PHM and LOM additionally consider global  and local geometric consistency, respectively, between region matchings
We also compare our SCNet-learned feature against whitened HOG [N], the best performing handcraft feature of [NN]
Experiments on the PF-WILLOW  dataset [NN] showed similar results with the ones on the PFPASCAL dataset
For details, refer to our project webpage
 Quantitative comparison
Figure N(a) compares SCNet  methods with the proposal flow methods [NN] on the PFPASCAL dataset
Our SCNet models outperform the  other methods that use the HOG feature
Our geometric models (SCNet-AG, SCNet-AG+) substantially outperform the appearance-only model (SCNet-A), and SCNetAG+ slightly outperform SCNet-AG
This can also be seen  from the area under curve (AuC) presented in the legend
 This clearly show the effectiveness of deep learned features  as well as geometric matching
In this comparison, we fix  the VGGNN layer and only learn the FC layers
In our experiment, we also learned all layers including VGG NN and  the FC layers in our model (fully finetuned), but the improvement over the partially learned model was marginal
 Figure N(b) shows the performance of NAM, PHM, LOM  matching when replacing HOG feature with our learned feature in SCNet-A
We see that SCNet features greatly improves all the matching methods
Interestingly, LOM using SCNet feature outperforms our best performing SCNet model, SCNet-AG+
However, the LOM method is  more than N0 times slower than SCNet-AG+: on average  the method takes 0.NNs for SCNet-A feature extraction and N.NNs for the actual matching process, whereas our SCNet- AG+ only takes 0.NNs in total
Most of the time in LOM is spent in computing its geometric consistency term
We further evaluated three additional baselines using ImageNettrained VGG (see Figure N(c))
Namely (i) VGG: We directly use the features from ImageNet-trained VGG, followed by ROI-pooling to make the features for each proposal of the same size (N × N × NNN)
We then flatten the features into vectors of dimension NNNNNN
(ii) VGG-LN:  We lN-normalize the flattened feature of (i)
(iii) VGG-LNFC: We perform a random projection from (ii) to a feature  of dimension N0NN (the same dimension with SCNet, NN.NN times smaller than (i) and (ii)) by adding a randomly initialized FC layer on top of (ii)
Note that this is exactly equivalent to SCNet-A without training on the target dataset
 Results with different object proposals
SCNet can be  combined with any region proposal methods
In this experiment, we train and evaluate SCNet-AG+ on PF-PASCAL  with four region proposal methods: randomized prim (RP)  [NN], selective search (SS) [NN], random uniform sampling (US), and sliding window (SW)
US and SW are extracted using the work of [NN], and SW is similar to regular  grid sampling used in other popular methods [N0, NN, NN]
 Figure N(d) compares matching performance in PCR and  mIoU@k when using the different proposals
RP performs  best, and US performs worst with a large margin
This  NNNN    bike image pair NAMHOG [NN] SCNet-A [N0N] SCNet-AG+ [N0N]  wine bottle image pair NAMHOG [NN] SCNet-A [NNN] SCNet-AG+ [NN0]  Figure N: Region matching examples
Numbers beside methods stand for numbers of correct matches
 Table N: Per-class PCK on PF-PASCAL at τ = 0.N
For all methods using object proposals, we use N000 RP proposals [NN]
 Method aero bike bird boat bottle bus car cat chair cow d.table dog horse moto person plant sheep sofa train tv mean  NAMHOG [NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N  PHMHOG [NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N N0.0 NN.N N0.N  LOMHOG [NN] NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N  UCN [N] NN.N NN.N NN.N NN.N NN.0 NN.N NN.0 NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.0 NN.0 NN.0 NN.N  SCNet-A NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  SCNet-AG NN.N NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N N0.0 NN.N NN.N N0.0 NN.N  SCNet-AG+ NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N N0.N N0.0 NN.N NN.N NN.N NN.N  shows that the region proposal process is an important factor for matching performance
 Qualitative comparison
Region matching results for  NAM, SCNet-A, and SCNet-AG+ are shown in Figure N
 In this example, at the IoU threshold 0.N, the numbers of correct matches are shown for all methods
We can see that  SCNet models perform significantly better than NAM with  HOG feature, and SCNet-A is outperformed by SCNetAG+ that learns a geometric consistency term
 N.N
Flow field  Given a sparse region matching result and its corresponding scores, we generate dense semantic flow using a  densifying technique in [NN]
In brief, we select out a region  match with the highest score, and assign dense correspondences to the pixels within the matched regions by linear  interpolation
This process is repeated without replacement  of the region match until we assign dense correspondences  to all pixels in the source image
The results are evaluated  on PF-PASCAL dataset
To evaluate transferability performance of the models, we also test them on other datasets  such as PF-WILLOW [NN], Caltech-N0N [N] and PASCAL  Parts [NN] datasets, and compare with state-of-the-art results  on these datasets
In these cases direct comparison between  learning-based methods may not be fair in the sense that  they are trained on different datasets
 Results on PF-PASCAL
We compare SCNet with Proposal Flow [NN] and UCN [N] on the PF-PASCAL dataset,  and summarize the result in Table N
The UCN is retrained  using the code provided by the authors on the PF-PASCAL  dataset for fair comparison
Using the raw network of [N]  trained on a different subset of PASCAL yields as expected  lower performance, with a mean PCK of NN.0 as opposed to  the NN.N obtained for the retrained network
The three variants of SCNet do consistently better than UCN as well as  all methods in [NN], with a PCK of NN.N or above
Among  all the methods, SCNet-AG+ performs best with a PCK of  NN.N
Figure N presents two examples of dense matching  for PF-PASCAL
Ground truth are presented as circles and  predicted keypoints are presented as crosses
We observe a  better performance of SCNet-AG and SCNet-AG+
 Results on PF-WILLOW
For evaluating transferability,  we test (PF-PASCAL trained) SCNet and UCN on the PFWILLOW dataset [NN] and compare the results with recent  methods in Table N where PCK is averaged over all classes
 The postfix ‘w/SF’ and ‘w/PF’ represent that matching is  performed by SIFT Flow [NN] and Proposal Flow [NN], respectively
On this dataset where the data has a different distribution, SCNet-AG slightly outperforms the A and AG+  variants (PCK@0.0N)
We observe that all SCNet models significantly outperform UCN, which is trained on the same  dataset with the SCNet models, as well as other methods  NNNN    Source Target NAM SCNet-A SCNet-AG+  Target LOM SCNet-A  Source Target NAMHOG LOMHOG SCNet-A SCNet-AG+  Figure N: Quantitative comparison of dense correspondence
We show the keypoints of the target image in circles and the  predicted keypoints of the source in crosses, with a vector that depicts the matching error
(Best viewed in pdf.)  Table N: Fixed-threshold PCK on PF-WILLOW
 Method PCK@0.0N PCK@0.N PCK@0.NN  SIFT Flow [NN] 0.NNN 0.NN0 0.N0N  DAISY w/SF [NN] 0.NNN 0.NNN 0.NNN  DeepC w/SF [NN] 0.NNN 0.NNN 0.NNN  LIFT w/SF [NN] 0.NNN 0.NNN 0.NNN  VGG w/SF [NN] 0.NNN 0.NNN 0.NNN  FCSS w/SF [NN] 0.NNN 0.NNN 0.NNN  FCSS w/PF [NN] 0.NNN 0.NNN 0.NNN  LOMHOG [NN] 0.NNN 0.NNN 0.NNN  UCN[N] 0.NNN 0.NNN 0.NNN  SCNet-A 0.NN0 0.NNN 0.NNN  SCNet-AG 0.NNN 0.NNN 0.NNN  SCNet-AG+ 0.NNN 0.N0N 0.NNN  Table N: Results on Caltech-N0N
 Methods LT-ACC IoU LOC-ERR  NAMHOG [NN] 0.N0 0.NN 0.NN  PHMHOG [NN] 0.NN 0.NN 0.NN  LOMHOG [NN] 0.NN 0.N0 0.NN  DeepFlow [NN] 0.NN 0.N0 0.NN  SIFT Flow [NN] 0.NN 0.NN 0.NN  DSP [N0] 0.NN 0.NN 0.NN  FCSS w/SF [NN] 0.N0 0.N0 0.NN  FCSS w/PF [NN] 0.NN 0.NN 0.NN  SCNet-A 0.NN 0.N0 0.NN  SCNet-AG 0.NN 0.N0 0.NN  SCNet-AG+ 0.NN 0.NN 0.NN  Table N: Results on PASCAL Parts
 Methods IoU PCK  NAMHOG [NN] 0.NN 0.NN  PHMHOG [NN] 0.NN 0.NN  LOMHOG [NN] 0.NN 0.NN  Congealing [NN] 0.NN 0.NN  RASL [NN] 0.NN 0.NN  CollectionFlow [NN] 0.NN 0.NN  DSP [N0] 0.NN 0.NN  FCSS w/SF [NN] 0.NN 0.NN  FCSS w/PF [NN] 0.NN 0.NN  SCNet-A 0.NN 0.NN  SCNet-AG 0.NN 0.NN  SCNet-AG+ 0.NN 0.NN  using hand-crafted features [NN, NN, NN] and learned features [NN, NN, NN, NN, NN, NN]
 Results on Caltech-N0N
We also evaluate our approach  on the Caltech-N0N dataset [N]
Following the experimental protocol in [N0], we randomly select NN pairs of images for each object class, and evaluate matching accuracy  with three metrics: Label transfer accuracy (LT-ACC) [NN],  the IoU metric, and the localization error (LOC-ERR) of  corresponding pixel positions
Table N shows that SCNet  achieves comparable results with the state of the art
The  best performer, FCSS [NN], is trained on images from the  same Caltech-N0N dataset, while SCNet models are not
 Results on PASCAL Parts
Following [NN], we use the  dataset provided by [NN] where the images are sampled from  the PASCAL part dataset [N]
For this experiment, we measure the weighted IoU score between transferred segments  and the ground truth, with weights determined by the pixel  area of each part
To evaluate alignment accuracy, we measure the PCK metric (α = 0.0N) using keypoint annotations  for the PASCAL classes
Following [NN] once again, we  use selective search (SS) to generate proposals for SCNet  in this experiment
The results are summarized in Table N
 SCNet models outperform all other results on the dataset  in IoU, and SCNet-AG+ performs best among them
FCSS  w/PF [NN] performs better in PCK on this dataset
 These results verify that SCNet models have successfully  learned semantic correspondence
 N
Conclusion  We have introduced a novel model for learning semantic  correspondence, and proposed the corresponding CNN architecture that uses object proposals as matching primitives  and learns matching in terms of appearance and geometry
 The proposed method substantially outperforms both recent  deep learning architectures and previous methods based on  hand-crafted features
The result clearly demonstrates the  effectiveness of learning geometric matching for semantic correspondence
In future work, we will explore better  models and architectures to leverage geometric information
 NNNN    Acknowledgments
This work was supported by the ERC  grants VideoWorld and Allegro, the Institut Universitaire  de France, the National Research Foundation of Korea  (NRF) grant funded by the Korea government (MSIP) (No
 N0NNRNCNBN00NNNN) as well as the MSIT (Ministry of Science and ICT), Korea, under the ICT Consilience Creative  program (IITP-N0NN-R0NNN-NN-N00N)
We gratefully acknowledge the support of NVIDIA Corporation with the  donation of a Titan X Pascal GPU used for this research
 We also thank JunYoung Gwak and Christopher B
Choy  for their help in comparing with UCN
 References  [N] P
Arbelaez, J
Pont-Tuset, J
Barron, F
Marques, and J
Malik
Multiscale combinatorial grouping
In Proc
IEEE Conf
 Comp
Vision Patt
Recog., N0NN
N  [N] H
Bristow, J
Valmadre, and S
Lucey
Dense semantic correspondence where every pixel is a classifier
In Proc
Int
 Conf
Comp
Vision, N0NN
N, N  [N] X
Chen, R
Mottaghi, X
Liu, S
Fidler, R
Urtasun, et al
Detect what you can: Detecting and representing objects using  holistic models and body parts
In Proc
IEEE Conf
Comp
 Vision Patt
Recog., N0NN
N  [N] M
Cho, S
Kwak, C
Schmid, and J
Ponce
Unsupervised  object discovery and localization in the wild: Part-based  matching with bottom-up region proposals
In Proc
IEEE  Conf
Comp
Vision Patt
Recog., N0NN
N, N  [N] C
Choy, J
Gwak, S
Savarese, and M
Chandraker
Universal correspondence network
In Proc
Neural Info
Proc
 Systems, N0NN
N, N, N, N, N, N  [N] N
Dalal and B
Triggs
Histograms of oriented gradients for  human detection
In Proc
IEEE Conf
Comp
Vision Patt
 Recog., N00N
N, N  [N] M
Everingham, L
Van Gool, C
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes challenge  N00N (vocN00N) results
N  [N] L
Fei-Fei, R
Fergus, and P
Perona
One-shot learning of  object categories
IEEE Trans
Patt
Anal
Mach
Intell.,  NN(N):NNN–NNN, N00N
N, N, N  [N] P
Fischer, A
Dosovitskiy, E
Ilg, P
Häusser, C
Hazırbaş,  V
Golkov, P
van der Smagt, D
Cremers, and T
Brox
 Flownet: Learning optical flow with convolutional networks
 In Proc
IEEE Conf
Comp
Vision Patt
Recog., N0NN
N, N  [N0] R
Girshick
Fast r-cnn
In Proc
Int
Conf
Comp
Vision,  N0NN
N, N  [NN] B
Ham, M
Cho, C
Schmid, and J
Ponce
Proposal flow
In  Proc
IEEE Conf
Comp
Vision Patt
Recog., N0NN
N, N, N,  N, N, N, N  [NN] X
Han, T
Leung, Y
Jia, R
Sukthankar, and A
C
Berg
 MatchNet: Unifying feature and metric learning for patchbased matching
In Proc
IEEE Conf
Comp
Vision Patt
 Recog., N0NN
N, N, N  [NN] B
Hariharan, J
Malik, and D
Ramanan
Discriminative  decorrelation for clustering and classification
In Proc
European Conf
Comp
Vision, pages NNN–NNN
Springer, N0NN
 N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Spatial pyramid pooling  in deep convolutional networks for visual recognition
In  Proc
European Conf
Comp
Vision, N0NN
N  [NN] B
K
Horn and B
G
Schunck
Determining optical flow: A  retrospective
Artificial Intelligence, NN(N):NN–NN, NNNN
N  [NN] J
Hosang, R
Benenson, P
Dollár, and B
Schiele
What  makes for effective detection proposals? IEEE Trans
Patt
 Anal
Mach
Intell., N0NN
N, N  [NN] J
Hur, H
Lim, C
Park, and S
C
Ahn
Generalized deformable spatial pyramid: Geometry-preserving dense correspondence estimation
In Proc
IEEE Conf
Comp
Vision  Patt
Recog., N0NN
N, N  [NN] A
Kanazawa, D
W
Jacobs, and M
Chandraker
WarpNet:  Weakly supervised matching for single-view reconstruction
 In Proc
IEEE Conf
Comp
Vision Patt
Recog., N0NN
N  [NN] I
Kemelmacher-Shlizerman and S
M
Seitz
Collection  flow
In Proc
IEEE Conf
Comp
Vision Patt
Recog., N0NN
 N  [N0] J
Kim, C
Liu, F
Sha, and K
Grauman
Deformable spatial  pyramid matching for fast dense correspondences
In Proc
 IEEE Conf
Comp
Vision Patt
Recog., N0NN
N, N, N, N  [NN] S
Kim, D
Min, B
Ham, S
Jeon, S
Lin, and K
Sohn
Fcss:  Fully convolutional self-similarity for dense semantic correspondence
In Proc
IEEE Conf
Comp
Vision Patt
Recog.,  N0NN
N, N, N  [NN] S
W
Kim, D
Min, B
Ham, and K
Sohn
Dasc: Dense adaptative self-correlation descriptor for multi-modal and multispectral correspondence
In Proc
IEEE Conf
Comp
Vision  Patt
Recog., N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Proc
Neural Info
Proc
Systems, N0NN
N  [NN] E
G
Learned-Miller
Data driven image models through  continuous joint alignment
IEEE Trans
Patt
Anal
Mach
 Intell., NN(N):NNN–NN0, N00N
N  [NN] C
Liu, J
Yuen, and A
Torralba
Nonparametric scene parsing via label transfer
IEEE Trans
Patt
Anal
Mach
Intell.,  NN(NN):NNNN–NNNN, N0NN
N  [NN] C
Liu, J
Yuen, and A
Torralba
SIFT flow: Dense correspondence across scenes and its applications
IEEE Trans
 Patt
Anal
Mach
Intell., NN(N):NNN–NNN, N0NN
N, N, N, N, N  [NN] J
L
Long, N
Zhang, and T
Darrell
Do convnets learn  correspondence? In Proc
Neural Info
Proc
Systems, N0NN
 N  [NN] D
G
Lowe
Distinctive image features from scale-invariant  keypoints
Int
J
of Comp
Vision, N0(N):NN–NN0, N00N
N  [NN] S
Manen, M
Guillaumin, and L
Van Gool
Prime object  proposals with randomized Prim’s algorithm
In Proc
Int
 Conf
Comp
Vision, N0NN
N, N, N, N, N  [N0] J
Matas, O
Chum, M
Urban, and T
Pajdla
Robust widebaseline stereo from maximally stable extremal regions
Image and vision computing, NN(N0):NNN–NNN, N00N
N  [NN] M
Okutomi and T
Kanade
A multiple-baseline stereo
 IEEE Trans
Patt
Anal
Mach
Intell., NN(N):NNN–NNN, NNNN
 N  [NN] Y
Peng, A
Ganesh, J
Wright, W
Xu, and Y
Ma
Rasl:  Robust alignment by sparse and low-rank decomposition for  NNNN    linearly correlated images
IEEE Trans
Patt
Anal
Mach
 Intell., NN(NN):NNNN–NNNN, N0NN
N  [NN] J
Revaud, P
Weinzaepfel, Z
Harchaoui, and C
Schmid
 Deepmatching: Hierarchical deformable dense matching
 ArXiv e-prints, N0NN
N, N, N  [NN] C
Rhemann, A
Hosni, M
Bleyer, C
Rother, and  M
Gelautz
Fast cost-volume filtering for visual correspondence and beyond
In Proc
IEEE Conf
Comp
Vision Patt
 Recog., N0NN
N  [NN] E
Simo-Serra, E
Trulls, L
Ferraz, I
Kokkinos, P
Fua, and  F
Moreno-Noguer
Discriminative learning of deep convolutional feature point descriptors
In Proc
Int
Conf
Comp
 Vision, N0NN
N, N  [NN] K
Simonyan and andrew Zisserman
Very deep convolutional networks for large-scale visual recognition
In Proc
 IEEE Conf
Comp
Vision Patt
Recog., N0NN
N, N  [NN] T
Taniai, S
N
Sinha, and Y
Sato
Joint recovery of dense  correspondence and cosegmentation in two images
In Proc
 IEEE Conf
Comp
Vision Patt
Recog., N0NN
N, N  [NN] M
Tau and T
Hassner
Dense correspondences across scenes  and scales
IEEE Trans
Patt
Anal
Mach
Intell., N0NN
N  [NN] E
Tola, V
Lepetit, and P
Fua
Daisy: An efficient dense  descriptor applied to wide-baseline stereo
IEEE Trans
Patt
 Anal
Mach
Intell., NN(N):NNN–NN0, N0N0
N, N  [N0] J
R
Uijlings, K
E
van de Sande, T
Gevers, and A
W
 Smeulders
Selective search for object recognition
Int
J
of  Comp
Vision, N0N(N):NNN–NNN, N0NN
N, N  [NN] J
R
R
Uijlings, K
E
A
van de Sande, T
Gevers, and  A
W
M
Smeulders
Selective search for object recognition
 Int
J
of Comp
Vision, N0N(N):NNN–NNN, N0NN
N  [NN] P
Weinzaepfel, J
Revaud, Z
Harchaoui, and C
Schmid
 Deepflow: Large displacement optical flow with deep matching
In Proc
Int
Conf
Comp
Vision, N0NN
N  [NN] H
Yang, W.-Y
Lin, and J
Lu
Daisy filter flow: A generalized discrete approach to dense correspondences
In Proc
 IEEE Conf
Comp
Vision Patt
Recog., N0NN
N, N, N  [NN] Y
Yang and D
Ramanan
Articulated human detection with  flexible mixtures of parts
IEEE Trans
Patt
Anal
Mach
 Intell., NN(NN):NNNN–NNN0, N0NN
N  [NN] K
M
Yi, E
Trulls, V
Lepetit, and P
Fua
Lift: Learned  invariant feature transform
In Proc
European Conf
Comp
 Vision, N0NN
N  [NN] S
Zagoruyko and N
Komodakis
Learning to compare image patches via convolutional neural networks
In Proc
 IEEE Conf
Comp
Vision Patt
Recog., N0NN
N, N  [NN] J
Žbontar and Y
LeCun
Computing the stereo matching  cost with a convolutional neural network
In Proc
IEEE  Conf
Comp
Vision Patt
Recog., N0NN
N, N  [NN] J
Zbontar and Y
LeCun
Stereo matching by training a convolutional neural network to compare image patches
Journal of Machine Learning Research, NN(N-NN):N, N0NN
N, N  [NN] T
Zhou, Y
Jae Lee, S
X
Yu, and A
A
Efros
FlowWeb:  Joint image set alignment by weaving consistent, pixel-wise  correspondences
In Proc
IEEE Conf
Comp
Vision Patt
 Recog., N0NN
N, N, N  [N0] T
Zhou, P
Krähenbühl, M
Aubry, Q
Huang, and A
A
 Efros
Learning dense correspondence via Nd-guided cycle  consistency
In Proc
IEEE Conf
Comp
Vision Patt
Recog.,  N0NN
N, N  [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object proposals from edges
In Proc
European Conf
Comp
Vision,  N0NN
N, N  NNN0Soft Proposal Networks for Weakly Supervised Object Localization   Soft Proposal Networks for Weakly Supervised Object Localization  Yi ZhuN, Yanzhao ZhouN, Qixiang Ye†N, Qiang QiuN and Jianbin Jiao†N  NUniversity of Chinese Academy of Sciences NDuke University  {zhuyiNNN, zhouyanzhaoNNN}@mails.ucas.ac.cn, {qxye, jiaojb}@ucas.ac.cn, qiang.qiu@duke.edu  Abstract  Weakly supervised object localization remains challenging, where only image labels instead of bounding boxes are  available during training
Object proposal is an effective  component in localization, but often computationally expensive and incapable of joint optimization with some of  the remaining modules
In this paper, to the best of our  knowledge, we for the first time integrate weakly supervised object proposal into convolutional neural networks  (CNNs) in an end-to-end learning manner
We design a  network component, Soft Proposal (SP), to be plugged into  any standard convolutional architecture to introduce the  nearly cost-free object proposal, orders of magnitude faster  than state-of-the-art methods
In the SP-augmented CNNs,  referred to as Soft Proposal Networks (SPNs), iteratively  evolved object proposals are generated based on the deep  feature maps then projected back, and further jointly optimized with network parameters, with image-level supervision only
Through the unified learning process, SPNs  learn better object-centric filters, discover more discriminative visual evidence, and suppress background interference,  significantly boosting both weakly supervised object localization and classification performance
We report the best  results on popular benchmarks, including PASCAL VOC,  MS COCO, and ImageNet
N  N
Introduction  The success of object proposal methods greatly drives  the progress of the object localization
With the popularity  of deep learning, object detection is evolving from pipelined  frameworks [NN, NN] to unified frameworks [NN, NN, NN],  thanks to the unprecedentedly learning capability of convolutional neural networks (CNNs) and abundant object  bounding box annotations
 †Corresponding Authors NSource code is publicly available at yzhou.work/SPN  S p o t l i g h t  proposal map  generate  couple  S o f t P r o p o s a l  �  Figure N
Soft Proposal (SP) module can be inserted after any CNN  layer
A proposal map M is generated based on deep feature maps  U and then projected back, which results in feature maps V 
During the end-to-end learning procedure, M iteratively evolves and  jointly optimizes with the feature maps to spotlight informative  object regions
 Despite the unified frameworks achieve remarkable performance in supervised object detection, they can not be  directly applied to weakly supervised object localization  where only image-level labels, i.e., the presence or absence  of object categories, are available during training
 To tackle the problem of weakly supervised object localization, many of the conventional methods follow a multiinstance learning (MIL) framework by using object proposal methods [N, N, NN, NN, NN]
The learning objective  is designed to choose an instance (a proposal) from each  bag (an image with multiple proposals) to minimize the image classification error; however, the pipelined proposaland-classification method is sub-optimal as the two steps  can not be jointly optimized
Recent research [N] demonstrates that the convolutional filters in CNN can be seen as  object detectors and their feature maps can be aggregated to  produce Class Activation Map (CAM) [NN], which specifies  NNNNN  http://yzhou.work/SPN/   the spatial distribution of discriminative patterns for different image classes
This end-to-end network demonstrates  a surprising capability to localize objects under weak supervision
However, without the prior knowledge of informative object regions during training, conventional CNNs  can be misled by co-occurrence patterns and noisy backgrounds, Fig
N
The weakly supervised setting increases  the importance of high-quality object proposals, but the  problem to integrate the proposal functionality into a unified framework for weakly supervised object localization  remains open
 In this paper, we design a network component, Soft Proposal (SP), to be plugged into standard convolutional architectures for nearly cost-free object proposal (∼0.Nms per image, N0×faster than RPN [NN], N00×faster than Edge- Boxes [NN]), Fig
N
CNNs using SP module are referred  to as Soft Proposal Networks (SPNs)
In SPNs, iteratively  evolved object proposals are projected back on the deep  feature maps, and further jointly optimized with network  parameters, using image-level labels only
We further apply the SP module to successful CNNs including CNN-S,  VGG, and GoogLeNet, and upgrade them to Soft Proposal  Networks (SPNs), which can learn better object-centric filters and discover more discriminative visual evidence for  weakly supervised localization tasks
 The meaning of the word “soft” is threefold
First of all,  instead of extracting multiple materialized proposal boxes,  we predict objectness score for each receptive field, based  on the deep feature maps
Next, the proposal couples with  deep activation in a probabilistic manner, which not only  avoids threshold tuning but also aggregates all information  to improve performance
Last but not least, the proposal  iteratively evolves along with CNN filters updating
 To summarize, the main contributions of this paper are:  • We design a network component, Soft Proposal (SP), to upgrade conventional CNNs to Soft Proposal Networks (SPNs), in which the network parameters can  be jointly optimized with the nearly cost-free object  proposal
 • We upgrade successful CNNs to SPNs, including CNN-S, VGGNN, and GoogLeNet, and improve the  state-of-the-art of weakly supervised object localization by a significant margin
 N
Related Work  Weakly supervised object localization problems are often solved with a pipelined approach, i.e., an object proposal method [N0, NN] is first applied to decompose images  into object proposals, with which a latent variable learning  method, e.g., multi-instance learning (MIL), is used to iteratively perform proposal selection and classifier estimation  CNN SPN  cow  person  train  Image  Figure N
Visualization of Class Activation Maps (CAM) [NN] for  generic CNN and the proposed SPN
CNNs can be misled by noisy  backgrounds, e.g., grass for “cow”, and co-occurrence patterns,  e.g., rail for “train”, and thus miss informative object evidence
In  contrast, SPNs focus on informative object regions during training  to discover more fine-detailed evidence, e.g., hands for “person”,  while suppressing background interference
Best viewed in color
 [N, NN, NN, NN, N, N, NN]
With the popularity of deep learning, the pipelined approaches have been evolving to end-toend MIL networks [N0, NN] by learning convolutional filters  as detectors and using response maps to localize objects
 N.N
Object Proposal  Conventional object proposal methods, e.g., Selective  Search (SS) [N0] and EdgeBoxes (EB) [NN], use redundant  proposals generated with hand-craft features to hypothesize objects locations
Region Proposal Network (RPN)  regresses object locations using deep convolutional features [NN], reports the state-of-the-art proposal performance
 The success of RPN roots in the localization capability of  deep convolutional features; however, such capability is not  available until the network is well trained with precise annotations about object locations, i.e., bounding boxes, which  limits its applicability to weakly supervised methods
 Our SPN is specified for weakly supervised object localization task with only image-level annotations, i.e., presence or absence of object categories
The key difference  between our method to existing ones is that the “soft” proposal is an objectness confidence map instead of materialized boxes
Such a proposal couples with convolutional activation and evolves with the deep feature learning
 N.N
Weakly Supervised Localization  Pipelined methods
Weakly supervised localization  methods often use a stepwise strategy, i.e., first extracting  candidate proposals and then learning classification model  together with selecting proposals to localize objects
Many  approaches have been explored to prevent the learning procedure from getting stuck to a local minimum, e.g., prior  NNNN    C O N V  C O N V  S P  � ,� ,��,  Epoch N Epoch N Epoch N Epoch N Epoch N  E n d - t o - E n d  L e a r n i n g  Proposal Map  Response Map (for cow )  cow  Figure N
The first row shows the Soft Proposal Network architecture
The second row illustrates the evolution of the proposal map during  training epochs (corresponding to the outer loop of Algorithm N)
The third row presents the evolution of the response map for “cow”
The  proposal map produced by SP module iteratively evolves and jointly optimizes with convolutional filters during the learning phase, leading  SPN to discover fine-detailed visual evidence for localization
Best viewed in color
 regularization [N], multi-fold learning [N], and smooth optimization methods [NN, N]
One representative method is  WSDDN [N], which significantly improves the object detection performance by performing proposal selection together  with classifier learning
ContextLoc [NN] updates WSDDN  by introducing two context-aware modules which try to expand or contract the fixed proposals in learning procedure  to leverage the surrounding context to improve localization
 Attention net [NN] computes an attention score for each precomputed object proposals
ProNet [NN] uses parallel CNN  streams for multiple scales to propose possible object regions and then classify these regions via cascaded CNNs
 To the best of our knowledge, we are the first to integrate proposal step into CNNs and achieve jointly updating  among proposal generation, object region selection, and object detector estimation under weak supervision
 Unified frameworks
Another line of research shows  up in weakly supervised localization uses unified network  frameworks to perform both localization and classification
 The essence of the method Oquab et al
[N0] is that the  deep feature maps are interpreted as a “bag” of instances,  where only the highest responses of feature maps contribute  to image label prediction in an MIL-like learning procedure
Zhou et al
[NN] achieve remarkable localization performance by leveraging a global average pooling layer behind the top convolutional layer to aggregate class-specific  activation
In the following works, Zhang et al
[NN] formulate such a class activation procedure as conditional probability backward propagation along convolutional layers to  localize discriminative patterns in generic CNNs
Bency et  al
[N] propose a heuristic search strategy to hypothesize locations of feature maps in a multi-scale manner and grade  the corresponding receptive fields by the classification layer
 The main idea of these methods is that the convolutional  filters can behave as detectors to activate locations on the  deep feature maps, which provide informative evidence for  image classification
Despite the simplicity and efficiency  of these networks, they are observed missing useful object  evidence, as well as being misled by complex backgrounds
 The reason behind this phenomenon can be that the filters  learned for common object classes are challenged with object appearance variations and background complexity
Our  proposed SPN targets at solving such problems by utilizing image-specific objectness prior and coupling it with the  network learning
 N
Soft Proposal Network  In this section, we present a network component, Soft  Proposal (SP), to be plugged into standard convolutional architectures for nearly cost-free object proposal
CNNs using SP module are referred to as Soft Proposal Networks  (SPNs), Fig
N
Despite the SP module can be inserted after  any CNN layer, we apply it after the last convolutional layer  where the deep features are most informative
For weakly  supervised object localization, SPN has an spatial pooling  layer with the output features connected to image labels, as  illustrated later
 In the learning procedure of SPN, the Soft Proposal Generation step spotlights potential object locations via performing graph propagation over the receptive fields of deep  responses, and the Soft Proposal Coupling step aggregates  feature maps with the generated proposal map
With iterative proposal generation, coupling, and activation, SPN performs weakly supervised learning in an end-to-end manner
 N.N
Soft Proposal Generation  The proposal map, M ∈ RN×N , is an objectness map generated by SP module based on the deep feature maps,  NNNN    �  iter
N  iter
N  Proposal Map  iter
N  … …  � ,,  Figure N
Soft Proposal Generation in a single SPN feedforward  pass (corresponding to the inner loop of Algorithm N)
Experimentally, the generation reaches stable in about ten iterations
 Fig
N
Consider a SP module is inserted after the l-th convolutional layer, let U l ∈ RK×N×N denote the deep feature maps of the l-th convolutional layer, where K is the number of feature maps (channels), N × N denotes the spatial size of a feature map
Each location (i, j) on U l has a deep feature vector ulij = U  l ·,i,j ∈ R  K from all K channels of  U l
To generate M , a fully connected directed graph G is  first constructed by connecting every location on U l, with  the weight matrix D ∈ RN N ×NN where DiN+j,pN+q indicating the weight of edge from node (i, j) to node (p, q)
To calculate the weight matrix D, two kinds of objectness measures are utilized: N)
Image regions from  the same object category share similar deep features
N)
 Neighboring regions exhibit semantic relevance
The objectness confidence are reflected with a dissimilarity measure that combines feature difference and spatial distance,  as D′iN+j,pN+q , ‖u l ij−u  l pq‖·L(i−p, j−q), and L(a, b) ,  exp(−a N +bN  NǫN ), where ǫ is empirically set as 0.NNN in all experiments
And then the weights of the outbound edges of  each node are normalized to N, i.e., Da,b = D′a,b∑  N a=N  D′ a,b  
 With the weight matrix D defining the edge weight between nodes, a graph propagation algorithm, i.e., random  walk [NN], is utilized to generate the proposal map M 
The  random walk algorithm iteratively accumulates objectness  confidence at the nodes that have high dissimilarity with  their surroundings
A node receives confidence from inbound directed edges, and then the confidence among the  nodes can be diffused along the outbound directed edges  which are connected to all other nodes, Fig
N
In this procedure, a location transfer confidence to others via globally  objectness flow, which not only collects local object evidence but also depresses noise regions
For the convenience  of random walk operation, we first reshape the ND proposal  map M to a vector with NN element, initialized with the  value N NN  
M is updated with iteratively multiplying with  the weight matrix D, as  M ← D ×M
(N)  The above procedure is a variant of the eigenvector centrality measure [NN], which outputs a proposal map to indicate  the objectness confidence of each location on the deep feature maps
Note that the weight matrix D is conditional  on the deep feature maps U l, and U l is conditional on the  convolutional filters of the l-th layer, W l, in the learning  procedure
To show such dependency, Eq
N is updated as  M ← D (  U l(W l) )  ×M
(N)  The random walk procedure can be seen as a Markov chain  that can reach unique stable state because the chain is ergodic, a property which emerges from the fact that the graph  G is by construction strongly connected [NN]
Given deep  feature maps U , Eq
N usually reaches its stable state in  about ten iterations, and the output M is reshaped from a  vector to a ND proposal map M ∈ RN×N 
 N.N
Soft Proposal Coupling  The proposal map generated with the deep feature maps  in a weakly supervised manner can be regarded as a kind  of objectness map, which indicates possible object regions
 From the perspective of image representation, the proposal  map spotlights “regions of interest” that are informative to  image classification
M can be integrated into the end-toend learning via SP module, Fig
N, to aggregate the imagespecific discriminative patterns from deep responses
 In the forward propagation of a SP-augmented CNN, i.e.,  SPN, each feature map of the coupled V ∈ RN×N is the Hadamard product of the corresponding feature map of U  and M ,  Vk = U l k(W  l) ◦M, k=N,N,...,K, (N)  where the subscript k denotes the channel index and “◦” denotes element-wise multiplication
The coupled feature  maps V pass forward to predict scores y ∈ RC of C classes, and then the prediction error E = ℓ(y, t) of each sample comes out according to the image labels t
ℓ(·) is the loss function
In the back-propagation procedure of SPN, the  gradient is apportioned by M , as  W l = W l +∆W (M)  ∆W (M) = −η ∂E  ∂W l (M)  (N)  where η is the network learning rate
∆W (M) means that W l is conditional on M , as the gradients of filters ∂E  ∂W l are  conditional on M , Eq
N
Since W l is conditional on M , the  SPN learns more informative image regions in each image  and depresses noisy backgrounds
 NNNN    Algorithm N Learning SPN with Soft Proposal Coupling  Input: Training images with category labels  Output: Network parameters, proposal map for each image
 N: repeat  N: initial each element in M with N NN  N: repeat  N: M ← D (  U l(W l) )  ×M N: until stable state reached  N: V = U l(W l) ◦M , feed forward
N: W l = W l +∆W (M), backward
N: for all the convolutional layers l do  N: U l = W l ∗ U l−N  N0: end for  NN: until Learning converges  Given the Soft Proposal Generation defined by Eq
N, the  Soft Proposal Coupling defined by Eq
N, and the back propagation procedure defined by Eq
N, it is clear that U l, W l,  and M are conditional on each other
During training, once  the convolutional filters W l changed by Eq
N, U l will also  change
Once U l is updated, a random walk procedure, described in Sec
N.N, is utilized to update the proposal map  M 
The proposal map M helps SPNs to progressively spotlight feature maps U l and learn discriminative filters W l,  thus the proposals and filters are jointly optimized in SPNs,  Fig
N
The procedure is described in Algorithm N
 N.N
Weakly Supervised Activation  The weakly supervised learning task is performed by  firstly using an spatial pooling layer to aggregate deep feature maps to a feature vector, and connecting such a feature  vector to image categories with a fully connect layer, Fig
N
 Such an architecture uses weak supervision posed from the  end of the network, i.e., the image category annotations, to  activate potential object regions
 In the forward propagation of SPN, proposal map M is  generated by the SP module inserted behind the l-th convolutional layer
The feature maps U l is computed as  U lj = ( ∑  i∈Sj  U l−Ni ∗W l ij + b  l j) ◦M, (N)  where Sj is a selection of input maps, b l j is the additive bias,  and W lij is the convolutional filters between the i-th input  map in U l−N and the j-th output map in U l
 In the backward propagation of SPN, the error propagates from layer l + N to layer l via the δ, as  δl = ∂E  ∂U l =  ∂E  ∂U l+N ∂U l+N  ∂U l  = δl+N ∂[(U l ∗W l+N + bl) ◦M ]  ∂U l  = δl+N ∗W l+N ◦M,  (N)  which indicates that the proposal map M spotlights not only  informative regions on feature maps but also worth-learning  locations
Since the M flows along with gradients δ, inserting one SP module after the top convolutional layer can  effect all CNN filters
 Once δl is calculated, we can immediately compute the  gradients for filters as  ∂E  ∂W lij =  ∑  p,q  (δlj)pq(x l−N i )pq  = ∑  p,q  (δl+Nj ∗W l+N j· )pqMpq(x  l−N i )pq,  (N)  and compute the gradients for bias as  ∂E  ∂blij =  ∑  p,q  (δlj)pq  = ∑  p,q  (δl+Nj ∗W l+N j· )pqMpq,  (N)  where W l+Nj· denotes the filters of layer l+N that are used to  calculate U lj , and (x l−N i )pq denotes the patch centered (p, q)  on U l−Ni 
With Eq
N and Eq
N, the proposal map M which  indicates the objectness confidence of an image combines  with the gradient maps in the weakly supervised activation  procedure, driving SPN to learn more useful patterns
 For weakly supervised object localization, we calculate  the response map Rc for the c-th class, similar to [NN], Rc = ∑  k wk,c · Ûk ◦M where Ûk is the k-th feature map of the last convolutional layer, wk,c is the weight value of the fully  connected layer which connects the c-th output node and the  k-th feature vector, Fig
N
 N
Experiment  We upgrade state-of-the-art CNN architectures, e.g.,  VGGNN and GoogLeNet, to SPNs, and evaluate them on  popular benchmarks
In Sec
N.N, we compare SPN with  conventional object proposal methods, showing that it can  generate high-quality proposals with negligible computational overhead
In Sec
N.N, on a weakly supervised pointbased object localization task, we demonstrate SPNs can  learn better object-centric filters, which produce precise responses on class-specific objects
In Sec
N.N, SPNs are further tested on a weakly supervised object bounding box localization task, validating its capability of discovering more  fine-detailed visual evidence in complex cluttered scenes
 In Sec
N.N, the significant improvement of classification  performance on PASCAL VOC [N0] (N0-classes, ∼N0k im- ages), MS COCO [NN] (N0-classes, ∼NN0k images), and ImageNet [NN] (N000-classes, ∼NN00k images), shows the superiority of SPNs beyond weakly supervised object localization tasksN
We train SPNs using SGD with crossNPlease refer to supplementary materials for more results
 NNNN    Figure N
Proposal examples
The first row presents input images
 The second row presents proposal coupled images, by composing the proposal map with the original images
The third row  shows top-N00 scored receptive fields according to the proposal  map
Best viewed in color
 Method ObjectEnergy(%) Time(ms)  Selective Search [N0] NN.N N000  EdgeBoxes [NN] NN.N N00  RPN (supervised) [NN] NN.N N0.N  SPN (weakly supervised) NN.N 0.N  Table N
Proposal quality evaluation on VOCN00N test set
The  Object Energy in the second column indicates the percentage of  spotlighted object areas
Note that RPN is learned with object  bounding box annotations (supervised) while SPN is learned with  image label annotations (weakly supervised)
The third column  describes the average time cost per image
RPN and SPN are tested  with a NVIDIA Tesla KN0 GPU while Selective Search and EdgeBoxes are tested on CPU due to algorithm complexity
 entropy loss
We use a weight decay of 0.000N with a momentum of 0.N and set the initial learning rate to 0.0N
 N.N
Proposal Quality  On the VOCN00N dataset, we assess the quality of proposals by an Object Energy metric defined below
For the  compared Selective Search [N0], EdgeBoxes [NN] and RPN  [NN] methods, the energy value of a pixel is the sum of  scores of the proposal boxes that cover the pixel
Therefore,  all objectness values in an image constitute an energy map  that indicates the informative object regions predicted by  the method
For the SPN, we produce Object Energy maps  by rescaling proposal maps to the image size, Fig
N
We  further normalize each energy map and compute the sum of  Object Energy of pixels those fall into ground-truth bounding boxes as the Object Energy
 It can be seen from the definition that the Object EnSS  EB  RPN  SPN(ours)  0.0 0.N 0.N 0.N 0.N N.0 0.0  0.N  0.N  0.N  0.N  N.0  Object Size  O b  je ct   E n  e rg  y  Epoch  NN  NN  N0  NN  NN  NN  m e a n   O b  je ct   E n  e rg  y  (a) Object Energy distribution (b) Object Energy evolution  N N NN NN NN  Figure N
(a) Object Energy curves
The x-coordinate is the ratio  between the object area to the image size, and y-coordinate is the  Object Energy
The curves are produced by using a N-polynomial  regression on the dots, each of which denotes an image
(b) Evolution of Object Energy during the learning procedure
Best viewed  zooming on screen
 ergy values range in [0.0, N.0], which indicates how many informative object areas in the image are spotlighted by the  method
The second column in Tab
N demonstrates that the  proposals generated by SPN are of high-quality
The Object  Energy of SPN proposals is significantly larger than those  of Selective Search and EdgeBoxes, which usually produce  redundant proposals and cover many background regions
 Surprisingly, The Object Energy of SPN proposals obtained  by weakly supervised learning is comparable to that of supervised RPN method (NN.N% vs
NN.N%)
It can be seen  in Fig
N(a) that the proposed SPN can spotlight small objects significantly better than the Selective Search and EdgeBoxes methods, despite that the proposal maps are based on  low-resolution deep feature maps
Fig
N(b) demonstrates  that the SPN proposals can iteratively evolve and jointly  optimize with network filters during the end-to-end training
Moreover, the implementation of SPN is simple and  naturally compatible with GPU parallelization
It can be  seen from the third column of Tab
N that the proposed SP  module can introduce weakly supervised object proposal to  CNNs in a nearly cost-free manner
 N.N
Pointing Localization  Pointing without prediction
To evaluate whether the  proposed SPN can learn more discriminative filters which  are effective to produce accurate response maps, we test it  on the weakly supervised pointing task
We select three  successful CNNs, including CNN-S [N], VGGNN [NN], and  GoogLeNet [NN] and upgrade them to SPNs by inserting  the SP module after their last convolution layers, Fig
N
All  SPNs are fine-tuned on the VOCN00N training set with same  hyper-parameters, and we calculate the response maps as  described in Sec
N.N with ground-truth labels for pointing  localization
Following the setting of c-MWP [NN], a stateof-the-art method, we calculate the accuracy of pointing loNNNN    GT SPN c-MWP  p o  tt e d   p la  n t  ap p  le u  m b  re lla  ca r  Figure N
Examples of pointing localization, which shows that  SPN is effective in complex scenes: a) Noisy co-occurrence patterns, e.g., leaves for “potted plant”
b) Small objects, e.g., “apple”  in hand
c) Cluttered backgrounds, e.g., “car” on the street
d) Infrequent form, e.g., closed “umbrella”
Best viewed in color
 calization as below: a hit is counted if the pixel of maximum  response falls in one of the ground truth bounding boxes of  the cued object category within NN pixels tolerance
Otherwise, a miss is counted
We measure the per-class localization accuracy by Acc = Hits Hits+Misses  
The overall results  are the mean value of per-class point localization accuracy
 For the VOCN00N dataset, we use two test sets, i.e., All  and Difficult (Diff.) [NN]
All means the overall test set  and Diff
means a difficult subset which has mixed categories and contains small objects
As shown in Tab
N,  upgrading conventional CNNs to SPNs brings significant  performance improvement
Specifically, the SP-VGGNet  outperforms c-MWP by N.N% (NN.N % vs N0.0 %) for All  and NN.N% (NN.N% vs NN.N%) for Diff.
The SP-GoogLeNet  outperforms c-MWP by N.N% and N.N% for All and Diff.,  respectively
The significant improvement of pointing localization performance validates the effectiveness of the SP  module for guiding SPNs to learn better object-centric filters, which can pick up accurate object responses
 We made multiple observations in Tab
N
N)
SPVGGNet has better performance than SP-GoogLeNet on  pointing localization
The reason can be that the receptive fields of SP-VGGNet are smaller than that of SPGoogLeNet
Without much overlap between receptive  fields, the objectness propagation in SP module can be more  effective
N)
The accuracy improvement on Diff
is larger  Method CNN-S VGGNN GoogLeNet  Center NN.N/NN.N NN.N/NN.N NN.N/NN.N  Grad [NN] NN.N/NN.N NN.0/NN.N NN.N/NN.N  Deconv [NN] NN.N/NN.N NN.N/NN.N NN.N/NN.N  LRP [N] NN.N/NN.N - NN.N/N0.N  CAM [NN] - - N0.N/NN.N  MWP [NN] NN.N/NN.N NN.N/NN.N NN.N/N0.N  c-MWP [NN] NN.N/NN.N N0.0/NN.N NN.N/NN.N  SPN NN.N/NN.N NN.N/NN.N NN.N/NN.N  Table N
Pointing localization accuracy (%) on VOCN00N test set  (All/Diff.)
Center is a baseline method which uses the image  centers as estimation of object centers
 Method mAP (%)  Dataset VOC COCO  Oquab et al
[N0] NN.N NN.N  Sun et al
[NN] NN.N NN.N  Bency [N] NN.N NN.N  SPN NN.N NN.N  Table N
Mean Average Precision (mAP) of location prediction on  VOCN0NN val
set and COCON0NN val
set
 than that on All, which shows that the proposal functionality  of SPNs is particularly effective in cluttered scenes
 Pointing with prediction
We further test SPN on a  more challenging pointing-with-prediction task
The task  requires the network output not only the correct prediction of the presence/absence of the object categories in test  images, but also the correct pointing localization of objects, i.e., the point of maximum response falls in one of  the ground truth bounding boxes within NN pixels tolerance  [N0]
 We upgrade a pre-trained VGGNN model to SPN and respectively fine-tune it on VOCN0NN and COCON0NN dataset  for N0 epochs
Results are reported in Tab
N
Without multiscale setting, SPN outperforms the state-of-the-art method  [N] by a significant margin (N.N% mAP for VOCN0NN, N%  mAP for COCON0NN)
This evaluation demonstrates that  the Soft Proposal module endows CNNs accurate localization capability while keeping its classification ability
In  Sec
N.N, we will show that upgrading CNNs to SPNs can  even improve the classification performance
 N.N
Bounding Box Localization  Although without object-level annotations involved in  the learning phase, our method can also be used to estimate  object bounding boxes with the help of response maps
We  calculate each response map with ground truth labels and  convert them to binary maps with the mean value as thresholds
We then rescale them to the original image size and  extract the tightest box covering the foreground pixels as the  predicted object bounding box
 The Correct Localization (CorLoc) metric [N] is used  NNNN    Method aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv mean  Bilen et al
[N] NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N N0.0 NN.N NN.N NN.N N0.N N0.N NN.N NN.N  Wang et al
[NN] N0.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N  Cinbis et al
[N] NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0  WSDDN [N] NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N N0.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  ContextLoc [NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N  SP-VGGNet NN.N NN.N NN.0 NN.0 NN.N NN.0 NN.N NN.N N0.N NN.N NN.0 NN.N NN.N N0.0 N0.0 NN.N NN.N NN.N NN.N N0.N N0.N  Table N
Correct Localization rate (CorLoc [N]) on the positive trainval images of the VOCN00N dataset (%)
 so fa  ca t  p e rs  o n  Our  response map SPN WSDDN  Figure N
Bounding box localization results on the VOCN00N test  set
By activating fine-detailed evidence like arm or leg for “person”, paw for “cat”, and texture fragments for “sofa”, the estimated bounding boxes are more precise than those by WSDDN
 to evaluate the bounding box localization performance
It  can be seen in Tab
N that the mean CorLoc of our method  outperforms the state-of-the-art ContextLoc method [NN] by  about N%
Surprisingly, on the “dog”, “cat”, “horse”, and  “person” classes, SPN outperforms the compared method  up to N0-N0%
It can be seen from Fig
N that the conventional method tends to use the most discriminative part  for each category, e.g., faces, while SPN can discover more  fine-detailed object evidence, e.g., hands and legs, thanks to  the objectness prior introduced by the SP module
On the  ”sofa” and ”table” classes, our method outperforms other  methods by N0%, demonstrating the capability of SPN to  correctly localize the occluded objects, Fig
N, which shows  that the graph propagation in the Soft Proposal Generation  step helps to find object fragments of similar appearance
 N.N
Image Classification  Although to predict the presence/absence of object categories in an image does not require accurate located and  comprehensive visual cues, the proposal functionality of  SPNs which highlights informative regions while suppressing disturbing backgrounds during training should also benefit the classification performance
 We use GoogLeNetGAP [NN], a simplified version of  GoogLeNet, as the baseline
By inserting SP module after  Method CAM c-MWP MWP Fb[NN] SPN  Error (%) NN.N NN.0 NN.N NN.N NN.N  Table N
Bounding box localization errors on ILSVRCN0NN val
 set
 Method ImageNet COCO VOC  GoogLeNetGAP[NN] NN.0/NN.N NN.N NN.N  SP-GoogLeNetGAP NN.N/NN.N NN.0 NN.N  Table N
Classification results
The second column is the top-N/topN error rate (%) on ILSVRCN0NN val
set
The third and fourth  column are mAP (%) on VOCN00N test set and COCO val
set
 the last convolution layer, the GoogLeNetGAP is upgraded  to a SPN
The SPN is trained on the ILSVRCN0NN dataset,  i.e., ImageNet, for N0 epochs with the SGD method
It can  be seen in the second column of Tab
N that the SPN significantly outperforms the baseline GoogLeNetGAP by N.N%,  which shows that the SPNs can learn more informative feature representation
We then fine-tune each trained model  on COCON0NN and VOCN00N by N0 and N0 epochs to assess the generalization capability of SPN
As shown in the  third column of Tab
N
SP-GoogLeNetGAP surpasses the  baseline by a large margin, e.g., N.N% on VOCN00N
This  further demonstrates that the weakly supervised object proposal is effective for both localization and classification
 N
Conclusions  In this paper, we proposed a simple yet effective technique, Soft Proposal (SP), to integrate nearly cost-free object proposal into CNNs for weakly supervised object localization
We designed the SP module to upgrade conventional CNNs, e.g., VGG and GoogLeNet, to Soft Proposal  Networks (SPNs)
In SPNs, iteratively evolved object proposals are generated based on the deep feature maps then  projected back, leading filters to discover more fine-detailed  evidence through the unified learning procedure
SPNs significantly outperforms state-of-the-art methods on weakly  supervised localization and classification tasks, demonstrating the effectiveness of coupling object proposal with network learning
 Acknowledgements  The authors are very grateful for support by NSFC grant  NNNNNNNN, BMSTC grant ZNNNN0000NNNN00N
 NNNN    References  [N] S
Bach, A
Binder, G
Montavon, F
Klauschen, K.-R
 Müller, and W
Samek
On pixel-wise explanations for nonlinear classifier decisions by layer-wise relevance propagation
PloS one, N0(N):e0NN0NN0, N0NN
N  [N] A
J
Bency, H
Kwon, H
Lee, S
Karthikeyan, and B
S
 Manjunath
Weakly supervised localization using deep feature maps
In European Conference on Computer Vision  (ECCV), pages NNN–NNN, N0NN
N, N  [N] H
Bilen, M
Pedersoli, and T
Tuytelaars
Weakly supervised  object detection with posterior regularization
In British Machine Vision Conference (BMVC), volume N, N0NN
N, N  [N] H
Bilen, M
Pedersoli, and T
Tuytelaars
Weakly supervised  object detection with convex clustering
In IEEE Conference  on Computer Vision and Pattern Recognition (CVPR), pages  N0NN–N0NN, N0NN
N  [N] H
Bilen and A
Vedaldi
Weakly supervised deep detection  networks
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNNN–NNNN, N0NN
N, N, N,  N  [N] Z
Bolei, A
Khosla, A
Lapedriza, A
Oliva, and A
Torralba
 Object detectors emerge in deep scene cnns
In International  Conference on Learning Representations (ICLR), N0NN
N  [N] K
Chatfield, K
Simonyan, A
Vedaldi, and A
Zisserman
 Return of the devil in the details: Delving deep into convolutional nets
In British Machine Vision Conference (BMVC),  N0NN
N  [N] R
G
Cinbis, J
J
Verbeek, and C
Schmid
Weakly supervised object localization with multi-fold multiple instance  learning
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), NN(N):NNN–N0N, N0NN
N, N, N,  N  [N] T
Deselaers, B
Alexe, and V
Ferrari
Weakly supervised  localization and learning with generic knowledge
International Journal of Computer Vision (IJCV), N00(N):NNN–NNN,  N0NN
N, N  [N0] M
Everingham, S
M
A
Eslami, L
J
V
Gool, C
K
I
 Williams, J
M
Winn, and A
Zisserman
The pascal visual object classes challenge: A retrospective
International  Journal of Computer Vision (IJCV), NNN(N):NN–NNN, N0NN
N  [NN] R
B
Girshick
Fast R-CNN
In IEEE International Conference on Computer Vision (ICCV), pages NNN0–NNNN, N0NN
 N  [NN] R
B
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich  feature hierarchies for accurate object detection and semantic  segmentation
In IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), pages NN0–NNN, N0NN
N  [NN] J
Harel, C
Koch, and P
Perona
Graph-based visual  saliency
In Neural Information Processing Systems (NIPS),  pages NNN–NNN, N00N
N  [NN] V
Kantorov, M
Oquab, M
Cho, and I
Laptev
Contextlocnet: Context-aware deep network models for weakly supervised localization
In European Conference on Computer  Vision (ECCV), pages NN0–NNN, N0NN
N, N, N, N  [NN] M
P
Kumar, B
Packer, and D
Koller
Self-paced learning  for latent variable models
In Neural Information Processing  Systems (NIPS), pages NNNN–NNNN, N0N0
N  [NN] T
Lin, M
Maire, S
J
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft COCO: common objects in context
In European Conference on Computer Vision (ECCV), pages NN0–NNN, N0NN
N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
E
Reed,  C
Fu, and A
C
Berg
SSD: single shot multibox detector
In European Conference on Computer Vision (ECCV),  pages NN–NN, N0NN
N  [NN] L
Lovász
Random walks on graphs
Combinatorics, Paul  erdos is eighty, N:N–NN, NNNN
N  [NN] M
E
Newman
The mathematics of networks
The new  palgrave encyclopedia of economics, N(N00N):N–NN, N00N
N  [N0] M
Oquab, L
Bottou, I
Laptev, and J
Sivic
Is object localization for free? - weakly-supervised learning with convolutional neural networks
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNN–NNN, N0NN
 N, N, N  [NN] J
Redmon, S
K
Divvala, R
B
Girshick, and A
Farhadi
 You only look once: Unified, real-time object detection
In  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNN–NNN, N0NN
N  [NN] S
Ren, K
He, R
B
Girshick, and J
Sun
Faster R-CNN:  towards real-time object detection with region proposal networks
In Neural Information Processing Systems (NIPS),  pages NN–NN, N0NN
N, N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
S
Bernstein,  A
C
Berg, and F
Li
Imagenet large scale visual recognition  challenge
International Journal of Computer Vision (IJCV),  NNN(N):NNN–NNN, N0NN
N  [NN] K
Simonyan, A
Vedaldi, and A
Zisserman
Deep inside convolutional networks: Visualising image classification models and saliency maps
International Conference on  Learning Representations (ICLR Workshop), N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In International  Conference on Learning Representations (ICLR), N0NN
N  [NN] H
O
Song, R
B
Girshick, S
Jegelka, J
Mairal, Z
Harchaoui, and T
Darrell
On learning to localize objects with  minimal supervision
In International Conference on Machine Learning (ICML), pages NNNN–NNNN, N0NN
N, N  [NN] C
Sun, M
Paluri, R
Collobert, R
Nevatia, and L
D
Bourdev
Pronet: Learning to propose object-specific boxes for  cascaded neural networks
In IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), pages NNNN–NNNN,  N0NN
N, N, N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
E
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In IEEE Conference on  Computer Vision and Pattern Recognition (CVPR), pages N–  N, N0NN
N  [NN] E
W
Teh, M
Rochan, and Y
Wang
Attention networks for  weakly supervised object localization
In British Machine  Vision Conference (BMVC), N0NN
N  [N0] J
R
R
Uijlings, K
E
A
van de Sande, T
Gevers, and  A
W
M
Smeulders
Selective search for object recognition
International Journal of Computer Vision (IJCV),  N0N(N):NNN–NNN, N0NN
N, N  NNNN    [NN] C
Wang, W
Ren, K
Huang, and T
Tan
Weakly supervised  object localization with latent category learning
In European Conference on Computer Vision (ECCV), pages NNN–  NNN, N0NN
N, N  [NN] Q
Ye, T
Zhang, Q
Qiu, B
Zhang, J
Chen, and G
Sapiro
 Self-learning scene-specific pedestrian detectors using a progressive latent model
CoRR, abs/NNNN.0NNNN, N0NN
N  [NN] M
D
Zeiler and R
Fergus
Visualizing and understanding  convolutional networks
In European Conference on Computer Vision (ECCV), pages NNN–NNN, N0NN
N  [NN] D
Zhang, D
Meng, C
Li, L
Jiang, Q
Zhao, and J
Han
 A self-paced multiple-instance learning framework for cosaliency detection
In IEEE International Conference on  Computer Vision (ICCV), pages NNN–N0N, N0NN
N  [NN] J
Zhang, Z
L
Lin, J
Brandt, X
Shen, and S
Sclaroff
Topdown neural attention by excitation backprop
In European  Conference on Computer Vision (ECCV), pages NNN–NNN,  N0NN
N, N, N, N  [NN] B
Zhou, A
Khosla, À
Lapedriza, A
Oliva, and A
Torralba
Learning deep features for discriminative localization
 In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages NNNN–NNNN, N0NN
N, N, N, N, N, N  [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object  proposals from edges
In European Conference on Computer  Vision (ECCV), pages NNN–N0N, N0NN
N, N  NNN0An Analysis of Visual Question Answering Algorithms   An Analysis of Visual Question Answering Algorithms  Kushal Kafle Christopher Kanan∗  Rochester Institute of Technology  Rochester, New York  kkN0NN,kanan@rit.edu  Abstract  In visual question answering (VQA), an algorithm must  answer text-based questions about images
While multiple datasets for VQA have been created since late N0NN,  they all have flaws in both their content and the way algorithms are evaluated on them
As a result, evaluation  scores are inflated and predominantly determined by answering easier questions, making it difficult to compare different methods
In this paper, we analyze existing VQA algorithms using a new dataset called the Task Driven Image Understanding Challenge (TDIUC), which has over N.N  million questions organized into NN different categories
We  also introduce questions that are meaningless for a given  image to force a VQA system to reason about image content
We propose new evaluation schemes that compensate  for over-represented question-types and make it easier to  study the strengths and weaknesses of algorithms
We analyze the performance of both baseline and state-of-the-art  VQA models, including multi-modal compact bilinear pooling (MCB), neural module networks, and recurrent answering units
Our experiments establish how attention helps  certain categories more than others, determine which models work better than others, and explain how simple models  (e.g
MLP) can surpass more complex models (MCB) by  simply learning to answer large, easy question categories
 N
Introduction  In open-ended visual question answering (VQA) an algorithm must produce answers to arbitrary text-based questions about images [NN, N]
VQA is an exciting computer  vision problem that requires a system to be capable of many  tasks
Truly solving VQA would be a milestone in artificial  intelligence, and would significantly advance human computer interaction
However, VQA datasets must test a wide  range of abilities for progress to be adequately measured
 VQA research began in earnest in late N0NN when the  ∗Corresponding author  Figure N: A good VQA benchmark tests a wide range of  computer vision tasks in an unbiased manner
In this paper,  we propose a new dataset with NN distinct tasks and evaluation metrics that compensate for bias, so that the strengths  and limitations of algorithms can be better measured
 DAQUAR dataset was released [NN]
Including DAQUAR,  six major VQA datasets have been released, and algorithms  have rapidly improved
On the most popular dataset, ‘The  VQA Dataset’ [N], the best algorithms are now approaching N0% accuracy [N] (human performance is NN%)
While  these results are promising, there are critical problems with  existing datasets in terms of multiple kinds of biases
Moreover, because existing datasets do not group instances into  meaningful categories, it is not easy to compare the abilities  of individual algorithms
For example, one method may excel at color questions compared to answering questions requiring spatial reasoning
Because color questions are far  more common in the dataset, an algorithm that performs  well at spatial reasoning will not be appropriately rewarded  for that feat due to the evaluation metrics that are used
 Contributions: Our paper has four major contributions  aimed at better analyzing and comparing VQA algorithms:  N) We create a new VQA benchmark dataset where questions are divided into NN different categories based on the  task they solve; N) We propose two new evaluation metrics  that compensate for forms of dataset bias; N) We balance  the number of yes/no object presence detection questions to  NNNNN    assess whether a balanced distribution can help algorithms  learn better; and N) We introduce absurd questions that force  an algorithm to determine if a question is valid for a given  image
We then use the new dataset to re-train and evaluate both baseline and state-of-the-art VQA algorithms
We  found that our proposed approach enables more nuanced  comparisons of VQA algorithms, and helps us understand  the benefits of specific techniques better
In addition, it also  allowed us to answer several key questions about VQA algorithms, such as, ‘Is the generalization capacity of the algorithms hindered by the bias in the dataset?’, ‘Does the use  of spatial attention help answer specific question-types?’,  ‘How successful are the VQA algorithms in answering lesscommon questions?’, and ’Can the VQA algorithms differentiate between real and absurd questions?’  N
Background  N.N
Prior Natural Image VQA Datasets  Six datasets for VQA with natural images have been  released between N0NN–N0NN: DAQUAR [NN], COCOQA [NN], FM-IQA [N], The VQA Dataset [N], VisualNW [NN], and Visual Genome [NN]
FM-IQA needs human judges and has not been widely used, so we do not discuss it further
Table N shows statistics for the other datasets
 Following others [NN, NN, NN], we refer to the portion of The  VQA Dataset containing natural images as COCO-VQA
 Detailed dataset reviews can be found in [NN] and [NN]
 All of the aforementioned VQA datasets are biased
 DAQUAR and COCO-QA are small and have a limited  variety of question-types
Visual Genome, VisualNW, and  COCO-VQA are larger, but they suffer from several biases
 Bias takes the form of both the kinds of questions asked and  the answers that people give for them
For COCO-VQA, a  system trained using only question features achieves N0%  accuracy [NN]
This suggests that some questions have predictable answers
Without a more nuanced analysis, it is  challenging to determine what kinds of questions are more  dependent on the image
For datasets made using Mechanical Turk, annotators often ask object recognition questions,  e.g., ‘What is in the image?’ or ‘Is there an elephant in the  image?’
Note that in the latter example, annotators rarely  ask that kind of question unless the object is in the image
 On COCO-VQA, NN% of questions beginning with ‘Is there  a’ will have ‘yes’ as their ground truth answer
 In N0NN, the VQA N.0 [N] dataset was introduced
In  VQA N.0, the same question is asked for two different  images and annotators are instructed to give opposite answers, which helped reduce language bias
However, in  addition to language bias, these datasets are also biased in  their distribution of different types of questions and the distribution of answers within each question-type
Existing  VQA datasets use performance metrics that treat each test  instance with equal value (e.g., simple accuracy)
While  some do compute additional statistics for basic questiontypes, overall performance is not computed from these subscores [N, NN]
This exacerbates the issues with the bias  because the question-types that are more likely to be biased are also more common
Questions beginning with  ‘Why’ and ‘Where’ are rarely asked by annotators compared to those beginning with ‘Is’ and ’Are’
For example,  on COCO-VQA, improving accuracy on ‘Is/Are’ questions  by NN% will increase overall accuracy by over N%, but answering all ‘Why/Where’ questions correctly will increase  accuracy by only N.N% [NN]
Due to the inability of the existing evaluation metrics to properly address these biases,  algorithms trained on these datasets learn to exploit these  biases, resulting in systems that work poorly when deployed  in the real-world
 For related reasons, major benchmarks released in the  last decade do not use simple accuracy for evaluating image  recognition and related computer vision tasks, but instead  use metrics such as mean-per-class accuracy that compensates for unbalanced categories
For example, on CaltechN0N [N], even with balanced training data, simple accuracy  fails to address the fact that some categories were much easier to classify than others (e.g., faces and planes were easy  and also had the largest number of test images)
Mean perclass accuracy compensates for this by requiring a system  to do well on each category, even when the amount of test  instances in categories vary considerably
 Existing benchmarks do not require reporting accuracies  across different question-types
Even when they are reported, the question-types can be too coarse to be useful,  e.g., ‘yes/no’, ‘number’ and ‘other’ in COCO-VQA
To improve the analysis of the VQA algorithms, we categorize the  questions into meaningful types, calculate the sub-scores,  and incorporate them in our evaluation metrics
 N.N
Synthetic Datasets that Fight Bias  Previous works have studied bias in VQA and proposed  countermeasures
In [NN], the Yin and Yang dataset was created to study the effect of having an equal number of binary  (yes/no) questions about cartoon images
They found that  answering questions from a balanced dataset was harder
 This work is significant, but it was limited to yes/no questions and their approach using cartoon imagery cannot be  directly extended to real-world images
 One of the goals of this paper is to determine what kinds  of questions an algorithm can answer easily
In [N], the  SHAPES dataset was proposed, which has similar objectives
SHAPES is a small dataset, consisting of NN images  that are composed by arranging colored geometric shapes in  different spatial orientations
Each image has the same NNN  yes/no questions, resulting in NN,NNN questions
Although  SHAPES serves as an important adjunct evaluation, it alone  NNNN    cannot suffice for testing a VQA algorithm
The major limitation of SHAPES is that all of its images are of ND shapes,  which are not representative of real-world imagery
Along  similar lines, Compositional Language and Elementary Visual Reasoning (CLEVR) [NN] also proposes use of ND rendered geometric objects to study reasoning capacities of a  model
CLEVR is larger than SHAPES and makes use of  ND rendered geometric objects
In addition to shape and  color, it adds material property to the objects
CLEVR has  five types of questions: attribute query, attribute comparison, integer comparison, counting, and existence
 Both SHAPES and CLEVR were specifically tailored for  compositional language approaches [N] and downplay the  importance of visual reasoning
For instance, the CLEVR  question, ‘What size is the cylinder that is left of the brown  metal thing that is left of the big sphere?’ requires demanding language reasoning capabilities, but only limited visual  understanding is needed to parse simple geometric objects
 Unlike these three synthetic datasets, our dataset contains  natural images and questions
To improve algorithm analysis and comparison, our dataset has more (NN) explicitly  defined question-types and new evaluation metrics
 N
TDIUC for Nuanced VQA Analysis  In the past two years, multiple publicly released datasets  have spurred the VQA research
However, due to the biases  and issues with evaluation metrics, interpreting and comparing the performance of VQA systems can be opaque
 We propose a new benchmark dataset that explicitly assigns  questions into NN distinct categories
This enables measuring performance within each category and understand  which kind of questions are easy or hard for today’s best  systems
Additionally, we use evaluation metrics that further compensate for the biases
We call the dataset the  Task Driven Image Understanding Challenge (TDIUC)
The  overall statistics and example images of this dataset are  shown in Table N and Fig
N respectively
 TDIUC has NN question-types that were chosen to represent both classical computer vision tasks and novel highlevel vision tasks which require varying degrees of image  understanding and reasoning
The question-types are:  N
Object Presence (e.g., ‘Is there a cat in the image?’)  N
Subordinate Object Recognition (e.g., ‘What kind of  furniture is in the picture?’)  N
Counting (e.g., ’How many horses are there?’)  N
Color Attributes (e.g., ‘What color is the man’s tie?’)  N
Other Attributes (e.g., ‘What shape is the clock?’)  N
Activity Recognition (e.g., ‘What is the girl doing?’)  N
Sport Recognition (e.g.,‘What are they playing?’)  N
Positional Reasoning (e.g., ‘What is to the left of the  man on the sofa?’)  N
Scene Classification (e.g., ‘What room is this?’)  Q: What color is the suitcase? A:  Absurd Q: What color is the man’s  hat? A: White Q: What sport is  this? A: Tennis  Q: What is to the left of the blue  bus? A: Car Q: Is there a train in  the photo? A: No Q: How many bicycles are there? A: One  Figure N: Images from TDIUC and their corresponding  question-answer pairs
 N0
Sentiment Understanding (e.g.,‘How is she feeling?’)  NN
Object Utilities and Affordances (e.g.,‘What object  can be used to break glass?’)  NN
Absurd (i.e., Nonsensical queries about the image)  The number of each question-type in TDIUC is given in  Table N
The questions come from three sources
First,  we imported a subset of questions from COCO-VQA and  Visual Genome
Second, we created algorithms that generated questions from COCO’s semantic segmentation annotations [NN], and Visual Genome’s objects and attributes  annotations [NN]
Third, we used human annotators for certain question-types
In the following sections, we briefly  describe each of these methods
 N.N
Importing Questions from Existing Datasets  We imported questions from COCO-VQA and Visual  Genome belonging to all question-types except ‘object utilities and affordances’
We did this by using a large number  of templates and regular expressions
For Visual Genome,  we imported questions that had one word answers
For  COCO-VQA, we imported questions with one or two word  answers and in which five or more annotators agreed
 For color questions, a question would be imported if  it contained the word ‘color’ in it and the answer was a  commonly used color
Questions were classified as activity or sports recognition questions if the answer was one of  nine common sports or one of fifteen common activities and  the question contained common verbs describing actions or  sports, e.g., playing, throwing, etc
For counting, the question had to begin with ‘How many’ and the answer had to  be a small countable integer (N-NN)
The other categories  were determined using regular expressions
For example, a  question of the form ‘Are feeling ?’ was classified  as sentiment understanding and ‘What is to the right of/left  of/ behind the ?’ was classified as positional reasoning
 Similarly, ‘What <OBJECT CATEGORY> is in the image?’  NNNN    Table N: Comparison of previous natural image VQA datasets with TDIUC
For COCO-VQA, the explicitly defined number  of question-types is used, but a much finer granularity would be possible if they were individually classified
MC/OE refers  to whether open-ended or multiple-choice evaluation is used
 Images Questions Annotation  Source  Question  Types  Unique  Answers MC/OE  DAQUAR N,NNN NN,NN0 Both N NNN OE  COCO-QA NNN,NNN NNN,NNN Auto N NN0 OE  COCO-VQA N0N,NNN NNN,NNN Manual N NNN,NNN Both  VisualNW NN,N00 NNN,NNN Manual N NN,NNN MC  Visual Genome N0N,000 N,NNN,NNN Manual N N0N,NNN OE  TDIUC (This Paper) NNN,NNN N,NNN,NNN Both NN N,NNN OE  and similar templates were used to populate subordinate object recognition questions
This method was used for questions about the season and weather as well, e.g., ‘What season is this?’, ‘Is this rainy/sunny/cloudy?’, or ‘What is the  weather like?’ were imported to scene classification
 N.N
Generating Questions using Image Annotations  Images in the COCO dataset and Visual Genome both  have individual regions with semantic knowledge attached  to them
We exploit this information to generate new questions using question templates
To introduce variety, we  define multiple templates for each question-type and use  the annotations to populate them
For example, for counting we use N templates, e.g., ‘How many <objects> are  there?’, ‘How many <objects> are in the photo?’, etc
 Since the COCO and Visual Genome use different annotation formats, we discuss them separately
 N.N.N Questions Using COCO annotations  Sport recognition, counting, subordinate object recognition,  object presence, scene understanding, positional reasoning,  and absurd questions were created from COCO, similar to  the scheme used in [NN]
For counting, we count the number of object instances in an image annotation
To minimize  ambiguity, this was only done if objects covered an area of  at least N,000 pixels
 For subordinate object recognition, we create questions that require identifying an object’s subordinate-level  object classification based on its larger semantic category
 To do this, we use COCO supercategories, which are semantic concepts encompassing several objects under a common theme, e.g., the supercategory ‘furniture’ contains  chair, couch, etc
If the image contains only one type of  furniture, then a question similar to ‘What kind of furniture is in the picture?’ is generated because the answer is  not ambiguous
Using similar heuristics, we create questions about identifying food, electronic appliances, kitchen  appliances, animals, and vehicles
 To create object presence questions, we find images  with objects that have an area larger than N,000 pixels and  then produce a question similar to ‘Is there a <object>  in the picture?’ These questions will have ‘yes’ as an answer
To create negative questions, we ask questions about  COCO objects that are not present in an image
To make  this harder, we prioritize the creation of questions referring  to absent objects that belong to the same supercategory of  objects that are present in the image
A street scene is more  likely to contain trucks and cars than it is to contain couches  and televisions
Therefore, it is more difficult to answer ‘Is  there a truck?’ in a street scene than it is to answer ‘Is there  a couch?’  For sport recognition questions, we detect the presence of specific sports equipment in the annotations and  ask questions about the type of sport being played
Images  must only contain sports equipment for one particular sport
 A similar approach was used to create scene understanding  questions
For example, if a toilet and a sink are present  in annotations, the room is a bathroom and an appropriate  scene recognition question can be created
Additionally, we  use the supercategories ‘indoor’ and ‘outdoor’ to ask questions about where a photo was taken
 For creating positional reasoning questions, we use the  relative locations of bounding boxes to create questions  similar to ‘What is to the left/right of <object>?’ This  can be ambiguous due to overlapping objects, so we employ  the following heuristics to eliminate ambiguity: N) The vertical separation between the two bounding boxes should be  within a small threshold; N) The objects should not overlap  by more than the half the length of its counterpart; and N)  The objects should not be horizontally separated by more  than a distance threshold, determined by subjectively judging optimal separation to reduce ambiguity
We tried to generate above/below questions, but the results were unreliable
 Absurd questions test the ability of an algorithm to  judge when a question is not answerable based on the image’s content
To make these, we make a list of the objects  that are absent from a given image, and then we find questions from rest of TDIUC that ask about these absent objects, with the exception of yes/no and counting questions
 This includes questions imported from COCO-VQA, autoNNNN    Table N: The number of questions per type in TDIUC
 Questions Unique Answers  Scene Recognition NN,N0N NN  Sport Recognition NN,NNN NN  Color Attributes NNN,NNN NN  Other Attributes NN,NNN NNN  Activity Recognition N,NN0 NN  Positional Reasoning NN,NNN N,N00  Sub
Object Recognition NN,NNN NNN  Absurd NNN,NNN N  Utility/Affordance NNN NNN  Object Presence NNN,NNN N  Counting NNN,NNN NN  Sentiment Understanding N,0NN NN  Grand Total N,NNN,NNN N,NNN  generated questions, and manually created questions
We  make a list of all possible questions that would be ‘absurd’  for each image and we uniformly sample three questions per  image
In effect, we will have same question repeated multiple times throughout the dataset, where it can either be a  genuine question or a nonsensical question
The algorithm  must answer ‘Does Not Apply’ if the question is absurd
 N.N.N Questions Using Visual Genome annotations  Visual Genome’s annotations contain region descriptions,  relationship graphs, and object boundaries
However, the  annotations can be both non-exhaustive and duplicated,  which makes using them to automatically make QA pairs  difficult
We only use Visual Genome to make color and  positional reasoning questions
The methods we used are  similar to those used with COCO, but additional precautions  were needed due to quirks in their annotations
Additional  details are provided in the Supplemental Materials
 N.N
Manual Annotation  Creating sentiment understanding and object utility/affordance questions cannot be readily done using templates, so we used manual annotation to create these
 Twelve volunteer annotators were trained to generate these  questions, and they used a web-based annotation tool that  we developed
They were shown random images from  COCO and Visual Genome and could also upload images
 N.N
Post Processing  Post processing was performed on questions from all  sources
All numbers were converted to text, e.g., N became  two
All answers were converted to lowercase, and trailing  punctuation was stripped
Duplicate questions for the same  image were removed
All questions had to have answers  that appeared at least twice
The dataset was split into train  and test splits with N0% for train and N0% for test
 N
Proposed Evaluation Metric  One of the main goals of VQA research is to build computer vision systems capable of many tasks, instead of only  having expertise at one specific task (e.g., object recognition)
For this reason, some have argued that VQA is a kind  of Visual Turing Test [NN]
However, if simple accuracy  is used for evaluating performance, then it is hard to know  if a system succeeds at this goal because some questiontypes have far more questions than others
In VQA, skewed  distributions of question-types are to be expected
If each  test question is treated equally, then it is difficult to assess  performance on rarer question-types and to compensate for  bias
We propose multiple measures to compensate for bias  and skewed distributions
 To compensate for the skewed question-type distribution, we compute accuracy for each of the NN questiontypes separately
However, it is also important to have a  final unified accuracy metric
Our overall metrics are the  arithmetic and harmonic means across all per question-type  accuracies, referred to as arithmetic mean-per-type (Arithmetic MPT) accuracy and harmonic mean-per-type accuracy (Harmonic MPT)
Unlike the Arithmetic MPT, Harmonic MPT measures the ability of a system to have high  scores across all question-types and is skewed towards lowest performing categories
 We also use normalized metrics that compensate for bias  in the form of imbalance in the distribution of answers  within each question-type, e.g., the most repeated answer  ‘two’ covers over NN% of all the counting-type questions
 To do this, we compute the accuracy for each unique answer separately within a question-type and then average  them together for the question-type
To compute overall  performance, we compute the arithmetic normalized mean  per-type (N-MPT) and harmonic N-MPT scores
A large  discrepancy between unnormalized and normalized scores  suggests an algorithm is not generalizing to rarer answers
 N
Algorithms for VQA  While there are alternative formulations (e.g., [N, N0]),  the majority of VQA systems formulate it as a classification problem in which the system is given an image and a  question, with the answers as categories
[N, NN, N, NN, N, NN,  NN, N0, NN, NN, NN, NN, NN, NN, N0, NN]
Almost all systems  use CNN features to represent the image and either a recurrent neural network (RNN) or a bag-of-words model for the  question
We briefly review some of these systems, focusing on the models we compare in experiments
For a more  comprehensive review, see [NN] and [NN]
 Two simple VQA baselines are linear or multi-layer perceptron (MLP) classifiers that take as input the question and  image embeddings concatenated to each other [N, NN, NN],  where the image features come from the last hidden layer  NNNN    of a CNN
These simple approaches often work well and  can be competitive with complex attentive models [NN, NN]
 Spatial attention has been heavily investigated in VQA  models [N, NN, NN, N0, NN, N0, N]
These systems weigh  the visual features based on their relevance to the question,  instead of using global features, e.g., from the last hidden  layer of a CNN
For example, to answer ‘What color is the  bear?’ they aim emphasize the visual features around the  bear and suppress other features
 The MCB system [N] won the CVPR-N0NN VQA Workshop Challenge
In addition to using spatial attention, it implicitly computes the outer product between the image and  question features to ensure that all of their elements interact
 Explicitly computing the outer product would be slow and  extremely high dimensional, so it is done using an efficient  approximation
It uses an long short-term memory (LSTM)  networks to embed the question
 The neural module network (NMN) is an especially  interesting compositional approach to VQA [N, N]
The  main idea is to compose a series of discrete modules  (sub-networks) that can be executed collectively to answer a given question
To achieve this, they use a variety of modules, e.g., the find(x) module outputs a  heat map for detecting x
To arrange the modules, the  question is first parsed into a concise expression (called  an S-expression), e.g., ‘What is to the right of the car?’  is parsed into (what car);(what right);(what  (and car right))
Using these expressions, modules  are composed into a sequence to answer the query
 The multi-step recurrent answering units (RAU) model  for VQA is another state-of-the-art method [NN]
Each inference step in RAU consists of a complete answering block  that takes in an image, a question, and the output from the  previous LSTM step
Each of these is part of a larger LSTM  network that progressively reasons about the question
 N
Experiments  We trained multiple baseline models as well as state-ofthe-art VQA methods on TDIUC
The methods we use are:  • YES: Predicts ‘yes’ for all questions
 • REP: Predicts the most repeated answer in a questiontype category using an oracle
 • QUES: A linear softmax classifier given only question  features (image blind)
 • IMG: A linear softmax classifier given only image features (question blind)
 • Q+I: A linear classifier given the question and image.
 • MLP: A N-layer MLP fed question and image features
 • MCB: MCB [N] without spatial attention
 • MCB-A: MCB [N] with spatial attention
 • NMN: NMN from [N] with minor modifications
 • RAU: RAU [NN] with minor modifications
 For image features, ResNet-NNN [N] with NNN×NNN images  was used for all models
 QUES and IMG provide information about biases in the  dataset
QUES, Q+I, and MLP all use NN00-dimensional  skip-thought vectors [NN] to embed the question, as was  done in [NN]
For image features, these all use the ‘poolN’  layer of ResNet-NNN normalized to unit length
MLP is a Nlayer net with a softmax output layer
The N ReLU hidden  layers have N000, N000, and N000 units, respectively
During training, dropout (0.N) was used for the hidden layers
 For MCB, MCB-A, NMN and RAU, we used publicly  available code to train them on TDIUC
The experimental  setup and hyperparamters were kept unchanged from the  default choices in the code, except for upgrading NMN and  RAU’s visual representation to both use ResNet-NNN
 Results on TDIUC for these models are given in Table N
 Accuracy scores are given for each of the NN question-types  in Table N, and scores that are normalized by using meanper-unique-answer are given in supplementary Table N
 N
Detailed Analysis of VQA Models  N.N
Easy Question-Types for Today’s Methods  By inspecting Table N, we can see that some questiontypes are comparatively easy (> N0%) under MPT: scene  recognition, sport recognition, and object presence
High  accuracy is also achieved on absurd, which we discuss in  greater detail in Sec
N.N
Subordinate object recognition is  moderately high (> N0%), despite having a large number  of unique answers
Accuracy on counting is low across all  methods, despite a large number of training data
For the remaining question-types, more analysis is needed to pinpoint  whether the weaker performance is due to lower amounts of  training data, bias, or limitations of the models
We next investigate how much of the good performance is due to bias  in the answer distribution, which N-MPT compensates for
 N.N
Effects of the Proposed Accuracy Metrics  One of our major aims was to compensate for the fact  that algorithms can achieve high scores by simply learning  to answer more populated and easier question-types
For  existing datasets, earlier work has shown that simple baseline methods routinely exceed more complex methods using  simple accuracy [NN, NN, N0]
On TDIUC, MLP surpasses  MCB and NMN in terms of simple accuracy, but a closer  inspection reveals that MLP’s score is highly determined  by performance on categories with a large number of examples, such as ‘absurd’ and ‘object presence.’ Using MPT, we  find that both NMN and MCB outperform MLP
Inspecting normalized scores for each question-type (supplementary Table N) shows an even more pronounced differences,  which is also reflected in arithmetic N-MPT score presented  in Table N
This indicates that MLP is prone to overfitting
 NNN0    Table N: Results for all VQA models
The unnormalized accuracy for each question-type is shown
Overall performance is  reported using N metrics
Overall (Arithmetic MPT) and Overall (Harmonic MPT) are averages of these sub-scores, providing  a clearer picture of performance across question-types than simple accuracy
Overall Arithmetic N-MPT and Harmonic NMPT normalize across unique answers to better analyze the impact of answer imbalance (see Sec
N)
Normalized scores for  individual question-types are presented in the supplementary materials
* denotes training without absurd questions
 YES REP IMG QUES Q+I *Q+I MLP MCB *MCB MCB-A NMN RAU  Scene Recognition NN.N0 NN.N0 NN.NN NN.NN NN.NN NN.NN NN.NN NN.0N NN.NN NN.0N NN.NN NN.NN  Sport Recognition 0.00 NN.0N NN.NN NN.NN NN.NN NN.N0 N0.NN NN.NN NN.NN NN.NN NN.NN NN.NN  Color Attributes 0.00 NN.NN 0.NN NN.N0 NN.NN N0.NN NN.NN NN.NN NN.0N NN.NN NN.NN NN.NN  Other Attributes 0.00 NN.NN N.0N NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  Activity Recognition 0.00 NN.NN N.0N N0.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.N0  Positional Reasoning 0.00 N.0N N.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.0N NN.N0 NN.NN NN.NN  Sub
Object Recognition 0.00 N.NN N0.NN NN.N0 N0.NN NN.NN N0.NN NN.NN NN.NN NN.NN NN.0N NN.NN  Absurd 0.00 N00.00 NN.NN NN.NN NN.NN N/A NN.NN NN.NN N/A NN.NN NN.NN NN.0N  Utility and Affordances NN.N0 NN.N0 N.NN NN.NN NN.NN N0.NN NN.NN NN.NN NN.NN NN.0N NN.NN NN.NN  Object Presence N0.00 N0.00 N0.NN NN.0N NN.NN NN.N0 NN.NN NN.NN NN.NN NN.NN NN.N0 NN.NN  Counting 0.00 NN.NN 0.N0 NN.NN NN.NN NN.NN NN.NN N0.NN N0.0N NN.0N NN.NN NN.NN  Sentiment Understanding NN.NN NN.NN NN.NN NN.NN NN.00 NN.NN NN.NN NN.NN NN.NN NN.NN NN.0N N0.0N  Overall (Arithmetic MPT) NN.N0 NN.NN N.NN NN.NN NN.NN NN.0N N0.NN NN.NN NN.0N NN.N0 NN.NN NN.NN  Overall (Harmonic MPT) 0.00 NN.NN N.NN NN.NN NN.NN N0.N0 NN.N0 NN.0N NN.NN N0.NN NN.NN NN.00  Overall (Arithmetic N-MPT) N.NN NN.NN N.NN NN.NN NN.NN NN.N0 NN.NN NN.NN NN.NN NN.NN NN.00 NN.0N  Overall (Harmonic N-MPT) 0.00 0.NN N.NN N.NN NN.NN NN.N0 N.NN NN.NN NN.N0 NN.NN NN.NN NN.NN  Simple Accuracy NN.NN NN.NN NN.NN NN.NN NN.NN NN.N0 NN.0N NN.N0 NN.0N NN.NN NN.NN NN.NN  Similar observations can be made for MCB-A compared to  RAU, where RAU outperforms MCB-A using simple accuracy, but scores lower on all the metrics designed to compensate for the skewed answer distribution and bias
 Comparing the unnormalized and normalized metrics  can help us determine the generalization capacity of the  VQA algorithms for a given question-type
A large difference in these scores suggests that an algorithm is relying on the skewed answer distribution to obtain high scores
 We found that for MCB-A, the accuracy on subordinate object recognition drops from NN.NN% with unnormalized to  NN.NN% with normalized, and for scene recognition it drops  from NN.0N% (unnormalized) to NN.NN% (normalized)
Both  these categories have a heavily skewed answer distribution;  the top-NN answers in subordinate object recognition and  the top-N answers in scene recognition cover over N0% of  all questions in their respective question-types
This shows  that question-types that appear to be easy may simply be  due to the algorithms learning the answer statistics
A  truly easy question-type will have similar performance for  both unnormalized and normalized metrics
For example,  sport recognition shows only NN.NN% drop compared to a  N0.NN% drop for counting, despite counting having same  number of unique answers and far more training data
By  comparing relative drop in performance between normalized and unnormalized metric, we can also compare the  generalization capability of the algorithms, e.g., for subordinate object recognition, RAU has higher unnormalized  score (NN.NN%) compared to MCB-A (NN.NN%)
However,  for normalized scores, MCB-A has significantly higher performance (NN.NN%) than RAU (NN.NN%)
This shows RAU  may be more dependent on the answer distribution
Similar  observations can be made for MLP compared to MCB
 N.N
Can Algorithms Predict Rare Answers?  In the previous section, we saw that the VQA models  struggle to correctly predict rarer answers
Are the less repeated questions actually harder to answer, or are the algorithms simply biased toward more frequent answers? To  study this, we created a subset of TDIUC that only consisted  of questions that have answers repeated less than N000  times
We call this dataset TDIUC-Tail, which has NN,NN0  train and NN,0NN test questions
Then, we trained MCB on:  N) the full TDIUC dataset; and N) TDIUC-Tail
Both versions were evaluated on the validation split of TDIUC-Tail
 We found that MCB trained only on TDIUC-Tail outperformed MCB trained on all of TDIUC across all questiontypes (details are in supplementary Table N)
This shows  that MCB is capable of learning to correctly predict rarer  answers, but it is simply biased towards predicting more  common answers to maximize overall accuracy
Using normalized accuracy disincentivizes the VQA algorithms’ reliance on the answer statistics, and for deploying a VQA  system it may be useful to optimize directly for N-MPT
 N.N
Effects of Including Absurd Questions  Absurd questions force a VQA system to look at the image to answer the question
In TDIUC, these questions are  NNNN    sampled from the rest of the dataset, and they have a high  prior probability of being answered ‘Does not apply.’ This  is corroborated by the QUES model, which achieves a high  accuracy on absurd; however, for the same questions when  they are genuine for an image, it only achieves N.NN% accuracy on these questions
Good absurd performance is  achieved by sacrificing performance on other categories
A  robust VQA system should be able to detect absurd questions without then failing on others
By examining the accuracy on real questions that are identical to absurd questions, we can quantify an algorithm’s ability to differentiate the absurd questions from the real ones
We found that  simpler models had much lower accuracy on these questions, (QUES: N.NN%, Q+I: NN%), compared to more complex models (MCB: NN.NN%, MCB-A: NN.NN%)
 To further study this, we we trained two VQA systems,  Q+I and MCB, both with and without absurd
The results are presented in Table N
For Q+I trained without  absurd questions, accuracies for other categories increase  considerably compared to Q+I trained with full TDIUC, especially for question-types that are used to sample absurd  questions, e.g., activity recognition (NN% when trained with  absurd and NN% without)
Arithmetic MPT accuracy for  the Q+I model that is trained without absurd (NN.0N%) is  also substantially greater than MPT for the model trained  with absurd (NN.NN% for all categories except absurd)
 This suggests that Q+I is not properly discriminating between absurd and real questions and is biased towards misidentifying genuine questions as being absurd
In contrast,  MCB, a more capable model, produces worse results for  absurd, but the version trained without absurd shows much  smaller differences than Q+I, which shows that MCB is  more capable of identifying absurd questions
 N.N
Effects of Balancing Object Presence  In Sec
N.N, we saw that a skewed answer distribution  can impact generalization
This effect is strong even for  simple questions and affects even the most sophisticated  algorithms
Consider MCB-A when it is trained on both  COCO-VQA and Visual Genome, i.e., the winner of the  CVPR-N0NN VQA Workshop Challenge
When it is evaluated on object presence questions from TDIUC, which contains N0% ‘yes’ and N0% ‘no’ questions, it correctly predicts ‘yes’ answers with NN.N% accuracy, but only NN.N% for  questions with ‘no’ as an answer
However, after training it  on TDIUC, MCB-A is able to achieve NN.0N% for ‘yes’ and  NN.NN% for ‘no.’ MCB-A performed poorly by learning the  biases in the COCO-VQA dataset, but it is capable of performing well when the dataset is unbiased
Similar observations about balancing yes/no questions were made in [NN]
 Datasets could balance simple categories like object presence, but extending the same idea to all other categories is a  challenging task and undermines the natural statistics of the  real-world
Adopting mean-per-class and normalized accuracy metrics can help compensate for this problem
 N.N
Advantages of Attentive Models  By breaking questions into types, we can assess which  types benefit the most from attention
We do this by comparing the MCB model with and without attention, i.e.,  MCB and MCB-A
As seen in Table N, attention helped improve results on several question categories
The most pronounced increases are for color recognition, attribute recognition, absurd, and counting
All of these question-types  require the algorithm to detect specified object(s) (or lack  thereof) to be answered correctly
MCB-A computes attention using local features from different spatial locations,  instead of global image features
This aids in localizing individual objects
The attention mechanism learns the relative importance of these features
RAU also utilizes spatial  attention and shows similar increments
 N.N
Compositional and Modular Approaches  NMN, and, to a lesser extent, RAU propose compositional approaches for VQA
For COCO-VQA, NMN  has performed worse than some MLP models [NN] using  simple accuracy
We hoped that it would achieve better performance than other models for questions that require logically analyzing an image in a step-by-step manner, e.g., positional reasoning
However, while NMN  did perform better than MLP using MPT and N-MPT  metric, we did not see any substantial benefits in specific question-types
This may be because NMN is limited by the quality of the ‘S-expression’ parser, which  produces incorrect or misleading parses in many cases
 For example, ‘What color is the jacket of the man on  the far left?’ is parsed as (color jacket);(color  leave);(color (and jacket leave))
This expression not only fails to parse ‘the man’, which is a crucial  element needed to correctly answer the question, but also  wrongly interprets ‘left’ as past tense of leave
 RAU performs inference over multiple hops, and because each hop contains a complete VQA system, it can  learn to solve different tasks in each step
Since it is trained  end-to-end, it does not need to rely on rigid question parses
 It showed very good performance in detecting absurd questions and also performed well on other categories
 N
Conclusion  We introduced TDIUC, a VQA dataset that consists of  NN explicitly defined question-types, including absurd questions, and we used it to perform a rigorous analysis of recent  VQA algorithms
We proposed new evaluation metrics to  compensate for biases in VQA datasets
Results show that  the absurd questions and the new evaluation metrics enable  a deeper understanding of VQA algorithm behavior
 NNNN    References  [N] J
Andreas, M
Rohrbach, T
Darrell, and D
Klein
Deep  compositional question answering with neural module networks
In CVPR, N0NN
N, N, N  [N] J
Andreas, M
Rohrbach, T
Darrell, and D
Klein
Learning to compose neural networks for question answering
In  NAACL, N0NN
N  [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra, C
L
 Zitnick, and D
Parikh
VQA: Visual question answering
In  ICCV, N0NN
N, N, N  [N] L
Fei-Fei, R
Fergus, and P
Perona
One-shot learning of  object categories
IEEE Trans
Pattern Analysis and Machine  Intelligence, NN:NNN–NNN, N00N
N  [N] A
Fukui, D
H
Park, D
Yang, A
Rohrbach, T
Darrell, and  M
Rohrbach
Multimodal compact bilinear pooling for visual question answering and visual grounding
In EMNLP,  N0NN
N, N, N  [N] H
Gao, J
Mao, J
Zhou, Z
Huang, L
Wang, and W
Xu
 Are you talking to a machine? Dataset and methods for multilingual image question answering
In NIPS, N0NN
N, N  [N] Y
Goyal, T
Khot, D
Summers-Stay, D
Batra, and  D
Parikh
Making the V in VQA matter: Elevating the role  of image understanding in Visual Question Answering
In  CVPR, N0NN
N  [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N  [N] I
Ilievski, S
Yan, and J
Feng
A focused dynamic attention model for visual question answering
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N  [N0] A
Jabri, A
Joulin, and L
van der Maaten
Revisiting visual  question answering baselines
In ECCV, N0NN
N, N  [NN] A
Jiang, F
Wang, F
Porikli, and Y
Li
Compositional  memory for visual question answering
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
N  [NN] J
Johnson, B
Hariharan, L
van der Maaten, L
Fei-Fei, C
L
 Zitnick, and R
Girshick
CLEVR: A diagnostic dataset for  compositional language and elementary visual reasoning
In  CVPR, N0NN
N  [NN] K
Kafle and C
Kanan
Answer-type prediction for visual  question answering
In CVPR, N0NN
N, N, N, N  [NN] K
Kafle and C
Kanan
Visual question answering: Datasets,  algorithms, and future challenges
Computer Vision and Image Understanding, N0NN
N, N  [NN] K
Kafle, M
Yousefhussien, and C
Kanan
Data augmentation for visual question answering
In International Conference on Natural Language Generation (INLG), N0NN
N  [NN] J.-H
Kim, S.-W
Lee, D.-H
Kwak, M.-O
Heo, J
Kim, J.W
Ha, and B.-T
Zhang
Multimodal residual learning for  visual QA
In NIPS, N0NN
N  [NN] R
Kiros, Y
Zhu, R
Salakhutdinov, R
S
Zemel, A
Torralba,  R
Urtasun, and S
Fidler
Skip-thought vectors
In NIPS,  N0NN
N  [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, M
Bernstein, and L
Fei-Fei
Visual Genome: Connecting language and vision using crowdsourced dense image annotations
N0NN
N, N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft COCO: Common objects in context
In ECCV
N0NN
N  [N0] J
Lu, J
Yang, D
Batra, and D
Parikh
Hierarchical  question-image co-attention for visual question answering
 In NIPS, N0NN
N, N  [NN] M
Malinowski and M
Fritz
A multi-world approach to  question answering about real-world scenes based on uncertain input
In NIPS, N0NN
N, N, N  [NN] M
Malinowski, M
Rohrbach, and M
Fritz
Ask your neurons: A neural-based approach to answering questions about  images
In ICCV, N0NN
N  [NN] H
Noh and B
Han
Training recurrent answering units  with joint loss minimization for VQA
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [NN] H
Noh, P
H
Seo, and B
Han
Image question answering  using convolutional neural network with dynamic parameter  prediction
In CVPR, N0NN
N  [NN] M
Ren, R
Kiros, and R
Zemel
Exploring models and data  for image question answering
In NIPS, N0NN
N, N  [NN] K
Saito, A
Shin, Y
Ushiku, and T
Harada
Dualnet:  Domain-invariant network for visual question answering
 arXiv preprint arXiv:NN0N.0NN0N, N0NN
N  [NN] K
J
Shih, S
Singh, and D
Hoiem
Where to look: Focus  regions for visual question answering
In CVPR, N0NN
N, N  [NN] Q
Wu, D
Teney, P
Wang, C
Shen, A
Dick, and A
v
d
 Hengel
Visual question answering: A survey of methods  and datasets
Computer Vision and Image Understanding,  N0NN
N, N  [NN] Q
Wu, P
Wang, C
Shen, A
van den Hengel, and A
R
 Dick
Ask me anything: Free-form visual question answering based on knowledge from external sources
In CVPR,  N0NN
N, N  [N0] C
Xiong, S
Merity, and R
Socher
Dynamic memory networks for visual and textual question answering
In ICML,  N0NN
N  [NN] H
Xu and K
Saenko
Ask, attend and answer: Exploring  question-guided spatial attention for visual question answering
In ECCV, N0NN
N, N  [NN] Z
Yang, X
He, J
Gao, L
Deng, and A
J
Smola
Stacked  attention networks for image question answering
In CVPR,  N0NN
N, N  [NN] P
Zhang, Y
Goyal, D
Summers-Stay, D
Batra, and  D
Parikh
Yin and yang: Balancing and answering binary  visual questions
In CVPR, N0NN
N, N  [NN] B
Zhou, Y
Tian, S
Sukhbaatar, A
Szlam, and R
Fergus
Simple baseline for visual question answering
CoRR,  abs/NNNN.0NNNN, N0NN
N, N, N  [NN] Y
Zhu, O
Groth, M
Bernstein, and L
Fei-Fei
VisualNw:  Grounded question answering in images
In CVPR, N0NN
N  NNNNVQS: Linking Segmentations to Questions and Answers for Supervised Attention in VQA and Question-Focused Semantic Segmentation   VQS: Linking Segmentations to Questions and Answers for  Supervised Attention in VQA and Question-Focused Semantic Segmentation  Chuang GanN Yandong LiN Haoxiang LiN Chen SunN Boqing GongN NIIIS, Tsinghua University, China NCRCV, University of Central Florida, USA  NAdobe Research, USA NGoogle Research, USA  Abstract  Rich and dense human labeled datasets are among the  main enabling factors for the recent advance on visionlanguage understanding
Many seemingly distant annotations (e.g., semantic segmentation and visual question answering (VQA)) are inherently connected in that they reveal  different levels and perspectives of human understandings  about the same visual scenes — and even the same set of  images (e.g., of COCO)
The popularity of COCO correlates those annotations and tasks
Explicitly linking them  up may significantly benefit both individual tasks and the  unified vision and language modeling
 We present the preliminary work of linking the instance  segmentations provided by COCO to the questions and answers (QAs) in the VQA dataset, and name the collected  links visual questions and segmentation answers (VQS)
 They transfer human supervision between the previously  separate tasks, offer more effective leverage to existing  problems, and also open the door for new research problems and models
We study two applications of the VQS  data in this paper: supervised attention for VQA and a  novel question-focused semantic segmentation task
For the  former, we obtain state-of-the-art results on the VQA real  multiple-choice task by simply augmenting the multilayer  perceptrons with some attention features that are learned  using the segmentation-QA links as explicit supervision
To  put the latter in perspective, we study two plausible methods and compare them to an oracle method assuming that  the instance segmentations are given at the test stage
 N
Introduction  Connecting visual understanding with natural language  has received extensive attentions in recent years
We have  witnessed the resurgence of image captioning [NN, NN, NN,  N, NN, N, NN, NN, NN, N0] which is often addressed by jointly  modeling visual and textual content with deep neural networks
However, image captions tend to be diverse and subCode and data: https://github.com/Cold-Winter/vqs
 What time is it? Is the street empty?  How many buses have only a single level?  Is he wearing a tie?  What is next to the dog?  Figure N
Taking as input an image and a question about the image,  an algorithm for the question-focused semantic segmentation is  desired to generate some segmentation mask(s) over the entities in  the image that can visually answer the question
 jective — it is hard to evaluate the quality of captions generated by different algorithms [N, N0, N], and tend to miss  subtle details — in training, the models may be led to capturing the scene-level gist rather than fine-grained entities
 In light of the premises and demerits of image captioning,  visual question answering (VQA) [N, N0, NN, NN] and visual grounding [NN, NN, NN, NN, NN, NN, NN] are proposed, in  parallel, to accommodate automatic evaluation and multiple levels of focus on the visual entities (e.g., scene, object,  activity, attribute, context, relationships, etc.)
 Rich and dense human annotated datasets are arguably  the main “enabler”, among others, for this line of exciting works on vision-language understanding
COCO [NN]  is especially noticeable among them
It contains mainly  classical labels (e.g., segmentations, object categories and  instances, key points, etc.) and image captions
Many research groups have then collected additional labels of the  COCO images for a variety of tasks
Agrawal et al
crowdsourced questions and answers (QAs) about a subset of the  COCO images and abstract scenes [N]
Zhu et al
collected  seven types of QAs in which the object mentions are asNNNNN    sociated with bounding boxes in the images [N0]
Mao et  al
[NN] and Yu et al
[NN] have users to give referring expressions that each pinpoints a unique object in an image
 The Visual Genome dataset [NN] also intersects with COCO  in terms of images and provides dense human annotations,  especially scene graphs
 These seemingly distant annotations are inherently connected in the sense that they reveal different perspectives of  human understandings about the same COCO images
The  popularity of COCO could strongly correlate those annotations — and even tasks
Explicitly linking them up, as we  envision, can significantly benefit both individual tasks and  unified vision-language understanding, as well as the corresponding approaches and models
One of our contributions  in this paper is to initiate the preliminary work on this
In  particular, we focus on linking the segmentations provided  by COCO [NN] to the QAs in the VQA dataset [N]
Displaying an image and a QA pair about the image, we ask  the participant to choose the segmentation(s) of the image  in order to visually answer the question
 Figure N illustrates some of the collected “visual answers”
For the question “What is next to the dog?”, the  output is supposed to be the segmentation mask over the  man
For the question “What time is it?”, the clock should  be segmented out
Another intriguing example is that the  cars are the desired segmentations to answer “Is this street  empty?”, providing essential visual evidence for the simple  text answer “no”
Note that while many visual entities could  be mentioned in a question, we only ask the participants to  choose the target segmentation(s) that visually answer the  question
This simplifies the annotation task and results in  higher agreement between participants
Section N details  the annotation collection process and statistics
 Two related datasets
Das et al
have collected some human attention maps for the VQA task [N]
They blur the  images and then ask users to scratch them to seek visual  cues that help answer the questions
The obtained attention maps are often small, revealing meaningful parts rather  than complete objects
The object parts are also mixed with  background areas and with each other
As a result, the human attention maps are likely less accurate supervision for  the attention based approaches to VQA than the links we  built between segmentations and QAs
Our experiments  verify this hypothesis (cf
Section N)
While bounding boxes  are provided in VisualNW [N0] for object mentions in QAs,  they do not serve for the purpose of directly answering the  questions except for the “pointing” type of questions
In  contrast, we provide direct visual answers in the form of  segmentations to more question types
 N.N
Applications of the segmentation-QA links  We call the collected links between the COCO segmentations [NN] and QA pairs in the VQA dataset [N] visual questions and segmentation answers (VQS)
Such links transfer  human supervision between the previously separate tasks,  i.e., semantic segmentation and VQA
They enable us to  tackle existing problems with more effective leverage than  before and also open the door for new research problems  and models for the vision-language understanding
We  study two applications of our VQS dataset in this paper: supervised attention for VQA and a novel question-focused  semantic segmentation (QFSS) task
For the former, we  obtain state-of-the-art results on the VQA real multiplechoice task by simply augmenting the multilayer perceptrons (MLP) of [NN] with attention features
 N.N.N Supervised attention for VQA  VQA is designed to answer natural language questions  about images in the form of short texts
The attention  scheme is often found useful for VQA, by either attending  particular image regions [NN, NN, NN, NN, NN] or modeling  object relationships [N, NN]
However, lacking explicit attention annotations, the existing methods opt for latent variables and use indirect cues (e.g., text answers) for inference
As a result, the machine-generated attention maps are  poorly correlated with human attention maps [N]
This is  not surprising since latent variables hardly match semantic  interpretations due to the lack of explicit training signals;  similar observations exist in other studies, e.g., object detection [N], video recognition [NN] and text processing [NN]
 These phenomena highlight the need for explicit links  between the visual and text answers, realized in this work  as VQS
We show that, by supervised learning to attend  different image regions using the collected segmentationQA links, we can boost the simple MLP model [NN] to very  compelling performance on the VQA real multi-choice task
 N.N.N Question-focused semantic segmentation (QFSS)  In addition to the supervised attention for better tackling  VQA, VQS also enables us to explore a novel questionfocused semantic segmentation (QFSS) task
 Since VQA desires only text answers, there exist potential shortcuts for the learning agent, e.g., to generate  correct answers without accurately reasoning the locations  and relations of different visual entities
While visual  grounding (VG) avoids the caveat by placing bounding  boxes [NN, NN, NN, NN] or segmentations [NN] over the target  visual entities, the scope of the text expressions in existing  VG works is often limited to the visual entities present in  the image
In order to bring together the best of VQA and  VG, we propose the QFSS task, whose objective is to produce pixel-wise segmentations in order to visually answer  the questions about images
It effectively borrows the versatile questions from VQA and meanwhile resembles the  design of VG in terms of the pixel-wise segmentations as  the desired output
 NNNN    (a) Is there broccoli in the dish? (j) What is the person doing? (m) What sex of person holding remote?  (b) What color is the coffee cup?  (f) How many person wear hats?  (e) How many cows are standing up?  (d) How many computer keyboards?   (i) What shape is the bottom pizza?  (h) Which horse is closer to the camera?  (c) Is the woman wearing ring?  (k) Is the man riding bike?  (l) What sport is played?  (g) What is on the bench next to woman?  (n) What time is it?  (o) What fast food restaurant can be seen?  Figure N
Some typical examples in our VQS dataset
From the left to right, the underlying tasks are respectively about object localization,  semantic segmentation, understanding object relationships, fine-grained activity localization, and commonsense reasoning
 Given an image and a question about the image, we propose a mask aggregation approach to generating a segmentation mask as the visual answer
Since QFSS is a new task,  to put it in perspective, we not only compare the proposed  approach to competing baselines but also study an upperbound method by assuming all instance segmentations are  given as oracles at the test stage
 Hu et al.’s work [NN] is the most related to QFSS
They  learn to ground text expressions in the form of image segmentations
Unlike the questions used in this work that are  flexible to incorporate commonsense and knowledge bases,  the expressive scope of the text phrases in [NN] is often limited to the visual entities in the associated images
 The rest of this paper is organized as follows
Section N details the collection process and analyses of our  VQS data
In section N, we show how to use the collected  segmentation-QA links to learn supervised attention features and to augement the existing VQA methods
In section N.N, we study a few potential frameworks to address  the new question-focused semantic segmentation task
Section N concludes the paper
 N
Linking image segmentations to text QAs  In this section, we describe in detail how we collect the  links between the semantic image segmentations and text  questions and answers (QAs)
We build our work upon the  images and instance segmentation masks in COCO [NN] and  the QAs in the VQA dataset [N]
The COCO images are  mainly about everyday scenes that contain common objects  in their natural contexts, accommodating complex interactions and relationships between different visual entities
To  avoid trivial links between the segmentations and QA pairs,  we only keep the images that contain at least three instance  segmentations in this work
The questions in VQA [N] are  diverse and comprehensively cover various parts of an image, different levels of semantic interpretations, as well as  commonsense and knowledge bases
 Next, we elaborate the annotation instructions and provide some analyses about the collected dataset
 N.N
Annotation instructions  We display to the annotators an image, its instance segmentations from the COCO dataset, and a QA pair about the  image from the VQA dataset
The textual answer is given  in addition to the question, to facilitate the participants to  choose the right segmentations as the visual answer
Here  are the instructions we give to the annotators (cf
the supplementary materials for the GUI):  • Please choose the right segmentation(s) in the image to answer the question
Note that the text answer is  shown after the question
 • A question about the target entities may use other enti- ties to help refer to the target
Choose the target entities  only and nothing else (e.g., the purse for “What is on  the bench next to woman?” in Figure N(g))
 • A question may be about an activity
Choose all visual entities involved in the activity
Taking Figure N(j) for  instance, choose both the person and motorcycle for  the question “what is the person doing?”
 • Sometimes, in addition to the image regions covered by the segmentation masks, you may need other regions to answer the question
To include them, please  NNNN    Figure N
Distribution of the number of segmentations per  question-image pair
 draw tight bounding box(es) over the region(s)
 • For the “How many” type of questions, the number of selected segments (plus bounding boxes) must match  the answer
If the answer is greater than three, it is  fine to put one bounding box around the entities being  asked in the question
 • Please tick the black button under the question, if you think the question has to be answered by the full image
 • Please tick the gray button under the question, if you feel the question is ambiguous, or if you are not sure  which segment/region to select to answer the question
 Occasionally, the visual answer is supposed to be only  part of an instance segment given by COCO
For instance,  the McDonald logo answers “What fast food restaurant can  be seen?” in Figure N(o) but there is no corresponding segmentation for the logo in COCO
Another example is the region of the ring that answers “Is the woman wearing ring?”  (cf
Figure N(c))
For these cases, we ask the participants  to draw tight bounding boxes around them
If we segment  them out instead, a learning agent for QFSS may never be  able to produce the right segmentation for them unless we  include more training images in the future, since these regions (e.g., McDonald logo, ring) are very fine-grained visual entities and show up only a few times in our data collection process
 Quality control
We tried AMTurk to collect the annotations at the beginning
While the inter-annotator agreement  is high on the questions about objects and people, there  are many inconsistent annotations for the questions referring to activities (e.g., “What sport is played?”)
Besides,  the AMTurk workers tend to frequently tick the black button, which says the full image is the visual answer, and the  gray button, which tells the question is ambiguous
To obtain higher-quality annotations, we instead invited N0 undergraduate and graduate volunteers and trained them in person  (we include some slides used for the training in the supplementary materials)
To further control the annotation quality, each annotator was asked to finish an assignment of N00  images (around N00 question-answer pairs) before we met  with them again to look over their annotations together —  does/do  how many  is/are  otherwhat color  what is  what others  where  which who why  N%  NN%  NN%  N%NN%  NN%  NN%  N%  N% N% N%  Figure N
The distribution of question types in the VQS dataset
 all the volunteers were asked to participate the discussion  and jointly decide the expected annotations for every question
We also gradually increased the hourly payment rate  from $NN/hr to $NN/hr as incentives for high-quality work
 N.N
Tasks addressed by the participants  Thanks to the rich set of questions collected by Agrawal  et al
[N] and the complex visual scenes in COCO [NN], the  participants have to parse the question, understand the visual scene and context, infer the interactions between visual  entities, and then pick up the segmentations that answer the  questions
We find that many vision tasks may play roles  in this process
Figure N shows some typical examples to  facilitate the following discussion
 Object detection
Many questions directly ask about the  properties of some objects in the images
In Figure N(b), the  participants are supposed to identify the cup in the cluttered  scene for the question “What color is the coffee cup?”
 Semantic segmentation
For some questions, the visual  evidence to answers is best represented by semantic segmentations
Take Figures N(j) and (k) for instance
Simply  detecting the rider and/or the bike would be inadequate in  expressing their spatial interactions
 Spatial relationship reasoning
A question like “What  is on the bench next to the woman?” (Figure N(g)) poses a  challenge to the participants through the spatial relationship  between objects including bench, woman, and the answer  purse
Figure N(i) is another example in this realm
 Fine-grained activity recognition
When the question  is about an activity (e.g., “What sport is being played?”  in Figure N(l)), we ask the participants to label all the visual entities (e.g., person, tennis racket , and tennis ball)  involved
In other words, they are expected to spot the finegrained details of the activity
 Commonsense reasoning
Commonsense knowledge  can help the participants significantly reduce the search  space for the visual answers, e.g., the clock to answer “What  time is it?” in Figure N, and the McDonald logo to answer  “What fast food restaurant can be seen?” in Figure N(o)
 NNNN    How many sheep   are pictured?  Question  Image CNN Region Features  ri  Question Embedding Vector  VQS Visual Answer   pi  Region Weights  Attention   Supervision+  Attention Featuresxatt  Question  Embedding  Attention   Features  Image  Features  MLP + Softmax 0 N Answer   Embedding  Learning Attention Feature with VQS Visual Answers  MLP Multi-Choice Pipeline with Attention Features   Figure N
Supervised attention for VQA
To learn the attention features for each question-image pair, we use the corresponding segmentation  mask as supervision to train the attention network
After that, we augment the MLP model [NN] by the attention features
 N.N
Data statistics  After collecting the annotations, we remove the  question-image pairs for which the users selected the black  buttons (full image) or gray buttons (unknown) to avoid trivial and ambiguous segmentation-QA links, respectively
In  total, we keep NN,NNN images, NN,N0N questions, N0N,NNN  instance segmentations, and NN,NNN bounding boxes
In the  following, we do not differentiate the segmentations from  the bounding boxes for the ease of presentation and also for  the sake that the bounding boxes are tight, small, and much  fewer than the segmentations
 Figure N counts the distribution of the possible number  of instance segmentations selected per image in response to  a question
Over N0% of questions are answered by one  segmentation
On average, each question-image pair has  N.N candidate segmentations, among which N.N are selected  by the annotators as the visual answers
 In Figure N, we visualize the distribution of question  types
The most popular type is the “What” questions  (NN%)
There are NN,NNN “is/are” and “does/do” questions  (NN.N%)
Note that although the textual answers to them  are simply yes or no, in VQS, we ask the participants to  explicitly demonstrate their understanding about the visual  content by producing the semantic segmentation masks
In  the third column of Table N, we show the average number of  segmentations chosen by the users out of the average number of candidates for each of the question types
 N
Applications of VQS  The user linked visual questions and segmentations,  where the latter visually answers the former, are quite versatile
They offer better leverage than before for at least two  problems, i.e., supervised attention for VQA and questionfocused semantic segmentation (QFSS)
 N.N
Supervised attention for VQA  VQA is designed to answer natural language questions  about an image in the form of short texts
We conjecture  that a learning agent can produce more accurate text answers given the privileged access to the segmentations that  are user linked to the QAs in training
To verify this point,  we design a simple experiment to augment the MLP model  in [NN]
The augmented MLP significantly improves upon  the plain version and gives rise to state-of-the-art results on  the VQA real multiple-choice task [N]
 Experiment setup
We conduct experiments on the VQA  Real Multiple Choices [N]
The dataset contains NNN,NNN  questions for training, NNN,NNN for validation, and NNN,N0N  for testing
Each question has NN candidate answer choices  and the learning agent is required to figure out the correct  answer among them
We evaluate our results following the  metric suggested in [N]
 MLP for VQA Multiple Choice
Since the VQA  multiple-choice task supplies candidate answers to each  question, Jabri et al
propose to transform the problem to a  stack of binary classification problems [NN] and solve them  by the multilayer perceptrons (MLP) model:  y = σ(WN max(0,WNxiqa) + b) (N)  where xiqa is the concatenation of the feature representations of an image, a question about the image, and a candidate answer, and σ(·) is the sigmoid function
The hidden layer has N,0NN units and a ReLU activation
This model is  very competitive, albeit simple
 N.N.N Augmenting MLP by supervised attention  We propose to augment the MLP model by richer feature  representations of the questions, answers, images, and esNNNN    Table N
Comparison results on both VQA TestDev and Standard  for the Real Multiple Choice task
 Method Dev Standard  Two-layer LSTM [N] NN.N NN.N  Region selection [NN] NN.N NN.N  DPPNet [NN] NN.N NN.N  MCB [N] NN.N − Co-Attention [NN] NN.N NN.N  MRN [NN] NN.N NN.N  MLB [N0] − NN.N MLP + ResNet [NN] NN.N −  MLP + ResNet +Atten
NN.N − MLP + Attri
NN.N −  MLP + Attri
+ Atten
NN.N NN.N  N0 ensemble models N0.N N0.N  pecially by the supervised attention features detailed below
 Question and answer features xq&xa
For a ques- tion or answer, we represent it by averaging the N00D  wordNvec [NN] vectors of the constituent words, followed  by the lN normalization
This is the same as in [NN]
 Image features xi
We extract two types of features from  an input image: ResNet [NN] poolN activation and attribute  features [NN], where the latter is the attribute detection  scores
We implement an attribute detector by revising the  output layer of ResNet
Particularly, given C = NNN at- tributes, we impose a sigmoid function for each attribute  and then train the network using the binary cross-entropy  loss
The training data is obtained from the COCO image  captions [NN]
We keep the most frequent NNN words as the  attributes after removing the stop words
 Attention features xatt
We further concatenate attention  features xatt to the original input xiqa
The attention features are motivated by the weighted combination of image  regional features and question features in [NN, eq
(NN)],  where the the non-negative weight pi = f(Q, {ri}) for each image region is a function of the question Q and regional features {ri}
We borrow the network architecture as well as code implementation from Yang et al
[NN, Section N.N] for this function, except that we train this network  by a cross-entropy loss to match the weights {pi} to the “groundtruth” attentions derived from the segmentations in  our VQS dataset
In particular, we down-sample the segmentation map associated with each question-image pair to  the same size as the number of image regions, and then lN normalize it to a valid probability distribution
By training  the network to match the weights pi = f(Q, {ri}) toward such attentions, we enforce larger weights for the regions  that correspond to the user selected segmentations
 The upper panel of Figure N illustrates the process of extracting the attention features, and the bottom panel shows  the MLP model [NN] augmented with our attention features  for the VQA real multiple-choice task
 Table N
Comparison results on VQA TestDev Real Multiple  Choice task
Method Y/N Num
Others All  Plain MLP [NN] N0.NN NN.NN NN.NN NN.NN  HAT [N] N0.NN NN.NN NN.NN NN.NN  Bounding boxes N0.NN NN.N NN.NN NN.NN  VQS N0.N0 NN.NN NN.NN NN.NN  N.N.N Experimental results  Table N reports the comparison results of the attention features augmented MLP with several state-of-the-art methods  on the VQA real multiple-choice task
We mainly use the  Test Dev for comparison
After determining our best single  and ensemble models, we also submit them to the evaluation server to acquire the results on Test Standard
 First of all, we note that there is an N.N% absolute improvement over the plain MLP model (MLP + ResNet) by  simply augmenting it using the learned attention features  (MLP + ResNet + Atten.)
Second, the attribute features  for the images are actually quite effective
We gain N.0%  improvement over the plain MLP by replacing the ResNet  image features with the attribute features (cf
the row of  MLP + Attri
vs
MLP + ResNet)
Nonetheless, by appending attention features to MLP + Attri., we can still observe  N.N% absolute gain
Finally, with an ensemble of five MLP  + ResNet + Atten
models and five MLP + Attri
+ Atten
 models, our submission to the evaluation server was ranked  to the second on Test Standard for the VQA real multiplechoice task, as of the paper submission date
 N.N.N What is good supervision for attention in VQA?  In this section, we contrast the VQS data to the human attention maps (HAT) [N] and bounding boxes that are placed  tightly around the segmentations in VQS
The comparison  results, reported in Table N, are evaluated on the TestDev  dataset of VQA Real Multiple Choice
We can see that the  segmentaitons linked to QAs give rise to a little better results than bounding boxes, which further outperform HAT
 These confirm our conjecture that HAT might be suboptimal  for the supervised learning of attentions in VQA, since they  reveal usually small parts of objects and contain large proportions of background
However, we believe it remains interesting to examine VQS for more generic attention-based  VQA models [NN, NN, NN, NN, NN, N, NN]
 In the supplementary materials, we describe the detailed implementation for the ensemble model
We also  present additional results studying how different resolutions  of the segmentation masks influence the VQA results
 N.N
Question-focused semantic segmentation  This section explores a new task, question-focused semantic segmentation (QFSS), which is feasible thanks to  the collected VQS that connects two previously separate  tasks (i.e., segmentations and VQA)
Given a question about  NNNN    What color is the   umbrella?  Textual embedding  u  Generate   Mask  Proposals  …  Segment Embeddings  …  mi  ei  Aggregated Mask  P i si ∗ ei =  Segment embeddings  Question embedding  u  mi  Binary Classifier Image ωi  yi  Segments  … part of  visual answer   or not  Mapping   Figure N
Mask aggregation method for QFSS (left) and the method to estimate its upper bound performance (right)
 …  …  How any buses have   only a single level? Mapping   Textual  embedding  Visual embedding  Question  Image  Visual answer  Figure N
Our DeconvNet baseline for QFSS
 an image, QFSS expects the learning agent to output a visual answer by semantically segment the right visual entities out of the image
It is designed in a way similarly to the  segmentation from natural language expressions [NN], with  possible applications to robot vision, photo editing, etc
 In order to put the new task in perspective, we propose a  mask aggregation approach to QFSS, study a baseline, and  also investigate an upper bound method by assuming all instance segmentations are given as oracles at the test stage
 N.N.N Mask aggregation for QFSS  We propose a mask aggregation approach to tackling QFSS
 The modeling hypothesis is that the desired output segmentation mask can be composed from high-quality segmentation proposals
In particular, we use N = NN segmenta- tion proposals eN, eN, · · · , eN generated by SharpMask [NN] given an image
Each proposal is a binary segmentation  mask of the same size as the image
 We then threshold a convex combination of these masks  E = ∑  i siei as the final output in response to a questionimage pair, where the i-th combination coefficient si is determined by the question features xq and the representations zi of the i-th segmentation proposal through a softmax function, i.e., si = softmax(x T q Azi)
We learn the  model parameters A by minimizing an lN loss between the  the user selected segmentations E⋆ and the model generated  segmentation mask E
Our current model is “shallow” but it  is straightforward to make it deep, e.g., by stacking its output with the original input following the prior practice (e.g.,  memory network [NN] and stacked attention network [NN])
 An oracle upper bound
We devise an upper bound to  the proposed method by N) replacing the segmentation proposals with all the instance segmentations released by MS   How many animals are in the picture?  What sport is this?  Is this a passenger train?  Input Inage Ground truthAggregation Deconvoulution  Figure N
Qualitative results of mask aggregation and DeconvNet
 COCO, assuming they are available as oracles at testing,  and N) using a binary classifier to determine whether or not  an instance segmentation should be included into the visual  answer
The results can be considered an upper bound for  our approach because the segmentations are certainly more  accurate than the machine generated proposals, and the binary classification is arguably easier to solve than aggregating multiple masks
We re-train the MLP (eq
N) for the  binary classifier here; it now takes as input the concatenated  features of a segmentation and a question
 Figure N depicts the proposed approach and the upperbound method with a concrete question-image example
 A baseline using deconvolutional network
Finally, we  study a competitive baseline which is motivated by the textconditioned FCN [NN]
As Figure N shows, it contains three  components, a convolutional neural network (CNN) [NN],  a deconvolutional neural network (DeconvNet) [N0], and a  question embedding to attend the feature maps in CNN
All  the images are resized to NNN × NNN
The convolutional and deconvolutional nets follow the specifications in [N0]
 Namely, a VGG-NN [NN] is trimmed till the last convolutional layer, followed by two fully connected layers, and  then mirrored by DeconvNet
For the input question, we  use an embedding matrix to map it to the same size as the  feature map of the last convolutional layer
The question  embedding is then element-wsie multiplied with the feature  map
We train the network with an lN loss between the output mask and the groundtruth segmentation mask
 NNNN    Table N
Comparison results on QFSS (evaluated by IOU, the higher the better)
For the question representations, we consider the bag-ofwords features (B) and the word embedding based features (W)
 Type Num
#seg ans/candts Aggre
(B) Aggre
(W) DeconvNet (B) DeconvNet (W) Upper  All NNNNN N.N/N.N 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NN0N  does/do NNN N.N/N.0 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  how many NNNN N.N/N.N 0.NNNN 0.NNNN 0.NNNN 0.NNN0 0.NNNN  is/are NNNN N.N/N.N 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNNN  what color NNNN N.N/N.0 0.NNNN 0.NNNN 0.NNN0 0.NNNN 0.NNNN  what is NNNN N.N/N.N 0.NNNN 0.NNNN 0.N0N0 0.N00N 0.NNNN  what (other) NNNN N.N/N.N 0.NNNN 0.NNNN 0.NNNN 0.NN0N 0.NNNN  where NNN N.N/N.N 0.NNNN 0.N0N0 0.NNNN 0.NNNN 0.NN0N  which N0N N.N/N.N 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.NN0N  who NNN N.N/N.N 0.NNNN 0.NNNN 0.N00N 0.NNNN 0.NNNN  why NNN N.N/N.N 0.NNNN 0.NNNN 0.NNN0 0.NNNN 0.NNNN  others NNN N.N/N.N 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNNN  N.N.N Experiments on QFSS  Features
In addition to representing the questions using  the word embedding features xq as in Section N.N.N, we also  test the bag-of-words features
For each instance segmentation or proposal, we mask out all the other pixels in the  image with 0’s and then extract its features from the last  pooling layer of a ResNet-NNN [NN]
 Dataset Split
The SharpMask we use is learned from the  training set of MS COCO
Hence, we split our VQS data in  such a way that our test set does not intersect with the training set for SharpMask
Particularly, we use NN,NNN images  and correspondingly NN,N0N questions as our training set
 We split the remaining images and questions to two parts:  N,000 images and associated questions for validation, and  N,NNN images with NN,NNN questions as the test set
 Results
Table N reports the comparison results on QFSS,  evaluated by intersection-over-union (IOU)
In addition, the  first three columns are about the number of different types  of questions and the average numbers of user selected segmentations per question type
On average, more than one  segmentations are selected for any of the question types
 First of all, we note that the proposed mask aggregation  outperforms the baseline DeconvNet and yet is significantly  worse than its upper bound method
The mask aggregation is superior over DeconvNet partially because it has actually used extra supervised information beyond our VQS  data; namely, the SharpMask is trained using all the instance  segmentations in the training set of MS COCO
The upper  bound results indicate there is still large room for the mask  aggregation framework to improve; one possibility is make  it deep in the future work
 Besides, we find that the two question representations,  bag-of-wrods (B) and word embedding (W), give rise to  distinguishable results for either mask aggregation or DeconvNet
This observation is intriguing since it implies that  the QFSS task is responsive to the question representation  schemes
It is thus reasonable to expect that QFSS will both  benefit from and advance the progress on joint vision and  language modeling methods
 Finally, Figure N shows some qualitative segmentation  results
Note the two separate instance segmentations in the  first row that visually answer the “How many” question
 N
Conclusion  In this paper, we propose to link the instance segmentations provided by COCO [NN] to the questions and answers  in VQA [N]
The collected links, named visual questions  and segmentation answers (VQS), transfer human supervision between the individual tasks of semantic segmentation  and VQA, thus enabling us to study at least two problems  with better leverage than before: supervised attention for  VQA and a novel question-focused semantic segmentation  task
For the former, we obtain state-of-the-art results on  the VQA real multiple-choice task by simply augmenting  multilayer perceptrons with some attention features
For the  latter, we propose a new approach based on mask aggregation
To put it in perspective, we study a baseline method  and an upper-bound method by assuming the instance segmentations are given as oracles
 Our work is inspired upon observing the popularity of  COCO [NN]
We suspect that the existing and seemingly  distinct annotations about MSCOCO images are inherently  connected
They reveal different levels and perspectives of  human understandings about the same visual scenes
Explicitly linking them up can significantly benefit not only  individual tasks but also the overarching goal of unified  vision-language understanding
This paper just scratches  the surface
We will explore more types of annotations and  richer models in the future work
 Acknowledgement This work is supported in part by  the NSF award IIS #NNNNNNN, a gift from Adobe Systems Inc., and a GPU from NVIDIA
C
Gan is partially supported by the National Basic Research Program  of China N0NNCBA00N00 & N0NNCBA00N0N, and the National Natural Science Foundation of China NN0NN00N &  NNNNNNNN00N
 NNNN    References  [N] P
Anderson, B
Fernando, M
Johnson, and S
Gould
 SPICE: Semantic propositional image caption evaluation
In  ECCV, pages NNN–NNN, N0NN
N  [N] J
Andreas, M
Rohrbach, T
Darrell, and K
Dan
Neural  module networks
CVPR, NN:NN–NN, N0NN
N, N  [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra,  C
Lawrence Zitnick, and D
Parikh
VQA: Visual question  answering
In ICCV, pages NNNN–NNNN, N0NN
N, N, N, N, N,  N, N  [N] X
Chen and C
Lawrence Zitnick
Mind’s eye: A recurrent  visual representation for image caption generation
In CVPR,  pages NNNN–NNNN, N0NN
N  [N] A
Das, H
Agrawal, C
L
Zitnick, D
Parikh, and D
Batra
 Human attention in visual question answering: Do humans  and deep networks look at the same regions? arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N  [N] J
Donahue, L
Anne Hendricks, S
Guadarrama,  M
Rohrbach, S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual  recognition and description
In CVPR, pages NNNN–NNNN,  N0NN
N  [N] D
Elliott and F
Keller
Comparing automatic evaluation  measures for image description
ACL, NNN(NNN):NNN, N0NN
 N  [N] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and D
Ramanan
Object detection with discriminatively trained partbased models
T-PAMI, NN(N):NNNN–NNNN, N0N0
N  [N] A
Fukui, D
H
Park, D
Yang, A
Rohrbach, T
Darrell,  and M
Rohrbach
Multimodal compact bilinear pooling for  visual question answering and visual grounding
EMNLP,  N0NN
N  [N0] C
Gan, Z
Gan, X
He, J
Gao, and L
Deng
Stylenet: Generating attractive visual captions with styles
CVPR, N0NN
 N  [NN] C
Gan, N
Wang, Y
Yang, D.-Y
Yeung, and A
G
Hauptmann
DevNet: A deep event network for multimedia event  detection and evidence recounting
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N  [NN] Z
Gan, C
Gan, X
He, Y
Pu, K
Tran, J
Gao, L
Carin,  and L
Deng
Semantic compositional networks for visual  captioning
CVPR, N0NN
N  [NN] H
Gao, J
Mao, J
Zhou, Z
Huang, L
Wang, and W
Xu
 Are you talking to a machine? dataset and methods for multilingual image question
In NIPS, pages NNNN–NN0N, N0NN
 N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
CVPR, N0NN
N, N  [NN] R
Hu, M
Rohrbach, and T
Darrell
Segmentation from  natural language expressions
In ECCV, N0NN
N, N, N, N  [NN] R
Hu, H
Xu, M
Rohrbach, J
Feng, K
Saenko, and T
Darrell
Natural language object retrieval
In CVPR, N0NN
N,  N  [NN] A
Jabri, A
Joulin, and L
van der Maaten
Revisiting visual question answering baselines
In ECCV, pages NNN–NNN
 Springer, N0NN
N, N, N  [NN] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, pages  NNNN–NNNN, N0NN
N  [NN] J.-H
Kim, S.-W
Lee, D
Kwak, M.-O
Heo, J
Kim, J.-W
 Ha, and B.-T
Zhang
Multimodal residual learning for visual  qa
In NIPS, pages NNN–NNN, N0NN
N  [N0] J.-H
Kim, K.-W
On, J
Kim, J.-W
Ha, and B.-T
Zhang
 Hadamard product for low-rank bilinear pooling
ICLR,  N0NN
N  [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, et al
 Visual genome: Connecting language and vision using  crowdsourced dense image annotations
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet classification with deep convolutional neural networks
NIPS,  N0NN
N  [NN] R
Li and J
Jia
Visual question answering with question representation update (qru)
In Advances in Neural Information  Processing Systems, pages NNNN–NNNN, N0NN
N, N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV, pages NN0–NNN, N0NN
N,  N, N, N, N, N  [NN] J
Lu, J
Yang, D
Batra, and D
Parikh
Hierarchical  question-image co-attention for visual question answering
 NIPS, N0NN
N, N  [NN] M
Malinowski and M
Fritz
A multi-world approach to  question answering about real-world scenes based on uncertain input
NIPS, N0NN
N, N  [NN] J
Mao, J
Huang, A
Toshev, O
Camburu, A
Yuille, and  K
Murphy
Generation and comprehension of unambiguous  object descriptions
arXiv preprint arXiv:NNNN.0NNNN, N0NN
 N, N  [NN] J
Mao, W
Xu, Y
Yang, J
Wang, Z
Huang, and A
Yuille
 Deep captioning with multimodal recurrent neural networks  (m-RNN)
ICLR, N0NN
N  [NN] T
Mikolov, I
Sutskever, K
Chen, G
Corrado, and J
Dean
 Distributed representations of words and phrases and their  compositionality
NIPS, NN:NNNN–NNNN, N0NN
N  [N0] H
Noh, S
Hong, and B
Han
Learning deconvolution network for semantic segmentation
ICCV, pages NNN0–NNNN,  N0NN
N  [NN] H
Noh, P
Hongsuck Seo, and B
Han
Image question answering using convolutional neural network with dynamic  parameter prediction
In CVPR, pages N0–NN, N0NN
N  [NN] Y
Pan, T
Mei, T
Yao, H
Li, and Y
Rui
Jointly modeling  embedding and translation to bridge video and language
In  CVPR, pages NNNN–NN0N, N0NN
N  [NN] P
O
Pinheiro, T
Y
Lin, R
Collobert, and P
Dollár
Learning to refine object segments
ECCV, N0NN
N  [NN] B
A
Plummer, L
Wang, C
M
Cervantes, J
C
Caicedo,  J
Hockenmaier, and S
Lazebnik
FlickrN0k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models
In ICCV, pages NNNN–NNNN, N0NN
N,  N  NNNN    [NN] M
Ren, R
Kiros, and R
Zemel
Exploring models and data  for image question answering
In NIPS, pages NNNN–NNNN,  N0NN
N  [NN] A
Rohrbach, M
Rohrbach, R
Hu, T
Darrell, and  B
Schiele
Grounding of textual phrases in images by reconstruction
In ECCV, N0NN
N, N  [NN] K
J
Shih, S
Singh, and D
Hoiem
Where to look: Focus regions for visual question answering
In CVPR, pages  NNNN–NNNN, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
ICLR, N0NN
N  [NN] C
Sun, C
Gan, and R
Nevatia
Automatic concept discovery from parallel text and visual corpora
In ICCV, N0NN
 N  [N0] R
Vedantam, C
Lawrence Zitnick, and D
Parikh
Cider:  Consensus-based image description evaluation
In CVPR,  pages NNNN–NNNN, N0NN
N  [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In CVPR, pages  NNNN–NNNN, N0NN
N  [NN] L
Wang, Y
Li, and S
Lazebnik
Learning deep structurepreserving image-text embeddings
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages N00N–N0NN, N0NN
N  [NN] Q
Wu, C
Shen, L
Liu, A
Dick, and A
van den Hengel
 What value do explicit high level concepts have in vision to  language problems? In CVPR, pages N0N–NNN, N0NN
N  [NN] C
Xiong, S
Merity, and R
Socher
Dynamic memory networks for visual and textual question answering
ICML,  N0NN
N, N, N  [NN] H
Xu and K
Saenko
Ask, attend and answer: Exploring  question-guided spatial attention for visual question answering
ECCV, N0NN
N, N  [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
Courville, R
Salakhudinov, R
Zemel, and Y
Bengio
Show, attend and tell: Neural  image caption generation with visual attention
In ICML,  pages N0NN–N0NN, N0NN
N  [NN] Z
Yang, X
He, J
Gao, L
Deng, and A
Smola
Stacked  attention networks for image question answering
CVPR,  N0NN
N, N, N  [NN] C.-N
J
Yu and T
Joachims
Learning structural svms with  latent variables
In ICML, pages NNNN–NNNN
ACM, N00N
N  [NN] L
Yu, P
Poirson, S
Yang, A
C
Berg, and T
L
Berg
Modeling context in referring expressions
In European Conference on Computer Vision, pages NN–NN
Springer, N0NN
N,  N  [N0] Y
Zhu, O
Groth, M
Bernstein, and L
Fei-Fei
VisualNw:  Grounded question answering in images
CVPR, N0NN
N, N  NNN0VPGNet: Vanishing Point Guided Network for Lane and Road Marking Detection and Recognition   VPGNet: Vanishing Point Guided Network for Lane and Road Marking  Detection and Recognition  Seokju Lee† Junsik Kim† Jae Shin Yoon† Seunghak Shin† Oleksandr Bailo†  Namil Kim† Tae-Hee Lee‡ Hyun Seok Hong‡ Seung-Hoon Han‡ In So Kweon†  †Robotics and Computer Vision Lab., KAIST ‡Samsung Electronics DMC R&D Center  {sjlee,jskimN,jsyoon,shshin,obailo,nikim}@rcv.kaist.ac.kr  {thNN0.lee,hyunseokNN.hong,luoes.han}@samsung.com, iskweon@kaist.ac.kr  Abstract  In this paper, we propose a unified end-to-end trainable  multi-task network that jointly handles lane and road marking detection and recognition that is guided by a vanishing  point under adverse weather conditions
We tackle rainy  and low illumination conditions, which have not been extensively studied until now due to clear challenges
For example, images taken under rainy days are subject to low illumination, while wet roads cause light reflection and distort  the appearance of lane and road markings
At night, color  distortion occurs under limited illumination
As a result, no  benchmark dataset exists and only a few developed algorithms work under poor weather conditions
To address this  shortcoming, we build up a lane and road marking benchmark which consists of about N0,000 images with NN lane  and road marking classes under four different scenarios:  no rain, rain, heavy rain, and night
We train and evaluate  several versions of the proposed multi-task network and validate the importance of each task
The resulting approach,  VPGNet, can detect and classify lanes and road markings,  and predict a vanishing point with a single forward pass
 Experimental results show that our approach achieves high  accuracy and robustness under various conditions in realtime (N0 fps)
The benchmark and the VPGNet model will  be publicly available N
 N
Introduction  Autonomous driving is a large system that consists of  various sensors and control modules
The first key step for  robust autonomous driving is to recognize and understand  the environment around a subject
However, simple recognition of obstacles and understanding of geometry around  a vehicle is insufficient
There are traffic regulations dictated by traffic symbols such as lane and road markings that  N https://github.com/SeokjuLee/VPGNet  (a) (b)  (c) (d)  Figure N
Examples of our lane and road markings detection  results in: (a) complex city scene; (b) multiple road markings; (c) night scene; (d) rainy condition
Yellow region is  the vanishing area
Each class label is annotated in white
 should be complied with
Moreover, for an algorithm to be  applicable to autonomous driving, it should be robust under  diverse environments and perform in real-time
 However, research on lane and road marking detection  thus far has been limited to fine weather conditions
Handcrafted feature based methods exploit edge, color or texture information for detection, which results in a performance drop when the algorithm is tested under challenging weather and illumination conditions
Likewise, methods based on a combination of a Convolutional Neural Network (CNN) and hand-crafted features face the same challenge
Recently, a few CNN based approaches have been  developed to tackle the problem in an end-to-end fashion including learning-based algorithms
They demonstrate good  performance on benchmarks and in real road scenes, but are  still limited to fine weather and simple road conditions
 The lack of public lane and road marking datasets is anNNNN    other challenge for the advancement of autonomous driving
Available datasets are often limited and insufficient  for deep learning methods
For example, Caltech Lanes  Dataset [N] contains N,NNN images taken from four different  places
Further, Road Marking Dataset [NN] contains N,NNN  images manually labeled into NN classes of road markings
 Existing datasets are all taken under sunny days with a clear  scene and adverse weather scenarios are not considered
 With recent advances in deep learning, the key to robust recognition in challenging scenes is a large dataset that  incorporates data captured under various circumstances
 Since no proper datasets available for lane and road marking recognition, we have collected and annotated lanes and  road markings of challenging scenes captured in urban areas
Additionally, a higher network capability with a proper  training scheme is required to generate a fine representation  to cope with varied data
We propose to train a network that  recognizes a global context in a manner similar to humans
 Interestingly, humans can drive along a lane even when  it is hard to spot
Research works [N0, NN, NN] have empirically shown that the drivers gaze direction is highly correlated with the road direction
This implies that a geometric  context plays a significant role in the lane localization
Inspired by this, we aim to utilize a vanishing point prediction  task to embed a geometric context recognition capability  to the proposed network
Further, we hope to advance autonomous driving research with the following contributions:  • We build up a lane and road marking detection and  recognition benchmark dataset taken under various  weather and illumination conditions
The dataset consists of about N0,000 images with NN manually annotated lane and road markings classes
Vanishing point  annotation is provided as well
 • We design a unified end-to-end trainable multi-task  network that jointly handles lane and road marking detection and recognition that is guided by the vanishing  point
We provide an extensive evaluation of our network on the created benchmark
The results show robustness under different weather conditions with realtime performance
Moreover, we suggest that the proposed vanishing point prediction task enables the network to detect lanes that are not explicitly seen
 This paper is organized as follows
Section N covers recent algorithms developed for lane and road marking detection
A description of the benchmark is given in Section N
 Section N explains our network architecture and training  scheme
Experimental results are reported in Section N
Finally, Section N concludes our work
 N
Related Work  In this section, we introduce previous works that aim to  resolve the road scene detection challenge
Our setup as  well as related works is based on a monocular vision setup
 N.N
Lane and Road Marking Detection  Although lane and road marking detection appears to  be a simple problem, the algorithm must be accurate in a  variety of environments and have fast computation time
 Lane detection methods based on hand-crafted features  [N, NN, NN, N, NN, NN, NN] detect generic shapes of markings  and try to fit a line or a spline to localize lanes
This group of  algorithms performs well for certain situations while showing poor performance in unfamiliar conditions
In the case  of road marking detection algorithms, most of the works  are based on hand-crafted features
Tao et al
[NN] extract  multiple regions of interest as Maximally Stable Extremal  Regions (MSER) [NN], and rely on FAST [NN] and HOG[N]  features to build templates for each road marking
Similarly,  Greenhalgh et al
[NN] utilizes HOG features and a SVM is  trained to produce class labels
However, as in the lane detection case, these approaches show a performance drop in  unfamiliar conditions
 Recently, deep learning methods have shown great success in computer vision, including lane detection
[NN, NN]  proposes a lane detection algorithm based on a CNN
Jun Li  et al
[NN] uses both a CNN and a Recurrent Neural Network  (RNN) to detect lane boundaries
In this work, the CNN  provides geometric information of lane structures, and this  information is utilized by the RNN that detects the lane
Bei  He et al
[NN] proposes using a Dual-View Convolutional  Neutral Network (DVCNN) framework for lane detection
 In this approach, the front-view and top-view images are fed  as input to the DVCNN
Similar to the lane detection algorithms, several works have examined the application of neural networks as a feature extractor and a classifier to enhance  the performance of road marking detection and recognition
 Bailo et al
[N] proposes a method that extracts multiple regions of interest as MSERs [NN], merges regions that possibly belong to the same class, and finally classifies region  proposals by utilizing a PCANet [N] and a neural network
 Although the aforementioned approaches provide a  promising performance of lane and road marking detection  using deep learning, the problem of detection under poor  conditions is still not solved
In this paper, we propose a  network that performs well in any situation including bad  weather and low illumination conditions
 N.N
Object Detection by CNNs  With advances of deep learning, recognition tasks such  as detection, classification, and segmentation have been  solved under a wide set of conditions, yet there is no leading solution
RCNN and its variants [NN, NN, NN] provide  a breakthrough in detection and classification, outperforming previous approaches
Faster RCNN [NN] replaces handcrafted proposal methods with a convolutional network in a  way that the region proposal layer shares extracted features  with the classification layer
Overfeat [N0] shows that a conNNNN    volutional network with a sliding window approach can be  efficiently computed
Its performance in object recognition  and localization using multi-scale images is also reported
 Some of its variants [NN, NN] achieve state of the art performance in detection tasks
Although these approaches show  cutting edge results on large-scale benchmarks [N, N0, NN],  which contain objects that occupy a significant part of an  image, the performance decreases for smaller and thinner  objects (e.g
lane or road markings)
 Several deep learning approaches specialize in a lane and  small object recognitions
For example, Huval et al
[NN] introduce a method for lanes and vehicles detection based on  a fully convolutional architecture
They use the structure of  [N0] and extend the method with an integrated regression  module composed of seven convolutional layers for feature  sharing
The network is divided into two branches which  perform binary classification and regression task
They  evaluate results under a nice weather on a highway without complex road symbols, but do not perform a multi-label  classification
Additionally, Zhu et al
[NN] propose a multitask network for traffic sign (relatively small size) detection  and classification
In this work, the classification layer is  added in parallel to the [NN] network to perform detection  and classification
As a result, this work reports better performance of detecting small objects than Fast RCNN [NN]
 N
Benchmark  N.N
Data Collection and Annotation  We have collected the dataset in various circumstances  and categorized the images according to the time of the day  and weather conditions
The dataset comprises situations  during day time with different levels of precipitation: no  rain, rainfall, and heavy rainfall
Night time images are not  subdivided by weather condition but include general images  taken in a challenging situation with low illumination
The  number of images for each scenario is shown in Table N
 Since our dataset is captured under bad weather conditions,  we mount a camera inside a vehicle (in the center)
In this  way, we can avoid damaging the camera sensor while also  preventing direct water drops on the camera lens
However,  since several videos are recorded in heavy rain, a part of  a window wiper is captured occasionally
The camera is  directed to the front view of the car
Image resolution is  NNNN×NNN
Our data are captured in a downtown area of  Seoul, South Korea
The shapes and symbols of the lane  and road markings follow the regulations of South Korea
 We manually annotate corner points of lane and road  markings
Corner points are connected to form a polygon  which results in a pixel-level mask annotation for each object
In a similar manner, each pixel contains a class label
 However, if the network is trained with a thin lane annotation, the information tends to vanish through convolution and pooling layers
Further, since most of the neuTable N
Number of frames for each scenario in the dataset
Scenario (Scn.) Total frames Training set Test set  Daytime  No rain (Scn
N) NN,NNN N,NNN N,NNN  Rain (Scn
N) N,0NN N,NNN NNN  Heavy rain (Scn
N) NNN NNN NNN  Night (Scn
N) N,NNN N,NNN NNN  Total NN,0NN NN,NNN N,NNN  Single yellow  Dashed white  Double yellow  Straight arrow  Figure N
Pixel- and grid-level annotations of the dataset
 ral networks require a resized image (usually smaller than  original size), the thin annotations become barely visible
 Therefore, we propose projecting pixel-level annotation to  the grid-level mask
The image is divided into a grid N×N  and the grid cell is filled with a class label if any pixel from  the original annotation lies within the grid cell
Considering  that the input size of our network is NN0×NN0 and the output size is N0×N0, the grid size is set to be proportional to  the scale factor (N/N) between the input and output images
Specifically, the grid size is set to be N×N
Figure N shows  an annotation example
 The vanishing point annotation is also provided
We localize the vanishing point in a road scene where parallel  lanes supposedly meet
The vanishing point is manually  annotated by a human
Depending on the scene, a difficulty level (EASY, HARD, NONE) is assigned to every  vanishing point
EASY level includes a clear scene (e.g
 straight road); HARD level includes a cluttered scene (e.g
 traffic jam); NONE is where a vanishing point does not exist  (e.g
intersection)
It is important to note that both straight  and curved lanes are utilized to predict the vanishing point
 We describe the definition of our vanishing point in detail  in Section N.N
Furthermore, annotation examples are presented in the supplementary material
 N.N
Dataset Statistics  Our dataset consists of about N0,000 images taken during three weeks of driving in Seoul
The raw video (N0 fps)  is sampled at NHz intervals to generate image data
Images  of the complex urban traffic scenes contain lane and road  markings under various weather conditions during different  time of the day
In total, NN classes are annotated covering  the most common markings found on the road
Although  we recorded the video in various circumstances, a data imbalance between different types of lane and road markings  is observed
For example, in the case of lane classes, dashed  white and double yellow lines are more common than other  lane types
Regarding road marking classes, straight arrows  and crosswalks appear most frequently
We also define a  “Other markings” class containing road markings that are  NNNN    Table N
Number of instances for each class in the dataset
Lane Road marking Vanishing point  Single white NN,NNN Stop line N,NNN EASY NN,N0N  Dashed white NN,NNN Left arrow N,NNN HARD NNN  Double white N0N Right arrow NNN NONE N,NNN  Single yellow NN,0NN Straight arrow N,NNN  Dashed yellow N,NNN U-turn arrow NNN  Double yellow N,NNN Speed bump N,NNN  Dashed blue N,N0N Crosswalk NN,NNN  Zigzag N,NNN Safety zone N,0NN  Other markings NN,NNN  present only in South Korea, or have an insufficient number of instances to be trained as a separate class
Types of  classes and the number of instances are listed in Table N
 N
Neural Network  N.N
Architecture  Our network, VPGNet, is inspired by the work of [NN]  and [NN]
The competitive advantage of our network is that  it is specialized to detect and recognize lane and road markings as well as to localize vanishing point
 We propose a data layer to induce grid-level annotation  that enables training of both lane and road markings simultaneously
Originally in [NN], [NN], the box regression task  aims to fit a single box to a particular object
This works  well for objects with a blob shape (traffic signs or vehicles),  but lane and road markings cannot be represented by a single box
Therefore, we propose an alternative regression  that utilizes a grid-level mask
Points on the grid are regressed to the closest grid cell and combined by a multilabel classification task to represent an object
This enables  us to integrate two independent targets, lane and road markings, which have different characteristics and shapes
For  the post-processing, lane classes only use the output of the  multi-label task, and road marking classes utilize both grid  box regression and multi-label task (see Section N.N)
Additionally, we add a vanishing point detection task to infer a  global geometric context during training of patterns of lane  and road markings (explained in Section N.N)
 The overall architecture is described in Table N and Figure N
The network has four task modules and each task  performs complementary cooperation: grid box regression,  object detection, multi-label classification, and prediction  of the vanishing point
This structure allows us to detect and  classify the lane and road markings, and predict the vanishing region simultaneously in a single forward pass
 N.N
Vanishing Point Prediction Task  Due to poor weather environments, illumination conditions, and occlusion, the visibility of lanes decreases
 However, in such situations, humans intuitively can predict  the locations of the lanes from global information such as  nearby structures of roads or the flow of traffic [N0, NN, NN]
 Inspired by this, we have designed a Vanishing Point Prediction (VPP) task that guides robust lane and road marking detection similar to human vision
A vanishing point is a point  where parallel lines in a three-dimensional space converge  to a two-dimensional plane by a graphical perspective
In  most cases of driving, lane and road markings converge to  a single point regardless of whether the roads are curved or  straight
In this paper, “Vanishing Point (VP)” is defined as  the nearest point on the horizon where lanes converge and  disappear predictively around the farthest point of the visible lane
This VP can be used to provide a global geometric  context of a scene, which is important to infer the location  of lanes and road markings
We integrate the VPP module  with the multi-task network to train the geometric patterns  of lane convergence to one point
 Borji [N] has shown that a CNN can localize the VP
The  author vectorizes the spatial output of the network to predict the exact location of a VP by using a softmax classifier
 However, selecting exactly one point over the whole network’s output size results in imprecise localization
In order to provide more robust localization, we perform several  experiments to guide the VP
 First, for the VPP task, we tried regression losses (i.e
LN,  LN, hinge losses) that directly calculate pixel distances from  a VP
Unfortunately, the results are not favorable since it  is difficult to balance the losses with other tasks (object  detection/multi-label classification) due to the difference in  the loss scale
Therefore, we adopt a cross entropy loss  to balance the gradients propagated from each of the detection tasks
By using cross entropy loss, first we apply  a binary classification method that directly classifies background and foreground ( i.e
vanishing area, see Figure Na),  as in the object detection task
The binary mask is generated in the data layer by drawing a fixed size circle centered  at the VP we annotated
However, using this method on the  VPP task results in extremely fast convergence of the training loss
This is caused by the imbalance of the number  of background and foreground pixels
Since the vanishing  area is drastically smaller than the background, the network  is initialized to infer every pixel as background class
This  phenomenon contradicts our original intention of training  the VPP to learn the global context of a scene
 Considering the challenge imposed by the aforementioned imbalance during the binary VPP method, we have  newly designed the VPP module
As stated before, the purpose of attaching the VPP task is to improve a scene representation that implies a global context to predict invisible  lanes due to occlusions or extreme illumination condition
 The whole scene should be taken into account to efficiently  reflect global information inferring lane locations
We use  a quadrant mask that divides the whole image into four sections
The intersection of these four sections is a VP
In this  way, we can infer the VP using four quadrant sections which  cover the structures of a global scene
To implement this,  we define five channels for the output of the VPP task: one  absence channel and four quadrant channels
Every pixel  NNN0    Table N
Proposed network structure
Layer Conv N Conv N Conv N Conv N Conv N Conv N Conv N Conv N  Kernel size, stride, pad NN, N, 0 N, N, N N, N, N N, N, N N, N, N N, N, N N, N, 0 N, N, 0  Pooling size, stride N, N N, N N, N  Addition LRN LRN Dropout Dropout, branched Branched  Receptive field NN NN NN NNN NNN NNN NNN NNN  ConvN+Pool   (NN×NN)  N NN NNN NNN NNN  ConvN+Pool  (N×N) ConvN  (N×N) ConvN+Pool  (N×N) ConvN  (N×N) ConvN  (N×N)  NNN  NN0  NN0  NN  NN  NN  NN  NN  NN  NN  NN  NN  NN  NN  N0  N0NN  N0NN  N0NN  N0NN  NN  N0  NN  N0  NN  N0  N0NN  N0NN  N0NN  N0NN  NN  N0  NN  N0  NN  N0  NN  N0  ConvN (N×N)  ConvN (N×N)  ConvN (N×N)  ConvN (N×N)  Tiling (N×N) NNN  Tiling (N×N)  NNN  Tiling (N×N) N0NN  Tiling (N×N) NN0  Grid Box  (NN0×NN0×N)  Object Mask  (NN0×NN0×N)  Multi-label  (N0×N0×NN)  VPP  (NN0×NN0×N)  NN  N0  NN  N0  NN  N0  NN  N0  ConvN (N×N)  ConvN (N×N)  ConvN (N×N)  ConvN (N×N)  Shared layers Branch layers  Figure N
VPGNet performs four tasks: grid regression, object detection, multi-label classification, and vanishing point  prediction
 in the output image chooses to belong to one of the five  channels
The absence channel is used to represent a pixel  with no VP, while the four quadrant channels stand for one  of the quadrant sections on the image
For example, if the  VP is present in the image, every pixel should be assigned  to one of the quadrant channels, while the absence channel cannot be chosen
Specifically, the third channel would  be guided by the upper right diagonal edges from the road  scene, and the fourth channel would extract the upper left  diagonal edges from the road scene
On the other hand, if  the VP is hard to be identified (e.g
intersection roads, occlusions), every pixel will tend to be classified as the absence  channel
In this case, the average confidence of the absence  channel would be high
 Unlike the binary classification approach, our quadrant  method enriches the gradient information that contains a  global structure of a scene
The loss comparison in Figure Nb indirectly shows that the network is trained without  overfitting compared to the binary case
Note that we only  use the quadrant VPP method for the evaluation
The binary VPP method is introduced only to show readers that  a naive VPP training scheme does not yield satisfactory results
The whole multi-task network allows us to detect and  recognize the lane and road marking, as well as to predict  the VP simultaneously in a single forward pass
 N.N
Training  Our network includes four tasks which cover different  contexts
The detection task recognizes objects and covers  a local context, while the VPP task covers a global context
If those tasks are trained altogether at the same training phase, the network can be highly influenced by a certain  dominant task
We noticed that during the training stage the  VPP task became dependent on the lane detection task
The  dependency between lanes and the VP implies a strong information correlation
In this case, the VP provides redunBinary annotation  Quadrant annotation  Training image  Network prediction (Red: high, Blue: low confidence)  N  0  N  0  (a)  0 N00 N00 N00 N00  N00  Iterations (xN00)  N00 N00 N00 N00  Lo ss  0  0.N  0.N  0.N  0.N  0.N  0.N  VPP VPP   task loss (w/ binary VPP)   task loss (w/ quadrant VPP)  Object mask  Object mask   task loss (w/ binary VPP)   task loss (w/ quadrant VPP)  Multi-label task loss (w/ binary VPP)  Multi-label task loss (w/ quadrant VPP)  (b)  Figure N
(a) Output visualization of binary and quadrant  VPP methods
For the prediction of the quadrant method,  only four quadrant channels are visualized except for an absence channel
(b) The loss comparison of two methods
 dant information to the network, leading to marginal lane  detection improvement
In order to prevent this side effect,  we train the network in two phases to tolerate the balance  between the tasks
 In the first phase, we train only the VPP task
We fix the  learning rates to zero for every task except the VPP module
In this way, we can train the kernels to learn a global  context of the image
The training of this phase stops upon  reaching convergence of the VP detection task
Although  we train only the VPP task, due to the weight update of the  mutually shared layers, losses of the other detection tasks  are also decreased by about N0%
This shows that lane and  road marking detection and VPP tasks share some common  characteristics in the feature representation layers
 In the second phase, we further train all the tasks using  the initialized kernels from the first phase
Since all tasks  are trained together at this point, it is important to balance  their learning rates
If a certain task loss weight is small, it  becomes dependent on other tasks and vice versa
Equation  (N) shows the summation of four losses from each task:  Loss = wNLreg + wNLom + wNLml + wNLvp (N)  where Lreg is a grid regression LN loss, Lom and Lml and  NNNN    Lvp are cross entropy losses in each branch of the network
We balance the tasks by weight terms wN∼wN in the fol- lowing way
First, wN∼wN are set to be equal to N, and the starting losses are observed
Then, we set the reciprocal of  these initial loss values to the loss weight so that the losses  are uniform
In the middle of the training, if the scale difference between losses becomes large, this process is repeated  to balance the loss values
The second phase stops when the  validation accuracy is converged
 N.N
Post-Processing  Each lane and road marking class and VPs are required  to be represented suitably for real world application
Therefore, we implement post-processing techniques to generate  visually satisfying results
 Lane In the case of the lane classes, we use the following techniques: point sampling, clustering, and lane regression
First, we subsample local peaks from the region  where the probability of lane channels from the multi-label  task is high
The sampled points are potential candidates  to become the lane segments
Further, selected points are  projected to the birds-eye view by inverse perspective mapping (IPM) [N]
IPM is used to separate the sampled points  near the VP
This is useful not only for the case of straight  roads but also curved ones
We then cluster the points by  our modified density-based clustering method
We sequentially decide the cluster by the pixel distance
After sorting  the points by the vertical index, we stack the point in a bin  if there is a close point among the top of the existing bins
 Otherwise, we create a new bin for a new cluster
By doing  this, we can reduce the time complexity of the clustering
 The last step is quadratic regressions of the lines from the  obtained clusters utilizing the location of the VP
If the farthest sample point of each lane cluster is close to the VP,  we include it in the cluster to estimate a polynomial model
 This makes the lane results stable near the VP
The class  type is assigned to each line segment from the multi-labeled  output of the network
 Road marking For the road marking classes, grid sampling and box clustering are applied
First, we extract grid  cells from the grid regression task with high confidence for  each class from the multi-label output
We then select corner points of each grid and merge them with the nearby grid  cells iteratively
If no more neighboring grid cells belong  to the same class, the merging is terminated
Some road  markings such as crosswalks or safety zones that are difficult to define by a single box are localized by grid sampling  without subsequent merging
 Vanishing point Our VPP module outputs five channels of the confidence map: four quadrant channels and one  absence channel
Through these quadrants, we generate the  location of a VP
The VP is where all four quadrants intersect
That is, we need to find a point where four confidences  from each quadrant channel become close
Equation (N) and  (N) describe the boundary intersection of each quadrant:  Pavg = N− (  ∑ p0(x, y))/(m× n)  N (N)  locvp = argmin (x,y)  N∑  n=N  |Pavg − pn(x, y)| N  (N)  where Pavg is the probability that a VP exists in the im- age, pn(x, y) is the confidence of (x, y) on nth channel (n = 0: absence channel), m×n is the confidence map size, and locvp is the location of the VP
 N
Results  Our experiments consist of six parts
First, we show the  experimental settings such as dataset splits and training parameters
Secondly, we provide an analysis of our network
 We explore how multiple tasks jointly cooperate and affect  the performance of each other
Third, our evaluation metric  for each target is introduced
Lastly, we show lanes, road  markings, and VPs detection and classification results
 N.N
Experimental Settings  A summary of the datasets is provided in Table N
During  the training, we double the number of images by flipping the  original ones
This, in turn, doubles the training set and also  prevents positional bias that comes from the lane positions
 More specifically, the dataset is obtained in a right-sided  driving country, and by flipping the dataset we can simulate  a left-sided environment
 At the first training phase, we initialize the network only  by the VPP task
After the initialization, all four tasks are  trained simultaneously
For every task, we use Stochastic  Gradient Descent optimization with a momentum of 0.N and  a mini-batch size of N0
Since multiple tasks must converge  proportionally, we tune the learning rate of each task
 We train three models of the network divided by task:  N-Task (revised [NN]), N-Task (revised [NN]), and N-Task  (VPGNet)
N-Task network includes regression and binary  classification tasks
N-Task network includes N-Task and a  multi-label classification task
N-Task network includes NTask and a VPP task, which is the VPGNet
Since the lane  detection in [NN] is not fully reproducible, we modify the  data layer to handle the grid mask and move one convolutional layer from shared layers to branch layers, as in the Nand N-Task networks
The N-Task network is similar to [NN],  but we modify the data layer to handle the grid mask
 We test our models on NVIDIA GTX Titan X and  achieve a speed of N0 Hz by using only a single forward  pass
Specifically, the single forward pass takes about N0  ms and the post-processing takes about N0 ms or less
 N.N
Analysis of Multi Task Learning  In this section, we validate whether our multi-task modules contribute to improvement of the network training
We  NNNN    ConvN ConvN ConvN ConvN  (a) N-Task  (b) N-Task  (c) N-Task  Figure N
Activated neurons in the feature sharing network
 Intensity scale in each layer activation is equalized
 observe the activated neurons in the feature sharing network
From the lower to higher layer, the abstraction level  is accelerated
Figure N shows the activated neurons after  each convolutional layer before the branch
We average  over all channel values
For a fair comparison, we equalize  the intensity scale in each layer activation
As the results  show, if we use more tasks, more neurons respond, especially around the boundaries of roadways
 N.N
Evaluation Metrics  In this section, we show the newly proposed evaluation metrics for our benchmark evaluation
First, we introduce our evaluation metric for the lane detection
Since  the ground truth of our benchmark is annotated with grid  cells we compute the minimum distance from the center of  each cell to the sampled lane points for every cell
If the  minimum distance is within the boundary R, we mark these sampled points as true positive and the corresponding grid  cell as detected
By measuring every grid cell on the lane,  we can strictly evaluate the location of lane segments
Additionally, we measure FN score for the comparison
 In the case of road markings, we use mitigated evaluation  measurement
Since the only information we need while  driving is the road marking in front of us rather than the  exact boundary of the road markings, we measure the precision of predicted blobs
Specifically, we count all predicted  cells overlapped with the ground truth grid cells
The overlapped cells are marked as true positive cells
If the number  of true positive cells is greater than half of the number of  all predicted cells over a clustered blob, the overlaid ground  truth target is defined as detected
Additionally, we measure  the recall score for comparison
 For evaluation of the VP, we measure the Euclidean distance between a ground truth point and a predicted VP
The  recall score is evaluated by varying the threshold distance  R from the ground truth VP
Figure N shows a summary of how we measure all three targets of our network
 Sampled points  GT grid boxes  R: Threshold   distance  Ground truth Lane  Vanishing pointRoad marking  GT  Prediction  Detected VP  GT point  R  Figure N
Graphical explanation of the evaluation metrics
 N.N
Lane Detection and Recognition  For lane classes, we measure detection, as well as simultaneous detection and classification performance
First, we  compare our multi-task networks with the baseline methods  in the Caltech Lanes Dataset [N] (see Figure N)
We set R to equal to the average half value of the lane thickness (N0  pixels)
Due to perspective effect, the double lane in front of  the camera is about N0 to N0 pixels thick, and it is as small  as N pixels (a single grid size) near the VP
Since this dataset  contains relatively easy scenes during daytime, the overall  performance of N-, N-, and N-Task networks is very similar
 Nevertheless, our network achieves the best FN score
 Further, we provide a comparison of the proposed three  versions of multi-task networks and the FCN-Ns [NN] segmentation method on our benchmark dataset
It is important  to note that our networks utilize grid-level annotation, while  FCN-Ns is trained independently with both pixel- and gridlevel annotations
For testing purposes, four scenarios have  been selected as in Section N.N, and the FN score is compared in each scenario
Figure N shows the experimental  results
Noticeably, our method shows significantly better  lane detection performance in each bad weather condition  scenario
Moreover, the forward pass time of the VPGNet  is N0 ms, while FCN-Ns [NN] takes NN0 ms
 Interestingly, FCN-Ns shows better performance with the  proposed grid-level annotation scheme compared to pixellevel annotation
This proves that the grid-level annotation  is more suitable for lane detection and recognition
The reason is that grid-level annotation generates stronger gradients  from the edge information around the thinly annotated area  (i.e
lane or road markings), which, in turn, results in en0.NNN  0.NNN  0.NNN 0.NNN0.NNN 0.NNN  0.NNN 0.NNN  0.N  0.N  0.N  0.N  Caltech Set N (CordovaN) Caltech Set N (WashingtonN)  F N   S co  re  Caltech [N]  N-Task (Revised [NN])  N-Task (Revised [NN])  N-Task (VPGNet)  Figure N
Lane detection score on Caltech lanes dataset
 NNNN    F N   S co  re  0.NNN 0.NNN  0.NNN  0.NNN  0.NN  0.NNN  0.NNN  0.NNN  0.NN  0.NNN  0.NNN  0.NNN  0.NNN  0.N0N  0.NNN 0.NNN  0.NNN  0.NNN  0.NNN  0.N0N  0.N  0.N  0.N  0.N  Scenario N Scenario N Scenario N Scenario N  N-Task (Revised [NN])  N-Task (Revised [NN])  N-Task (VPGNet)  FCN-Ns (Pixel-annotated [NN])  FCN-Ns (Grid-annotated [NN])  Figure N
Lane detection score on our benchmark
 riched training and leads to better performance
 In order to see what happens if the VP does not exist, we  conducted an additional test on images without the VP (e.g
 intersection roads or occlusions)
Table N shows the results  of the experiment, demonstrating that the enhancement of  feature representation through the VPP task helps to find  lanes even when there is no VP
Selected results are shown  in the supplementary material
 For the simultaneous detection and classification of lane  classes, due to the class imbalance, we measure the FN score  of the top four lane classes by the number of instances
The  selected classes are: single white, dashed white, single yellow, and double yellow lines
Table N shows the performance of the N- and N-Task networks
Except for “no rain,  daytime condition”, recognition of the single white line is  highly improved
This shows that using the VPP task on  rainy and night conditions improves the activation of roadway boundaries which are usually marked with single white  lines
 N.N
Road Marking Detection and Recognition  In the case of road marking classes, we evaluate the simultaneous detection and classification performance
Due  to the dataset imbalance of road marking classes, we measure the recall score of the top four road marking classes by  the number of instances
The selected classes are as follows:  stop line, straight arrow, crosswalk, and safety zone
Table N  shows the performance of N- and N-Task networks
Except  for the stop line class in “no rain, daytime condition”, the  evaluation results are highly improved
This makes sense  because the stop line has horizontal edges which are not  Table N
Lane detection score on No-VP set (Red: Best)
 FCN-Ns  (pixel)  FCN-Ns  (grid)  N-Task  (revised [NN])  N-Task  (VPGNet)  No-VP set 0.NNN0 0.NNNN 0.NNNN 0.NNNN  Table N
Simultaneous detection and classification FN score  for lane classes (Red: Best)
 Lane class Single white Dashed white Single yellow Double yellow  Scenario N N-Task 0.NN 0.NN 0.NN 0.NN  N-Task 0.NN 0.NN 0.NN 0.NN  Scenario N N-Task 0.NN 0.NN 0.NN 0.NN  N-Task 0.NN 0.NN 0.NN 0.NN  Scenario N N-Task 0.NN 0.NN 0.N0 0.NN  N-Task 0.NN 0.NN 0.NN 0.N0  Scenario N N-Task 0.NN 0.NN 0.NN 0.NN  N-Task 0.NN 0.NN 0.NN 0.N0  Table N
Simultaneous detection and classification recall  score for road marking classes (Red: Best)
 Road marking class Stop line Straight arrow Crosswalk Safety zone  Scenario N N-Task 0.NN 0.NN 0.NN 0.NN  N-Task 0.NN 0.N0 0.NN 0.N0  Scenario N N-Task 0.N0 0.NN 0.NN 0.NN  N-Task 0.NN 0.NN 0.NN 0.NN  Scenario N N-Task 0.NN 0.NN 0.NN 0.NN  N-Task 0.NN 0.NN 0.NN 0.NN  Scenario N N-Task 0.N0 0.NN 0.NN 0.NN  N-Task 0.N0 0.NN 0.NN 0.NN  0 N0 N0 N0   Pixel distance N0 N00  Th e   pr op  or tio  n  of   tr ue   fr am  es  0  0.N  0.N  0.N  0.N  N  Daytime (no rain) Daytime (rain)  Daytime (heavy rain)  Night  0 N0 N0 N0   Pixel distance N0 N00  Th e   pr op  or tio  n  of   tr ue   fr am  es  0  0.N  0.N  0.N  0.N  N  VPP after Nst training phase  VPP after Nnd training phase  Figure N
Evaluation on the VPP task
 closely related to the VPP task
Other road markings have  shapes that give directions to VP from a geometric perspective
Consequently, responses to those classes become  highly activated
 N.N
Vanishing Point Prediction  In the case of a VP, we compare the VPP-only and N-Task  networks
In this manner, we can observe how the VPP is  influenced by the lane and road marking detection
Moreover, we compare the performances of each scenario
Figure N shows the experimental results
The left graph shows  a comparison between two outputs: a prediction after the  first phase and a prediction after the second phase
The prediction after the second phase is highly improved meaning  that the VPP task gets help from lane and road marking detection tasks
The right graph shows the results of the prediction after the second phase for each scenario
 N
Conclusions  In this work, we introduced lane and road marking  benchmark that covers four scenarios: daytime (no rain,  rain, heavy rain) and night conditions
We have also proposed a multi-task network for simultaneous detection and  classification of lane and road markings, guided by a VP
 The evaluation shows that the VPGNet model is robust  under different weather conditions and performs in realtime
Furthermore, we have concluded that the VPP task  enhances both lane and road marking detection and classification by enhancing activation of lane and road markings  and the boundary of the roadway
 Acknowledgement  This work was supported by DMC R&D Center of Samsung Electronics Co
 NNNN    References  [N] M
Aly
Real time detection of lane markers in urban streets
 In IV, N00N
 [N] O
Bailo, S
Lee, F
Rameau, J
S
Yoon, and I
S
Kweon
Robust road marking detection and recognition using densitybased grouping and machine learning techniques
In WACV,  N0NN
 [N] M
Bertozzi and A
Broggi
Real-time lane and obstacle detection on the system
IV, NNNN
 [N] A
Borji
Vanishing point detection with convolutional neural  networks
arXiv preprint arXiv:NN0N.00NNN, N0NN
 [N] A
Borkar, M
Hayes, and M
T
Smith
A novel lane detection system with efficient ground truth generation
IEEE  Transactions on Intelligent Transportation Systems (TITS),  NN(N):NNN–NNN, N0NN
 [N] T.-H
Chan, K
Jia, S
Gao, J
Lu, Z
Zeng, and Y
Ma
 Pcanet: A simple deep learning baseline for image classification? IEEE Transactions on Image Processing (TIP),  NN(NN):N0NN–N0NN, N0NN
 [N] N
Dalal and B
Triggs
Histograms of oriented gradients for  human detection
In CVPR, N00N
 [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
In  CVPR, N00N
 [N] H
Deusch, J
Wiest, S
Reuter, M
Szczot, M
Konrad, and  K
Dietmayer
A random finite set approach to multiple lane  detection
In ITSC, N0NN
 [N0] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
International Journal of Computer Vision (IJCV),  NN(N):N0N–NNN, N0N0
 [NN] R
Girshick
Fast r-cnn
In ICCV, pages NNN0–NNNN, N0NN
 [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
 [NN] J
Greenhalgh and M
Mirmehdi
Automatic detection and  recognition of symbols and text on the road surface
In  ICPRAM, N0NN
 [NN] B
He, R
Ai, Y
Yan, and X
Lang
Accurate and robust lane  detection based on dual-view convolutional neutral network
 In IV, N0NN
 [NN] J
Hur, S.-N
Kang, and S.-W
Seo
Multi-lane detection in  urban driving environments using conditional random fields
 In IV, N0NN
 [NN] B
Huval, T
Wang, S
Tandon, J
Kiske, W
Song,  J
Pazhayampallil, M
Andriluka, P
Rajpurkar, T
Migimatsu,  R
Cheng-Yue, et al
An empirical evaluation of deep learning on highway driving
arXiv preprint arXiv:NN0N.0NNNN,  N0NN
 [NN] H
Jung, J
Min, and J
Kim
An efficient lane detection algorithm for lane departure detection
In IV, N0NN
 [NN] J
Kim and M
Lee
Robust lane detection based on convolutional neural network and random sample consensus
In  ICONIP, N0NN
 [NN] M
Land, J
Horwood, et al
Which parts of the road guide  steering? Nature, NNN(NNNN):NNN–NN0, NNNN
 [N0] M
F
Land and D
N
Lee
Where do we look when we steer
 Nature, NNN(NNNN):NNN–NNN, NNNN
 [NN] J
Li, X
Mei, and D
Prokhorov
Deep neural network  for structural prediction and lane detection in traffic scene
 IEEE Transactions on Neural Networks and Learning Systems (TNNLS), PP(NN):N–NN, N0NN
 [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV, N0NN
 [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, and S
Reed
 Ssd: Single shot multibox detector
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
 [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
 [NN] J
Matas, O
Chum, M
Urban, and T
Pajdla
Robust widebaseline stereo from maximally stable extremal regions
Image and Vision Computing, NN(N0):NNN–NNN, N00N
 [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
arXiv  preprint arXiv:NN0N.0NNN0, N0NN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
 [NN] D
D
Salvucci and R
Gray
A two-point visual control  model of steering
Perception, NN(N0):NNNN–NNNN, N00N
 [NN] R
Satzoda and M
Trivedi
Vision-based lane analysis: Exploration of issues and approaches for embedded realization
 In CVPR Workshops, N0NN
 [N0] P
Sermanet, D
Eigen, X
Zhang, M
Mathieu, R
Fergus,  and Y
LeCun
Overfeat: Integrated recognition, localization  and detection using convolutional networks
arXiv preprint  arXiv:NNNN.NNNN, N0NN
 [NN] H
Tan, Y
Zhou, Y
Zhu, D
Yao, and K
Li
A novel curve  lane detection based on improved river flow and ransa
In  ITSC, N0NN
 [NN] D
G
Viswanathan
Features from accelerated segment test  (fast), N00N
 [NN] P.-C
Wu, C.-Y
Chang, and C
H
Lin
Lane-mark extraction  for automobiles under complex conditions
Pattern Recognition, NN(N):NNNN–NNNN, N0NN
 [NN] T
Wu and A
Ranganathan
A practical system for road  marking detection and recognition
In IV, N0NN
 [NN] Z
Zhu, D
Liang, S
Zhang, X
Huang, B
Li, and S
Hu
 Traffic-sign detection and classification in the wild
In  CVPR, N0NN
 NNNNTemporal Dynamic Graph LSTM for Action-Driven Video Object Detection   Temporal Dynamic Graph LSTM for Action-driven Video Object Detection  Yuan YuanN Xiaodan LiangN Xiaolong WangN Dit-Yan YeungN Abhinav GuptaN  NThe Hong Kong University of Science and Technology N Carneige Mellon University  yyuanad@ust.hk, xiaodanN@cs.cmu.edu, xiaolonw@cs.cmu.edu, dyyeung@cse.ust.hk, abhinavg@cs.cmu.edu  Abstract  In this paper, we investigate a weakly-supervised object detection framework
Most existing frameworks focus  on using static images to learn object detectors
However,  these detectors often fail to generalize to videos because of  the existing domain shift
Therefore, we investigate learning  these detectors directly from boring videos of daily activities
Instead of using bounding boxes, we explore the use of  action descriptions as supervision since they are relatively  easy to gather
A common issue, however, is that objects  of interest that are not involved in human actions are often absent in global action descriptions known as “missing  label”
To tackle this problem, we propose a novel temporal dynamic graph Long Short-Term Memory network (TDGraph LSTM)
TD-Graph LSTM enables global temporal  reasoning by constructing a dynamic graph that is based  on temporal correlations of object proposals and spans the  entire video
The missing label issue for each individual  frame can thus be significantly alleviated by transferring  knowledge across correlated objects proposals in the whole  video
Extensive evaluations on a large-scale daily-life action dataset (i.e., Charades) demonstrates the superiority of  our proposed method
We also release object bounding-box  annotations for more than N,000 frames in Charades
We  believe this annotated data can also benefit other research  on video-based object recognition in the future
 N
Introduction  With the recent success of data-driven approaches in  recognition, there has been a growing interest in scaling  up object detection systems [NN]
However, unlike classification, exhaustively annotating object instances with  diverse classes and bounding boxes is hardly scalable
 Therefore, there has been a surge in exploring in unsupervised and weakly-supervised approaches for object detection
However, fully unsupervised approaches [N0, NN]  without any annotations currently give considerably inferior  performance on similar tasks, while conventional weaklysupervised methods [N, NN, NN] use static images to learn  the detectors
These object detectors, however, fail to generalize to videos due to shift in domain
One alternative is  to use these weakly-supervised approaches but using video  frames themselves
However, current approaches rely heavily on the accuracy of image-level labels and are vulnerable  to missing labels (as shown in Figure N)
Can we design a  learning framework that is robust to these missing labels ?  In this paper, we explore a novel slightly-supervised  video object detection pipeline that uses human action labels as supervision for object detection
As illustrated in  Figure N, the coarse human action labels spanning multiple frames (e.g., watching a laptop or sitting in a chair)  help indicate the presence of participating object instances  (e.g., laptop and chair)
Compared to prior works, our investigated setting has two major merits: N) the textual action descriptions for videos are much cheaper to collect,  e.g., through text tags, search queries and action recognition datasets [NN, N0, NN]; and N) the intrinsic temporal coherence in video domain provides more cues to facilitate the  recognition of each object instance and help overcome the  missing label problem
 Action-driven supervision for object detection is much  more challenging since it can only access object labels for  some specific frames, while a considerable number of uninvolved object labels are unknown
As shown in the right  column of Figure N, four action categories are labeled for  different periods in the given video
In each period, the action label (e.g., tidying a shelf ) only points out the shelf  category and misses the rest of the categories such as laptop, table, chair and refrigerator
On the other hand, the  missed categories (e.g., laptop) may appear in other labeled  actions in the same video
Inspired by this observation, we  propose to alleviate the missing label issue by exploiting the  rich temporal correlations of object instances in the video
 The core idea is that action labels in a different period may  help to infer the presence of some objects in this current period
Specifically, a novel temporal dynamic graph LSTM  (TD-Graph LSTM) framework is introduced to model the  complex and dynamic temporal graph structure for object  proposals in the whole video and thus enable the joint reasoning for all frames
The knowledge of all action labels in  NNN0N    shelf laptop laptop  table/desk  laptop  table/desk  chair  laptop  table/desk  chair  shelf  food/sandwich  laptop  table/desk  chair  refrigerator  shelf  food/sandwich  laptop  table/desk  chair  refrigerator  shelf  laptop  table/desk  chair  refrigerator  shelf  laptop  table/desk  chair  refrigerator  shelf  laptop  table/desk  chair  refrigerator  Tidying a shelf Watching a laptop Sitting in a chair Sitting at a tableAction label:  Video frames  Action labels  person horse chair pottedplant  cat motorbike  Weakly-supervised object detection Action-driven video object detection  Figure N
(Left) shows the traditional weakly-supervised object detection setting
Each training image has an accurate image-level annotation about object categories
(Right) shows our action-driven weakly-supervised video detection setting
Video-level action labels  are provided for each video, indicating what and when (the start and end) the action happened in the video
For each frame, the object  categories in its left-below are the participating objects in the action label, while those in its right-below are all objects appearing in the  frame
 the video can thus be effectively transfered into all frames  to enhance their frame-level categorizations
 To incorporate the temporal correlation of object proposals for global reasoning, we resort to the family of recurrent  neural networks [NN] due to their good sequential modeling  capability
However, existing recurrent networks are largely  limited in the constrained information propagation on fixed  nodes following predefined routes such as tree-LSTM [NN],  graph-LSTM [N0] and structural-RNN [NN, NN]
In contrast, due to the unknown object localizations and temporal  motion, it is difficult to find an optimal structure that connects object proposals for routed information propagation  to achieve weakly-supervised video object detection
The  proposed TD-Graph LSTM, posed as a general dynamic recurrent structure, overcomes these limitations by performing the dynamic information propagation based on an adaptive temporal graph that varies over both time periods in the  video and model status in each updating step
 Specifically, the dynamic temporal graph is constructed  based on the visual correlation of object proposals across  neighboring frames
The set of graph nodes denotes the entire collection of object proposals in all the frames, while  graph edges are adaptively specified for consecutive frames  in distinct learning steps
At each iteration, given the updated feature representation of object proposals, we only activate the edge connections with object proposals that have  highest similarities with each current proposal
The adaptive graph topology can thus be constructed where different  proposals are connected with different temporal correlated  neighbors
TD-Graph LSTM alternatively performs the information propagation through each temporal graph topology and updates the graph topology at each iteration
In  this way, our model enables the joint optimization of feature  learning and temporal inference towards a robust slightlysupervised detection framework
 The contributions of this paper are summarized as N)  We explore a new slightly-supervised video object detection pipeline that leverages convenient action descriptions  as the supervision; N) A novel TD-Graph LSTM framework alleviates the missing label issue by enabling global  reasoning over the whole video; N) TD-Graph LSTM is  posed as a general dynamic recurrent structure that performs temporal information propagation on an adaptively  updated graph topology at each iteration; N) We collect and  release N,000 frame annotations with object-level bounding  boxes on daily-life videos, with the goal of evaluating our  model and also helping advance the object detection community
 N
Related Works  Weakly-Supervised Object Detection
Though recent  state-of-the-art fully-supervised detection pipelines [N, NN,  N, NN, NN] have achieved great progress, they heavily rely  on large-scale bounding-box annotations
To alleviate this  expensive annotation labor, weakly-supervised methods [N,  NN, N, NN, NN, N, NN, NN] have recently attracted a lot of interest
These approaches use cheaper image-level object labels rather than bounding boxes
Beyond the image domain,  another line of research [NN, NN, NN, NN, NN, NN, NN, NN]  attempts to exploit the temporal information embedded in  videos to facilitate the weakly-supervised object detection
 Different from all the existing pipelines, we investigate a  much cheaper action-driven object detection setting that  aims to detect all object instances given only action descriptions
In addition, instead of employing multiple separate  steps (e.g., detection and tracking) [NN, NN, NN, NN, NN] to  capture motion patterns, our TD-graph LSTM is an end-toend framework that incorporates the intrinsic temporal coherence with a designed dynamic recurrent network structure into the action-driven slightly-supervised detection
 Sequential Modeling
Recurrent neural networks, espeNN0N    �"#$  Spatial ConvNet  Temporal Dynamic Graph Construction  Spatial region features  Region-level  Classification	  Module  �"  �"%$  …  TD-Graph	  LSTM	unit  Action-driven	  Loss  Temporal-aware  region features  Temporal-aware  region features  Temporal-aware  region features  Action-driven	  Loss  Action-driven	  Loss  Figure N
Our TD-Graph LSTM
Each frame is first passed into a spatial ConvNet to extract region-level features
A temporal graph  structure is then constructed by dynamic edge connections between regions in two consecutive frames
TD-Graph LSTM then recurrently  propagates information over the updated graph to generate temporal-aware feature representations for all regions
A region-level classification module is then adopted to produce category confidences of all regions in each frame, which are aggregated to obtain frame-level action  predictions
The final action-driven loss for each frame is used to feedback signals into the whole model
After each gradient updating, the  temporal graph is dynamically updated based on new visual features
For clarity, some edges in the graph are omitted
 cially Long Short-Term Memory (LSTM) [NN], have been  adopted to address many video processing tasks such as  action recognition [NN], action detection [NN], video prediction [NN, NN], and video summarization [NN]
However,  limited by the fixed propagation route of existing LSTM  structures [NN], most of the previous works [NN, NN, NN] can  only learn the temporal interdependency between the holistic frames rather than more fine-grained object-level motion  patterns
Some recent approaches develop more complicated recurrent network structures
For instance, structuralRNN [NN] develops a scalable method for casting an arbitrary spatio-temporal graph as a rich RNN mixture
A more  recent Graph LSTM [NN] defined over a pre-defined graph  topology enables the inference for more complex structured  data
However, both of them require a pre-fixed network  structure for information propagation, which is impractical  for weakly-supervised/slightly-supervised object detection  without the knowledge of object localizations and precise  object class labels
To handle the propagation over dynamically specified graph structures, we thus propose a new temporal dynamic network structure that supports the inference  over the constantly changing graph topologies in different  training steps
 N
The proposed TD-Graph LSTM  Overview
We establish a fully-differentiable temporal dynamic graph LSTM (TD-Graph LSTM) framework  for the action-driven video object detection task
For each  video, the provided annotations are a set of action labels  Y = {yN, 


, yN}, each of which describes the action yi =< ai, ci > appearing within a consecutive sequence of frames {Ids  i , 


, Ide  i }, where dsi and d  e i indicate the action  starting and ending frame index
ai denotes the corresponding action noun while ci denotes the object noun
For example, the action tidying a shelf is comprised of the action  Tidying and object a shelf
To achieve weakly-supervised  object detection, we only extract the object nouns {ci} of action labels in all videos and eliminate the prepositions  (e.g., a, the) to produce an object category corpus (e.g.,  shelf, door, cup) with C classes
Each frame I can be  thus assigned with several participating object classes
For  example, frames with two actions will be assigned with  more than one participating object class, as shown in Figure N
The action-driven object detection is thus posed as a  multi-class weakly-supervised video object detection problem
For simplicity, we eliminate the subscript i of action  labels in the following
 Figure N gives an overview of our TD-Graph LSTM
 Each frame in the input video is first passed through a  spatial ConvNet to obtain spatial visual features for region proposals
Based on visual features, similar regions  in two consecutive frames are discovered and associated  to indicate the same object across the temporal domain
 A temporal graph structure is constructed by connecting  all of the semantically similar regions in two consecutive  frames, where graph nodes are represented by region proposals
The TD-Graph LSTM unit is then employed to recurrently propagate information over the whole temporal  graph, where LSTM units take the spatial visual features  as the input states
Benefiting from the graph topology, TDGraph LSTM is capable of incorporating temporal motion  patterns for participating objects in the action in a more efficient and meaningful way
TD-Graph LSTM outputs the  enhanced temporal-aware features of all regions
Regionlevel classification is then employed to produce classification confidences
These region-level predictions can finally  be aggregated to generate frame-level object class prediction, supervised by the object classes from action labels
 The action-driven object categorization loss thus enables  the holistic back-propagation into all regions in the video,  NN0N    Frame-level	  hidden	 states	�" #$% &  Frame-level	  memory	states	�" #$% &  �#,% &  �* #,% &  Visual features   �#,+ &�* #,+  &  �#,, &�* #,,  &  Shared	  LSTM	unit…  �#,% & �#,+  & �#,, &… �#,%  & �#,+ & �#,,  &…  Frame-level	  hidden	 states	�" # &  Frame-level	  memory	states	�" # &  Temporal  context features   �#$%,-
&  �#$%,-/ &  �#$%,-0 &  �#,#$%,+,-
&  �#,#$%,+,-/ &  �#,#$%,+,-0 &…  …  Temporal Graph �&  …  …  Average Average  Region-level  Classification	 Module  Recurrently update states of frame �� �:  Update  next frame  Figure N
Illustration of the TD-Graph LSTM layer at t-th gradient updating
Given the constructed temporal graph Gt, the TD-Graph LSTM recurrently updates the hidden states of each frame Ii, i ∈ {N, 


, N} as the enhanced temporal-aware visual feature, and then feeds these features into a region-level classification module to compute final category confidences of all regions
Specially, each LSTM  unit takes the shared frame-level hidden states h̄ti−N and memory states m̄ t i−N, and input features for all regions as the inputs
Then the  updated hidden states and memory states for all regions are produced, which are then averaged to generate the new frame-level hidden  states h̄ti and memory states m̄ t i for updating next frame Ii+N
The input features of each region consist of the visual features f  t i,j and  temporal context features f̂ ti,j that are aggregated by its connected regions with edge weights in the preceding frame
 where the prediction of each frame can mutually benefit  from each other
 N.N
TD-Graph LSTM Optimization  The proposed TD-Graph LSTM is comprised by three  parametrized modules: spatial ConvNet Φ(·) for visual fea- ture extraction, TD-Graph LSTM unit Ψ(·) for recurrent temporal information propagation, and region-level classification module ϕ(·)
These three modules are iteratively updated, targeted at the action-driven object detection
 At each model updating step t, a temporal graph structure Gt =< V, Et > for each video is constructed based on the updated spatial visual features f t of all regions r in  the videos, defined as Gt = β(Φt(r))
β(·) is a function to calculate the dynamic edge connections Et conditioning on the updated visual features f t = Φt(r)
The TD-Graph LSTM unit Ψt recurrently functions on the visual features f t of all frames and propagates temporal information over  the graph Gt to obtain the enhanced temporal-aware fea- tures f̂ t = Ψt(f t|Gt) of all regions in the video
Based  on the enhanced f̂ t, the region-level classification module  ϕ produces classification confidences rct for all regions, as  rct = ϕ(f̂ t)
These region-level category confidences rct  can be aggregated to produce frame-level category confidences pct = γ(rct) of all frames by summing the cate- gory confidences of all regions of each frame
 During training, we define the action-driven loss for each  frame as a hinge loss function and train a multi-label image  classification objective for all frames in the videos:  L(Φ,Ψ, ϕ) = N  CN  C∑  c=N  N∑  i=N  max(0, N− yc,ipcc,i)  = N  CN  C∑  c=N  N∑  i=N  max(0, N− yc,iγ(ϕ(Ψ(fi|G)))),  (N)  where C is the number of classes and yc,i, i ∈ {N, 


, N} represents action-driven object labels for each frame
For  each frame Ii, yc,i = N only if the action-driven object la- bel c is assigned to the frame Ii, otherwise as -N
The objective function defined in Eq
N can be optimized by the  Stochastic Gradient Descent (SGD) back-propagation
At  each t-th gradient updating, the temporal graph structure Gt  is accordingly updated by β(Φt(r)) for each video
Thus, the TD-Graph LSTM unit optimizes over a dynamically updated graph structure Gt
In the following sections, we in- troduce the above-defined parametrized modules
 N.N
Spatial ConvNet  Given each frame Ii, we first extract category-agnostic  region proposals and then extract their visual features by  passing them into a spatial ConvNet Φ(·) following [N]
To provide a fair comparison on action-driven object detection, we adopt the EdgeBoxes [N0] proposal generation method which does not require any object annotations for pretraining
We select the top M = N00 pro- posals ri = {ri,N, ri,N, ..., ri,M} for the frame Ii with the highest objectness scores, considering the computation efficiency
At the t-th updating step, visual features f ti = {f ti,N, f  t i,N, ..., f  t i,M} ∈ R  M×D of all regions ri are extracted  using the updated spatial ConvNet model, i.e., f ti = Φ t(ri)
 The spatial ConvNet Φ(·) consists of several convolutional layers from the base net and one ROI-pooling layer [N], and  two fully-connected layers
 N.N
TD-Graph LSTM Unit  Dynamic Graph Updating
Given the updated visual  features f ti of each frame Ii, the temporal graph structure  Gt =< V, Et > can be accordingly constructed by learn- ing the dynamic edge connections Et
The graph node  NN0N    V = {vi,j}, j = {N, 


,M} is represented by visual fea- tures {f ti,j} of all regions in all frames; that is, M×N nodes for M region proposals of N frames
Each node vi,j is connected with nodes in the preceding frame Ii−N and the nodes  in subsequent frame Ii+N
To incorporate the motion dependency in consecutive frames, the edge connections Eti,i−N between nodes in Ii and Ii−N are mined by considering  their appearance similarities in visual features
Specifically,  the edge weight between each pair of nodes (vi,j , vi−N,j′) is first calculated as NN exp(−||f  t i,j − f  t i−N,j′ ||N)
To make  the model inference efficient and alleviate the missing issue, each node vi,j is only connected to K nodes vi−N,j′  with the top-K highest edge weights in preceding frame  Ii−N, and these activated edge weights are normalized to  be summed as N
We denote the normalized edge weight as  ωti,i−N,j,j′ 
Thus, the updated temporal graph structure G t  can be regarded as an undirected K-neighbor graph where  each node vi,j is connected with at most K nodes in previous frames
 TD-Graph LSTM
TD-Graph LSTM layer propagates  temporal context over graph and recurrently updates the  hidden states {hti,j} of all regions in each frame Ii to construct enhanced temporal-aware feature representations
 These features are fed into the region-level classification  module to compute the category-level confidences of each  region
TD-Graph LSTM updates hidden state of frame i  by incorporating information from frame-level hidden state  h̄ti−N and memory state m̄ t i−N
The usage of the shared  frame-level hidden state and memory state enables the provision of a compact memorization of temporal patterns in  the previous frame and is more suitable for massive and  possibly missing graph nodes (e.g., N00 in our setting) in  a large temporal graph
After performing N updating steps  for all frames, our model effectively embeds the rich temporal dependency to obtain the enhanced temporal-aware  feature representations of all regions in all frames
For updating the features of each node vi,j in the frame Ii, the  TD-Graph LSTM unit takes as the input its own visual features f ti,j , temporal context features f̂ t i,j , frame-level hidden states h̄ti−N and memory states m̄ t i−N, and outputs the  new hidden states hti,j 
Given the dynamic edge connections ei,j = {< vi,j , vi−N,j′ >}, j ′ ∈ NG(vi,j), the temporal context features f̂ ti,j can be calculated by performing a  weighted summation of features of connected regions:  f̂ ti,j = ∑  j′∈NG(vi,j)  ωti,i−N,j,j′f t i−N,j′ 
(N)  And the shared frame-level hidden states h̄ti−N and memory  states m̄ti−N can be computed as  h̄ti−N = N  M  M∑  j=N  hti−N,j , m̄ t i−N =  N  M  M∑  j=N  mti−N,j 
(N)  The TD-Graph LSTM unit consists of four gates for each  node vi,j : the input gate gu t i,j , the forget gate gf  t i,j ,  the memory gate gcti,j , and the output gate go t i,j 
The  Wut ,W f t ,W  c t ,W  o t are the recurrent gate weight matrices  specified for input visual features and Wutt ,W ft t ,W  ct t ,W  ot t  are those for temporal context features
Uut , U f t , U  c t , U  o t  are the weight parameters specified for frame-level hidden  states
The new hidden states and memory states in the  graph Gt can be calculated as follows:  guti,j =δ(W u t f  t i,j +W  ut t f̂  t i,j + U  u t h̄  t i−N + b  u t ),  gf ti,j =δ(W f t f  t i,j +W  ft t f̂  t i,j + U  f t h̄  t i−N + b  f t ),  goti,j =δ(W o t f  t i,j +W  ot t f̂  t i,j + U  o t h̄  t i−N + b  o t ),  gcti,j =tanh(W c t f  t i,j +W  ct t f̂  t i,j + U  c t h̄  t i−N + b  c t),  mti,j =gf t i,j ⊙ m̄  t i−N + gu  t i,j ⊙ gc  t i,j ,  hti,j =go t i,j ⊙ tanh(m  t i,j)
 (N)  Here δ is a logistic sigmoid function, and ⊙ indicates a point-wise product
Given the updated hidden states {hti,j} and memory states {mti,j} of all regions in frame Ii, we can  obtain new frame-level hidden states h̄ti and memory states  m̄ti for updating the states of regions in frame Ii+N
The  TD-LSTM unit recurrently updates the states of all regions  in each frame, and thus the past temporal information in  preceding frames can be utilized for updating each frame
 The TD-Graph LSTM layer is illustrated in Figure N
 N.N
Region-level Classification Module  Given the updated hidden states hti,j for each node vi,j ,  we use a region-level classification module to obtain the category confidences of all regions, that is, rcti = ϕ(h t i) of all  M regions
Following the two-stream architecture of WSDDN [N], the region-level classification module contains a  detection stream and a classification stream, and produces  final classification scores by performing element-wise multiplication between them
The classification stream takes  the region-level feature vectors hti of all regions as the input  and feeds it to a linear layer that outputs a set of class scores  Sti ∈ R M×C for C classes of all M regions
Here, we use  the reproduced WSDDN in [NN] that does not employ an additional softmax in the classification stream
These differences have a minor effect on the detection accuracy as has  been discussed in [NN]
The detection stream also takes hti as the input and feeds it to another linear layer that outputs  a set of class scores, giving a matrix of scores Lti ∈ R M×C 
 Lti is then fed to another softmax layer to normalize the  scores over the regions in the frame
The final scores of all  regions rcti are obtained by taking the element-wise multiplication of the two scoring matrices Sti and L t i
We sum all  the region-level class scores rcti to obtain the frame-level  class prediction scores pcti
 NN0N    N
Experiments  N.N
Dataset and Evaluation Measures  Dataset Analysis
We evaluate the action-drive weaklysupervised object detection performance on the Charades  dataset [NN]
The Charades video dataset is composed of  daily indoor activities collected through Amazon Mechanical Turk
There are NNN action classes and on average N.N  actions in each video, which occur in various orders and  contexts
In order to detect objects in videos by using action  labels, we only consider the action labels that are related  to objects for training
Therefore, there are NN action labels that are related to NN object classes in our experiments
 We show distribution of object classes (in a random subset  of videos) in Figure N (a)
The training set contains N,NNN  videos
Videos are down-sampled to N fps and we only sample the frames assigned with action labels in each video
 During training, only frame-level action labels are provided  for each video
 In order to evaluate the video object detection performance over NN daily object classes, we collect the bounding  box annotations for N,000 test frames from N00 videos in  the Charades test set
The bounding box number distribution in each frame is shown in Figure N (b), ranging from N  to NN boxes appearing in the frame
More than N0% frames  have more than N bounding boxes and most video frames  exhibit severe motion blurs and low resolution
This poses  more challenges for the object detection model compared to  an image-based object detection dataset, such as the most  popular PASCAL VOC [N] that is widely used in existing  weakly-based object detection methods
Figure N further  shows example frames with action labels on the Charades  dataset
It can be seen that each action label only provides  one piece of object class information for the frame that may  contain several object classes, which can be regarded as the  missing label issue for training a model under this actiondriven setting
Moreover, the video frames often appear  with a very cluttered background, blurry objects and diverse  viewpoints, which are more challenging and realistic compared to existing image datasets (e.g., MS COCO[NN] and  ImageNet[NN]) and video datasets (e.g., UCFN0N[NN])
 Evaluation Measures
We evaluate the performance of  both object detection and image classification tasks on Charades
For detection, we report the average precision (AP)  at N0% intersection-over-union (IOU) of the detected boxes  with the ground truth boxes
For classification, we also report the AP on frame-level object classification
 N.N
Implementation Details  Our TD-Graph LSTM adopts the VGG-CNN-F  model [N] pre-trained on ImageNet ILSVRC N0NN challenge data [NN] as the base model, and replaces the last  pooling layer poolN with an SPP layer [N] to be compatible  Holding a cup Throwing a towel  Sitting at a table Lying on a sofa Putting a towel Holding a laptop Putting a towel  Lying on a sofa Pour something into a cup Taking a broom  broom  towellaptop  cupsofa  towelsofatable  towelcup  Figure N
Several samples of key frames from videos in Charades
 The action labels are given at the bottom of the image and the  related objects are listed at the top of the image
 with the first fully connected layer
We use the EdgeBoxes  algorithm [NN] to generate the top N00 regions that have  width and height larger than N0 pixels as candidate regions  for each frame
To balance the performance and time  cost, we set the number of edges linked to each node K to  N00
For training, we use stochastic gradient descent with  momentum 0.N and weight decay N × N0−N
All weight matrices used in the TD-Graph LSTM units are randomly  initialized from a uniform distribution of [−0.N, 0.N]
TD-Graph LSTM predicts the hidden and memory states  with the same dimension as the previous region-level CNN  features
Each mini-batch contains at most N consecutive  sampled frames in a video
The network is trained on  the Charades training set by using fine-tuning on all  layers, including those of the pre-trained base CNN model
 The experiments are run for N0 epochs for the model  convergence
The learning rates are set to N0−N for the first ten epochs, then decreased to N0−N
All our models are implemented on the public Torch [N] platform, and all  experiments are conducted on a single NVIDIA GeForce  GTX TITAN X GPU with NN GB memory
The runtime is  N.N fps and N.N fps for training and testing respectively
 N.N
Results and Comparisons  We compare the proposed TD-Graph LSTM model with  two state-of-the-art weakly-supervised learning methods  on the Charades dataset, WSDDN [N] and ContextLocNet [NN]
As both of the two methods were proposed  for image-based weakly-supervised image object detection,  here we run the source code of ContextLocNet [NN] and  their reproduced WSDDNN on the Charades dataset to  make a fair comparison with our method
Their models  are trained by treating the action-related object labels in  each frame as the supervision information and are evaluated  on each video frame
The difference between our model  and WSDDN [N] is our usage of TD-Graph LSTM layers  to leverage rich temporal correlations in the whole video
 Similar to WSDDN, ContextLocNet is also a two stream  model with an enhanced localization module using various  Nhttps://github.com/vadimkantorov/contextlocnet  NN0N  https://github.com/vadimkantorov/contextlocnet   0  N  N  N  N  N0  NN  NN  NN  NN  0 N N N N N N N N N N0 NN NN NN NN NN NN NN NN NN N0 NN NN NN  P e  rc e  n ta  g e   ( %  )  Number of  Objects  0  N00  N000  NN00  N000  NN00  N000  NN00  N000 N  u m  b e  rs  o  f  O  b je  ct s  (�) (�)  Figure N
(a) The distribution of object classes appearing in the action labels of the training set
(b) The distribution of the ground truth  bounding box numbers in each image of the test set
 Table N
Per-class performance comparison of our proposed models with two state-of-the-art weakly-supervised learning methods when  evaluating on the Charades dataset[NN], test classification average precision (%)
 Method bed broom chair cup dish door laptop mirror pillow refri shelf sofa table tv towel vacuum window mAP  WSDDN [N] NN.N N.NN NN.N NN NN.N NN.N N0.N N.N N.N N.N N.N NN.N NN.N N NN.N N.N N.N NN.NN  ContextLocNet [NN] NN.NN N.NN NN.NN NN.NN NN.NN N.NN NN.NN N.N N.NN NN.NN N.NN NN.NN NN.NN NN.NN N.NN NN.NN N.NN NN.NN  TD-Graph LSTM w/o LSTM NN.NN N.NNN NN.NN NN.N NN.NN NN.NN NN.NN N.NN N.NN N.N N.N NN.NN NN N.NN NN.N N.NN N.NN NN.NN  TD-Graph LSTM w/o graph NN.0N N.NN NN.NN NN.NN NN.N NN.NN NN.NN N.NN N.NN N.N N.0N NN.NN NN.N N.NN NN.NN NN.NN N.NN NN.NN  TD-Graph LSTM NN.NN NN.NN NN.0N NN.NN NN.N NN.N N0.N N.0N NN.NN N.NN N.NN NN.NN NN.NN N.NN NN.NN NN.0N N.NN NN.NN  Table N
Per-class performance comparison of our proposed models with two state-of-the-art weakly-supervised learning methods when  evaluating on the Charades dataset[NN], test detection average precision (%)
 Method bed broom chair cup dish door laptop mirror pillow refri shelf sofa table tv towel vacuum window mAP  WSDDN [N] N.NN 0.0N N.NN 0.0N 0.NN 0.NN N.NN 0.NN 0.0N 0.NN 0.0N 0.NN N.NN N.NN 0.0N 0.0N 0.NN 0.NN  ContextLocNet [NN] N.N 0.0N 0.NN 0.0N 0.0N 0.NN N.NN 0.NN 0 0.0N N.NN N.NN 0.NN 0.NN 0.0N 0.NN 0.NN N.NN  TD-Graph LSTM w/o LSTM N.NN 0.0N N 0.0N 0.0N 0.NN 0.NN 0.NN 0.0N 0.NN 0.NN N.NN N.NN N.NN 0.NN 0.NN N.NN N.NN  TD-Graph LSTM w/o graph N.NN 0.0N N.NN 0.NN 0.0N 0.NN N.NN 0.NN 0.0N 0.NN N.NN N.NN N.NN N.0N 0.0N 0.NN 0.NN N.NN  TD-Graph LSTM N.NN 0.0N N.NN 0.NN 0.NN N.NN N.NN 0.N 0.0N 0.NN N.NN N.NN N.NN N.NN 0.0N 0.N 0.NN N.NN  Frame NNN Frame NNN Frame NNN   Figure N
Our TD-Graph LSTM addresses well the missing label  issue
It can successfully detect the refrigerator that is not referred  to by any action labels (A green box shows the detection result and  yellow box the ground truth.)  surrounding context
Specifically, we use the contrastive-S  setup of ContextLocNet
All of these models use the same  base model and region proposal method, i.e., VGG-CNN-F  model [N] and EdgeBoxes [NN]
 We report the comparisons with two state-of-the-art on  classification mAP and detection mAP in Table N and Table N, respectively
It can be observed that our TD-Graph  LSTM model substantially outperforms two baselines on  both classification mAP and detection mAP, particularly,  N.0N% higher than ContextLocNet [NN] and N.NN% than  WSDDN [N] in terms of classification mAP
Especially, our  TD-Graph LSTM surpasses two baselines in small objects,  e.g., over NN.NN% for pillow class and N.NN% for cup class
 Although our model and two baselines all obtain low detection mAP under this challenging setting, our TD-Graph  LSTM still surpasses two baselines on detecting crowded  and small objects in the video
The superiority of our TDGraph LSTM clearly demonstrates its effectiveness in challenging action-driven weakly-supervised object detection  where the missing label issue is quite severe and a considerable number of bounding boxes appear in each frame  with very low quality
We further show the qualitative comparison with two state-of-the-arts in Figure N
Our model  is able to produce more precise object detection for even  very small objects (e.g., the cup in the middle row) and  objects with heavy occlusion (e.g., the sofa in the bottom  row)
Our TD-Graph LSTM takes the advantage of exploiting complex temporal correlations between region proposals by propagating knowledge into a whole dynamic temporal graph, which effectively alleviates the critical missing  label issue, as shown in Figure N
 NN0N    TD-Graph LSTMWSDDN ContextLocNet  laptop  cup  sofa  Figure N
Qualitative comparisons with two state-of-the-arts on video object detection
The green boxes indicate detection results and  yellow ones are the ground truth
 Table N
Performance comparison of using different graph topologies when evaluating on the Charades dataset, test detection mAP  (%) and classification mAP (%)
 Method det mAP cls mAP  Ours w/o Graph N.NN NN.NN  Ours w/ Mean Graph N.NN NN.NN  Ours w/ Static Graph N.NN NN.NN  Ours N.NN NN.NN  N.N
Ablation Study  The results of model variants are reported in Table N,  Table N and Table N
 The effectiveness of incorporating graph
The main  difference between our TD-Graph with a conventional  LSTM structure for sequential modeling is in propagating  information over a dynamic graph structure
To verify its  effectiveness, we thus compare our full model with the variant “TD-Graph LSTM w/o graph” that eliminates the edge  connections between regions in consecutive frames, and updates the frame-level hidden and memory states with the  original region-level features
Our TD-Graph LSTM consistently obtains better results over “TD-Graph LSTM w/o  graph”, which speaks to the advantage of incorporating a  graph for the challenging action-driven object detection
 The effectiveness of temporal LSTM
We further verify that recurrent sequential modeling by the LSTM units  over the temporal graph is beneficial for exploiting complex  object motion patterns in daily videos
“TD-Graph LSTM  w/o LSTM” indicates removing the LSTM units and directly aggregating the temporal context features to enhance  features of each region
The performance gap between our  full model and “TD-Graph LSTM w/o LSTM” verifies the  benefits of adopting LSTM
 Dynamic graph vs Static graph vs Mean graph
Besides the proposed dynamic graph, another commonly used  alternative is the fully-connected graph where each region is  densely connected with all regions in the preceding frame;  that is, “Ours w/ Static Graph” and “Ours w/ Mean Graph”
 “Ours w/ Static Graph” uses the adaptive edge weights similar to TD-Graph LSTM while “Ours w/ Mean Graph” uses  the same weights for all edge connections
It can be seen  that applying a dynamic graph structure can help significantly boost both detection and classification performance  over other fully-connected graphs
The reason is that meaningful temporal correlations between regions can be discovered by the dynamic graph and leveraged to transfer motion  context into the whole video
 N
Conclusion  In this paper, we propose a novel temporal dynamic  graph LSTM architecture to address action-driven weaklysupervised object detection
It recurrently propagates the  temporal context on a constructed dynamic graph structure  for each frame
The global action knowledge in the whole  video can be effectively leveraged for object detection in  each frame, which helps alleviate the missing label problem
Extensive experiments on a large-scale daily-life action dataset Charades demonstrate the superiority of our  model over the state-of-the-arts
 Acknowledgements: This work was supported by ONR MURI  N000NNNNNN00N and Sloan Fellowship to AG
XL was supported by the  Department of Defense under Contract No
FANN0N-NN-D-000N with CMU  for the operation of the Software Engineering Institute, a federally funded  research and development center
 NN0N    References  [N] H
Bilen, M
Pedersoli, and T
Tuytelaars
Weakly supervised  detection with posterior regularization
In BMVC, N0NN
N  [N] H
Bilen and A
Vedaldi
Weakly supervised deep detection  networks
In CVPR, N0NN
N, N, N, N  [N] K
Chatfield, K
Simonyan, A
Vedaldi, and A
Zisserman
 Return of the devil in the details: Delving deep into convolutional nets
In BMVC, N0NN
N, N  [N] R
G
Cinbis, J
Verbeek, and C
Schmid
Weakly supervised  object localization with multi-fold multiple instance learning
TPAMI, N0NN
N  [N] R
Collobert, K
Kavukcuoglu, and C
Farabet
TorchN: A  Matlab-like environment for machine learning
In BigLearn,  NIPS Workshop, N0NN
N  [N] T
Deselaers, B
Alexe, and V
Ferrari
Localizing objects  while learning their appearance
In ECCV, N0N0
N  [N] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The PASCAL Visual Object Classes (VOC)  challenge
IJCV, NN(N):N0N–NNN, N0N0
N  [N] R
Girshick
Fast R-CNN
In ICCV, N0NN
N, N  [N] K
He, X
Zhang, S
Ren, and J
Sun
Spatial pyramid pooling in deep convolutional networks for visual recognition
 TPAMI, NN(N):NN0N–NNNN, N0NN
N, N  [N0] F
C
Heilbron, V
Escorcia, B
Ghanem, and J
C
Niebles
 ActivityNet: A large-scale video benchmark for human activity understanding
In CVPR, N0NN
N  [NN] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural Computation, N(N):NNNN–NNN0, NNNN
N, N  [NN] A
Jain, A
R
Zamir, S
Savarese, and A
Saxena
StructuralRNN: Deep learning on spatio-temporal graphs
In CVPR,  N0NN
N, N  [NN] Z
Jie, Y
Wei, X
Jin, J
Feng, and W
Liu
Deep self-taught  learning for weakly supervised object localization
In CVPR,  N0NN
N  [NN] A
Joulin, K
Tang, and L
Fei-Fei
Efficient image and video  co-localization with Frank-Wolfe algorithm
In ECCV, N0NN
 N  [NN] K
Kang, W
Ouyang, H
Li, and X
Wang
Object detection  from video tubelets with convolutional neural networks
In  CVPR, N0NN
N  [NN] V
Kantorov, M
Oquab, M
Cho, and I
Laptev
ContextLocNet: Context-aware deep network models for weakly supervised localization
In ECCV, N0NN
N, N, N, N  [NN] S
Kwak, M
Cho, I
Laptev, J
Ponce, and C
Schmid
Unsupervised object discovery and tracking in video collections
 In ICCV, N0NN
N, N  [NN] X
Liang, L
Lin, X
Shen, J
Feng, S
Yan, and E
Xing
 Interpretable structure-evolving LSTM
In CVPR, N0NN
N  [NN] X
Liang, S
Liu, Y
Wei, L
Liu, L
Lin, and S
Yan
Towards computational baby learning: A weakly-supervised  approach for object detection
In ICCV, N0NN
N  [N0] X
Liang, X
Shen, J
Feng, L
Lin, and S
Yan
Semantic  object parsing with graph LSTM
In ECCV, N0NN
N  [NN] X
Liang, X
Shen, D
Xiang, J
Feng, L
Lin, and S
Yan
 Semantic object parsing with local-global long short-term  memory
In CVPR, N0NN
N  [NN] T
Lin, M
Maire, S
J
Belongie, L
D
Bourdev, R
B
Girshick, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
 Zitnick
Microsoft COCO: Common objects in context
In  ECCV, N0NN
N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
E
Reed,  C
Fu, and A
C
Berg
SSD: Single shot multibox detector
In ECCV, N0NN
N  [NN] J
Y
Ng, M
J
Hausknecht, S
Vijayanarasimhan, O
Vinyals,  R
Monga, and G
Toderici
Beyond short snippets: Deep  networks for video classification
In CVPR, N0NN
N  [NN] A
Papazoglou and V
Ferrari
Fast object segmentation in  unconstrained video
In ICCV, N0NN
N  [NN] A
Prest, C
Leistner, J
Civera, C
Schmid, and V
Ferrari
Learning object class detectors from weakly annotated  video
In CVPR, N0NN
N  [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In  CVPR, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN: Towards real-time object detection with region proposal networks
In NIPS, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet large scale visual  recognition challenge
IJCV, N0NN
N  [N0] S
Schulter, C
Leistner, P
M
Roth, and H
Bischof
Unsupervised object discovery and segmentation in videos
In  BMVC, N0NN
N  [NN] X
Shi, Z
Chen, H
Wang, D
Yeung, W
Wong, and W
Woo
 Convolutional LSTM network: A machine learning approach for precipitation nowcasting
In NIPS, N0NN
N  [NN] G
A
Sigurdsson, G
Varol, X
Wang, A
Farhadi, I
Laptev,  and A
Gupta
Hollywood in homes: Crowdsourcing data  collection for activity understanding
In ECCV, N0NN
N, N, N  [NN] K
K
Singh, F
Xiao, and Y
J
Lee
Track and transfer:  Watching videos to simulate strong human supervision for  weakly-supervised object detection
In CVPR, N0NN
N  [NN] P
Siva, C
Russell, and T
Xiang
In defence of negative  mining for annotating weakly labelled data
In ECCV, N0NN
 N  [NN] H
O
Song, R
B
Girshick, S
Jegelka, J
Mairal, Z
Harchaoui, T
Darrell, et al
On learning to localize objects with  minimal supervision
In ICML, N0NN
N  [NN] K
Soomro, A
R
Zamir, and M
Shah
UCFN0N: A dataset  of N0N human action classes from videos in the wild
In  CRCV-TR-NN-0N, N0NN
N, N  [NN] N
Srivastava, E
Mansimov, and R
Salakhutdinov
Unsupervised learning of video representations using LSTMs
In  ICLR, N0NN
N  [NN] C
Sun, A
Shrivastava, S
Singh, and A
Gupta
Revisiting  unreasonable effectiveness of data in deep learning era
In  arXiv:NN0N.0NNNN, N0NN
N  [NN] K
S
Tai, R
Socher, and C
D
Manning
Improved semantic  representations from tree-structured long short-term memory  networks
arXiv preprint arXiv:NN0N.000NN, N0NN
N  [N0] J
Uijlings, K
van de Sande, T
Gevers, and A
Smeulders
 Selective search for object recognition
IJCV, N0NN
N  NN0N    [NN] C
Wang, W
Ren, K
Huang, and T
Tan
Weakly supervised  object localization with latent category learning
In ECCV,  N0NN
N  [NN] L
Wang, G
Hua, R
Sukthankar, J
Xue, Z
Niu, and  N
Zheng
Video object discovery and co-segmentation with  extremely weak supervision
TPAMI, N0NN
N  [NN] X
Wang and A
Gupta
Unsupervised learning of visual representations using videos
In ICCV, N0NN
N  [NN] S
Yeung, O
Russakovsky, G
Mori, and F
Li
End-to-end  learning of action detection from frame glimpses in videos
 In CVPR, N0NN
N  [NN] D
Zhang, J
Han, L
Jiang, S
Ye, and X
Chang
Revealing event saliency in unconstrained video collection
IEEE  Transactions on Image Processing, NN(N):NNNN–NNNN, N0NN
 N  [NN] D
Zhang, D
Meng, and J
Han
Co-saliency detection via  a self-paced multiple-instance learning framework
IEEE  transactions on pattern analysis and machine intelligence,  NN(N):NNN–NNN, N0NN
N  [NN] K
Zhang, W.-L
Chao, F
Sha, and K
Grauman
Video summarization with long short-term memory
In ECCV, N0NN
 N  [NN] C
L
Zitnick and P
Dollár
Edge Boxes: Locating object  proposals from edges
In ECCV, N0NN
N, N  NNN0Generating High-Quality Crowd Density Maps Using Contextual Pyramid CNNs   Generating High-Quality Crowd Density Maps using Contextual Pyramid CNNs  Vishwanath A
Sindagi and Vishal M
Patel  Rutgers University, Department of Electrical and Computer Engineering  NN Brett Road, Piscataway, NJ 0NNNN, USA  vishwanath.sindagi@rutgers.edu, vishal.m.patel@rutgers.edu  Abstract  We present a novel method called Contextual Pyramid  CNN (CP-CNN) for generating high-quality crowd density  and count estimation by explicitly incorporating global and  local contextual information of crowd images
The proposed CP-CNN consists of four modules: Global Context  Estimator (GCE), Local Context Estimator (LCE), Density  Map Estimator (DME) and a Fusion-CNN (F-CNN)
GCE  is a VGG-NN based CNN that encodes global context and  it is trained to classify input images into different density  classes, whereas LCE is another CNN that encodes local  context information and it is trained to perform patch-wise  classification of input images into different density classes
 DME is a multi-column architecture-based CNN that aims  to generate high-dimensional feature maps from the input  image which are fused with the contextual information estimated by GCE and LCE using F-CNN
To generate high  resolution and high-quality density maps, F-CNN uses a set  of convolutional and fractionally-strided convolutional layers and it is trained along with the DME in an end-to-end  fashion using a combination of adversarial loss and pixellevel Euclidean loss
Extensive experiments on highly challenging datasets show that the proposed method achieves  significant improvements over the state-of-the-art methods
 N
Introduction  With ubiquitous usage of surveillance cameras and advances in computer vision, crowd scene analysis [NN, NN]  has gained a lot of interest in the recent years
In this  paper, we focus on the task of estimating crowd count  and high-quality density maps which has wide applications in video surveillance [NN, NN], traffic monitoring, public safety, urban planning [NN], scene understanding and  flow monitoring
Also, the methods developed for crowd  counting can be extended to counting tasks in other fields  such as cell microscopy [NN, NN, NN, N], vehicle counting  [NN, NN, NN, NN, NN], environmental survey [N, NN], etc
The  task of crowd counting and density estimation has seen a  Figure N: Density estimation results
Top Left: Input image  (from the ShanghaiTech dataset [N0])
Top Right: Ground  truth
Bottom Left: Zhang et al
[N0] (PSNR: NN.N dB  SSIM: 0.NN)
Bottom Right: CP-CNN (PSNR: NN.N dB  SSIM: 0.NN)
 significant progress in the recent years
However, due to the  presence of various complexities such as occlusions, high  clutter, non-uniform distribution of people, non-uniform illumination, intra-scene and inter-scene variations in appearance, scale and perspective, the resulting accuracies are far  from optimal
 Recent CNN-based methods using different multi-scale  architectures [N0, NN, NN] have achieved significant success in addressing some of the above issues, especially in  the high-density complex crowded scenes
However, these  methods tend to under-estimate or over-estimate count in  the presence of high-density and low-density crowd images, respectively (as shown in Fig
N)
A potential solution is to use contextual information during the learning process
Several recent works for semantic segmentation [NN], scene parsing [NN] and visual saliency [NN] have  demonstrated that incorporating contextual information can  provide significant improvements in the results
Motivated  by their success, we believe that availability of global context shall aid the learning process and help us achieve better  NNNN    Figure N: Average estimation errors across various density levels
Current state-of-the-art method [N0] overestimates/underestimates count in the presence of lowdensity/high-density crowd
 count estimation
In addition, existing approaches employ  max-pooling layers to achieve minor translation invariance  resulting in low-resolution and hence low-quality density  maps
Also, to the best of our knowledge, most existing  methods concentrate only on the quality of count rather than  that of density map
Considering these observations, we  propose to incorporate global context into the learning process while improving the quality of density maps
 To incorporate global context, a CNN-based Global Context Estimator (GCE) is trained to encode the context of  an input image that is eventually used to aid the density  map estimation process
GCE is a CNN-based on VGGNN architecture
A Density Map Estimator (DME), which  is a multi-column architecture-based CNN with appropriate max-pooling layers, is used to transform the image into  high-dimensional feature maps
Furthermore, we believe  that use of local context in the image will guide the DME to  estimate better quality maps
To this effect, a Local Context  Estimator CNN (LCE) is trained on input image patches to  encode local context information
Finally, the contextual  information obtained by LCE and GCE is combined with  the output of DME using a Fusion-CNN (F-CNN)
Noting  that the use of max-pooling layers in DME results in lowresolution density maps, F-CNN is constructed using a set  of fractionally-strided convolutions [NN] to increase the output resolution, thereby generating high-quality maps
In a  further attempt to improve the quality of density maps, the  F-CNN is trained using a weighted combination of pixelwise Euclidean loss and adversarial loss [N0]
The use of  adversarial loss helps us combat the widely acknowledge  issue of blurred results obtained by minimizing only the Euclidean loss [NN]
 The proposed method uses CNN networks to estimate  context at various levels for achieving lower count error and  better quality density maps
It can be considered as a set of  CNNs to estimate pyramid of contexts, hence, the proposed  method is dubbed as Contextual Pyramid CNN (CP-CNN)
 To summarize, the following are our main contributions:  • We propose a novel Contextual Pyramid CNN (CP- CNN) for crowd count and density estimation that encodes local and global context into the density estimation process
 • To the best of our knowledge, ours is the first attempt to concentrate on generating high-quality density maps
 Also, in contrast to the existing methods, we evaluate the quality of density maps generated by the proposed method using different quality measures such as  PSNR/SSIM and report state-of-the-art results
 • We use adversarial loss in addition to Euclidean loss for the purpose of crowd density estimation
 • Extensive experiments are conducted on three highly challenging datasets ([N0, NN, NN]) and comparisons  are performed against several recent state-of-the-art  approaches
Further, an ablation study is conducted to  demonstrate the improvements obtained by including  contextual information and adversarial loss
 N
Related work  Various approaches have been proposed to tackle the  problem of crowd counting in images [NN, N, NN, NN, N0]  and videos [N, N, NN, N]
Initial research focussed on detection style [NN] and segmentation framework [NN]
These  methods were adversely affected by the presence of occlusions and high clutter in the background
Recent approaches  can be broadly categorized into regression-based, density  estimation-based and CNN-based methods
We briefly review various methods among these cateogries as follows:  Regression-based approaches
To overcome the issues  of occlusion and high background clutter, researchers attempted to count by regression where they learn a mapping  between features extracted from local image patches to their  counts [N, NN, N]
These methods have two major components: low-level feature extraction and regression modeling
 Using a similar approach, Idrees et al
[NN] fused count from  multiple sources such as head detections, texture elements  and frequency domain analysis
 Density estimation-based approaches
While regressionbased approaches were successful in addressing the issues  of occlusion and clutter, they ignored important spatial information as they were regressing on the global count
Lempitsky et al
[NN] introduced a new approach of learning  a linear mapping between local patch features and corresponding object density maps using regression
Observing  that it is difficult to learn a linear mapping, Pham et al
in  [NN] proposed to learn a non-linear mapping between local patch features and density maps using a random forest  framework
Many recent approaches have proposed methods based on density map regression [NN, NN, N0]
A more  comprehensive survey of different crowd counting methods  NNNN    Figure N: Overview of the proposed CP-CNN architecture
 The network incorporates global and local context using  GCE and LCE respectively
The context maps are concatenated with the output of DME and further processed by FCNN to estimate high-quality density maps
 can be found in [NN, N, NN, NN]
 CNN-based methods
Recent success of CNN-based methods in classification and recognition tasks has inspired researchers to employ them for the purpose of crowd counting and density estimation [NN, NN, NN, N0]
Walach et al
 [NN] used CNNs with layered training approach
In contrast to the existing patch-based estimation methods, Shang  et al
[N0] proposed an end-to-end estimation method using  CNNs by simultaneously learning local and global count  on the whole sized input images
Zhang et al
[N0] proposed a multi-column architecture to extract features at different scales
Similarly, Onoro-Rubio and López-Sastre in  [NN] addressed the scale issue by proposing a scale-aware  counting model called Hydra CNN to estimate the object  density maps
Boominathan et al
in [N] proposed to tackle  the issue of scale variation using a combination of shallow  and deep networks along with an extensive data augmentation by sampling patches from multi-scale image representations
Marsden et al
explored fully convolutional networks [NN] and multi-task learning [N0] for the purpose of  crowd counting
 Inspired by cascaded multi-task learning [NN, N], Sindagi  et al
[NN] proposed to learn a high-level prior and perform density estimation in a cascaded setting
In contrast to  [NN], the work in this paper is specifically aimed at reducing  overestimation/underestimation of count error by systemically leveraging context in the form of crowd density levels at various levels using different networks
Additionally,  we incorporate several elements such as local context and  adversarial loss aimed at improving the quality of density  maps
Most recently, Sam et al
[NN] proposed a SwitchingCNN network that intelligently chooses the most optimal  regressor among several independent regressors for a particular input patch
A comprehensive survey of recent cnnbased methods for crowd counting can be found in [NN]
 Recent works using multi-scale and multi-column architectures [N0, NN, NN] have demonstrated considerable success in  achieving lower count errors
We make the following observations regarding these recent state-of-the-art approaches:  N
These methods do not explicitly incorporate contextual information which is essential for achieving further improvements
N
Though existing approaches regress on density maps, they are more focussed on improving count errors rather than quality of the density maps, and N
Existing CNN-based approaches are trained using a pixel-wise  Euclidean loss which results in blurred density maps
In  view of these observations, we propose a novel method to  learn global and local contextual information from images  for achieving better count estimates and high-quality density maps
Furthermore, we train the CNNs in a Generative  Adversarial Network (GAN) based framework [N0] to exploit the recent success of adversarial loss to achieve highquality and sharper density maps
 N
Proposed method (CP-CNN)  The proposed CP-CNN method consists of a pyramid  of context estimators and a Fusion-CNN as illustrated in  Fig
N
It consists of four modules: GCE, LCE, DME,  and F-CNN
GCE and LCE are CNN-based networks that  encode global and local context present in the input image respectively
DME is a multi-column CNN that performs the initial task of transforming the input image to  high-dimensional feature maps
Finally, F-CNN combines  contextual information from GCE and LCE with highdimensional feature maps from DME to produce highresolution and high-quality density maps
These modules  are discussed in detail as follows
 N.N
Global Context Estimator (GCE)  As discussed in Section N, though recent state-of-the-art  multi-column or multi-scale methods [N0, NN, NN] achieve  significant improvements in the task of crowd count estimation, they either underestimate or overestimate counts in  high-density and low-density crowd images respectively (as  explained in Fig
N)
We believe it is important to explicilty  model context present in the image to reduce the estimation  error
To this end, we associate global context with the level  of density present in the image by considering the task of  learning global context as classifying the input image into  five different classes: extremely low-density (ex-lo), lowdensity (lo), medium-density (med), high-density (hi) and  extremely high-density (ex-hi)
Note that the number of  classes required is dependent on the crowd density variation in the dataset
A dataset containing large variations  may require higher number of classes
In our experiments,  we obtained significant improvements using five categories  of density levels
 In order to learn the classification task, a VGG-NN [NN]  based network is fine-tuned with the crowd training data
 Network used for GCE is as shown in Fig
N
The convolutional layers from the VGG-NN network are retained,  however, the last three fully connected layers are replaced  NNNN    with a different configuration of fully connected layers in  order to cater to our task of classification into five categories
Weights of the last two convolutional layers are finetuned while keeping the weights fixed for the earlier layers
 The use of pre-trained VGG network results in faster convergence as well as better performance in terms of context  estimation
 Figure N: Global context estimator based on VGG-NN architecture
The network is trained to classify the input images  into various density levels thereby encoding the global context present in the image
 N.N
Local Context Estimator (LCE)  Existing methods for crowd density estimation have primarily focussed on achieving lower count errors rather than  estimating better quality density maps
As a result, these  methods produce low-quality density maps as shown in Fig
 N
After an analysis of these results, we believe that some  kind of local contextual information can aid us to achieve  better quality maps
To this effect, similar to GCE, we propose to learn an image’s local context by learning to classify it’s local patches into one of the five classes: {ex-lo, lo, med, hi, ex-hi}
The local context is learned by the LCE whose architecture shown in Fig
N
It is composed of a  set of convolutional and max-pooling layers followed by N  fully connected layers with appropriate drop-out layers after the first two fully connected layers
Every convolutional  and fully connected layer is followed by a ReLU layer except for the last fully connected layer which is followed by  a sigmoid layer
 Figure N: Local context estimator: The network is trained  to classify local input patches into various density levels  thereby encoding the local context present in the image
 N.N
Density Map Estimator (DME)  The aim of DME is to transform the input image into a  set of high-dimensional feature maps which will be concatenated with the contextual information provided by GCE and  LCE
Estimating density maps from high-density crowd images is especially challenging due to the presence of heads  with varying sizes in and across images
Previous works  on multi-scale [NN] or multi-column [N0] architectures have  demonstrated abilities to handle the presence of considerably large variations in object sizes by achieving significant  improvements in such scenarios
Inspired by the success  of these methods, we use a multi-column architecture similar to [N0]
However, notable differences compared to their  work are that our columns are much deeper and have different number of filters and filter sizes that are optimized for  lower count estimation error
Also, in this work, the multicolumn architecture is used to transform the input into a  set of high-dimensional feature map rather than using them  directly to estimate the density map
Network details for  DME are illustrated in Fig
N
 It may be argued that since the DME has a pyramid of  filter sizes, one may be able to increase the filter sizes and  number of columns to address larger variation in scales
 However, note that addition of more columns and the filter sizes will have to be decided based on the scale variation  present in the dataset, resulting in new network designs that  cater to different datasets containing different scale variations
Additionally, deciding the filter sizes will require  time consuming experiments
With our network, the design  remains consistent across all datasets, as the context estimators can be considered to perform the task of coarse crowd  counting
 Figure N: Density Map Estimator: Inspired by Zhang et al
 [N0], DME is a multi-column architecture
In contrast to  [N0], we use slightly deeper columns with different number  of filters and filter sizes
 N.N
Fusion-CNN (F-CNN)  The contextual information from GCE and LCE are combined with the high-dimensional feature maps from DME  using F-CNN
The F-CNN automatically learns to incorporate the contextual information estimated by context estimators
The presence of max-pooling layers in the DME network (which are essential to achieve translation invariance)  results in down-sampled feature maps and loss of details
 NNNN    Since, the aim of this work is to estimate high-resolution  and high-quality density maps, F-CNN is constructed using  a set of convolutional and fractionally-strided convolutional  layers
The set of fractionally-strided convolutional layers  help us to restore details in the output density maps
The  following structure is used for F-CNN: CR(NN,N)-CR(NN,N)TR(NN)-CR(NN,N)-TR(NN)-C(N,N), where, C is convolutional layer, R is ReLU layer, T is fractionally-strided convolution layer and the first number inside every brace indicates the  number of filters while the second number indicates filter  size
Every fractionally-strided convolution layer increases  the input resolution by a factor of N, thereby ensuring that  the output resolution is the same as that of input
 Once the context estimators are trained, DME and FCNN are trained in an end-to-end fashion
Existing methods for crowd density estimation use Euclidean loss to train  their networks
It has been widely acknowledged that minimization of LN error results in blurred results especially for image reconstruction tasks [NN, NN, NN, NN, NN]
Motivated  by these observations and the recent success of GANs for  overcoming the issues of LN-minimization [NN], we attempt  to further improve the quality of density maps by minimizing a weighted combination of pixel-wise Euclidean loss  and adversarial loss
The loss for training F-CNN and DME  is defined as follows:  LT = LE + λaLA, (N)  LE = N  WH  W ∑  w=N  H ∑  h=N  ‖φ(Xw,h)− (Y w,h)‖N, (N)  LA = − log(φD(φ(X)), (N)  where, LT is the overall loss, LE is the pixel-wise Eu- clidean loss between estimated density map and it’s corresponding ground truth, λa is a weighting factor, LA is the adversarial loss, X is the input image of dimen- sions W × H , Y is the ground truth density map, φ is the network consisting of DME and F-CNN and φD is the discriminator sub-network for calculating the adversarial loss
Following structure is used for the discriminator sub-network: CP(NN)-CP(NNN)-M-CP(NNN)-MCP(NNN)-CP(NNN)-M-C(N)-Sigmoid, where C represents convolutional layer, P represents PReLU layer and M is max-pooling layer
 N
Training and evaluation details  In this section, we discuss details of the training and evaluation procedures
 Training details: Let D be the original training dataset
Patches N/Nth the size of original image are cropped from N00 random locations from every image in D
Other aug- mentation techniques like horizontal flipping and noise addition are used to create another N00 patches
The random  cropping and augmentation resulted in a total of N00 patches  per image in the training dataset
Let this set of images be  called as Ddme
Another training set Dlc is formed by crop- ping patches of size NN × NN from N00 random locations in every training image in D
 GCE is trained using the dataset Ddme
The correspond- ing ground truth categories for each image is determined  based on the number of people present in it
Note that the  images are resized to NNN × NNN before feeding them into the VGG-based GCE network
The network is then trained  using the standard cross-entropy loss
LCE is trained using  the NN × NN patches in Dlc
The ground truth categories of the training patches is determined based on the number of  people present in them
The network is then trained using  the standard cross-entropy loss
 Next, the DME and F-CNN networks are trained in  an end-to-end fashion using input training images from  Ddme and their corresponding global and local contexts N
 The global context (F igc) for an input training image X i  is obtained in the following way
First, an empty global  context F igc of dimension N × Wi/N × Hi/N is created, where Wi × Hi is the dimension of Xi
Next, a set of classification scores yi,jgc (j = N...N) is obtained by feeding  Xi to GCE
Each feature map in global context F i,j gc is  then filled with the corresponding classification score  yi,jg 
The local context (F i lc) for X  i is obtained in the  following way
An empty local context F ilc of dimension N × Wi × Hi is first created
A sliding window classifier (LCE) of size NN × NN is run on Xi to obtain the classi- fication score yi,j,wlc (j = N...N) where w is the window  location
The classification scores yi,j,wlc are used to fill the corresponding window location w in the respective local context map F i,jgc 
F  i,j gc is then resized to a size of  Wi/N × Hi/N
After the context maps are estimated, Xi is fed to DME to obtain a high-dimensional feature map  F idme which is concatenated with F i gc and F  i lc
These  concatenated feature maps are then fed into F-CNN
The  two CNNs (DME and F-CNN) are trained in an end-toend fashion by minimizing the weighted combination of  pixel-wise Euclidean loss and adversarial loss (given by  (N)) between the estimated and ground truth density maps
 Inference details: Here, we describe the process to estimate the density map of a test image Xti 
First, the global context map F itgc for X  t i is calculated in the following way
 The test image Xti is divided into non-overlapping blocks of size W ti /N × H  t i /N
All blocks are then fed into GCE  to obtain their respective classification scores
As in training, the classification scores are used to build the context  maps for each block to obtain the final global context feature map F itgc
Next, the local context map F i tlc for X  t i is  NOnce GCE and LCE are trained, their weights are frozen
 NNNN    calculated in the following way: A sliding window classifier  (LCE) of size NN × NN is run across Xti and the classifica- tion scores from every window are used to build the local  context F itlc
Once the context information is obtained, X t i  is fed into DME to obtain high-dimensional feature maps  F itdme
F i tdme is concatenated with F  i tgc and F  i tlc and fed  into F-CNN to obtain the output density map
Note that due  to additional context processing, inference using the proposed method is computationally expensive as compared to  earlier methods such as [N0, NN]
 N
Experimental results  In this section, we present the experimental details and  evaluation results on three publicly available datasets
First,  the results of an ablation study conducted to demonstrate  the effects of each module in the architecture is discussed
 Along with the ablation study, we also perform a detailed  comparison of the proposed method against a recent stateof-the-art-method [N0]
This detailed analysis contains  comparison of count metrics defined by (N), along with  qualitative and quantitative comparison of the estimated  density maps
The quality of density maps is measured using two standard metrics: PSNR (Peak Signal-to-Noise Ratio) and SSIM (Structural Similarity in Image [NN])
The  count error is measured using Mean Absolute Error (MAE)  and Mean Squared Error (MSE):  MAE = N  N  N ∑  i=N  |yi − y ′  i|,MSE =  √  √  √  √  N  N  N ∑  i=N  |yi − y′i| N,  (N)  where N is number of test samples, yi is the ground truth count and y′i is the estimated count corresponding to the i  th  sample
The ablation study is followed by a discussion and  comparison of proposed method’s results against several  recent state-of-the-art methods on three datasets: ShanghaiTech [N0], WorldExpo ’N0 [NN] and UCF CROWD N0  [NN]
 N.N
Ablation study using ShanghaiTech Part A  In this section, we perform an ablation study to demonstrate the effects of different modules in the proposed  method
Each module is added sequentially to the network  and results for each configuration are compared
Following four configurations are evaluated: (N) DME: The highdimensional feature maps of DME are combined using N×N conv layer whose output is used to estimate the density map
 LE loss is minimized to train the network
(N) DME with only GCE and F-CNN: The output of DME is concatenated  with the global context
DME and F-CNN are trained to estimate the density maps by minimizing LE loss
(N) DME with GCE, LCE and F-CNN
In addition to the third configuration, local context is also used in this case and the  Count  estimation  error  Density map  quality  Method MAE MSE PSNR SSIM  Zhang et al.[N0] NN0.N NNN.N N0.NN 0.NN  DME N0N.N NNN.N N0.NN 0.NN  DME+GCE+FCNN NN.N NNN.N N0.NN 0.NN  DME + GCE +  LCE + FCNN NN.N NN0.N NN.N 0.NN  DME+GCE+LCE+  FCNN with LA+LE NN.N N0N.N NN.NN 0.NN  Table N: Estimation errors for different configurations of  the proposed network on ShanghaiTech Part A[N0]
Addition of contextual information and the use of adversarial  loss progressively improves the count error and the quality  of density maps
 network is trained using LE loss
(N) DME with GCE, LCE and F-CNN with LA + LE (entire network)
These results are compared with a fifth configuration: Zhang et al
[N0]  (which is a recent state-of-the-art method) in order to gain a  perspective of the improvements achieved by the proposed  method and its various modules
 The evaluation is performed on Part A of ShanghaiTech  [N0] dataset which contains NNNN annotated images with a  total of NN0,NNN people
This dataset consists of two parts:  Part A with NNN images and Part B with NNN images
Both  parts are further divided into training and test datasets with  training set of Part A containing N00 images and that of Part  B containing N00 images
Rest of the images are used as test  set
Due to the presence of large variations in density, scale  and appearance of people across images in the Part A of this  dataset, estimating the count with high degree of accuracy  is difficult
Hence, this dataset was chosen for the detailed  analysis of performance of the proposed architecture
 Count estimation errors and quality metrics of the estimated density images for the various configurations are  tabulated in Table N
We make the following observations:  (N) The network architecture for DME used in this work is  different from Zhang et al
[N0] in terms of column depths,  number of filters and filter sizes
These changes improve  the count estimation error as compared to [N0]
However,  no significant improvements are observed in the quality of  density maps
(N) The use of global context in (DME + GCE  + F-CNN) greatly reduces the count error from the previous  configurations
Also, the use of F-CNN (which is composed  of fractionally-strided convolutional layers), results in considerable improvement in the quality of density maps
(N)  The addition of local context and the use of adversarial loss  progressively reduces the count error while achieving better  quality in terms of PSNR and SSIM
 Estimated density maps from various configurations on  sample input images are shown in Fig
N
It can be observed  that the density maps generated using Zhang et al
[N0] and  NNNN    Figure N: Comparison of results from different configurations of the proposed network along with Zhang et al
[N0]
Top  Row: Sample input images from the ShanghaiTech dataset
Second Row: Ground truth
Third Row: Zhang et al
[N0]
(Loss  of details can be observed)
Fourth Row: DME
Fifth Row: DME + GCE + F-CNN
Sixth Row:DME + GCE + LCE +  F-CNN
Bottom Row: DME + GCE + LCE + F-CNN with adversarial loss
Count estimates and the quality of density maps  improve after inclusion of contextual information and adversarial loss
 DME (which regress on low-resolution maps) suffer from  loss of details
The use of global context information and  fractionally-strided convolutional layers results in better estimation quality
Additionally, the use of local context and  minimization over a weighted combination of LA and LE further improves the quality and reduces the estimation error
 N.N
Evaluations and comparisons  In this section, the results of the proposed method are  compared against recent state-of-the-art methods on three  challenging datasets
 ShanghaiTech
The proposed method is evaluated against  four recent approaches: Zhang et al
[NN], MCNN [N0],  Cascaded-MTL [NN] and Switching-CNN [NN] on Part A  and Part B of the ShanghaiTech dataset are shown in Table  N
The authors in [NN] proposed a switchable learning function where they learned their network by alternatively training on two objective functions: crowd count and density estimation
They made use of perspective maps for appropriate ground truth density maps
In another approach, Zhang  et al
[N0] proposed a multi-column convolutional network  (MCNN) to address scale issues and a sophisticated ground  truth density map generation technique
Instead of using  the responses of all the columns, Sam et al
[NN] proposed  a switching-CNN classifier that chooses the optimal regressor
Sindagi et al
[NN] incorporate high-level prior in the  form of crowd density levels and perform a cascaded multiNNNN    task learning of estimating prior and density map
It can be  observed from Table N, that the proposed method is able to  achieve superior results as compared to the other methods,  which highlights the importance of contextual processing in  our framework
 Part A Part B  Method MAE MSE MAE MSE  Zhang et al
[NN] NNN.N NNN.N NN.0 NN.N  MCNN [N0] NN0.N NNN.N NN.N NN.N  Cascaded-MTL [NN] N0N.N NNN.N N0.0 NN.N  Switching-CNN [NN] N0.N NNN.0 NN.N NN.N  CP-CNN (ours) NN.N N0N.N N0.N N0.N  Table N: Estimation errors on the ShanghaiTech dataset
 WorldExpo’N0
The WorldExpo’N0 dataset was introduced  by Zhang et al
[NN] and it contains N,NN0 annotated frames  from N,NNN video sequences captured by N0N surveillance  cameras
The frames are divided into training and test sets
 The training set contains N,NN0 frames and the test set contains N00 frames from five different scenes with NN0 frames  per scene
They also provided Region of Interest (ROI) map  for each of the five scenes
For a fair comparison, perspective maps were used to generate the ground truth maps similar to the work of [NN]
Also, similar to [NN], ROI maps  are considered for post processing the output density map  generated by the network
 The proposed method is evaluated against five recent  state-of-the-art approaches: Chen et al
[N], Zhang et al
 [NN], MCNN [N0], Shang et al
[N0] and Switching-CNN  [NN] is presented in Table N
The authors in [N] introduced cumulative attributive concept for learning a regression model for crowd density and age estimation
Shang et  al
[N0] proposed an end-to-end CNN architecture consisting of three parts: pre-trained GoogLeNet model for feature  generation, long short term memory (LSTM) decoders for  local count and fully connected layers for the final count
It  can be observed from Table N that the proposed method outperforms existing approaches on an average while achieving  comparable performance in individual scene estimations
 Method SceneN SceneN SceneN SceneN SceneN Avgerage  Chen et al
[N] N.N NN.N N.N NN.N N.N NN.N  Zhang et al
[NN] N.N NN.N NN.N NN.N N.N NN.N  MCNN [N0] N.N N0.N NN.N NN.0 N.N NN.N  Shang et al
[N0] N.N NN.N NN.N NN.N N.N NN.N  Switching-CNN [NN] N.N NN.N N0.0 NN.0 N.N N.N  CP-CNN (ours) N.N NN.N N0.N N0.N N.N N.NN  Table N: Average estimation errors on the WorldExpo’N0  dataset
 UCF CC N0
The UCF CC N0 is an extremely challenging dataset introduced by Idrees et al
[NN]
The dataset  contains N0 annotated images of different resolutions and  aspect ratios crawled from the internet
There is a large  variation in densities across images
Following the standard  protocol discussed in [NN], a N-fold cross-validation was  performed for evaluating the proposed method
Results are  compared with seven recent approaches: Idrees et al
[NN],  Zhang et al
[NN], MCNN [N0], Onoro et al
[NN], Walach  et al
[NN], Cascaded-MTL [NN] and Switching-CNN [NN]
 The authors in [NN] proposed to combine information from  multiple sources such as head detections, Fourier analysis  and texture features (SIFT)
Onoro et al
in [NN] proposed  a scale-aware CNN to learn a multi-scale non-linear regression model using a pyramid of image patches extracted at  multiple scales
Walach et al
[NN] proposed a layered approach of learning CNNs for crowd counting by iteratively  adding CNNs where every new CNN is trained on residual  error of the previous layer
It can be observed from Table N  that our network achieves the lowest MAE and MSE count  errors
This experiment clearly shows the significance of  using context especially in images with widely varying densities
 Method MAE MSE  Idrees et al
[NN] NNN.N NNN.N  Zhang et al
[NN] NNN.0 NNN.N  MCNN [N0] NNN.N N0N.N  Onoro et al
[NN] Hydra-Ns NNN.N NNN.N  Onoro et al
[NN] Hydra-Ns NNN.N NNN.N  Walach et al
[NN] NNN.N NNN.N  Cascaded-MTL [NN] NNN.N.N NNN.N  Switching-CNN [NN] NNN.N NNN.N  CP-CNN (ours) NNN.N NN0.N  Table N: Estimation errors on the UCF CC N0 dataset
 N
Conclusion  We presented contextual pyramid of CNNs for incorporating global and local contextual information in an image to generate high-quality crowd density maps and lower  count estimation errors
The global and local contexts are  obtained by learning to classify the input images and its  patches into various density levels
This context information is then fused with the output of a multi-column DME  by a Fusion-CNN
In contrast to the existing methods, this  work focuses on generating better quality density maps in  addition to achieving lower count errors
In this attempt, the  Fusion-CNN is constructed with fractionally-strided convolutional layers and it is trained along with the DME in  an end-to-end fashion by optimizing a weighted combination of adversarial loss and pixel-wise Euclidean loss
Extensive experiments performed on challenging datasets and  comparison with recent state-of-the-art approaches demonstrated the significant improvements achieved by the proposed method
 Acknowledgement  This work was supported by US Office of Naval Research (ONR) Grant YIP N000NN-NN-N-NNNN
 NNNN    References  [N] L
Boominathan, S
S
Kruthiventi, and R
V
Babu
Crowdnet: A deep convolutional network for dense crowd counting
 In Proceedings of the N0NN ACM on Multimedia Conference,  pages NN0–NNN
ACM, N0NN
N  [N] G
J
Brostow and R
Cipolla
Unsupervised bayesian detection of independent motion in crowds
In N00N IEEE Computer Society Conference on Computer Vision and Pattern  Recognition (CVPR’0N), volume N, pages NNN–N0N
IEEE,  N00N
N  [N] A
B
Chan and N
Vasconcelos
Bayesian poisson regression  for crowd counting
In N00N IEEE NNth International Conference on Computer Vision, pages NNN–NNN
IEEE, N00N
N  [N] J.-C
Chen, A
Kumar, R
Ranjan, V
M
Patel, A
Alavi, and  R
Chellappa
A cascaded convolutional neural network for  age estimation of unconstrained faces
In International Conference on BTAS, pages N–N
IEEE, N0NN
N  [N] K
Chen, S
Gong, T
Xiang, and C
Change Loy
Cumulative attribute space for age and crowd density estimation
In  Proceedings of the IEEE conference on computer vision and  pattern recognition, pages NNNN–NNNN, N0NN
N, N  [N] K
Chen, C
C
Loy, S
Gong, and T
Xiang
Feature mining  for localised crowd counting
In European Conference on  Computer Vision, N0NN
N, N, N  [N] S
Chen, A
Fern, and S
Todorovic
Person count localization in videos from noisy foreground and detections
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNNN–NNNN, N0NN
N  [N] G
French, M
Fisher, M
Mackiewicz, and C
Needle
Convolutional neural networks for counting fish in fisheries  surveillance video
In British Machine Vision Conference  Workshop
BMVA Press, N0NN
N  [N] W
Ge and R
T
Collins
Marked point processes for crowd  counting
In Computer Vision and Pattern Recognition,  N00N
CVPR N00N
IEEE Conference on, pages NNNN–NNN0
 IEEE, N00N
N  [N0] I
Goodfellow, J
Pouget-Abadie, M
Mirza, B
Xu,  D
Warde-Farley, S
Ozair, A
Courville, and Y
Bengio
Generative adversarial nets
In Advances in Neural Information  Processing Systems, pages NNNN–NNN0, N0NN
N, N  [NN] M.-R
Hsieh, Y.-L
Lin, and W
H
Hsu
Drone-based object  counting by spatially regularized regional proposal networks
 In The IEEE International Conference on Computer Vision  (ICCV), N0NN
N  [NN] H
Idrees, I
Saleemi, C
Seibert, and M
Shah
Multi-source  multi-scale counting in extremely dense crowd images
In  Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N, N  [NN] P
Isola, J.-Y
Zhu, T
Zhou, and A
A
Efros
Image-to-image  translation with conditional adversarial networks
In IEEE  International Conference on Computer Vision and Pattern  Recognition
IEEE, N0NN
N, N  [NN] J
Johnson, A
Alahi, and L
Fei-Fei
Perceptual losses for  real-time style transfer and super-resolution
In European  Conference on Computer Vision, pages NNN–NNN
Springer,  N0NN
N  [NN] D
Kang, Z
Ma, and A
B
Chan
Beyond counting: Comparisons of density maps for crowd analysis tasks-counting,  detection, and tracking
arXiv preprint arXiv:NN0N.N0NNN,  N0NN
N  [NN] V
Lempitsky and A
Zisserman
Learning to count objects  in images
In Advances in Neural Information Processing  Systems, pages NNNN–NNNN, N0N0
N, N  [NN] M
Li, Z
Zhang, K
Huang, and T
Tan
Estimating the number of people in crowded scenes by mid based foreground  segmentation and head-shoulder detection
In Pattern Recognition, N00N
ICPR N00N
NNth International Conference on,  pages N–N
IEEE, N00N
N  [NN] T
Li, H
Chang, M
Wang, B
Ni, R
Hong, and S
Yan
 Crowded scene analysis: A survey
IEEE Transactions on  Circuits and Systems for Video Technology, NN(N):NNN–NNN,  N0NN
N, N  [NN] M
Marsden, K
McGuiness, S
Little, and N
E
O’Connor
 Fully convolutional crowd counting on highly congested  scenes
arXiv preprint arXiv:NNNN.00NN0, N0NN
N  [N0] M
Marsden, K
McGuinness, S
Little, and N
E
O’Connor
 Resnetcrowd: A residual deep learning architecture for  crowd counting, violent behaviour detection and crowd density level classification
arXiv preprint arXiv:NN0N.N0NNN,  N0NN
N  [NN] R
Mottaghi, X
Chen, X
Liu, N.-G
Cho, S.-W
Lee, S
Fidler, R
Urtasun, and A
Yuille
The role of context for object  detection and semantic segmentation in the wild
In Proceedings of the IEEE Conference on Computer Vision and Pattern  Recognition, pages NNN–NNN, N0NN
N  [NN] H
Noh, S
Hong, and B
Han
Learning deconvolution network for semantic segmentation
In Proceedings of the IEEE  International Conference on Computer Vision, pages NNN0–  NNNN, N0NN
N  [NN] D
Onoro-Rubio and R
J
López-Sastre
Towards  perspective-free object counting with deep learning
In European Conference on Computer Vision, pages NNN–NNN
 Springer, N0NN
N, N, N, N  [NN] V.-Q
Pham, T
Kozakaya, O
Yamaguchi, and R
Okada
 Count forest: Co-voting uncertain number of targets using  random forest for crowd density estimation
In Proceedings  of the IEEE International Conference on Computer Vision,  pages NNNN–NNNN, N0NN
N  [NN] R
Ranjan, V
Patel, and R
Chellappa
Hyperface: A deep  multi-task learning framework for face detection, landmark  localization, pose estimation, and gender recognition
IEEE  transactions on PAMI, N0NN
N  [NN] M
Rodriguez, I
Laptev, J
Sivic, and J.-Y
Audibert
 Density-aware person detection and tracking in crowds
In  N0NN International Conference on Computer Vision, pages  NNNN–NNN0
IEEE, N0NN
N  [NN] D
Ryan, S
Denman, C
Fookes, and S
Sridharan
Crowd  counting using multiple local features
In Digital Image  Computing: Techniques and Applications, N00N
DICTA’0N.,  pages NN–NN
IEEE, N00N
N  [NN] S
A
M
Saleh, S
A
Suandi, and H
Ibrahim
Recent survey  on crowd density estimation and counting for visual surveillance
Engineering Applications of Artificial Intelligence,  NN:N0N–NNN, N0NN
N  NNNN    [NN] D
B
Sam, S
Surya, and R
V
Babu
Switching convolutional neural network for crowd counting
In Proceedings  of the IEEE Conference on Computer Vision and Pattern  Recognition, N0NN
N, N, N, N, N  [N0] C
Shang, H
Ai, and B
Bai
End-to-end crowd counting  via joint learning local and global count
In Image Processing (ICIP), N0NN IEEE International Conference on, pages  NNNN–NNNN
IEEE, N0NN
N, N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In International  Conference on Learning Representations, N0NN
N  [NN] V
A
Sindagi and V
M
Patel
Cnn-based cascaded multitask learning of high-level prior and density estimation for  crowd counting
In Advanced Video and Signal Based  Surveillance (AVSS), N0NN IEEE International Conference  on
IEEE, N0NN
N, N, N  [NN] V
A
Sindagi and V
M
Patel
A survey of recent advances  in cnn-based single image crowd counting and density estimation
Pattern Recognition Letters, N0NN
N  [NN] E
Toropov, L
Gui, S
Zhang, S
Kottur, and J
M
Moura
 Traffic flow from a low frame rate city camera
In Image  Processing (ICIP), N0NN IEEE International Conference on,  pages NN0N–NN0N
IEEE, N0NN
N  [NN] P
Tu, T
Sebastian, G
Doretto, N
Krahnstoever, J
Rittscher,  and T
Yu
Unified crowd segmentation
In European Conference on Computer Vision, pages NNN–N0N
Springer, N00N
 N  [NN] E
Walach and L
Wolf
Learning to count with cnn boosting
 In European Conference on Computer Vision, pages NN0–  NNN
Springer, N0NN
N, N, N  [NN] C
Wang, H
Zhang, L
Yang, S
Liu, and X
Cao
Deep  people counting in extremely dense crowds
In Proceedings  of the NNrd ACM international conference on Multimedia,  pages NNNN–NN0N
ACM, N0NN
N  [NN] Y
Wang and Y
Zou
Fast visual object counting via  example-based density estimation
In Image Processing  (ICIP), N0NN IEEE International Conference on, pages  NNNN–NNNN
IEEE, N0NN
N, N  [NN] Z
Wang, A
C
Bovik, H
R
Sheikh, and E
P
Simoncelli
 Image quality assessment: from error visibility to structural  similarity
IEEE TIP, NN(N):N00–NNN, N00N
N  [N0] F
Xia and S
Zhang
Block-coordinate frank-wolfe optimization for counting objects in images
N  [NN] F
Xiong, X
Shi, and D.-Y
Yeung
Spatiotemporal modeling  for crowd counting in videos
In IEEE International Conference on Computer Vision
IEEE, N0NN
N  [NN] B
Xu and G
Qiu
Crowd density estimation based on rich  features and random projection forest
In N0NN IEEE Winter Conference on Applications of Computer Vision (WACV),  pages N–N
IEEE, N0NN
N  [NN] B
Zhan, D
N
Monekosso, P
Remagnino, S
A
Velastin,  and L.-Q
Xu
Crowd analysis: a survey
Machine Vision  and Applications, NN(N-N):NNN–NNN, N00N
N  [NN] C
Zhang, H
Li, X
Wang, and X
Yang
Cross-scene crowd  counting via deep convolutional neural networks
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNN–NNN, N0NN
N, N, N, N, N  [NN] H
Zhang and K
Dana
Multi-style generative network for  real-time transfer
arXiv preprint, N0NN
N  [NN] H
Zhang, V
A
Sindagi, and V
M
Patel
Image de-raining  using a conditional generative adversarial network
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] H
Zhang, V
A
Sindagi, and V
M
Patel
Joint transmission  map estimation and dehazing using deep networks
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] S
Zhang, G
Wu, J
P
Costeira, and J
M
Moura
Understanding traffic density from large-scale web camera data
 In IEEE Computer Vision and Pattern Recognition
IEEE,  N0NN
N  [NN] S
Zhang, G
Wu, J
P
Costeira, and J
M
F
Moura
Fcnrlstm: Deep spatio-temporal neural networks for vehicle  counting in city cameras
In IEEE International Conference  on Computer Vision
IEEE, N0NN
N  [N0] Y
Zhang, D
Zhou, S
Chen, S
Gao, and Y
Ma
Singleimage crowd counting via multi-column convolutional neural network
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages NNN–NNN, N0NN
 N, N, N, N, N, N, N  [NN] H
Zhao, J
Shi, X
Qi, X
Wang, and J
Jia
Pyramid scene  parsing network
In Proceedings of the IEEE Conference on  Computer Vision and Pattern Recognition, N0NN
N  [NN] R
Zhao, W
Ouyang, H
Li, and X
Wang
Saliency detection by multi-context deep learning
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N  NNN0Phrase Localization and Visual Relationship Detection With Comprehensive Image-Language Cues   Phrase Localization and Visual Relationship Detection with Comprehensive  Image-Language Cues  Bryan A
Plummer Arun Mallya Christopher M
Cervantes Julia Hockenmaier  Svetlana Lazebnik  University of Illinois at Urbana-Champaign  {bplummeN, amallyaN, ccervanN, juliahmr, slazebni}@illinois.edu  Abstract  This paper presents a framework for localization or  grounding of phrases in images using a large collection  of linguistic and visual cues
We model the appearance,  size, and position of entity bounding boxes, adjectives that  contain attribute information, and spatial relationships between pairs of entities connected by verbs or prepositions
 Special attention is given to relationships between people  and clothing or body part mentions, as they are useful for  distinguishing individuals
We automatically learn weights  for combining these cues and at test time, perform joint inference over all phrases in a caption
The resulting system  produces state of the art performance on phrase localization on the FlickrN0k Entities dataset [NN] and visual relationship detection on the Stanford VRD dataset [NN].N  N
Introduction  Today’s deep features can give reliable signals about a  broad range of content in natural images, leading to advances in image-language tasks such as automatic captioning [N, NN, NN, NN, NN] and visual question answering [N, N, NN]
A basic building block for such tasks is localization or grounding of individual phrases [N, NN, NN, NN,  NN, N0, NN]
A number of datasets with phrase grounding  information have been released, including FlickrN0k Entities [NN], ReferIt [NN], Google Referring Expressions [NN],  and Visual Genome [NN]
However, grounding remains  challenging due to open-ended vocabularies, highly unbalanced training data, prevalence of hard-to-localize entities  like clothing and body parts, as well as the subtlety and variety of linguistic cues that can be used for localization
 The goal of this paper is to accurately localize a bounding box for each entity (noun phrase) mentioned in a caption  for a particular test image
We propose a joint localization  objective for this task using a learned combination of singlephrase and phrase-pair cues
Evaluation is performed on the  NCode: https://github.com/BryanPlummer/pl-clc  A man carries a baby under a red  and blue umbrella next to a woman   in a red jacket!  Input Sentence and Image! Cues! Examples!  N)! Entities! man, baby, umbrella,   woman, jacket !  N)! Candidate Box Position ! ——!  N)! Candidate Box Size ! ——!  N)! Common Object ! Detectors !  man →! baby →!  woman →!   person !  person !  person!  N)! Adjectives! umbrella →! umbrella →!  jacket →!   red !  blue!  red !  N)! Subject - Verb! (man, carries)!  N)! Verb – Object !  (carries, baby)!  N)! Verbs! (man, carries, baby)!  N)! Prepositions! (baby, under, umbrella)! (man, next to, woman)!  N0)!Clothing & Body Parts ! (woman, in, jacket)!  Figure N: Left: an image and caption, together with ground truth bounding  boxes of entities (noun phrases)
Right: a list of all the cues used by our  system, with corresponding phrases from the sentence
 challenging recent FlickrN0K Entities dataset [NN], which  provides ground truth bounding boxes for each entity in the  five captions of the original FlickrN0K dataset [NN]
 Figure N introduces the components of our system using  an example image and caption
Given a noun phrase extracted from the caption, e.g., red and blue umbrella, we obtain single-phrase cue scores for each candidate box based  on appearance (modeled with a phrase-region embedding as  well as object detectors for common classes), size, position,  and attributes (adjectives)
If a pair of entities is connected  by a verb (man carries a baby) or a preposition (woman  in a red jacket), we also score the pair of corresponding  candidate boxes using a spatial model
In addition, actions  may modify the appearance of either the subject or the object (e.g., a man carrying a baby has a characteristic appearance, as does a baby being carried)
To account for this, we  learn subject-verb and verb-object appearance models for  the constituent entities
We give special treatment to relationships between people, clothing, and body parts, as these  are commonly used for describing individuals, and are also  among the hardest entities for existing approaches to localize
To extract as complete a set of relationships as possible,  we use natural language processing (NLP) tools to resolve  pronoun references within a sentence: e.g., by analyzing the  NNNN  https://github.com/BryanPlummer/pl-clc   Method  Single Phrase Cues Phrase-Pair Spatial Cues Inference  Phrase-Region Candidate Candidate Object Adjectives Verbs  Relative Clothing & Joint  Compatibility Position Size Detectors Position Body Parts Localization  Ours X X X X* X X X X X  (a) NonlinearSP [N0] X – – – – – – – –  GroundeR [NN] X – – – – – – – –  MCB [N] X – – – – – – – –  SCRC [NN] X X – – – – – – –  SMPL [NN] X – – – – – X* – X  RtP [NN] X – X X* X* – – – –  (b) Scene Graph [NN] – – – X X – X – X  ReferIt [NN] – X X X X* – X – –  Google RefExp [NN] X X X – – – – – –  Table N: Comparison of cues for phrase-to-region grounding
(a) Models applied to phrase localization on FlickrN0K Entities
(b) Models on related tasks
 * indicates that the cue is used in a limited fashion, i.e
[NN, NN] restricted their adjective cues to colors, [NN] only modeled possessive pronoun phrase-pair  spatial cues ignoring verb and prepositional phrases, [NN] and we limit the object detectors to N0 common categories
 sentence A man puts his hand around a woman, we can determine that the hand belongs to the man and introduce the  respective pairwise term into our objective
 Table N compares the cues used in our work to those in  other recent papers on phrase localization and related tasks  like image retrieval and referring expression understanding
 To date, other methods applied to the FlickrN0K Entities  dataset [N, NN, NN, N0, NN] have used a limited set of singlephrase cues
Information from the rest of the caption, like  verbs and prepositions indicating spatial relationships, has  been ignored
One exception is Wang et al
[NN], who tried  to relate multiple phrases to each other, but limited their relationships only to those indicated by possessive pronouns,  not personal ones
By contrast, we use pronoun cues to the  full extent by performing pronominal coreference
Also,  ours is the only work in this area incorporating the visual  aspect of verbs
Our formulation is most similar to that  of [NN], but with a larger set of cues, learned combination  weights, and a global optimization method for simultaneously localizing all the phrases in a sentence
 In addition to our experiments on phrase localization, we  also adapt our method to the recently introduced task of  visual relationship detection (VRD) on the Stanford VRD  dataset [NN]
Given a test image, the goal of VRD is to detect all entities and relationships present and output them in  the form (subject, predicate, object) with the corresponding bounding boxes
By contrast with phrase localization,  where we are given a set of entities and relationships that  are in the image, in VRD we do not know a priori which  objects or relationships might be present
On this task, our  model shows significant performance gains over prior work,  with especially acute differences in zero-shot detection due  to modeling cues with a vision-language embedding
This  adaptability to never-before-seen examples is also a notable  distinction between our approach and prior methods on related tasks (e.g
[N, NN, NN, N0]), which typically train their  models on a set of predefined object categories, providing  no support for out-of-vocabulary entities
 Section N discusses our global objective function for simultaneously localizing all phrases from the sentence and  describes the procedure for learning combination weights
 Section N.N details how we parse sentences to extract entities, relationships, and other relevant linguistic cues
Sections N.N and N.N define single-phrase and phrase-pair cost  functions between linguistic and visual cues
Section N  presents an in-depth evaluation of our cues on FlickrN0K  Entities [NN]
Lastly, Section N presents the adaptation of  our method to the VRD task [NN]
 N
Phrase localization approach  We follow the task definition used in [N, NN, NN, NN, N0,  NN]: At test time, we are given an image and a caption with  a set of entities (noun phrases), and we need to localize each  entity with a bounding box
Section N.N describes our inference formulation, and Section N.N describes our procedure  for learning the weights of different cues
 N.N
Joint phrase localization  For each image-language cue derived from a single  phrase or a pair of phrases (Figure N), we define a cuespecific cost function that measures its compatibility with an  image region (small values indicate high compatibility)
We  will describe the cost functions in detail in Section N; here,  we give our test-time optimization framework for jointly localizing all phrases from a sentence
 Given a single phrase p from a test sentence, we score each region (bounding box) proposal b from the test image based on a linear combination of cue-specific cost functions  φ{N,··· ,KS}(p, b) with learned weights w S :  S(p,b;wS)=  KS ∑  s=N  ✶s(p)φs(p,b)w S s , (N)  where ✶s(p) is an indicator function for the availability of cue s for phrase p (e.g., an adjective cue would be avail- able for the phrase blue socks, but would be unavailable for  NNNN    socks by itself)
As will be described in Section N.N, we use  NN single-phrase cost functions: region-phrase compatibility score, phrase position, phrase size (one for each of the  eight phrase types of [NN]), object detector score, adjective,  subject-verb, and verb-object scores
 For a pair of phrases with some relationship r = (p, rel, p′) and candidate regions b and b′, an analogous scoring function is given by a weighted combination of pairwise costs ψ{N,··· ,KQ}(r, b, b ′):  Q(r,b,b′;wQ)=  KQ ∑  q=N  ✶q(r)ψq(r,b,b ′)wQq 
(N)  We use three pairwise cost functions corresponding to spatial classifiers for verb, preposition, and clothing and body  parts relationships (Section N.N)
 We train all cue-specific cost functions on the training set  and the combination weights on the validation set
At test  time, given an image and a list of phrases {pN, · · · , pN}, we first retrieve top M candidate boxes for each phrase pi using Eq
(N)
Our goal is then to select one bounding box  bi out of the M candidates per each phrase pi such that the following objective is minimized:  min bN,···,bN        ∑  pi  S(pi,bi) + ∑  rij=(pi,relij ,pj)  Q(rij ,bi,bj)        (N)  where phrases pi and pj (and respective boxes bi and bj) are related by some relationship relij 
This is a binary quadratic programming formulation inspired by [NN]; we  relax and solve it using a sequential QP solver in MATLAB
The solution gives a single bounding box hypothesis for each phrase
Performance is evaluated using Recall@N, or proportion of phrases where the selected box has  Intersection-over-Union (IOU) ≥ 0.N with the ground truth
 N.N
Learning scoring function weights  We learn the weights wS and wQ in Eqs
(N) and (N) by directly optimizing recall on the validation set
We start by  finding the unary weights wS that maximize the number of correctly localized phrases:  wS = argmax w  N ∑  i=N  ✶IOU≥0.N(b ∗ i , b̂(pi;w)), (N)  where N is the number of phrases in the training set, ✶IOU≥0.N is an indicator function returning N if the two  boxes have IOU ≥ 0.N, b∗i is the ground truth bounding box  for phrase pi, b̂(p;w) returns the most likely box candidate for phrase p under the current weights, or, more formally, given a set of candidate boxes B,  b̂(p;w) = min b∈B  S(p, b;w)
(N)  We optimize Eq
(N) using a derivative-free direct search  method [NN] (MATLAB’s fminsearch)
We randomly initialize the weights, keep the best weights after N0 runs based  on validation set performance (takes just a few minutes to  learn weights for all single phrase cues in our experiments)
 Next, we fix wS and learn the weights wQ over phrase- pair cues in the validation set
To this end, we formulate an  objective analogous to Eq
(N) for maximizing the number  of correctly localized region pairs
Similar to Eq
(N), we  define the function ρ̂(r;w) to return the best pair of boxes for the relationship r = (p, rel, p′):  ρ̂(r;w)= min b,b′∈B  S(p,b;wS)+S(p′,b′;wS)+Q(r,b,b′;w)
(N)  Then our pairwise objective function is  wQ = argmax w  M ∑  k=N  ■PairIOU≥0.N(ρ ∗ k, ρ̂(rk;w)), (N)  where M is the number of phrase pairs with a relation- ship, ■PairIOU≥0.N returns the number of correctly localized boxes (0, N, or N), and ρ∗k is the ground truth box pair for the relationship rk = (pk, relk, p  ′ k)
 Note that we also attempted to learn the weights wS and wQ using standard approaches such as rank-SVM [NN], but found our proposed direct search formulation to work better
In phrase localization, due to its Recall@N evaluation  criterion, only the correctness of one best-scoring candidate  region for each phrase matters, unlike in typical detection  scenarios, where one would like all positive examples to  have better scores than all negative examples
The VRD  task of Section N is a more conventional detection task, so  there we found rank-SVM to be more appropriate
 N
Cues for phrase-region grounding  Section N.N describes how we extract linguistic cues from  sentences
Sections N.N and N.N give our definitions of the  two types of cost functions used in Eqs
(N) and (N): single phrase cues (SPC) measure the compatibility of a given  phrase with a candidate bounding box, and phrase pair cues  (PPC) ensure that pairs of related phrases are localized in a  spatially coherent manner
 N.N
Extracting linguistic cues from captions  The FlickrN0k Entities dataset provides annotations for  Noun Phrase (NP) chunks corresponding to entities, but linguistic cues corresponding to adjectives, verbs, and prepositions must be extracted from the captions using NLP tools
 Once these cues are extracted, they will be translated into  visually relevant constraints for grounding
In particular,  we will learn specialized detectors for adjectives, subjectverb, and verb-object relationships (Section N.N)
Also, because pairs of entities connected by a verb or preposition  NNN0    have constrained layout, we will train classifiers to score  pairs of boxes based on spatial information (Section N.N)
 Adjectives are part of NP chunks so identifying them is  trivial
To extract other cues, such as verbs and prepositions that may indicate actions and spatial relationships, we  obtain a constituent parse tree for each sentence using the  Stanford parser [NN]
Then, for possible relational phrases  (prepositional and verb phrases), we use the method of Fidler et al
[N], where we start at the relational phrase and  then traverse up the tree and to the left until we reach a noun  phrase node, which will correspond to the first entity in an  (entityN, rel, entityN) tuple
The second entity is given by  the first noun phrase node on the right side of the relational  phrase in the parse tree
For example, given the sentence A  boy running in a field with a dog, the extracted NP chunks  would be a boy, a field, a dog
The relational phrases would  be (a boy, running in, a field) and (a boy, with, a dog)
 Notice that a single relational phrase can give rise to multiple relationship cues
Thus, from (a boy, running in, a  field), we extract the verb relation (boy, running, field) and  prepositional relation (boy, in, field)
An exception to this  is a relational phrase where the first entity is a person and  the second one is of the clothing or body part type,N e.g.,  (a boy, running in, a jacket)
For this case, we create a single special pairwise relation (boy, jacket) that assumes that  the second entity is attached to the first one and the exact  relationship words do not matter, i.e., (a boy, running in, a  jacket) and (a boy, wearing, a jacket) are considered to be  the same
The attachment assumption can fail for phrases  like (a boy, looking at, a jacket), but such cases are rare
 Finally, since pronouns in FlickrN0k Entities are not  annotated, we attempt to perform pronominal coreference  (i.e., creating a link between a pronoun and the phrase  it refers to) in order to extract a more complete set of  cues
As an example, given the sentence Ducks feed themselves, initially we can only extract the subject-verb cue  (ducks, feed), but we don’t know who or what they are feeding
Pronominal coreference resolution tells us that the  ducks are themselves eating and not, say, feeding ducklings
 We use a simple rule-based method similar to knowledgepoor methods [NN, NN]
Given lists of pronouns by type,N our  rules attach each pronoun with at most one non-pronominal  mention that occurs earlier in the sentence (an antecedent)
 We assume that subject and object pronouns often refer to  the main subject (e.g
[A dog] laying on the ground looks  up at the dog standing over [him]), reflexive and reciprocal pronouns refer to the nearest antecedent (e.g
[A tennis  player] readies [herself].), and indefinite pronouns do not  refer to a previously described entity
It must be noted that  NEach NP chunk from the FlickrN0K dataset is classified into one of  eight phrase types based on the dictionaries of [NN]
NRelevant pronoun types are subject, object, reflexive, reciprocal, relative, and indefinite
 compared with verb and prepositional relationships, relatively few additional cues are extracted using this procedure  (NNN pronoun relationships in the test set and NN,NNN in the  train set, while the counts for the other relationships are on  the order of N0K and N00K)
 N.N
Single Phrase Cues (SPCs)  Region-phrase compatibility: This is the most basic cue  relating phrases to image regions based on appearance
It  is applied to every test phrase (i.e., its indicator function  in Eq
(N) is always N)
Given phrase p and region b, the cost φCCA(p, b) is given by the cosine distance between p and b in a joint embedding space learned using normal- ized Canonical Correlation Analysis (CCA) [N0]
We use  the same procedure as [NN]
Regions are represented by the  fcN activations of a Fast-RCNN model [N] fine-tuned using  the union of the PASCAL N00N and N0NN trainval sets [N]
 After removing stopwords, phrases are represented by the  HGLMM fisher vector encoding [NN] of wordNvec [N0]
 Candidate position: The location of a bounding box in  an image has been shown to be predictive of the kinds of  phrases it may refer to [N, NN, NN, NN]
We learn location  models for each of the eight broad phrase types specified  in [NN]: people, clothing, body parts, vehicles, animals,  scenes, and a catch-all “other.” We represent a bounding  box by its centroid normalized by the image size, the percentage of the image covered by the box, and its aspect  ratio, resulting in a N-dim
feature vector
We then train  a support vector machine (SVM) with a radial basis function (RBF) kernel using LIBSVM [N]
We randomly sample  EdgeBox [NN] proposals with IOU < 0.N with the ground truth boxes for negative examples
Our scoring function is  φpos(p, b) = − log(SVMtype(p)(b)),  where SVMtype(p) returns the probability that box b is of the phrase type type(p) (we use Platt scaling [NN] to convert the SVM output to a probability)
 Candidate size: People have a bias towards describing  larger, more salient objects, leading prior work to consider  the size of a candidate box in their models [N, NN, NN]
We  follow the procedure of [NN], so that given a box b with di- mensions normalized by the image size, we have  φsizetype(p)(p, b) = N− bwidth × bheight
 Unlike phrase position, this cost function does not use a  trained SVM per phrase type
Instead, each phrase type is  its own feature and the corresponding indicator function returns N if that phrase belongs to the associated type
 Detectors: CCA embeddings are limited in their ability to  localize objects because they must account for a wide range  of phrases and because they do not use negative examples  NNNN    during training
To compensate for this, we use Fast RCNN [N] to learn three networks for common object categories, attributes, and actions
Once a detector is trained, its  score for a region proposal b is  φdet(p, b) = − log(softmaxdet(p, b)),  where softmaxdet(p, b) returns the output of the softmax layer for the object class corresponding to p
We manu- ally create dictionaries to map phrases to detector categories  (e.g., man, woman, etc
map to ‘person’), and the indicator  function for each detector returns N only if one of the words  in the phrase exists in its dictionary
If multiple detectors for  a single cue type are appropriate for a phrase (e.g., a black  and white shirt would have two adjective detectors fire, one  for each color), the scores are averaged
Below, we describe  the three detector networks used in our model
Complete  dictionaries can be found in supplementary material
 Objects: We use the dictionary of [NN] to map nouns to the  N0 PASCAL object categories [N] and fine-tune the network  on the union of the PASCAL VOC N00N and N0NN trainval  sets
At test time, when we run a detector for a phrase that  maps to one of these object categories, we also use bounding box regression to refine the original region proposals
 Regression is not used for the other networks below
 Adjectives: Adjectives found in phrases, especially color,  provide valuable attribute information for localization [N,  NN, NN, NN]
The FlickrN0K Entities baseline approach [NN]  used a network trained for NN colors
As a generalization  of that, we create a list of adjectives that occur at least N00  times in the training set of FlickrN0k
After grouping together similar words and filtering out non-visual terms (e.g.,  adventurous), we are left with a dictionary of NN adjectives
 As in [NN], we consider color terms describing people (black  man, white girl) to be separate categories
 Subject-Verb and Verb-Object: Verbs can modify the appearance of both the subject and the object in a relation
For  example, knowing that a person is riding a horse can give  us better appearance models for finding both the person and  the horse [NN, NN]
As we did with adjectives, we collect  verbs that occur at least N00 times in the training set, group  together similar words, and filter out those that don’t have  a clear visual aspect, resulting in a dictionary of NN verbs
 Since a person running looks different than a dog running,  we subdivide our verb categories by phrase type of the subject (resp
object) if that phrase type occurs with the verb  at least N0 times in the train set
For example, if there are  enough animal-running occurrences, we create a new category with instances of all animals running
For the remaining phrases, we train a catch-all detector over all the phrases  related to that verb
Following [NN], we train separate detectors for subject-verb and verb-object relationships, resulting  in dictionary sizes of NNN (resp
NNN)
We also attempted to  learn subject-verb-object detectors as in [NN, NN], but did not  see a further improvement
 N.N
Phrase-Pair Cues (PPCs)  So far, we have discussed cues pertaining to a single  phrase, but relationships between pairs of phrases can also  provide cues about their relative position
We denote such  relationships as tuples (pleft , rel, pright) with left , right in- dicating on which side of the relationship the phrases occur
As discussed in Section N.N, we consider three distinct  types of relationships: verbs (man, riding, horse), prepositions (man, on, horse), and clothing and body parts (man,  wearing, hat)
For each of the three relationship types, we  group phrases referring to people but treat all other phrases  as distinct, and then gather all relationships that occur at  least N0 times in the training set
Then we learn a spatial  relationship model as follows
Given a pair of boxes with  coordinates b = (x, y, w, h) and b′ = (x′, y′, w′, h′), we compute a four-dim
feature  [(x− x′)/w, (y − y′)/h, w′/w, h′/h] , (N)  and concatenate it with combined SPC scores S(pleft , b), S(pright , b  ′) from Eq
(N)
To obtain negative examples, we randomly sample from other box pairings with IOU < 0.N with the ground truth regions from that image
We train  an RBF SVM classifier with Platt scaling [NN] to obtain a  probability output
This is similar to the method of [NN], but  rather than learning a Gaussian Mixture Model using only  positive data, we learn a more discriminative model
Below  are details on the three types of relationship classifiers
 Verbs: Starting with our dictionary of NN verb detectors  and following the above procedure of identifying all relationships that occur at least N0 times in the training set, we  end up with NN0 (pleft , relverb , pright) SVM classifiers
 Prepositions: We first gather a list of prepositions that occur at least N00 times in the training set, combine similar words, and filter out words that do not indicate a clear  spatial relationship
This yields eight prepositions (in, on,  under, behind, across, between, onto, and near) and NNN  (pleft , relprep , pright) relationships
 Clothing and body part attachment: We collect  (pleft , relc&bp , pright) relationships where the left phrase is always a person and the right phrase is from the clothing or  body part type and learn N0N such classifiers
As discussed  in Section N.N, this relationship type takes precedence over  any verb or preposition relationships that may also hold between the same phrases
 N
Experiments on FlickrN0k Entities  N.N
Implementation details  We utilize the provided train/test/val split of NN,NNN  training, N,000 validation, and N,000 testing images [NN]
 NNNN    Method Accuracy  (a) Single-phrase cues  CCA NN.0N  CCA+Det NN.NN  CCA+Det+Size NN.NN  CCA+Det+Size+Adj NN.NN  CCA+Det+Size+Adj+Verbs NN.NN  CCA+Det+Size+Adj+Verbs+Pos (SPC) NN.NN  (b) Phrase pair cues  SPC+Verbs NN.NN  SPC+Verbs+Preps NN.NN  SPC+Verbs+Preps+C&BP (SPC+PPC) NN.NN  (c) State of the art  SMPL [NN] NN.0N  NonlinearSP [N0] NN.NN  GroundeR [NN] NN.NN  MCB [N] NN.NN  RtP [NN] N0.NN  Table N: Phrase-region grounding performance on the FlickrN0k Entities  dataset
(a) Performance of our single-phrase cues (Sec
N.N)
(b) Further  improvements by adding our pairwise cues (Sec
N.N)
(c) Accuracies of  competing state-of-the-art methods
This comparison excludes concurrent  work that was published after our initial submission [N]
 Following [NN], our region proposals are given by the top  N00 EdgeBox [NN] proposals per image
At test time, given  a sentence and an image, we first use Eq
(N) to find the  top N0 candidate regions for each phrase after performing  non-maximum suppression using a 0.N IOU threshold
Restricted to these candidates, we optimize Eq
(N) to find a  globally consistent mapping of phrases to regions
 Consistent with [NN], we only evaluate localization for  phrases with a ground truth bounding box
If multiple  bounding boxes are associated with a phrase (e.g., four individual boxes for four men), we represent the phrase as the  union of its boxes
For each image and phrase in the test set,  the predicted box must have at least 0.N IOU with its ground  truth box to be deemed successfully localized
As only a  single candidate is selected for each phrase, we report the  proportion of correctly localized phrases (i.e
Recall@N)
 N.N
Results  Table N reports our overall localization accuracy for combinations of cues and compares our performance to the state  of the art
Object detectors, reported on the second line of  Table N(a), show a N% overall gain over the CCA baseline
 This includes the gain from the detector score as well as the  bounding box regressor trained with the detector in the Fast  R-CNN framework [N]
Adding adjective, verb, and size  cues improves accuracy by a further N%
Our last cue in Table N(a), position, provides an additional N% improvement
 We can see from Table N(b) that the spatial cues give only  a small overall boost in accuracy on the test set, but that  is due to the relatively small number of phrases to which  they apply
In Table N we will show that the localization  improvement on the affected phrases is much larger
 Table N(c) compares our performance to the state of  the art
The method most similar to ours is our earlier  model [NN], which we call RtP here
RtP relies on a subset  of our single-phrase cues (region-phrase CCA, size, object  detectors, and color adjectives), and localizes each phrase  separately
The closest version of our current model to  RtP is CCA+Det+Size+Adj, which replaces the NN colors  of [NN] with our more general model for NN adjectives, and  obtains almost N% better performance
Our full model is  N% better than RtP
It is also worth noting that a rank-SVM  model [NN] for learning cue combination weights gave us  N% worse performance than the direct search scheme of  Section N.N
 Table N breaks down the comparison by phrase type
 Our model has the highest accuracy on most phrase types,  with scenes being the most notable exception, for which  GroundeR [NN] does better
However, GroundeR uses Selective Search proposals [NN], which have an upper bound  performance that is N% higher on scene phrases despite using half as many proposals
Although body parts have the  lowest localization accuracy at NN.NN%, this represents an  N% improvement in accuracy over prior methods
However,  only around NN% of body part phrases have a box with high  enough IOU with the ground truth, showing a major area of  weakness of category-independent proposal methods
Indeed, if we were to augment our EdgeBox region proposals  with ground truth boxes, we would get an overall improvement in accuracy of about N% for the full system
 Since many of the cues apply to a small subset of the  phrases, Table N details the performance of cues over only  the phrases they affect
As a baseline, we compare against  the combination of cues available for all phrases: regionphrase CCA, position, and size
To have a consistent set of  regions, the baseline also uses improved boxes from bounding box regressors trained along with the object detectors
 As a result, the object detectors provide less than N% gain  over the baseline for the phrases on which they are used,  suggesting that the regression provides the majority of the  gain from CCA to CCA+Det in Table N
This also confirms  that there is significant room for improvement in selecting candidate regions
By contrast, adjective, subject-verb,  and verb-object detectors show significant gains, improving  over the baseline by N-N%
 The right side of Table N shows the improvement on  phrases due to phrase pair cues
Here, we separate the  phrases that occur on the left side of the relationship, which  corresponds to the subject, from the phrases on the right  side
Our results show that the subject, is generally easier to localize
On the other hand, clothing and body parts  show up mainly on the right side of relationships and they  tend to be small
It is also less likely that such phrases will  have good candidate boxes – recall from Table N that body  parts have a performance upper bound of only NN%
Although they affect relatively few test phrases, all three of our  relationship classifiers show consistent gains over the SPC  NNNN    People Clothing Body Parts Animals Vehicles Instruments Scene Other  #Test N,NNN N,N0N NNN NNN N00 NNN N,NNN N,NNN  SMPL [NN] NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  GroundeR [NN] NN.00 NN.NN N0.NN NN.NN NN.NN NN.NN NN.NN NN.0N  RtP [NN] NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  SPC+PPC (ours) NN.NN N0.NN NN.NN NN.NN NN.N0 NN.N0 NN.NN NN.NN  Upper Bound NN.NN NN.NN NN.NN NN.NN NN.00 NN.N0 NN.NN NN.0N  Table N: Comparison of phrase localization performance over phrase types
Upper Bound refers to the proportion of phrases of each type for which there  exists a region proposal having at least 0.N IOU with the ground truth
 Single Phrase Cues (SPC) Phrase-Pair Cues (PPC)  Method Object  Detectors Adjectives  SubjectVerb  VerbObject  Verbs Prepositions Clothing &  Body Parts  Left Right Left Right Left Right  Baseline NN.NN NN.NN NN.NN N0.N0 NN.NN NN.0N NN.NN NN.0N NN.0N N0.NN  +Cue NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  #Test N,0NN N,N0N N,0NN N,NNN NNN NNN NN0 NNN N,NNN N,NNN  #Train NNN,NNN NN0,NNN NN,NNN NN,NNN NN,NNN NN,NNN NN,NNN NN,N0N NN,0NN NN,NNN  Table N: Breakdown of performance for individual cues restricted only to test phrases to which they apply
For SPC, Baseline is given by  CCA+Position+Size
For PPC, Baseline is the full SPC model
For all comparisons, we use the improved boxes from bounding box regression on top  of object detector output
PPC evaluation is split by which side of the relationship the phrases occur on
The bottom two rows show the numbers of affected  phrases in the test and training sets
For reference, there are NN.Nk visual phrases in the test set and NNNk visual phrases in the train set
 model
This is encouraging given that many of the relationships that are used on the validation set to learn our model  parameters do not occur in the test set (and vice versa)
 Figure N provides a qualitative comparison of our output  with the RtP model [NN]
In the first example, the prediction  for the dog is improved due to the subject-verb classifier for  dog jumping
For the second example, pronominal coreference resolution (Section N.N) links each other to two men,  telling us that not only is a man hitting something, but also  that another man is being hit
In the third example, the RtP  model is not able to locate the woman’s blue stripes in her  hair despite having a model for blue
Our adjective detectors take into account stripes as well as blue, allowing us  to correctly localize the phrase, even though we still fail  to localize the hair
Since the blue stripes and hair should  co-locate, a method for obtaining co-referent entities would  further improve performance on such cases
In the last example, the RtP model makes the same incorrect prediction  for the two men
However, our spatial relationship between  the first man and his gray sweater helps us correctly localize  him
We also improve our prediction for the shopping cart
 N
Visual Relationship Detection  In this section, we adapt our framework to the recently  introduced Visual Relationship Detection (VRD) benchmark of Lu et al
[NN]
Given a test image without any text  annotations, the task of VRD is to detect all entities and  relationships present and output them in the form (subject,  predicate, object) with the corresponding bounding boxes
 A relationship detection is judged to be correct if it exists  in the image and both the subject and object boxes have  IOU ≥ 0.N with their respective ground truth
In contrast to phrase grounding, where we are given a set of entities and  relationships that are assumed to be in the image, here we  do not know a priori which objects or relationships might  be present
On the other hand, the VRD dataset is easier  than FlickrN0K Entities in that it has a limited vocabulary  of N00 object classes and N0 predicates annotated in N000  training and N000 test images
 Given the small fixed class vocabulary, it would seem  advantageous to train N00 object detectors on this dataset,  as was done by Lu et al
[NN]
However, the training set  is relatively small, the class distribution is unbalanced, and  there is no validation set
Thus, we found that training detectors and then relationship models on the same images  causes overfitting because the detector scores on the training images are overconfident
We obtain better results by  training all appearance models using CCA, which also takes  into account semantic similarity between category names  and is trivially extendable to previously unseen categories
 Here, we use fcN features from a Fast RCNN model trained  on MSCOCO [NN] due to the larger range of categories  than PASCAL, and wordNvec for object and predicate class  names
We train the following CCA models: N
CCA(entity box, entity class name): this is the equivalent to region-phrase CCA in Section N.N and is used  to score both candidate subject and object boxes
N
CCA(subject box, [subject class name, predicate class  name]): analogous to subject-verb classifiers of Section N.N
The N00-dimensional wordNvec features of  subject and predicate class names are concatenated
N
CCA(object box, [predicate class name, object class  name]): analogous to verb-object classifiers of Section  N.N
N
CCA(union box, predicate class name): this model  measures the compatibility between the bounding box  of both subject and object and the predicate name
N
CCA(union box, [subject class name, predicate class  name, object class name])
 NNNN    A young man kneeling in front   of a young girl who has blond   hair and blue stripes
!  RtP!  Ours !  Two people are hitting each   other in a karate match, while   an audience and referee watch
!  This dog is jumping through   the water
!  man !  A man in a gray sweater   speaks to two women and   a man pushing a shopping   cart through Walmart
!  Figure N: Example results on FlickrN0k Entities comparing our SPC+PPC model’s output with the RtP model [NN]
See text for discussion
 Note that models N and N had no analogue in our phrase  localization system
On that task, entities were known to be  in the image and relationships simply provided constraints,  while here we need to predict which relationships exist
To  make predictions for predicates and relationships (which is  the goal of models N and N), it helps to see both the subject  and object regions
Union box features were also less useful  for phrase localization due to the larger vocabularies and  relative scarcity of relationships in that task
 Each candidate relationship gets six CCA scores (model  N above is applied both to the subject and the object)
In  addition, we compute size and position scores as in Section N.N for subject and object, and a score for a pairwise  spatial SVM trained to predict the predicate based on the  four-dimensional feature of Eq
(N)
This yields an NNdim
feature vector
By contrast with phrase localization,  our features for VRD are dense (always available for every  relationship)
 In Section N.N we found feature weights by maximizing our recall metric
Here we have a more conventional  detection task, so we obtain better performance by training a linear rank-SVM model [NN] to enforce that correctly  detected relationships are ranked higher than negative detections (where either box has < 0.N IOU with the ground truth)
We use the test set object detections (just the boxes,  not the scores) provided by [NN] to directly compare performance with the same candidate regions
During testing,  we produce a score for every ordered pair of detected boxes  and all possible predicates, and retain the top N0 predicted  relationships per pair of (subject, object) boxes
 Consistent with [NN], Table N reports recall, R@{N00, N0}, or the portion of correctly localized relationships in the top N00 (resp
N0) ranked relationships in the image
The  right side shows performance for relationships that have not  been encountered in the training set
Our method clearly  outperforms that of Lu et al
[NN], which uses separate visual, language, and relationship likelihood cues
We also  Method Rel
Det
Zero-shot Rel
Det
 R@N00 R@N0 R@N00 R@N0  (a) Visual Only Model [NN] N.NN N.NN 0.NN 0.NN  Visual + Language + NN.N0 NN.NN N.NN N.NN  Likelihood Model [NN]  VTransE [NN] NN.N0 NN.0N N.NN N.NN  (b) CCA NN.NN N0.0N NN.NN N.NN  CCA + Size NN.0N N0.NN NN.NN N.NN  CCA + Size + Position NN.NN NN.0N NN.NN N.NN  Table N: Relationship detection recall at different thresholds  (R@{N00,N0})
CCA refers to the combination of six CCA models (see text)
Position refers to the combination of individual box position  and pairwise spatial classifiers
This comparison excludes concurrent  work that was published after our initial submission [NN, NN]
 outperform Zhang et al
[NN], which combines object detectors, visual appearance, and object position in a single neural network
We observe that cues based on object class and  relative subject-object position provide a noticeable boost in  performance
Further, due to using CCA with multi-modal  embeddings, we generalize better to unseen relationships
 N
Conclusion  This paper introduced a framework incorporating a comprehensive collection of image- and language-based cues  for visual grounding and demonstrated significant gains  over the state of the art on two tasks: phrase localization on  FlickrN0k Entities and relationship detection on the VRD  dataset
For the latter task, we got particularly pronounced  gains for the zero-shot learning scenario
In future work, we  would like to train a single network for combining multiple  cues
Doing this in a unified end-to-end fashion is challenging, since one needs to find the right balance between  parameter sharing and specialization or fine-tuning required  by individual cues
To this end, our work provides a strong  baseline and can help to inform future approaches
 Acknowledgments
This work was partially supported by  NSF grants N0NNNNN, NN0NNNN, NN0NNNN, NN0NNNN, and  NNNNNNN, Xerox UAC, the Sloan Foundation, and a Google  Research Award
 NNNN    References  [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra, C
L
 Zitnick, and D
Parikh
Vqa: Visual question answering
In  ICCV, N0NN
N  [N] C.-C
Chang and C.-J
Lin
LIBSVM: A library for  support vector machines
ACM Transactions on Intelligent Systems and Technology, N:NN:N–NN:NN, N0NN
Software available at http://www.csie.ntu.edu.tw/  ˜cjlin/libsvm
N  [N] K
Chen, R
Kovvuri, J
Gao, and R
Nevatia
MSRC: Multimodal spatial regression with semantic context for phrase  grounding
In ICMR, N0NN
N  [N] S
K
Divvala, D
Hoiem, J
H
Hays, A
A
Efros, and  M
Heber
An empirical study of context in object detection
 In CVPR, N00N
N  [N] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn,  and A
Zisserman
The PASCAL Visual Object Classes  Challenge N0NN (VOCN0NN) Results
http://www.pascalnetwork.org/challenges/VOC/vocN0NN/workshop/index.html,  N0NN
N, N  [N] H
Fang, S
Gupta, F
Iandola, R
Srivastava, L
Deng, P
Dollar, J
Gao, X
He, M
Mitchell, J
Platt, L
Zitnick, and  G
Zweig
From captions to visual concepts and back
In  CVPR, N0NN
N  [N] S
Fidler, A
Sharma, and R
Urtasun
A sentence is worth a  thousand pixels
In CVPR, N0NN
N, N, N  [N] A
Fukui, D
H
Park, D
Yang, A
Rohrbach, T
Darrell, and  M
Rohrbach
Multimodal compact bilinear pooling for visual question answering and visual grounding
In EMNLP,  N0NN
N, N, N  [N] R
Girshick
Fast r-cnn
In ICCV, N0NN
N, N, N  [N0] Y
Gong, Q
Ke, M
Isard, and S
Lazebnik
A multi-view embedding space for modeling internet images, tags, and their  semantics
IJCV, N0N(N):NN0–NNN, N0NN
N  [NN] S
Harabagiu and S
Maiorano
Knowledge-lean coreference resolution and its relation to textual cohesion and coherence
In Proceedings of the ACL-NN Workshop on the relation of discourse/dialogue structure and reference, pages  NN–NN, NNNN
N  [NN] R
Hu, H
Xu, M
Rohrbach, J
Feng, K
Saenko, and T
Darrell
Natural language object retrieval
In CVPR, N0NN
N,  N  [NN] T
Joachims
Training linear svms in linear time
In SIGKDD,  N00N
N, N, N  [NN] J
Johnson, A
Karpathy, and L
Fei-Fei
Densecap: Fully  convolutional localization networks for dense captioning
In  CVPR, N0NN
N  [NN] J
Johnson, R
Krishna, M
Stark, L.-J
Li, D
A
Shamma,  M
Bernstein, and L
Fei-Fei
Image retrieval using scene  graphs
In CVPR, N0NN
N, N  [NN] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, N0NN
 N  [NN] A
Karpathy, A
Joulin, and L
Fei-Fei
Deep fragment embeddings for bidirectional image sentence mapping
In NIPS,  N0NN
N  [NN] S
Kazemzadeh, V
Ordonez, M
Matten, and T
Berg
 Referitgame: Referring to objects in photographs of natural  scenes
In EMNLP, N0NN
N, N, N, N  [NN] B
Klein, G
Lev, G
Sadeh, and L
Wolf
Associating neural word embeddings with deep image representations using  fisher vector
In CVPR, N0NN
N  [N0] C
Kong, D
Lin, M
Bansal, R
Urtasun, and S
Fidler
What  are you talking about? text-to-image coreference
In CVPR,  N0NN
N  [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, M
Bernstein, and L
Fei-Fei
Visual genome: Connecting language  and vision using crowdsourced dense image annotations
 IJCV, N0NN
N  [NN] J
C
Lagarias, J
A
Reeds, M
H
Wright, and P
E
 Wright
Convergence properties of the nelder-mead simplex  method in low dimensions
SIAM Journal of Optimization,  N(N):NNNNNN, NNNN
N  [NN] L.-J
Li, H
Su, Y
Lim, and L
Fei-Fei
Object bank: An  object-level image representation for high-level visual recognition
IJCV, N0N(N):N0–NN, N0NN
N  [NN] Y
Li, W
Ouyang, X
Wang, and X
Tang
ViP-CNN: Visual  phrase guided convolutional neural network
In CVPR, N0NN
 N  [NN] X
Liang, L
Lee, and E
P
Xing
Deep variation-structured  reinforcement learning for visual relationship and attribute  detection
In CVPR, N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft COCO: Common objects in context
In ECCV, N0NN
N  [NN] C
Lu, R
Krishna, M
Bernstein, and L
Fei-Fei
Visual relationship detection with language priors
In ECCV, N0NN
N,  N, N, N  [NN] L
Ma, Z
Lu, L
Shang, and H
Li
Multimodal convolutional  neural networks for matching image and sentence
In ICCV,  N0NN
N  [NN] J
Mao, J
Huang, A
Toshev, O
Camburu, A
Yuille, and  K
Murphy
Generation and comprehension of unambiguous  object descriptions
In CVPR, N0NN
N, N  [N0] T
Mikolov, K
Chen, G
Corrado, and J
Dean
Efficient estimation of word representations in vector space
 arXiv:NN0N.NNNN, N0NN
N  [NN] R
Mitkov
Robust pronoun resolution with limited knowledge
In Proceedings of the NNth Annual Meeting of the Association for Computational Linguistics and NNth International  Conference on Computational Linguistics-Volume N, pages  NNN–NNN
Association for Computational Linguistics, NNNN
 N  [NN] J
C
Platt
Probabilistic outputs for support vector machines  and comparisons to regularized likelihood methods
In Advances in Large Margin Classifiers, pages NN–NN
MIT Press,  NNNN
N, N  [NN] B
A
Plummer, L
Wang, C
M
Cervantes, J
C
Caicedo,  J
Hockenmaier, and S
Lazebnik
FlickrN0k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models
IJCV, NNN(N):NN–NN, N0NN
N, N, N, N, N,  N, N, N  NNNN  http://www.csie.ntu.edu.tw/~cjlin/libsvm http://www.csie.ntu.edu.tw/~cjlin/libsvm   [NN] A
Rohrbach, M
Rohrbach, R
Hu, T
Darrell, and  B
Schiele
Grounding of textual phrases in images by reconstruction
In ECCV, N0NN
N, N, N  [NN] F
Sadeghi, S
K
Divvala, and A
Farhadi
Viske: Visual  knowledge extraction and question answering by visual verification of relation phrases
In CVPR, N0NN
N  [NN] M
A
Sadeghi and A
Farhadi
Recognition using visual  phrases
In CVPR, N0NN
N  [NN] R
Socher, J
Bauer, C
D
Manning, and A
Y
Ng
Parsing  With Compositional Vector Grammars
In ACL, N0NN
N  [NN] J
Tighe, M
Niethammer, and S
Lazebnik
Scene parsing with object instances and occlusion ordering
In CVPR,  N0NN
N  [NN] J
Uijlings, K
van de Sande, T
Gevers, and A
Smeulders
 Selective search for object recognition
IJCV, N0N(N), N0NN
 N  [N0] L
Wang, Y
Li, and S
Lazebnik
Learning deep structurepreserving image-text embeddings
In CVPR, N0NN
N, N,  N  [NN] M
Wang, M
Azab, N
Kojima, R
Mihalcea, and J
Deng
 Structured matching for phrase localization
In ECCV, N0NN
 N, N, N  [NN] K
Xu, J
Ba, R
Kiros, A
Courville, R
Salakhutdinov,  R
Zemel, and Y
Bengio
Show, attend and tell: Neural image caption generation with visual attention
In ICML, N0NN
 N  [NN] P
Young, A
Lai, M
Hodosh, and J
Hockenmaier
From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions
TACL,  N:NN–NN, N0NN
N  [NN] L
Yu, E
Park, A
C
Berg, and T
L
Berg
Visual Madlibs:  Fill in the blank Image Generation and Question Answering
 In ICCV, N0NN
N  [NN] H
Zhang, Z
Kyaw, S.-F
Chang, and T.-S
Chua
Visual  translation embedding network for visual relation detection
 In CVPR, N0NN
N  [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object  proposals from edges
In ECCV, N0NN
N, N  NNNNMulti-Modal Factorized Bilinear Pooling With Co-Attention Learning for Visual Question Answering   Multi-modal Factorized Bilinear Pooling with Co-Attention Learning  for Visual Question Answering  Zhou Yu†, Jun Yu†∗, Jianping Fan‡, Dacheng Tao§  † Key Laboratory of Complex Systems Modeling and Simulation,  School of Computer Science and Technology, Hangzhou Dianzi University, P
R
China ‡ Department of Computer Science, University of North Carolina at Charlotte, USA  § UBTECH Sydney AI Centre, School of IT, FEIT, The University of Sydney, Australia  yuz@hdu.edu.cn, yujun@hdu.edu.cn, jfan@uncc.edu, dacheng.tao@sydney.edu.au  Abstract  Visual question answering (VQA) is challenging because  it requires a simultaneous understanding of both the visual  content of images and the textual content of questions
The  approaches used to represent the images and questions in a  fine-grained manner and questions and to fuse these multimodal features play key roles in performance
Bilinear  pooling based models have been shown to outperform traditional linear models for VQA, but their high-dimensional  representations and high computational complexity may  seriously limit their applicability in practice
For multimodal feature fusion, here we develop a Multi-modal Factorized Bilinear (MFB) pooling approach to efficiently and  effectively combine multi-modal features, which results in  superior performance for VQA compared with other bilinear pooling approaches
For fine-grained image and question representation, we develop a ‘co-attention’ mechanism  using an end-to-end deep network architecture to jointly  learn both the image and question attentions
Combining  the proposed MFB approach with co-attention learning in  a new network architecture provides a unified model for  VQA
Our experimental results demonstrate that the single  MFB with co-attention model achieves new state-of-theart performance on the real-world VQA dataset
Code  available at https://github.com/yuzcccc/mfb
 N
Introduction  Thanks to recent advances in computer vision and natural language processing, computers are expected to be able  to automatically understand the semantics of images and  natural languages in the near future
Such advances have  ∗Jun Yu is the corresponding author  also stimulated new research topics like image-text retrieval  [NN, NN], image captioning [N, NN], and visual question  answering [N, NN]
 Compared with image-text retrieval and image captioning (which just require the underlying algorithms to search  or generate a free-form text description for a given image),  visual question answering (VQA) is a more challenging  task that requires fine-grained understanding of the semantics of both the images and the questions as well as supports  complex reasoning to predict the best-matching answer  correctly
In some aspects, the VQA task can be treated as a  generalization of image captioning and image-text retrieval
 Thus building effective VQA algorithms, which can achieve  close performance like human beings, is an important step  towards enabling artificial intelligence in general
 Existing VQA approaches usually have three stages: (N)  representing the images as visual features and questions as  textual features; (N) combining these multi-modal features  to obtain fused image-question features; (N) using the  integrated image-question features to learn a multi-class  classifier and to predict the best-matching answer
Deep  neural networks (DNNs) are effective and flexible, many  existing approaches model the three stages in one DNN  model and train the model in an end-to-end fashion through  back-propagation
In the three stages, feature representation and multi-modal feature fusion particular affect VQA  performance
 With respect to multi-modal feature fusion, most existing  approaches simply use linear models for multi-modal feature fusion (e.g., concatenation or element-wise addition)  to integrate the image’s visual feature with the question’s  textual feature [NN, NN]
Since multi-modal feature distributions may vary dramatically, the integrated image-question  representations obtained by such linear models may not be  sufficiently expressive to fully capture complex associations  between the visual features from images and the textual  NNNNN  https://github.com/yuzcccc/mfb   features from questions
In contrast to linear pooling,  bilinear pooling [NN] has recently been used to integrate  different CNN features for fine-grained image recognition  [NN]
However, the high dimensionality of the output  features and the huge number of model parameters may  seriously limit the applicability of bilinear pooling
Fukui  et al
proposed the Multi-modal Compact Bilinear (MCB)  pooling model to effectively and simultaneously reduce  the number of parameters and computation time using the  Tensor Sketch algorithm [N]
Using the MCB model, the  group proposed a network architecture for the VQA task  and won the VQA challenge N0NN
Nevertheless, the MCB  model lies on a high-dimensional output feature to guarantee robust performance, which may limit its applicability  due to huge memory usage
To overcome this problem, Kim  et al
proposed the Multi-modal Low-rank Bilinear (MLB)  pooling model based on the Hadamard product of two  feature vectors [NN]
Since MLB generate output features  with lower dimensions and models with fewer parameters,  it is highly competitive with MCB
However, MLB has a  slow convergence rate and is sensitive to the learned hyperparameters
To address these issues, here we develop the  Multi-modal Factorized Bilinear pooling (MFB) method,  which enjoys the dual benefits of compact output features  of MLB and robust expressive capacity of MCB
 With respect to feature representation, directly using  global features for image representation may introduce  noisy information that is irrelevant to the given question
 Therefore, it is intuitive to introduce visual attention mechanism [NN] into the VQA task to adaptively learn the most  relevant image regions for a given question
Modeling  visual attention may significantly improve performance  [N]
However, most existing approaches only model image attention without considering question attention, even  though question attention is also very important since the  questions interpreted in natural languages may also contain  colloquialisms that can be regarded as noise
Therefore,  based on our MFB approach, we design a deep network  architecture for the VQA task using a co-attention learning  module to jointly learn both image and question attentions
 To summarize, the main contributions of this study  are as follows: First, we develop a simple but effective  Multi-modal Factorized Bilinear pooling (MFB) approach  to fuse the visual features from images with the textual  features from questions
MFB significantly outperforms  existing multi-modal bilinear pooling approaches such as  MCB [N] and MLB [NN]
Second, based on the MFB  module, a co-attention learning architecture is designed to  jointly learn both image and question attention
Our MFB  approach with co-attention model achieves the state-of-theart performance on the VQA dataset
We also conduct  detailed and extensive experiments to show why our MFB  approach is effective
Our experimental results demonstrate  that normalization techniques are extremely important in  bilinear models
 N
Related Work  In this section, we briefly review the most relevant  research on VQA, especially those studies that use multimodal bilinear models
 N.N
Visual Question Answering (VQA)  Malinowski et al
[NN] made an early attempt at solving  the VQA task
Since then, solving the VQA task has  received increasing attention from the computer vision and  natural language processing communities
VQA approaches can be classified into the following methodological  categories: the coarse joint-embedding models [NN, N, NN,  NN], the fine-grained joint-embedding models with attention  [N, NN, N, N, NN, NN, N0] and the external knowledge based  models [NN, N0, NN]
 The coarse joint-embedding models are the most straightforward VQA solutions
Image and question are first  represented as global features and then integrated to predict  the answer
Zhou et al
proposed a baseline approach for the  VQA task by using the concatenation of the image CNN  features and the question BoW (bag-of-words) features,  with a linear classifier learned to predict the answer [NN]
 Some approaches introduce more complex deep models,  e.g., LSTM networks [N] or residual networks [NN], to tackle  the VQA task in an end-to-end fashion
 One limitation of coarse joint-embedding models is that  their global features may contain noisy information, making  it hard to correctly answer fine-grained problems (e.g.,  “what color are the cat’s eyes?”) 
Therefore, recent VQA  approaches introduce the visual attention mechanism [NN]  into the VQA task by adaptively learning the local finegrained image features for a given question
Chen et al
 proposed a “question-guided attention map” that projects  the question embeddings to the visual space and formulates  a configurable convolutional kernel to search the image  attention region [N]
Yang et al
proposed a stacked attention network to learn the attention iteratively [NN]
Some  approaches introduce off-the-shelf object detectors [N] or  object proposals [NN] as the attention region candidates  and then use the question to identify related ones
Fukui  et al
proposed multi-modal compact bilinear pooling to  integrate image features from spatial grids with textual  features from the questions to predict the attention [N]
In  addition, some approaches apply attention learning to both  the images and questions
Lu et al
proposed a co-attention  learning framework to alternately learn the image attention  and the question attention [NN]
Nam et al
proposed a  multi-stage co-attention learning framework to refine the  attentions based on memory of previous attentions [NN]
 NNNN    Despite joint embedding models for VQA delivering  impressive performance, they are not good enough for  answering problems that require complex reasoning or  common sense knowledge
Therefore, introducing external  knowledge is beneficial for VQA
However, existing approaches have either only been applied to specific datasets  [NN, N0], or have been ineffective on benchmark datasets  [NN]
There is room for further exploration and development
 N.N
Multi-modal Bilinear Models for VQA  Multi-modal feature fusion plays an important and fundamental role in VQA
After the image and question features are obtained, concatenation or element-wise summations are most frequently used for multi-modal feature  fusion
Since the distributions of two feature sets in different modalities (i.e.,the visual features from images and  the textual features from questions) may vary significantly,  the representation capacity of the fused features may be  insufficient, limiting the final prediction performance
 Fukui et al
first introduced the bilinear model to solve  the problem of multi-modal feature fusion in VQA
In  contrast to the aforementioned approaches, they proposed  the Multi-modal Compact Bilinear pooling (MCB), which  uses the outer product of two feature vectors to produce a  very high-dimensional feature for quadratic expansion [N]
 To reduce the computational cost, they used a samplingbased approximation approach that exploits the property  that the projection of two vectors can be represented as their  convolution
The MCB model outperformed the simple  fusion approaches and demonstrated superior performance  on the VQA dataset [N]
Nevertheless, MCB usually needs  high-dimensional features (e.g., NN,000-D) to guarantee  robust performance, which may seriously limit its applicability due to limitations in GPU memory
 To overcome this problem, Kim et al
proposed the  Multi-modal Low-rank Bilinear Pooling (MLB) approach  based on the Hadamard product of two feature vectors  (i.e., the image feature x ∈ Rm and the question feature y ∈ Rn) in the common space with two low-rank projection matrices: [NN]:  z = MLB(x, y) = (UTx) ◦ (V T y) (N)  where U ∈ Rm×o and V ∈ Rn×o are the projection matrices, o is the dimensionality of the output feature, and ◦ denotes the Hadamard product or the element-wise multiplication of two vectors
To further increase model  capacity, nonlinear activation like tanh is added after z
Since the MLB approach can generate feature vectors with  low dimensions and deep models with fewer parameters, it  has achieved comparable performance to MCB
In [NN], the  experimental results indicated that MLB may lead to a slow  convergence rate (the MLB with attention model takes NN0k  iterations, which is about NN0 epochs, to converge [NN])
 N
Multi-modal Factorized Bilinear Pooling  Given two feature vectors in different modalities, e.g.,  the visual features x ∈ Rm for an image and the textual features y ∈ Rn for a question, the simplest multi-modal bilinear model is defined as follows:  zi = x TWiy (N)  where Wi ∈ R m×n is a projection matrix, zi ∈ R  is the output of the bilinear model
The bias term is  omitted here since it is implicit in W 
To obtain a o- dimensional output z, we need to learn W = [Wi, ...,Wo] ∈ R  m×n×o
Although bilinear pooling can effectively capture  the pairwise interactions between the feature dimensions, it  also introduces huge number of parameters that may lead to  high computational cost and a risk of over-fitting
 Inspired by the matrix factorization tricks for uni-modal  data [NN, NN], the projection matrix Wi in Eq.(N) can be factorized as two low-rank matrices:  zi = x TUiV  T  i y =  k∑  d=N  xTudv T  d y  = NT (UT i x ◦ V T  i y)  (N)  where k is the factor or the latent dimensionality of the factorized matrices Ui = [uN, ..., uk] ∈ R  m×k and Vi = [vN, ..., vk] ∈ R  n×k, ◦ is the Hadmard product or the element-wise multiplication of two vectors, N ∈ Rk is an all-one vector
 To obtain the output feature z ∈ Ro by Eq.(N), the weights to be learned are two three-order tensors U = [UN, ..., Uo] ∈ R  m×k×o and V = [VN, ..., Vd] ∈ R n×k×o  accordingly
Without loss of generality, we can reformulate  U and V as N-D matrices Ũ ∈ Rm×ko and Ṽ ∈ Rn×ko  respectively with simple reshape operations
Accordingly,  Eq.(N) can be rewritten as follows:  z = SumPooling(ŨTx ◦ Ṽ T y, k) (N)  where the function SumPooling(x, k) means using a one- dimensional non-overlapped window with the size k to perform sum pooling over x
We name this model Multi- modal Factorized Bilinear pooling (MFB)
 The detailed procedures of MFB are illustrated in Fig
 N(a)
The approach can be easily implemented by combining some commonly-used layers such as fully-connected,  element-wise multiplication and pooling layers
Furthermore, to prevent over-fitting, a dropout layer is added  after the element-wise multiplication layer
Since elementwise multiplication is introduced, the magnitude of the  output neurons may vary dramatically, and the model might  NNNN    Sum Pooling  (a) Multi-modal Factorized Bilinear Pooling  FC FC  Eltwise Multiplication  Dropout  Power Normalization  LN Normalization  Sum Pooling  Expand Stage  Squeeze Stage  x y  (b) MFB module  Figure N
The flowchart of Multi-modal Factorized Bilinear  Pooling and completed design of the MFB module
 converge to an unsatisfactory local minimum
Therefore,  similar to [N], the power normalization (z ← sign(z)|z|0.N) and ℓN normalization (z ← z/‖z‖) layers are appended after MFB output
The flowchart of the entire MFB module  is illustrated in Fig
N(b)
 Relationship to MLB
Eq.(N) shows that the MLB  in Eq.(N) is a special case of the proposed MFB with  k = N, which corresponds to the rank-N factorization
Figuratively speaking, MFB can be decomposed into two  stages (see in Fig
N(b)): first, the features from different  modalities are expanded to a high-dimensional space and  then integrated with element-wise multiplication
After  that, sum pooling followed by the normalization layers are  performed to squeeze the high-dimensional feature into the  compact output feature, while MLB directly projects the  features to the low-dimensional output space and performs  element-wise multiplication
Therefore, with the same  dimensionality for the output features, the representation  capacity of MFB is more powerful than MLB
 N
Network Architectures for VQA  The goal of the VQA task is to answer a question about  an image
The inputs to the model contain an image and  a corresponding question about the image
Our model  extracts both the image and the question representations,  integrates the multi-modal features using the MFB module in Figure N(b), treats each individual answer as one  class and performs multi-class classification to predict the  correct answer
In this section, two network architectures  are introduced
The first is the MFB baseline with one  MFB module, which is used to perform ablation analysis  with different hyper-parameters for comparison with other  baseline approaches
The second network introduces coattention learning which jointly learns the image and question attentions, to better capture fine-grained correlations  between the image and the question, which may lead to a  model with better representation capability
 CNN  LSTM  M F  B  F C  S o  ft m  a x   Banana   What s the mustache made of ?  Figure N
MFB baseline network architecture for VQA
 N.N
MFB Baseline  Similar to [N], we extract the image features using NNNlayer ResNet model [N] pre-trained on the ImageNet dataset
 Images are resized to NNN× NNN, and N0NN-D poolN features (with ℓN normalization) are used for image representation
Questions are first tokenized into words, and then further  transformed to one-hot feature vectors with max length T 
Then, the one-hot vectors are passed through an embedding  layer and fed into a two-layer LSTM networks with N0NN  hidden units [N]
Each LSTM layer outputs a N0NN-D  feature for each word
Similar to [N], we extract the output  feature of the last word from each LSTM network, and  concatenate the obtained features of two LSTM networks  to form a N0NN-D feature vector for question representation
 For predicting the answers, we simply use the top-N most frequent answers as N classes since they follow the long- tail distribution
 The extracted image and question features are fed to the  MFB module to generate the fused feature z
Finally, z is fed to a N -way classifier with the KL-divergence loss
Therefore, all the weights except the ones for the ResNet  (due to the limitation of GPU memory) are optimized jointly  in an end-to-end manner
The whole network architecture  is illustrated in Figure N
 N.N
MFB with Co-Attention  For a given image, different questions could result in  entirely different answers
Therefore, an image attention  model, which can predict the relevance of each spatial grid  to the question, is beneficial for predicting the accurate  answer
In [N], NN×NN (NNN) image spatial grids (resNc feature maps in ResNet) are used to represent the input  image
After that, the question feature is merged with  each of the NNN image features using MCB, followed by  some feature transformations (e.g., N × N convolution and ReLU activation) and softmax normalization to predict the  attention weight for each grid location
Based on the attention map, the attentional image features are obtained by the  weighted sum of the spatial grid vectors
Multiple attention  maps are generated to enhance the learned attention map,  and these attention maps are concatenated to output the  NNNN    CNN  LSTM  M F  B   Banana   C o  n v  C o  n v  R e  LU  C o  n v  S o  ft m  a x Im  a g  e  A tt  
 F  e a  t
 M F  B  F C  S o  ft m  a x  Image Attention  Q u  e st  io n  A tt  
 F  e a  t
 S o  ft m  a x  Question Attention  R e  LU  C o  n v  What s the mustache made of ?  What s the mustache made of ?  Im a  g e  A tt  
 F  e a  t
 C o  n ca  t  Figure N
MFB with Co-Attention network architecture for VQA
Different from the network of MFB baseline, the images and questions  are firstly represented as the fine-grained features respectively
Then, Question Attention and Image Attention modules are jointly modeled  in the framework to provide more accurate answer predictions
 attentional image features
Finally, the attentional image  features are merged with the question features using MCB  to determine the final answer prediction
 From the results reported in [N], one can see that incorporating an attention mechanism allows the model to  effectively learn which region is important for the question,  clearly contributing to better performance than the model  without attention
However, the attention model in [N]  only focuses on learning image attention while completely  ignoring question attention
Since the questions are interpreted as natural language, the contribution of each word  is significantly different
Therefore, here we develop a coattention learning approach (see Figure N) to jointly learn  both the question and image attentions
 The difference between the network architecture of our  co-attention model and the attention model in [N] is that  we additionally place a question attention module after the  LSTM networks to learn the attention weights of every  word in the question
Different to other co-attention models  for VQA [NN, NN], in our model, the image and question  modules are loosely coupled such that we do not exploit  the image features when learning the question attention  module
This is because we assume that the network can  directly infer the question attention (i.e., the key words of  the question) without seeing the image, as humans do
We  name this network MFB with Co-Attention (MFB+CoAtt)
 N
Experiments  In this section, we conduct several experiments to evaluate the performance of our MFB models on the VQA  task using the VQA dataset [N] to verify our approach
 We first perform ablation analysis on the MFB baseline  model to verify the efficiency of the proposed approach over  existing state-of-the-art methods such as MCB [N] and MLB  [NN]
We then provide detailed analyses of the reasons why  our MFB model outperforms its counterparts
Finally, we  choose the optimal hyper-parameters for the MFB module  and train the model with co-attention (MFB+CoAtt) for fair  comparison with other state-of-the-art approaches on the  VQA dataset [N]
 N.N
Datasets  The VQA dataset [N] consists of approximately N00,000  images from the MS-COCO dataset [NN], with N questions  per image and N0 answers per question
The data set is  split into three: train (N0k images and NNNk questions),  val (N0k images and NNNk questions), and test (N0k images  and NNNk questions)
Additionally, there is a NN% test split subset named test-dev
Two tasks are provided to  evaluate performance: Open-Ended (OE) and MultipleChoices (MC)
We use the tools provided by Antol et al
 [N] to evaluate the performance on the two tasks
 N.N
Experimental Setup  For the VQA dataset, we use the Adam solver with βN = 0.N, βN = 0.NN
The base learning rate is set to 0.000N and decays every N0,000 iterations using an exponential rate of  0.N
We terminate training at N00,000 iterations (N00,000  iterations if the training set is augmented with the largescale Visual Genome dataset [NN])
Dropouts are used after  each LSTM layer (dropout ratio p = 0.N) and MFB module (p = 0.N) like [N]
The number of answers N = N000
For all experiments (except for the ones shown in Table N,  which use the train and val splits together as the training set  like the comparative approaches), we train on the train split,  validate on the val split, and report the results on the test  NNNN    (a) Standard (b) w/o power norm
(c) w/o ℓN norm
(d) w/o power and ℓN norms
 Figure N
The evolution of the output distribution of one typical neuron with different normalization settings, shown as {NN,N0,NN}th percentiles
Both normalization techniques, especially the ℓN normalization make the neuron values restricted within a narrow range, thus  leading to a more stable model
Best viewed in color
 Table N
Overall accuracies and model sizes of approaches and on  the test-dev set of the Open-Ended task
The reported accuracy is  the overall accuracy of all question types
The model size includes  the parameters for the LSTM networks
 Model Acc
Model Size  MCB[N] (d = NN000) NN.N NNM MLB[NN] (d = N000) NN.N NNM MFB(k = N, o = N000) N0.N NNM MFB(k = N, o = N000) N0.N NNM MFB(k = N0, o = N00) N0.N NNM MFB(k = N, o = N00) NN.N NNM MFB(k = N, o = N00) N0.N NNM MFB(k = N, o = N000) N0.N NNM MFB(k = N, o = N000) N0.N N0NM MFB(k = N, o = N000) - - -w/o power norm
N0.N -w/o ℓN norm
NN.N - -w/o power and ℓN norms
NN.N splitN
The batch size is set to N00 for the models without  the attention mechanism, and set to NN for the models with  attention (due to GPU memory limitation)
All experiments  are implemented with the Caffe toolbox [N0] and performed  on a workstation with GTX N0N0 GPUs
 N.N
Ablation Analysis  In Table N, we compare MFB’s performance with other  state-of-the-art bilinear pooling models, namely MCB [N]  and MLB (for fair comparison, we replace the tanh function  in MLB with the proposed power+ℓN normalizations ) [NN], under the same experimental settings
None of these  methods introduce the attention mechanism
Furthermore,  we explore different hyper-parameters and normalizations  introduced in MFB to explore why MFB outperform the  compared bilinear models
 From Table N, we can see that:  Nthe submission attempts for the test-standard split are strictly limited
 Therefore, we evaluate most of our settings on the test-dev split and only  report the best results on the test-standard split
 First, MFB significantly outperforms MCB and MLB
 With N/N parameters, MFB(k = N, o = N000) achieves about a N% accuracy improvement compared with MCB
Moreover, with only N/N parameters , MFB(k = N, o = N00) obtains similar results to MCB
These characteristics allows us to train our model on a memory limited GPU  with larger batch-size
Furthermore, the validation accuracy  of MCB gradually falls after N0,000 iterations, indicating  that it suffers from overfitting with the high-dimensional  output features
In comparison, the performance of our  MFB model is relatively robust
 Second, when ko is fixed to a constant, e.g., N000, the number of factors k affects the performance
Increasing k from N to N, produces a 0.N% performance gain
When k = N0, the performance has approached saturation
This phenomenon can be explained by the fact that a large k cor- responds to using a large window to sum pool the features,  which can be treated as a compressed representation and  may loss some information
When k is fixed, increasing o does not produce further improvements
This suggests that  high-dimensional output features may be easier to overfit
 Similar results can be seen in [N]
In summary, k = N and o = N000 may be a suitable combination for our MFB model on the VQA dataset, so we use these settings in our  follow-up experiments
 Finally, both the power and ℓN normalization bene- fit MFB performance
Power normalization results in  about 0.N% improvement and ℓN normalization, perhaps surprisingly, results in about N% improvement
Results without ℓN and power normalizations were also reported in [N] and are similar to those reported here
To explain  why normalization are so important, we randomly choose  one typical neuron from the MFB output feature before  normalization to illustrate how its distribution evolves over  time in Figure N
It can be seen that the standard MFB  model (with both normalizations) leads to the most stable  neuron distribution and without the power normalization,  about N0,000 iterations are needed to achieve stabilization
 Without the ℓN normalization, the distribution varies seriNNNN    Table N
Open-Ended (OE) and Multiple-Choice (MC) results on VQA dataset compared with the state-of-the-art approaches in terms of  accuracy in %
Att
indicates whether the approach introduce the attention mechanism, W.E
indicates whether the approach uses external  word embedding models
VG indicates the model is trained with the Visual Genome dataset additionally
All the reported results are  obtained with a single model
For the test-dev set, the best results in each split are bolded
For the test-standard set, the best results overall  all the splits are bolded
 Model Att
W.E
Test-dev Test-standard  OE MC OE MC  All Y/N Num Other All All Y/N Num Other All  iBOWIMG [NN] NN.N NN.N NN.0 NN.N - NN.N NN.N NN.0 NN.N NN.0  DPPnet [NN] NN.N N0.N NN.N NN.N - NN.N N0.N NN.N NN.N VQA team [N] NN.N N0.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N  AYN [N0] NN.N NN.N NN.N NN.N - NN.N NN.N NN.N NN.N AMA [NN] NN.N NN.0 NN.N NN.N - NN.N NN.N NN.N NN.N DMN+ [NN] N0.N N0.N NN.N N0.N - N0.N - - - MCB [N] NN.N NN.N NN.N NN.0 - NN.N NN.N NN.N NN.0 MRN [NN] NN.N NN.N NN.N NN.N - NN.N NN.N NN.N NN.N NN.N  MFB (Ours) NN.N NN.N NN.N NN.N NN.N - - - - SMem [NN] X NN.0 N0.N NN.N NN.N - NN.N N0.N NN.N NN.N NMN [N] X NN.N NN.N NN.0 NN.0 - NN.N NN.N NN.N NN.0 SAN [NN] X NN.N NN.N NN.N NN.N - NN.N - - - FDA [N] X NN.N NN.N NN.N NN.N - NN.N - - - DNMN [N] X NN.N NN.N NN.N NN.N - NN.N - - - HieCoAtt [NN] X NN.N NN.N NN.N NN.N NN.N NN.N - - - RAU [NN] X NN.N NN.N NN.0 NN.0 NN.N NN.N NN.N NN.N NN.N NN.N  MCB+Att [N] X NN.N NN.N NN.N NN.N - - - - - DAN [NN] X NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.0  MFB+Att (Ours) X NN.N NN.N NN.N NN.N NN.N - - - - MFB+CoAtt (Ours) X NN.N NN.N NN.N NN.N N0.0 - - - - MCB+Att+GloVe [N] X X NN.N NN.N NN.N NN.N - - - - - MLB+Att+StV [NN] X X NN.N NN.N NN.N NN.N - NN.N NN.0 NN.N NN.N NN.N  MFB+CoAtt+GloVe (Ours) X X NN.N NN.0 NN.N NN.N N0.N NN.N NN.N NN.N NN.N N0.N  MCB+Att+GloVe+VG [N] X X NN.N NN.N NN.N NN.N - - - - - MLB+Att+StV+VG [NN] X X NN.N NN.N NN.N NN.N - - - - - MFB+CoAtt+GloVe+VG (Ours) X X NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  ously over the entire training course
This observation is  consistent with the results shown in Table N
 N.N
Comparison with State-of-the-art  Table N compares our approaches with the current stateof-the-art
The table is split into four parts over the rows:  the first summarizes the methods without introducing the  attention mechanism; the second includes the methods with  attention; the third illustrates the results of approaches  with external pre-trained word embedding models, e.g.,  GloVe [NN] or Skip-thought Vectors (StV) [NN]; and the last  includes the models trained with the external large-scale  Visual Genome dataset [NN] additionally
To best utilize  model capacity, the training data set is augmented so that  both the train and val splits are used as the training set, result  in about N% ∼ N% overall accuracy improvement on the OE task
Also, to better understand the question semantics,  pre-trained GloVe word vectors are concatenated with the  learned word embedding
The MFB model corresponds to  the MFB baseline model
The MFB+Att model indicates  the model that replaces the MCB with our MFB in the  MCB+Att model [N]
The MFB+CoAtt model represents  the network shown in Figure N
 From Table N, we have the following observations:  First, the model with MFB outperforms other comparative approaches significantly
The MFB baseline outperforms all other existing approaches without the attention  mechanism for both the OE and MC tasks, and even  surpasses some approaches with attention
When attention  is introduced, MFB+Att consistently outperforms current  next-best model MCB+Att, highlighting the efficacy and  robustness of the proposed MFB
 Second, the co-attention model further improve the performance over the attention model with only considering  the image attention
By introducing co-attention learning,  MFB+CoAtt delivers a 0.N% improvement on the OE task compared with the MFB+Att model in terms of overall accuracy, indicating the additional benefits of the co-attention  learning framework
 Finally, with the external pre-trained GloVe model and  the Visual Genome dataset, the performance of our models  are further improved
The MFB+CoAtt+GloVe+VG model  NNNN    Q: what color are the  cats eyes A: yellow   P: yellow  what color are the  cats eyes ?  Q: what color is the  catchers pants A: black     P: white  what color is the  catchers pants  A: yellow   P: yellow cats eyes   what color is the   A: black     white  what color is the  pants  Q: how many birds  are flying A: N    P: N  how many birds are  flying  Q: how many flags  are shown A: N     P: N  how many flags are  shown  is the boy on the  bottom playing left  handed  Q: is the man smiling A: yes     P: yes  is the man smiling  is the boy on the  bottom playing handed  Q: is the boy on the bottom  playing left handed A: yes      P: no  Q: what is on the  floor A: cat     P: cat  what is on the floor  Q: what are the red  things A: meat   P: tomatoes  what are the red  things    P  P  P  P        Figure N
Typical examples of the learned image and question of the MFB+CoAtt+GloVe model
The top row shows four examples of four  correct predictions while the bottom row shows four incorrect predictions
For each example, the query image, question (Q), answer (A)  and prediction (P) are presented from top to bottom; the learned image and question attentions are presented next to them
The brightness  of images and darkness of words represent their attention weights
 Table N
Comparison with the state-of-the-art results (with model  ensemble) on the test-standard set of the VQA dataset
The best  results are bolded
Model OE MC  All Y/N Num Other All  HieCoAtt [NN] NN.N N0.0 NN.N NN.0 NN.N  RAU [NN] NN.N NN.N NN.0 NN.N NN.N  N MCB models [N] NN.N NN.N NN.N NN.0 N0.N  N MLB models [NN] NN.N NN.N NN.N NN.N N0.N  N MFB models (Ours) NN.N NN.N N0.N NN.N NN.N  Human [N] NN.N NN.N NN.N NN.N NN.N  significantly outperforms the best reported results with a  single model on both the OE and MC task
 In Table N, we compare our approach with the stateof-the-art methods with model ensemble
Similar with  [N, NN], we train N individual MFB+CoAtt+GloVe models  and average the prediction scores of them
Four of the  seven models additionally introduce the Visual Genome  dataset [NN] into the training set
For fair comparison,  only the published results are demonstrated
From Table  N, the ensemble of MFB models outperforms the next best  approach by N.N% on the OE task and by N.N% on the MC task respectively
Finally, compared with the results  obtained by human, there is still a lot of room for the  improvement to approach the human-level
 To better demonstrate the effects of co-attention learning, in Figure N we visualize the learned question and  image attentions of some examples from the validation  set
The examples are randomly picked from different  question types
It can be seen that the learned question  and image attentions are usually closely focus on the key  words and the most relevant image regions
From the  incorrect examples, we can also draw conclusions about the  weakness of our approach, which are perhaps common to  all VQA approaches: N) some key words in the question are  neglected by the question attention module, which seriously  affects the learned image attention and final predictions  (e.g., the word catcher in the first example and the word  bottom in the third example); N) even the intention of  the question is well understood, some visual contents are  still unrecognized (e.g., the flags in the second example)  or misclassified (the meat in the fourth example), leading  to the wrong answer for the counting problem
These  observations are useful to guide further improvements for  VQA in the future
 N
Conclusions  In this paper, we develop a Multi-modal Factorized Bilinear pooling (MFB) approach to fuse multi-modal features  for the VQA task
Compared with existing bilinear pooling  methods, the MFB approach achieves significant performance improvement for the VQA task
Based on MFB, we  design a network architecture with co-attention learning that  achieves new state-of-the-art performance on the real-world  VQA dataset
This explorations of multi-modal bilinear  pooling and co-attention learning are applicable to a wide  range of tasks involving multi-modal data
 N
Acknowledgement  This work was supported in part by the National Natural  Science Foundation of China under Grants NNNNNN0N,  NNN0NNNN and NNNNNNN0, the Zhejiang Provincial Natural  Science Foundation of China under Grant LRNNF0N000N,  the Australian Research Council under Project FLNN0N00NNN, DP-NN0N0NNNN, and LP-NN0N00NNN
 NNNN    References  [N] J
Andreas, M
Rohrbach, T
Darrell, and D
Klein
Learning  to compose neural networks for question answering
arXiv  preprint arXiv:NN0N.0NN0N, N0NN
N, N  [N] J
Andreas, M
Rohrbach, T
Darrell, and D
Klein
Neural  module networks
In IEEE Conference on Computer Vision  and Pattern Recognition (CVPR), pages NN–NN, N0NN
N  [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra,  C
Lawrence Zitnick, and D
Parikh
Vqa: Visual question  answering
In International Conference on Computer Vision  (ICCV), pages NNNN–NNNN, N0NN
N, N, N, N, N, N, N  [N] K
Chen, J
Wang, L.-C
Chen, H
Gao, W
Xu, and  R
Nevatia
Abc-cnn: An attention based convolutional  neural network for visual question answering
arXiv preprint  arXiv:NNNN.0NNN0, N0NN
N  [N] J
Donahue, L
Anne Hendricks, S
Guadarrama,  M
Rohrbach, S
Venugopalan, K
Saenko, and T
Darrell
 Long-term recurrent convolutional networks for visual  recognition and description
In IEEE Conference on  Computer Vision and Pattern Recognition (CVPR), pages  NNNN–NNNN, N0NN
N  [N] A
Fukui, D
H
Park, D
Yang, A
Rohrbach, T
Darrell,  and M
Rohrbach
Multimodal compact bilinear pooling  for visual question answering and visual grounding
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
N, N, N, N, N, N, N  [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
arXiv preprint arXiv:NNNN.0NNNN,  N0NN
N  [N] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural computation, N(N):NNNN–NNN0, NNNN
N  [N] I
Ilievski, S
Yan, and J
Feng
A focused dynamic  attention model for visual question answering
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
N, N  [N0] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long,  R
Girshick, S
Guadarrama, and T
Darrell
Caffe:  Convolutional architecture for fast feature embedding
In  ACM International Conference on Multimedia (ACMMM),  pages NNN–NNN, N0NN
N  [NN] J.-H
Kim, S.-W
Lee, D
Kwak, M.-O
Heo, J
Kim, J.-W
 Ha, and B.-T
Zhang
Multimodal residual learning for visual  qa
In Advances in neural information processing systems  (NIPS), pages NNN–NNN, N0NN
N, N  [NN] J.-H
Kim, K
W
On, J
Kim, J.-W
Ha, and B.-T
Zhang
 Hadamard product for low-rank bilinear pooling
arXiv  preprint arXiv:NNN0.0NNNN, N0NN
N, N, N, N, N, N  [NN] R
Kiros, Y
Zhu, R
R
Salakhutdinov, R
Zemel, R
Urtasun,  A
Torralba, and S
Fidler
Skip-thought vectors
In  Advances in neural information processing systems (NIPS),  pages NNNN–NN0N, N0NN
N  [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, et al
 Visual genome: Connecting language and vision using  crowdsourced dense image annotations
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N, N  [NN] Y
Li, N
Wang, J
Liu, and X
Hou
Factorized  bilinear models for image recognition
arXiv preprint  arXiv:NNNN.0NN0N, N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona,  D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco:  Common objects in context
In European Conference on  Computer Vision (ECCV), pages NN0–NNN, N0NN
N  [NN] T.-Y
Lin, A
RoyChowdhury, and S
Maji
Bilinear cnn  models for fine-grained visual recognition
In International  Conference on Computer Vision (ICCV), pages NNNN–NNNN,  N0NN
N  [NN] J
Lu, J
Yang, D
Batra, and D
Parikh
Hierarchical  question-image co-attention for visual question answering
 In Advances in neural information processing systems  (NIPS), pages NNN–NNN, N0NN
N, N, N, N, N  [NN] M
Malinowski and M
Fritz
A multi-world approach  to question answering about real-world scenes based on  uncertain input
In Advances in neural information  processing systems (NIPS), pages NNNN–NNN0, N0NN
N, N  [N0] M
Malinowski, M
Rohrbach, and M
Fritz
Ask your  neurons: A neural-based approach to answering questions  about images
In International Conference on Computer  Vision (ICCV), pages N–N, N0NN
N  [NN] H
Nam, J.-W
Ha, and J
Kim
Dual attention networks  for multimodal reasoning and matching
arXiv preprint  arXiv:NNNN.00NNN, N0NN
N, N, N  [NN] H
Noh and B
Han
Training recurrent answering units  with joint loss minimization for vqa
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N  [NN] H
Noh, P
Hongsuck Seo, and B
Han
Image question  answering using convolutional neural network with dynamic  parameter prediction
In IEEE Conference on Computer  Vision and Pattern Recognition (CVPR), pages N0–NN, N0NN
 N  [NN] J
Pennington, R
Socher, and C
D
Manning
Glove: Global  vectors for word representation
In EMNLP, volume NN,  pages NNNN–NNNN, N0NN
N  [NN] S
Rendle
Factorization machines
In International  Conference on Data Mining (ICDM), pages NNN–N000, N0N0
 N  [NN] K
Saito, A
Shin, Y
Ushiku, and T
Harada
Dualnet:  Domain-invariant network for visual question answering
 arXiv preprint arXiv:NN0N.0NN0N, N0NN
N  [NN] K
J
Shih, S
Singh, and D
Hoiem
Where to look: Focus  regions for visual question answering
In IEEE Conference  on Computer Vision and Pattern Recognition (CVPR), pages  NNNN–NNNN, N0NN
N  [NN] J
B
Tenenbaum and W
T
Freeman
Separating style and  content
Advances in neural information processing systems  (NIPS), pages NNN–NNN, NNNN
N  [NN] P
Wang, Q
Wu, C
Shen, A
v
d
Hengel, and A
Dick
 Explicit knowledge-based reasoning for visual question  answering
arXiv preprint arXiv:NNNN.0NNN0, N0NN
N, N  [N0] P
Wang, Q
Wu, C
Shen, A
v
d
Hengel, and A
Dick
 Fvqa: Fact-based visual question answering
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N  [NN] Q
Wu, P
Wang, C
Shen, A
Dick, and A
van den Hengel
 Ask me anything: Free-form visual question answering  based on knowledge from external sources
In IEEE  Conference on Computer Vision and Pattern Recognition  (CVPR), pages NNNN–NNN0, N0NN
N, N, N  NNNN    [NN] C
Xiong, S
Merity, and R
Socher
Dynamic memory  networks for visual and textual question answering
In  International Conference on Machine Learning (ICML),  pages NNNN–NN0N, N0NN
N  [NN] H
Xu and K
Saenko
Ask, attend and answer: Exploring  question-guided spatial attention for visual question answering
In European Conference on Computer Vision (ECCV),  pages NNN–NNN, N0NN
N  [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
C
Courville,  R
Salakhutdinov, R
S
Zemel, and Y
Bengio
Show,  attend and tell: Neural image caption generation with visual  attention
In International Conference on Machine Learning  (ICML), volume NN, pages NN–NN, N0NN
N, N  [NN] Y
Yang, Y.-T
Zhuang, F
Wu, and Y.-H
Pan
Harmonizing  hierarchical manifolds for multimedia document semantics  understanding and cross-media retrieval
IEEE Transactions  on Multimedia, N0(N):NNN–NNN, N00N
N  [NN] Z
Yang, X
He, J
Gao, L
Deng, and A
Smola
Stacked  attention networks for image question answering
In IEEE  Conference on Computer Vision and Pattern Recognition  (CVPR), pages NN–NN, N0NN
N, N  [NN] Z
Yu, F
Wu, Y
Yang, Q
Tian, J
Luo, and Y
Zhuang
 Discriminative coupled dictionary hashing for fast crossmedia retrieval
In International Conference on Research on  Development in Information Retrieval (SIGIR), pages NNN–  N0N, N0NN
N  [NN] Z
Zhao, Q
Yang, D
Cai, X
He, and Y
Zhuang
 Video question answering via hierarchical spatio-temporal  attention networks
International Joint Conference on  Artificial Intelligence (IJCAI), N0NN
N  [NN] B
Zhou, Y
Tian, S
Sukhbaatar, A
Szlam, and R
Fergus
 Simple baseline for visual question answering
arXiv  preprint arXiv:NNNN.0NNNN, N0NN
N, N, N  [N0] L
Zhu, Z
Xu, Y
Yang, and A
G
Hauptmann
Uncovering  temporal context for video question and answering
arXiv  preprint arXiv:NNNN.0NNN0, N0NN
N  NNN0PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise R-FCN   PPR-FCN: Weakly Supervised Visual Relation Detection via Parallel Pairwise  R-FCN  Hanwang Zhang†, Zawlin Kyaw‡, Jinyang Yu†, Shih-Fu Chang†  †Columbia University, ‡National University of Singapore  {hanwangzhang, kzl.zawlin, yjyNNNNNN}@gmail.com; shih.fu.chang@columbia.edu  Abstract  We aim to tackle a novel vision task called Weakly  Supervised Visual Relation Detection (WSVRD) to detect  “subject-predicate-object” relations in an image with object relation groundtruths available only at the image level
 This is motivated by the fact that it is extremely expensive to  label the combinatorial relations between objects at the instance level
Compared to the extensively studied problem,  Weakly Supervised Object Detection (WSOD), WSVRD is  more challenging as it needs to examine a large set of regions pairs, which is computationally prohibitive and more  likely stuck in a local optimal solution such as those involving wrong spatial context
To this end, we present a  Parallel, Pairwise Region-based, Fully Convolutional Network (PPR-FCN) for WSVRD
It uses a parallel FCN architecture that simultaneously performs pair selection and  classification of single regions and region pairs for object  and relation detection, while sharing almost all computation shared over the entire image
In particular, we propose  a novel position-role-sensitive score map with pairwise RoI  pooling to efficiently capture the crucial context associated  with a pair of objects
We demonstrate the superiority of  PPR-FCN over all baselines in solving the WSVRD challenge by using results of extensive experiments over two visual relation benchmarks
 N
Introduction  Visual relation detection (VRD) aims to detect objects  and predict their relationships in an image, especially  subject-predicate-object triplets like personhold-ball (verb), dog-on-sofa (spatial), car-withwheel (preposition), and personN-taller-personN  (comparative) [N0]
As an intermediate task between lowlevel object detection [NN] and high-level natural language  modeling [NN], VRD has received increasing attention recently, in areas of new benchmarks [NN, N0], algorithms [NN,  NN, N, NN], and visual reasoning [NN, NN, NN]
VRD is exSki Under Person  Person NextTo Person  Person Wear  Helmet  Image-level  Annotation  Region Proposals WSOD  WSPP  Input Image WSODSub./Obj
WSPP  Predicate  WSVRD Training WSVRD Testing  Figure N
The illustration of WSVRD training and testing stages
It  includes weakly supervised object detection (WSOD) and weakly  supervised predicate prediction (WSPP)
Note that the key difference from WSOD is that WSVRD requires pairwise region modeling for many weakly labeled region pairs in WSPP
 pected to become an important building block for the connection between vision and language
 Like any other visual detection task, VRD is also datahungry
However, labeling high-quality relation triplets is  much more expensive than objects as it requires the tedious inspection of a combinatorial number of object interactions
On the other hand, collecting image-level relation annotation is relatively easier
For example, there are  abundant image-caption data [N0, NN] and Web image-text  pairs [N0], where image-level relation descriptions can be  automatically extracted from the text using state-of-the-art  text parsers [NN, N]
Therefore, to make VRD of practical  use at a large scale, it is necessary to study the novel and  challenging task: weakly supervised visual relation detection (WSVRD), with triplet annotation available only at the  image level
 Figure N shows the WSVRD problem studied in this paper
As there are no instance-level object annotations (e.g.,  bounding boxes), we first exploit region proposal generators [N0, NN] for a set of candidate proposals (or RoIs) and  then predict their object classes
This step is also known  as Weakly Supervised Object Detection (WSOD) [N, N]
 Then, as the image-level relation does not specify which  pairs of objects are related, it exhaustively enumerates every RoI pairs as candidate subject-object pairs for  predicate prediction (e.g., relationships), which results  in that WSVRD is more challenging than WSOD
More  specifically, first, as the spatial context annotation of pairwise regions is missing, we should carefully model the spaNNNNN    tial constraints in WSVRD otherwise the relationships will  be easily confused by incorrect subject-object configurations, e.g., one can detect hat-on-person correctly  but the hat is on someone else; second, for N regions,  WSVRD has to scan through O(NN) region pairs, thus, the weakly supervised learning based on alternating between instance selection and classification (i.e., predicate  prediction) in WSVRD is more easily trapped in bad local optimal solution than that in WSOD [NN]; third, the  O(NN) computational cost in WSVRD would become pro- hibitively expensive if per-RoI fully-connected subnetwork  is still adopted [NN, NN], since WSVRD usually uses many  more object regions (e.g., >N00 regions and >N0,000 pairs)  than supervised VRD (e.g., <N0 regions and <N00 pairs) to  ensure high recall of object instances
 We present a Parallel, Pairwise Region-based, end-toend Fully Convolutional Network (PPR-FCN) to tackle the  above challenges in WSVRD
The architecture is illustrated  in Figure N and detailed in Section N
It consists of a  WSOD module for weakly supervised object detection and  a Weakly Supervised Predicate Prediction (WSPP) module  for weakly supervised region pair modeling
PPR-FCN is  a two-branch parallel network, inspired by the recent success of using parallel networks to avoid bad local optima in  WSOD [N]
We use FCN [NN] as our backbone network to  exploit its advantages in sharing computation over the entire image, making efficient pairwise score estimation possible [NN, NN]
The WSPP module has two novel designs:  N
Position-Sequence-Sensitive Score Map
Inspired by  the position-sensitive score map in R-FCN [NN], we develop a set of position-role-sensitive conv-filters to generate a score map, where every spatial pixel encodes the object class-agnostic spatial context (e.g., subject is above  object for predicate sit on) and roles (e.g., the first  part of the pixel channels is subject and the rest is  object)
 N
Pairwise RoI Pooling
To shepherd the training of the  position-role-sensitive conv-filters, we append a pairwise  RoI pooling layer on top of the score map for fast score estimation
Our pooling design preserves the spatial context  and subject/object roles for relations
 To the best of our knowledge, PPR-FCN is the first detection network for the WSVRD task
We believe that PPRFCN will serve as a critical foundation in this novel and  challenging vision task
 N
Related Work  Fully Convolutional Networks
A recent trend in deep  networks is to use convolutions instead of fully-connected  (fc) layers such as ResNets [NN] and GoogLeNet [NN]
Different from fc layers where the input and output are fixed  size, FCN can output dense predictions from arbitrarysized inputs
Therefore, FCN is widely used in segmentation [NN, NN], image restoration [N], and dense object detection windows [NN]
In particular, our PPR-FCN is inspired  by another benefit of FCN utilized in R-FCN [NN]: per-RoI  computation can be shared by convolutions
This is appealing because the expensive computation of pairwise RoIs is  replaced by almost cost-free pooling
 Weakly Supervised Object Detection
As there are no  instance-level bounding boxes for training, the key challenge of WSOD is to localize and classify candidate RoIs  simultaneously [N, NN, NN, NN]
The parallel architecture in  PPR-FCN is inspired by the two-branch network of Bilen  and Vedaldi [N], where the final detection score is a product  of the scores from the parallel localization and classification branches
Similar structures can be also found in et  al
[NN, NN]
Such parallel design is different from MIL [N0]  in a fundamental way as regions are selected by a localization branch, which is independent of the classification  branch
In this manner, it helps to avoid one of the pitfalls  of MIL, namely the tendency of the method to get stuck in  local optima
 Visual Relation Detection Modeling the interactions  between objects such as verbs [NN, N], actions [NN, NN, NN],  and visual phrases [NN, N, NN, N] are not new in literature
 However, we are particularly interested in the VRD that  simultaneously detects generic subject-predicateobject triplets in an image, which is an active research  topic [NN, NN, NN, NN, N, NN] and serves as a building block  for connecting vision and language [N0, NN, NN, NN]
But,  a key limitation is that it is very expensive to label relation triplets as the complexity is combinatorial
Perhaps  the most related work to ours is done by Prest et al
[NN]  on weakly-supervised learning human and object interactions
However, their spatial configurations and definitions  of relations are limited to one person and one object while  our relations include generic objects and diverse predicates
 There are recent works on referring expression groundings,  e.g., localizing an object by its relationship to another object [NN, NN, NN]
However, they require stronger supervision, i.e., at least one of the objects is labeled with bounding  box
We also notice that we are not the only work towards  the efficiency of VRD
Li et al
[NN] and Zhang et al
[NN]  proposed to use groundtruth pairwise bounding boxes to  learn triplet proposals to reduce the number of region pairs;  however, these methods are fully supervised
 N
PPR-FCN  As illustrated in Figure N, PPR-FCN consists of two  modules: N) WSOD module for object detection and N)  WSPP module for predicate prediction
At test time, PPRFCN first detects a set of objects and then predicts the predicate for every object pairs
In this section, we will detail  each module
 NNNN    WSOD Module  Subject  FCN  Object FCN  Pos.-Role  Sensitive  Score Map  Pairwise RoI Pooling  Subject  FCN  Object FCN  Pos.-Role  Sensitive  Score Map  Pairwise RoI Pooling  S r sel  S r cls  Sr  Sel 
  Bra nch  Cls
Branch  WSPP Module  RoIs  Input Image  NMSSc RoIo Ss So  Output  Sperson Shold Sumbrella Shat Son Sperson  = 0.N  = 0.N  Figure N
The overview of the proposed PPR-FCN architecture for WSVRD
It has two modules: WSOD for object detection (Section N.N)  and WSPP for predicate prediction (Section N.N), each module is composed by a pair selection branch and a classification branch
 N.N
WSOD Module  The goal of WSOD module is to predict the object class  score Sc(P ) for a RoI P of any class c ∈ {0, N, ..., C}
Then, NMS is performed to generate the final detection result: subject and object RoIs (i.e., bounding boxes), and  their classes
It is worth noting that any state-of-the-art  WSOD method can be used as the WSOD module in PPRFCN
In this paper, we adopt a parallel design similar in  WSDDN [N] as it achieves the state-of-the-art results on  benchmarks (cf
Section N.N) and it is easy to replace the  backbone network with R-FCN [NN], which is also compatible with the subsequent WSPP module
During training and test, we first use EdgeBox [N0] to generate N object RoIs, where N is initially N,000 and then reduced to  N,000 by NMS with IoU>0.N and discard those with objectness score<0.N; the objectness score for a region is the  sum over all class scores from a N-epoch pre-trained WSOD  with the initial N,000 RoIs
Then, given the N,000 RoIs,  for each class, we perform NMS with IoU>0.N and score  threshold>0.N to select NN∼N0 RoIs, resulting ∼N00 de- tected objects, where this number is significantly larger than  that in supervised detection (e.g.,∼N0), since we need to en- sure enough recall for true objects
Please see Section N.N  for the training loss of this module
 N.N
WSPP Module  WSPP module predicts the predicate score Sr(Pi, Pj) of any predicate r ∈ {N, ..., R} for two RoIs detected by the previous WSOD module
As shown in Figure N, it is  a two-branch network with independent parameters for pair  selection (i.e., which pair of regions are related) and classification
In particular, the input feature map for WSPP is  the same as WSOD, which is the base CNN feature map  followed by a trainable conv-layer as in R-FCN [NN]
The  predicate score, i.e., the likelihood of subject-object  pair being associated with predicate r, is defined as:  Sr(Pi, Pj) = S sel r (Pi, Pj) · S  cls r (Pi, Pj), (N)  where we split the challenging estimation of the predicate  score using only image-level annotation into two simpler  problems: one is responsible for pair selection and the  other is for predicate classification
In particular, Sselr (or  Sclsr ) is the predicate score from the selection (or classification) branch
Sselr is softmax normalized over all possible region pairs with respect to a predicate class, i.e.,  Sselr (Pi, Pj) ← softmaxi,jS sel r (Pi, Pj); while S  cls r is softmax normalized over possible predicate classes for a region pair, i.e., Sclsr (Pi, Pj) ← softmaxrS cls r (Pi, Pj)
Note  that such normalizations assign different objectives to two  branches and hence they are unlikely to learn redundant  models [N]
Essentially, the normalized selection score can  be considered as a soft-attention mechanism used in weakly  supervised vision tasks [NN, N] to determine the likely RoIs
 Next, we will introduce how to calculate the scores before  normalization
Without loss of generality, we use Sclsr as  the example and discard the superscript
 N.N.N Position-Sequence-Sensitive Score Map  First, predicate score should be position-sensitive as the  spatial context of two objects is informative for the relationship
Second, as the predicate score is usually dependent on  the role-sequence of two RoIs, the score should be also rolesensitive to ensure asymmetric scores of Sr(Pi, Pj) and Sr(Pj , Pi)
For example, for ride score, person-ride- bike is more likely than bike-ride-person; personon-bike is different from bike-on-person, as the former usually indicates “person riding a bike” while the latter  suggests “person carrying a bike”
Inspired by the positionsensitive score map design in R-FCN [NN], we propose to  use two sets of trainable size N×N and stride N conv-filters to generate N ·kNR-channel position-role-sensitive score maps from the input feature map
As illustrated in Figure N, the  first kNR-channel score map encodes R predicate scores  at kN spatial positions for subject and the second kNRchannel map encodes scores for object
By using these  filters, the computation of predciate prediction is amortized  over the entire image
Note that the score maps are classagnostic, i.e., they are only aware of whether a spatial location is subject or object but not aware of whether  NNNN    it is “dog” or “car”
This is scalable to relation detection  for many classes and predicates as the complexity is only  O(C +R) but not O(CNR)
 N.N.N Pairwise RoI Pooling  To sheperd the training of the above position-role-sensitive  filters, we design a pairwise RoI pooling strategy to obtain  the predicate score Sr(Pi, Pj) for a RoI pair
It includes three pooling steps: N) subject pooling, N) object pooling,  and N) joint pooling
Thus, the final Sr(Pi, Pj) is the sum of these steps:  Sr(Pi, Pj) = S sub r (Pi) + S  obj r (Pj) + S  joint r (Pi, Pj)
(N)  Next, we will detail the three pooling steps as illustrated in  Figure N
 Subject/Object Pooling
This pooling aims to score  whether an RoI is subject or object in a relation
 Without loss of generality, we use subject pooling as the  walk-through example
We first divide the RoI P into k×k spatial grids
Suppose (x, y) ∈ g(i, j) is the set of pix- els within the grid g(i, j) ∈ P , where N ≤ i, j ≤ k, and Xx,y,g(i,j),r is the score of the r-th predicate at the position  (x, y) inside grid g(i, j) in the subject score map X , then the subject pooling for P is defined as:  Ssubr (P ) = vote g(i,j)∈P  (  pool (x,y)∈g(i,j)  (  Xx,y,g(i,j),c )  )  , (N)  where k = N, pool(·) is mean pooling, and vote(·) is aver- age voting (e.g., average pooling for the scores of the grids)
 Ssubr (P ) is position-sensitive because Eq
(N) aggregates re- sponses for a spatial grid of RoI subject to the corresponding one from the kN maps (e.g., in Figure N left, the dark  red value pooled from the top-left grid of the RoI) and then  votes for all the spatial grids
Therefore, the training will  shepherd the kNR subject filters to capture subject position  in an image
 Joint Pooling
The above subject/object pooling does not  capture the relative spatial context of a predicate
Therefore, we use joint pooling to capture how two RoIs interacts with respect to a predicate
As shown in Figure N  right, different from the single RoI pooling where the kN  spatial grids are over the entire RoI, the pairwise RoI pooling is based on the grids over the joint region and the pooling result for the subject Pi or object Pj is from the  intersected grids between Pi (or Pj) and Pi ∪ Pj , where the latter joint RoI is divided into k × k spatial grids
Denote (x, y) ∈ g(i′, j′) as the pixel coordinates within the grid g(i′, j′) ∈ Pi ∪ Pj , where N ≤ i  ′, j′ ≤ k, and Xsx,y,g(i′,j′),r (or X  o x,y,g(i′,j′),r) is the score of the rth predicate at the position (x, y) within g(i′, j′) from the  kNR R  k x k x R subject object  Joint R oI  Vote Vote  Joint Pooling  Subject/Object  Pooling  Sc or  e M ap  Sc or  e M ap  Sub
RoI  Obj
RoI  Figure N
Illustrations of pairwise RoI pooling with kN(k = N) spatial grids and R predicates
Left: subject/object pooling for a  single RoI
Right: joint pooling
For each score map, we use kN  colors to represent different position channels
Each color channel  has R predicate channels
For the joint pooling, uncolored pooling  results indicate zero and the back-propagation is disabled through  these grids
Note that the score maps in subject/object pooling and  joint pooling are different, i.e., there are N · N · kNR conv-filters
 subject (or object) score map
Therefore, the joint  RoI pooling is defined as:  Sjointr (Pi, Pj) = vote g(i′,j′)∈Pi∪Pj  (  pool (x,y)∈g(i′,j′)∩Pi  (  Xsx,y,g(i′,j′),r  )  + pool (x,y)∈g(i′,j′)∩Pj  (  Xox,y,g(i′,j′),r  )  )  ,  (N)  where g(i′, j′) ∩ Pi denotes the intersected pixels between g(i′, j′) and Pi; in particular, if g(i  ′, j′)∩Pi = φ, pool(·) is zero and the gradient is not back-propagated
We set k = N, pool(·) to average pooling, and vote(·) to average voting
For example, for relation person-ride-bike, the pooling result of person RoI is usually zero at the lower grids  of the joint RoIs while that of bike RoI is usually zero at  the upper grids
 N.N
Loss Functions  We follow the conventional image-centric training strategy [NN], i.e., a training mini-batch arises from the set  of region proposals in a single image
We resized images to the longer side of NN0 pixels
Multiple scales at  {0.N,0.N,N.0,N.N,N.N} and random horizontal flips are ap- plied to the images during training
 WSOD Loss
Suppose C is the set of image-level object class groundtruth, Sc =  ∑  i Sc(Pi) is the image-level class scoreN , the loss is defined as:  Lobjimg = −  C ∑  c=N  (  N[c∈C] logSc + N[c/∈C] log (N− Sc) )  , (N)  where N[x] is N if x is true and 0 otherwise
However,  the above image-level loss does not guarantee the spatial  NNote that Sc is also a sum of element-wise product of softmax normalized scores, i.e., Sc(Pi) = S loc c (Pi) · S  cls c (Pi) and thus it is < N
 NNNN    smoothness of detection scores
Inspired by the positive  and negative bounding box sampling in supervised object  detection [N0], we regularize the smoothness as: N) for each  foreground class c ∈ {N, ..., C}, the top high-scored regions (e.g., top N) and their neighborhood with IoU ≥ 0.N should both have high scores; we consider them as the pseudo positive regions; and N) the neighborhood of the pseudo positive regions with 0.N ≤ IoU ≤ 0.N should be pseudo back- ground regions (c = 0)
In this way, our spatial smoothness regularization loss is:  Lobjreg = − ∑  c∈C  ∑  i∈Cc  logSc(Pi)− ∑  i∈B  logS0(Pi), (N)  where Cc is the set of pseudo positive regions for class c N= 0, and B is the set of pseudo background regions
We follow a similar sampling strategy as in [NN]: NNN regions  are sampled, where at least NN% are the pseudo background  regions
 WSPP Loss
Suppose R is the set of the image-level relation groundtruth triplets, specifically, (s, r, o) ∈ R, where s, o ∈ {N, ..., C} are the labels of the subject and object
Suppose Cs and Co are the region sets of subject s and object o, respectively
Denote Sr = ∑  i∈Cs,j∈Co Sr(Pi, Pj) as the image-level predicate score,  the image-level predicate prediction loss is defined as:  Lpredimg =−  R ∑  r=N  (  N[(s,r,o)∈R] logSr+N[(s,r,o)/∈R] log(N−Sr) )  
 (N)  Overall Loss
The overall loss of PPR-FCN is a multi-task  loss that consists of the above WSOD and WSVRD losses:  LPPR−FCN = L obj img + L  pred img + αL  obj reg, (N)  where α is empirically set to 0.N
We train the PPR-FCN  model by SGD with momentum [NN]
 N
Experiments  N.N
Datasets  We used two recently released datasets with a wide range  of relation annotation
Every image from the above two  datasets is annotated with a set of subject-predicateobject triplets, where every instance pair of subject  and object is labeled with bounding boxes
At training  time, we discarded the object bounding boxes to conform  with the weakly supervised setting
 VRD: the Visual Relationships Dataset collected by Lu et  al
[NN]
It contains N,000 images with N00 object classes  and N0 predicates, resulting in NN,NNN relation annotations  with N,NNN unique relations and NN.NN predicates per object  class
We followed the official N,000/N,000 train/test split
 VG: the latest Visual Genome Version N.N relation dataset  constructed by Krishna et al
[N0]
VG is annotated by  crowd workers and thus the relations labeling are noisy,  e.g., free-language and typos
Therefore, we used the  pruned version provided by Zhang et al
[NN]
As a result,  VG contains NN,NNN images with N00 object categories and  N00 predicates, N,NNN,NNN relation annotations with NN,NNN  unique relations and NN predicates per object category
We  followed the same NN,N0N/NN,NNN train/test split
 N.N
Evaluation Protocols and Metrics  Since the proposed PPR-FCN has two modules: WSOD  and WSPP, we first evaluated them separately and then overall
Thus, we have the following protocols and metrics that  are used in evaluating one object detection task [N, N, NN]  and three relation-related tasks [NN, NN]:  N) Object Detection
We used the WSOD module trained  with image-level object annotations to detect objects in  VRD and VG
We followed the Pascal VOC conventions that a correct detection is at least 0.N IoU with the  groundtruth
 N) Predicate Prediction
Given the groundtruth objects  with bounding boxes, we predict the predicate class between every pair of regions
This protocol allows us to  study how well the proposed position-role-sensitive score  map and pairwise RoI pooling perform without the limitations of object detection
 N) Phrase Detection
We predict a relation triplet with a  bounding box that contains both subject and object
 The prediction is correct if the predicted triplet is correct and the predicted bounding box is overlapped with the  groundtruth by IoU>0.N
 N) Relation Detection
We predict a relation triplet with  the subject and object bounding boxes
The prediction is correct if the predicted triplet is correct and both of  the predicted subject and object bounding boxes are  overlapped with the groundtruth by IoU>0.N
 Note that both the objects and relations in VRD and VG  are not completely annotated
Therefore, the popular Average Precision is not a proper metric as the incomplete annotation will penalize the detection if we do not have that  particular groundtruthN
To this end, following [NN, NN],  we used Recall@N0 (R@N0) and Recall@N00 (R@N00)  for evaluation
R@K computes the fraction of times a  groundtruth is in the top K confident predictions in an image
 N.N
Evaluations of Object Detection  Comparing Methods
We compared the proposed  WSOD module named WSOD with three state-of-theNFor example, even though R-FCN is arguably better than than Faster  R-CNN, R-FCN only achieves N.NN% mAP while Faster-RCNN achieves  NN.NN% mAP on VRD
 NNNN    Table N
Weakly supervised object detection performances  (R@K%) of various methods on VRD and VG
The last row is  supervised object detection performances by R-FCN
Dataset VRD VG  Metric R@N0 R@N00 R@N0 R@N00  WSDDN [N] NN.0N NN.NN N.NN N.NN  ContextLocNet [NN] N.NN NN.NN N.NN N.NN  WSL [NN] NN.N0 NN.NN N.NN N.NN  WSOD NN.NN NN.NN N.NN N.NN  R-FCN [NN] NN.NN NN.NN NN.NN NN.N0  art weakly supervised object detection methods: N) WSDDN [N], the weakly-supervised deep detection network
 It has a two-branch localization and classification structure with spatial regularization; N) ContextLocNet [NN],  the context-aware localization network
Besides it is also a  two-branch structure, the localization branch is further subbranched to three context-aware RoI pooling and scoring  subnetworks; N) WSL [NN], the weakly supervised object  localization model with domain adaption
It is a two-stage  model
First, it filters out the noisy object proposal collection to mine confident candidates as pseudo object instances
Second, it learns a standard Faster-RCNN [NN]  using the pseudo instances
We used their official source  codes as implementations on the VRD and VG datasets in  this paper
For fair comparison, we used the same ResNetN0 [NN] as the base network
We also provided the fullysupervised detection model R-FCN [NN] as the object detection upper bound
 Results
From Table N, we can see that our WSOD is  considerably better than the state-of-the-art methods
This  is largely contributed by the parallel FCN architecture
It  is worth noting that the quality of the top N,000 proposal  RoIs is significant to WSOD; if we directly used the original scores of EdgeBox, the performance will drop significantly by about N points
Note that we are still far behind the fully supervised method such as R-FCN, which  shows that there is still a large space to improve WSVRD  by boosting WSOD
As illustrated in Figure N, WSOD usually detects the discriminative parts of objects, which is a  common failure in state-of-the-art models
We also compared WSOD with other methods on the completely annotated Pascal VOC N00N, where we also achieved the best  NN.N% mAP, surpassing WSDDN (NN.N%), ContextLocNet  (NN.N%), and WSL (NN.N%)
 N.N
Evaluations of Predicate Prediction  Comparing Methods
Note that the task of predicate  prediction is in the supervised setting given the groundtruth  of subject and object
Thus, we removed the WSOD  module and the localization branch from the WSPP module
In this experiment, our goal is to compare our proposed  position-role-sensitive score map and pairwise RoI pooling,  namely PosSeq+Pairwise with other three ablated methTable N
Predicate prediction performances (R@K%) of various  methods on VRD and VG
The last two rows are fc-based methods
Dataset VRD VG  Metric R@N0 R@N00 R@N0 R@N00  Pos+Pairwise NN.N0 NN.N0 NN.N0 NN.NN  Pos+JointBox NN.NN NN.NN NN.NN NN.NN  PosSeq+Pairwise NN.NN NN.NN NN.NN NN.NN  PosSeq+Pairwise+fc NN.NN NN.NN NN.NN NN.NN  VTransE [NN] NN.NN NN.NN NN.NN NN.NN  ods: N) Pos-Pairwise denoting position-sensitive score map  followed by pairwise RoI pooling; N) Pos+JointBox denoting position-sensitive score map followed by joint boxes  RoI pooling, where the joint RoI is the tight groundtruth  regions that cover both subject and object; and N)  PosSeq+Pairwise+fc denoting position-role-sensitive score  map followed by pairwise RoI pooling, but the score is obtained by fully-connected subnetworks
Note that this fcbased method is also comparable to N) VtransE [NN] using  the concatenated RoI features from subject and object  as the input to its fc prediction network
 Results
From Table N, we can see that our PosSeq+Pairwise outperforms the baselines with non-order  score maps and non-pairwise pooling significantly
As illustrated in Figure N, compared to the conventional positionsensitive score maps and pooling, we can observe that  PosSeq+Pairwise can capture the contextual configuration  better
For example, for the relation bus-on-road, the  subject responses are more active at upper positions  while the object response are more active at lower positions, and thus the spatial context of on is depicted by  adding the pairwise pooling; however, Pos+JointBox seems  agnostic to relations but more likely sensitive to objects
 It is worth noting that pooling-based methods are  worse than fc-based methods such as VTransE and  PosSeq+Pairwise+fc, which contains region-based fullyconnected (fc) subnetworks for relation modeling
We  noticed that some prepositions such as of and by, and  verbs such as play and follow, contain very diverse  visual cues and may not be captured by only spatial context
Therefore, fc layers followed by the concatenation  of subject and object RoI features might be necessary to model such high-level semantics
Nevertheless,  the fact that PosSeq+Pairwise+fc considerably outperforms  VTransE demonstrates the effectiveness of exploiting the  pairwise spatial context
Note that such unshared regionbased subnetworks will lead to inefficient learning in WSPP  as there are tens of thousands candidate RoI pairs and millions of fc parameters
 N.N
Evaluations of Phrase & Relation Detection  Comparing Methods
We evaluate the overall performance of PPR-FCN for WSVRD
We compared the following methods: N) GroundR [NN], a weakly supervised viNNNN    Figure N
Two illustrative examples (without the base ResNet-N0 fine-tuned) of N (k = N) position-role-sensitive score maps trained by pairwise RoI pooling (Subject and Object) and the position-sensitive score maps trained by joint boxes pooling (JointBox)
Dashed grids  are the joint RoI of subject and object
In Subject and Object score maps, subject and object RoIs are in solid rectangles
The  solid grids denotes pooling at corresponding positions
Note that position-role-sensitive pooling is defined as null if the RoI has no overlap  with the joint RoI at a position, e.g., road at top left
Please view in color and zoom in
 Figure N
Illustrative top N relation detection results from VRD
Red and green borders denote incorrect and correct detections, respectively
 Most of the failed cases are due to the wrongly detected objects
 sual phrase grounding method
We used the image-level  triplets as the input short phrases for their language embedding LSTM; N) VisualPhrase-WSDNN, its idea was  originally proposed in [NN] that considers a whole relation  triplet as a class label
As it can be reduced to a weakly  supervised object detection task, we used WSDDN [N]  pipeline to implement VisualPhrase
Note that our parallel FCN architecture cannot be adopted in VisualPhrase  since the number of relation classes is too large to construct  the conv-filters
N) VTransE-MIL, we followed the same  pipeline of VTransE [NN] but using the NoisyOR Multiple  Instance Learning (MIL) [N0] as the loss function for object and relation detections
N) PPR-FCN-single, we only  use the classification branch to implement PPR-FCN
We  also compared three fully supervised models as baselines  VTransE[NN], Lu’s-VLK [NN], and Supervised-PPR-FCN,  which is our proposed PPR-FCN applied in the supervised  setting
Note that GroundR and VisualPhrase are based  on joint boxes prediction and thus they can only perform  phrase detection
 Results Table N and N reports the phrase and relation detection performances of various methods
We have the following observations:  N) For phrase detection, GroundR and VisualPhraseWSDDN perform much poorly than VTransE-MIL and  PPR-FCN
The reason is two-fold
First, EdgeBox is not  designed to generate joint proposals of two interacted objects and thus we limited the number of proposals to N00  to handle N0,000 region pairs, where the top N00 proposals may be low recall of objects
Second, as discovered  in [NN, NN], once we consider the relation as a whole class  label, the training samples for each relation class are very  sparse, which will worsen the training of WSPP
 N) PPR-FCN outperforms VTransE-MIL in both phrase and  relation detections
The reason is that VTransE does not explicitly model the spatial context in relation modeling while  our PPR-FCN does
Note that this is crucial since the context can remove some incorrect subject-object configurations, especially when the supervision is only at the  image level
For example, Figure N shows that the positionrole-sensitive score map and pooling design in PPR-FCN  can correct misaligned subject and object when there  are multiple instances
 N) Our parallel design of PPR-FCN is significantly better  NNNN    Table N
Phrase detection performances (R@K%) of various methods in weakly supervised and supervised settings (bottom three  rows) on VRD and VG
Dataset VRD VG  Metric R@N0 R@N00 R@N0 R@N00  GroundR [NN] 0.NN 0.NN 0.NN 0.NN  VisualPhrase-WSDDN 0.NN 0.NN 0.NN 0.NN  VTransE-MIL N.0N N.NN N.NN N.0N  PPR-FCN-single N.NN N.NN 0.NN 0.NN  PPR-FCN N.NN N.NN N.NN N.NN  Lu’s-VLK [NN] NN.NN NN.0N – –  VTransE [NN] NN.NN NN.NN N.NN N0.NN  Supervised-PPR-FCN NN.NN NN.NN N0.NN NN.0N  than its counterpart PPR-FCN-single
This demonstrates  that for weakly supervised learning with many candidate  instances (i.e., region pairs), the parallel design without parameter sharing can prevent from bad solutions
 N) There is a large gap between WSVRD and supervised  VRD, e.g., PPR-FCN can only achieve less than a half of  the performance of supervised VRD such as SupervisedPPR-FCN and VTransE
We believe that the bottleneck is  mainly due to the WSOD module that tends to detect small  discriminative part instead of the whole object region
As  shown in Figure N, most of the failed relation detection is  due to the failure of object detection
For example, for large  and background-like objects such as mountain, sky and  building, only small regions are detected; for tower,  only the most discriminative “spire” is detected
 N) Even though the fully-connected subnetworks is very  helpful in predicate prediction as we discussed in Section N.N, Supervised-PPR-FCN can still outperform the fcbased VTransE due to the effectiveness of the pairwise RoI  pooling, which can correct wrong spatial context (Figure N)  Note that since PPR-FCN is designed for WSVRD, we  cannot remove bad RoI pairs using pairwise groundtruth  bounding boxes, which may lead to significant improvement in supervised settings [NN]
 N) Thanks to the FCN architecture introduced in PPR-FCN,  it can not only speed up the WSOD, but also efficiently handle tens of thousands region pairs in WSVRD
For example, as reported in Table N, PPR-FCN is about N× faster than VTransE-MIL using per-region fc subnetworks
It is  worth noting that the number of parameters of PPR-FCN  is much smaller that VTransE-MIL (e.g., millions of fc parameters) as we only have O(kN(C + N + R)) conv-filters
Our current bottleneck is mainly due to the EdgeBox [N0]  proposal generation time, as we strictly stick to the weak supervision setting that any module should not exploit bounding boxes
However, in practice, we can use generic classagnostic RPN [NN] to generate proposals in N00 ms/img
 N
Conclusion  We presented a parallel, pairwise region-based, fully  convolutional network: PPR-FCN, for the challenging task  Figure N
Qualitative examples of relation detection on VG
Compared to the results of PPR-FCN (solid green bounding boxes),  VTransE-MIL (dashed green bounding boxes) is more likely to  misalign subject to object if there are multiple instances of  subject
 Table N
Relation detection performances (R@K%) of various  methods in weakly supervised and supervised settings (bottom  three rows) on VRD and VG
Dataset VRD VG  Metric R@N0 R@N00 R@N0 R@N00  VTransE-MIL N.NN N.NN 0.NN 0.N0  PPR-FCN-single N.NN N.NN N.0N N.NN  PPR-FCN N.NN N.NN N.NN N.N0  Lu’s-VLK [NN] NN.NN NN.N0 – –  VTransE [NN] NN.0N NN.N0 N.NN N.0N  Supervised-PPR-FCN NN.NN NN.NN N.0N N.NN  Table N
Titan X GPU test time (ms/img) of the fc subnetwork  based weakly supervised method, VTransE-MIL and PPR-FCN  (excluding the proposal generation time cost by EdgeBox, which is  N00 ms/img)
Both VTransE-MIL and PPR-FCN adopts ResNetN0 as the base CNN and N00 detected object proposals, i.e., N0,000  region pairs for predicate prediction
VTransE-MIL PPR-FCN  NN0 NN0  of weakly supervised visual relation detection (WSVRD)
 PPR-FCN has two novel designs towards the optimization  and computation difficulties in WSVRD: N) PPR-FCN is a  parallel FCN network for simultaneous classification and  selection of objects and their pairwise relations, and N) the  position-role-sensitive conv-filters and pairwise RoI pooling that captures the spatial context of relations
Thanks  to the shared computation on the entire image, PPR-FCN  can be efficiently trained with a huge amount of pairwise  regions
PPR-FCN provides the first baseline for the novel  and challenging WSVRD task, which can foster practical  visual relation detection methods for connecting computer  vision and natural language
We found that the bottleneck  of PPR-FCN is the WSOD performance
Therefore, future  research direction may focus on jointly modeling WSOD  and WSVRD by incorporating relations as the contextual  regularization for objects
 NNN0    References  [N] G
Angeli, M
J
Premkumar, and C
D
Manning
Leveraging linguistic structure for open domain information extraction
In ACL,  N0NN
N  [N] Y
Atzmon, J
Berant, V
Kezami, A
Globerson, and G
Chechik
 Learning to generalize to new compositions in image understanding
 In EMNLP, N0NN
N  [N] H
Bilen and A
Vedaldi
Weakly supervised deep detection networks
In CVPR, N0NN
N, N, N, N, N, N  [N] Y.-W
Chao, Z
Wang, Y
He, J
Wang, and J
Deng
Hico: A benchmark for recognizing human-object interactions in images
In ICCV,  N0NN
N  [N] L
Chen, H
Zhang, J
Xiao, L
Nie, J
Shao, W
Liu, and T.-S
Chua
 Sca-cnn: Spatial and channel-wise attention in convolutional networks for image captioning
In CVPR, N0NN
N  [N] R
G
Cinbis, J
Verbeek, and C
Schmid
Weakly supervised object localization with multi-fold multiple instance learning
TPAMI,  N0NN
N, N, N  [N] B
Dai, Y
Zhang, and D
Lin
Detecting visual relationships with  deep relational networks
In CVPR, N0NN
N, N  [N] C
Desai and D
Ramanan
Detecting actions, poses, and objects with  relational phraselets
In ECCV, N0NN
N  [N] D
Eigen, D
Krishnan, and R
Fergus
Restoring an image taken  through a window covered with dirt or rain
In ICCV, N0NN
N  [N0] R
Girshick
Fast r-cnn
In ICCV, N0NN
N  [NN] A
Gupta and L
S
Davis
Beyond nouns: Exploiting prepositions  and comparative adjectives for learning visual classifiers
In ECCV,  N00N
N  [NN] A
Gupta, A
Kembhavi, and L
S
Davis
Observing human-object  interactions: Using spatial and functional compatibility for recognition
TPAMI, N00N
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for  image recognition
N0NN
N, N  [NN] R
Hu, M
Rohrbach, J
Andreas, T
Darrell, and K
Saenko
Modeling relationships in referential expressions with compositional modular networks
arXiv preprint arXiv:NNNN.0NNNN, N0NN
N  [NN] R
Hu, M
Rohrbach, J
Andreas, T
Darrell, and K
Saenko
Modeling relationships in referential expressions with compositional modular networks
In CVPR, N0NN
N, N  [NN] Z
Jie, Y
Wei, X
Jin, J
Feng, and W
Liu
Deep self-taught learning  for weakly supervised object localization
In CVPR, N0NN
N  [NN] J
Johnson, R
Krishna, M
Stark, L.-J
Li, D
A
Shamma, M
S
 Bernstein, and L
Fei-Fei
Image retrieval using scene graphs
In  CVPR, N0NN
N, N  [NN] V
Kantorov, M
Oquab, M
Cho, and I
Laptev
Contextlocnet:  Context-aware deep network models for weakly supervised localization
In ECCV, N0NN
N, N, N  [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
 arXiv preprint arXiv:NNNN.NNN0, N0NN
N  [N0] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, et al
Visual genome:  Connecting language and vision using crowdsourced dense image  annotations
IJCV, N0NN
N, N, N  [NN] M
P
Kumar, B
Packer, and D
Koller
Self-paced learning for latent  variable models
In NIPS, N0N0
N  [NN] D
Li, J.-B
Huang, Y
Li, S
Wang, and M.-H
Yang
Weakly supervised object localization with progressive domain adaptation
In  CVPR, N0NN
N  [NN] Y
Li, K
He, J
Sun, et al
R-fcn: Object detection via region-based  fully convolutional networks
In NIPS, N0NN
N, N, N, N  [NN] Y
Li, W
Ouyang, and X
Wang
Vip-cnn: Visual phrase guided  convolutional neural network
In CVPR, N0NN
N, N, N  [NN] Y
Li, W
Ouyang, B
Zhou, K
Wang, and X
Wang
Scene graph  generation from objects, phrases and region captions
In ICCV, N0NN
 N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan,  P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in  context
In ECCV, N0NN
N  [NN] S
Liu, C
Wang, R
Qian, H
Yu, R
Bao, and Y
Sun
Surveillance  video parsing with single frame supervision
In CVPR, N0NN
N  [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional networks  for semantic segmentation
In CVPR, N0NN
N  [NN] C
Lu, R
Krishna, M
Bernstein, and L
Fei-Fei
Visual relationship  detection with language priors
In ECCV, N0NN
N, N, N, N, N  [N0] O
Maron and T
Lozano-Pérez
A framework for multiple-instance  learning
In NIPS, NNNN
N, N  [NN] V
K
Nagaraja, V
I
Morariu, and L
S
Davis
Modeling context  between objects for referring expression understanding
In ECCV,  N0NN
N  [NN] A
Prest, C
Schmid, and V
Ferrari
Weakly supervised learning of  interactions between humans and objects
TPAMI, N0NN
N  [NN] V
Ramanathan, C
Li, J
Deng, W
Han, Z
Li, K
Gu, Y
Song,  S
Bengio, C
Rossenberg, and L
Fei-Fei
Learning semantic relationships for better action retrieval in images
In CVPR, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards realtime object detection with region proposal networks
In NIPS, N0NN
 N, N, N, N, N  [NN] A
Rohrbach, M
Rohrbach, R
Hu, T
Darrell, and B
Schiele
 Grounding of textual phrases in images by reconstruction
In ECCV,  N0NN
N, N, N, N  [NN] M
A
Sadeghi and A
Farhadi
Recognition using visual phrases
In  CVPR, N0NN
N, N  [NN] S
Schuster, R
Krishna, A
Chang, L
Fei-Fei, and C
D
Manning
 Generating semantically precise scene graphs from textual descriptions for improved image retrieval
In Workshop on Vision and Language, N0NN
N  [NN] H
O
Song, R
B
Girshick, S
Jegelka, J
Mairal, Z
Harchaoui,  T
Darrell, et al
On learning to localize objects with minimal supervision
In ICML, pages NNNN–NNNN, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed, D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
Going deeper with convolutions
In CVPR, N0NN
N  [N0] B
Thomee, D
A
Shamma, G
Friedland, B
Elizalde, K
Ni,  D
Poland, D
Borth, and L.-J
Li
YfccN00m: The new data in multimedia research
Communications of the ACM, N0NN
N  [NN] J
R
Uijlings, K
E
Van De Sande, T
Gevers, and A
W
Smeulders
 Selective search for object recognition
IJCV, N0NN
N  [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and tell: A  neural image caption generator
In CVPR, N0NN
N  [NN] C
Wang, W
Ren, K
Huang, and T
Tan
Weakly supervised object  localization with latent category learning
In ECCV, N0NN
N  [NN] Q
Wu, P
Wang, C
Shen, A
Dick, and A
van den Hengel
Ask me  anything: Free-form visual question answering based on knowledge  from external sources
In CVPR, N0NN
N, N  [NN] B
Yao and L
Fei-Fei
Modeling mutual context of object and human  pose in human-object interaction activities
In CVPR, N0N0
N  [NN] M
Yatskar, L
Zettlemoyer, and A
Farhadi
Situation recognition:  Visual semantic role labeling for image understanding
In CVPR,  N0NN
N  [NN] L
Yu, P
Poirson, S
Yang, A
C
Berg, and T
L
Berg
Modeling  context in referring expressions
In ECCV, N0NN
N  [NN] H
Zhang, Z
Kyaw, S.-F
Chang, and T.-S
Chua
Visual translation  embedding network for visual relation detection
In CVPR, N0NN
N,  N, N, N, N, N  [NN] J
Zhang, M
Elhoseiny, S
Cohen, W
Chang, and A
Elgammal
Relationship proposal networks
In CVPR, N0NN
N, N  [N0] C
L
Zitnick and P
Dollár
Edge boxes: Locating object proposals  from edges
In ECCV, N0NN
N, N, N  NNNNLearning the Latent "Look": Unsupervised Discovery of a Style-Coherent Embedding From Fashion Images   Learning the Latent “Look”: Unsupervised Discovery of a  Style-Coherent Embedding from Fashion Images  Wei-Lin Hsiao UT-Austin  kimhsiao@cs.utexas.edu  Kristen Grauman UT-Austin  grauman@cs.utexas.edu  Abstract  What defines a visual style? Fashion styles emerge organically from how people assemble outfits of clothing,  making them difficult to pin down with a computational  model
Low-level visual similarity can be too specific to  detect stylistically similar images, while manually crafted  style categories can be too abstract to capture subtle style  differences
We propose an unsupervised approach to learn  a style-coherent representation
Our method leverages  probabilistic polylingual topic models based on visual attributes to discover a set of latent style factors
Given a  collection of unlabeled fashion images, our approach mines  for the latent styles, then summarizes outfits by how they mix  those styles
Our approach can organize galleries of outfits  by style without requiring any style labels
Experiments on  over N00K images demonstrate its promise for retrieving,  mixing, and summarizing fashion images by their style
 N
Introduction  Computer vision methods that can understand fashion  could transform how individual consumers shop for their  clothing as well as how the fashion industry can analyze its  own trends at scale
The scope for impact is high: fashion is  already a $N.N trillion USD global industry, popular social  commerce websites like Chictopia and Polyvore draw millions of users, and online subscription services like StitchFix blend algorithms and stylists to personalize shopping  selections
In sync with these growing possibilities, recent  research explores new vision methods for fashion, with exciting advances for parsing clothing [NN, NN], recognizing  clothing, attributes [N, N0], and styles [N0, NN], matching  clothing seen on the street to catalogs [NN, NN, NN, NN, NN],  and recommending clothing [NN, NN, NN, NN, NN]
 Despite substantial progress on all these fronts, capturing the style-coherent similarity between outfits remains  an important challenge
In particular, a visual representation with style coherency would capture the relationship  Query Instance match Label-based Latent looks  – low diversity
– inconsistent
– consistent, diverse
 (a) (b) (c)  Figure N: The leftmost query image relates to instances like those  in (a),(b),(c) in distinct ways
In contrast to matching nearduplicate outfits (a) or classifying broad styles (b), we propose  to discover the latent “looks”—compositions of clothing elements  that are stylistically similar (c)
 between clothing outfits that share a “look”, even though  they may differ in their specific composition of garments
 A style-coherent representation would be valuable for i)  browsing, where a consumer wants to peruse diverse outfits  similar in style, ii) recommendation, where a system should  suggest new items that add novelty to a consumer’s closet  without straying from his/her personal style, and iii) style  trend tracking, where analysts would like to understand the  popularity of items over time
 Style coherency differs from traditional notions of visual similarity
The problem of style coherency sits between  the two extremes currently studied in the literature: on one  end of the spectrum are methods that seek robust instance  matching, e.g., to allow a photo of a garment seen on the  street to be matched to a catalog [NN, NN, NN, NN, NN] (see  Figure N(a)); on the other end of the spectrum are methods  that seek coarse style classification, e.g., to label an outfit  as one of a small number predefined categories like Hipster, Preppy, or Goth [N0, NN] (see Figure N(b))
In contrast  to these two extremes, style coherency refers to consistent  fine-grained trends exhibited by distinct assemblies of garments
In other words, coherent styles reflect some latent  “look”
See Figure N(c)
 NNN0N    We propose an unsupervised approach to learn a stylecoherent representation
Given a large repository of unlabeled fashion images, the goal is to discover the latent factors that naturally guide how people dress—that is, the underlying compositions of visual attributes that define styles
 To this end, we explore probabilistic topic models
Topic  models in natural language processing [N, NN] represent text  documents as distributions over concepts, each of which  is a distribution over words
In our case, an outfit is a  “document”, a predicted visual attribute (e.g., polka dotted,  flowing, wool) is a “word”, and each style is a discovered  “topic”
Furthermore, we consider polylingual topic models [NN] in order to model style consistency across multiple  regions of the body, enforcing that the discovered style factors for each region of the body should interact compatibly
 Our idea makes it possible to organize galleries of outfits  by style without any style labels
Building on the proposed  model, we develop methods to mix styles or summarize an  image gallery by its styles
 The proposed approach is well-suited for the problem at  hand, for several reasons
First, being unsupervised, our  algorithm discovers the underlying elements of style, as opposed to us manually defining them
It is often difficult  to manually craft labels, especially in the domain of fashion
Clothing styles emerge organically from instances of  what people choose to wear—not from some top-down preordained bins of outfit types—and furthermore they evolve  over time, meaning today’s hand-crafted lexicon will eventually fade in relevance
Secondly, our approach accounts  for the fact that style is about the Gestalt: individual items  do not dictate a style; rather, it is their composition that creates a look [N]
Finally, our topic model approach also naturally accounts for the soft boundaries of style, describing  outfits as mixtures of overlapping styles
 Our experiments on two challenging fashion  datasets [N0, NN] demonstrate the advantages of our  unsupervised representation compared to style-based  CNNs and more basic attribute descriptions
We validate  that our styles align well with human perceived styles
 We further show their value for retrieval, mixing, and  summarization
Finally, we introduce a new dataset of NNK  images labeled for fine-grained, body-localized attributes  relevant for fashion analysis
 N
Related Work  Attributes for fashion Describable attributes, such as floral, denim, long-sleeved, are often of interest in analyzing  fashion images
Prior work explores a variety of recognition schemes [N, N0, N, N, NN, N], including ways to jointly  recognize attributes [N0, N] or simultaneously detect clothing articles and attributes [N]
In our work, attributes serve  as a starting point for discovering styles (i.e., the “words” in  our topic model), and improvements in attribute prediction  from work like the above would also benefit our model
 Retrieval and matching for clothing Given an image  of a garment, clothing retrieval methods identify exact or  close matches, which supports shopping needs
Clothing  deformations and complex body poses make retrieval challenging, so researchers develop ways to learn the similarity of outfits or garments [NN, NN, NN, NN, NN]
Matching  clothing seen “on the street” to instances stored in catalogs “at the shop” requires new ideas in domain adaptation [NN, NN, NN, NN]
Existing methods are largely supervised, i.e., provided with street-shop pairs or pairs of outfits judged as similar by human annotators
In contrast, we  develop an unsupervised approach that can leverage ample  unlabeled data
More importantly, whereas retrieval work  aims to match the same (or similar) garment(s), we aim to  identify style-coherent complete outfits
 Models of style, fashionability, compatibility Previous  work offers a few perspectives on the meaning of visual  style
The Hipster Wars project defines five style categories  (Hipster, Goth, Preppy, Pinup, Bohemian) and recognizes  them based on patches on body part keypoints [N0]
Another approach pre-trains a neural network for style using  weak meta-data labels [NN]
Compared to the retrieval work  above, both (like us) aim to capture a broader notion of  style
However, unlike [N0, NN] we treat styles as discoverable latent factors rather than manually defined categories,  which has the advantages discussed in the Introduction
 Whereas style refers to a characterization of whatever it  is people wear, compatibility refers to how well-coordinated  individual garments are [NN, NN, NN], and fashionability  refers to the popularity of clothing items, e.g., as judged  by the number of “like” votes on an image posted online [NN]
Recent work explores forecasting the popularity  of styles [N]
 Topic models Topic models originate in text processing
 The well-known topic model Latent Dirichlet Allocation  (LDA) [N] and its polylingual extension [NN] use multinomial distributions to represent the generation of documents  comprised of words
Polylingual topic models are applied  to Web fashion data in [NN] to discover links between textual design element meta-data and textual style meta-data,  with no computer vision
Early uses of topic models in vision relied on “visual words” (quantized image patches) to  discover representations for scene recognition [N0] or perform object category discovery in unlabeled images [NN]
 More recently, topic models are used to recommend a colorcoordinating garment in [NN]
To our knowledge, we are the  first to propose discovering visual styles for outfits using  topic models
Deriving topic models on top of semantic  visual attributes is also new, and has the added benefit of  yielding interpretable latent topics
 NN0N    z x ϕ  N (outer)  α θ ..
..
β  z x ϕ  N (hosiery) K(style)  M (outfit)  Figure N: Graphical model of the polylingual visual style LDA
 N
Approach  After providing background on topic models in Section N.N, we introduce our visual fashion topic model in Section N.N
Next in Section N.N we overview the fine-grained  localized attributes used in our model
Finally, we leverage  the learned style-coherent embedding for retrieval, mixing,  and summarization tasks in Section N.N
 N.N
Background: Topic models  We explore unsupervised topic models originating from  text analysis to discover visual styles
In particular, we  employ Latent Dirichlet Allocation (LDA) [N]
LDA is a  Bayesian multinomial mixture model that supposes a small  number of K latent topics account for the distribution of observed words in any given document
It uses the following  generative process for a corpus D consisting of M documents each of length Ni:  N
Choose θi ∼ Dir(α), where i ∈ {N, 


,M} and Dir(α) is the Dirichlet distribution for parameter α  N
Choose ϕk ∼ Dir(β), where k ∈ {N, 


,K} N
For each word indexed by i, j, where j ∈ {N, 


, Ni},  and i ∈ {N, 


,M}  (a) Choose a topic zi,j ∼ Multinomial(θi) (b) Choose a word xi,j ∼ Multinomial(ϕzi,j )  Only the word occurrences are observed
 Polylingual LDA [NN] extends LDA (we will call it  MonoLDA) to process an aligned corpus of documents expressed in multiple languages
The idea is to recover topics  that preserve the ties between translated text
In particular,  translated documents form a tuple, and all documents in a  tuple have the same distribution over topics
Each topic is  produced from a set of distributions over words, one distribution per language
 N.N
Discovering a style-coherent embedding  We propose to learn a style-coherent embedding using  a topic model
In our setting, the latent topics will be discovered from unlabeled full-body fashion images, meaning  images of people wearing an entire outfit (as opposed to  catalog images of individual garments)
In this way, we aim  to discover the compositions of lower-level visual cues that  characterize the main visual themes—styles—emerging in  how people choose to assemble their outfits
 The basic mapping from document topic models to our  visual style topic models is as follows: an observed outfit is a “document”, a predicted visual attribute (e.g., polka  dotted, flowing, wool) is a “word”, and each style is a discovered “topic”
 A potential limitation of this mapping, however, is that  it treats an outfit as a bag of attributes
Thus it loses valuable information about the attributes’ associated articles of  clothing
For example, learned topics could interchange the  appearance of wool pants with a wool jacket, when the two  may in reality signify distinct latent styles
A partial solution is to specify localized attributes
For example, we  could expand wool to wool pants and wool jacket
However,  this expansion may suffer from allowing LDA to decouple  topics across different regions of the body
For example,  in Figure N (left), MonoLDA dedicates topic N to shirt and  topic N to skirt
 Thus, we consider a polylingual LDA (PolyLDA)  model [NN]
In this case, each region of the body is a  “language”, and an outfit is a “document tuple” in multiple  languages
As above, latent styles are topics and inferred  clothing attributes are words
The body regions (denoted  as R) we consider are: outer layer (i.e., where a jacket  or blazer goes), upper body (shirt/blouse/sweater), lower  body (pants/skirt/shorts), and hosiery (tights/leggings)
The  polylingual topic model adds a structural constraint that  forces body regions to share styles, such that we can learn  styles consistent across body regions
The generative process of PolyLDA is as follows:  N
For each topic k ∈ {N, 


,K}  (a) For each body part r ∈ R  i
Choose attribute distribution ϕ (r) k  ∼ Dir(β)  N
For each outfit i ∈ {N, 


,M}  (a) Choose style distribution θi ∼ Dir(α) (b) For each body part r ∈ R  i
For each attribute belonging to that body part, indexed by i, j, where j ∈ {N, N, 


, Ni (r)}  – Draw a style z (r) ij ∼ Mult(θi)  – Draw an attribute x (r) ij ∼ Mult(ϕ  (r)  z (r) ij  )  Figure N shows the associated graphical model
In contrast to MonoLDA, PolyLDA captures the interaction of  garment regions, such that each style specifies a full-body  trend (Figure N, right)
 NN0N    Mono-topicN Mono-topicN Poly-topicN Poly-topicN  U shirt collar L skirt O deco button O length short  U deco button L skirt short O pattern plain O sleeve long  U buttoned L skirt full O blazer O pullover  G deco button L skirt pleat U buttoned U shirt collar  U sleeve long L skirt high-rise U shirt collar U color white  G pattern plaid G pattern plain L length long L skirt short  G pattern plain G front pullover L shape straight L skirt full  G deco button G deco button H pattern plain  G pants H length short  G jacket G sweater  Figure N: Mono vs
Polylingual LDA: U for upper body, O  for outer layer, L for lower body, H for hosiery and G for  global
MonoLDA (left) learns a topic either for U or L , while  PolyLDA’s styles (right) span the whole body
 By applying PolyLDA to a database of unlabeled outfit  images, we obtain a set of discovered styles (see Figure N in  our experiment section for examples) with which to encode  novel images
Each topic k has its attribute probability ϕ (r) k  depending on body region r
Given an outfit d, we represent  it in a style-coherent embedding by its topic proportions:  θd = [θdN, 


, θdK ], (N) where θdk ≥ 0, Σkθdk = N
The resulting embedding ac- counts for the fact that a composition of style elements defines a look [N]
 We stress that our style-coherent embedding is fully unsupervised
Our method discovers styles from unlabeled  images, as opposed to learning a style embedding with supervision
For example, one could gather pairs of fashion  images and ask human annotators to label them as similar  or dissimilar in style (or use noisy tags as labels [NN]), then  learn an embedding that keeps similar pairs close
Or, in  the spirit of [NN], one could train classifiers to target a predefined set of style categories
While the attribute models are trained on a disjoint pool of attribute labeled images, our style model runs on predicted attributes; annotators do not touch the images on which we perform discovery
Our unsupervised strategy saves manual effort
More  importantly, it also addresses challenges specific to visual  styles—namely, their ever-evolving nature, the difficulty in  enumerating them with words, and their soft boundaries
 N.N
Fine-grained localized fashion attributes  We next discuss our approach to infer attributes in fullbody fashion images
We consider both global and localFigure N: Attributes present for an outfit in the localized vocabulary (top) or global vocabulary (bottom)
 ized attributes
Global attributes indicate the presence of  a property somewhere on the body (e.g., floral), whereas  a localized attribute links it specifically to a body region  (e.g., floral-shirt and floral-skirt are distinct words)
Figure N shows an example image and the attributes from either vocabulary that are present, as well as the body region  association for the localized ones
 Vocabulary and data collection As input to our style discovery model, we need a rich attribute vocabulary that is  both localized and fine-grained
In existing fashion datasets,  the attributes lack one or both of these aspects [N, N, N, N0]  or are not publicly available [NN]
Thus, we curate a new  dataset for attribute training
 For the vocabulary, we build on the NN attributes enumerated in [NN]
First we remove those too subtle for most annotators to discern (chiffon; jewel collar)
Then we add missing but frequently appearing attributes (e.g., pink; polkadot)
Finally, we expand the set so that color, material, and  pattern are localized to each body region
This yields NNN  total attributes (see Supp for details)
 To gather images, we use keyword search with the attribute name on Google, then manually prune those where  the attribute is absent
This yields N0 to N00 positive  training images per attribute
We also gather N000 random street images from chictopia.com (manually pruned for  false-negatives) to serve as negative examples
In total, the  new dataset has NN,NNN images.N  Training with multilabel outfits The clothing outfit images are multilabel in terms of their attributes
To circumvent the expense of labeling all NNK images for all NNN attributes, and to deal appropriately with highly localized attributes, we develop a piecewise training procedure
First  we group the attributes into six types: pattern, material,  shape, collar type, clothing article, and color
The types  Nvision.cs.utexas.edu/projects/StyleEmbedding/  NN0N  vision.cs.utexas.edu/projects/StyleEmbedding/   background  blazer  belt  bag  blouse  boots  face  hair  pants  skin  sunglasses  T-shirt  leggings  shoes  Color    segmentation  Clothing article    segmentation  Applied    DenseCRF  Figure N: We intersect the article and color labels to generate the  final blazer-color-blue, pants-color-red prediction
 have N0N, NN, N0, N, NN, NN attributes, respectively.N Then  we train separate convolutional neural networks (CNN) per  type
This allows us to directly use the positive examples  for each attribute, and all others from other keywords as  negatives, while still yielding predictions at test time that  are multilabel
We find it is also important during training  to have negatives with none of the named attributes present,  since such outfits are rather common
 Our attribute learning framework accounts for the challenge that many attributes occupy a small portion of a fashion image
First, we detect people using faster-RCNN [N0],  and extract a crop from the detected person bounding box  according to whether the image is a training image for an  upper or lower body attribute
We give the network both  the whole-body person box and the crop as two instances  with the same attribute label, allowing it to leverage any  useful cues
We fine-tune our attribute prediction networks  (one per type for the first four types) on ResNet-N0 [NN] pretrained with ImageNet [N]
 For both the clothing article and color types, we  train a segmentation network
For both, we fine-tune  DeepLab’s [N] repurposed VGG-NN network and apply  DenseCRF [NN]
The networks target NN pixel-wise clothing  article labels and NN pixel-wise color labels from the Fashionista data [NN], respectively
At test time, we i) record the  detected clothing article names, and ii) intersect the color  and clothing semantic segmentations to produce articlespecific color attributes, e.g., shirt-color-blue (Fig
N)
 The resulting attribute classifiers offer a fairly reliable  basis for style discovery
For the validation split of our NNK  image dataset, they attain N0% average precision
Figure N shows attribute predictions on novel test images
 N.N
Using the style-coherent embedding  Our method produces a style-coherent representation for  fashion images
Our experiments consider three tasks leveraging this representation, defined next
 Retrieval of style-related images For retrieval, the system is given a query image and must return database images  NAttributes within pattern and collar types are mutually exclusive, thus  their multilabeling can be done efficiently
We obtain complete N0-label  and NN-label multilabeling for types material and shapes
 Upper Lower Hosiery Global  blue short dr
blue floral blue  purple loose dr
translucent purple  white flat dr
dress white  white dr
shoe beige  blue dr
stocking cardigan  purple dr
red  floral floral dr
floral  blue tight dr
dress  green flat dr
blue  beige blue dr
green  green dr
beige  beige dr
 Figure N: Example of predicted attributes on a HipsterWars [N0]  (top) image and a DeepFashion [NN] (bottom) image (dr.=dress)  that illustrate similar style
Here we simply use our learned  embedding to retrieve images close to a query image, i.e.,  nearest neighbors in the space of θq for query image q
 Our embedding retrieves images that maintain style coherence with the query
While conventional embeddings  (e.g., CNNs) can return the examples closest in appearance, our embedding can return those that are close in style
 Whereas the former is preferable when doing street-to-shop  visual matching [NN, NN, NN, NN], the latter is preferable for  many browsing scenarios, e.g., to view recommendations  related to past purchases
 Mixing and “traversing between” styles A new task  supported by our approach is to mix fashion styles
In  this scenario, a user identifies T styles of interest S := {SN, 


, ST }, St ∈ {N, 


,K}, and queries for images that exhibit a blend of those styles
For example, the user  could manually select styles of interest by viewing images  associated with each discovered style, or the styles for mixing could be automatically discovered based on the dominant styles in his photo album or shopping history
We  measure the relevance of an image Ii as:  MixRelevance(θi,S) = min t∈S  θit, (N)  where θi is the style embedding for Ii
The min assures that  an image is only as relevant to the requested style mix as it  is close to its most distant style
Similarly, we can offer new  browsing capabilities by depicting a gradual transition from  one style to another (see Figure N)
 Summarizing styles A third application uses our stylecoherent embedding to summarize the styles in an image collection
Given images {IN, 


, IN}, we calculate the relative influence of each style k as Influence(Sk) =∑N  i=N θik
With these frequencies we can visualize the collection compactly by sampling images dominant for each  influential style (Figure NN in our experiment section)
 NN0N    Pinup  PinupPinup  Pinup Goth Goth  GothGoth  Preppy  Preppy  Preppy  Preppy Hipster  Hipster  Hipster  Bohemian Bohemian  BohemianBohemian  Preppy  Figure N: Top images for the five discovered style topics with PolyLDA
Labels indicate human-assigned styles from HipsterWars [N0],  which are not seen by our algorithm
Our approach successfully discovers the five human-perceived styles
Please see Supp for more  examples and baseline cluster results
 Avg AP NMI  MonoLDA 0.NN 0.N0  PolyLDA 0.NN 0.NN  Table N: Mono vs
poly LDA discovery accuracy judged against  the manual style labels of HipsterWars, using GT attributes
 N
Experiments  We first show that our discovered topics align with  human-perceived styles (Section N.N)
Then, we apply the  embedding for retrieval (Section N.N), mixing (Section N.N),  and summarizing styles (Section N.N)
 Datasets We use two datasets: (i) HipsterWars [N0],  which has N,NNN images, each labeled by one of N style labels: Hipster, Preppy, Goth, Pinup, Bohemian; (ii) DeepFashion [NN], from which we take all N0N,NNN images that  have at least one of the NN0 style labels and a fully-visible  person
Because the NN0 style labels in DeepFashion are  noisy labels, we collapse them into NN higher level styles by  affinity propagation [NN] using cosine similarity to measure  co-occurrence of styles in an outfit (see Supp)
We use the  attribute networks trained with our new NNK image dataset  (Section N.N) to predict the attributes in the HipsterWars and  DeepFashion images
 Baselines We compare with four baseline: (i)  StyleNet [NN]: a state-of-the-art feature for clothing  that fine-tunes a CNN using NNN metadata labels (e.g., redsweater) on images from the Fashion NNNK dataset [NN], (ii)  vanilla ResNet-N0 [NN]: the last layer of a state-of-the-art  CNN pretrained for ImageNet, (iii) Attr-ResNet: ResNetN0 fine-tuned to classify the same NNN attributesN used by  our method with the same training data, and (iv) Attributes:  indicator vectors using the same attributes as our model
 NThe attributes in types: pattern, material, shape, collar type; we did  not include clothing article and color because they are predicted differently, from a segmentation network
 N.N
Consistency with human labeled styles  First we analyze how well our discovered styles align  with human perception, as captured by the datasets’ style  labels (never seen by our approach)
We use two metrics: (i)  Normalized Mutual Information (NMI), which captures the  overall alignment of topics with ground truth (GT) styles;  and (ii) averaged maximal average precision (AP) per style,  which uses each topic’s probability as a relevance score for  a style to sort all images, then records the AP per topic  per style
The best (max) AP a style has in all topics is  that style’s final score
We average the max AP scores of  all styles (N in HipsterWars and N00+ in DeepFashion) to  get “avg maximal AP”
To extract clusters for the baseline  representations we use K-means clustering
We also tried  GMM and AP-clustering and found the clustering algorithm  itself has negligible impact on their results
For all methods, we set the number of clusters/topics to be the number  of style labels in the respective dataset
 First we examine the impact of our polylingual model
 Table N shows the results for both LDA variants on HipsterWars
Here we use ground truth attributes, in order to  evaluate the LDA models independent of attribute prediction quality
We see that the polylingual model has an advantage, and thus adopt it as our model for all experiments
 Next we quantify discovery accuracy for our approach  against the baselines
Table N shows the results on both  datasets
For the attribute-indicator baseline and our approach, we show results with predicted and ground truth  attributes in order to separate the success of discovery from  the success of attribute prediction.N Overall, PolyLDA is  the strongest
Both PolyLDA and Attributes perform better with perfect attributes, reinforcing that attribute precision is an important ongoing research challenge [N0, N, N]
 However, even with predicted attributes, we outperform all  baselines on both datasets for both metrics
Despite having  been pretrained to capture noisy fashion labels, the StyleNet  CNN [NN] does not discover the human-perceived styles as  NDeepFashion has GT for only global attributes
 NN0N    well, though it does soundly outperform the vanilla ResNet  baseline
Attr-ResNet falls in between StyleNet and vanilla  ResNet, as expected
The absolute accuracy on DeepFashion is much lower for all methods, a function of its larger  size and more varied and noisy style labels
Whereas the  Hipster style labels are manually curated through a rigorous crowdsourcing procedure [N0], the DeepFashion style  labels are gleaned from text meta-data [NN]
 Figure N shows the most central images for our discovered styles on HipsterWars (see Supp for DeepFashion)
 The qualitative examples reinforce the quantitative result  above
Our model discovers the human-perceived styles  better than the CNN and attributes clusters (see Supp)
Our  style-coherent embedding better tolerates superficial visual  differences in intra-style images
 N.N
Style-coherent retrieval  Having shown that the discovered styles are meaningful, next we evaluate the style embedding for retrieval
In  this task, a user queries by example for images related by  style, e.g., for recommendation or catalog browsing relative to some currently viewed item (query)
Recall, this is  distinct from instance retrieval for near-duplicates
Thus,  we evaluate performance simultaneously by style coherence  and diversity or novelty
Diversity refers to the retrieved  images’ mutual visual dissimilarity with each other, and  novelty refers to their collective dissimilarity to the query
 The goal is to obtain retrieval results that maintain style coherence while avoiding redundancy
For style coherence,  we evaluate NDCG [NN] against ground truth style labels
 For diversity/novelty, we learn a metric to mimic humanperceived dissimilarity, following [NN]
In particular, we  collect NN0 triplets labeled by N human annotators and learn  a ranking function [NN] on top of the attribute and CNN descriptors that respects human-given judgments
 Figure N shows the results for both datasets
For HipsterWars (top), we treat each image as a query in turn, and  for DeepFashion (bottom) we sample N,000 of the N0N,NNN  images as queries
Our model offers a good combination of  coherency and diversity/novelty
On HipsterWars, it maintains diversity/novelty while maintaining a similar or better  level of coherence as the baselines
As before, predicted attributes diminish coherence, yet the topic model coherence  appears to degrade more gracefully
 N.N
Mixing styles  Next we consider mixing styles
Since evaluation  of mixing requires images labeled for multiple humanperceived styles as well as instances exhibiting exclusively  one style, we collect a ground truthed test set of NNN  Web images using the HipsterWars style names (see Supp)
 While our mixing approach (Sec N.N) can blend arbitrary  selected styles, for sake of evaluation we focus on blend0.N 0.NN 0.N  N.N  N  N.N  Attr
 Attr.*Stylenet  Resnet  PolyLDA  PolyLDA*  Style Coherence  N o v el  ty  0.N 0.NN 0.N  N.N  N  N.N  Attr
 Attr.* Stylenet  Resnet  PolyLDA  PolyLDA*  Style Coherence  D iv  er si  ty  0.N 0.N 0.N 0.N  N  N.N  N  Stylenet  Resnet Attr
 Attr.*  PolyLDA  PolyLDA*  Style Coherence  N o v el  ty  0.N 0.N 0.N 0.N  0.N  N  N.N  N.N  Stylenet  Resnet Attr
 Attr.*  PolyLDA*  PolyLDA  Style Coherence  D iv  er si  ty  Figure N: Style retrieval on HipsterWars (top) and DeepFashion  (bottom)
* denotes use of GT attributes
The ideal method would  sit in the top right corner of the plots
Our embedding offers a  good trade-off in style coherency and diversity/novelty
 HipsterWars DeepFashion  Avg AP NMI Avg AP NMI  StyleNet [NN] 0.NN 0.N0 0.0N0N 0.00NN  ResNet [NN] 0.N0 0.NN 0.0NNN 0.000N  Attr-ResNet 0.NN 0.NN 0.0NNN 0.000N  Attributes 0.NN / 0.NN 0.NN / 0.NN 0.0NN0 / 0.NNNN 0.00NN / 0.00NN  PolyLDA 0.N0 / 0.NN 0.NN / 0.NN 0.0NNN / 0.NNNN 0.0NNN / 0.0NNN  Table N: Discovery accuracy for both datasets
Attributes and  PolyLDA show result if using either predicted attributes (first) or  ground truth attributes (second)
 ing pairs of GT-labeled styles, then score the AP against the  ground truth, i.e., images exhibiting both the initial selected  styles
For the baselines, we use their clusters analogously  to our topics, creating K-dim embeddings that record the  distance of the image to each cluster’s centroid
We use  K = NN topics/clusters; K ∈ (NN, N0) gives similar results
Table N shows the results
On the whole, our approach  does better than the baselines, and in most cases this is true  even using predicted attributes
This result highlights the  power of the topic model over the raw attributes, which are  too low-level for adequate mixing
 Figure N0 shows example images predicted as strong exemplars for two style blends
Figure N shows an example  gradually mixing from a source style to a target style
 N.N
Style summaries  Finally, we demonstrate the power of our model to organize galleries of outfits
As proof of concept, we select two  users from chictopia.com, and download N00 photos from  each of their albums
Figure NN shows the results
We show  snapshots from their albums along with summary piecharts  NN0N    Figure N: Visualization generated by mixing our style topics, gradually traversing from one style (Bohemian) to another (Hipster)
 (a) Hipster×Bohemian  (b) Hipster×Goth  Figure N0: Top retrievals for two mixes
Incorrect images are labeled with red actual labels: Hipster, Goth, Preppy
 Preppy× Hipster× Preppy× Goth× Bohemian× Goth Goth Hipster Bohemian Hipster  StyleNet [NN] 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN ResNet [NN] 0.NNN 0.NNN 0.NNN 0.NNN 0.0NN Attributes 0.NNN / 0.NNN 0.NNN / 0.NNN 0.0N0 / 0.0NN 0.NNN / 0.NNN 0.0N0 / 0.NNN PolyLDA 0.NNN / 0.N0N 0.NNN / 0.NN0 0.NNN / 0.NNN 0.NN0 / 0.NNN 0.NNN / 0.NNN  Table N: Accuracy (AP) of retrieving a mixture of styles
 computed by our approach to highlight the dominant styles
 Gray pie slices indicate insignificant styles for a user
Our  summaries convey the user’s tastes in a glance
In contrast,  the status quo would entail manually paging through all N00  photos in the album in an arbitrary order
 N
Conclusion  This work explores unsupervised discovery of complex  styles in fashion
We develop an approach based on polylingual topic models to model the composition of outfits from  visual attributes
The resulting styles offer a fine-grained  representation valuable for organizing unlabeled fashion  photos beyond their superficial visual ties (e.g., same literal garments or attributes)
While by necessity our results  rely on external style labels for evaluation, we stress that the  Figure NN: Style summarization for two users
Left is the user’s  album, right is the visual style summary breaking down the main  trends discovered in the album
 generality of the discovered styles is an asset, and they offer representational power beyond what traditional (supervised) classification schemes can do
Our example results  highlighting blended styles, trajectories between styles, and  style summaries suggest a few such applications of interest
 Acknowledgements: We thank Suyog Jain and ChaoYuan Wu for helpful discussions
This research is supported  in part by NSF IIS-N0NNNN0 and a gift from Amazon
 NNN0    References  [N] Z
Al-Halah, R
Stiefelhagen, and K
Grauman
Fashion forward: Forecasting visual style in fashion
In ICCV, N0NN
 N  [N] R
Barthes
The language of fashion
A&C Black, N0NN
N,  N  [N] D
M
Blei, A
Y
Ng, and M
I
Jordan
Latent dirichlet  allocation
JMLR, N00N
N, N  [N] L
Bossard, M
Dantone, C
Leistner, C
Wengert, T
Quack,  and L
Van Gool
Apparel classification with style
In ACCV,  N0NN
N, N, N  [N] H
Chen, A
Gallagher, and B
Girod
Describing clothing by  semantic attributes
In ECCV, N0NN
N, N, N, N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected crfs
In ICLR, N0NN
N  [N] Q
Chen, J
Huang, R
Feris, L
M
Brown, J
Dong, and  S
Yan
Deep domain adaptation for describing people based  on fine-grained clothing attributes
In CVPR, N0NN
N  [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A Large-Scale Hierarchical Image Database
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition (CVPR), N00N
N  [N] W
Di, C
Wah, A
Bhardwaj, R
Piramuthu, and N
Sundaresan
Style finder: Fine-grained clothing style detection and  retrieval
In CVPR, N0NN
N, N  [N0] L
Fei-Fei and P
Perona
A bayesian hierarchical model for  learning natural scene categories
In CVPR, N00N
N  [NN] B
J
Frey and D
Dueck
Clustering by passing messages  between data points
Science, N00N
N  [NN] J
Fu, J
Wang, Z
Li, M
Xu, and H
Lu
Efficient clothing  retrieval with semantic-preserving visual phrases
In ACCV,  N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N, N, N, N  [NN] Y
Hu, X
Yi, and L
Davis
Collaborative fashion recommendation: A functional tensor factorization approach
In  ACM MM, N0NN
N  [NN] J
Huang, R
Feris, Q
Chen, and S
Yan
Cross-domain image retrieval with a dual attribute-aware ranking network
In  ICCV, N0NN
N, N  [NN] T
Iwata, S
Watanabe, and H
Sawada
Fashion coordinates  recommender system using photographs from fashion magazines
In IJCAI, N0NN
N, N  [NN] T
Joachims
Training linear svms in linear time
In ACM  SIGKDD, N00N
N  [NN] Y
Kalantidis, L
Kennedy, and L.-J
Li
Getting the look:  Clothing recognition and segmentation for automatic product  suggestions in everyday photos
In ICMR, N0NN
N, N, N  [NN] M
H
Kiapour, X
Han, and S
Lazebnik
Where to buy it:  Matching street clothing photos in online shops
In ICCV,  N0NN
N, N, N  [N0] M
H
Kiapour, K
Yamaguchi, A
Berg, and T
Berg
Hipster wars: Discovering elements of fashion styles
In ECCV,  N0NN
N, N, N, N, N  [NN] A
Kovashka, D
Parikh, and K
Grauman
Whittlesearch:  Image search with relative attribute feedback
In CVPR,  N0NN
N  [NN] P
Krähenbühl and V
Koltun
Efficient inference in fully  connected crfs with gaussian edge potentials
In NIPS, N0NN
 N  [NN] K
Lin, H.-F
Yang, K.-H
Liu, J.-H
Hsiao, and C.-S
Chen
 Rapid clothing retrieval via deep learning of binary codes  and hierarchical search
In ACM ICMR, pages NNN–N0N,  N0NN
N  [NN] S
Liu, J
Feng, Z
Song, T
Zheng, H
Lu, C
Xu, and S
Yan
 Hi, magic closet, tell me what to wear! In ACM MM, N0NN
 N, N  [NN] S
Liu, Z
Song, G
Liu, C
Xu, H
Lu, and S
Yan
Street-toshop: Cross-scenario clothing retrieval via parts alignment  and auxiliary set
In CVPR, N0NN
N, N, N, N  [NN] Z
Liu, P
Luo, S
Qiu, X
Wang, and X
Tang
Deepfashion:  Powering robust clothes recognition and retrieval with rich  annotations
In CVPR, N0NN
N, N, N, N, N  [NN] C
D
Manning, P
Raghavan, H
Schütze, et al
Introduction  to information retrieval, volume N
Cambridge university  press Cambridge, N00N
N  [NN] J
McAuley, C
Targett, Q
Shi, and A
van den Hengel
 Image-based recommendations on styles and substitutes
In  SIGIR, N0NN
N  [NN] D
Mimno, H
M
Wallach, J
Naradowsky, D
A
Smith, and  A
McCallum
Polylingual topic models
In EMNLP, N00N
 N, N  [N0] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS
N0NN
N  [NN] M
Rosen-Zvi, T
Griffiths, M
Steyvers, and P
Smyth
The  author-topic model for authors and documents
In UAI, N00N
 N  [NN] E
Simo-Serra, S
Fidler, F
Moreno-Noguer, and R
Urtasun
Neuroaesthetics in Fashion: Modeling the Perception  of Fashionability
In CVPR, N0NN
N, N  [NN] E
Simo-Serra and H
Ishikawa
Fashion Style in NNN Floats:  Joint Ranking and Classification using Weak Data for Feature Extraction
In CVPR, N0NN
N, N, N, N, N  [NN] J
Sivic, B
Russell, A
Efros, A
Zisserman, and W
Freeman
 Discovering Objects and their location in Images
In ICCV,  N00N
N  [NN] K
Vaccaro, S
Shivakumar, Z
Ding, K
Karahalios, and  R
Kumar
The elements of fashion style
In ACM UIST,  N0NN
N  [NN] A
Veit, B
Kovacs, S
Bell, J
McAuley, K
Bala, and S
Belongie
Learning visual clothing style with heterogeneous  dyadic co-occurrences
In ICCV, N0NN
N, N  [NN] S
Vittayakorn, K
Yamaguchi, A
C
Berg, and T
L
Berg
 Runway to realway: Visual analysis of fashion
In WACV,  N0NN
N, N  [NN] K
Yamaguchi, M
Hadi Kiapour, and T
L
Berg
Paper doll  parsing: Retrieving similar styles to parse clothing items
In  ICCV, N0NN
N, N  [NN] K
Yamaguchi, H
Kiapour, L
Ortiz, and T
Berg
Parsing  clothing in fashion photographs
In CVPR, N0NN
N  NNNN    [N0] K
Yamaguchi, T
Okatani, K
Sudo, K
Murasaki, and  Y
Taniguchi
Mix and match: Joint model for clothing and  attribute recognition
In BMVC, N0NN
N, N, N, N  NNNNSpeaking the Same Language: Matching Machine to Human Captions by Adversarial Training   Speaking the Same Language:  Matching Machine to Human Captions by Adversarial Training  Rakshith ShettyN Marcus RohrbachN,N Lisa Anne HendricksN  Mario FritzN Bernt SchieleN  NMax Planck Institute for Informatics, Saarland Informatics Campus, Saarbrücken, Germany NUC Berkeley EECS, CA, United States NFacebook AI Research  Abstract  While strong progress has been made in image captioning recently, machine and human captions are still quite distinct
This is primarily due to the deficiencies in the generated word distribution, vocabulary size, and strong bias in  the generators towards frequent captions
Furthermore, humans – rightfully so – generate multiple, diverse captions,  due to the inherent ambiguity in the captioning task which  is not explicitly considered in today’s systems
 To address these challenges, we change the training objective of the caption generator from reproducing groundtruth captions to generating a set of captions that is indistinguishable from human written captions
Instead of  handcrafting such a learning target, we employ adversarial training in combination with an approximate Gumbel  sampler to implicitly match the generated distribution to the  human one
While our method achieves comparable performance to the state-of-the-art in terms of the correctness of  the captions, we generate a set of diverse captions that are  significantly less biased and better match the global uni-,  bi- and tri-gram distributions of the human captions
 N
Introduction  Image captioning systems have a variety of applications  ranging from media retrieval and tagging to assistance for  the visually impaired
In particular, models which combine  state-of-the-art image representations based on deep convolutional networks and deep recurrent language models have  led to ever increasing performance on evaluation metrics  such as CIDEr [NN] and METEOR [N] as can be seen e.g
 on the COCO image Caption challenge leaderboard [N]
 Despite these advances, it is often easy for humans to  differentiate between machine and human captions – particularly when observing multiple captions for a single image
 Ours: a person on skis jumping  over a ramp  Ours: a skier is making a turn  on a course  Ours: a cross country skier  makes his way through the snow  Ours: a skier is headed down a  steep slope  Baseline: a man riding skis down a snow covered slope  Figure N: Four images from the test set related to skiing,  with captions from our model and a baseline
Baseline describes all four images with a generic caption, whereas our  model produces diverse and more image specific captions
 As we analyze in this paper, this is likely due to artifacts and  deficiencies in the statistics of the generated captions, which  is more apparent when observing multiple samples
Specifically, we observe that state-of-the-art systems frequently  “reveal themselves” by generating a different word distribution and using smaller vocabulary
Further scrutiny reveals  that generalization from the training set is still challenging  and generation is biased to frequent fragments and captions
 Also, today’s systems are evaluated to produce a single  caption
Yet, multiple potentially distinct captions are typically correct for a single image – a property that is reflected  in human ground-truth
This diversity is not equally reproduced by state-of-the-art caption generators [N0, NN]
 Therefore, our goal is to make image captions less distinguishable from human ones – similar in the spirit to a Turing  NNNN    Test
We also embrace the ambiguity of the task and extend  our investigation to predicting sets of captions for a single  image and evaluating their quality, particularly in terms of  the diversity in the generated set
In contrast, popular approaches to image captioning are trained with an objective  to reproduce the captions as provided by the ground-truth
 Instead of relying on handcrafting loss-functions to  achieve our goal, we propose an adversarial training mechanism for image captioning
For this we build on Generative  Adversarial Networks (GANs) [NN], which have been successfully used to generate mainly continuous data distributions such as images [N, N0], although exceptions exist [NN]
 In contrast to images, captions are discrete, which poses a  challenge when trying to backpropagate through the generation step
To overcome this obstacle, we use a Gumbel  sampler [N0, NN] that allows for end-to-end training
 We address the problem of caption set generation for images and discuss metrics to measure the caption diversity  and compare it to human ground-truth
We contribute a  novel solution to this problem using an adversarial formulation
The evaluation of our model shows that accuracy of  generated captions is on par to the state-of-the-art, but we  greatly increase the diversity of the caption sets and better  match the ground-truth statistics in several measures
Qualitatively, our model produces more diverse captions across  images containing similar content (Figure N) and when sampling multiple captions for an image (see supplementary)N
 N
Related Work  Image Description
Early captioning models rely on first  recognizing visual elements, such as objects, attributes, and  activities, and then generating a sentence using language  models such as a template model [NN], n-gram model [NN],  or statistical machine translation [NN]
Advances in deep  learning have led to end-to-end trainable models that combine deep convolutional networks to extract visual features  and recurrent networks to generate sentences [NN, NN, NN]
 Though modern description models are capable of producing coherent sentences which accurately describe an  image, they tend to produce generic sentences which are  replicated from the train set [N0]
Furthermore, an image  can correspond to many valid descriptions
However, at  test time, sentences generated with methods such as beam  search are generally very similar
[N0, NN] focus on increasing sentence diversity by integrating a diversity promoting  heuristic into beam search
[NN] attempts to increase the  diversity in caption generation by training an ensemble of  caption generators each specializing in different portions of  the training set
In contrast, we focus on improving diversity of generated captions using a single model
Our method  achieves this by learning a corresponding model using a difNhttps://goo.gl/NyRVnq  ferent training loss as opposed to after training has completed
We note that generating diverse sentences is also  a challenge in visual question generation, see concurrent  work [NN], and in language-only dialogue generation studied in the linguistic community, see e.g
[NN, NN]
 When training recurrent description models, the most  common method is to predict a word wt conditioned on an image and all previous ground truth words
At test time,  each word is predicted conditioned on an image and previously predicted words
Consequently, at test time predicted  words may be conditioned on words that were incorrectly  predicted by the model
By only training on ground truth  words, the model suffers from exposure bias [NN] and cannot effectively learn to recover when it predicts an incorrect  word during training
To avoid this, [N] proposes a scheduled sampling training scheme which begins by training  with ground truth words, but then slowly conditions generated words on words previously produced by the model
 However, [NN] shows that the scheduled sampling algorithm  is inconsistent and the optimal solution under this objective does not converge to the true data distribution
Taking a different direction, [NN] proposes to address the exposure bias by gradually mixing a sequence level loss (BLEU  score) using REINFORCE rule with the standard maximum  likelihood training
Several other works have followed this  up with using reinforcement learning based approaches to  directly optimize the evaluation metrics like BLEU, METEOR and CIDER [NN, NN]
However, optimizing the evaluation metrics does not directly address the diversity of the  generated captions
Since all current evaluation metrics use  n-gram matching to score the captions, captions using more  frequent n-grams are likely to achieve better scores than  ones using rarer and more diverse n-grams
 In this work, we formulate our caption generator as a  generative adversarial network
We design a discriminator  that explicitly encourages generated captions to be diverse  and indistinguishable from human captions
The generator is trained with an adversarial loss with this discriminator
Consequently, our model generates captions that better  reflect the way humans describe images while maintaining  similar correctness as determined by a human evaluation
 Generative Adversarial Networks
The Generative Adversarial Networks (GANs) [NN] framework learns generative models without explicitly defining a loss from a target distribution
Instead, GANs learn a generator using a  loss from a discriminator which tries to differentiate real  and generated samples, where the generated samples come  from the generator
When training to generate real images,  GANs have shown encouraging results [N, N0]
In all these  works the target distribution is continuous
In contrast our  target, a sequence of words, is discrete
Applying GANs to  discrete sequences is challenging as it is unclear how to best  back-propagate the loss through the sampling mechanism
 NNNN  https://goo.gl/NyRVnq   A few works have looked at generating discrete distributions using GANs
[NN] aim to generate a semantic image  segmentation with discrete semantic labels at each pixel
 [NN] uses REINFORCE trick to train an unconditional text  generator using the GAN framework but diversity of the  generated text is not considered
 Most similar to our work are concurrent works which use  GANs for dialogue generation [NN] and image caption generation [N]
While [NN, NN, N] rely on the reinforcement rule  [NN] to handle backpropagation through the discrete samples, we use the Gumbel Softmax [N0]
See Section N.N for  further discussion
[NN] aims to generate a diverse dialogue  of multiple sentences while we aim to produce diverse sentences for a single image
Additionally, [NN] uses both the  adversarial and the maximum likelihood loss in each step  of generator training
We however train the generator with  only adversarial loss after pre-training
Concurrent work [N]  also applies GANs to diversify generated image captions
 Apart from using the gumbel softmax as discussed above,  our work differs from [N] in the discriminator design and  quantitative evaluation of the generator diversity
 N
Adversarial Caption Generator  The image captioning task can be formulated as follows:  given an input image x the generator G produces a caption, G(x) = [w0, 


, wn−N], describing the contents of the im- age
There is an inherent ambiguity in the task, with multiple possible correct captions for an image, which is also reflected in diverse captions written by human annotators (we  quantify this in Table N)
However, most image captioning  architectures ignore this diversity during training
The standard approach to model G(x) is to use a recurrent language model conditioned on the input image x [NN, NN], and train it using a maximum likelihood (ML) loss considering every  image–caption pair as an independent sample
This ignores  the diversity in the human captions and results in models  that tend to produce generic and commonly occurring captions from the training set, as we will show in Section N.N
 We propose to address this by explicitly training the generator G to produce multiple diverse captions for an input image using the adversarial framework [NN]
In adversarial frameworks, a generative model is trained by pairing it  with adversarial discriminator which tries to distinguish the  generated samples from true data samples
The generator is  trained with the objective to fool the discriminator, which is  optimal when G exactly matches the data distribution
This is well-suited for our goal because, with an appropriate discriminator network we could coax the generator to capture  the diversity in the human written captions, without having  to explicitly design a loss function for it
 To enable adversarial training, we introduce a second  network, D(x, s), which takes as input an image x and a caption set Sp = {sN, 


, sp} and classifies it as either real  Figure N: Caption generator model
Deep visual features are  input to an LSTM to generate a sentence
A Gumbel sampler is used to obtain soft samples from the softmax distribution, allowing for backpropagation through the samples
 or fake
Providing a set of captions per image as input to the  discriminator allows it to factor in the diversity in the caption set during the classification
The discriminator can penalize the generator for producing very similar or repeated  captions and thus encourage the diversity in the generator
 Specifically, the discriminator is trained to classify the  captions drawn from the reference captions set, R(x) = {r0, · · · , rk−N}, as real while classifying the captions pro- duced by the generator, G(x), as fake
The generator G can now be trained using an adversarial objective, i.e
G is trained to fool the discriminator to classify G(x) as real
 N.N
Caption generator  We use a near state-of-the art caption generator model  based on [NN]
It uses the standard encoder-decoder framework with two stages: the encoder model which extracts  feature vectors from the input image and the decoder which  translates these features into a word sequence
 Image features
Images are encoded as activations from a  pre-trained convolutional neural network (CNN)
Captioning models also benefit from augmenting the CNN features  with explicit object detection features [NN]
Accordingly,  we extract a feature vector containing the probability of occurrence of an object and provide it as input to the generator
 Language Model
Our decoder shown in Figure N, is  adopted from a Long-Short Term Memory (LSTM) based  language model architecture presented in [NN] for image  captioning
It consists of a three-layered LSTM network  with residual connections between the layers
The LSTM  network takes two features as input
First is the object detection feature, xo, which is input to the LSTM at only 0th time step and shares the input matrix with the word vectors
 Second is the global image CNN feature, xc, and is input to the LSTM at all time-steps through its own input matrix
 The softmax layer at the output of the generator produces  NNNN    a probability distribution over the vocabulary at each step
 yt = LSTM(wt−N, xc, yt−N, ct−N) (N)  p(wt|wt−N, x) = softmax [βWd ∗ yt] , (N)  where ct is the LSTM cell state at time t and β is a scalar parameter which controls the peakyness of the distribution
 Parameter β allows us to control how large a hypothesis space the generator explores during adversarial training
An  additional uniform random noise vector z, is input to the LSTM in adversarial training to allow the generator to use  the noise to produce diversity
 Discreteness Problem
To produce captions from the  generator we could simply sample from this distribution  p(wt|wt−N, x), recursively feeding back the previously sampled word at each step, until we sample the END token
One can generate multiple sentences by sampling and  pick the sentence with the highest probability as done in  [NN]
Alternatively we could also use greedy search approaches like beam-search
However, directly providing  these discrete samples as input to the discriminator does  not allow for backpropagation through them as they are discontinuous
Alternatives to overcome this are the reinforce  rule/trick [NN], using the softmax distribution, or using the  Gumbel-Softmax approximation [N0, NN]
 Using policy gradient algorithms with the reinforce  rule/trick [NN] allows estimation of gradients through discrete samples [NN, N, NN, NN]
However, learning using reinforce trick can be unstable due to high variance [NN] and  some mechanisms to make learning more stable, like estimating the action-value for intermediate states by generating multiple possible sentence completions (e.g used  in [NN, N]), can be computationally intensive
 Another option is to input the softmax distribution to the  discriminator instead of samples
We experimented with  this, but found that the discriminator easily distinguishes  between the softmax distribution produced by the generator  and the sharp reference samples, and the GAN training fails
 The last option, which we rely on in this work, it to  use a continuous relaxation of the samples encoded as onehot vectors using the Gumbel-Softmax approximation proposed in [N0] and [NN]
This continuous relaxation combined with the re-parametrization of the sampling process  allows backpropagation through samples from a categorical distribution
The main benefit of this approach is that it  plugs into the model as a differentiable node and does not  need any additional steps to estimate the gradients
Whereas  most previous methods to applying GAN to discrete output generators use policy gradient algorithms, we show that  Gumbel-Softmax approximation can also be used successfully in this setting
An empirical comparison between the  two approaches can be found in [N0]
 We use straight-through variation of the GumbelSoftmax approximation [N0] at the output of our generator  Figure N: Discriminator Network
Caption set sampled  from the generator is used to compute image to sentence  (distx(Sp, x)) and sentence-to-sentence (dists(Sp)) dis- tances
They are used to score the set as real/fake
 to sample words during the adversarial training
 N.N
Discriminator model  The discriminator network, D takes an image x, repre- sented using CNN feature xc, and a set of captions Sp = {sN, 


, sp} as input and classifies Sp as either real or fake
Ideally, we want D to base this decision on two criteria: a) do si ∈ Sp describe the image correctly ? b) is the set Sp is diverse enough to match the diversity in human captions ?  To enable this, we use two separate distance measuring  kernels in our discriminator network as shown in Figure N
 The first kernel computes the distances between the image  x and each sentence in Sp
The second kernel computes the distances between the sentences in Sp
The architecture of these distance measuring kernels is based on the minibatch  discriminator presented in [NN]
However, unlike [NN], we  only compute distances between captions corresponding to  the same image and not over the entire minibatch
 Input captions are encoded into a fixed size sentence embedding vector using an LSTM encoder to obtain vectors  f(si) ∈ R M 
The image feature, xc, is also embedded into  a smaller image embedding vector f(xc) ∈ R M 
The distances between f(si), i ∈ {N, 


, p} are computed as  Ki = Ts · f(si) (N)  cl(si, sj) = exp (−‖Ki,l −Kj,l‖LN) (N)  dl(si) =  p ∑  j=N  cl(si, sj) (N)  dists(Sp) = [dN(sN), ..., dO(sN), ..., dO(sp)] ∈ R p×O (N)  where Ts is a M ×N ×O dimensional tensor and O is the number of different M ×N distance kernels to use
 NNNN    Distances between f(si), i ∈ N, 


, p and f(xc) are ob- tained with similar procedure as above, but using a different  tensor Tx of dimensions M×N×O to yield distx(Sp, x) ∈ R  p×O
These two distance vectors capture the two aspects  we want our discriminator to focus on
distx(Sp, x) cap- tures how well Sp matches the image x and dists(Sp) cap- tures the diversity in Sp
The two distance vectors are concatenated and multiplied with a output matrix followed  by softmax to yield the discriminator output probability,  D(Sp, x), for Sp to be drawn from reference captions
 N.N
Adversarial Training  In adversarial training both the generator and the discriminator are trained alternatively for ng and nd steps re- spectively
The discriminator tries to classify Srp ∈ R(x) as real and Sgp ∈ G(x) as fake
In addition to this, we found it important to also train the discriminator to classify few  reference captions drawn from a random image as fake, i.e
 Sfp ∈ R(y), y N= x
This forces the discriminator to learn to match images and captions, and not just rely on diversity  statistics of the caption set
The complete loss function of  the discriminator is defined by  L(D) = − log (  D(Srp , x) )  − log (  N−D(Sgp , x) )  − log (  N−D(Sfp , x) )  (N)  The training objective of the generator is to fool the discriminator into classifying Sgp ∈ G(x) as real
We found helpful to additionally use the feature matching loss [NN]
 This loss trains the generator to match activations induced  by the generated and true data at some intermediate layer  of the discriminator
In our case we use an lN loss to match the expected value of distance vectors dists(Sp) and distx(Sp, x) between real and generated data
The genera- tor loss function is given by  L(G) = − log (  D(Sgp , x) )  +‖E [  dists(S g p) ]  − E [  dists(S r p) ]  ‖N  +‖E [  distx(S g p , x)  ]  − E [  distx(S r p , x)  ]  ‖N,  (N)  where the expectation is over a training mini-batch
 N
Experimental Setup  We conduct all our experiments on the MS-COCO  dataset [N]
The training set consists of NNk images with  five human captions each
We use the publicly available  test split of N000 images [NN] for all our experiments
Section N.N uses a validation split of N000 images
 For image feature extraction, we use activations from  resNc layer of the NNN-layered ResNet [NN] convolutional  neural network (CNN) pre-trained on ImageNet
The input  images are scaled to NNN× NNN dimensions for ResNet fea- ture extraction
Additionally we use features from the VGG  network [NN] in our ablation study in Section N.N
Following [NN], we additionally extract N0-dimensional object detection features using a Faster Region-Based Convolutional  Neural Network (RCNN) [NN] trained on the N0 object categories in the COCO dataset
The CNN features are input  to both the generator (at xp) and the discriminator
Object detection features are input only to the generator at the xi input and is used in all the generator models reported here
 N.N
Insights in Training the GAN  As is well known [N], we found GAN training to be sensitive to hyper-parameters
Here we discuss some settings  which helped stabilize the training of our models
 We found it necessary to pre-train the generator using standard maximum likelihood training
Without pretraining, the generator gets stuck producing incoherent sentences made of random word sequences
We also found  pre-training the discriminator on classifying correct imagecaption pairs against random image-caption pairs helpful to  achieve stable GAN training
We train the discriminator for  N iterations for every generator update
We also periodically  monitor the classification accuracy of the discriminator and  train it further if it drops below NN%
This prevents the generator from updating using a bad discriminator
 Without the feature matching term in the generator loss,  the GAN training was found to be unstable and needed additional maximum likelihood update to stabilize it
This was  also reported in [NN]
However with the feature matching  loss, training is stable and the ML update is not needed
 A good range of values for the Gumbel temperature was  found to be (0.N, 0.N)
Beyond this range training was un- stable, but within this range the results were not sensitive to  it
We use a fixed temperature setting of 0.N in the experi- ments reported here
The softmax scaling factor, β in (N), is set to value N.0 for training of all the adversarial models reported here
The sampling results are also with β = N.0
 N
Results  We conduct experiments to evaluate our adversarial caption generator w.r.t
two aspects: how human-like the generated captions are and how accurately they describe the  contents of the image
Using diversity statistics and word  usage statistics as a proxy for measuring how closely the  generated captions mirror the distribution of the human reference captions, we show that the adversarial model is more  human-like than the baseline
Using human evaluation and  automatic metrics we also show that the captions generated  by the adversarial model performs similar to the baseline  model in terms of correctness of the caption
 Henceforth, Base and Adv refer to the baseline and adversarial models, respectively
Suffixes bs and samp indicate decoding using beamsearch and sampling respectively
 NNNN    N.N
Measuring if captions are human-like  Diversity
We analyze n-gram usage statistics, compare vo- cabulary sizes and other diversity metrics presented below  to understand and measure the gaps between human written captions and the automatic methods and show that the  adversarial training helps bridge some of these gaps
 To measure the corpus level diversity of the generated  captions we use:  • Vocabulary Size - number of unique words used in all generated captions  • % Novel Sentences - percentage of generated captions not seen in the training set
 To measure diversity in a set of captions, Sp, corresponding to a single image we use:  • Div-N - ratio of number of unique unigrams in Sp to number of words in Sp
Higher is more diverse
 • Div-N - ratio of number of unique bigrams in Sp to number of words in Sp
Higher is more diverse
 • mBleu - Bleu score is computed between each caption in Sp against the rest
Mean of these p Bleu scores is the mBleu score
Lower values indicate more diversity
 Correctness
Just generating diverse captions is not useful  if they do not correctly describe the content of an image
To  measure the correctness of the generated captions we use  two automatic evaluation metrics Meteor [N] and SPICE [N]
 However since it is known that the automatic metrics do  not always correlate very well with human judgments of the  correctness, we also report results from human evaluations  comparing the baseline model to our adversarial model
 N.N
Comparing caption accuracy  Table N presents the comparison of our adversarial model  to the baseline model
Both the baseline and the adversarial models use ResNet features
The beamsearch results are  with beam size N and sampling results are with taking the  best of N samples
Here the best caption is obtained by ranking the captions as per probability assigned by the model
 Table N also shows the metrics from some recent methods from the image captioning literature
The purpose of  this comparison is to illustrate that we use a strong baseline  and that our baseline model is competitive to recent published work, as seen from the Meteor and Spice metrics
 Comparing baseline and adversarial models in Table N  the adversarial model does worse in-terms of Meteor scores  and overall spice metrics
When we look at Spice scores on  individual categories shown in Table N we see that adversarial models excel at counting relative to the baseline and  describing the size of an object correctly
 However, it is well known that automatic metrics do not  always correlate with human judgments on correctness of a  caption
A primary reason the adversarial models do poorly  on automatic metrics is that they produce significantly more  Method Meteor Spice  ATT-FCN [NN] 0.NNN –  MSM [NN] 0.NNN –  KWL [NN] 0.NNN 0.NNN  Ours Base-bs 0.NNN 0.NNN  Ours Base-samp 0.NNN 0.NNN  Ours Adv-bs 0.NNN 0.NNN  Ours Adv-samp 0.NNN 0.NNN  Table N: Meteor and Spice metrics comparing performance  of baseline and adversarial models
 Method Spice  Color Attribute Object Relation Count Size  Base-bs 0.N0N 0.0NN 0.NNN 0.0NN 0.0NN 0.0NN  Base-samp 0.0NN 0.0NN 0.NNN 0.0NN 0.0NN 0.0NN  Adv-bs 0.0NN 0.0NN 0.NNN 0.0NN 0.0N0 0.0NN  Adv-samp 0.0NN 0.0NN 0.NNN 0.0NN 0.0NN 0.0NN  Table N: Comparing baseline and adversarial models in different categories of Spice metric
 Comparison Adversarial - Better Adversarial - Worse  Beamsearch NN.N NN.N  Sampling NN.N NN.N  Table N: Human evaluation comparing adversarial model vs  the baseline model on NNN random samples
Correctness  of captions
With agreement of at least N out of N judges  in %
Humans agreed in NN.N% and NN.N% of images in  beamsearch and sampling cases respectively
 unique sentences using a much larger vocabulary and rarer  n-grams, as shown in Section N.N
Thus, they are less likely  to do well on metrics relying on n-gram matches
To verify this claim, we conduct human evaluations  comparing captions from the baseline and the adversarial  model
Human evaluators from Amazon Mechanical Turk  are shown an image and a caption each from the two models and are asked “Judge which of the two sentences is a  better description of the image (w.r.t
correctness and relevance)!”
The choices were either of the two sentences or to  report that they are the same
Results from this evaluation  are presented in Table N
We can see that both adversarial and baseline models perform similarly, with adversarial  models doing slightly better
This shows that despite the  poor performance in automatic evaluation metrics, the adversarial models produce captions that are similar, or even  slightly better, in accuracy to the baseline model
 N.N
Comparing vocabulary statistics  To characterize how well the captions produced by the  automatic methods match the statistics of the human written  NNN0    N00 N0N N0N N0N N0N N0N Uni-gram counts  0.0  0.N  N.0  N.N  N.0  N.N  N.0  N.N  N.0  N.N  M ea  n  U  ni -g  ra m   c ou  nt  ra  tio  adversarial baseline test-references  0 N0 N00 NN0 N00 NN0 N00 NN0 N00 NN0 number of words  0.0  0.N  N.0  N.N  N.0  N.N  N.0  N.N  N.0  U ni  -g ra  m  c  ou nt   ra tio  adversarial baseline test-references  N00 N0N N0N N0N N0N Bi-gram counts  0.0  0.N  N.0  N.N  N.0  N.N  N.0  N.N  N.0  N.N  M ea  n  Bi  -g ra  m  c  ou nt   ra tio  adversarial baseline test-references  0 N0 N00 NN0 N00 NN0 N00 NN0 N00 NN0 number of Bi-grams  0.0  0.N  N.0  N.N  N.0  N.N  N.0  N.N  N.0  Bi -g  ra m   c ou  nt  ra  tio  adversarial baseline test-references  N00 N0N N0N N0N Tri-gram counts  0.0  0.N  N.0  N.N  N.0  N.N  N.0  N.N  N.0  N.N  M ea  n  Tr  i-g ra  m  c  ou nt   ra tio  adversarial baseline test-references  0 N0 N00 NN0 N00 NN0 N00 NN0 N00 number of Tri-grams  0.0  0.N  N.0  N.N  N.0  N.N  N.0  N.N  N.0  Tr i-g  ra m   c ou  nt  ra  tio  adversarial baseline test-references  Figure N: Comparison of n-gram count ratios in generated test-set captions by different models
Left side shows the  mean n-gram count-ratios as a function of counts on train- ing set
Right side shows the histogram of the count-ratios
 captions, we look at n-gram usage statistics in the generated captions
Specifically, we compute the ratio of the actual  count of an n-gram in the caption set produced by a model to the expected n-gram count based on the training data
 Given that an n-gram occurred m times in the training set we can expect that it occurs m ∗ |test-set|/|train-set| times in the test set
However actual counts may vary depending  on how different the test set is from the training set
We  compute these ratios for reference captions in the test set to  get an estimate of the expected variance of the count ratios
 The left side of Figure N shows the mean count ratios for  uni-, bi- and tri-grams in the captions generated on test-set  plotted against occurrence counts in the training set
Histogram of these ratios are shown on the right side
 Count ratios for the reference captions from the test-set  are shown in green
We see that the n-gram counts match well between the training and test set human captions and  the count ratios are spread around N.0 with a small variance
 The baseline model shows a clear bias towards more frequently occurring n-grams
It consistently overuses more  Vocab- % Novel  Method n Div-N Div-N mBleu-N ulary Sentences  Base-bs N of N – – – NNN NN.NN  N of N 0.NN 0.NN 0.NN N0NN NN.NN  Base-samp N of N – – – NNN NN.0N  N of N 0.NN 0.NN 0.NN NNN0 NN.NN  Adv-bs N of N – – – NN0N NN.NN  N of N 0.NN 0.NN 0.N0 NNNN NN.NN  Adv-samp N of N – – – NNNN NN.NN  N of N 0.NN 0.NN 0.NN NNNN NN.NN  Human N of N – – – NNNN NN.N0  captions N of N 0.NN 0.NN 0.N0 NNNN NN.0N  Table N: Diversity Statistics described in Section N.N
 Higher values correspond to more diversity in all except  mBleu-N, where lower is better
 Advbs  a group of friends enjoying a dinner at the  restauarant  several cows in their  pen at the farm  A dog is trying to get  something out of the  snow  Basebs  a group of people sitting  around a wooden table  a herd of cattle standing next to each other  a couple of dogs that  are in the snow  Figure N: Some qualitative examples comparing comparing  captions generated by the our model to the baseline model
 frequent n-grams (ratio>N.0) from the training set and under-uses less frequent ones (ratio<N.0)
This trend is seen in all the three plots, with more frequent tri-grams particularly prone to overuse
It can also be observed in the  histogram plots of the count ratios, that the baseline model  does a poor job of matching the statistics of the test set
 Our adversarial model does a much better job in matching these statistics
The histogram of the uni-gram count  ratios are clearly closer to that of test reference captions
 It does not seem to be significantly overusing the popular  words, but there is still a trend of under utilizing some of  the rarer words
It is however clearly better than the baseline  model in this aspect
The improvement is less pronounced  with the bi- and tri-grams, but still present
 Another clear benefit from using the adversarial training  is observed in terms of diversity in the captions produced by  the model
The diversity in terms of both global statistics  and per image diversity statistics is much higher in captions  produced by the adversarial models compared to the baseline models
This result is presented in Table N
We can see  that the vocabulary size approximately doubles from N0NN  in the baseline model to NNNN in the adversarial model usNNNN    N00 N0N N0N N0N N0N N0N Threshold on Word Count  N00  N0N  N0N  N0N  N0N  Vo ca  bu la  ry  S  iz e  test-references baseline adversarial  Figure N: Vocabulary size as a function of word counts
 ing beamsearch
A similar trend is also seen comparing the  sampling variants
As expected more diversity is achieved  when sampling from the adversarial model instead of using beamsearch with vocabulary size increasing to NNNN in  Adv-samp
The effect of this increased diversity can be in  the qualitative examples shown in Figure N
More qualitative samples are included in the supplementary material
 We can also see that the adversarial model learns to  construct significantly more novel sentences compared to  the baseline model with Adv-bs producing novel captions  NN.NN% of the time compared to just NN.NN% by the beambs
All three per-image diversity statistics also improve in  the adversarial models indicating that they can produce a  more diverse set of captions for any input image
 Table N also shows the diversity statistics on the reference captions on the test set
This shows that although adversarial models do considerably better than the baseline,  there is still a gap in diversity statistics when compared to  the human written captions, especially in vocabulary size
 Finally, Figure N plots the vocabulary size as a function  of word count threshold, k
We see that the curve for the adversarial model better matches the human written captions  compared to the baseline for all values of k
This illustrates  that the gains in vocabulary size in adversarial models does  not arise from using words with specific frequency, but is  instead distributed evenly across word frequencies
 N.N
Ablation Study  We conducted experiments to understand the importance  of different components of our architecture
The results are  presented in Table N
The baseline model for this experiment uses VGG [NN] features as xp input and is trained us- ing maximum likelihood loss and is shown in the first row  of Table N
The other four models use adversarial training
 Comparing rows N and N of Table N, we see that adversarial training with a discriminator evaluating a single caption  does badly
Both the diversity and Meteor score drop compared to the baseline
In this setting the generator can get  away with producing one good caption (mode collapse) for  Image  Feature  Evalset  size (p)  Feature  Matching  Meteor Div-N Vocab
 Size  VGG baseline 0.NNN 0.NN NNNN  VGG N No 0.NNN 0.N0 NNN  VGG N No 0.NNN 0.NN NNN0  VGG N yes 0.N0N 0.NN NNNN  ResNet N yes 0.NNN 0.NN NNNN  Table N: Performance comparison of various configurations  of the adversarial caption generator on the validation set
 an image as the discriminator is unable to penalize the lack  of diversity in the generator
 However, comparing rows N and N, we see that adversarial training using a discriminator evaluating N captions  simultaneously does much better in terms of Div-N and vocabulary size
Adding feature matching loss further improves the diversity and also slightly improves accuracy  in terms of Meteor score
Thus simultaneously evaluating  multiple captions and using feature matching loss allows us  to alleviate mode collapse generally observed in GANs
 Upgrading to the ResNet[NN] increases the Meteor score  greatly and slightly increases the vocabulary size
ResNet  features provide richer visual information which is used by  the generator to produce diverse but still correct captions
 We also notice that the generator learns to ignore the input noise
This is because there is sufficient stochasticity in  the generation process due to sequential sampling of words  and thus the generator doesn’t need the additional noise input to increase output diversity
Similar observation was  reported in other conditional GAN architectures [NN, NN]  N
Conclusions  We have presented an adversarial caption generator  model which is explicitly trained to generate diverse captions for images
We achieve this by utilizing a discriminator network designed to promote diversity and use the adversarial learning framework to train our generator
Results  show that our adversarial model produces captions which  are diverse and match the statistics of human generated captions significantly better than the baseline model
The adversarial model also uses larger vocabulary and is able to  produce significantly more novel captions
The increased  diversity is achieved while preserving accuracy of the generated captions, as shown through a human evaluation
 Acknowledgements  This research was supported by the German Research  Foundation (DFG CRC NNNN) and by the Berkeley Artificial Intelligence Research (BAIR) Lab
 NNNN    References  [N] P
Anderson, B
Fernando, M
Johnson, and S
Gould
Spice:  Semantic propositional image caption evaluation
In European Conference on Cmoputer Vision (ECCV), N0NN
N  [N] J
Andreas and D
Klein
Reasoning about pragmatics with  neural listeners and speakers
In Proceedings of the Conference on Empirical Methods in Natural Language Processing  (EMNLP), N0NN
N  [N] M
Arjovsky and L
Bottou
Towards principled methods for  training generative adversarial networks
In Proceedings of  the International Conference on Learning Representations  (ICLR), N0NN
N  [N] S
Bengio, O
Vinyals, N
Jaitly, and N
Shazeer
Scheduled  sampling for sequence prediction with recurrent neural networks
In Advances in Neural Information Processing Systems (NIPS), N0NN
N  [N] X
Chen, T.-Y
L
Hao Fang, R
Vedantam, S
Gupta, P
Dollr,  and C
L
Zitnick
Microsoft COCO captions: Data collection and evaluation server
arXiv preprint arxiv:NN0N.00NNN,  N0NN
N  [N] COCO
Microsoft COCO Image Captioning Challenge
https://competitions.codalab.org/  competitions/NNNN#results, N0NN
N  [N] B
Dai, D
Lin, R
Urtasun, and S
Fidler
Towards diverse  and natural image descriptions via a conditional gan
N0NN
 N, N  [N] M
Denkowski and A
Lavie
Meteor universal: Language  specific translation evaluation for any target language
ACL  N0NN, N0NN
N, N  [N] E
L
Denton, S
Chintala, R
Fergus, et al
Deep generative image models using a laplacian pyramid of adversarial  networks
In Advances in Neural Information Processing  Systems (NIPS), N0NN
N  [N0] J
Devlin, H
Cheng, H
Fang, S
Gupta, L
Deng, X
He,  G
Zweig, and M
Mitchell
Language models for image captioning: The quirks and what works
In Proceedings of the  Annual Meeting of the Association for Computational Linguistics (ACL), N0NN
N  [NN] J
Donahue, L
A
Hendricks, S
Guadarrama, M
Rohrbach,  S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual recognition and description
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), N0NN
N, N  [NN] J
Donahue, L
A
Hendricks, M
Rohrbach, S
Venugopalan,  S
Guadarrama, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual recognition and description
IEEE Transactions on Pattern Analysis and Machine Intelligence, N0NN
N  [NN] A
Farhadi, M
Hejrati, M
Sadeghi, P
Young, C
Rashtchian,  J
Hockenmaier, and D
Forsyth
Every picture tells a story:  Generating sentences from images
In Proceedings of the  European Conference on Computer Vision (ECCV), N0N0
N  [NN] I
Goodfellow, J
Pouget-Abadie, M
Mirza, B
Xu,  D
Warde-Farley, S
Ozair, A
Courville, and Y
Bengio
Generative adversarial nets
In Advances in Neural Information  Processing Systems (NIPS), N0NN
N, N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),  N0NN
N, N  [NN] L
A
Hendricks, Z
Akata, M
Rohrbach, J
Donahue,  B
Schiele, and T
Darrell
Generating visual explanations
 In Proceedings of the European Conference on Computer Vision (ECCV), N0NN
N  [NN] F
Huszar
How (not) to train your generative model:  Scheduled sampling, likelihood, adversary? arXiv preprint  arXiv:NNNN.0NN0N, N0NN
N  [NN] P
Isola, J.-Y
Zhu, T
Zhou, and A
A
Efros
Image-to-image  translation with conditional adversarial networks
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), N0NN
N  [NN] U
Jain, Z
Zhang, and A
Schwing
Creativity: Generating  diverse questions using variational autoencoders
N0NN
N  [N0] E
Jang, S
Gu, and B
Poole
Categorical reparameterization with gumbel-softmax
Proceedings of the International  Conference on Learning Representations (ICLR), N0NN
N, N,  N  [NN] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In Proceedings  of the IEEE Conference on Computer Vision and Pattern  Recognition (CVPR), N0NN
N, N  [NN] G
Kulkarni, V
Premraj, V
Ordonez, S
Dhar, S
Li, Y
Choi,  A
C
Berg, and T
L
Berg
Babytalk: Understanding and  generating simple image descriptions
IEEE Transactions  on Pattern Analysis and Machine Intelligence, NN(NN), N0NN
 N  [NN] J
Li, M
Galley, C
Brockett, J
Gao, and B
Dolan
A  diversity-promoting objective function for neural conversation models
In Proceedings of the Conference of the North  American Chapter of the Association for Computational Linguistics (NAACL), N0NN
N, N  [NN] J
Li, W
Monroe, T
Shi, A
Ritter, and D
Jurafsky
Adversarial learning for neural dialogue generation
In Proceedings of the Conference on Empirical Methods in Natural  Language Processing (EMNLP), N0NN
N, N, N, N  [NN] S
Liu, Z
Zhu, N
Ye, S
Guadarrama, and K
Murphy
Optimization of image description metrics using policy gradient  methods
arXiv preprint arXiv:NNNN.00NN0, N0NN
N  [NN] J
Lu, C
Xiong, D
Parikh, and R
Socher
Knowing when  to look: Adaptive attention via a visual sentinel for image  captioning
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), N0NN
N  [NN] P
Luc, C
Couprie, S
Chintala, and J
Verbeek
Semantic segmentation using adversarial networks
In Advances  in Neural Information Processing Systems Workshops (NIPS  Workshops), N0NN
N, N  [NN] C
J
Maddison, A
Mnih, and Y
W
Teh
The concrete  distribution: A continuous relaxation of discrete random  variables
Proceedings of the International Conference on  Learning Representations (ICLR), N0NN
N, N  [NN] M
Mathieu, C
Couprie, and Y
LeCun
Deep multi-scale  video prediction beyond mean square error
Proceedings of  the International Conference on Learning Representations  (ICLR), N0NN
N  NNNN  https://competitions.codalab.org/competitions/NNNN#results https://competitions.codalab.org/competitions/NNNN#results   [N0] A
Radford, L
Metz, and S
Chintala
Unsupervised representation learning with deep convolutional generative adversarial networks
Proceedings of the International Conference  on Learning Representations (ICLR), N0NN
N  [NN] M
Ranzato, S
Chopra, M
Auli, and W
Zaremba
Sequence  level training with recurrent neural networks
In Proceedings  of the International Conference on Learning Representations  (ICLR), N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN: Towards real-time object detection with region proposal networks
In Advances in Neural Information Processing Systems (NIPS), N0NN
N  [NN] S
J
Rennie, E
Marcheret, Y
Mroueh, J
Ross, and V
Goel
 Self-critical sequence training for image captioning
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition (CVPR), N0NN
N  [NN] M
Rohrbach, W
Qiu, I
Titov, S
Thater, M
Pinkal, and  B
Schiele
Translating video content to natural language  descriptions
In Proceedings of the IEEE International Conference on Computer Vision (ICCV), N0NN
N  [NN] T
Salimans, I
Goodfellow, W
Zaremba, V
Cheung, A
Radford, and X
Chen
Improved techniques for training gans
In  Advances in Neural Information Processing Systems (NIPS),  N0NN
N, N  [NN] R
Shetty, H
R-Tavakoli, and J
Laaksonen
Exploiting  scene context for image captioning
In ACMMM Vision and  Language Integration Meets Multimedia Fusion Workshop,  N0NN
N, N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In Proceedings  of the International Conference on Learning Representations  (ICLR), N0NN
N, N  [NN] R
S
Sutton and A
G
Barto
Reinforcement learning: An  introduction, volume N
MIT press Cambridge, NNNN
N  [NN] R
Vedantam, C
Lawrence Zitnick, and D
Parikh
CIDEr:  Consensus-based image description evaluation
In Proceedings of the IEEE Conference on Computer Vision and Pattern  Recognition (CVPR), N0NN
N  [N0] A
K
Vijayakumar, M
Cogswell, R
R
Selvaraju, Q
Sun,  S
Lee, D
Crandall, and D
Batra
Diverse beam search: Decoding diverse solutions from neural sequence models
arXiv  preprint arXiv:NNN0.0NNNN, N0NN
N, N  [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), N0NN
N, N  [NN] Z
Wang, F
Wu, W
Lu, J
Xiao, X
Li, Z
Zhang, and  Y
Zhuang
Diverse image captioning via grouptalk
In Proceedings of the International Joint Conference on Artificial  Intelligence (IJCAI), N0NN
N  [NN] R
J
Williams
Simple statistical gradient-following algorithms for connectionist reinforcement learning
Machine  learning, N(N-N), NNNN
N, N  [NN] T
Yao, Y
Pan, Y
Li, Z
Qiu, and T
Mei
Boosting image  captioning with attributes
arXiv preprint arXiv:NNNN.0NNNN,  N0NN
N  [NN] Q
You, H
Jin, Z
Wang, C
Fang, and J
Luo
Image  captioning with semantic attention
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition (CVPR), N0NN
N  [NN] L
Yu, W
Zhang, J
Wang, and Y
Yu
SeqGAN: sequence  generative adversarial nets with policy gradient
Proceedings  of the Conference on Artificial Intelligence (AAAI), N0NN
N,  N  NNNNAttention-Based Multimodal Fusion for Video Description   Attention-Based Multimodal Fusion for Video Description  Chiori Hori Takaaki Hori Teng-Yok Lee Ziming Zhang  Bret Harsham John R
Hershey Tim K
Marks Kazuhiko Sumi∗  Mitsubishi Electric Research Laboratories (MERL)  {chori, thori, tlee, zzhang, harsham, hershey, tmarks}@merl.com, sumi@it.aoyama.ac.jp  Abstract  Current methods for video description are based on  encoder-decoder sentence generation using recurrent neural networks (RNNs)
Recent work has demonstrated the  advantages of integrating temporal attention mechanisms  into these models, in which the decoder network predicts  each word in the description by selectively giving more  weight to encoded features from specific time frames
Such  methods typically use two different types of features: image features (from an object classification model), and motion features (from an action recognition model), combined  by naı̈ve concatenation in the model input
Because different feature modalities may carry task-relevant information at different times, fusing them by naı̈ve concatenation may limit the model’s ability to dynamically determine  the relevance of each type of feature to different parts of  the description
In this paper, we incorporate audio features in addition to the image and motion features
To fuse  these three modalities, we introduce a multimodal attention model that can selectively utilize features from different  modalities for each word in the output description
Combining our new multimodal attention model with standard  temporal attention outperforms state-of-the-art methods on  two standard datasets: YouTubeNText and MSR-VTT
 N
Introduction  Automatic video description, also known as video captioning, refers to the automatic generation of a natural language description, such as a sentence that summarizes an  input video
Video description has widespread applications  including video retrieval, automatic description of home  movies or online uploaded video clips, and video descriptions for the visually impaired
Moreover, developing systems that can describe videos may help us to elucidate some  key components of general machine intelligence
 Recent work in video description has demonstrated the ad∗On sabbatical from Aoyama Gakuin University
 vantages of integrating temporal attention mechanisms into  encoder-decoder neural networks, in which the decoder network predicts each word in the description by selectively  giving more weight to encoded features from different times  in the video
Typically, two different types of features are  used: image features (learned from an object classification  task), and motion features (learned from an action recognition task)
These are combined by naı̈ve concatenation in  the input to the video description model
Because different feature modalities may carry task-relevant information  at different times, fusing them by naı̈ve concatenation may  limit the model’s ability to dynamically determine the relevance of each type of feature to different parts of the description
In this paper, we expand the feature set to include  the audio modality, in addition to the image and motion features
 In this work, we propose a new use of attention: to fuse information across different modalities
Here we use modality  loosely to refer to different types of features derived from  the video, such as appearance, motion, or depth, as well as  features from different sensors such as video and audio features
Different modalities of input may be important for  selecting each word in the description
For example, the  description “A boy is standing on a hill” refers to objects  and their relations
In contrast, “A boy is jumping on a hill”  may rely on motion features to determine the action
“A boy  is listening to airplanes flying overhead” may require audio  features to recognize the airplanes, if they do not appear  in the video
Not only do the relevant modalities change  from sentence to sentence, but also from word to word, as  we move from action words that describe motion to nouns  that define object types
Attention to the appropriate modalities, as a function of the context, may help with choosing  the right words for the video description
Often features  from different modalities can be complementary, in that either can provide reliable cues at different times for some  aspect of a scene
Multimodal fusion is thus an important  longstanding strategy for robustness
However, optimally  combining information requires estimating the reliability of  each modality, which remains a challenging problem
 NNNNN    A longstanding area of research addresses how to effectively combine information from multiple modalities for  machine perception tasks [N0]
Previous methods typically  used stream weights (e.g., [N]) or Bayesian adaptation approaches (e.g., [NN])
As far as we know, our approach is the  first to fuse multimodal information using attention between  modalities in a neural network
Our method dynamically  adjusts the relative importance of each modality to generate  better descriptions
The benefits of attentional multimodal  fusion include: (N) the modalities that are most helpful to  discriminate each word in the description can dynamically  receive a stronger weight, and (N) the network can detect  interference (e.g., noise) and other sources of uncertainty in  each modality and dynamically down-weight the modalities  that are less certain
Not only does our proposed method  achieve these benefits, but it does so using a model that can  be discriminatively trained end-to-end
 In this work, we present results of video description on two  large datasets: YouTubeNText and the subset of MSR-VTT  that was still available at the time of the experiments
We  show that combining our new multimodal attention model  with temporal attention outperforms state-of-the-art methods, which are based on temporal attention alone
 N
Related Work  Sentence generation using an encoder-decoder architecture  was originally used for neural machine translation (NMT),  in which sentences in a source language are converted into  sentences in a target language [N0, N]
In this paradigm,  the encoder takes an input sentence in the source language  and maps it to a fixed-length feature vector in an embedding space
The decoder uses this feature vector as input  to generate a sentence in the target language
However, the  fixed length of the feature vector limited performance, particularly on long input sentences, so [N] proposed to encode  the input sentence as a sequence of feature vectors
They  employed a recurrent neural network (RNN)-based soft attention model that enables the decoder to pay attention to  features derived from specific words of the input sentence  when generating each output word
The encoder-decoder  based sequence to sequence framework has been applied not  only to machine translation but also to other application areas including speech recognition [N], image captioning [N0],  and dialog management [NN]
 In image captioning, the input is a single image, and the  output is a natural-language description
Recent work on  RNN-based image captioning includes [N0, N0]
To improve performance, [NN] added an attention mechanism, to  enable focusing on specific parts of the image when generating each word of the description
Encoder-decoder networks have also been applied to the task of video description [NN]
In this task, the inputs to the encoder network  are video information features that may include static image features extracted using convolutional neural networks  (CNNs), temporal dynamics of videos extracted using spatiotemporal ND CNNs [NN], dense trajectories [NN], optical  flow, and audio features [NN]
From the encoder outputs,  the decoder network generates word sequences using recurrent neural networks (RNNs) with long short-term memory (LSTM) units [NN] or gated recurrent units (GRUs) [N]
 Such systems can be trained end-to-end using videos labeled with text descriptions
 One inherent problem in video description is that the sequence of video features and the sequence of words in the  description are not synchronized
In fact, the order in which  objects and actions appear over time in the video may be  different from their order in the sentence
When choosing  the right words to describe something, the features that directly correspond to that object or action are most relevant,  and other features may be a source of clutter
It may be  possible for an LSTM to learn to selectively encode different objects into its latent features and remember them  until they are retrieved
However, attention mechanisms  have been used to boost the network’s ability to retrieve  the relevant features from the corresponding parts of the input, in applications such as machine translation [N], speech  recognition [N], image captioning [NN], and dialog management [NN]
In recent work, these attention mechanisms have  been applied to video description [NN, NN]
Whereas in image captioning the attention is spatial (attending to specific  regions of the image), in video description the attention may  be temporal (attending to specific time frames of the video)  in addition to (or instead of) spatial
 We first described the proposed method in an arXiv paper [NN]
In this paper, we expand upon [N] by testing on an  additional dataset and precisely analyzing the significance  of the improvements due to our method
The approach we  describe here is not limited to the modalities of video and  audio
It could also be applied to other types of sources,  such as text for machine translation and summarization, or  to information from multiple sensors to predict user status  (e.g., driver confusion) [NN]
In this work, we tested our attentional multimodal fusion using MSR-VTT and precisely  analyzed significance of improvements
 N.N
Encoder-Decoder-Based Sentence Generator  One basic approach to video description is based on  sequence-to-sequence learning
The input sequence (image  sequence) is first encoded to a fixed-dimensional semantic  vector
Then the output sequence (word sequence) is generated from the semantic vector
In this case, both the encoder  and the decoder (sentence generator) are usually modeled as  Long Short-Term Memory (LSTM) networks
 Given a sequence of images, X = xN, xN, 


, xL, each im- age is first fed to a feature extractor, which can be a pretrained CNN for an image or video classification task such  NNNN    as GoogLeNet [NN], VGG-NN [NN], or CND [NN]
The sequence of image features, X ′ = x′N, x ′  N, 


, x ′  L, is obtained  by extracting the activation vector of a fully-connected layer  of the CNN for each input image.N The sequence of feature  vectors is then fed to the LSTM encoder, and the hidden  state of the LSTM is given by  ht = LSTM(ht−N, x ′  t;λE), (N)  where LSTM(h, x;λ) represents an LSTM function of hid- den and input vectors h and x, which is computed with pa- rameters λ
In Eq
(N), λE denotes the encoder’s parame- ters
 The decoder predicts the next word iteratively beginning  with the start-of-sentence token, <sos>, until it predicts the  end-of-sentence token, <eos>
Given decoder state si−N, the decoder network λD infers the next word probability distribution as  P (y|si−N) = softmax (  W (λD)s si−N + b (λD) s  )  , (N)  and generates the word yi that has the highest probability according to  yi = argmax y∈V  P (y|si−N), (N)  where V denotes the vocabulary
The decoder state is up- dated using the LSTM network of the decoder as  si = LSTM(si−N, y ′  i;λD), (N)  where y′i is a word-embedding vector of ym, and the initial state s0 is obtained from the final encoder state hL and y  ′  0 = Embed(<sos>)
In the training phase, Y = yN, 


, yM is given as the ref- erence
However, in the test phase, the best word sequence  needs to be found based on  Ŷ = argmax Y ∈V ∗  P (Y |X)  = argmax yN,...,yM∈V ∗  P (yN|s0)P (yN|sN) · · ·  P (yM |sM−N)P (<eos>|sM )
(N)  Accordingly, we use a beam search in the test phase to keep  multiple states and hypotheses with the highest cumulative  probabilities at each mth step, and select the best hypothesis from those having reached the end-of-sentence token
 N.N
Attention-Based Sentence Generator  Another approach to video description is an attention-based  sequence generator [N], which enables the network to emphasize features from specific times or spatial regions depending on the current context, enabling the next word to be  NIn the case of CND, multiple images are fed to the network at once to  capture dynamic features in the video
 Output word sequence   LSTM decoder   Input image sequence   Feature extractor   (CNN)   Attention mechanism   xN xN xN xN xN   yN yN   sN sN   αN,N αN,N   s0   x’N x’N x’N x’N x’N   <eos> … <sos>   cN cN   Figure N
An encoder-decoder based sentence generator with temporal attention mechanism
 predicted more accurately
Compared to the basic approach  described in Section N.N, the attention-based generator can  exploit input features selectively according to the input and  output contexts
The efficacy of attention models has been  shown in many tasks such as machine translation [N]
 Figure N shows an example of the attention-based sentence  generator from video, which has a temporal attention mechanism over the input image sequence
 The input sequence of feature vectors is obtained using one  or more feature extractors
Generally, attention-based generators employ an encoder based on a bidirectional LSTM  (BLSTM) or Gated Recurrent Units (GRU) to further convert the feature vector sequence so that each vector contains its contextual information
In video description tasks,  however, CNN-based features are often used directly, or one  more feed-forward layer is added to reduce the dimensionality
 If we use an BLSTM encoder following the feature extraction, then the activation vectors (i.e., encoder states) are obtained as  ht =  [  h (f) t  h (b) t  ]  , (N)  where h (f) t and h  (b) t are the forward and backward hidden  activation vectors:  h (f) t = LSTM(h  (f) t−N, x  ′  t;λ (f) E ) (N)  h (b) t = LSTM(h  (b) t+N, x  ′  t;λ (b) E )
(N)  If we use a feed-forward layer, then the activation vector is  calculated as  ht = tanh(Wpx ′  t + bp), (N)  where Wp is a weight matrix and bp is a bias vector
If we use the CNN features directly, then we assume ht = x  ′  t
 The attention mechanism is realized by using attention  weights to the hidden activation vectors throughout the inNNNN    put sequence
These weights enable the network to emphasize features from those time steps that are most important  for predicting the next output word
 Let αi,t be an attention weight between the ith output word and the tth input feature vector
For the ith output, the vec- tor representing the relevant content of the input sequence  is obtained as a weighted sum of hidden unit activation vectors:  ci =  L ∑  t=N  αi,tht
(N0)  The decoder network is an Attention-based Recurrent Sequence Generator (ARSG) [N][N] that generates an output  label sequence with content vectors ci
The network also has an LSTM decoder network, where the decoder state can  be updated in the same way as Equation (N)
 Then, the output label probability is computed as  P (y|si−N, ci) = softmax (  W (λD)s si−N +W (λD) c ci + b  (λD) s  )  ,  (NN)  and word yi is generated according to  yi = argmax y∈V  P (y|si−N, ci)
(NN)  In contrast to Equations (N) and (N) of the basic encoderdecoder, the probability distribution is conditioned on the  content vector ci, which emphasizes specific features that are most relant to predicting each subsequent word
One  more feed-forward layer can be inserted before the softmax  layer
In this case, the probabilities are computed as follows:  gi = tanh (  W (λD)s si−N +W (λD) c ci + b  (λD) s  )  , (NN)  and  P (y|si−N, ci) = softmax(W (λD) g gi + b  (λD) g )
(NN)  The attention weights are computed in the same manner as  in [N]:  αi,t = exp(ei,t)  ∑L  τ=N exp(ei,τ ) (NN)  and  ei,t = w ⊺  A tanh(WAsi−N + VAht + bA), (NN)  where WA and VA are matrices, wA and bA are vectors, and ei,t is a scalar
 N
Attention-Based Multimodal Fusion  We propose an attention model to handle fusion of multiple modalities, where each modality has its own sequence  of feature vectors
For video description, multimodal inputs such as image features, motion features, and audio features are available
Furthermore, combinations of multiple  Multimodal  Naïve fusion   xNN xNN xNL   yi   gi  si-N si   x’NN x’NN   yi-N yi+N   xNN xNN xNL’   x’NN x’NN   di   x’NL x’NL’   αN,i,N  αN,i,L   αN,i,N   αN,i,N αN,i,L’  αN,i,N   cN,i   WCN WCN   cN,i   Figure N
Naı̈ve Fusion of multimodal features
 xNN xNN xNL   yi   gi  si-N si   x’NN x’NN   αN,i,N  αN,i,L   αN,i,N   yi-N yi+N   xNN xNN xNL’   x’NN x’NN   αN,i,N αN,i,L’  αN,i,N   βN,i βN,i   dN,i dN,i   x’NL x’NL’   Attentional  multimodal fusion   cN,i   WCN WCN   cN,i   Figure N
Our Attentional Fusion of multimodal features
 features from different feature extraction methods are often  effective to improve the accuracy of descriptions
 In [NN], content vectors from VGG-NN (image features) and  CND (spatiotemporal motion features) are combined into  one vector, which is used to predict the next word
This  is performed in the fusion layer, in which the following activation vector is computed instead of Eq
(NN):  gi = tanh (  W (λD)s si−N + di + b (λD) s  )  , (NN)  where  di = W (λD) cN cN,i +W  (λD) cN cN,i, (NN)  and cN,i and cN,i are two feature vectors obtained using different feature extractors and/or different input modalities
Figure N illustrates this approach, which we call  Naı̈ve Fusion, in which multimodal feature vectors are combined using one projection matrix WcN for the first modal- ity (input sequence xNN, 


, xNL), and a different projec- tion matrix WcN for the second modality (input sequence x′NN, 


, xNL′ )
However, these feature vectors are combined in the sentence generation step with projection matrices WcN and  NNNN    WcN, which do not depend on time
Consequently, for each modality (or each feature type), all of the feature vectors  from that modality are given the same weight during fusion,  independent of the decoder state
Note that Naı̈ve Fusion is  a type of late fusion, because the inherent difference in sampling rate of the three feature streams precludes early fusion  (concatenation of input features)
The Naı̈ve Fusion architecture lacks the ability to exploit multiple types of features  effectively, because it does not allow the relative weights of  each modality (of each feature type) to change based on the  context of each word in the sentence
 Our proposed method extends the attention mechanism to  multimodal fusion
We call it attentional fusion, or multimodal attention
In our model, based on the current decoder state, the decoder network can selectively attend to  specific modalities of input (or specific feature types) to  predict the next word
Let K be the number of modalities, i.e., the number of sequences of input feature vectors
Our  attention-based feature fusion is performed using  gi = tanh  (  W (λD)s si−N +  K ∑  k=N  βk,idk,i + b (λD) s  )  , (NN)  where  dk,i = W (λD) ck ck,i + b  (λD) ck 
(N0)  The multimodal attention weights βk,i are obtained in a similar way to the temporal attention mechanism:  βk,i = exp(vk,i)  ∑K  κ=N exp(vκ,i) , (NN)  where  vk,i = w ⊺  B tanh(WBsi−N + VBkck,i + bBk), (NN)  where WB and VBk are matrices, wB and bBk are vectors, and vk,i is a scalar
Figure N shows the architecture of our sentence generator,  including the multimodal attention mechanism
Unlike in  the Naı̈ve multimodal fusion method shown in Figure N, in  our method (shown in Figure N) the multimodal attention  weights can change according to the decoder state and the  feature vectors
This enables the decoder network to attend  to a different set of features and/or modalities when predicting each subsequent word in the description
 N
Experiments  N.N
Datasets  We evaluated our proposed feature fusion using the  YouTubeNText [N] and MSR-VTT [NN] video datasets
 YouTubeNText has N,NN0 video clips with multiple natural  language descriptions
There are N0,NNN sentences in total,  with about NN annotated sentences per clip
Each sentence  on average contains about N words
The words contained in  all the sentences constitute a vocabulary of NN,0N0 unique  lexical entries
The dataset is open-domain and covers a  wide range of topics including sports, animals, and music
 Following [NN], we split the dataset into a training set of  N,N00 video clips, a validation set of N00 clips, and a test set  consisting of the remaining NN0 clips
 MSR-VTT [NN] consists of N0,000 web video clips with  NN.N hours and N00,000 clip-sentence pairs in total, covering a comprehensive list of N0 categories and a wide variety  of video content
Each clip was annotated with about N0  natural sentences
The dataset is split into training, validation, and testing sets of NN%, N%, N0%, corresponding to  N,NNN, NNN, and N,NN0 clips respectively
However, because  the video clips are hosted on YouTube, some of the MSRVTT videos have been removed due to content or copyright issues
At the time we downloaded the videos (February N0NN), approximately NN% were unavailable
Thus, we  trained and tested our approach using just the subset of  the MSR-VTT dataset that were available, which consist of  N,NNN, NNN, and N,NNN clips for train, validation, and test  respectively
 N.N
Video Processing  The image data are extracted from each video clip at  NN frames per second and rescaled to NNN×NNN-pixel im- ages
For extracting image features, we use a VGG-NN  network [NN] that was pretrained on the ImageNet dataset  [NN]
The hidden activation vectors of fully connected layer  fcN are used for the image features, which produces a sequence of N0NN-dimensional feature vectors
To model  motion and short-term spatiotemporal activity, we use the  pretrained CND [NN] (which was trained on the Sports-NM  dataset [NN])
The CND network reads sequential frames  in the video and outputs a fixed-length feature vector every NN frames
We extracted activation vectors from fullyconnected layer fcN-N
 N.N
Audio Processing  Unlike previous methods that use the YouTubeNText dataset  [NN, NN, NN], we additionally incorporate audio features
 Since the packaged YouTubeNText dataset does not include  the audio from the YouTube videos, we extracted the audio  data via the original video URLs
Although some of the  videos were no longer available on YouTube, we were able  to collect audio data for N,NNN video clips, which is NN%  of the dataset
The NN kHz-sampled audio data are downsampled to NN kHz, and mel-frequency cepstral coefficients  (MFCCs) are extracted from each N0 ms time window with  NN ms shift
The sequence of NN-dimensional MFCC features are then concatenated into one vector for every group  of N0 consecutive frames, resulting in a sequence of NN0dimensional vectors
The MFCC features are normalized so  NNNN    Table N
Evaluation results on the YouTubeNText test set
The top three rows of the upper table present results of previous state-of-the-art  methods for YouTubeNText, which use only only visual features and only temporal attention
The rest of the tables show results from our  own implementations
Naı̈ve Fusion indicates the conventional approach using temporal attention only (see Figure N)
Attentional Fusion  is our proposed Modal-attention approach (see Figure N)
The symbol (V) denotes methods that only use the visual modalities (image  features and spatiotemporal features)
The symbol (AV) denotes our methods that use all three modalities (audio features as well as the two  types of video features
Our baseline method “Naı̈ve Fusion (V)” is very similar to the approach of [NN]
In the second table, we evaluate  our methods on the subset of the YouTubeNText videos whose audio is not obscured by overdubbed music
 YouTubeNText Full Dataset  Modalities (feature types) Evaluation metric  Method Attention Image Spatiotemporal Audio BLEUN METEOR CIDEr  LSTM-E [NN] VGG-NN CND 0.NNN 0.NN0 –  TA [NN] Temporal GoogLeNet ND CNN 0.NNN 0.NNN 0.NNN  h-RNN [NN] Temporal VGG-NN CND 0.NNN 0.NNN 0.NNN  Naı̈ve Fusion (V) Temporal VGG-NN CND 0.NNN 0.NNN 0.NNN  Naı̈ve Fusion (AV) Temporal VGG-NN CND MFCC 0.N0N 0.N0N 0.NNN  Attentional Fusion (V) Temporal & Multimodal VGG-NN CND 0.NNN 0.NN0 0.NNN  Attentional Fusion (AV) Temporal & Multimodal VGG-NN CND MFCC 0.NNN 0.NNN 0.NNN  YouTubeNText Subset without Overdubbed Music  Naı̈ve Fusion (V) Temporal VGG-NN CND 0.NNN 0.NNN 0.NNN  Naı̈ve Fusion (AV) Temporal VGG-NN CND MFCC 0.NNN 0.NNN 0.NNN  Attentional Fusion (V) Temporal & Multimodal VGG-NN CND 0.NNN 0.NNN 0.N0N  Attentional Fusion (AV) Temporal & Multimodal VGG-NN CND MFCC 0.NNN 0.NNN 0.NNN  Table N
Evaluation results on MSR-VTT Subset
Approximately NN% of the MSR-VTT videos have been removed from YouTube, so we  train and test on the remaining Subset of MSR-VTT videos that we were able to download
We cannot directly compare with the results  in [NN], because they used the full MSR-VTT dataset
Our Naı̈ve Fusion (V) baseline method is extremely similar to the method of [NN],  so it may be viewed as our implementation of their method using the available subset of the MSR-VTT dataset
 MSR-VTT Subset  Modalities (feature types) Evaluation metric  Fusion method Attention Image Spatiotemporal Audio BLEUN METEOR CIDEr  Naı̈ve Fusion (V) Temporal VGG-NN CND 0.NNN 0.NNN 0.NNN  Naı̈ve Fusion (AV) Temporal VGG-NN CND MFCC 0.NNN 0.NN0 0.NNN  Attentional Fusion (V) Temporal & Multimodal VGG-NN CND 0.NNN 0.NNN 0.N0N  Attentional Fusion (AV) Temporal & Multimodal VGG-NN CND MFCC 0.NNN 0.NNN 0.N00  that the mean and variance vectors are 0 and N in the training set
The validation and test sets are also adjusted using  the original mean and variance vectors from the training set
 Unlike for the image features, we apply a BLSTM encoder  network for MFCC features, which is trained jointly with  the decoder network
If audio data are not available for a  video clip, then we feed in a sequence of dummy MFCC  features, which is simply a sequence of zero vectors
 N.N
Experimental Setup  The similarity between ground truth (human-generated)  and automatic video description results is evaluated using  two metrics that were motivated by machine translation,  BLEU [NN] and METEOR [N], as well as a newly proposed  metric for image description, CIDEr [NN]
We used the publicly available evaluation script prepared for the image captioning challenge [N]
Each video in YouTubeNText has  multiple “ground-truth” descriptions, but some “groundtruth” answers are incorrect
Since BLEU and METEOR  scores for a video do not consider frequency of words in  the ground truth, they can be strongly affected by one incorrect ground-truth description
METEOR is even more  susceptible, since it also accepts paraphrases of incorrect  ground-truth words
In contrast, CIDEr is a voting-based  metric that is robust to errors in ground truth
 The caption generation model, i.e
the decoder network,  is trained to minimize the cross entropy criterion using the  training set
Image features are fed to the decoder network  through one projection layer of NNN units, while audio features, i.e
MFCCs, are fed to the BLSTM encoder followed  by the decoder network
The encoder network has one projection layer of NNN units and bidirectional LSTM layers of  NNN cells
The decoder network has one LSTM layer with  NNN cells
Each word is embedded to a NNN-dimensional  vector when it is fed to the LSTM layer
We compared the  AdaDelta optimizer [NN] and RMSprop [NN] to update the  NNNN    Table N
Sample video description results on YouTubeNText
The first row of descriptions were generated by a unimodal system with only  image features (VGG-NN) and temporal attention
The other model names are the same as in Table N
 Sample Image  Unimodal (VGG-NN) a monkey is running a man is slicing a potato a woman is riding a horse a man is singing  Naı̈ve Fusion (V) a dog is playing a woman is cutting an onion a girl is riding a horse a man is singing  Naı̈ve Fusion (AV) a monkey is running a woman is peeling an onion a girl is riding a horse a man is playing a guitar  Attentional Fusion (V) a monkey is pulling a dogs tail a man is slicing a potato a man is riding a horse a man is playing a guitar  Attentional Fusion (AV) a monkey is playing a woman is peeling an onion a girl is riding a horse a man is playing a violin  Discussion  Attentional Fusion (V) (i.e.,  Multimodal attention on visual features) worked best
 Our inclusion of audio features enabled the “peeling”  action to be identified
 Attentional fusion is best
 Audio hurts performance  due to overdubbed music
 Both audio features and  multimodal attention are  needed to identify ”violin”
 parameters, which is widely used for optimizing attention  models
In this video description task, we used LN regularization for all experimental conditions and compared RMSprop and AdaDelta
RMSprop outperformed AdadDelta  for all experimental conditions, so we reporte the results  using RMSprop in Tables N and N
The LSTM and attention  models were implemented using Chainer [NN]
 N
Results and Discussion  Tables N and N show the evaluation results on the  YouTubeNText and MSR-VTT Subset datasets
On each  dataset, we compare the performance of our multimodal attention model (Attentional Fusion), which integrates temporal and multimodal attention mechanisms, to a naı̈ve additive multimodal fusion (Naı̈ve Fusion)
We test versions  of our system that use only visual (image and spatiotemporal) features “(V)”, and versions that additionally use audio features “(AV)”
Our baseline system is the “Naı̈ve  Fusion (V)” method that uses only temporal attention and  only visual features (no audio)
This baseline is extremely  similar to the methods used in [NN] and [NN], which are the  current state-of-the-art methods on the two datasets
 The results demonstrate the effectiveness of our proposed  model
In Table N, the proposed methods outperform the  previously published results in all but one evaluation metric  of one previous method
In both Tables N and N, our proposed methods outperform the “Naı̈ve Fusion (V)” baseline, which is our implementation of the state-of-the-art  methods [NN] and [NN]
Furthermore, our proposed Attentional Fusion model outperforms the corresponding Naı̈ve  Fusion model, both with audio features (AV) and without  audio features (V), on both datasets
These results clearly  demonstrate the benefits of our proposed multimodal attention model
Table N shows generated descriptions for four  example videos from the YouTubeNText data set
These and  more examples, including the original videos with sound,  are in the supplementary material
 N.N
Significance of Improvements  To understand performance improvements via the metrics,  we measured the relative improvement in performance, defined as P = (  Proposed − Baseline )  /Baseline, where Proposed is the score for Attentional Fusion (AV), and  Baseline refers to Naı̈ve Fusion (AV)
The relative improvements P for all metrics on the YouTubeNText Full Dataset and MSR-VTT Subset are shown in part (A) of Table N
The  use of relative scores highlights the significance of the improvements due to Attentional Fusion
In addition, to establish an upper bound related to human performance, we evaluated inter-rater reliability of the human captions in leaveone-out fashion: we compared each reference sentence for  each video to the remaining set of reference sentences for  that video, using all three metrics
The mean of these “Human” scores are shown in part (B) of Table N
Our scores are  quite close to this inter-rater reliability upper bound
Furthermore, our model scores significantly close the gap between the baseline and this ”Human” upper bound
We can  quantify the gap in terms of the relative reduction, R, de- fined as R = (Proposed − Baseline)/(Human − Baseline)
The relative gap reduction, R, for all metrics is shown in part (C) of Table N
These scores indicate that our model  makes significant progress from the baseline toward humanlevel performance
Note that for BLEUN on the MSR-VTT  Subset, both the baseline and our system are “super-human”  by this standard, so there is no gap to close
Nevertheless,  our model still outperforms the “Naı̈ve Fusion” baseline
 N.N
Impact of Audio Features  In some experiments, including audio features (AV) improves performance over the corresponding visual-only (V)  case, but in other cases it does not
Including audio features can degrade performance for some video clips because  some YouTube videos include unrelated noise that was not  in the original scene, such as overdubbed music that was  added to the video in post-production
Attentional Fusion  NNNN    Table N
Significance of Improvement by Attentional Fusion (AV)  in terms of (A) Relative Improvement, P , compared to the Naı̈ve  Fusion (AV) baseline, (B) Mean of the “Human” Scores, and  (C) Relative Gap Reduction, R, compared to the “Human” Scores
 Data set BLEUN METEOR CIDEr  (A) Relative Improvement in Performance, P  YouTubeNText Full Dataset N.N% N.N% N.N%  MSR-VTT Subset N.N% N.N% N0.N%  (B) Mean of the “Human” Scores  YouTubeNText Full Dataset 0.NN 0.NN N.NN  MSR-VTT Subset 0.NN 0.N0 0.N0  (C) Relative Gap Reduction to Human, R  YouTubeNText Full Dataset NN% NN% N%  MSR-VTT Subset NA NN% N0%  mitigated the degradation by the audio feature
On the other  hand, the audio feature contributed to the performance for  both Naı̈ve and Attentional fusion models
 We found the negative impact of audio features on some  evaluation metrics—i.e., cases in which (AV) methods perform worse than their (V) counterparts in Tables N and N
 We hypothesized that this degradation due to audio features was due to overdubbed sound that was not present in  the original scene
To test this hypothesis, we performed  an experiment in which we manually removed all of the  YouTubeNText videos in which overdubbed music obscured  the sound that was captured during filming
The subsection of Table N titled “YouTubeNText Subset without Overdubbed Music” shows the results for the remaining subset of  YouTubeNText (NN0 videos)
The results show that whereas  the Naı̈ve fusion baseline did not make good use of the audio features in these videos, our proposed Attentional Fusion method does, yielding a significant score improvement  over the baseline for all metrics
 N.N
Impact of Multimodal Attention  A particular advantage of the proposed multimodal attention is that we can easily inspect the attention distributions over modalities produced by the network for each  word
Table N shows the average attention weights used  for each modality when generating various words, sorted  in descending order by weight
The image features, which  were trained for object classification (VGG-NN ImageNet),  are strongly selected for the words that describe generic object types
The motion features (CND), which were trained  to identify different sports scenes, appear to be selected  when describing objects and scenes that tend to be in motion, such as sports and vehicles
The audio features, which  were not pretrained (MFCC), overall have smaller weights  and were less strongly selected
Nevertheless, the words  with the strongest audio weights appear to be action verbs  associated with sound, such as talking, singing, and driving
 Thus the overall pattern of weights is consistent with our hypothesis about the role of attention to different modalities in  selecting different types of words
 Table N
A list of words with strong average attention weights for  each modality, obtained on the the MSR-VTT Subset using our  “Attentional Fusion (AV)” multimodal attention method
 Image Motion Audio  (VGG-NN) (CND) (MFCC)  bowl 0.NN0N track 0.NNNN talking 0.NNNN  pan 0.NNNN motorcycle 0.NNNN shown 0.N0NN  recipe 0.NN0N baseball 0.NNNN playing 0.NNNN  piece 0.NNNN football 0.NNNN singing 0.NNNN  paper 0.N0NN horse 0.NNNN driving 0.NNNN  kitchen 0.NNNN soccer 0.N0NN working 0.N00N  toy 0.NNNN basketball 0.N0NN walking 0.NNNN  folding 0.NNNN tennis 0.NNNN riding 0.NN00  makeup 0.NNNN player 0.NNN0 showing 0.NNNN  guitar 0.NNNN two 0.NNNN dancing 0.NNNN  applying 0.NNNN video 0.NNNN wrestling 0.NNNN  food 0.NNNN men 0.NNNN running 0.NNNN  making 0.NNN0 running 0.NNN0 applying 0.NNNN  cooking 0.NNNN wrestling 0.NNNN cooking 0.NNNN  working 0.NNNN people 0.NNNN making 0.NNNN  showing 0.NNNN stroller 0.NNNN characters 0.NNNN  computer 0.NNNN game 0.NNNN folding 0.N0NN  band 0.NNNN group 0.NN0N program 0.0NNN  cartoon 0.NNNN riding 0.NNNN character 0.0NNN  character 0.NNNN girl 0.NNNN something 0.0NNN  cat 0.NNNN man 0.NNNN makeup 0.0NN0  characters 0.NNNN walking 0.NNNN game 0.0NNN  car 0.NNNN dancing 0.NN0N player 0.0NNN  song 0.NNNN stage 0.NNNN tennis 0.0NNN  person 0.NNNN table 0.NNNN food 0.0NNN  something 0.NNNN driving 0.NNNN two 0.0NNN  woman 0.N0N0 dog 0.NNNN men 0.0NNN  program 0.N0NN woman 0.NN0N people 0.0NNN  dog 0.NNNN person 0.NN0N stage 0.0NN0  table 0.NNNN song 0.NNNN cartoon 0.00NN  N
Conclusion  We proposed a new modality-dependent attention mechanism, which we call multimodal attention, for video description based on encoder-decoder sentence generation using recurrent neural networks (RNNs)
In this approach,  the attention model selectively attends not just to specific  times, but to specific modalities of input such as image features, spatiotemporal motion features, and audio features
 In addition, Attentional Fusion enables us to analyze the attention weights for each word to examine how each modality contributes to each word
We evaluated our method on  the YouTubeNText and MSR-VTT datasets, achieving results that are competitive with current state-of-the-art methods that employ temporal attention models
More importantly, we demonstrate that our model incorporating multimodal attention as well as temporal attention outperforms  the state-of-the-art baseline models that use temporal attention alone
The attention mechanism also provides a means  for introspection in the model, in the sense that the weights  across modalities that are used in generating each word can  be used to explore what features are useful in various contexts
Examination of these attention weights confirms that  the focus of attention on the appropriate modality is well  aligned to the semantics of the words
 NN00    References  [N] D
Bahdanau, K
Cho, and Y
Bengio
Neural machine  translation by jointly learning to align and translate
 CoRR, abs/NN0N.0NNN, N0NN
 [N] D
Bahdanau, J
Chorowski, D
Serdyuk, P
Brakel,  and Y
Bengio
End-to-end attention-based large vocabulary speech recognition
pages NNNN–NNNN, N0NN
 [N] X
Chen, H
Fang, T
Lin, R
Vedantam, S
Gupta,  P
Dollár, and C
L
Zitnick
Microsoft COCO captions: Data collection and evaluation server
CoRR,  abs/NN0N.00NNN, N0NN
 [N] K
Cho, B
van Merrienboer, D
Bahdanau, and  Y
Bengio
On the properties of neural machine  translation: Encoder-decoder approaches
CoRR,  abs/NN0N.NNNN, N0NN
 [N] K
Cho, B
van Merrienboer, Ç
Gülçehre, D
Bahdanau, F
Bougares, H
Schwenk, and Y
Bengio
 Learning phrase representations using RNN encoderdecoder for statistical machine translation
In Proceedings of the N0NN Conference on Empirical Methods in Natural Language Processing, EMNLP N0NN,  October NN-NN, N0NN, Doha, Qatar, A meeting of  SIGDAT, a Special Interest Group of the ACL, pages  NNNN–NNNN, N0NN
 [N] J
K
Chorowski, D
Bahdanau, D
Serdyuk, K
Cho,  and Y
Bengio
Attention-based models for speech  recognition
In C
Cortes, N
D
Lawrence, D
D
 Lee, M
Sugiyama, and R
Garnett, editors, Advances  in Neural Information Processing Systems NN, pages  NNN–NNN
Curran Associates, Inc., N0NN
 [N] M
J
Denkowski and A
Lavie
Meteor universal:  Language specific translation evaluation for any target language
In Proceedings of the Ninth Workshop  on Statistical Machine Translation, WMT@ACL N0NN,  June NN-NN, N0NN, Baltimore, Maryland, USA, pages  NNN–NN0, N0NN
 [N] G
Gravier, S
Axelrod, G
Potamianos, and C
Neti
 Maximum entropy and mce based hmm stream weight  estimation for audio-visual asr
In Acoustics, Speech,  and Signal Processing (ICASSP), N00N IEEE International Conference on, volume N, pages I–NNN
IEEE,  N00N
 [N] S
Guadarrama, N
Krishnamoorthy, G
Malkarnenkar, S
Venugopalan, R
Mooney, T
Darrell, and  K
Saenko
YoutubeNtext: Recognizing and describing arbitrary activities using semantic hierarchies and  zero-shot recognition
In Proceedings of the IEEE  International Conference on Computer Vision, pages  NNNN–NNNN, N0NN
 [N0] M
E
Hennecke, D
G
Stork, and K
V
Prasad
 Visionary speech: Looking ahead to practical  speechreading systems
In Speechreading by Humans  and Machines, pages NNN–NNN
Springer, NNNN
 [NN] S
Hochreiter and J
Schmidhuber
Long short-term  memory
Neural Computation, N(N):NNNN–NNN0, NNNN
 [NN] C
Hori, T
Hori, T
Lee, K
Sumi, J
R
Hershey, and  T
K
Marks
Attention-based multimodal fusion for  video description
CoRR, abs/NN0N.0NNNN, N0NN
 [NN] C
Hori, S
Watanabe, T
Hori, B
A
Harsham, J
R
 Hershey, Y
Koji, Y
Fujii, and Y
Furumoto
Driver  confusion status detection using recurrent neural networks
In IEEE International Conference on Multimedia and Expo, ICME N0NN, Seattle, WA, USA, July  NN-NN, N0NN, pages N–N, N0NN
 [NN] T
Hori, H
Wang, C
Hori, S
Watanabe, B
Harsham,  J
L
Roux, J
Hershey, Y
Koji, Y
Jing, Z
Zhu, and  T
Aikawa
Dialog state tracking with attention-based  sequence-to-sequence learning
In N0NN IEEE Spoken Language Technology Workshop, SLT N0NN, San  Diego, CA, USA, December NN-NN, N0NN
 [NN] Q
Jin, J
Liang, and X
Lin
Generating Natural Video  Descriptions via Multimodal Processing
In Interspeech, N0NN
 [NN] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar, and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In Proceedings of the IEEE conference on Computer Vision and  Pattern Recognition, pages NNNN–NNNN, N0NN
 [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet classification with deep convolutional neural  networks
In F
Pereira, C
J
C
Burges, L
Bottou,  and K
Q
Weinberger, editors, Advances in Neural Information Processing Systems NN, pages N0NN–NN0N
 Curran Associates, Inc., N0NN
 [NN] M
Lin, Q
Chen, and S
Yan
Network in network
 CoRR, abs/NNNN.NN00, N0NN
 [NN] R
Lowe, N
Pow, I
Serban, and J
Pineau
The ubuntu  dialogue corpus: A large dataset for research in unstructured multi-turn dialogue systems
In Proceedings of the SIGDIAL N0NN Conference, The NNth Annual Meeting of the Special Interest Group on Discourse and Dialogue, N-N September N0NN, Prague,  Czech Republic, pages NNN–NNN, N0NN
 [N0] J
Mao, W
Xu, Y
Yang, J
Wang, and A
L
Yuille
 Deep captioning with multimodal recurrent neural networks (m-rnn)
CoRR, abs/NNNN.NNNN, N0NN
 [NN] J
R
Movellan and P
Mineiro
Robust sensor fusion:  Analysis and application to audio visual speech recognition
Machine Learning, NN(N):NN–N00, NNNN
 [NN] Y
Pan, T
Mei, T
Yao, H
Li, and Y
Rui
Jointly  modeling embedding and translation to bridge video  and language
CoRR, abs/NN0N.0NNNN, N0NN
 NN0N    [NN] K
Papineni, S
Roukos, T
Ward, and W
Zhu
Bleu:  a method for automatic evaluation of machine translation
In Proceedings of the N0th Annual Meeting of the  Association for Computational Linguistics, July N-NN,  N00N, Philadelphia, PA, USA., pages NNN–NNN, N00N
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
 CoRR, abs/NN0N.NNNN, N0NN
 [NN] T
Tieleman and G
Hinton
Lecture N.N—RmsProp:  Divide the gradient by a running average of its recent  magnitude
COURSERA: Neural Networks for Machine Learning, N0NN
 [NN] S
Tokui, K
Oono, S
Hido, and J
Clayton
Chainer:  a next-generation open source framework for deep  learning
In Proceedings of Workshop on Machine  Learning Systems (LearningSys) in The Twenty-ninth  Annual Conference on Neural Information Processing  Systems (NIPS), N0NN
 [NN] D
Tran, L
D
Bourdev, R
Fergus, L
Torresani, and  M
Paluri
Learning spatiotemporal features with Nd  convolutional networks
In N0NN IEEE International  Conference on Computer Vision, ICCV N0NN, Santiago, Chile, December N-NN, N0NN, pages NNNN–NNNN,  N0NN
 [NN] R
Vedantam, C
L
Zitnick, and D
Parikh
Cider:  Consensus-based image description evaluation
In  IEEE Conference on Computer Vision and Pattern  Recognition, CVPR N0NN, Boston, MA, USA, June NNN, N0NN, pages NNNN–NNNN, N0NN
 [NN] S
Venugopalan, H
Xu, J
Donahue, M
Rohrbach,  R
J
Mooney, and K
Saenko
Translating videos  to natural language using deep recurrent neural networks
In NAACL HLT N0NN, The N0NN Conference  of the North American Chapter of the Association for  Computational Linguistics: Human Language Technologies, Denver, Colorado, USA, May NN - June N,  N0NN, pages NNNN–NN0N, N0NN
 [N0] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show  and tell: A neural image caption generator
In IEEE  Conference on Computer Vision and Pattern Recognition, CVPR N0NN, Boston, MA, USA, June N-NN, N0NN,  pages NNNN–NNNN, N0NN
 [NN] H
Wang, A
Kläser, C
Schmid, and C.-L
Liu
Action Recognition by Dense Trajectories
In IEEE Conference on Computer Vision & Pattern Recognition,  pages NNNN–NNNN, Colorado Springs, United States,  June N0NN
 [NN] J
Xu, T
Mei, T
Yao, and Y
Rui
Msr-vtt: A large  video description dataset for bridging video and language
In Proceedings of the IEEE Conference on  Computer Vision and Pattern Recognition (CVPR),  N0NN
 [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
C
Courville,  R
Salakhutdinov, R
S
Zemel, and Y
Bengio
Show,  attend and tell: Neural image caption generation with  visual attention
In Proceedings of the NNnd International Conference on Machine Learning, ICML N0NN,  Lille, France, N-NN July N0NN, pages N0NN–N0NN, N0NN
 [NN] L
Yao, A
Torabi, K
Cho, N
Ballas, C
J
Pal,  H
Larochelle, and A
C
Courville
Describing videos  by exploiting temporal structure
In N0NN IEEE International Conference on Computer Vision, ICCV N0NN,  Santiago, Chile, December N-NN, N0NN, pages NN0N–  NNNN, N0NN
 [NN] H
Yu, J
Wang, Z
Huang, Y
Yang, and W
Xu
Video  paragraph captioning using hierarchical recurrent neural networks
CoRR, abs/NNN0.0NNNN, N0NN
 [NN] M
D
Zeiler
ADADELTA: an adaptive learning rate  method
CoRR, abs/NNNN.NN0N, N0NN
 NN0NRotational Subgroup Voting and Pose Clustering for Robust ND Object Recognition   Rotational Subgroup Voting and Pose Clustering for  Robust ND Object Recognition  Anders Glent Buch Lilita Kiforenko Dirk Kraft  University of Southern Denmark  {anbu,lilita,kraft}@mmmi.sdu.dk  Abstract  It is possible to associate a highly constrained subset of  relative N DoF poses between two ND shapes, as long as the  local surface orientation, the normal vector, is available  at every surface point
Local shape features can be used  to find putative point correspondences between the models  due to their ability to handle noisy and incomplete data
 However, this correspondence set is usually contaminated  by outliers in practical scenarios, which has led to many  past contributions based on robust detectors such as the  Hough transform or RANSAC
The key insight of our work  is that a single correspondence between oriented points on  the two models is constrained to cast votes in a N DoF rotational subgroup of the full group of poses, SE(N)
Kernel  density estimation allows combining the set of votes efficiently to determine a full N DoF candidate pose between  the models
This modal pose with the highest density is stable under challenging conditions, such as noise, clutter, and  occlusions, and provides the output estimate of our method
 We first analyze the robustness of our method in relation  to noise and show that it handles high outlier rates much  better than RANSAC for the task of N DoF pose estimation
 We then apply our method to four state of the art data sets  for ND object recognition that contain occluded and cluttered scenes
Our method achieves perfect recall on two LIDAR data sets and outperforms competing methods on two  RGB-D data sets, thus setting a new standard for general  ND object recognition using point cloud data
 N
Introduction  There is an ever-increasing need for robust perception systems and automated solutions in industry, service  robotics and other applications
One of the great challenges  is for an autonomous system to navigate in unstructured environments, which for manipulation tasks crucially relies  on the ability to recognize and localize the parts or objects of interest
Although some recognition tasks naturally  lend themselves to image-based techniques—some examples are pedestrian detection, traffic sign recognition and  gesture recognition—it is vital for an autonomous agent to  acquire the pose of objects in the ambient space to be able  to perform any real manipulation tasks
This involves determining the full N DoF position and N DoF rotation of an  object, which can become a computationally expensive operation in cluttered scenes
 Many contributions have been made on this matter [NN],  in recent years heavily based on range or ND data in the  form of either RGB-D images, point clouds or meshes
 These data are acquired from Kinect sensors, industrial  grade laser scanners or the likes
Steady improvements  have been achieved, when considering overall object recognition performances in ND data sets, where the aim usually  is to find the full N DoF pose of multiple objects in unstructured or semi-structured scenes
In this very general  free-form recognition and localization scenario, one needs  to deal with some nuisances, including noise in the acquired  sensor data and partial occlusions of the objects due to obscuring elements
In this work, we focus on the underlying  problem of recovering the N DoF pose of an object under  these conditions and adopt a feature-based approach, where  multiple local shape features are used to describe a full ND  object model
These local features can be matched with  a scene, providing a set of point correspondences between  the object and the scene
In real applications this set of correspondences is heavily contaminated by outliers, making  the search for the pose which brings the object into correct  alignment with the scene a very challenging problem
 This paper describes a method that can be used to localize ND objects within a scene acquired from a depth or ND  sensor
The method is shown to be particularly robust towards high fractions of outliers, which results in very competitive recognition rates for a number of applications
To  achieve this, our method uses geometric constraints to cast  full N DoF votes for the pose using individual correspondences
These votes are shown to lie on a small N DoF  manifold, which allows for a tractable inference step based  on kernel density estimation
Our method differs from preNNNN    vious methods since these usually require two [NN], three  [N] or more correspondences to compute pose candidates,  making the sampling process much more expensive
Our  method uses single correpondences to vote for a set of candidate poses and delays the determination of the correct  pose to a subsequent clustering process
We tested our  method on two well-known free-form object recognition  data sets and two recent RGB-D data set
In all cases, our  method outperforms competing methods
 This paper is structured as follows
In Sect
N we provide an overview of related work within the field of object  recognition and pose estimation in ND data
Sect
N gives  the details of our algorithm and in Sect
N we explain how  our algorithm is used in a ND object recognition and pose  estimation pipeline
In Sect
N we present experimental results and in Sect
N we conclude on our findings
 N
Related work  Object recognition and pose estimation in ND data has  been an active research area for more than two decades
 Early works include [N, NN], from which the well-known  Spin Images used local shape descriptors and correspondence grouping for recognizing objects in range images
In  the years that followed, several variations of local shape descriptors appeared
In [NN] a local ND shape context descriptor was used for matching segmented point cloud models
 Similarly, in [N] a set of local descriptors were used to recognize shapes in scenes with no or limited amounts of clutter
A more elaborate system for both ND object modeling  and recognition in cluttered scenes was presented in [N0]
 Recognition was performed using randomly sampled point  pairs for which an area-based descriptor was computed
The  use of point pairs eased the process of computing a relative  pose between the object and scene models but came at an  increased computational cost
Progress in these cluttered  scenarios continued, with other methods, e.g
[NN], using  RANSAC [NN] for robust pose estimation, and [N] using a  tree search through the set of possible correspondences
In  an influential work [NN], the use of point pairs was revisited, but now with a simpler and computationally cheaper  feature and a fast pose estimation algorithm using a variant  of geometric hashing
The method was further developed in  several later works, including [N, N0, NN]
The development  of local shape features continued for a considerable variation of applications such as mesh based keypoint detection  and description [N0] point cloud based registration [NN] and  of course recognition and pose estimation [NN]
 A very different class of methods rely on N.ND data, e.g
 from RGB-D sensors
The best-known method is arguably  LINEMOD [NN], which allowed for real-time matching of  thousands of object templates in RGB-D data
Many competing methods using template-based approaches were introduced afterward, including [N, NN, NN] and most recently  [N, NN, NN]
The last three achieved very high detection rates  by learning an intermediate feature layer with either convolutional or autoencoder neural networks
 Our method bears similarities with the ND Hough voting  of [NN], which computed full rotation frames at each feature and used the feature correspondences to cast votes for  the N DoF translation component of the pose
Although a  full rotation can also be computed for each vote, the authors  instead performed the mode finding in the reduced N DoF  Hough space and used RANSAC [NN] to find the rotation
In  contrast, our method casts multiple full N DoF votes for each  feature correspondence, and we use a branch and bound  search strategy on the actual pose samples to find peaks
 The method that we use for computing the individual pose  votes is in principle similar to that of the point pair features  [NN]
In this work randomly sampled oriented point pairs  were matched and pose clustering was performed by binning in a low-dimensional pose space
However, while this  method used individual point pair correspondences to compute a single pose vote, our method computes a constant  number of pose votes for a single point correspondence and  delays the inference of the modal pose to the subsequent  clustering stage
This difference reduces the complexity of  our sampling stage from quadratic to linear
 Finally, some recent methods [N, N, NN] introduced an additional joint optimization stage
These methods used multiple candidate poses per object and performed a global optimization over the possible combinations of poses to find  a configuration of objects that was consistent with the observed scene data
Any pose estimation method, including ours, can in principle be used to provide inputs to these  joint optimization frameworks, but the overall performance  depends entirely on the ability of the underlying pose estimation algorithms to produce good candidate poses
This  ability is the focus of the method presented in this work and  the competing methods included in our experiments
 N
Method  This section gives the details of our method, which is an  algorithm for computing one or more relative N DoF poses  between one or more ND object models and a scene
Scene  data are usually obtained using a sensor, e.g
an RGB-D  camera or a laser scanner
In the general case, the challenge  is to locate the instance(s) of the object(s) present within the  scene, while remaining robust towards inaccuracies (noise),  missing data (occlusions) and irrelevant data (cluttering elements)
Our algorithm assumes correspondences between  surface points on the two models (the object and the scene)  are available
We will explain how these correspondences  are obtained using local shape features in Sect
N
 Our algorithm is related to the point pair feature (PPF)  method [NN], as it uses surface normals to define local  frames
The PPF method uses a large set of point pair  NNNN    Figure N: An example scenario visualizing the pose voting process between the Chicken model and the first scene of the  UWA data set used for the first test in Sect
N
Left: two correct (green) and incorrect (red) correspondences are used to show  the geometry of our method
The votes (dashed circles) of the correct correspondences cluster near the center of the object  instance in the scene (rightmost black dot)
Middle: the same scene—now seen from the back—showing all the votes cast for  the model center, with each vote colored proportionally to its SE(N) density estimate (blue is the highest)
Right: the final modal pose estimate shown by overlaying the aligned object model in green
 matches between the models to compute full object-relative  rotation frames and thereby cast votes for the object pose
 Another type of methods [NN, NN] estimates a full local reference frame (LRF) directly at each surface point from the  underlying point cloud data
This approach reduces the voting to a linear operation, but results have been suboptimal  [NN], most likely because the estimation of LRF is unstable  in noisy and occluded data
Our algorithm lies in between  these two approaches
Similar to the LRF method, we require only a linear number of votes, but we avoid the estimation of a potentially unstable LRF
Instead, for each point  match, we compute multiple LRFs using the surface point  and the center point of the object
The use of the normal  orientation significantly limits the number of possible votes  for each point, as the votes are constrained to a N DoF manifold
The method is formalized in the following sections
 N.N
Subgroup voting  We denote an oriented point on the object model as  (p, n), with p being the point coordinates and n be- ing the ND normal vector pointing away from the surface
A matched feature from the scene provides a  correspondence with an oriented point in the scene,  which we denote (p′, n′)
In the left part of Fig
N c  p n  r δ we show some examples of correctmatches (green) and incorrect matches(red)
We first compute the scalar projec- tion δ of the vector going from the object  center c to the object point p onto the unit normal n:  δ = (p− c) · n (N)  We now start from p and follow the negative of n with a distance of δ
Then we compute a radial vector r going  from this point to the center c:  r = c− (p− δn) (N)  t  p' n'  r'δ q At this point we have enough informationto cast votes for possible translations ofthe object center into the scene
Using the matched point (p′, n′) we again follow the  negative scene normal using the stored projection for the  object center, δ
We call this the radial point, q:  q = p′ − δn′ (N)  The voting for the translation of the object center into the  scene proceeds as follows
We start by sampling a random  vector orthogonal to n′ and scale it to a length equal to the radial vector ‖r‖
Denote this vector r′, and note that it is an instantiation of the object radial vector r in the scene
 p' n' δ tN  tN0  We now choose a tessellation level Nr and perform incremental rotations of r′ around n′ with an angle of θ = NN0◦/Nr
Ro- drigues’ rotation formula can be used to rotate a vector (r′) around another (n′) by a specified angle (θ) and we use it Nr times to get the next instantiation of the radial vector:  r′ ← r′ cos θ + (n′ × r′) sin θ + n′(n′ · r′)(N− cos θ)  = r′ cos θ + (n′ × r′) sin θ (N)  In our case, n′ and r′ are always orthogonal, allowing us to eliminate the last term of this equation, as is done in the  second line above
For each of the Nr rotated versions of r ′,  we add it to the radial point q and get a candidate translation t of the center of the object into the scene as follows:  t = q + r′ (N)  NNNN    We refer again to Fig
N for a visualization of the different  geometric elements described here
 In the final part of our voting scheme, we show how  to recover a full N DoF relative rotation for each of the  Nr candidate translations t
Looking at Fig
N, it can be observed that all the translation candidates lie on a circle
This is a result of the fact that when the corresponding normal vectors n and n′—which both have N DoFs— are aligned, there is only N degree of freedom left to determine
To find this last DoF, we first compute a full N  DoF rotation frame at the oriented object point (p, n) and do the same for the Nr tessellation points in the scene
 c  p n  r δx y z  x  y  z  On the object side, this frame is constructed by setting the third column of the  rotation frame equal to the normal vector  n
The radial vector r is always orthogonal to n, and we set the first column to this vector normalized
The final vector making up a full rotation frame for the feature point is computed using the cross product
We thus get  a N-by-N rotation matrix Rr as follows:  Rr = [  r  ‖r‖ n× r  ‖r‖ n ]  (N)  t  p'n'  r'δ qxy z  x  y  z  cam  The same operation is applied to the Nr ro- tated versions of r′ in the scene to get Nr candidate rotation frames Rr′ as follows:  Rr′ = [  r ′  ‖r′‖ n ′ × r  ′  ‖r′‖ n ′ ]  (N)  Finally, the candidate relative rotation R for aligning the object with the scene is given as follows:  R = R⊤ r′ ·Rr (N)  p  c  p'  cam  T To summarize, we now have Nr N DoF ro- tations R and N DoF translations t
Putting each of these together in a N-by-N transformation matrix T gives us Nr pose candi- dates:  T =  [  R t 0 0 0 N  ]  ∈ SE(N) (N)  For every correct correspondence, there will be one correct  and Nr − N incorrect pose votes, all lying on a N DoF sub- group of SO(N)
All in all, there will a pose vote count equal to Nr times the number of correspondences
We have tried a number of different values for Nr and overall achieved better performance for finer tessellations
We have  therefore chosen Nr = N0 tessellations, giving an angular resolution of θ = N◦ of our pose votes
In the following paragraph, we describe how to perform mode finding within  these poses to find the correct pose
 N.N
Density estimation and clustering  The many pose votes produced by the method described  in the previous subsection contain a significant fraction of  incorrect candidate poses
However, the six dimensions of  the pose group makes it very unlikely that incorrect poses  cluster together
This leaves a possibility for the correct  poses to cluster together near a detectable mode in SE(N), even though there are only very few of these
This inference  process can in principle be performed in different ways,  e.g
using using k-means clustering, mean shift or other mode seeking methods [N]
Unfortunately, the dimensionality of the search space makes many of these approaches  intractable due to either excessive memory requirements or  high computational complexities
 To overcome this, we use a kernel density estimate on  SE(N), computed at each of the pose votes T 
This requires a measure of distance from each vote to all other votes
A  bandwidth σ is used to preserve locality of the density esti- mate at each vote
The kernel density estimate K for a pose vote T is computed as follows:  K(T ) =  NT ∑  i=N  fK(d(T, Ti)/σ) (N0)  where NT is the total number of pose votes, d is some mea- sure of distance between two poses, and fK is the kernel function
In this work we use the unnormalized Gaussian  kernel:  fK(x) = exp  (  −xN  N  )  (NN)  Defining the metric d on SE(N) is non-trivial
Instead, we decompose the density estimate to a product of two ND  Gaussian kernels, one for translations and one for rotations:  K(T ) = ∑  T̂∈N (T )  fK  (  dt(t̂, t)  σt  )  · fK  (  dR(R̂, R)  σR  )  (NN)  We use the Euclidean distance for the translations:  dt(t̂, t) = ‖t̂− t‖ (NN)  For the rotations we use the minimal geodesic distance  along the manifold:  dR(R̂, R) = arccos  (  trace(R̂⊤R)− N  N  )  (NN)  which lies in the interval [0, NN0]◦ and gives the minimal an- gle needed to rotate R into R̂
In the SE(N) density estimate in (NN) we changed the summation to occur over a neigh- borhood around T , denoted N (T )
This change is possible because the kernel fK decays rapidly away from the center
We, therefore, do not need to brute-force loop over all other  NNN0    votes to compute a reliable density estimate for T 
Instead, we can perform a branch and bound radius search around  T , with the influence radius set equal to the bandwidth, and find all pose votes within a neighborhood
 To find the neighbor poses N (T ), we first perform a ra- dius search in RN using a k-d tree to find an initial set of pose neighbors within the translation bandwidth:  N (t) = {t̂ : dt(t̂, t) ≤ σt} (NN)  The full pose neighbors N (T ) are now bounded signifi- cantly, since none of these can be outside the set N (t)
We can, therefore, do a linear search within N (t) to find the subset of neighbors within the rotation bandwidth:  N (T ) = {R̂ ∈ N (t̂) : dR(R̂, R) ≤ σR} (NN)  To summarize, during inference we visit every pose vote,  make a search query to find the neighbors, and then visit  every neighbor pose and accumulate the density estimate  using (NN)
The poses with high densities represent local modes in SE(N) and provide the output pose estimates T for aligning the object model with one or more instances  present in the scene data:  T = argmax T̂  K(T̂ ) (NN)  An alignment using the modal pose is shown in the right  part of Fig
N
 N.N
Computational complexity  Like many other competing methods, our algorithm is  based on correspondences from local shape features
The  computation of features is an O(N logN) operation in the number of feature points N [NN]
The PPF method does not require expensive feature computation, but instead it  requires a sampling stage with a complexity of O(NN)
RANSAC does rely on feature correspondences but needs  samples with a cardinality of at least three for computing  a candidate pose, in which case the complexity rises to  O(NN)
The most computationally expensive part of our method is the density estimation stage, where we perform  a radius search among all the SE(N) votes
This is an O(NrN log(NrN)) operation, where Nr is the number of rotational tessellations
As mentioned in the previous section, we use a fixed value of Nr = N0
We thus have a complexity of O(N logN) with a considerable factor
 N
Object recognition and pose estimation  pipeline  The contributed method detailed in the previous section  takes part of a feature-based ND object recognition pipeline
 The input to our pose clustering method is a set of correspondences, which are obtained by matching local features,  which again require a good estimate of the local surface normals
When dealing with multiple objects, we process the  models sequentially and invoke the pose clustering once for  each object
We explain in this section the overall approach  taken in our recognition system and end by detailing how  we can also use our method for multi-instance recognition  of the same object
The source code for our method is publicly available in the CoViS C++ libraryN
 Preprocessing Our algorithm can use both triangular  meshes or raw point clouds as input models; both types of  input data are treated similarly, with the one difference being that for a mesh model we use the faces to ensure globally consistent (i.e
outward-pointing) normals for the object models
For point clouds, we compute surface normals  using PCL [NN] and use a breadth-first search to traverse  the object surface and orient the normals consistently
We  then downsample both models to a constant resolution using a voxel grid to limit the amount of data for processing
 To further reduce the processing time, we avoid computing  local features at all surface points but use only a uniform  subset of around N0000 feature points per object model
 Feature computation and matching To boost the performance of our recognition system, we use our own library  for computing discriminative local features
We use randomized k-d trees for fast approximate neighbor searches [NN] and we always perform search queries with the scene  features into an offline generated randomized k-d tree index of all the object features
 Multi-object and multi-instance recognition The feature matching stage produces a set of correspondences relating the feature points in the scene to points on the objects
 In case of multiple objects, we order the objects according  to how many correspondences were found for them, starting  with the object with most correspondences in the scene
We  then run our pose clustering to get the modal pose and use  ICP [N] to refine the estimate
We then adopt a fast, greedy  approach, where we segment out the scene data containing  the object instance, before proceeding to the next object
 Our pose clustering method also allows for multiinstance recognition, which is tested in the last part of the  next section
We first let our pose clustering method return  the highest-density pose of each correspondence and then  perform non-maximum suppression on this subset of poses  using a Euclidean threshold on the translation components  equal to N0 % of the object model bounding box diagonal
 The next section presents test results for our recognition  system in a range of recognition applications
 Nhttps://gitlab.com/caro-sdu/covis  NNNN  https://gitlab.com/caro-sdu/covis   0 0.N N N.N N N.N N  Noise level [%]  0  0.0N  0.0N  0.0N  0.0N  0.0N  0.0N T  ra n s la  ti o n  e  rr o r   [m ]  RANSAC  Pose clustering  0 0.N N N.N N N.N N  Noise level [%]  0  N0  N0  N0  N0  N0  N0  R o  ta ti o  n  e  rr o  r  [d  e g  ]  0 0.N N N.N N N.N N  Noise level [%]  0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  In lie  r  ra  te  0  N  N0  NN  N0  NN  N o  rm a  l  a  n g  le  e  rr o  r  [d  e g  ]  Figure N: Sensitivity analysis results obtained by matching the Stanford Bunny to its increasingly noisy counterparts using  RANSAC and our method
Left: translation errors for increasing noise levels
Middle: the corresponding rotation errors
 Right: ground truth inlier rates for the correspondence sets (black) and average normal angle deviations (gray)
 N
Results  This section presents five experiments on ND object  recognition and N DoF pose estimation
We first present  a sensitivity analysis to motivate the use of our method for  robust pose estimation under challenging conditions
Then  we show results for two well-known data sets, where the  objective is to perform recognition and pose estimation for  several objects in highly occluded and cluttered real scenes  captured by a LIDAR sensor
Finally, we show results for  two newer RGB-D data sets, one made for multi-instance  detections and one made for detection of domestic objects
 N.N
Pose clustering as a robust pose estimator  We first show a sensitivity analysis of our method
For  comparison, we included RANSAC [NN], which is likely the  most robust pose estimator available, cf
its widespread use  in ND recognition, e.g
[N, NN, NN]
The task is to align a ND  model to itself under varying noise levels while monitoring  the pose errors
We took the classical Stanford BunnyN, consisting of NNNNN points and NNNNN triangles, and applied increasing random uniform displacements to the points using  MeshLabN, with the displacement norm bounded to a specified percentage of the diagonal
For the Bunny, the diagonal is 0.NN m, and we used noise levels from 0.N % to N.0 %  with increments of 0.N %
We computed local features with  a spacing of 0.00N m—giving approximately N000 feature  descriptors—on the original model and each noisy version
 We matched the original model to each corrupted model  and monitored the ground truth inlier rate by checking how  many features on the clean model matched to the same point  on the corrupted model with a tolerance of 0.00N m
 The relative pose to be found here is simply the identity  transformation, which allows us to easily measure pose errors
For positions, the error is given by the norm of the  estimated translation, and for rotations, we can compute the  error as the geodesic distance between the rotation estimate  Nhttp://graphics.stanford.edu/data/NDscanrep Nhttp://meshlab.sourceforge.net  and the identity rotation by (NN)
Contrary to all subsequent experiments, no refinement was used, since the purpose was  to investigate the robustness of the two estimators
 The results are shown in Fig
N
For RANSAC, we tuned  the number of iterations to N0000 to allow the algorithm  to spend approximately the same runtime as ours
For our  method, we used a translation bandwidth of 0.0N m and a  rotation bandwidth of NN.N◦, which are exactly the same parameters that we used in all other experiments
One difference, however, is that since RANSAC is non-deterministic  by design, we repeated the the estimation N0 times and took  the mean over the N0 runs at each noise level
As shown, our  method remains considerably more robust towards noise
 We believe the explanation is that our method is better at  handling many outliers (wrong correspondences) occurring  at high noise levels by virtue of a more discriminating score  function
Indeed, our method produces a score proportional  to the frequency that a pose occurs in a certain N DoF neighborhood of SE(N), which makes spurious local maxima highly accidental
Conversely, RANSAC samples a cubic  number of poses and uses a geometric consistency scoring  criterion
When the noise increases, there is a much higher  risk that many correspondences will support a wrong pose
 The inlier rate, shown rightmost in Fig
N, drops from  NN % at the lowest noise level to 0.N % at the highest noise  level
Even under such extreme conditions, our method  produces the correct pose, whereas RANSAC fails to estimate the relative pose with a translation error of 0.0NN m  and a rotation error of NN◦
Finally, we include a plot of  the noise in the surface normals, since our method crucially  relies on these for the voting process
We computed the  average angular deviation between the normals on each of  the corrupted models and those on the original model
As  can be seen rightmost in Fig
N, there is a strong linear dependency of these normal deviations on the artificial point  noise, leading to the conclusion that our pose estimates are  equivalently robust towards noise in the normals
The normals achieve average displacements of almost NN◦, while  our method still produces correct results
 NNNN  http://graphics.stanford.edu/data/NDscanrep http://meshlab.sourceforge.net   Method UWA Queen’s  Spin Images [NN] 0.NNN —  Tensor matching [N0] 0.NNN —  PPF* 0.NNN 0.NNN  EM [N] 0.NNN 0.NNN  RoPS [NN] 0.NNN 0.NNN  VD-LSD [NN] — 0.NNN  Pose clustering N.00 N.00  Table N: Recall rates for the UWA [N0] and Queen’s [NN]  data sets
All results except the ones for PPF* and Pose  clustering are taken from the literature
 N.N
Recognition results on the UWA data set  We first tested our method on the UWA data set [N0],  which is the most well-known and established data set in  the literature and has been the subject of several evaluations
 The data set contains four complete object models and N0  scenes, all captured with a laser scanner and given as highresolution triangular meshes
Almost all scenes contain all  objects, giving NNN instances to recognize in total
The objects are highly occluded, with a less than NN % average  visibility in each scene
We ran the multi-object pipeline  outlined in Sect
N to search for the objects in each scene
 We compared against a select number of classical and recent, best-performing methods
For the PPF method, originally proposed in [NN], we used the latest and optimized  implementation of the PPF method, which is now part of  the commercial machine vision software Halcon NN.0.0.NN
 We will denote this method PPF* in the folllowing
 Results for the UWA data set are given in the middle column of Tab
N
For all methods, we give recall rates between  0 and N, where N means N00 % recognition rate
Concerning precision, we used a lower threshold on the modal pose  density (NN) to reject false positives and increase precision
To our knowledge, we are the first to achieve a N00 %  recognition rate on this data set without the use of joint optimization, as in e.g
[N, NN]
Additionally, our method produced few false positives, resulting in a precision of NN.N %  and a maximum FN score of 0.NNN
 N.N
Recognition results on Queen’s data set  The next tested data set was the Queen’s data set [NN],  created in a similar manner to the UWA data set
This data  set has five objects, N0 scenes and NN0 instances
Compared  to UWA, it contains a higher variation in the number of objects present in each scene
Each scene also contains spurious data from the ground plane and in general all models  are of lower quality and have non-uniform resolution
 We report comparative results for the Queen’s data set  in the right column of Tab
N, which reveals reduced recogNhttp://www.mvtec.com/products/halcon  Method Object Recall  Tejani et al
[NN] Coffee cup 0.NNN  Juice 0.NNN  Doumanoglou et al
[N] Coffee cup 0.NNN  Juice 0.NNN  PPF* Coffee cup 0.NNN  Juice 0.NNN  Pose clustering Coffee cup 0.NNN  Juice 0.NNN  Table N: Recall rates for the bin picking data set [N]
 nition rates for many of the competing methods relative to  UWA
Remarkably, both our method and PPF* perform better than UWA on this data set
PPF* achieves a recall of  NN.N % and the same precision
On this data set, our method  achieves N00 % recall at a precision of N00 %
 N.N
Recognition results on the bin picking data set  Another experiment was done on a very recent data set  [N], where the authors introduced a bin picking data set consisting of NNN RGB-D images showing multiple instances  of two test objects in a small bin
The scenes are split up  in three sequences: one where the bin contains NN instances  of the Coffee cup object, one where the bin contains five instances of the Juice box object and finally a mixed sequence  where each image shows the bin containing nine and four  instances of the Coffee cup and the Juice box, respectively
 The protocol for this data set is to match the two objects to  their dedicated sequences and to the mixed sequence
Although this data set is primarily targeted at another class of  detection methods—namely RGB-D based systems that use  both color and depth information—we wanted to test the  performance of our method, even though our method relies purely on geometric cues
On the other hand, the PPF*  method and ours do not require expensive training but derive the features for matching directly from the oriented surfaces
Contrary to the two previous experiments, the objective is now multi-instance detection, so we extracted the ten  top ranked modes after non-maximum suppression using  our method
The same applies to the PPF* method where  we set it to return the ten top scoring poses
Other than  that, all experimental parameters for both methods were the  same as previously
The results, including the baseline results from [N, NN], are given in Tab
N and a multi-instance  recognition example is shown in Fig
N
 The results show that the ND methods (PPF* and ours)  compete well with RGB-D based methods
This is achieved  in approximately the same runtime for all methods, which  is in the order of seconds per recognized instance
These  results support the use of our method for multi-instance  recognition problems
We believe the main reason why our  method is able to outperform PPF* is that we use smooth  NNNN  http://www.mvtec.com/products/halcon   Figure N: Multi-instance recognition output (the top ten detections) for the first bin picking scene with nine true positives (green) and one false positive in the back (red)
 density estimates on SE(N), whereas PPF* uses approx- imate clustering
Our method achieves a substantial improvement over other methods, in particular the two RGB-D  based methods, which were designed for this kind of data
 N.N
Recognition results on domestic data set  The final experiment was performed on the data set  of [NN], which is a challenging RGB-D based recognition  data set for domestic environments, containing thousands of  test scenes
Results have been reported for LINEMOD [NN],  PPF [NN] and two new RGB-D based methods [N, NN]
As  recommended in [NN], we extract the top five modes in each  scene to build the precision-recall curves
For all objects except the Shampoo, our method produces a higher FN score than the other ND method, PPF
For the Camera object, the  most recent RGB-D method [N] outperforms ours
On average, our method outperforms existing methods, producing  the highest average FN score
The results are listed in Tab
N
We stress that the methods [N, NN, NN] use both geometric  and appearance cues from RGB-D templates, whereas PPF  and our method use only the geometry to match a full ND  model to a scene view
We would like to try using colorbased local ND features with our approach, as this should  allow for further improvements for RGB-D data sets
 N.N
Runtimes  In practice, our system has a per-object runtime—  including preprocessing, feature computation and matching, which are all amortized over all objects—of N–N s (N.N s  for UWA, N s for Queen’s, N s for the bin picking data set and  N s for the domestic data set)
These numbers are obtained  by execution on a consumer laptop with a N.N0 GHz Intel iNNN00U CPU with four cores, leaving a potential for speedup  on other architectures and with further optimizations
More  than N0 % of this time is spent on ND features and matching  Object [NN] [NN] [NN] [N] Ours  Coffee cup 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  Shampoo 0.NNN 0.NNN 0.NNN 0.NNN 0.N0N  Joystick 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  Camera 0.NNN 0.N0N 0.NNN 0.N0N 0.NNN  Juice 0.NNN 0.N0N 0.NN0 0.NNN 0.NNN  Milk 0.NNN 0.NNN 0.NNN 0.NN0 0.NNN  Average 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN  Table N: FN scores for the data set [NN] and the follow- ing methods: LINEMOD [NN], PPF [NN], Tejani et al
[NN],  Doumanoglou et al
[N] and our pose clustering method
 to obtain correspondences
Thus, the clustering and mode  finding is not the bottleneck in our system
 For PPF* we used a regularly updated implementation  from the Halcon software (we used v
NN.0.0.N), which also  runs in a few seconds per instance
In [N], the total processing time is unspecified, although it is stated that the main  bottleneck of the system takes N–N s
In [NN] the runtime is  unspecified
Other systems relying on local features report  runtimes such as a few seconds [N] and minutes [NN, N0]
 N
Conclusions and future work  This work contributed a method for ND object recognition using a new pose voting and clustering method for obtaining robust pose estimates in cluttered scenes
The pose  voting exploited the fact that corresponding oriented points  between two models can be used to cast a constrained number of votes for the correct pose aligning the two models
 For the final inference step, a branch and bound search was  performed to compute density estimates for each pose
An  initial sensitivity analysis showed increased robustness to  outlier correspondences compared to RANSAC
When integrated into a local feature-based recognition pipeline, our  method achieved perfect recall for two well-known recognition data sets and it has outperformed recent methods on  two RGB-D recognition data sets
 Our method is slightly sensitive towards planar or repetitive structures, since the local feature correspondences scatter randomly in their presence
We are currently investigating whether other local features, e.g
edge based, can  be used to obtain better correspondences under such conditions
We are also working on incorporating appearance information into our method using color-based local features,  which should allow for increased accuracy on RGB-D data
 Acknowledgements  The research leading to these results has been funded in  part by Innovation Fund Denmark as a part of the project  “MADE — Platform for Future Production” and by the EU  FoF Project ReconCell (project number NN0NNN)
 NNNN    References  [N] A
Aldoma, F
Tombari, L
Di Stefano, and M
Vincze
 A global hypothesis verification framework for Nd object  recognition in clutter
IEEE Transactions on Pattern Analysis and Machine Intelligence, NN(N):NNNN–NNNN, N0NN
N, N,  N, N  [N] P
Bariya, J
Novatnack, G
Schwartz, and K
Nishino
 Nd geometric scale variability in range images: Features  and descriptors
International Journal of Computer Vision,  NN(N):NNN–NNN, N0NN
N, N  [N] P
Besl and N
D
McKay
A method for registration of N-d  shapes
IEEE Transactions on Pattern Analysis and Machine  Intelligence, NN(N):NNN–NNN, NNNN
N  [N] T
Birdal and S
Ilic
Point pair features based object detection and pose estimation revisited
In IEEE International  Conference on ND Vision, pages NNN–NNN, N0NN
N  [N] E
Brachmann, A
Krull, F
Michel, S
Gumhold, J
Shotton,  and C
Rother
Learning Nd object pose estimation using Nd  object coordinates
In European Conference on Computer  Vision, pages NNN–NNN, N0NN
N  [N] H
Chen and B
Bhanu
Nd free-form object recognition in  range images using local surface patches
Pattern Recognition Letters, NN(N0):NNNN–NNNN, N00N
N  [N] Y
Cheng
Mean shift, mode seeking, and clustering
IEEE  Transactions on Pattern Analysis and Machine Intelligence,  NN(N):NN0–NNN, NNNN
N  [N] C
S
Chua and R
Jarvis
Point signatures: A new representation for Nd object recognition
International Journal of  Computer Vision, NN(N):NN–NN, NNNN
N  [N] A
Doumanoglou, R
Kouskouridas, S
Malassiotis, and T.K
Kim
Recovering Nd object pose and predicting next-bestview in the crowd
In IEEE Conference on Computer Vision  and Pattern Recognition, June N0NN
N, N, N  [N0] B
Drost and S
Ilic
Nd object detection and localization using multimodal point pair features
In Second International  Conference on ND Imaging, Modeling, Processing, Visualization & Transmission, pages N–NN, N0NN
N  [NN] B
Drost, M
Ulrich, N
Navab, and S
Ilic
Model globally,  match locally: Efficient and robust Nd object recognition
In  IEEE Conference on Computer Vision and Pattern Recognition, volume N, page N, N0N0
N, N, N  [NN] M
A
Fischler and R
C
Bolles
Random sample consensus: a paradigm for model fitting with applications to image  analysis and automated cartography
Communications of the  ACM, NN(N):NNN–NNN, NNNN
N, N  [NN] A
Frome, D
Huber, R
Kolluri, T
Bülow, and J
Malik
Recognizing objects in range data using regional point descriptors
In European Conference on Computer Vision, pages  NNN–NNN, N00N
N  [NN] Y
Guo, M
Bennamoun, F
Sohel, M
Lu, and J
Wan
Nd  object recognition in cluttered scenes with local surface features: a survey
IEEE Transactions on Pattern Analysis and  Machine Intelligence, NN(NN):NNN0–NNNN, N0NN
N  [NN] Y
Guo, F
Sohel, M
Bennamoun, M
Lu, and J
Wan
Rotational projection statistics for Nd local surface description  and object recognition
International Journal of Computer  Vision, N0N(N):NN–NN, N0NN
N, N, N, N  [NN] S
Hinterstoisser, C
Cagniart, S
Ilic, P
Sturm, N
Navab,  P
Fua, and V
Lepetit
Gradient response maps for realtime detection of textureless objects
IEEE Transactions on  Pattern Analysis and Machine Intelligence, NN(N):NNN–NNN,  N0NN
N, N  [NN] S
Hinterstoisser, V
Lepetit, N
Rajkumar, and K
Konolige
 Going further with point pair features
In European Conference on Computer Vision, pages NNN–NNN, N0NN
N  [NN] A
E
Johnson and M
Hebert
Using spin images for efficient  object recognition in cluttered Nd scenes
IEEE Transactions  on Pattern Analysis and Machine Intelligence, NN(N):NNN–  NNN, NNNN
N, N  [NN] W
Kehl, F
Milletari, F
Tombari, S
Ilic, and N
Navab
Deep  learning of local rgb-d patches for Nd object detection and  Nd pose estimation
In European Conference on Computer  Vision, pages N0N–NN0, N0NN
N  [N0] A
S
Mian, M
Bennamoun, and R
Owens
Threedimensional model-based object recognition and segmentation in cluttered scenes
IEEE Transactions on Pattern Analysis and Machine Intelligence, NN(N0):NNNN–NN0N, N00N
N,  N, N  [NN] M
Muja and D
G
Lowe
Scalable nearest neighbor algorithms for high dimensional data
IEEE Transactions on Pattern Analysis and Machine Intelligence, NN(NN):NNNN–NNN0,  N0NN
N  [NN] C
Papazov and D
Burschka
An efficient ransac for Nd object recognition in noisy and occluded scenes
In Asian Conference on Computer Vision, pages NNN–NNN, N0N0
N, N, N  [NN] R
Rios-Cabrera and T
Tuytelaars
Discriminatively trained  templates for Nd object detection: A real time scalable approach
In IEEE International Conference on Computer Vision, pages N0NN–N0NN, N0NN
N  [NN] R
B
Rusu and S
Cousins
Nd is here: Point cloud library  (pcl)
In IEEE International Conference on Robotics and  Automation, pages N–N, N0NN
N  [NN] S
Salti, F
Tombari, and L
Di Stefano
Shot: unique signatures of histograms for surface and texture description
Computer Vision and Image Understanding, NNN:NNN–NNN, N0NN
 N  [NN] B
Taati and M
Greenspan
Local shape descriptor selection  for object recognition in range data
Computer Vision and  Image Understanding, NNN(N):NNN–NNN, N0NN
N, N, N  [NN] A
Tejani, D
Tang, R
Kouskouridas, and T.-K
Kim
Latentclass hough forests for Nd object detection and pose estimation
In European Conference on Computer Vision, pages  NNN–NNN, N0NN
N, N, N  [NN] F
Tombari and L
Di Stefano
Object recognition in Nd  scenes with occlusions and clutter by hough voting
In  Fourth Pacific-Rim Symposium on Image and Video Technology, pages NNN–NNN, N0N0
N, N  [NN] P
Wohlhart and V
Lepetit
Learning descriptors for object  recognition and Nd pose estimation
In IEEE Conference  on Computer Vision and Pattern Recognition, pages NN0N–  NNNN, N0NN
N  [N0] A
Zaharescu, E
Boyer, and R
Horaud
Keypoints and local descriptors of scalar functions on Nd manifolds
International Journal of Computer Vision, N00(N):NN–NN, N0NN
 N  NNNNBlitzNet: A Real-Time Deep Network for Scene Understanding   BlitzNet: A Real-Time Deep Network for Scene Understanding  Nikita Dvornik∗ Konstantin Shmelkov∗ Julien Mairal Cordelia Schmid  Inria†  Abstract  Real-time scene understanding has become crucial in  many applications such as autonomous driving
In this paper, we propose a deep architecture, called BlitzNet, that  jointly performs object detection and semantic segmentation in one forward pass, allowing real-time computations
 Besides the computational gain of having a single network  to perform several tasks, we show that object detection  and semantic segmentation benefit from each other in terms  of accuracy
Experimental results for VOC and COCO  datasets show state-of-the-art performance for object detection and segmentation among real time systems
 N
Introduction  Object detection and semantic segmentation are two fundamental problems for scene understanding in computer vision
The task of object detection is to identify on an image  all objects of predefined categories and localize them via  bounding boxes
Semantic segmentation operates at a finer  scale; its aim is to parse an image and associate a class label  to each pixel
Despite the similarities of the two tasks, only  few works have tackled them jointly [N, NN, NN, NN]
 Yet, there is a strong motivation to address both problems simultaneously
On the one hand, good segmentation  is sufficient to perform detection in some cases
As Figure N suggests, an object may be sometimes identified and  localized from segmentation only by simply looking at connected components of pixels sharing the same label
In the  more general case, it is easy to conduct a simple experiment showing that ground-truth segmentation is a meaningful clue for detection, using for instance ground-truth segmentation as the input of an object detection pipeline
On  the other hand, correctly identified detections are also useful for segmentation as shown by the success of weakly supervised segmentation techniques that learn from bounding  box annotation only [NN]
The goal of our paper is to solve  ∗The authors contributed equally
†Univ
Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, NN000  Grenoble, France
 (a) Generated bounding boxes (b) Generated masks  Fig
N: The outputs of our pipeline
(a) The results of object  detection
(b) The results of semantic segmentation
 efficiently both problems at the same time, by exploiting  image data annotated at the global object level (via bounding boxes), at the pixel level (via partially or fully annoated  segmentation maps), or at both levels
 As most recent image recognition pipelines, our approach is based on convolutional neural networks [NN],  which are widely adopted for object detection [N] and semantic segmentation [NN]
More precisely, deep neural networks were first used as feature extractors to classify a large  number of candidate bounding boxes [N], which is computationally expensive
The improved version [N] reduces the  computational cost but relies on shallow techniques for extracting bounding box proposals and does not allow end-toend training
This issue was later solved in [NN] by making  the object proposal mechanism a part of the neural network
 Yet, the approach remains expensive and relies on a regionbased strategy (see also [NN]) that makes the network architecture inappropriate for semantic segmentation
 To match the real-time speed requirement, we choose  instead to base our work on the Single Shot Detection (SSD) [NN] approach, which consists of a fullyconvolutional model to perform object detection in one forward pass
Besides the fact that it allows all computations to  be performed in real time, the pipeline is more generic, imposes less constraints on the network architecture and opens  new perspectives to solve our multi-task problem
 Interestingly, recent work on semantic segmentations are  also moving in the same direction, see for instance [NN]
 Specific to semantic segmentation, [NN] also introduces new  NNNN    ideas such as the joint use of feature maps of different resolutions, in order to obtain more accurate classification
The  idea was then improved by adding deconvolutional layers  at all scales to better aggregate context, in addition to using  skip and residual connections [NN]
Deconvolutional layers  turned out to be useful to estimate precise segmentations,  and are thus good candidates to design architectures where  localization is important
 In this paper, we consider the multi-task scene understanding problem consisting of joint object detection and semantic segmentation
For that purpose, we propose a novel  pipeline called BlitzNet, which will be released as an opensource software package
BlitzNet is able to provide accurate segmentation and object bounding boxes in real time
 With a single network for solving both problems, the computational cost is reduced, and we show also that the two  tasks benefit from each other in terms of accuracy
 The paper is organized as follows: Section N discusses  related work; Section N presents our real-time multi-task  pipeline called BlitzNet
Finally, Section N is devoted to  our experiments, and Section N concludes the paper
 N
Related Work  Before we introduce our approach, we now present techniques for object detection, semantic segmentation, and previous attempts to combine both tasks
 Object detection
The field of object detection has been  recently dominated by variants of the R-CNN architecture [N, NN], where bounding-box proposals are independently classified by a convolutional neural network, and  then filtered by a non-maximum suppression algorithm
It  provides great accuracy, but relatively low inference speed  since it requires a significant amount of computation per  proposal
R-FCN [NN] is a fully-convolutional variant that  further improves detection and significantly reduces the  computational cost per proposal
Its region-based mechanism is however dedicated to object detection only
 SSD [NN] is a recent state-of-the-art object detector,  which uses a sliding window approach instead of generated  proposals to classify all boxes directly
SSD creates a scale  pyramid to find objects of various sizes in one forward pass
 Because of its speed and high accuracy, we have chosen to  build our work on, and subsequently improve, the SSD approach
Finally, YOLO [NN, NN] also provides real-time object detection and shares some ideas with SSD
 Semantic segmentation and deconvolutional layers
 Deconvolutional architectures consist of adding to a classical convolutional neural networks with feature pooling, a  sequence of layers whose purpose is to increase the resolution of the output feature maps
This idea is natural in  the context of semantic segmentation [N0], since segmentation maps are expected to have the same resolution as input  images
Yet, it was also successfully evaluated in other contexts, such as pose estimation [NN], and object detection, as  extensions of SSD [N] and Faster-R-CNN [NN]
 Joint semantic segmentation and object detection
The  idea of joint semantic segmentation and object detection  was investigated first for shallow approaches in [N, NN, NN,  N]
There, it was shown that learning both tasks simultaneously could be better than learning them independently
 More recently, UberNet [NN] integrates multiple vision  tasks such as semantic segmentation and object detection  into a single deep neural network
The detection part is  based on the Faster R-CNN approach and is thus neither  fully-convolutional nor real-time
Closely related to our  work, but dedicated to autonomous driving, [NN] also proposes to integrate semantic segmentation and object detection via a deep network
There, the VGGNN network [NN] is  used to compute image features (encoding step), and then  two different sub-networks are used for the prediction of  object bounding boxes and segmentation maps (decoding)
 Our work is inspired by these previous attempts, but goes  a step further in integrating the two tasks, with a fully convolutional approach where network weights are shared for  both tasks until the last layer, which has advantages in terms  of speed, feature sharing, and simplicity for training
 N
Scene Understanding with BlitzNet  In this section, we introduce the BlitzNet architecture  and discuss its different building blocks
 N.N
Global View of the Pipeline  The joint object detection and segmentation pipeline is  presented in Figure N
The input image is first processed  by a convolutional neural network to produce a map that  carries high-level features
Because of its high performance  for classification and good trade-off for speed, we use the  network ResNet-N0 [N] as our feature encoder
 Then, the resolution of the feature map is iteratively reduced to perform a multi-scale search of bounding boxes,  following the SSD approach [NN]
Inspired by the hourglass architecture [NN] for pose estimation and an earlier  work on semantic segmentation [N0], the feature maps are  then up-scaled via deconvolutional layers in order to predict  subsequently precise segmentation maps
Recent DSSD approach [N] uses a similar strategy for object detection the  top part of our architecture presented in Figure N may be  seen as a variant of DSSD with a simpler “deconvolution  module”, called ResSkip, that involves residual and skip  connections
 Finally, prediction is achieved by single convolutional  layers, one for detection, and one for segmentation, in one  NNNN    Fig
N: The BlitzNet architecture, which performs object detection and segmentation with one fully convolutional network
 On the left, CNN denotes a feature extractor, here ResNet-N0 [N]; it is followed by the downscale-stream (in blue) and the last  part of the net is the upscale-stream (in purple), which consists of a sequence of deconvolution layers interleaved with ResSkip  blocks (see Figure N)
The localization and classification of bounding boxes (top) and pixelwise segmentation (bottom) are  performed in a multiscale fashion by single convolutional layers operating on the output of deconvolution layers
 forward pass, which is the main originality of our work
 N.N
SSD and Downscale Stream  The Single Shot MultiBox Detector [NN] tiles an input  image with a regular grid of anchor boxes and then uses  a convolutional neural network to classify these boxes and  predict corrections to their initial coordinates
In the original paper [NN], the base network VGG-NN [NN] is followed  by a cascade of convolutional and pooling layers to form  a sequence of feature maps with progressively decreasing  spatial resolution and increasing field of view
In [NN],  each of these layers is processed separately in order to classify and predict coordinates correction for a set of default  bounding boxes of a particular scale
At test time, the set  of predicted bounding boxes is filtered by non-maximum  suppression (NMS) to form the final output
 Our pipeline uses such a cascade (see Figure N), but the  classification of bounding boxes and pixels to build the segmentation maps is performed in subsequent layers, called  deconvolutional layers, which will be described next
 N.N
Deconvolution Layers and ResSkip Blocks  Modeling visual context is often a key to complicated  scenes parsing, which is typically achieved by pooling layers in a convolutional neural network, leading to large receptive fields for each output neuron
For semantic segmentation, precise localization is equally important, and [N0]  proposes to use deconvolutional operations to solve that issue
Later, this process was improved in [NN] by adding  skip connections
Apart from combining high- and lowlevel features it also eases the learning process [N]
 Like [N] for object detection and [NN] for pose estimation, we also use such a mechanism with skip connections  that combines feature maps from the downscale and upscale streams (see Figure N)
More precisely, maps from  the downscale and upscale streams are combined with a  simple strategy, which we call ResSkip, presented in Figure N
First, incoming feature maps are upsampled to the  size of corresponding skip connection via bilinear interpolation
Then both skip connection feature maps and upsampled maps are concatenated and passed through a block  (N × N convolution, N × N convolution, N × N convolution)  and summed with the upsampled input through a residual  connection
The benefits of this topology will be justified  and discussed in more details in the experimental section
 N.N
Multiscale Detection and Segmentation  The problem of semantic segmentation and object detection share several key properties
They both require perregion classification, based on the pixels inside an object  while taking into account its surrounding, and benefit from  rich features that include localization information
Instead  of training a separate network to perform these two tasks,  we train a single one that allows weight sharing, such that  both tasks can benefit from each other
 In our pipeline, most of the weights are shared
Object  detection is performed by a single convolutional layer that  predicts a class and coordinate corrections for each bounding box in the feature maps of the upscale stream
Similarly,  a single convolutional layer is used to predict the pixel laNNNN    Fig
N: ResSkip block integrating feature maps from the upscale and downscale streams, with skip connection
 bels and produce segmentation maps
To achieve this we  upscale all the activations of the upscale stream, concatenate them and feed to the final classification layer
 N.N
Speeding up Non-Maximum Suppression  Increasing the number of anchor boxes heavily affects  inference time because it performs NMS on a potentially  huge number of proposals (in the worst case scenario, it may  be all of them)
Indeed, we observed that by using sliding  window proposals, addition of small scale proposals slows  down the inference even more than increasing image resolution
Surprisingly, non-maximum suppression may then  become the bottleneck at inference time
We observed that  this occurred sometimes for particular object classes that return a lot of bounding box candidates
 Therefore, we suggest a different post-processing strategy to accelerate detection when there are too many proposals
For each class, we pre-select the top N00 boxes  with largest scores, and perform NMS leaving only N0 of  them
Overall, the final detection is the top N00 highest scoring boxes per image after non-maximum suppression
This  strategy yields a reasonable computational time for NMS,  and has marginal impact on accuracy
 N.N
Training and Loss Functions  Given labeled training data where each data point is annotated with segmentation maps, or bounding boxes, or  with both, we consider a loss function which is simply the  sum of two loss functions of the two task
Note that we tried  reweighting the two loss functions, but we did not observe  noticeable improvements in terms of accuracy
 For segmentation, the loss is the cross-entropy between  predicted and target class distribution of pixels [N]
Specifically, we use a N×N convolutional operation with NN channels to map each layer of the upscale-stream to an intermediate representation
After this, each layer is upscaled to  the size of the last layer using bilinear interpolation and all  maps are concatenated
This representation is mapped to c feature maps, where c is the number of classes, by using N× N convolutions to predict posterior class probabilities
 For detection, we use the same loss function as [NN]  when performing tiling of the input image with anchor  boxes and matching them to ground truth bounding boxes
 We use activations of each layer in the upscale-stream to  regress corrections for coordinates of the anchor boxes and  to predict the class probability distribution
We use the same  data augmentation suggested in the original SSD pipeline,  namely photometric distortions, random crops, horizontal  flips and zoom-out operation
 N
Experiments  We now present various experiments conducted on the  COCO, Pascal VOC N00N and N0NN datasets, for which both  bounding box annotations and segmentation maps are available
Section N.N discusses in more details the datasets and  the metrics we used; Section N.N presents technical details  that are useful to make our work reproducible, and then  each subsequent subsection is devoted to a particular experiment
The last two sections discuss the inference speed  and clarify particular choices in the network architecture
 Our code is now available as an open-source software package at http://thoth.inrialpes.fr/research/  blitznet/
 N.N
Datasets and Metrics  We use the COCO [NN], VOC0N, and VOCNN  datasets [N]
All images in the VOC datasets are annotated  with ground truth bounding boxes of objects and only a subset of VOCNN is annotated with target segmentation masks
 The VOC0N dataset is divided into N subsets, trainval (N0NN  images) and test (NNNN images)
The VOCNN-train subset  contains NNNN images annotated for detection and NNNN of  them have segmentation ground truth as well (VOCNN-trainseg), while VOCNN-val has NNNN images for detection and  NNNN images for segmentation (we call this subset VOCNNval-seg)
Both datasets have N0 object classes
 The COCO dataset includes N0 object categories for detection and instance segmentation
For the task of detection, there are N0k images for training and N0k for validation
There is no either a protocol for evaluation of semantic segmentation or even annotations to train it from
In this  work, we are interested particularly in semantic segmentation masks so we obtain them from instance segmentation  annotations by combining instances of one category
 To carry out more extensive experiments we leverage extra annotations for VOCNN segmentation provided by [N],  which gives a total of N0,NNN fully annotated images for  NNNN  http://thoth.inrialpes.fr/research/blitznet/ http://thoth.inrialpes.fr/research/blitznet/   training that we call VOCNN-train-seg-aug
We still keep the  original PASCAL annotations in VOCNN val-seg, even if a  more precise annotation is available in [N], for a fair comparison with other methods that do not benefit from these  extra annotations
 In VOCNN and VOC0N datasets, a predicted bounding  box is correct if its intersection over union with the ground  truth bounding box is higher than 0.N
The metric for eval- uation detection performance is the mean average precision  (mAP) and the quality of predicted segmentation masks is  measured with mean intersection over union (mIoU)
 N.N
Experimental Setup  In this section, we discuss the common setup to all experiments
BlitzNet is coded in Python and TensorFlow
 All experiments were conducted on a single Titan X GPU  (Maxwell architecture), which makes the speed comparison  with previous work easy, as long as they use the same GPU
 Optimization Setup
In all our experiments, unless explicitly stated otherwise, we use the Adam algorithm [N0],  with a mini-batch size of NN images
The initial learning rate  is set to N0−N and decreased twice during training by a factor N0
We also use a weight decay parameter of N× N0−N
 Modeling setup
As already mentioned, we use ResNetN0 [N] as a feature extractor, NNN feature maps for each  layer in down-scale and up-scale streams, NN channels for  intermediate representations in the segmentation branches;  BlitzNetN00 takes input images of size N00 × N00 and  BlitzNetNNN uses NNN × NNN images
Different versions  of the network vary in the stride of the last layer of the  upscaling-stream
Strides N and N in the result tables are  denoted as (sN) and (sN) suffix respectively
 N.N
PASCAL VOC N00N  In this experiment, we train our networks on the union  of VOC0N trainval set and VOCNN trainval set; then, we test  them on the VOC0N test set
The results are reported in  the Table N
For experiments that involve segmentation, we  leverage ground truth segmentation masks during training  if they are available in VOCNN train-seg-aug or in VOCNN  val-seg
When using images of size N00× N00 as input, the  stochastic gradient descent algorithm is performed by training for NNK iterations with the initial learning rate, which  is then decreased after NNK and N0K steps
When training  on NNN × NNN images, we choose the batch size of NN and  learn for NNK iterations decreasing the learning rate after NNK and N0K steps
 The results show that BlitzNetN00 outperforms SSDN00  and YOLO with a NN.N mAP, while being a real time detec- tor
BlitzNetNNN (sN) performs 0.N% better than R-FCN - the most accurate competitive model, scoring NN.N% mAP
 We further improve the results by training for detection and  segmentation jointly achieving NN.N% and NN.N% mAP with BlitzNetN00 (sN) and BlitzNetNNN (sN) respectively
 We think that the performance gain for BlitzNetN00 over  BlitzNetNNN could be explained by the larger stride used for  the last layer, which is N, vs N for BlitzNetNNN, and seems  to be helpful for better learning finer details
Unfortunately,  training BlitzNetNNN with stride N was impossible because  of memory limitations on our single GPU
 N.N
PASCAL VOC N0NN  In this experiment, we use VOCNN train-seg-aug for  training and VOCNN val-seg for testing both segmentation  and detection
We train the models for N0K steps with the  initial learning rate, and then decrease it after NNK and NNK  iterations
As Table N shows, joint training improves accuracy on both tasks comparing to learning a single task
 Detection improves by more than N% while segmentation  mIoU grows by 0.N%
We argue that this result could be explained by feature sharing in the universal architecture
 To confirm this fact, we conducted another experiment  by adding the VOC0N trainval images to VOCNN train-segaug for training
Then, the proportion of images that have  segmentation annotations to the ones that have detection  ones only is N/N, in contrast to the previous experiments where all the images where annotated for both tasks
To  deal with cases where a mini-batch has no images to train  for segmentation, we set the corresponding loss to 0 and  do not back propagate with respect to these images, otherwise we use all images that have target segmentation masks  in a batch to update the weights
The results presented in  Table N show an improvement of N.N%
Detection also im- proves in mAP by 0.N%
Figure N shows that extra data for detection helps to improve classification results and to mitigate confusion between similar categories
In Table N, we  report results for these models on the VOCNN test server,  which again shows that our results are competitive
More  qualitative results, including failure cases, are presented in  the supplementary material
 N.N
Microsoft COCO Dataset  To further validate the proposed framework, we conduct  experiments on the COCO dataset [NN]
Here, as explained  in Section N.N, we obtain segmentation masks and again  training the model on different types of data, i.e., detection,  segmentation and both, to study the influence of joint training on detection accuracy
 We train the BlitzNetN00 or BlitzNetNNN models for  N00k iterations in total, starting from the initial learning rate  N0−N and then decreasing it after the N00k and NN0k iterations by the factor of N0
Table N shows clear benefits from  joint training for both of the tasks on the COCO dataset
 To be comparable with other methods, we also report the  NNNN    network backbone mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv  SSDN00* [NN] VGG-NN NN.N NN.N NN.0 NN.N NN.N N0.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  SSDN00* (our reimpl) ResNet-N0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N  BlitzNetN00 (sN) ResNet-N0 NN.N NN.N NN.N N0.N NN.N N0.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N N0.0 NN.0 NN.0  BlitzNetN00 (sN) ResNet-N0 NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.0 NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  BlitzNetN00 + seg (sN) ResNetN0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  SSDNNN* [NN] VGG-NN NN.N NN.N NN.N N0.N NN.0 NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N  BlitzNetNNN (sN) ResNet-N0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  R-FCN[NN] ResNet-N0N N0.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Faster RCNN ResNet-N0N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.0  YOLO [NN] YOLO net NN.N - - - - - - - - - - - - - - - - - - - - BlitzNetNNN + seg (sN) ResNetN0 NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N  Table N: Comparison of detection performance on Pascal VOC N00N test set
The models where trained on VOC0N trainval  + VOCNN trainval
The models that have suffix “+ seg” where trained for segmentation jointly with data from VOCNN trainval  and extra annotations provided by [N]
The values in columns correspond to average precision per class (%)
 network backbone mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbike persn plant sheep sofa train tv  SSDN00* [NN] VGG-NN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N  BlitzNetN00 ResNetN0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N  BlitzNetN00 + COCO ResNetN0 N0.N NN.0 NN.N N0.0 N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  R-FCN[NN] ResNet-N0N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N N0.N NN.N NN.0 NN.N NN.N NN.0 N0.N NN.N NN.N NN.N  Faster RCNN ResNet-N0N NN.N NN.N NN.N NN.N NN.0 NN.0 NN.N NN.N NN.N NN.N N0.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  YOLO [NN] YOLOnet NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N  SSDNNN* [NN] VGG-NN NN.N N0.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N  BlitzNetNNN ResNetN0 NN.0 NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N  BlitzNetNNN + COCO ResNetN0 NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.0  Table N: Comparison of detection performance on Pascal VOC N0NN test set
The models where trained on VOC0N  trainval + VOCNN trainval
The BlitzNet models where trained for segmentation jointly with data from VOCNN trainval and  extra annotations provided by [N]
Suffix ‘+ COCO’ means that the model was pretrained on the COCO dataset
The reported  values correspond to average precision per class (%)
Detailed results of submissions are available on the VOCNN test server
 network seg det mIoU mAP  BlitzNetN00 X - NN.N  BlitzNetN00 X X NN.N N0.0  BlitzNetN00 X NN.N Table N: The effect of joint learning on both tasks
The  networks where trained on VOCNN train-seg-aug, and tested  on VOCNN val
 network seg det mIoU mAP  BlitzNetN00 X - NN.0  BlitzNetN00 X X NN.N NN.N  BlitzNetN00 X NN.N Table N: The effect of extra data with bounding box annotations on segmentation performance
The networks  were trained on VOCNN trainval (aug) + VOC0N tainval
 Detection performance is measured in average precision  (%) and mean IoU is the metric for segmentation segmentation(%)
 network seg det mIoU mAP  BlitzNetNNN X - NN.N  BlitzNetNNN X X NN.N NN.N  BlitzNetNNN X NN.N Table N: The effect of joint training tested on COCO  minivalN0NN
The networks were trained on COCO train
 method minivalN0NN test-devN0NN  int 0.N 0.NN int 0.N 0.NN  BlitzNetN00 NN.N NN.N NN.N NN.N NN.N NN.N  BlitzNetNNN NN.N NN.N NN.N NN.N NN.N NN.N  Table N: Detection performance of BlitzNet on the  COCO dataset, with minivalN0NN and test-devN0NN splits  The networks were trained on COCO trainval dataset
Detection performance is measured in average precision (%)  with different criteria, namely, minimum Jaccard overlap  between annotated and predicted bounding box is 0.N, 0.NN  or integrated from 0.N to 0.NN % (column “int”)
 NNNN    network backbone mAP % FPS # proposals input resolution  Faster-RCNN[NN] VGG-NN NN.N N - ∼ N000× N00  R-FCN[NN] ResNet-N0N N0.N N - ∼ N000× N00  SSDN00*[NN] VGG-NN NN.N NN NNNN N00× N00  SSDNNN*[NN] VGG-NN N0.N NN NNNNN NNN× NNN  YOLO [NN] YOLO net NN.N NN - BlitzNetN00 (sN) ResNet-N0 NN.N NN NNNN0 N00× N00  BlitzNetNNN (sN) ResNet-N0 NN.N NN.N NNNNN NNN× NNN  Table N: Comparison of inference time on PASCAL VOC N00N, when running on a Titan X (Maxwell) GPU
 Block type mAP mIoU  Hourglass-style [NN] NN.N NN.N  Refine-style [NN] NN.0 NN.N  ResSkip (no res) NN.N NN.N  ResSkip (ours) NN.N NN.N  Table N: The effect of fusion block type on performance,  measured on detection (VOC0N-test) and segmentation  (VOCNN-val) The networks were trained on VOCNN-train  (aug) + VOC0N tainval, see Sec
N.N
Detection performance  is measured in average precision (%) and mean IoU is the  metric for segmentation segmentation(%)
 detection results on COCO test-devN0NN in Table N
Our  results are also publicly available on the COCO evaluation  test server
 N.N
Inference Speed Comparison  In Table N and Figure N, we report speed comparison to  other state-of-the-art detection pipelines
Our approach is  the most accurate among the real time detectors working NN  frames per second (FPS) and in the setting close to real time  (NN FPS), it provides the most accurate detections among  the counterparts, while also providing semantic segmentation mask
Note that all methods are run using the same  GPU (Titan X, Maxwell architecture)
 N.N
Study of the Network Architecture  The BlitzNet pipeline simultaneously operates with several types of data
To demonstrate the effectiveness of the  ResSkip block, we set up the following experiment: we  leave the pipeline unchanged while only substituting this  block with another one
We consider in particular fusion  blocks that appear in the state-of-the-art approaches on semantic segmentation
[NN] [NN] [NN]
Table N shows that  our ResSkip block performs similar or better (on average)  than all counterparts, which may be due to the fact that its  design uses similar skip-connections as the Backbone network ResNetN0, making the overall architecture more homogeneous
 Optimal parameters for the size of intermediate representations in segmentation stream (NN) as well as the number of  channels in the upscale-stream (NNN) where found by using  a validation set
We did not conduct experiments by changing the number of layers in the upscale-stream as long as  our architecture is designed to be symmetric with respect  to the convolutions and the deconvolutions steps
Reducing the number of the steps will result in a smaller number  of layers in the upscale stream, which may deteriorate the  performance as noted in [NN]
 0 N0 N0 N0 N0 N0 speed, FPS  NN.N  NN.0  NN.N  N0.0  NN.N  NN.0  NN.N  N0.0  NN.N  m AP   % Faster-RCNN  R-FCN  SSDN00  SSDNNN  YOLO  BlitzNetN00  BlitzNetN00  Fig
N: Speed comparison with other methods
The detection accuracy of different methods measured in mAP is  depicted on y-axis
x-coordinate is their speed, in FPS
 N
Conclusion  In this paper, we introduce a joint approach for object detection and semantic segmentation
By using a single fullyconvolutional network to solve both problems at the same  time, learning is facilitated by weight sharing between the  two tasks, and inference is performed in real time
Moreover, we show that our pipeline is competitive in terms of  accuracy, and that the two tasks benefit from each other
 NNN0    Fig
N: Effect of extra data annotated for detection on the quality of estimated segmentation masks
The first column  displays test images; the second column contains its segmentation ground truth masks
The third column corresponds to  segmentations predicted by BlitzNetN00 trained on VOCNN train-segmentation augmented with extra segmentation masks  and VOC0N
The last row is segmentation masks produced by the same architecture but trained without VOC0N
 Acknowledgements
This work was supported by a grant  from ANR (MACARON, ANR-NN-CENN-000N-0N) and by  the ERC projects SOLARIS and ALLEGRO
We gratefully  acknowledge the Intel gift and the support of NVIDIA Corporation with the donation of GPUs used for this research
 NNNN    References  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected CRFs
In ICLR, N0NN
 [N] M
Everingham, L
Van Gool, C
K
Williams, J
Winn,  and A
Zisserman
The PASCAL visual object classes  (VOC) challenge
International Journal of Computer Vision,  NN(N):N0N–NNN, N0N0
 [N] S
Fidler, R
Mottaghi, A
Yuille, and R
Urtasun
Bottom-up  segmentation for top-down detection
In CVPR, N0NN
 [N] C.-Y
Fu, W
Liu, A
Ranga, A
Tyagi, and A
C
Berg
 DSSD: Deconvolutional single shot detector
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [N] R
Girshick
Fast R-CNN
In ICCV, N0NN
 [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
 [N] S
Gould, T
Gao, and D
Koller
Region-based segmentation  and object detection
In NIPS, N00N
 [N] B
Hariharan, P
Arbeláez, L
Bourdev, S
Maji, and J
Malik
 Semantic contours from inverse detectors
In ICCV, N0NN
 [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
 [N0] D
Kingma and J
Ba
Adam: A method for stochastic optimization
In ICLR, N0NN
 [NN] I
Kokkinos
UberNet: Training a universal convolutional  neural network for low-, mid-, and high-level vision using  diverse datasets and limited memory
In CVPR, N0NN
 [NN] Y
LeCun, B
Boser, J
S
Denker, D
Henderson, R
E
 Howard, W
Hubbard, and L
D
Jackel
Backpropagation  applied to handwritten zip code recognition
Neural computation, N(N):NNN–NNN, NNNN
 [NN] Y
Li, K
He, J
Sun, et al
R-FCN: Object detection via  region-based fully convolutional networks
In NIPS, N0NN
 [NN] T.-Y
Lin, P
Dollár, R
Girshick, K
He, B
Hariharan, and  S
Belongie
Feature pyramid networks for object detection
 In CVPR, N0NN
 [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft COCO: Common objects in context
In ECCV, N0NN
 [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.-Y
 Fu, and A
C
Berg
SSD: Single shot multibox detector
In  ECCV, N0NN
 [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
 [NN] R
Mottaghi, X
Chen, X
Liu, N.-G
Cho, S.-W
Lee, S
Fidler, R
Urtasun, and A
Yuille
The role of context for object  detection and semantic segmentation in the wild
In CVPR,  N0NN
 [NN] A
Newell, K
Yang, and J
Deng
Stacked hourglass networks for human pose estimation
In ECCV, N0NN
 [N0] H
Noh, S
Hong, and B
Han
Learning deconvolution network for semantic segmentation
In ICCV, N0NN
 [NN] G
Papandreou, L.-C
Chen, K
P
Murphy, and A
L
Yuille
 Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation
In ICCV,  N0NN
 [NN] P
O
Pinheiro, T.-Y
Lin, R
Collobert, and P
Dollár
Learning to refine object segments
In ECCV, N0NN
 [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In  CVPR, N0NN
 [NN] J
Redmon and A
Farhadi
YOLON000: better, faster,  stronger
arXiv preprint arXiv:NNNN.0NNNN, N0NN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN: Towards real-time object detection with region proposal networks
In NIPS, N0NN
 [NN] O
Ronneberger, P
Fischer, and T
Brox
U-net: Convolutional networks for biomedical image segmentation
In MICCAI, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 [NN] M
Teichmann, M
Weber, M
Zoellner, R
Cipolla, and  R
Urtasun
Multinet: Real-time joint semantic reasoning  for autonomous driving
arXiv preprint arXiv:NNNN.0NNNN,  N0NN
 [NN] J
Yao, S
Fidler, and R
Urtasun
Describing the scene as  a whole: Joint object detection, scene classification and semantic segmentation
In CVPR, N0NN
 NNNNAligned Image-Word Representations Improve Inductive Transfer Across Vision-Language Tasks   Aligned Image-Word Representations Improve Inductive Transfer Across  Vision-Language Tasks  Tanmay GuptaN Kevin ShihN Saurabh SinghN Derek HoiemN  NUniversity of Illinois, Urbana-Champaign NGoogle Inc
 {tguptaN, kjshihN, dhoiem}@illinois.edu saurabhsingh@google.com  Abstract  An important goal of computer vision is to build systems  that learn visual representations over time that can be applied to many tasks
In this paper, we investigate a visionlanguage embedding as a core representation and show that  it leads to better cross-task transfer than standard multitask learning
In particular, the task of visual recognition  is aligned to the task of visual question answering by forcing each to use the same word-region embeddings
We show  this leads to greater inductive transfer from recognition to  VQA than standard multitask learning
Visual recognition  also improves, especially for categories that have relatively  few recognition training labels but appear often in the VQA  setting
Thus, our paper takes a small step towards creating more general vision systems by showing the benefit of  interpretable, flexible, and trainable core representations
 N
Introduction  Consider designing a vision system that solves many  tasks
Ideally, any such system should be able to reuse  representations for different applications
As the system  is trained to solve more problems, its core representations  should become more complete and accurate, facilitating the  learning of additional tasks
Vision research often focuses  on designing good representations for a given task, but what  are good core representations to facilitate learning the next?  The application of knowledge learned while solving one  task to solve another task is known as transfer learning or  inductive transfer
Inductive transfer has been demonstrated  in recent vision-language tasks in [NN, NN, NN, NN, NN, NN],  where the hidden or output layers of deep networks learned  from pre-training (e.g
on ImageNet [NN]) or multitask  learning serve as the foundation for learning new tasks
 However, the relations of features to each new task needs  to be re-learned using the new task’s data
The goal of our  work is to transfer knowledge between related tasks without  the need to re-learn this mapping
Further, as we are working with vision-language tasks, we aim to transfer knowledge of both vision and language across tasks
 Mountain  Person  Snow  Jacket  Red  White  Rock  Shadow  Image region   representations  Word representations  Mountain Snow  White  Rock  Snow  White  Person  Jacket  Red  White  Shadow  Visual Recognition Task  Q: What color is the person’s jacket? A: Red  Q: Why is the ground white?  A: Snow  Visual Question Answering Task  Shared Vision-Language Representation Space  In fe re n ce  Le a rn in g In  fe re n ce  Le a rn in g  Figure N: Sharing image-region and word representations  across multiple vision-language domains: The SVLR module  projects images and words into a shared representation space
The  resulting visual and textual embeddings are then used for tasks like  Visual Recognition and VQA
The models for individual tasks are  formulated in terms of inner products of region and word representations enforcing an alignment between them in the shared space
 In this work we propose a Shared Vision-Language Representation (SVLR) module that improves inductive transfer  between related vision-language tasks (see Fig
N)
We apply our approach to visual recognition (VR) and attentionbased visual question answering (VQA)
We formulate VR  in terms of a joint embedding of textual and visual representations computed by the SVLR module
Each region is  mapped closest to its correct (textual) class label
For example, the embedding of “dog” should be closer to an embedded region showing a dog than any other object label
 We formulate VQA as predicting an answer from a relevant  region, where relevance and answer scores are computed  from embedded word-region similarities
For example, a  region will be considered relevant to “Is the elephant wearing a pink blanket?” if the embedded “pink” and either  “elephant” or “blanket” are close to the embedded region
 Similarly, the answer score considers embedded similarities, but in a more comprehensive manner
We emphasize  that the same word-region embedding is learned for both  VR and VQA
Our experiments show that formulating both  tasks in terms of the SVLR module leads to better cross-task  transfer than if features are shared through multitask learnNNNNN    ing but without exploiting the alignment between words and  regions
 In summary, our main contribution is to show that the  proposed SVLR module leads to better inductive transfer  than unaligned feature sharing through multitask learning
 As an added benefit, attention in our VQA model is highly  interpretable: we can show what words cause the system  to score a particular region as relevant
We take a small  step towards lifelong-learning vision systems by showing  the benefit of an interpretable, flexible, and trainable core  representation
 N
Related Work  Never-ending learning: NEL [NN, N, NN, N0, NN] aims to  continuously learn from multiple tasks such that learning  to solve newer problems becomes easier
Representation  learning [N], multitask learning [N0], and curriculum learning [NN] are different aspects of this larger paradigm
Inductive transfer through shared representations is a necessary  first step for NEL
Most works focus on building transferable representations within a single modality such as language or vision only
We extend this framework to learn a  joint vision-language representation which enables a much  larger class of new vision-language tasks to easily build on  and contribute to the shared representation
 VR using Vision-language embeddings: Traditionally, visual recognition has been posed as multiclass classification  over discrete labels [NN, NN, NN]
Using these recognizers for tasks like VQA and image captioning is challenging because of the open-vocabulary nature of these problems
However, availability of continuous word embeddings (e.g
wordNvec [N0]) has allowed reformulation of  visual recognition as a nearest neighbor search in a learned  image-language embedding space [NN]
Such embeddings  have been successfully applied to a variety of tasks that require recognition such as image captioning [NN, NN], phrase  localization [NN, NN], referring expressions [NN, NN], and  VQA [N, NN, N0]
 Our recognition model is related to previous openvocabulary recognition/localization models [NN, NN, NN],  which learn to map visual CNN features to continuous  word vector representations
However, we specifically focus on the multitask setting where VR forms a part of a  higher-level vision-language task such as VQA
Since the  SVLR module is reused in both tasks with inner products  in the embedding space forming the basis for both models, during joint training VQA provides a weak supervision for recognition as well
Fang et al
[NN] also learn  object and attribute classifiers from weak supervision in  the form of image-caption pairs using a multiple instance  learning (MIL) framework, but do not use a vision-language  embedding
Liu et al
[NN] similarly use VR annotation  from FlickrN0K entities [NN] to co-supervise attention in a  caption-generation model on the same dataset
Our work  goes further by allowing the supervision to come from separate datasets, thereby increasing the amount of training data  available for the shared parameters
Additionally, we look  at how each task has benefited from jointly training with the  other
 VQA: Visual Question Answering (VQA) involves responding to a natural language query about an image
Our  VQA model is closely related to attention-based VQA models [NN, NN, NN, NN, NN, NN, N, N, NN, NN] which attempt to  compute a distribution (region relevance or attention) over  the regions/pixels in an image using inner product of imageregion and the full query embedding [NN, NN, NN, NN]
Region relevance is used as a weight to pool relevant visual  information which is usually combined with the language  representation to create a multimodal representation
Various methods of pooling such as elementwise-addition, multiplication, and outer-products have been explored [NN, NN]
 Attention models are themselves an active area of research with applications in visual recognition [NN, NN], object localization, caption generation [NN], question answering [NN, NN, NN], machine comprehension [NN] and translation [N, NN], and neural turing machines [N0]
 Our model explicitly formulates attention in VQA as image localization of nouns and adjectives mentioned in a candidate QA pair
Ilievski et al
[NN] use a related approach  for attention
They use wordNvec to map individual words  in the question to the class labels of a pre-trained object detector which then generates the attention map by identifying  regions for those labels
Tommasi et al
[NN] similarly use a  pre-trainined CCA [NN] vision-language embedding model  to localize noun phrases, then extracts scene, attribute, and  object features to answer VQA questions
Our model differs from these methods in two ways: (i) vision-language  embeddings for VR allow for end-to-end trainability, and  (ii) jointly training on VR provides additional supervision  of attention through a different (non-VQA) dataset
 Andreas et al
[N, N] rely heavily on the syntactic parse  to dynamically arrange a set of parametrized neural modules
Each module performs a specific function such as localizing a specific word or verifying relative locations
In  contrast, our approach uses a static model but relies on language parse to make it interpretable and modular
 N
Method  We propose an SVLR module to facilitate greater inductive transfer across vision-language tasks
As shown in  Fig
N, the word and region representations required for object recognition, attribute recognition, and VQA are computed through the SVLR module
By specifically formulating each task in terms of inner products of word and region  representations and training on all tasks jointly, we ensure  each task provides a consistent, non-conflicting training sigNNNN    �":  �":	‘White’,	‘Fluffy’  �$:  �$:	‘Dog’  �%:  �:	Is	this	a	fluffy	dog?  �:	No  �): �*:  �+:  �"(�"; �0NNN)  �(�; �0NNN)	∀	� ∈ �  �:	N000	object	categories  �$ �$, � = �$ �$; �0NNN ?�(�; �0NNN)  �:	N000	attribute	categories  Attribute	Scoring  �" �", � = �" �"; �0NNN ?�(�; �0NNN)  �, �  Object	Recognition	  Loss  ℒ$AB(�0NNN , �$, �$)  Attribute	Recognition	  Loss  ℒ"CN(�0NNN , �" , �")  Visual	QA	  Loss  ℒN%"(�0NNN , �N%" , �% , �, �)  +  �$(�$; �0NNN)  �(�; �0NNN)	∀	� ∈ �  Image	Object	Embedding  Class	Label	Word	Embeddings  Image	Attribute	Embedding  Class	Label	Word	Embeddings  +  �$(�D; �0NNN)∀	� = N,N,⋯ , �  �(�; �0NNN)	∀	� ∈ � ∪ �  �" �D; �0NNN ∀	� = N,N,⋯ , �  Region	Object	Embedding  QA	Words	Embeddings  Region	Attribute	Embedding  Attention  Answer	Scoring  �N%"  VQA	Model  O b je ct 	R e co g n it io n 	  T ra in in g 	o n 	G e n o m e  A tt ri b u te 	R e co g n it io n 	  T ra in in g 	o n 	G e n o m e  V is u a l	 Q A  T ra in in g 	o n 	V Q A  Training	Loss  �(�; �0NNN)	∀	� ∈ � ∪ �  Class	Label	Word	Embeddings  ⋯  Edge	Boxes	  Region	Proposals  S V LR 	M  o d u le  � 0 N NN  Generating	Representations Task	Specific	Models  Object	Scoring  � � �$ ∝ exp	(�$(�$, �))	  ∀	� ∈ �  Softmax  � � �" ∝ exp(�"(�", �)))  	∀	� ∈ �  Softmax  Figure N: Joint Training on Visual Recognition(VR) and Visual Question Answering(VQA) with the proposed SVLR Module: The  figure depicts sharing of image and word representations through the SVLR module during joint training on object recognition, attribute  recognition, and VQA
The recognition tasks use object and attribute labelled regions from Visual Genome while VQA uses images  annotated with questions and answers from the VQA dataset
The benefit of joint training is that while the VQA dataset does not provide  region groundings of nouns and adjectives in the QA (e.g
“fluffy”,“dog”), this complementary supervision is provided by the Genome  recognition dataset
Models for each task involve image and word embeddings produced by SVLR module or their inner products (See  Fig N for VQA model architecture)
 nal for aligning words and region representations
During training, the joint-task model is fed batches containing  training examples from each task’s dataset
 N.N
Shared Vision Language Representation  The SVLR module converts words and image-regions  into feature representations that are aligned to each other  and shared across tasks
 Word Representations: The representation g(w) for a word w is constructed by applying two fully connected layers (with N00 output units each) to pretrained wordNvec  representation [NN] of w with ReLU after the first layer
 Region Representations: A region R is represented using two N00 dimensional feature vectors fo(R) and fa(R) that separately encode the objects and attributes contained
We  used two representations instead of one to encourage disentangling of these two factors of variation
For example, we  do not expect “red” to be similar to “apple”, but we expect  fo(R) and fa(R) to be similar to g(“red”) and g(“apple”) if R depicts a red apple
The features are constructed by ex- tracting the average pooled features from Resnet [NN] pretrained on ImageNet and then passing through separate object and attribute networks
Both networks consist of two  fully connected layers (with N0NN and N00 output units) with  batch normalization [NN] and ReLU activations
 N.N
Visual Recognition using SVLR  N.N.N Inference  The visual recognition task is to classify image regions  into one or more object and attribute categories
The  classification score for region R and object category w is fTo (R)g(w)
The classification score for an attribute cate- gory v is fTa (R)g(v)
Attributes may include adjectives and adverbs (e.g., “standing”)
Though our recognition dataset  has a limited set of object categories O and attribute cate- gories T , our model can produce classification scores for any object or attribute label given its wordNvec representation
In experiments, the O and T consist of N000 most fre- quent object and attribute categories in the Visual Genome  dataset [NN]
 N.N.N Training  Our VR model is trained using the Visual Genome dataset  which provides image regions annotated with object and  attribute labels
VR uses only the parameters for the  embedding functions fo, fa and g that are part of the SVLR module
The parameters of fo receive gradients from the object loss while those of fa receive gradients from the attribute loss
The parameters of word embedding model g receive gradients from both losses
 NNNN    Object loss: We use a multi-label loss as object classes may  not be mutually exclusive (e.g., “man” is a “person”)
For a  region Rj , we denote the set of annotated object categories and their hypernyms extracted from WordNet [NN] by Hj 
The object loss forces the true labels and their hypernyms  to score higher than all other object labels by a margin ηobj 
For a batch of M samples {(Rj ,Hj)}  M j=N the object loss is:  Lobj = N  M  M ∑  j=N  N  |Hj|  ∑  l∈Hj  N  |O|  ∑  k∈O\Hj  max{0, ηobj + f T o (Rj)g(k)− f  T o (Rj)g(l)} (N)  Attribute Loss: The attribute loss is a multi-label classification loss with two differences from object classification
 Attribute labels are even less likely to be mutually exclusive than object labels
As such, we predict each attribute  with independent cross entropy losses
We also weigh the  samples based on fraction of positive labels in the batch to  balance the positive and negative labels in the dataset
For a  batch with M samples {(Rj , Tj)} M j=N where Tj is the set of  attributes annotated for region Rj , the attribute loss is:  Latr = N  M  M ∑  j=N  ∑  t∈T  ✶ [t ∈ Tj ] (N− Γ(t)) log [  σ(fTa (Rj)g(t)) ]  +  ✶ [t /∈ Tj ] Γ(t) log [  N− σ(fTa (Rj)g(t)) ]  (N)  where σ is a sigmoid activation function and Γ(t) is the frac- tion of positive samples for attribute t in the batch
 N.N
Visual Question Answering using SVLR  Our VQA model is illustrated in Fig
N
The input to  our VQA model is an image, a question, and a candidate answer
Regions are extracted from the image using  Edge Boxes [NN]
The same SVLR module used by VR  (Sec
N.N) is explicitly applied to VQA for attention and answer scoring
Our system assigns attention scores to each  region according to how well it matches words in the question/answer, then scores each answer based on the question,  answer, and attention-weighted scores for all objects (O) and attributes (T )
 Attention Scoring: Unlike other attention models [NN, NN]  that are free to learn any correlation between regions and  question/answers, our attention model encodes an explicit  notion of vision-language grounding
Let R be the set of region proposals extracted from the image, and N and J denote the set of nouns and adjectives in the (Q,A) pair
Each region R ∈ R(I) is assigned an attention score a(R) as follows:  a′(R) = max n∈N  fTo (R)g(n) + max j∈J  fTa (R)g(j) (N)  a(R) = exp(a′(R))  ∑  R′∈R(I) exp(a ′(R′))  (N)  Thus, a region’s attention score is the sum of maximum  adjective and noun scores for words mentioned in the question or answer (which need not be in sets O and T )
 Image Representation: To score an answer, the content of  region R is encoded using the VR scores for all objects and attributes in O and T , as presence of unmentioned objects or attributes may help answer the question
The image representation is an attention-weighted average of these scores  across all regions:  f(I) = ∑  R∈R(I)  a(R)  [  so(R) sa(R)  ]  (N)  where I is the image, so(R) are the scores for N000 objects in O for each image region R, sa(R) are the scores for N000 attributes in T , and a(R) is the attention score
 Question/Answer Representation: To construct representations q(Q) and a(A) for the question and answer, we follow Shih et al
[NN], dividing question words into  N bins, averaging word representations in each bin, and  concatenating the bin representations resulting in a NN00  (= N00 × N) dimensional vector q(Q)
The answer rep- resentation a(A) ∈ RN00 is obtained by averaging the word representations of all answer words
The word representations used here are produced by the SVLR module
 Answer Scoring: We combine the image and Q/A representations to jointly score the (Q, I,A) triplet
To ensure equal contribution of language and visual features, we apply batch normalization [NN] on linear transformations of these features before adding them together to get  a bimodal representation β(Q, I,A) ∈ RNN00:  β(Q, I,A) = BN(WNf(I)) + BN  (  WN  [  q(Q) a(A)  ])  (N)  Here, BN,BN denote batch normalization and WN ∈ R  NN00×N000 and WN ∈ R NN00×NN00 define the linear transformations
The bimodal representation is:  S(Q, I,A) = WN ReLU(β(Q, I,A)) (N)  with WN ∈ R N×NN00
 Training: We use the VQA dataset [N] for training parameters of our VQA model: WN,WN,WN, and scales and off- sets of batch normalization layers
In addition, the VQA  loss backpropagates into fo, fa, and g which are part of the SVLR module
Each sample in the dataset consists of a  question Q about an image I with list of answer options including a positive answer A+ and N negative answers {A−(i)|i = N, · · · , N}
 The VQA loss encourages the correct answer A+ to be scored higher than all incorrect answer options {A−(i)|i = N, · · · , N} by a margin ηans
Given batch samples {(Qj , Ij , Aj)}  P j=N, the loss is written as  NNNN    Q: What color is the skier’s jacket?  A: Red  Im ag  e  R  ep re  se n  ta ti  o n  �( �)  Q A   R ep  re se  n ta  ti o  n  [� � ,�  � ]  F C  N F  C N  B at  ch n  o rm  B at  ch n  o rm  + FC NReLU  S co  re    �( �, �,�  )  Bimodal Pooling  Region Proposals  �N  �N  �N  �N �N �N�N  �N �N �N  C at  D o g  M o u n ta  in  P er  so n  … R ed  R o u g h  W h it  e  C u te  …  C o lo  r  S k ie  r  Ja ck  et  max  R ed  Region   Relevance  Object   Categories (�) Attribute   Categories (�)  Relevant Visual Content Extraction  Region Relevance/Attention Prediction  Attention weighted   feature pooling  Nominal Subject: jacket | First N question words: what color | Other Nouns: color skier | Other words: is the  Answer words: red  softmax  �(�N) �(�N) �(�N) �(�N) �(�N)  QA Nouns   (�) QA Adjectives   (�)  I:  Matrix  Multiplication  :� �; �(�;) N  ;=N  �(�N) �(�N) �(�N) �(�N) �(�N)  Figure N: Inference in our VQA model: The image is first broken down into Edge Box region proposals[NN]
Each region R is represented  by visual category scores s(R) = [so(R), sa(R)] obtained using the visual recognition model
Using the SVLR module, the regions are also assigned an attention score using the inner products of region features with representations of nouns and adjectives in the question  and answer
The region features are then pooled using the relevance scores as weights to construct the attended image representation
 Finally, the image and question/answer representations are combined and passed through a neural network to produce a score for the input  question-image-answer triplet
 Lans = N  NP  P ∑  j=N  N ∑  i=N  max{0,  ηans + S(Qj , Ij , A − j (i))− S(Qj , Ij , A  + j )} (N)  N.N
Zero-Shot VQA  The representations produced by SVLR module should  be directly usable in related vision-language tasks without any additional learning
To demonstrate this zeroshot cross-task transfer, we train the SVLR module using Genome VR data only and apply to VQA
Since bimodal pooling and scoring layers cannot be learned without  VQA data, we use a proxy scoring function constructed using region-word scores only
For each region, we compute  pq(R) as the sum of its scores for the maximally aligned question nouns and question adjectives (Eq
N with only  question words)
A score pa(R) is similarly computed us- ing answer nouns and adjectives
The final score for the  answer is defined by  S(Q, I,A) = ∑  R∈R  a(R)min(pq(R), pa(R)) (N)  where a is the attention score computed using Eq
N
There- fore, the highest score is given to QA pairs where question  as well as answer nouns and adjectives can be localized in  the image
Note that the since the model is not trained on  even a single question from VQA, the zero-shot VQA task  also shows that our model does use the image to answer  questions instead of solely relying on the language prior  which is a common concern with most VQA models [N, NN]
 N
Implementation and Training Details  We use N00 region proposals resized to NNN× NNN for all experiments
Resnet-N0 was used for image feature extraction in all experiments except those in Tab
N which used  Resnet-NNN
The nouns and adjectives are extracted from  the (Q,A) and lemmatized using the part-of-speech tagger and WordNet lemmatizer in NLTK [N]
We use the Stanford Dependency Parser [NN] to parse the question into bins  as detailed in [NN]
All models are implemented and trained  using TensorFlow [N]
We train the model jointly for the  recognition and VQA tasks by minimizing the following  loss function using Adam [N0]:  L = αansLans + αobjLobj + αatrLatr (N0)  We observe that values of αobj and αatr relative to αans can be used to trade-off performance between visual recognition and VQA tasks
For experiments that analyze the  effect of transfer from VR to VQA (Sec
N.N), we set  αans = N, αobj = 0.N, and αatr = 0.N
For VQA only and Genome only baselines, we set the corresponding α to N and others to 0
For experiments dealing with transfer in the  other direction (Sec
N.N), we set αans = 0.N, αobj = N, and αatr = N
The margins used for object and answer losses are ηans = ηobj = N
The object and attribute losses are computed for the same set of Visual Genome regions with  a batch size of M = N00
The answer loss is computed for a batch size of P = N0 questions sampled from VQA
We use an exponentially decaying learning rate schedule with  an initial learning rate of N0−N and decay rate of 0.N every  NNNN    Figure N: Interpretable inference in VQA: Our model produces interpretable intermediate computation for region relevance and object/attribute predictions for the most relevant regions
Our region relevance explicitly grounds nouns and adjectives from the Q/A input  in the image
We also show object and attribute predictions for the most relevant region identified for a few correctly answered questions
 The relevance masks are generated from relevance scores projected back to their source pixels locations
 NN000 iterations
Weight decay is used on all trainable variables with a coefficient of N0−N
All the variables are Xavier initialized [NN]
 N
Experiments  Our experiments investigate the extent to which using  SVLR as a core representation improves transfer in multitask learning
We first analyze how including the VR  task improves VQA (Sec
N.N, Tab
N)
We find that using  SVLR doubles the improvement compared to standard multitask learning, and demonstrate performance well above  chance in a zero-shot setup (trained only on VR, applied  to VQA)
We then analyze improvement to VR due to training with (weakly supervised) VQA (Sec
N.N, Fig
N)
We  find moderate overall improvements (N.N%), with the largest improvements for classes that have few VR training examples
We also quantitatively evaluate how well our attention maps correlate with that of humans using data provided  by [NN] in Table N
We include results of our VQA system  trained with ResNet-NNN architecture on val, test-dev, teststd, along with state-of-the-art (Tab
N)
 N.N
Datasets  Our model is trained on two separate datasets: one for  VQA supervision, one for visual recognition (attributes and  object classification)
We use the image-question-answer  annotation triplets from Antol et al
[N] and bounding box  annotations for object and attribute categories from Visual  Genome [NN]
The train-val-test splits for the datasets are as  follows
 VQA: We split the train set into train-subset and trainheld-out and use the latter for model selection
The trainsubset consists of NNN,NNN (Q, I,A) samples whereas train- held-out contains NN,0NN samples
The val and test set contain NNN,NNN and NNN,N0N samples respectively
There are  exactly N questions per image
We use VQA val for evaluating on specific question types
 Visual Genome: We use only images from Visual Genome  not in VQA (overlaps identified using mdN hashes)
The selected images were divided into train-val-test using an NNN-N0 split, yielding N,NNN,NN0, N0,NNN and NNN,NNN annotated regions in each
We use val for selecting the model for  evaluating recognition performance
 N.N
Inductive Transfer from VR to VQA  In Table N, we analyze the role of SVLR module for inductive transfer in both joint training and zero-shot settings
 Joint Training: During joint training, the VR models and  VQA model are simultaneously trained using object and  attribute annotations from Genome, and Q/A annotations  from the VQA dataset
The common approach to joint training is to use a common network for extracting image features (e.g
class logits from ResNet), which feeds into the  task-specific networks as input
We refer to this approach  in Table N as Joint Multitask
This baseline is implemented  by replacing g(y) (see Fig
N), with a fixed set of vectors hy for each of the predetermined N000 object and N000 at- tribute categories in the VR models
The embedding g(y) is still in the VQA model, but is no longer shared across tasks
 Our proposed Joint SVLR outperforms VQA-only by N.N%, doubling the N.N% improvement achieved by Joint Multi- task
Our formulation of VR and VQA tasks in terms of  shared word-region representations more effectively transNNNN    Accuracies on Real-MCQ-VQA Validation Set w h  a t  co lo  r  w h  a t  is th  e  (w o )m  a n  /p er  so n  w h  a t  is in  /o n  w h  a t  k in  d /  ty p  e/ a n  im a l  w h  a t  ro o m  /s p  o rt  ca n /c  o u ld  /  d o es  /d o /h  as  w h at  d o es  /  n u m  b er  /n am  e  w h at  b ra  n d  w h ic  h /w  h o  w h at  is /a  re  w h y /h  o w  h o w  m an  y  w h at  ti m  e  w h er  e  is /a  re /w  as  n o n e  o f  th e  ab o v e  o th  er  n u m  b er  y es  /n o  o v er  al l  ac cu  ra cy  VQA Only NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N N0.N NN.0 NN.N NN.N NN.N NN.N  Joint Multitask NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N NN.N NN.N NN.N NN.N  Joint SVLR NN.N NN.N NN.N N0.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N  Zero-Shot VQA NN.N NN.0 NN.N NN.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.N 0.N N.N NN.N NN.N NN.N NN.N N.N NN.N NN.N  Table N: Inductive transfer from VR to VQA through SVLR in joint training and zero-shot settings: We evaluate the performance of  our model with SVLR module trained jointly with VR and VQA supervision (provided by Genome and VQA datasets respectively) on the  VQA task
We compare this jointly-trained model to a model trained on only VQA data
We also compare to a traditional multitask learning  setup that is jointly trained on VQA and VR (i.e
uses same amount of data as Joint SVLR) and shares visual features but does not use the  object and attribute word embeddings for recognition
While multitask learning outperforms VQA-only model, using the SVLR module  doubles the improvement
Our model is most suited for the question types in bold that require visual recognition without specialized skills  like counting or reading
Formulation of VR and attention in VQA in terms of inner products between word and region representations  enables Zero-Shot VQA
In this setting we train on Genome VR data and apply to VQA val (Sec N.N)
 fers recognition knowledge from VR than shared features
 The gain is often larger on questions that involve recognition (in bold in Table N)
For example, what color questions  improve by N.N% due to SVLR
 Surprisingly, pre-training the visual classifiers on  Genome prior to joint training performs worse than the  model trained jointly from scratch: NN.N% versus NN.N%
 Zero-Shot VQA: We evaluate Zero-shot VQA to further highlight transfer from VR to VQA
We train on only  Genome VR annotations but test on VQA val
The model  has not seen any Q/A training data, but achieves an overall  accuracy of NN.N% where random guessing yields N.N% (NN choices)
Our zero-shot system does not exploit language  priors, which alone can score as high as NN.0% [NN]
This shows that some knowledge can be directly applied to related tasks using SVLR without additional training
 N.N
Inductive Transfer from VQA to VR  We compare the performance of our SVLR based model  trained jointly on VQA and VR data with a model trained  only on Genome data to analyze transfer from VQA to VR
 Genome test is used for evaluation
We observe an increase  in the overall object recognition accuracy from NN.N% to  NN.N%, whereas average attribute accuracy remained unchanged at NN.N%
In Fig
N, we show that nouns that are  rare in Genome (left columns) but have N0 or more examples in VQA (upper rows) benefit the most from weak supervision provided by VQA
On average, we measure improvement from NN% to NN% for the N classes that have fewer than NNN examples in Genome train but occur more  than NN0 times in VQA questions
We conducted the same  analysis on Genome attributes, but did not observe any notable pattern, possibly due to the inherent difficult in evaluating the multi-label attribute classification problem (the  absence of attributes is not annotated in Genome)
 0 − NNN  NNN −  NN0  NN0 −  N00  N00 −  NK  NK −  NK  NK −  NK  NK −  NK NK +  0 −  N 0  N 0  − N 0  N 0  − N 0  N 0  − N N 0  N N 0 +  NN.NN  + 0.NN  (NN)  NN.NN  – 0.NN  (NNN)  NN.0N  + N.NN  (N0)  NN.N  + N.N  (NN)  NN.NN  – 0.NN  (NN)  NN.NN  + N.NN  (N)  NN.NN  + N.NN  (N)  NN.NN  – N.NN  (N)  N0.NN  + N.NN  (NN)  NN.0N  + N.NN  (NN)  NN.NN  + N.NN  (NN)  NN.NN  + N.NN  (NN)  NN.NN  + N.N  (NN)  NN.NN  – N.NN  (N)  N0.NN  – N.NN  (N)  NN.NN  + N.NN  (N)  NN.N  + N.NN  (N)  NN.0N  + N.0N  (NN)  NN.NN  + 0.NN  (NN)  NN.0N  – N.NN  (NN)  NN.NN  + 0.NN  (NN)  NN.0N  – 0.0N  (N)  NN.NN  – N.NN  (N)  NN.NN  + N.NN  (N)  N.NN  – N.NN  (N)  NN.NN  + N.NN  (NN)  NN.NN  + N.NN  (NN)  NN.NN  + N.NN  (NN)  N0.NN  + 0.NN  (N0)  NN.NN  – 0.NN  (NN)  NN.NN  – 0.0N  (NN)  NN.NN  – N.NN  (NN)  N0.NN  + NN.NN  (N)  NN.NN  – 0.NN  (NN)  NN.NN  – N.NN  (NN)  N0.NN  + N.NN  (NN)  NN.NN  + 0.NN  (NN)  NN.NN  – 0.NN  (NN)  NN.NN  + N.NN  (NN)  NN.NN  + 0.NN  (NN)  Occurences in Genome Train  O cc u re n ce s in  V Q A  T ra in  Change in Top-N0 Object Accuracy on Genome Test (%)  −N0  −N  0  N  N0  Change in Accuracy  Figure N: Transfer from VQA to Object Recognition: Each  cell’s color reflects the mean change in accuracy for classes within  the corresponding frequency ranges of both datasets’ training split
 Most gains are in nouns rare in Genome but common in VQA (top  left), suggesting that the weak supervision provided by training  VQA attention augments recognition performance via the SVLR
 The numbers in each cell show the Genome-only mean accuracy  +/- the change due to SVLR multitask training, followed by the  number of classes in the cell in parentheses
 N.N
Interpretable Inference for VQA  As shown in Fig
N, our VQA model produces interpretable intermediate outputs such as region relevance and  visual category predictions, similar to [NN]
The answer  choice is explained by the object and attribute predictions  associated with the most relevant regions
Because relevance is posed as the explicit localization of words in the  question and answer, we can qualitatively evaluate the relevance prediction by verifying that the predicted regions  match said words
This also provides greater insight into  the failure modes as shown in Fig
N
We also quantitatively evaluate our attention using collected human attentions from Das et al
[NN] in Table N
 NNNN    Figure N: Failure modes: Our model cannot count or read, though it will still identify the relevant regions
It is blind to relations and thus  fails to recognize that birds, while present, are not drinking water
The model may give a low score to the correct answer despite accurate  visual recognition
For instance, the model observes asphalt but predicts concrete, likely due to language bias
A clear example of an error  due to language bias is in the top-left image as it believes the lady is holding a baby rather than a dog, even though visual recognition  confirms evidence for dog
Finally, our model fails to answer questions that require complex reasoning comparing multiple regions
 HiCo[NN] SANN[NN] WTL[NN] SVLR Center Human  Corr
0.NN 0.NN 0.NN 0.NN 0.NN 0.NN  Table N: Human Attention Comparison: We compare our attention maps with human attentions collected by Das et al
[NN]
 Comparison was done by resizing attention maps to NN×NN and computing the rank correlation as in [NN]
We include a strong  baseline using a synthetic center-focused heatmap (also used by  [NN]), highlighting the center-bias in the data
Scores for HiCo and  SANN were recomputed using released data from [NN], and differ  slightly from originally reported
Our model leads to significantly  higher correlation with human annotations than existing models
 N.N
Learned Word Representations  In Table N, we compare the word representations of the  SVLR model to that of WordNVec [NN] by showing several  nearest neighbors from both embeddings
We observe a  shift from non-visual neighborhoods and meanings (monitor, control) to visual ones (monitor, keyboard)
Neighbors  were computed using cosine distance after mean centering
 N
Conclusion  Humans learn new skills by building upon existing  knowledge and experiences
We attempt to apply this behavior to AI models by demonstrating cross-task learning  for the class of vision-language problems using VQA and  VR
To enhance inductive transfer, we propose sharing core  vision and language representations across all tasks in a way  that exploits the word-region alignment
We plan to extend  our method to larger sets of vision-language tasks
 WTL[NN] FDA[NN] MLP[NN, NN] MCB[NN] HiCo[NN] Ours  val NN.N - NN.N - - NN.N  test-dev NN.N NN.0 NN.N NN.N NN.N NN.N test-std NN.N NN.N - - NN.N NN.N  Trained on train+val - train train+val train+val train  Table N: External Comparisons on VQA: We include external  comparisons, but note that internal comparisons are more controlled and informative
The MLP results use the implementation  from [NN]
For test accuracy, it is unclear whether FDA uses val  to train
The original MLP implementation [NN] using Resnet-N0N  yields NN.N and NN.N on test-dev and test-std respectively
MCB  reports only test-dev accuracy for the directly comparable model  (final without ensemble)
Note that the overall performance of our  model is slightly worse than MLP and MCB because only about  N0% of the VQA dataset benefits from visual attention
Our model achieves NN.N% on color questions using attention, outperforming WTL’s NN% and MLP’s NN.N%
 Word WordNVec SVLR  column newspaper, magazine, book, letter pillar, post, pole, tower, chimney counter curb, stem, foil, stop, dispenser shelf, stove, countertop, burner  horn piano, guitar, brass, pedal tail, harness, tag, paw meat chicken, lamb, food, uncooked rice, scrambled, piled, slice  monitor watch, control, checked, alert keyboard, computer, portable  Table N: Word Representations from SVLR vs WordNVec: We  compare nearest neighbors (cosine distance) for a set of words using wordNvec embeddings as well as SVLR
 N
Acknowledgements  This work is supported in part by NSF Awards NN-NNNNN  and N0-NNNNN and ONR MURI N0000NN-NN-N-N00N
 NNN0    References  [N] M
Abadi, A
Agarwal, P
Barham, E
Brevdo, Z
Chen,  C
Citro, G
S
Corrado, A
Davis, J
Dean, M
Devin, S
Ghemawat, I
Goodfellow, A
Harp, G
Irving, M
Isard, Y
Jia,  R
Jozefowicz, L
Kaiser, M
Kudlur, J
Levenberg, D
Mané,  R
Monga, S
Moore, D
Murray, C
Olah, M
Schuster,  J
Shlens, B
Steiner, I
Sutskever, K
Talwar, P
Tucker,  V
Vanhoucke, V
Vasudevan, F
Viégas, O
Vinyals, P
Warden, M
Wattenberg, M
Wicke, Y
Yu, and X
Zheng
TensorFlow: Large-scale machine learning on heterogeneous systems, N0NN
Software available from tensorflow.org
 [N] A
Agrawal, D
Batra, and D
Parikh
Analyzing the behavior of visual question answering models
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [N] J
Andreas, M
Rohrbach, T
Darrell, and D
Klein
Learning  to compose neural networks for question answering
arXiv  preprint arXiv:NN0N.0NN0N, N0NN
 [N] J
Andreas, M
Rohrbach, T
Darrell, and D
Klein
Neural  module networks
In Proceedings of the IEEE Conference  on Computer Vision and Pattern Recognition, pages NN–NN,  N0NN
 [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra,  C
Lawrence Zitnick, and D
Parikh
Vqa: Visual question  answering
In Proceedings of the IEEE International Conference on Computer Vision, pages NNNN–NNNN, N0NN
 [N] D
Bahdanau, K
Cho, and Y
Bengio
Neural machine  translation by jointly learning to align and translate
arXiv  preprint arXiv:NN0N.0NNN, N0NN
 [N] Y
Bengio, A
Courville, and P
Vincent
Representation  learning: A review and new perspectives
IEEE transactions  on pattern analysis and machine intelligence, NN(N):NNNN–  NNNN, N0NN
 [N] S
Bird, E
Klein, and E
Loper
Natural language processing  with Python
” O’Reilly Media, Inc.”, N00N
 [N] A
Carlson, J
Betteridge, B
Kisiel, B
Settles, E
R
Hruschka Jr, and T
M
Mitchell
Toward an architecture for  never-ending language learning
In AAAI, volume N, page N,  N0N0
 [N0] R
Caruana
Multitask learning
In Learning to learn, pages  NN–NNN
Springer, NNNN
 [NN] X
Chen, A
Shrivastava, and A
Gupta
Neil: Extracting  visual knowledge from web data
In Proceedings of the IEEE  International Conference on Computer Vision, pages NN0N–  NNNN, N0NN
 [NN] A
Das, H
Agrawal, C
L
Zitnick, D
Parikh, and D
Batra
 Human attention in visual question answering: Do humans  and deep networks look at the same regions? In Conference on Empirical Methods in Natural Language Processing,  N0NN
 [NN] M.-C
De Marneffe, B
MacCartney, C
D
Manning, et al
 Generating typed dependency parses from phrase structure  parses
In Proceedings of LREC, volume N, pages NNN–NNN,  N00N
 [NN] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
 In Computer Vision and Pattern Recognition, N00N
CVPR  N00N
IEEE Conference on, pages NNN–NNN
IEEE, N00N
 [NN] H
Fang, S
Gupta, F
Iandola, R
K
Srivastava, L
Deng,  P
Dollár, J
Gao, X
He, M
Mitchell, J
C
Platt, et al
From  captions to visual concepts and back
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
 [NN] A
Fukui, D
H
Park, D
Yang, A
Rohrbach, T
Darrell,  and M
Rohrbach
Multimodal compact bilinear pooling  for visual question answering and visual grounding
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
 [NN] X
Glorot and Y
Bengio
Understanding the difficulty of  training deep feedforward neural networks
In Aistats, volume N, pages NNN–NNN, N0N0
 [NN] Y
Gong, L
Wang, M
Hodosh, J
Hockenmaier, and  S
Lazebnik
Improving image-sentence embeddings using  large weakly annotated photo collections
In European Conference on Computer Vision, pages NNN–NNN
Springer International Publishing, N0NN
 [NN] Y
Goyal, T
Khot, D
Summers-Stay, D
Batra, and  D
Parikh
Making the v in vqa matter: Elevating the role  of image understanding in visual question answering
arXiv  preprint arXiv:NNNN.00NNN, N0NN
 [N0] A
Graves, G
Wayne, and I
Danihelka
Neural turing machines
arXiv preprint arXiv:NNN0.NN0N, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
arXiv preprint arXiv:NNNN.0NNNN,  N0NN
 [NN] K
M
Hermann, T
Kocisky, E
Grefenstette, L
Espeholt,  W
Kay, M
Suleyman, and P
Blunsom
Teaching machines  to read and comprehend
In Advances in Neural Information  Processing Systems, pages NNNN–NN0N, N0NN
 [NN] M
Hodosh, P
Young, and J
Hockenmaier
Framing image  description as a ranking task: Data, models and evaluation  metrics
Journal of Artificial Intelligence Research, NN:NNN–  NNN, N0NN
 [NN] I
Ilievski, S
Yan, and J
Feng
A focused dynamic attention model for visual question answering
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] A
Jabri, A
Joulin, and L
van der Maaten
Revisiting visual  question answering baselines
In European Conference on  Computer Vision, pages NNN–NNN
Springer, N0NN
 [NN] M
Jaderberg, K
Simonyan, A
Zisserman, et al
Spatial  transformer networks
In Advances in Neural Information  Processing Systems, pages N0NN–N0NN, N0NN
 [NN] J
Johnson, A
Karpathy, and L
Fei-Fei
Densecap: Fully  convolutional localization networks for dense captioning
 arXiv preprint arXiv:NNNN.0NNNN, N0NN
 [NN] S
Kazemzadeh, V
Ordonez, M
Matten, and T
L
Berg
 Referit game: Referring to objects in photographs of natural scenes
In EMNLP, N0NN
 [N0] D
Kingma and J
Ba
Adam: A method for stochastic optimization
arXiv preprint arXiv:NNNN.NNN0, N0NN
 [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, et al
 Visual genome: Connecting language and vision using  NNNN    crowdsourced dense image annotations
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in neural information processing systems, pages  N0NN–NN0N, N0NN
 [NN] A
Kumar, O
Irsoy, J
Su, J
Bradbury, R
English, B
Pierce,  P
Ondruska, I
Gulrajani, and R
Socher
Ask me anything:  Dynamic memory networks for natural language processing
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] Z
Li and D
Hoiem
Learning without forgetting
In European Conference on Computer Vision, pages NNN–NNN
 Springer, N0NN
 [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European Conference on Computer Vision, pages NN0–NNN
Springer, N0NN
 [NN] C
Liu, J
Mao, F
Sha, and A
Yuille
Attention correctness in  neural image captioning
arXiv preprint arXiv:NN0N.0NNNN,  N0NN
 [NN] J
Lu, J
Yang, D
Batra, and D
Parikh
Hierarchical  question-image co-attention for visual question answering
 arXiv preprint arXiv:NN0N.000NN, N0NN
 [NN] A
Mallya
simple-vqa: Code implementing VQA  MLP baseline from Revisiting Visual Question Answering Baselines
https://github.com/arunmallya/  simple-vqa, N0NN
[Online; accessed NN-Nov-N0NN]
 [NN] J
Mao, J
Huang, A
Toshev, O
Camburu, and K
Murphy
Generation and comprehension of unambiguous object  descriptions
In Computer Vision and Pattern Recognition,  N0NN
 [N0] T
Mikolov, K
Chen, G
Corrado, and J
Dean
Efficient  estimation of word representations in vector space
arXiv  preprint arXiv:NN0N.NNNN, N0NN
 [NN] T
Mikolov, I
Sutskever, K
Chen, G
S
Corrado, and  J
Dean
Distributed representations of words and phrases  and their compositionality
In Advances in neural information processing systems, N0NN
 [NN] G
A
Miller
Wordnet: a lexical database for english
Communications of the ACM, NN(NN):NN–NN, NNNN
 [NN] T
Mitchell
Never-ending learning
Technical report, DTIC  Document, N0N0
 [NN] V
Mnih, N
Heess, A
Graves, et al
Recurrent models of visual attention
In Advances in Neural Information Processing  Systems, pages NN0N–NNNN, N0NN
 [NN] A
Pentina, V
Sharmanska, and C
H
Lampert
Curriculum  learning of multiple tasks
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NNNN–NN00, N0NN
 [NN] B
A
Plummer, L
Wang, C
M
Cervantes, J
C
Caicedo,  J
Hockenmaier, and S
Lazebnik
FlickrN0k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models
In Proceedings of the IEEE International Conference on Computer Vision, pages NNNN–NNNN,  N0NN
 [NN] M
Ren, R
Kiros, and R
S
Zemel
Exploring models and  data for image question answering
In Proceedings of the  NNth International Conference on Neural Information Processing Systems, NIPS’NN, pages NNNN–NNNN, Cambridge,  MA, USA, N0NN
MIT Press
 [NN] A
Rohrbach, M
Rohrbach, R
Hu, T
Darrell, and  B
Schiele
Grounding of textual phrases in images by reconstruction
In European Conference on Computer Vision  (ECCV), N0NN
 [NN] K
J
Shih, S
Singh, and D
Hoiem
Where to look: Focus  regions for visual question answering
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, N0NN
 [N0] D
L
Silver, Q
Yang, and L
Li
Lifelong machine learning  systems: Beyond learning algorithms
In AAAI Spring Symposium: Lifelong Machine Learning, pages NN–NN
Citeseer,  N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
 [NN] S
Sukhbaatar, J
Weston, R
Fergus, et al
End-to-end memory networks
In Advances in neural information processing  systems, pages NNN0–NNNN, N0NN
 [NN] S
Thrun
Lifelong learning algorithms
In Learning to learn,  pages NNN–N0N
Springer, NNNN
 [NN] T
Tommasi, A
Mallya, B
Plummer, S
Lazebnik, A
C
 Berg, and T
L
Berg
Solving visual madlibs with multiple  cues
In Proceedings of the British Machine Vision Conference N0NN, N0NN
 [NN] L
Wang, Y
Li, and S
Lazebnik
Learning deep structurepreserving image-text embeddings
In Computer Vision and  Pattern Recognition, N0NN
 [NN] J
Weston, S
Chopra, and A
Bordes
Memory networks
 arXiv preprint arXiv:NNN0.NNNN, N0NN
 [NN] Y
Wu, M
Schuster, Z
Chen, Q
V
Le, M
Norouzi,  W
Macherey, M
Krikun, Y
Cao, Q
Gao, K
Macherey,  et al
Google’s neural machine translation system: Bridging the gap between human and machine translation
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
 [NN] H
Xu and K
Saenko
Ask, attend and answer: Exploring  question-guided spatial attention for visual question answering
In European Conference on Computer Vision (ECCV),  N0NN
 [NN] Z
Yang, X
He, J
Gao, L
Deng, and A
Smola
Stacked  attention networks for image question answering
arXiv  preprint arXiv:NNNN.0NNNN, N0NN
 [N0] L
Yu, E
Park, A
C
Berg, and T
L
Berg
Visual madlibs:  Fill in the blank description generation and question answering
In Proceedings of the IEEE International Conference on  Computer Vision, pages NNNN–NNNN, N0NN
 [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object  proposals from edges
In European Conference on Computer  Vision, pages NNN–N0N
Springer, N0NN
 NNNNDrone-Based Object Counting by Spatially Regularized Regional Proposal Network   Drone-based Object Counting by Spatially Regularized Regional Proposal  Network  Meng-Ru HsiehN, Yen-Liang LinN, and Winston H
HsuN  NNational Taiwan University, Taipei, Taiwan NGE Global Research, Niskayuna, NY, USA  mrulafi@gmail.com,yenlianglintw@gmail.com,whsu@ntu.edu.tw  Abstract  Existing counting methods often adopt regression-based  approaches and cannot precisely localize the target objects,  which hinders the further analysis (e.g., high-level understanding and fine-grained classification)
In addition, most  of prior work mainly focus on counting objects in static environments with fixed cameras
Motivated by the advent of  unmanned flying vehicles (i.e., drones), we are interested  in detecting and counting objects in such dynamic environments
We propose Layout Proposal Networks (LPNs) and  spatial kernels to simultaneously count and localize target  objects (e.g., cars) in videos recorded by the drone
Different from the conventional region proposal methods, we  leverage the spatial layout information (e.g., cars often park  regularly) and introduce these spatially regularized constraints into our network to improve the localization accuracy
To evaluate our counting method, we present a new  large-scale car parking lot dataset (CARPK) that contains  nearly N0,000 cars captured from different parking lots
To  the best of our knowledge, it is the first and the largest drone  view dataset that supports object counting, and provides the  bounding box annotations
 N
Introduction  With the advent of unmanned flying vehicles, new potential applications emerge for unconstrained images and  videos analysis for aerial view cameras
In this work, we  address the counting problem for evaluating the number of  objects (e.g., cars) in drone-based videos
Prior methods  [N0, N, N] for monitoring the parking lot often assume that  the locations of the monitored objects of a scene are already  known in advance and the cameras are fixed, and cast car  counting as a classification problem, which makes conventional car counting methods not directly applicable in unconstrained drone videos
 Layout Proposal   Networks Drone-View  Car Dataset  Counting number : NN cars  Object Counting   and Localization  Drone videos  Figure N
We propose a Layout Proposal Network (LPN) to localize and count objects in drone videos
We introduce the spatial  constraints for learning our network to improve the localization  accuracy
Detailed network structure is shown in Figure N
 Current object counting methods often learn a regression  model that maps the high-dimensional image space into  non-negative counting numbers [N0, NN]
However, these  methods can not generate precise object positions, which  limits the further investigation and applications (e.g., recognition)
 We observe that there exists certain layout patterns for a  group of object instances, which can be utilized to improve  the object counting accuracy
For example, cars are often  parked in a row and animals are gathered in a certain layout  (e.g., fish torus and duck swirl)
In this paper, we introduce a novel Layout Proposal Network (LPN) that counts  and localizes objects in drone videos (Figure N)
Different  from existing object proposal methods, we introduce a new  spatially regularized loss for learning our Layout Proposal  Network
Note that our method learns the general adjacent  relationship between object proposals and is not specific to  a certain scene
 Our spatially regularized loss is a weighting scheme that  NNNN  mrulafi@gmail.com, yenlianglintw@gmail.com, whsu@ntu.edu.tw   Table N
Comparison of aerial view car-related datasets
In contrast to the PUCPR dataset, our dataset supports a counting task with  bounding box annotations for all cars in a single scene
Most important of all, compared to other car datasets, our CARPK is the only  dataset in drone-based scenes and also has a large enough number in order to provide sufficient training samples for deep learning models
 Dataset Sensor Multi Scenes Resolution Annotation Format Car Numbers Counting Support  OIRDS [NN] satellite X low bounding box NN0 X  VEDAI [N0] satellite X low bounding box N,NN0 X  COWC [NN] aerial X low car center point NN,NNN X  PUCPR [N0] camera × high bounding box NNN,NNN × CARPK [ours] drone X high bounding box NN,NNN X  (a) (b) (c)  (d) (e)  Figure N
(a), (b), (c), (d), and (e) are the example scenes of OIRDS [NN], VEDAI [N0], COWC [NN], PUCPR [N0], and CARPK (ours)  dataset respectively (two images for each dataset)
Comparing to (a), (b), and (c), the PUCPR dataset and the CARPK dataset have greater  number of cars in a single scene which is more appropriate for evaluating the counting task
 re-weights the importance scores for different object proposals and encourages region proposals to be placed in correct locations
It can also generally be embedded in any object detector system for object counting and detection
By  exploiting spatial layout information, we improve the average recall of state-of-the-art region proposal approaches on  a public PUCPR dataset [N0] (from NN.N% to NN.N%)
 For evaluating the effectiveness and reliability of our approach, we introduce a new large-scale counting dataset  CARPK (Table N)
Our dataset contains NN,NNN cars, and  provides bounding box annotations for each car
Also, we  consider the sub-dataset PUCPR of PKLot [N0] which is  the one that the scenes are closed to the aerial view in the  PKLot dataset
Instead of a fixed camera view from a high  story building (Figure N) in the PUCPR dataset, our new  CARPK dataset provide the first and the largest-scale drone  view parking lot dataset in unconstrained scenes
Besides,  the PUCPR dataset can be only used in conjunction with a  classification task, which classifies the pre-cropped images  (car or not car) with given locations
Moreover, the PUCPR  dataset only annotates partial region-of-interest parking areas, and is therefore unable to support a counting task
 Since our task is to count objects in images, we also annotate all cars in single full-image for the partial PUCPR  dataset
The contents of our CARPK dataset are unscripted  and diverse in various scenes for N different parking lots
 To the best of our knowledge, our dataset is the first and  the largest drone-based dataset that can support a counting  task with manually labelled annotations for numerous cars  in full images
The main contributions of this paper are  summarized as follows:  N
To our knowledge, this is the first work that leverages  spatial layout information for object region proposal
 We improve the average recall of the state-of-the-art  region proposal methods (i.e., NN.N% [NN] to NN.N%)  on a public PUCPR dataset
 N
We introduce a new large-scale car parking lot dataset  (CARPK) that contains nearly N0,000 cars in dronebased high resolution images recorded from the diverse scenes of parking lots
Most important of all,  compared to other parking lot datasets, our CARPK  NNNN    dataset is the first and the largest dataset of parking  lots that can support countingN
 N
We provide in-depth analyses for different decision  choices of our region proposal method, and demonstrate that utilizing layout information can considerably reduce the proposals and improve the counting  results
 N
Related Work  N.N
Object Counting  Most contemporary counting methods can be broadly  divided into two categories
One is counting by regression method, the other is counting by detection instance  [NN, NN]
Regression counters are usually a mapping of  the high-dimension image space into non-negative counting numbers
Several methods [N, N, N, N, NN] try to predict counts by using global regressors trained with low-level  features
However, global regression methods ignore some  constraints, such as the fact that people usually walk on the  pavement and the size of instances
There are also a number  of density regression-based methods [NN, NN, N] which can  estimate object counts by the density of a countable object  and then aggregate over that density
 Recently, a wealth of works introduce deep learning into  the crowd counting task
Instead of counting objects for  constrained scenes in the preivous works, Zhang et al
[NN]  address the problem of cross-scene crowd counting task,  which is the weakness of the density estimation method  in the past
Sindagi et al
[NN] incorporate global and local contextual information for better estimating the crowd  counts
Mundhenk et al
[NN] evaluate the number of cars  in a subspace of aerial imagery by extracting representations of image patches to approximate the appearance of  object groups
Zhang et al
[NN] leverage FCN and LSTM  to jointly estimate the vehicle density and counts in low resolution videos from city cameras
However, the regressionbased methods can not generate precise object positions,  which seriously limits the further investigation and application (e.g., high-level understanding and fine-grained classification)
 N.N
Object Proposals  Recent years have seen deep networks for region proposals developing well
Because detecting objects at several  positions and scales during inference time requires a computationally demanding classifier, the best way to solve this  problem is to look at a tiny subset of possible positions
A  number of recent works prove that deep networks-based region proposal methods have surpassed the previous works  NThe images and annotations of our CARPK and PUCPR+ are available  at https://lafi.github.io/LPN/  [NN, N, NN, N], which are based on the low-level cues, by a  large margin
 DeepMask [NN], which is developed for learning segmentation proposals, has, compared to Selective Search  [NN], ten times fewer proposals (N00 v.s
N000) at the same  performance
The state-of-the-art object proposal method,  Region Proposal Networks (RPNs) [NN], has also shown  that they just need N00 proposals and can surpass the result of N000 proposals generated by [NN]
Other works  like Multibox [NN] and Deepbox [NN] also have higher proposal recall with fewer number of region proposals than the  previous works which are based on low-level cues
However, none of these region proposal methods have considered the spatial layout or the relation between recurring  objects
Hence, we propose a Layout Proposal Networks  (LPNs) that leverages thus structure information to achieve  higher recall while using a smaller number of proposals
 N
Dataset  Since there is a lack of large standardized public datasets  that contain numerous collections of cars in drone-based  images, it has been difficult to create an automated counting  system for deep learning models
For instance, OIRDS [NN]  has merely NN0 unique cars
The recent car-related dataset  VEDAI [N0] has N,NN0 cars, but these are still too few to utilize for the deep learners
A newer dataset COWC [NN] has  NN,NNN cars, but the resolutions of images remain low
It has  only NN to NN pixels per car
Besides, rather than labelling  in the format of bounding box, the annotation format is the  center pixel point of a car which can not support further investigation, such as car model retrieval, statistics of brands  of car, and exploring which kind of car most people will  drive in the local area
Moreover, all above datasets are low  resolution images and cannot provide detail informations  for learning a fine-grained deep model
The problems of  existing dataset are : N) low resolution images which might  harm the performance of the model trained on them and N)  less car numbers in the dataset which has the potential to  cause overfitting during training a deep model
 Because existing datasets have these aforementioned  problems, we have created a large-scale car parking lot  dataset from drone view images, which are more appropriate to deep learning algorithms
It supports object counting, object localizing, and further investigations by providing the annotations in terms of bounding boxes
The most  similar public dataset to ours, which also has the high resolution of car images, is the sub-dataset PUCPR of PKLot  [N0], which provides a view from the N0th floor of a building  and therefore similar to drone view images to a certain degree
However, the PUCPR dataset can be only used in conjunction with a classification task, which classifies the precropped images (car or not car) with given locations
Moreover, this dataset has only annotated a portion of cars (N00  NNNN    Low probability  High probability  Neighbor cues   Figure N
The key idea of our spatial layout scores
A predicted  position which has more nearby cars can get higher confidence  scores and has higher probability to be the position where the car  is
 certain parking spaces) from total NNN parking spaces in a  single image, making it unable to support both counting and  localizing tasks
Hence, we complete the annotations for  all cars in a single image from the partial PUCPR dataset,  called PUCPR+ dataset, which now has nearly NN,000 cars  in total
Besides the incomplete annotation problem of the  PUCPR, it has a fatal issue that their camera sensors are  fixed and set in the same place, making the image scene  of dataset completely the same – causing the deep learning  model to encounter a dataset bias problem
 For this reason, we introduce a brand new dataset  CARPK that the contents of our dataset are unscripted and  diverse in various scenes for N different parking lots
Our  dataset also contains approximately N0,000 cars in total with  the view of drone
It is different from the view of camera from high story building in the PUCPR dataset
This  is a large-scale dataset for car counting in the scenes of diverse parking lots
The image set is annotated by providing  a bounding box per car
All labeled bounding boxes have  been well recorded with the top-left points and the bottomright points
Cars located on the edge of the image are included as long as the marked region can be recognized and  it is sure that the instance is a car
To the best of our knowledge, our dataset is the first and the largest drone viewbased parking lot dataset that can support counting with  manually labeled annotations for a great amount of cars in  a full-image
The details of dataset are listed in Table N and  some examples are shown in Figure N
 N
Method  Our object counting system employs a region proposal  module which takes regularized layout structure into account
It is a deep fully convolutional network that takes an  image of arbitrary size as the input, and outputs the objectagnostic proposals which likely contain the instance
The  entire system is a single unified framework for object counting (Figure N)
By leveraging the spatial information of the  object of recurring instances, LPNs module is not only concerning the possible positions but also suggesting the object  detection module which direction it should look at in the  image
 N.N
Layout Proposal Network  We observed that there exists certain layout patterns for  a group of object instances, which can be used to predict  objects that might appear adjacently in the same direction  or near the same instances
Hence, we design a novel region  proposal module that can leverage the structure layout and  gather the confidence scores from nearby objects in certain  directions (Figure N)
 We comprehensively describe the designed network  structure of LPNs (Figure N) as follows
Similar to RPNs  [NN], the network generates region proposals by sliding a  small network over the shared convolutional feature map
 It takes as input an N × N windows on last convolutional layer for reducing the representation dimensions, and then  feeds features into two sibling N × N convolutional layers, where one is for localization and the other is for classifying  whether the box belongs to foreground or background
The  difference is that our loss function introduces the spatially  regularized weights for the predicted boxes at each location
With the weights from spatial information, we minimize the loss of multi-task object function in networks
The  loss function we use on each image is defined as:  L({ui}, {qi}, {pi}) = N  Nfg  ∑  i  K(ci, N ∗ i ;u  ∗ i ) · Lfg(ui, u  ∗ i )  + γ N  Nbg  ∑  i  Lbg(qi, q ∗ i )  + λ N  Nloc  ∑  i  Lloc(u ∗ i , pi, g  ∗ i )  (N)  where Nfg and Nbg are the normalized terms of the num- ber matching default boxes for foreground and background
 Nloc is the same as Nfg in that it only considers the number of foreground classes
The default box is marked u∗i = N if the default box has an Intersection-over-Union (IoU) overlap higher than 0.N with the ground-truth box, or the de- fault box which has the highest IoU overlap with a groundtruth box; otherwise, it is marked q∗i = 0 if the IoU over- lap is lower than 0.N
The Lfg(ui, u  ∗ i ) = −log[uiu  ∗ i ] and  Lbg(qi, q ∗ i ) = −log[(N − qi)(N − q  ∗ i )] are the negative loglikelihood that we want to minimize for true classes
Here,  the i is the index of predicted boxes
In front of the fore- ground loss, K represents that we apply the spatially regu- larized weights for re-weighting the objective score of each  predicted box
The weight is obtained by a Gaussian spatial  NNNN    N000 × N00 × N N00 × N00 × NNN   N000 × N00 × NN   NN0 × NN0 × NNN   NNN × NN × NNN NNN × NN × NNN   c   NNN × NN × NN   NNN × NN × NN   Lloc   Lfg + Lbg   …   Gaussian kernels   Convolution Sum   Spatially regularized loss    Input image   Neighboring ground truth   bounding boxes   Kernel weights K   Figure N
The structure of the Layout Proposal Networks
At the loss layer, the structure weights are integrated for re-weighting the  candidates to have better structure proposals
See more details in Section N.N
 kernel for the center position ci of predicted box
It will give a rearranged weight according to the m neighbor ground- truth box centers, which are near to the ci
The real neigh- bor centers for ci are denoted as N  ∗ i = {c  ∗ N, ..., c  ∗ m} ∈ S  ci ,  which fall inside the spatial window pixels size S on the in- put image
We use S = NNN in this paper to obtain a larger spatial range
 The Lloc is the localization loss, which is a robust loss function [NN]
This term is only active for foreground predicted boxes (u∗i = N), otherwise 0
Similar to [NN], we cal- culate the loss of offsets between the foreground predicted  box pi and the ground truth box gi with their center posi- tion (x, y), width (w), and height (h) based on the default box (d)
 Lloc(u ∗ i , pi, g  ∗ i ) =  ∑  i∈fg  ∑  v∈{x,y,w,h}  u∗i smoothLN(p v i , g  v∗ i )  (N)  , where gv∗i (similar to p v i ) is defined as below:  gx∗i = (g x i − d  x i )/d  w i ,  gw∗i = log(g w i /d  w i ),  gy∗i = (g y i − d  y i )/d  h i  gh∗i = log(g h i /d  h i )  (N)  In our experiment, we set γ and λ to be N
Besides, in order to handle the small objects, instead of convN-N layer, we select convN-N layer features for obtaining better tiling default  box stride on the input image and choose default box sizes  approximately four times smaller (NN× NN, N0× N0, N00× N00) than the default setting (NNN× NNN, NNN× NNN, NNN× NNN)
 N.N
Spatial Pattern Score  Most of the objects of an instance exhibit a certain pattern between each other
For instance, cars will align in one  direction on a parking lot and ships will hug the shore regularly
Even in biology, we can also find collective animal  behavior that makes them look into a certain layout (e.g.,  fish torus, duck swirl, and ant mill)
Hence, we introduce  a method for re-weighting the region proposals in the training phase in an end-to-end manner
The proposed method  can reduce the number of proposals in the inference phase  for abating the computational cost of the counting and detection processes
It is especially important on embedded  devices, such as the drone, to lower power consumption as  the battery power only can provide the drone with energy to  fly a mere N0 minutes
 For designing the pattern of layout, we apply different  direction ND Gaussian spatial kernels K (see Eq
N) on the space of input images, where the center of the Gaussian kernel is the predicted box position ci
We compute the confi- dence weights over all positive predicted boxes
By incorporating the prior knowledge of layout from ground-truth,  we can learn the weight for each predicted box
In Eq
N,  it illustrates that the spatial pattern score for predicted position ci is a summation of weights by the ground truth posi- tions which are inside the Sci 
We compute the score over the input triples (ci, N  ∗ i , u  ∗ i ):  K(ci, N ∗ i , u  ∗ i ) =  {  ∑  θ∈D  ∑m  j∈N∗ i G(j; θ) if u∗i = N  N otherwise, (N)  NNNN    in which  G(j; θ) = α · e −(  xθ j  NσNx +  yθ j  NσNy ) , (N)  is the ND Gaussian spatial kernel that takes different rotated  radius D = {θN, ...θr}, where we use r = N ranged from 0 to π
The coordinate tuple (xj , yj) is the center position of jth ground-truth box in Eq
N, and the coefficient α is the amplitude of the Gaussian function
All experiments use  α = N
We only give the weights for the foreground predicted  box ci where it is marked u ∗ i = N
By the means of aggregating weights from ground-truth boxes N∗i in differ- ent direction kernels (Eq
N), we can compute a summation of scores for taking various layout structures into account
It will give a higher probability to the object position,  which has larger weight
Namely, the more similar objects  of instances surrounding it, the more possible the predicted  boxes are the same category of instances
Therefore, the  predicted box collects the confidence from the same objects  which are nearby (Figure N)
By leveraging spatially regularized weights, we can learn a model for generating the  region proposals where the objects of instance will appear  with their own layout
 N
Experiment  In this section, we evaluate our approach on two different  datasets
The PUCPR+ dataset, made from the sub-dataset  of the public PKLot dataset [N0], and the CARPK dataset  are both used to estimate the validation of our proposed  LPNs
Then, we evaluate our object counting model, which  leverages the structure information on the PUCPR+ dataset  and our CARPK dataset
 N.N
Experiment Setup  We implement our model on Caffe [NN]
For fairness  in analyzing the effectiveness between different baseline  methods, we implemented all of them based on the VGG-NN  networks [NN] which contains NN convolutional layers and N  fully-connected layers
All the layer parameters of baselines and our proposed model are using the weights pretrained on ImageNet N0NN [NN], followed by fine-tuning the  models on our CARPK dataset or the PUCPR+ dataset depending on the experiments
We run our experiments on  the environment of Linux workstation with Intel Xeon ENNNN0 vN N.N GHz CPU, NNN GB memory, and one NVIDIA  Tesla KN0 GPU
Our multi-task joint training scheme takes  approximately one day to converge
 N.N
Evaluation of Region Proposal Methods  For evaluating the performance of our method LPNs, we  use five-fold cross-validation on the PUCPR+ dataset to ensure that the same image would not appear across both training set and testing set
In order to better evaluate the recall  while estimating localization accuracy, rather than reporting  recall at particular IoU thresholds, we report the Average  Recall (AR)
It is an average of recall with IoU threshold t between 0.N to N, where AR = N  t  ∑t  i Recall(IoUt)
As the metric of recall at IoU of 0.N is not predictive of detection  accuracy, proposals with high recall but at low overlap are  not effective for detection [NN]
Therefore, adopting the IoU  range of the AR metric can simultaneously measure both  proposal recall and localization accuracy to better predict  the result of counting and localizing performance
 Table N
Result on the PUCPR+ [N0] dataset for average recall at  N00, N00, N00, N00, and N000 proposals with the different components of approaches
The method in the middle column represents  the RPN training with the small default box size on convN-N layer
 #Proposals RPN [NN] RPN+small LPN (ours)  N00 N.N% N0.N% NN.N%  N00 N.N% NN.N% NN.N%  N00 NN.N% NN.N% NN.N%  N00 NN.N% NN.N% N0.N%  N000 NN.N% NN.N% NN.N%  We compare our proposal method LPNs against the  state-of-the-art object proposal generator RPNs [NN] on the  PUCPR+ dataset with different number of the object proposals
Our results are shown in Table N
It reveals that  our proposal method LPNs, which leverages the regularized layout information, can achieve higher recall and surpass RPNs in the same number of proposals
The state-ofthe-art object proposal RPNs suffer from poor performance  in average recall
We refer that the factors, which affect  the performance, are upon on the inappropriate anchor size  and the resolution of convolutional layer features
Hence,  in the same manner, we apply the smaller anchor box size  on RPNs on the convN-N layer, which is in the same setting as our approach
Table N shows that the RPNs utilize  the small anchor size and the higher resolution feature map  bring about a better improvement
It implies that the CNN  model is not as powerful in scale variance as we thought  when using inappropriate layers or unsuitable default box  size for prediction
This experiment also shows that the  performance of our proposed model LPNs with spatial regularized constraints still outperforms the revised RPNs (e.g.,  NN.N% better in N00 proposals and N.NN% better in N00 proposals)
Besides, we also found that our method with spatial  regularizer significantly performs better in the dense case N
 The result indicates that the prior layout knowledge could  NWe additionally divide the PUCPR+ dataset into dense and less dense  cases
Our method has NN.N0% large relative improvement compared to  RPN-small in dense case, which is better than N.NN% for less dense case
 Moreover, our method localizes the bounding box more precisely, i.e., our  method achieves NN.N% recall in IoU at 0.N which is almost N0% better  than RPN-small NN.N% for N00 proposals
 NNN0    potentially benefit the outcome by giving the correct confidence score to the position of instances in images
 Table N
Results on the CARPK dataset with different components
 #Proposals RPN [NN] RPN+small LPN (ours)  N00 NN.N% NN.N% NN.N%  N00 NN.N% NN.N% NN.N%  N00 NN.N% N0.0% NN.N%  N00 NN.N% NN.N% NN.N%  N000 NN.N% NN.N% NN.N%  For looking into the details of the effectiveness of our approach in region proposal, we also conduct the experiment  on our CARPK dataset
In order to ensure that the same  or the similar image scenes would not appear across both  training and testing set, which would affect the observation  of validation, we take N different scenes of the parking lot as  training set and the remaining one scene of the parking lot  as testing set
Table N reports the average recall of our methods, the state-of-the-art region proposal method RPNs, and  the revised RPNs on CARPK dataset
In the experiment  results, it comes as no surprise that by incorporating the  additional layout prior information, our LPNs model boots  both recall and localization accuracy of proposal method
 Again, this result shows that the proposals generated by our  approach are more effective and reliable
 N.N
Evaluation of Car Counting Accuracy  Since the goal of our proposed approach is to count  the number of cars from drone view scenes, we compare  our car counting system with three state-of-the-art methods  which can also achieve the car counting task
A one-look  regression-based counting method [NN] which is the up-todate method for estimating the number of cars in density  object counting measure and two prominent object detection systems, Faster R-CNN [NN] and YOLO [NN], which  have remarkable success in object detection task in recent  years
 For fair comparison, all the methods are built based on a  VGG-NN network [NN]
The only difference is that [NN] uses  a softmax with NN outputs as they assumed that the maximum number of cars in a scene is sufficiently small
However, the maximum number of cars in the CARPK dataset is  far more than NN
The maximum number of cars is NNN in  a single scene of the PUCPR+ dataset and NNN in a single  scene of the CARPK dataset
Hence, we follow the setting  from [NN] and train the network with a different output number to make this regression-based method compatible with  the two datasets
We set the softmax to N00 outputs for the  PUCPR+ dataset and N00 outputs for the CARPK dataset  for evaluation
Last, the setting of two datasets, PUCPR+  and CARPK, are the same as the experiment of region proposal phase
 We employ two metrics, Mean Absolute Error (MAE)  and Root Mean Squared Error (RMSE), for evaluating the  performance of counting methods
These two metrics have  the similar physical meaning that estimates the error between the ground-truth car numbers yi and the predicted car numbers fi
MAE is the average absolute difference between ground-truth quantity yi and predicted quantity fi over all testing scenes where MAE =  N n  ∑n  i |fi − yi|
Similar, RMSE is the square root of the average of squared differences between ground-truth quantity and predicted quantity over all testing scenes where RMSE = √  N n  ∑n  i (fi − yi) N
The difference of the two metrics is  that the RMSE should be more useful when large errors are  particularly undesirable
Since the errors are squared before  they are averaged, the RMSE gives a relatively high weight  to large errors
In the counting task, these metrics have good  physical meaning for representing the average counting error of cars in the scene
 Table N
Comparison with the object detection methods and the  global regression method for car counting on the PUCPR+ dataset
 Np is the number of candidate boxes used in the object detector,  which parameterizes the region proposal method
The ”∗” in front  of the baseline methods represents that the method has been finetuned on PUCPR+ dataset
The ”†” represents that the method is  revised to fit our dataset
 Method Np MAE RMSE  YOLO [NN] - NNN.NN N00.NN  Faster R-CNN [NN] N00 NNN.NN N00.NN  *YOLO - NNN.00 N00.NN  *Faster R-CNN N00 NNN.N0 NNN.NN  *Faster R-CNN (RPN-small) N00 NN.NN NN.NN  †One-Look Regression [NN] - NN.NN NN.NN Our Car Counting CNN Model N00 NN.NN NN.NN  We compare three methods on the PUCPR+ dataset,  where the maximum number of cars is NNN in a single scene
 Since the softmax output number of [NN] is designed to be  N00 for the PUCPR+ dataset, we also impartially compare  to this dense object counting method with the number of region proposals limited to N00, which is a strict condition to  our object counter
For YOLO, we select the parameter of  confidence threshold at 0.NN, which gives the best performance in our dataset
 The experimental results are shown in Table N
The asterisk ”∗” in front of the YOLO and Faster R-CNN meth- ods represents that the models have been fine-tuned on  the PUCPR+ dataset, otherwise they are fine-tuned on the  benchmark datasets (PASCAL VOC dataset and MS COCO  dataset respectively), where they also have the car categories
Our proposed method outperforms the best RMSE  on large-scale car counting, even with a very tough setting  in the number of proposals
Note that we have comparable  NNNN    Counting number: NNN cars   Ground Truth: NNN cars  Counting number: NNN cars   Ground Truth: NNN cars   Figure N
Selected examples of car counting and localizing results on the PUCPR+ dataset (left) and the CARPK dataset (right)
The  counting model uses our proposed LPN trained on a VGG-NN model and combined with an object detector (Fast R-CNN), where the  parameters setting of confidence score is 0.N and non maximum suppression (NMS) is 0.N for N000 proposals
 MAE performance to the state-of-the-art car counting regression method [NN], but the better RMSE implies that our  method has better capability in some extreme cases
The  methods that are fine-tuned on PASCAL and MS COCO  get worse results
It reveals that the inferior outcomes are  caused by the different perspective view of the object even  when training with car category samples
The experiment  results show that by incorporating the spatially regularized  information, our Car Counting CNN model boosts the performance of counting
A counting and localization example  result is shown in Figure N (left)
 We further compare the counting methods on our challenging large-scale CARPK dataset where the maximum  number of cars is NNN in a single scene
However, different from the PUCPR+ dataset which only has one parking  lot, our CARPK dataset provides various scenes of diverse  parking lots for cross-scene evaluation
In the setting of [NN]  method, we also deign a N00 softmax output network for  the CARPK dataset
In order to fairly compare the counting  methods, we again restrict the number of proposals of object counter which has utilized the region proposal method  with a tough number N00 N
The quantitative results of car  counting on our dataset are reported in Table N
The experiment results show that our car counting approach is reliably effective and has the best MAE and RMSE even in  the cross-scene estimation
An counting and localizing example result is shown in Figure N (right)
Still our method  can generate the feasible proposals and obtain the reasonable counting result close to the real number of cars in the  NOur method gets better performance when using bigger number of  proposals (e.g., N.0N and NN.0N for N000 proposals in MAE and RMSE respectively) in the PUCPR+ dataset
In the CARPK dataset, our method  also has NN.NN and NN.NN for N000 proposals in MAE and RMSE respectively
 scenes of the parking lots
 Table N
Comparison results on the CARPK dataset
The notation  definition is similar to Table N
 Method Np MAE RMSE  YOLO [NN] - N0N.NN NN0.0N  Faster R-CNN [NN] N00 N0N.NN NN0.NN  *YOLO - NN.NN NN.NN  *Faster R-CNN N00 NN.NN NN.NN  *Faster R-CNN (RPN-small) N00 NN.NN NN.NN  †One-Look Regression [NN] - NN.NN NN.NN Our Car Counting CNN Model N00 NN.N0 NN.NN  N
Conclusions  We have created the to-date largest drone view dataset,  called CARPK
It is a challenging dataset for various scenes  of parking lots in a large-scale car counting task
Also,  in the paper, we introduced a new way for generating the  feasible region proposals, which leverage the spatial layout information for an object counting task with regularized  structures
The learned deep model can specifically count  objects better with the prior knowledge of object layout patterns
Our future work will involve global information, such  as context, road scene, and other objects which can help distinguish between false car-like instances and real cars
 N
Acknowledgement  This work was supported in part by the Ministry of Science and Technology, Taiwan, under Grant MOST N0NNNNN-N-00N-00N and MOST N0N-NNNN-E-00N-0NN, and in  part by MediaTek Inc, and grants from NVIDIA and the  NVIDIA DGX-N AI Supercomputer
 NNNN    References  [N] M
Ahrnbom, K
Astrom, and M
Nilsson
Fast classification  of empty and occupied parking spaces using integral channel  features
In CVPR, N0NN
N  [N] G
Amato, F
Carrara, F
Falchi, C
Gennaro, and C
Vairo
 Car parking occupancy detection using smart camera networks and deep learning
In ISCC, N0NN
N  [N] S
An, W
Liu, and S
Venkatesh
Face recognition using  kernel ridge regression
In CVPR, N00N
N  [N] P
Arbeláez, J
Pont-Tuset, J
T
Barron, F
Marques, and  J
Malik
Multiscale combinatorial grouping
In CVPR,  N0NN
N  [N] C
Arteta, V
Lempitsky, J
A
Noble, and A
Zisserman
Interactive object counting
In ECCV, N0NN
N  [N] A
B
Chan, Z.-S
J
Liang, and N
Vasconcelos
Privacy preserving crowd monitoring: Counting people without people  models or tracking
In CVPR, N00N
N  [N] K
Chen, S
Gong, T
Xiang, and C
Change Loy
Cumulative attribute space for age and crowd density estimation
In  CVPR, N0NN
N  [N] K
Chen, C
C
Loy, S
Gong, and T
Xiang
Feature mining  for localised crowd counting
In BMVC, N0NN
N  [N] M.-M
Cheng, Z
Zhang, W.-Y
Lin, and P
Torr
Bing: Binarized normed gradients for objectness estimation at N00fps
 In CVPR, N0NN
N  [N0] P
R
de Almeida, L
S
Oliveira, A
S
Britto, E
J
Silva,  and A
L
Koerich
Pklot–a robust dataset for parking lot  classification
Expert Syst Appl, N0NN
N, N, N, N  [NN] R
Girshick
Fast r-cnn
In ICCV, N0NN
N  [NN] J
Hosang, R
Benenson, P
Dollár, and B
Schiele
What  makes for effective detection proposals? TPAMI, N0NN
N  [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional architecture for fast feature embedding
arXiv preprint  arXiv:NN0N.N0NN, N0NN
N  [NN] D
Kamenetsky and J
Sherrah
Aerial car detection and urban understanding
In DICTA, N0NN
N  [NN] D
Kong, D
Gray, and H
Tao
A viewpoint invariant approach for crowd counting
In ICPR, N00N
N  [NN] V
Lempitsky and A
Zisserman
Learning to count objects  in images
In NIPS, N0N0
N  [NN] T
Moranduzzo and F
Melgani
Automatic car counting  method for unmanned aerial vehicle images
IEEE Transactions on Geoscience and Remote Sensing, N0NN
N  [NN] T
N
Mundhenk, G
Konjevod, W
A
Sakla, and K
Boakye
 A large contextual dataset for classification, detection and  counting of cars with deep learning
In ECCV, N0NN
N, N, N,  N, N  [NN] P
O
Pinheiro, R
Collobert, and P
Dollar
Learning to segment object candidates
In NIPS, N0NN
N  [N0] S
Razakarivony and F
Jurie
Vehicle detection in aerial imagery: a small target detection benchmark
JVCIR, N0NN
N,  N  [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In  CVPR, N0NN
N, N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
N, N, N, N, N, N, N  [NN] M
Rodriguez, I
Laptev, J
Sivic, and J.-Y
Audibert
 Density-aware person detection and tracking in crowds
In  ICCV, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  et al
Imagenet large scale visual recognition challenge
 IJCV, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N, N  [NN] V
A
Sindagi and V
M
Patel
Generating high-quality crowd  density maps using contextual pyramid cnns
In ICCV, N0NN
 N  [NN] C
Szegedy, S
Reed, D
Erhan, and D
Anguelov
 Scalable, high-quality object detection
arXiv preprint  arXiv:NNNN.NNNN, N0NN
N  [NN] F
Tanner, B
Colder, C
Pullen, D
Heagy, M
Eppolito,  V
Carlan, C
Oertel, and P
Sallee
Overhead imagery research data setan annotated data library & tools to aid in the  development of computer vision algorithms
In AIPR, N00N
 N, N  [NN] J
R
Uijlings, K
E
van de Sande, T
Gevers, and A
W
 Smeulders
Selective search for object recognition
IJCV,  N0NN
N  [N0] M
Wang and X
Wang
Automatic adaptation of a generic  pedestrian detector to a specific traffic scene
In CVPR, N0NN
 N  [NN] C
Zhang, H
Li, X
Wang, and X
Yang
Cross-scene crowd  counting via deep convolutional neural networks
In CVPR,  N0NN
N  [NN] S
Zhang, G
Wu, J
P
Costeira, and J
M
Moura
Fcn-rlstm:  Deep spatio-temporal neural networks for vehicle counting  in city cameras
In ICCV, N0NN
N  [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object  proposals from edges
In ECCV, N0NN
N  NNNNCoupleNet: Coupling Global Structure With Local Parts for Object Detection   CoupleNet: Coupling Global Structure with Local Parts for Object Detection  Yousong ZhuN,N Chaoyang ZhaoN,N Jinqiao WangN,N Xu ZhaoN,N Yi WuN,N Hanqing LuN,N  NNational Laboratory of Pattern Recognition, Institute of Automation,  Chinese Academy of Sciences, Beijing, China NUniversity of Chinese Academy of Sciences  NSchool of Technology, Nanjing Audit University, Nanjing, China NDepartment of Medicine, Indiana University, Indianapolis, USA  {yousong.zhu, chaoyang.zhao, jqwang, xu.zhao, luhq}@nlpr.ia.ac.cn, ywu.china@gmail.com  Abstract  The region-based Convolutional Neural Network (CNN)  detectors such as Faster R-CNN or R-FCN have already  shown promising results for object detection by combining the region proposal subnetwork and the classification  subnetwork together
Although R-FCN has achieved higher  detection speed while keeping the detection performance,  the global structure information is ignored by the positionsensitive score maps
To fully explore the local and global  properties, in this paper, we propose a novel fully convolutional network, named as CoupleNet, to couple the global  structure with local parts for object detection
Specifically, the object proposals obtained by the Region Proposal  Network (RPN) are fed into the the coupling module which  consists of two branches
One branch adopts the positionsensitive RoI (PSRoI) pooling to capture the local part information of the object, while the other employs the RoI  pooling to encode the global and context information
Next, we design different coupling strategies and normalization  ways to make full use of the complementary advantages between the global and local branches
Extensive experiments  demonstrate the effectiveness of our approach
We achieve  state-of-the-art results on all three challenging datasets, i.e
 a mAP of NN.N% on VOC0N, N0.N% on VOCNN, and NN.N% on COCO
Codes will be made publicly availableN
 N
Introduction  General object detection requires to accurately locate  and classify all targets in the image or video
Compared  to specific object detection, such as face, pedestrian and vehicle detection, general object detection often faces more  challenges due to the large inter-class appearance differences
The variations arise not only from changes in a vaNhttps://github.com/tshizys/CoupleNet  Local part confidence   Global confidence   sofa:0.0N  sofa:0.NN  PSRoI  pooling  RoI  pooling  sofa  sofa:0.NN  Figure N
A toy example of object detection by combing local and  global information
Only considering the local part information or  global structure leads to low confidence score
By coupling the two kinds of information together, we can detect the sofa accurately  with a confidence score of 0.NN
Best viewed in color
 riety of non-rigid deformations, but also due to the truncations, occlusions and inter-class interference
However,  no matter how complicated the objects are, when humans identify a target, the recognition of object categories is  subserved by both a global process that retrieves structural  information and a local process that is sensitive to individual parts
This motivates us to build a detection model that  fused both global and local information
 With the revival of Convolutional Neural Networks [NN]  (CNN), CNN-based object detection pipelines [N, N, NN, NN]  have been proposed consecutively and made impressive improvements in generic benchmarks, e.g
PASCAL VOC [N]  and MS COCO [NN]
As two representative region-based  CNN approaches, Fast/Faster R-CNN [N, NN] uses a certain  subnetwork to predict the category of each region proposal  while R-FCN [NN] conducts the inference with the positionsensitive score maps
Through removing the RoI-wise subnetwork, R-FCN has achieved higher detection speed while  keeping the detection performance
However, the global  NNNNN  https://github.com/tshizys/CoupleNet   structure information is ignored by the PSRoI pooling
As  shown in Figure N, using PSRoI pooling to extract local  part information for final object category prediction, R-FCN  leads to a low confidence score of 0.0N for the sofa detection  since the local responses of sofa are disturbed by a women  and a dog (they are also the categories that need to be detected)
Conversely, the global structure of sofa could be  extracted by the RoI pooling, but the confidence score is  0.NN, which is also very low for the incomplete structure of  sofa
By coupling the global confidence with the local part  confidence together, we can obtain a more reliable prediction with the confidence score of 0.NN
 In fact, the idea of fusing global and local information  together is widely used in lots of visual tasks
In fingerprint  recognition, Gu et al
[N0] combined the global orientation  field and local minutiae cue to largely improve the performance
In clique-graph matching, Nie et al
[NN] proposed a  clique-graph matching method by preserving global cliqueto-clique correspondence and local unary and pairwise correspondences
In scene parsing, Zhao et al
[NN] designed a  pyramid pooling module to effectively extract hierarchical  global contextual prior, and then concatenated it with the  local FCN feature to improve the performance
In traditional object detection, Felzenszwalb et al
[N] incorporated a  global root model and several finer local part models to represent highly variable objects
All of which show that effective combination of the global structural properties and local  fine-grained details can achieve complementary advantages
 Therefore, to fully explore the global and local clues, in  this paper, we propose a novel full convolutional network  named as CoupleNet, to couple the global structure and local parts to boost the detection accuracy
Specifically, the  object proposals obtained by the RPN are fed into the coupling module which consists of two branches
One branch  adopts the PSRoI pooling to capture the local part information of the object, while the other employs the RoI pooling  to encode the global and context information
Moreover, we  design different coupling strategies and normalization ways  to make full use of the complementary advantages between  the global and local branches
With the coupling structure,  our network can jointly learn the local, global and context expression of the objects, which makes the model have  a more powerful representation capacity and generalization  ability
Extensive experiments demonstrate that CoupleNet  can significantly improve the detection performance
Our  detector shows competitive results on PASCAL VOC 0N/NN  and MS COCO compared to other state-of-the-art detectors,  even with model ensemble approaches
 In summary, our main contributions are as follows:  N
We propose a unified fully convolutional network to  jointly learn the local, global and context information for  object detection
 N
We design different normalization methods and coupling strategies to mine the compatibility and complementarity between the global and local branches
 N
We achieve the state-of-the-art results on all three  challenging datasets, i.e
a mAP of NN.N% on VOC0N, N0.N% on VOCNN, and NN.N% on MS COCO
 N
Related work  Before the arrival of CNN, visual tasks have been dominated by traditional paradigms [N, N, NN, NN, NN]
As one  of an outstanding framework, DPM [N] described the object system using mixtures of multi-scale deformable part  models, including a coarse global root model and several  finer local part models
The root model extracts structural  information of the objects, while the part models capture local appearance properties of an object
The sum of root response and weighted average response of each part is used  as the final confidence of an object
Although DPM provides an elegant framework for object detection, the handcrafted features, i.e
improved HOG [N], are not discriminative enough to express the diversity of object categories
 This is also the main reason that CNN completely surpassed  the traditional methods in a short period time
 In order to leverage the great success of deep neural networks for image classification [NN, NN], considerable object detection methods based on deep learning have been  proposed [N, NN, NN, N0, NN]
Although there are end-toend detection frameworks, like SSD [NN], YOLO [N0] and  DenseBox [NN], region-based systems (i.e
Fast/Faster RCNN [N, NN] and R-FCN [NN]) still dominate the detection  accuracy on generic benchmarks [N, NN]
 Compared to the end-to-end framework, the regionbased systems have several advantages
Firstly, by exploiting a divide-and-conquer strategy, the two-step framework  is more stable and easier to converge
Secondly, without the  complicated data augmentation and training skills, you can  still easily achieve state-of-the-art performance
The main  reason for these advantages is that there is a certain structure [N, NN, NN] to encode translation variance features for  each proposal, since in deep networks, higher-layers contain  more semantic meaning and less location information
As a  consequence, a RoI-wise subnetwork [N, NN] or a positionsensitive RoI pooling layer [NN] is used to achieve the translation variance in region-based systems
However, all the  existing region-based systems utilize either the region-level  or part-level features to learn the variations, where each one  alone is not representative enough for a variety of challenging situations
Therefore, this motivates us to design a certain structure to take advantages of both the global and local  features
 In addition, context [NN] is known to play an important  role in visual recognition
Considerable works have been  proposed for exploting context in object detection
Bell et  al
[N] explored the use of recurrent neural networks to modNNNNN    Conv N-N Conv N  Input  image  RoI pooling  PSRoI pooling  k x k  vote  kxk   conv  NxN   conv  softmax  normalize  RPN Local FCN  C+N  normalize  Global FCNRoI pooling  Figure N
The architecture of the proposed CoupleNet
We use ResNet-N0N as the basic feature extraction network
Given an input image,  we first exploit Region Proposal Network (RPN) [NN] to generate candidate proposals
Then each proposal flows to two different branches:  local FCN and global FCN, in order to extract the global structure information and learn the object-specific parts respectively
Finally the  output of the two branches are coupled together to predict the object categories
 el the contextual information
Gidaris et al
[N] proposed to  utilize multiple contextual regions around the object
Cai  et al
[N] collected the context by padding the proposals for  pedestrian and car detection
Similar to these works, we  also absorb the context prior to enhance the global feature  representation
 N
CoupleNet  In this section, we first introduce the architecture of the  proposed CoupleNet for object detection
Then we explain  in detail how we incorporate local representations, global  appearance and contextual information for robust object detection
 N.N
Network architecture  The architecture of our proposed CoupleNet is illustrated  in Figure N
Our CoupleNet includes two different branches: a) a local part-sensitive fully convolutional network to  learn the object-specific parts, denoted as local FCN; b) a  global region-sensitive fully convolutional network to encode the whole appearance structure and context prior of  the object, denoted as global FCN
We first use the ImageNet pre-trained ResNet-N0N released in [NN] to initialize  our network
For our detection task, we remove the last average pooling layer and the fc layer
Given an input image,  we extract candidate proposals by using the Region Proposal Network (RPN), which also shares convolution features  with CoupleNet following [NN]
Then each proposal flows  to two different branches: the local FCN and the global FCN
Finally, the output of global and local FCN are coupled  together as the final score of the object
We also perform  class-agnostic bounding box regression in a similar way
 N.N
Local FCN  To effectively capture the specific fine-grained parts in  local FCN, we construct a set of part-sensitive score maps by appending a NxN convolutional layer with kN(C + N) channels, where k means we divide the object into k × k local parts (here k is set to the default value N) and C + N is the number of object categories plus background
For  each category, there are totally kN channels and each channel is responsible for encoding a specific part of the object
 The final score of a category is determined by voting the  kN responses
Here we use position-sensitive RoI pooling  layer in [NN] to extract object-specific parts and we simply perform average pooling for voting
Then, we obtain  a (C + N)-d vector which indicates the probability that the object belongs to each class
This procedure is equivalent  to dividing a strong object category decision into the sum  of multiple weak classifiers, which serves as the ensemble  of several part models
Here we refer this part ensemble as  local structure representation
As shown in Figure N(a), for  the truncated person, one can hardly get a strong response  from the global description of the person due to truncation,  on the contrary, our local FCN can effectively capture several specific parts, such as human nose, mouth, etc., which  correspond to the regions with large responses in the feature  map
We argue that the local FCN is much concerned with  the internal structure and components, which can effectively  reflect the local properties of visual object, especially when  NNNNN    dining table:0.NN  person:0.NN  Local part confidence  Global confidence  Local part confidence   Global confidence  ＋  ＋  (a)  person  dining table  (b)  Figure N
An intuitive description of CoupleNet for object detection
(a) It is difficult to determine the target by using the global  structure information alone for objects with truncations
(b) Moreover, for those having simple spatial structure and encompassing  considerable background in the bounding box, e.g
dining table,  it is also not enough to use local parts alone to make robust predictions
Therefore, an intuitive idea is to simultaneously couple  global structure with local parts to effectively boost the confidence
 Best viewed in color
 the object is occluded or the whole boundary is incomplete
 However, for those having simple spatial structure and encompassing considerable background in the bounding box,  e.g
dining table, the local FCN alone is difficult to make  robust predictions
Thus it is necessary to add the global  structure information to enhance the discrimination
 N.N
Global FCN  For the global FCN, we aim to describe the object by  using the whole region-level features
Firstly, we attach a  N0NN-d NxN convolutional layer after the last convolutional  block in ResNet-N0N for reducing the dimension
Due to  the diverse size of the object, we insert a RoI pooling layer  in [N] to extract a fixed-length feature vector as the global  structure description of the object
Secondly, we use two  convolutional layers with kernal size k × k and N × N re- spectively (k is set to the default value N) to further abstract  the global representation of RoI
Finally, the output of NxN  convolution is fed into the classifier whose output is also a  (C + N)-d vector
In addition, context prior is the most basic and important factor for visual recognition tasks
For example, the  boat usually travels in the water while is unlikely to fly in  the sky
Despite the higher layers in deep neural network  can involve the spatial context information around the objects due to the large receptive field, Zhou et al
[NN] have  shown that the practical receptive field is actually much smaller than the theoretical one
Therefore, it is necessary to  explicitly collect the surrounding information to reduce the  chance of misclassification
To enhance the feature representation ability of the global FCN, here we introduce the  contextual information as an effective supplement
Specifically, we extend the context region by N times larger than  the size of original proposal
Then the features RoI pooled  from the original region and context region are concatenated together and fed into the latter RoI-wise subnetwork.As  shown in Figure N, the context region is embedded into the  global branch to extract a more complete appearance structure and discriminative prior representation, which will help  the classifier to better identity the object categories
 Due to the RoI pooling operation, the global FCN describes the proposal as a whole with CNN features, which  can be seen as a global structure description of the object
Therefore, it can easily deal with the objects with intact structure and finer scale
As shown in Figure N(b), our  global FCN shows a large confidence for the dining table
 However, in most cases, natural scenes consist of considerable objects with occlusions or truncations, making the  detection more difficult
Figure N(a) shows that using the  global structure information alone can hardly make a confident prediction for the truncated person
By adding local  part structural supports, the detection performance can be  significantly boosted
Therefore, it is essential to combine  both local and global descriptions for a robust detection
 N.N
Coupling structure  To match the same order of magnitude, we apply a normalization operation to the output of local and global FCN  before they are combined together
We explored two different methods to perform normalization: an LN normalization layer or a NxN convolutional layer to model the scale
Meanwhile, how to couple the local and global output is also a problem that needs to be researched
Here,  we investigated three different coupling methods: elementwise sum, element-wise product and element-wise maximum
Our experiments show that using NxN convolution  along with element-wise sum achieves the best performance  and we will discuss it in Section N.N
 With the coupling structure, CoupleNet simultaneously  exploits the local parts, global structure and context prior  for object detection
The whole network is fully convolutional and benefits from approximate joint training and  multi-task learning
We also note that the global branch can  be regarded as a lightweight Faster R-CNN, in which all  learnable parameters are from convolutional layers and the  depth of RoI-wise subnetwork is only two
Therefore, the  computational complexity is far less than the subnetwork in  ResNet-based Faster R-CNN system whose depth is ten
As  NNNNN    a consequence, our CoupleNet can perform the inference efficiently, which runs slightly slower than R-FCN but much  more faster than Faster R-CNN
 N
Experiments  We train and evaluate our method on three challenging  object detection datasets: PASCAL VOCN00N, VOCN0NN  and MS COCO
Since all these three datasets contain a variety of circumstances, which can sufficiently verify the effectiveness of our method
We demonstrate state-of-the-art  results on all three datasets without bells and whistles
 N.N
Ablation studies on VOCN00N  We first perform experiments on PASCAL VOC N00N  with N0 object categories for detailed analysis of our proposed CoupleNet detector
We train the models on the union  set of VOC N00N trainval and VOC N0NN trainval (“0N+NN”)  following [NN], and evaluate on VOC N00N test set
Object  detection accuracy is measured by mean Average Precision  (mAP), all the ablation experiments use single-scale training and testing, and we did not add the context prior
 Normalization
Since features extracted form different layers of CNN show various of scales, it is essential to  normalize different features before coupling them together
 Bell et al
[N] proposed to use LN normalization to each RoIpooled feature and re-scale back up by a empirical scale,  which shows a great gain on VOC dataset
In this paper, we  also explore two different normalization ways to normalize  the output of local and global FCN: an LN normalization  layer or a NxN convolutional layer to learn the scale
 As shown in Table N, we find that the use of LN normalization decreases the performance greatly, even worse  than the direct addition (without any normalization ways)
 To explain such a phenomenon, we measured the outputs of two branches before and after LN normalization
We  further found that LN normalization reduces the output gap  between different categories, which results in a smaller score gap
As we know, a small score gap between different  categories always means the classifier can not make a confident prediction
Therefore, we assume that this is the reason  for the performance degradation
Moreover, we also exploit  a NxN convolution to adaptively learn the scales between the  global and local branches
Table N shows that using NxN  convolution increases by 0.N points compared to the direct addition and N.N points over R-FCN
Therefore, we use NxN convolution to replace the LN normalization in the following  experiments
 Coupling strategy
We explore three different response  coupling strategies: element-wise sum, element-wise product and element-wise maximum
Table N shows the comparison results for the above three different implementations
We can see that the element-wise sum always achieves  Normalization methods SUM PROD MAX  eltwise NN.N - N0.N  LN+eltwise N0.N NN.N NN.N  NxN conv+eltwise NN.N - NN.N  Table N
Effects of different normalization operation and coupling methods
Metric: detection mAP(%) on VOC0N test
eltwise: combine the output from global and local FCN directly
 LN+eltwise: use LN normalization to normalize the output
NxN  conv+eltwise: use NxN convolution to learn the scale
 the best performance even though in different normalization methods
Generally, current advanced residual networks [NN] also use element-wise sum as the effective way  to integrate information from previous layers, which greatly facilitates the circulation of information and achieves the  complementary advantages
For element-wise product, we  argue that the system is relatively unstable and is susceptible to the weak side, which results in a large gradient  to update the weak branch that makes it difficult to converge
For element-wise maximum, it equals to an ensemble model within the network to some extent, which losts  the advantages of mutual support compared to element-wise  sum when both two branches are failed to detect the object
Moreover, a better coupling strategy can be taken into  consideration as the future work to further improve the accuracy, such as designing a more subtle nonlinear structure  to learn the coupling relationship
 Model ensemble
Model ensemble is commonly used  to improve the final detection performance, since diverse  initialization of parameters and the randomness of training  samples both lead to different performance for the same  model
Although the differences and complementarities  will be more pronounced for different models, the promotion is often very limited
As shown in Table N, we  also compare our CoupleNet with the model ensemble
 For a fair comparison, we first re-implemented Faster RCNN [NN] using ResNet-N0N and online hard example mining (OHEM) [NN], which achieves a mAP of NN.0% on VOC0N (NN.N% in original paper without OHEM)
We al- so re-implemented R-FCN with appropriate joint training  using the public available code py-R-FCNN, which achieves  a slightly lower result compared to [NN] (NN.N% vs
NN.N%)
We use our reimplementation models to conduct the comparisons for consistency
We found that the promotion  brought by model ensemble is less than N point
As shown  in Table N, it is far less than our method (NN.N%)
On the one hand, we argue that the naive model ensemble  just combines the results together and does not essentially  guide the learning process of the network, while our CoupleNet can simultaneously utilize the global and local inforNhttps://github.com/Orpine/py-R-FCN  NNNN0  https://github.com/Orpine/py-R-FCN   training data mAP (%) on VOC0N GPU test time (ms/img) Faster R-CNN [NN] 0N+NN NN.N KN0 NN0  R-FCN [NN] 0N+NN NN.N TITAN X NN  R-FCN multi-sc train [NN] 0N+NN N0.N TITAN X NN  CoupleNet 0N+NN NN.N TITAN X N0N  CoupleNet context 0N+NN NN.N TITAN X NNN  CoupleNet context multi-sc train 0N+NN NN.N TITAN X NNN  Table N
Comparisons with Faster R-CNN and R-FCN using ResNet-N0N
NNN samples are used for backpropagation and the top N00  proposals are selected for testing following [NN]
The input resolution is N00xN000
We also note that the TITAN X used here is the new  Pascal architecture along with CUDA N.0 and cuDNN-vN.N
“0N+NN”: VOC0N trainval union with VOCNN trainval
context: add the context  prior to assist the global branch
 Method Train mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbikepersn plant sheep sofa train tv  ION [N] 0N+NN+S NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  HyperNet [NN] 0N+NN NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.N NN.N NN.N NN.N N0.N NN.N  SSDN00∗ [NN] 0N+NN NN.N NN.N NN.N NN.0 NN.N N0.N NN.0 NN.N NN.N N0.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  SSDNNN∗ [NN] 0N+NN NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N N0.0  Faster§ [NN] 0N+NN NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.0  R-FCN [NN] 0N+NN N0.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  CoupleNet [ours] 0N+NN NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N N0.N  Table N
Results on PASCAL VOC N00N test set
The first four methods use VGGNN and the latter three use ResNet-N0N as the base  network
For fair comparison, we only list the results of single model without multi-scale testing, ensemble or iterative box regression  tricks in testing phase
“0N+NN”: VOC0N trainval union with VOCNN trainval
“0N+NN+S”: VOC0N trainval union with VOCNN trainval  plus segmentation labels
*: the results are updated using the latest models
§: this entry is directly obtained from [NN] without using OHEM
 Method mAP(%) Faster-ReIm NN.0  R-FCN-ReIm NN.N  Global FCN NN.N  Faster&R-FCN ensemble NN.N  Global FCN&R-FCN ensemble NN.N  CoupleNet NN.N  Table N
CoupleNet vs
model ensemble
ReIm: our reimplementation using OHEM
Global FCN: only the global branch of our  network
 mation to update the network and to infer the final results
 On the other hand, our method enjoys end-to-end training  and there is no need to train multiple models, thus greatly  reducing the training time
 Amount of parameters
Since our CoupleNet introduces a few more parameters compared with the single  branch detectors, to further verify effectiveness of the coupling structure, here we increase the parameters of the  prediction head for each single branch implementation to  maintain the same amount of parameters with CoupleNet  for comparison
In detail, we add a new residual variant  block with three convolution layers, where the kernel size  is NxNxNNN, NxNxNNN and NxNxN0NN respectively, to the prediction sub-network
We found that the standard R-FCN  with one or two extra heads got a mAP of NN.N% and NN.N%  respectively in VOC0N, which is slightly higher than our reimplemented version (NN.N%) in [NN] as shown in Table N
Meanwhile, our global FCN, which performs the ROI Pooling on top of convN, got a relative higher gain (a mAP of  NN.N% for one head, NN.0% for two heads)
The results in- dicate that simply adding more prediction layers obtains a  very limited performance gain, while our coupling structure  shows more discriminative power with the same amount of  parameters
 N.N
Results on VOCN00N  Using the public available ResNet-N0N as the initialization model, we note that our method is easy to follow and  the hyper-parameters for training are the same as in [NN]
 Similarly, we use the dilation strategy to reduce the effective stride of ResNet-N0N, just as [NN] shows, thus both the  global and local branches have a stride of NN
We also use a  N-GPU implementation, and the effective mini-batch size is  N images by setting the iter size to N
The whole network  is trained for N0k iterations with a learning rate of 0.00N and  then for N0k iterations with 0.000N
In addition, the context prior is proposed to further boost the performance while  keeping the iterations unchanged
Finally, we also perform multi-scale training with the shorter sides of images are  randomly resized from NN0 to NNN
 Table N shows the detailed comparisons with Faster RNNNNN    Method Train mAP aero bike bird boat bottle bus car cat chair cow table dog horse mbikepersn plant sheep sofa train tv  ION [N] 0N+NN+S NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  HyperNet [NN] 0N++NN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  SSDN00∗ [NN] 0N++NN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N  SSDNNN∗ [NN] 0N++NN NN.N N0.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N  Faster§ [NN] 0N++NN NN.N NN.N NN.N NN.N NN.0 NN.0 NN.N NN.N NN.N NN.N N0.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  R-FCN [NN] 0N++NN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N N0.N NN.N NN.0 NN.N NN.N NN.0 N0.N NN.N NN.N NN.N  CoupleNet [ours] 0N++NN N0.N† NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
Results on PASCAL VOC N0NN test set
For fair comparison, we only list the results of single model without multi-scale  testing, ensemble or iterative box regression tricks in testing phase
“0N++NN”: the union set of VOC0N trainval+test and VOCNN trainval
 “0N+NN+S”: VOC0N trainval union with VOCNN trainval plus segmentation labels
*: results are updated using the latest models
§: this entry is directly obtained from [NN] without using OHEM
†: http://host.robots.ox.ac.uk:N0N0/anonymous/MNCQTL
html
 Method train  data  AP AP  @0.N  AP  @0.NN  AP  small  AP  medium  AP  large  AR  max=N  AR  max=N0  AR  max=N00  AR  small  AR  medium  AR  large  SSDN00∗ [NN] trainvalNNk NN.N NN.N NN.N N.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N  SSDNNN∗ [NN] trainvalNNk NN.N NN.N N0.N N0.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N N0.N  ION [N] train+S NN.N NN.N NN.N N.0 NN.N N0.N NN.N NN.N NN.N N0.N NN.N NN.N  Faster+++ [NN] trainval NN.N NN.N - NN.N NN.N N0.N - - - - - R-FCN [NN] trainval NN.N NN.N - N0.N NN.N NN.N - - - - - R-FCN multi-sc train [NN] trainval NN.N NN.N - N0.N NN.N NN.0 - - - - - CoupleNet trainval NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  CoupleNet multi-sc train trainval NN.N NN.N NN.N NN.N NN.N N0.N N0.0 NN.0 NN.N N0.N NN.N NN.N  Table N
Results on COCO N0NN test-dev
The COCO metric AP is evaluated at IoU thresholds ranging from 0.N to 0.NN
AP@0.N:  PASCAL-type metric, IoU=0.N
AP@0.NN: evaluate at IoU=0.NN
“train+S”: train set plus segmentation labels
 CNN and R-FCN
As we can see that our single model  achieves a mAP of NN.N%, which outperforms the R-FCN by N.N points
However, while embedding the context prior to the global branch, our mAP rises up to NN.N%, which is the current best single model detector to our knowledge
 Moreover, we also evaluate the inference time of our network using a NVIDIA TITAN X GPU (pascal) along with  CUDA N.0 and cuDNN-vN.N
As shown in the last column  of Table N, our method is slightly slower than R-FCN, which  also reaches a real-time speed (i.e
N.N fps or N.N fps without  context) and achieves the best trade-off between accuracy  and speed
We argue that the sharing process of feature extraction between two branches and the design of lightweight  RoI-wise subnetwork after RoI pooling both greatly reduce  the model complexity
 As shown in Table N, we also compared our method  with other state-of-the-art single model
We found that our  method outperforms the others with a large margin, including the advanced end-to-end SSD method [NN], which requires complicated data augmentation and careful training  skills
Just as discussed earlier, CoupleNet shows a large  gain over the classes with occlusions, truncations and considerable background information, like sofa, person, table  and chair, which verifies our analyses
We also observed a  large improvement for airplane, bird, boat and pottedplant,  which usually have class-specific backgrounds, i.e
the sky  for airplane and bird, water for boat and so on
Therefore,  the context surrounding the objects provides an extra auxiliary discrimination
 N.N
Results on VOCN0NN  We also evaluate our method on the more challenging  VOCN0NN dataset by submitting results to the public evaluation server
We use VOC0N trainval, VOC0N test and  VOCNN trainval as the training set, which consists of NNk  images in total
We also follow the similar hyper-parameter  settings in VOC0N but change the iterations, since there are  more training images
We train our models with N GPUs,  and the effective mini-batch size thus becomes N (N per GPU)
As a result, the network is trained for N0k iterations  with a learning rate of 0.00N and 0.000N for the following  N0k iterations
Table N shows the results on the VOCN0NN  test set
Our method obtains a top mAP of N0.N%, which is N.N points higher than R-FCN
We note that without using  the extra tricks in the testing phase, our detector is the first  one with a mAP higher than N0%
Similar promotions over the specific classes analysed in VOC0N are also observed,  which once again validates the effectiveness of our method
 Figure N shows some detection examples on VOC N0NN test  set
 NNNNN  http://host.robots.ox.ac.uk:N0N0/anonymous/MNCQTL.html http://host.robots.ox.ac.uk:N0N0/anonymous/MNCQTL.html   Figure N
Detection examples of CoupleNet on PASCAL VOC N0NN test set
The model was trained on the union of VOC0N trainval+test  and VOCNN trainval (N0.N% mAp)
Our method works well with the occlusions, truncations, inter-class interference and clustered background
CoupleNet also shows good performance for the categories with class-specific backgrounds, e.g
airplane, bird, boat, etc
A score  threshold of 0.N is used to draw the detection bounding boxes
Each color is related to an object category
 N.N
Results on MS COCO  Next we present more results on the Microsoft COCO  object detection dataset
The dataset consists of N0k training  set, N0k validation set and N0k test-dev set, which involves  N0 object categories
All our models are trained on the union set of N0k training set and N0k validation set, and evaluated on N0k test-dev set
The COCO standard metric denotes as AP, which is evaluated at IoU ∈ [0.N : 0.0N : 0.NN]
Following the VOCN0NN, a N-GPU implementation is used  to accelerate the training process
We use an initial learning rate of 0.00N for the first NN0k iterations and 0.000N  for the next N0k iterations
In addition, we conduct multiscale training with the scales are randomly sampled from  {NN0, NNN, NNN, NNN, NNN} while testing in a single scale
 Table N shows our results
Our single-scale trained detector has already achieved a result of NN.N%, which outper- forms the R-FCN by N.N points
In addition, the multi-scale  training further improves the performance up to NN.N%
Interestingly, we observed that the more challenging the  dataset, the more the promotion (e.g., N.N% for VOC0N, N.N% for VOCNN and N.N% for COCO, all in multi-scale training), which directly proves that our approach can effectively cope with a variety of complex situations
 N
Conclusion  In this paper, we present the CoupleNet, a concise yet  effective network that simultaneously couples global, local  and context cues for accurate object detection
Our system naturally combines the advantages of different regionbased approaches with the coupling structure
With the  combination of local part representation, global structural  information and the contextual assistance, our CoupleNet  achieves state-of-the-art results on the challenging PASCAL VOC and COCO datasets without using any extra  tricks in the testing phase, which validates the effectiveness  of our method
 NNNNN    References  [N] S
Bell, C
L
Zitnick, K
Bala, and R
Girshick
Insideoutside net: Detecting objects in context with skip pooling  and recurrent neural networks
Computer Vision and Pattern  Recognition (CVPR), N0NN
 [N] Z
Cai, Q
Fan, R
S
Feris, and N
Vasconcelos
A unified  multi-scale deep convolutional neural network for fast object detection
In European Conference on Computer Vision,  pages NNN–NN0
Springer, N0NN
 [N] N
Dalal and B
Triggs
Histograms of oriented gradients for  human detection
In Computer Vision and Pattern Recognition, N00N
CVPR N00N
IEEE Computer Society Conference  on, volume N, pages NNN–NNN
IEEE, N00N
 [N] P
Dollár, R
Appel, S
Belongie, and P
Perona
Fast feature  pyramids for object detection
IEEE Transactions on Pattern  Analysis and Machine Intelligence, NN(N):NNNN–NNNN, N0NN
 [N] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
International journal of computer vision, NN(N):N0N–  NNN, N0N0
 [N] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and D
Ramanan
Object detection with discriminatively trained partbased models
IEEE transactions on pattern analysis and  machine intelligence, NN(N):NNNN–NNNN, N0N0
 [N] S
Gidaris and N
Komodakis
Object detection via a multiregion and semantic segmentation-aware cnn model
In Proceedings of the IEEE International Conference on Computer  Vision, pages NNNN–NNNN, N0NN
 [N] R
Girshick
Fast r-cnn
In Proceedings of the IEEE International Conference on Computer Vision, pages NNN0–NNNN,  N0NN
 [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In Proceedings of the IEEE conference on  computer vision and pattern recognition, pages NN0–NNN,  N0NN
 [N0] J
Gu, J
Zhou, and C
Yang
Fingerprint recognition by combining global structure and local cues
IEEE Transactions on  Image Processing, NN(N):NNNN–NNNN, N00N
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Spatial pyramid pooling  in deep convolutional networks for visual recognition
In  European Conference on Computer Vision, pages NNN–NNN
 Springer, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NN0–NNN, N0NN
 [NN] L
Huang, Y
Yang, Y
Deng, and Y
Yu
Densebox: Unifying  landmark localization with end to end object detection
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
 [NN] T
Kong, A
Yao, Y
Chen, and F
Sun
Hypernet: towards accurate region proposal generation and joint object detection
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNN–NNN, N0NN
 [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in neural information processing systems, pages  N0NN–NN0N, N0NN
 [NN] Y
Li, K
He, J
Sun, et al
R-fcn: Object detection via regionbased fully convolutional networks
In Advances in Neural  Information Processing Systems, pages NNN–NNN, N0NN
 [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European Conference on Computer Vision, pages NN0–NNN
Springer, N0NN
 [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.Y
Fu, and A
C
Berg
Ssd: Single shot multibox detector
 In European Conference on Computer Vision, pages NN–NN
 Springer, N0NN
 [NN] W.-Z
Nie, A.-A
Liu, Z
Gao, and Y.-T
Su
Clique-graph  matching by preserving global & local structure
In Proceedings of the IEEE conference on computer vision and pattern  recognition, pages NN0N–NNN0, N0NN
 [N0] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNN–NNN, N0NN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  Advances in neural information processing systems, pages  NN–NN, N0NN
 [NN] A
Shrivastava, A
Gupta, and R
Girshick
Training regionbased object detectors with online hard example mining
In  Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNN–NNN, N0NN
 [NN] P
Viola and M
J
Jones
Robust real-time face detection
 International journal of computer vision, NN(N):NNN–NNN,  N00N
 [NN] J
Wang, L
Duan, Q
Liu, H
Lu, and J
S
Jin
A multimodal scheme for program segmentation and representation  in broadcast video streams
IEEE Transactions on Multimedia, N0(N):NNN–N0N, N00N
 [NN] J
Wang, W
Fu, J
Liu, and H
Lu
Spatiotemporal group  context for pedestrian counting
IEEE Transactions on Circuits and Systems for Video Technology, NN(N):NNN0–NNN0,  N0NN
 [NN] J
Wang, W
Fu, H
Lu, and S
Ma
Bilayer sparse topic  model for scene analysis in imbalanced surveillance videos
 IEEE Transactions on Image Processing, NN(NN):NNNN–NN0N,  N0NN
 [NN] H
Zhao, J
Shi, X
Qi, X
Wang, and J
Jia
Pyramid scene  parsing network
arXiv preprint arXiv:NNNN.0NN0N, N0NN
 [NN] B
Zhou, A
Khosla, A
Lapedriza, A
Oliva, and A
Torralba
 Object detectors emerge in deep scene cnns
arXiv preprint  arXiv:NNNN.NNNN, N0NN
 [NN] Y
Zhu, J
Wang, C
Zhao, H
Guo, and H
Lu
Scale-adaptive  deconvolutional regression network for pedestrian detection
 In Asian Conference on Computer Vision, pages NNN–NN0
 Springer, N0NN
 NNNNNCross-Modal Deep Variational Hashing   Cross-Modal Deep Variational Hashing  Venice Erin LiongN,N, Jiwen LuN,∗, Yap-Peng TanN, and Jie ZhouN  NRapid-Rich Object Search (ROSE) Laboratory, Interdisciplinary Graduate School,  Nanyang Technological University, Singapore NDepartment of Automation, Tsinghua University, Beijing, China  NSchool of Electrical and Electronic Engineering, Nanyang Technological University, Singapore  veniceer00N@e.ntu.edu.sg; lujiwen@tsinghua.edu.cn; eyptan@ntu.edu.sg; jzhou@tsinghua.edu.cn  Abstract  In this paper, we propose a cross-modal deep variational hashing (CMDVH) method for cross-modality multimedia retrieval
Unlike existing cross-modal hashing methods  which learn a single pair of projections to map each example as a binary vector, we design a couple of deep neural  network to learn non-linear transformations from imagetext input pairs, so that unified binary codes can be obtained
We then design the modality-specific neural networks in a probabilistic manner where we model a latent  variable as close as possible from the inferred binary codes,  which is approximated by a posterior distribution regularized by a known prior
Experimental results on three benchmark datasets show the efficacy of the proposed approach
 N
Introduction  Recent years have witnessed that learning-based hashing  is an active research topic for efficient large-scale multimedia search [N, N, NN, NN, NN, NN]
The basic idea of learningbased hashing methods aims to learn a series of hash functions from the training set to map each visual sample into a  compact binary feature vector such that samples of the same  semantic content are mapped into same binary codes
 While recent works have achieved reasonably good performance in large-scale multimedia search, most existing  hashing methods are developed for single-modal retrieval,  which means that the query example and the examples stored in the database are from the same source of multimedia data
In many real-applications, it is easy to access  multi-modal data for multimedia retrieval
For example, images uploaded into social networks such as the Flickr and  Facebook websites are usually tagged with some text descriptions
Hence, it is desirable to retrieve semantically∗Corresponding author
 similar texts/images by using a query image/text
Because  there are large-scale multi-modal data over the Internet, it is  only necessary to develop an effective cross-modal similarity search methods for multimedia search
In this paper, we  propose a cross-modal deep variational hashing (CMDVH)  method for cross-modality retrieval
Figure N illustrates the  basic idea of the proposed approach
Unlike existing shallow cross-modal hashing methods which learn a single pair  of linear or nonlinear projections to map each example into  a binary vector, we employ an end-to-end hashing network  to learn multiple pairs of hierarchical non-linear transformations, under which the nonlinear relationship of samples can  be well exploited, the binarized neural codes having same  semantic are similar as possible, and neural codes having  different semantic are dissimilar as possible
Our model is  trained under two main steps: First, we perform binary code  inference to learn unified binary codes for each training pair  using a cross-modal fusion network such that we obtain  a common hamming space for the two modalities and the  modality gap can be implicitly reduced
We perform this in  a discrete and discriminative manner to avoid approximate  optimization loss caused by relaxing the binary constraint and strengthen the semantic correlation between modalities by using a classification-based hinge loss criterion, respectively
Second, we model the modality-specific hashing  networks which have a probabilistic interpretation such that  the latent variable is modeled similar to the inferred binary  code from the fusion network through a log likelihood criterion, which is also sampled based on an approximate posterior distribution regularized by a prior through a KullbackLiebler Divergence (KLD) criterion
By doing so, the hashing network can be in generative form, which is suitable for  out-of-sample extension
We perform learning in these two steps through a batch-wise gradient descent procedure
 Experimental results on three benchmark datasets show the  efficacy of the proposed approach
 NN0NN    Christianity has  the larges t   following in Peterborough ,  in particular the Church of   England, with a s ignificant   number of parish churches    and a ….
 Chris tianity has  the larges t  following in Peterborough ,   in particular the Church of   England, with a s ignificant   number of parish churches    and a ….
 Christianity has  the largest   following in Peterborough ,   in particular the Church of   England , with a s ignificant  number of parish churches   and a ….
 Latent Network  Text Network  Image NetworkImage Data  Text Data  Binary  Code  Negative Log   Likelihood  Classification   Learning (Hinge Loss )  Kullback–Leibler   divergence  Modality Specific NetworksCross-Modal Fusion Network  sample  sample  z  µ  µ  σ²  σ²z  p(z)  FC   Conv + Pooling FC   Text  Feature  Figure N
The basic idea of our proposed approach for cross-modality multimedia retrieval
Given a gallery set represented by two modalities (image and text), we learn a fusion hashing network and a joint binary code matrix, simultaneously
We learn them using an alternative  optimization procedure
First, we infer the binary codes in discrete manner such that we exploit label information through a classification  based hinge-loss criterion
Second, we minimize the loss between neural code and binary code by performing end-to-end deep training via  backpropagation to learn the parameters of the each network
This is done iteratively until convergence
Once the inferred binary codes are  learned, we learn modality-specific hashing networks (one for each modality) such that a latent variable is modeled based on two criterions
 First, given the image-text pair, the latent variable is forced to be similar as possible to the inferred binary code from the fusion network  through a negative log likelihood criterion
Second, the latent variable is also modeled such that approximated posterior distribution in the  form of Multivariate Gaussian is close to prior regularized by the KL-divergence criterion
During retrieval, given a query, we extract the  query binary code using the learned modality-specific hashing network and obtain the most similar binary codes from the gallery (learned  B) which are indexed to retrieve the most relevant images
 N
Related Work  Cross-Modal Retrieval: Unlike single-modal retrieval  where both the query example and the database are from  the same modality, the key idea of cross-modal retrieval is  to retrieve samples from another modality which is different from that of the query example but share similar semantics
Typically, cross-modal multimedia retrieval perform  two main tasks: N) retrieval of text documents by using a  given query image, and N) retrieval of images by using a  given query text or tag
In recent years, several methods  have been proposed for cross-modal retrieval, where the objective is to learn a common subspace between images and  text [NN, NN] to model the correlations
For example, Rasiwasia et al
[NN] used canonical component analysis (CCA)  to map both text documents and images into a latent space
 Wang et al
[NN] learned a coupled feature space method  to select the most relevant and discriminative features for  cross-modal matching
Gong et
al
[N] performed nonlinear kernel embedding followed by a linear dimensionality reduction and CCA for content-based retrieval and tagimage search
Kang et
al
[NN] proposed a feature learning  approach for cross-media matching by jointly learning consistent features for each modality in a supervised manner
 More recently, Wang et al
[N0] employed a feature selection scheme using multimodal-graph to represent the similarity between modalities
These retrieval methods usually  perform cross-modal matching with high-dimensional features, hence are not suitable for large-scale search due to  the scalability issue
Therefore, hashing is a more desirable  choice for cross-modal retrieval
 Shallow Cross-Modal Hashing: In recent years, several cross-modal hashing methods have also been proposed  in the literature, and most studies are in shallow form in  which it only performs a single-layer of linear or nonlinear transformation
These can be classified into two types:  unsupervised [N, NN, NN] and supervised [N, NN, NN, NN]
Unsupervised methods utilize co-occurence information such  that only the image-text pairs which occured in the same  article are known to be of similar semantic
For example,  Kumar et al
[NN] presented a cross-modal spectral hashing method so that the cross-modality similarity is also preserved in the learned hash functions
Zhu et al
[NN]  learned a common latent space by preserving the similarity between the example to the �-nearest centroids in each  modality and cross-modality
Zhou et al
[NN] obtained a  unified binary from a latent space learning method by using sparse coding and matrix factorization in the common  space
Ding et.al
[N] learned a unified binary code in  the training stage by performing matrix factorization with  latent factor model
Supervised methods utilize semanN0NN    tic labels to enhance the correlation of cross-modal data
For example, Brostein et al
[N] presented a crossmodal hashing method by preserving the intra-class similarity through eigen-decomposition and boosting
Zhang et  al
[NN] performed semantic correlation maximization using  label information to learn a modality-specific transformations which maximizes the correlation between modalities
 Lin et.al
[NN] learned a unified binary code by modeling  them in a probability distribution in a supervised manner  and performed kernel-embedding to learn the hashing functions
Xu et.al
[NN] also learned a unified binary code and  used a linear classifier to exploit the label information
 Unlike these methods which learn a pair of linear/nonlinear projections for hash functions learning, we  employ hashing networks to learn multiple pairs of hierarchical non-linear transformations, so that the nonlinear relationship of samples and the relationship of samples from  different modalities can be well exploited
Cross-modal  hashing methods can also be classified as learning a joint  binary code or separate binary codes during training
Several recent works learned unified binary codes [N,NN,NN,NN]  and these methods generally showed better performance because by learning a single discriminative and efficient binary code, the modality gap between the hashing functions are  implicitly reduced
Hence, we also perform a shared binary  code learning strategy in our hash function learning procedure, then perform modality-specific hash function learning  to have a generative model
 Deep Cross-Modal Hashing: Over the past few years,  a variety of deep learning algorithms have been proposed  in machine learning, and some of them were successfully applied to many computer vision applications such as  in object detection and recognition [NN, NN]
While there  are now also studies that perform deep learning for crossmodal retrieval [NN,NN,NN,NN] they are not suitable for largescale search due to its high dimension and large storage requirement
Only few works have performed deep learning  for cross-modal hashing
For example, Masci et al
[NN]  learned a similarity preserving network for cross-modalities  through a coupled siamese network with hinge loss
However this does not consider the binary constraints during  training, and simply performs binarization after training
 Cao et al
[N] designed a stacked auto-encoder architecture  to jointly maximize the feature and semantic correlation  across modalities
However, this work does not perform  end-to-end learning which may limit the discriminative representation of data samples, particularly in images
Jiang et  al
[N0] performed an end-to-end deep learning framework  with a negative log likelihood criterion to preserve the similarity between real-value representations having the same  class
However, their training model performs similarity preservation on real-value codes and not binary codes which  are used for the actual retrieval during testing
Another  work from Cao et al
[N] learned a visual semantic fusion  network with cosine hinge loss, to obtain the binary codes  and learned modality-specific deep networks to obtain the  hashing functions
However, a metric-based approach may  not fully utilize the label information during training
 N
Cross-Modal Deep Variational Hashing  We propose an end-to-end deep architecture for crossmodal hashing such that we are able to implicitly maximize  the correlation between the two modalities given image-text  training data pairs and its corresponding label information
 Our implementation composes of a fusion network for binary code inference that learns binary codes from image and  text data discretely and discriminatively, and a generative  modality-specific network to encode the image/text sample  to representative binary codes
We now present these networks and how to perform optimization in the proceeding  subsections
 Cross-Modal Fusion Network: Let X� = [x�N,x�N, ⋅ ⋅ ⋅ ,x�� ] ∈ ℝ��×� and X� = [x�N,x�N, ⋅ ⋅ ⋅ ,x�� ] ∈ ℝ  ��×� be the training sets  from different modalities, where � and � represent two  different modalities, � is the number of training samples in  each modality, and ℝ�� and ℝ�� are the feature dimension  for each sample in modalities � and �, respectively
Our  fusion network aims to transform the cross-modal sample  pair into a compact binary feature vector as follows:  ��,� : (ℝ �� ,ℝ�� ) → {−N, N}� (N)  where � is the length of the binary feature vector
Specifically, for image and text as the modality pairs, the fusion  network would comprise of convolution, pooling layers and  FC layers with parameters �� to process the images, and FC  layers with parameter �� to process the text data
To combine the output of two networks, we create a latent network  which composes of FC layers with parameters ��
The input and output of the latent layer would be as follows:  � = �(��(X�, ��) + ��(X�, ��)) (N)  h = ��(�, ��) (N)  where ��, �� and �� are the image, text and latent network  functions, respectively, and �(⋅) is the non-linear activation function
The output of the fusion network would then be  h ∈ ℝN×� 
We let the output for the whole training set of the fusion network be H ∈ ℝ�×� , the learned binary code matrix be B = [bN,bN, ⋅ ⋅ ⋅ ,b� ] ∈ {−N, N}�×� , the la- bel data be defined as Y = [yN,yN, ⋅ ⋅ ⋅ ,y� ] ∈ {N, 0}  �×�  where y�,� = N if the �-th sample belongs to class � and 0 otherwise, and a multi-class projection matrix be defined  as M = [mN,mN, ⋅ ⋅ ⋅ ,m� ] ∈ ℝ �×� 
We learn the binary  code and network parameters in a discrete manner such that  N0NN    we preserve the binary property and avoid the approximation loss caused by relaxation, but also learn discriminative  binary codes that are semantically correlated
This can be  done by the following optimization procedure:  min B,M,��,��,��  � = �N + ��N  = ∥M∥N� + �∑  �  �� + �(∥B−H∥ N � )  ∀�, � y�,�(m ⊤  � b�) ≥ N− ��  ∀� b� = {−N, N} (N)  where �N minimizes the multi-classification loss formed  from the hinge loss between the label information and binary code so that samples that are semantically relevant(irrelevant) have similar(dissimilar) binary codes as much  as possible
�N minimizes quantization loss between the  real-value code and binary code such that the energy of  the samples can be well-preserved in the hashing network
 Here, �� ≥ 0 is the slack variable and � is a constant pa- rameter to balance the effect of the two parameters
 The optimization problem in (N) is non-convex due to the  binary constraints, which makes it difficult to solve
However, it can be addressed using an iterative approach where  we keep other variables fixed and solve one alternatively  and iteratively
We learn the binary code, multi-class projection matrix and network parameters � = {��, ��, ��} as follows:  Update M with fixed B and �: We are left with a support  vector machine (SVM) formulation which can be solved  through a standard solverN to learn the classification matrix  M
 Update B with fixed M and �: We perform a discrete  optimization technique and simplify (N) as follows to learn  B:  min b�  �(b�) = − �∑  �=N  m⊤��,�b�  + �∥b� − h�∥ N �  subject to b� ∈ {−N, N} N×� (N)  (N) is a binary quadratic problem that can be solved through  a linear gradient technique similar to [NN]
We obtain a  closed-form solution as follows:  b� = sgn(y�M ⊤ + �h�) (N)  Update � with fixed M and B: We obtain the resulting formulation:  min � �(�) = �∥B−H∥N� (N)  Nwe use LibSVM: http://www.csie.ntu.edu.tw/ cjlin/libsvm/  Algorithm N: CMDVH - cross-modal fusion network  Input: Training set X� and X� , network learning  parameters, iterative number ����, objective  function parameter � and convergence error �
 Output: unified binary code matrix B Step N (Initialization):  N.N Initialize image, text and latent network parameters (see  Implementation details)  N.N Initialize binary code B, randomly and zero-centered
 Step N (Fusion Network and Binary Code Learning):  for � = N, N, ⋅ ⋅ ⋅ , ���� do - Compute H using the initial fusion network
 N.N (Classification Step):  - Obtain M by solving the SVM formulation in (N)
 N.N (Binary Code Learning Step):  - Obtain B according to (N)
 N.N (Hash Function Learning Step):  - Obtain the top-layer gradients according to (N)
 - Perform back propagation for the image, text and  latent network
 - Calculate �� using (N)
 If � > N and ∣�� − ��−N∣ < �
end  Return: B
 We employ the batch-wise gradient descent method to learn  parameters for the latent network and image/text networks
The gradient of � in (N) with respect to the neural code  representation are as follows:  ∂�  ∂H = −N�(B−H) (N)  For each layer of the network, the gradients can easily be  computed through the chain rule during backpropagation
 The parameters of the networks are updated using these  gradients based on a given learning rate, momentum and  weight decay
Algorithm N summarizes the detailed procedure of our the cross-modal fusion network of our CMDVH
 Modality-Specific Networks: After learning a representative binary code for the training cross-modal pairs  from a fusion network, we can now learn generative  modality-specific networks for encoding out-of-sample input
The aim of modality-specific networks is to directly  map each cross-modal sample pair into similar binary code  inferred from the fusion network as follows :  �� : ℝ �� → {−N, N}� , �� : ℝ  �� → {−N, N}� (N)  Inspired by the success of variational encoders [NN], we employ a probabilistic interpretation for the modality-specific  network to make it more general and suitable for out-ofsample extension
We assume that the output data is generated by a latent variable, z, sampled from a conditional distribution
Given data x∗� N, we assume that the latent  Nwhere ∗ = {�,�}
 N0N0    sample and binary code is generated as z∗� ∼ ��∗(z∗�) and b� ∼ ��∗(b�∣z∗�), respectively
Similar to [NN], we gener- ate a proposal distribution ��∗(z∗∣x∗�) to approximate the posterior distribution ��∗(z∗�∣x∗�) where we sample z∗� as follows:  z�∗� = �∗� + �∗� � � �  �� ∼ � (0, N) (N0)  where �� means the �-th sample of noise, � denotes elementwise multiplication, �∗� and �∗� would be the output of the  non-linear projection from network �(x∗�, �∗) with input x∗� and parameter �∗
From (N), we can have the proposal  distribution to be:  ��∗(z∗�∣x∗�) = � (z∗�∣�∗�, � N ∗�I) (NN)  We also assume that the prior over the latent variable is  centered by a multivariate gaussian ��∗(z∗) = � (z∗; 0, I)
From this assumption, we can derive the analytic form of  the Kullback-Liebler (KL) divergence as:  ���(��∗(z∗�∣x∗�)∣∣��∗(z∗�)) = N  N  �∑  �=N  (N + log((� (�) ∗� )  N  − (� (�) ∗� )  N − (� (�) ∗� )  N)(NN)  where � is the �-th element of � and �
The KL divergence  would act as a regularizer to the approximate posterior distribution
Finally, In order to ensure that the latent variable  produces binary codes similar to the learned codes in the  fusion network, we employ a probabilistic loss function in  the form of a log-likelihood loss as follows:  log �(� (�) � ∣�  (�) ∗� ) = log(N + �  � (�) �  � (�) ∗� ) (NN)  where � is the �-th bit of the binary code
From these approximations, the network learning formulation can then be  written as follows:  min �  ℒ = �∑  �=N  �∑  �=N  ℒ��� + �∑  �=N  �ℒ���  = �∑  �=N  �∑  �=N  − log(N + �� (�) �  � (�) ∗� ) (NN)  − �  N  �∑  �=N  �∑  �=N  (N + log((� (�) ∗� )  N − (� (�) ∗� )  N − (� (�) ∗� )  N)  ℒ��� ensures that the binary data likelihood under the ap- proximate posterior distribution is maximized
ℒ��� en- sures that the KL divergence between the proposed distribution and prior distribution for the latent variable is minimized
Finally, � is a constant parameter to balance the two  Algorithm N: CMDVH - modality-specific network  Input: Training set X� and X� with corresponding  binary code matrix B, network learning  parameters, iterative number ����, objective  function parameter �, and convergence error �
 Output: Network parameters �� and �� Step N (Initialization):  N.N Initialize modality-specific network parameters (see  Implementation details)  Step N (Modality-Specific Hashing Network Learning):  for ∗ = image (�), text (�) do for � = N, N, ⋅ ⋅ ⋅ , ���� do  N.N (Forward Propagation):  - Compute output of modality-specific network,  given input sample x∗
 - Split output to �∗ and �∗
 - Sample z∗ from (N0)
 N.N (Backward Propagation):  - Compute gradient of loss function (NN)
 - Perform gradient descent to learn �∗ end  Calculate ℒ� using (NN)
If � > N and ∣ℒ� − ℒ�−N∣ < �
 end  Return: {��, ��}
 loss terms
(NN) can be easily optimized by taking the gradient of the objective function and performing batch-wise  backpropagation
Algorithm N summarizes the detailed  procedure of the modality-specific networks of our CMDVH
 For new instances or query data, we simply use the  learned modality-specific networks to obtain the output  real-value codes and finally binarize them using the sign(⋅) function
During retrieval, given a text query (can be image), we extract the query binary code using the learned text  hashing network and obtain the most similar binary codes  from the gallery (learned B) which are indexed to retrieve  the most relevant images
 N
Experiments  We conducted experiments on three widely used datasets  to evaluate our CMDVH
The following describes the details of the experiments and results
 N.N
Datasets and Experimental Setup  Datasets: We employed three cross-modal datasets in  our experiments: Wiki, IAPRTCNN and NUS-WIDE
The  Wiki datasetN contains NNNN Wikipedia documents, where  each document contains a single image and a corresponding  text of at least N0 words
These documents are categorized  Nhttp://www.svcl.ucsd.edu/projects/crossmodal/
 N0NN    into N0 semantic classes, where each document is from one  class
Each text is represented by a N0-dimensional feature  vector which is computed from the latent Dirichlet Allocation (LDA) model
We randomly selected NN% documents  from this dataset as the database and the rest as query samples
 The IAPR TC-NN datasetN contains NNNNN images  with corresponding sentence descriptions
These imagesentence pairs present various semantics such as landscape,  action and people categories
Similar to [N], we use the top  NN frequent labels from the NNN concepts obtained generated from the segmentation taskN
For the text features, we  pre-process the sentence data removing the stop words and  extract a bag-of-words (BoW) representation with a dimension of N00
We randomly select N00 pairs per class as the  query set and the remaining data as the gallery set
Unlike  the Wiki where each image was associated with one category class, the images in IAPRTCNN may have more than one  label information
 The NUS-Wide datasetN contains NNNNNN images which  were annotated by NN concept tags
Following the same settings in previous works [NN, NN], we selected the N0 most  frequent concepts and constructed a subset which contains  NNNNNN images-tag pairs
Similar to the IAPRTCNN, each  image in the NUS-WIDE dataset is associated with multiple tags
In our experiments, each text is represented by a  N000-dimensional feature vector which is computed by the  bag-of-words model
We randomly selected NN% samples  to form the database and the rest as query samples
 Evaluation Metrics: For each dataset, we performed two cross-modal retrieval tasks: image-to-text retrieval and  text-to-image retrieval, which search texts by a query image and search images by a query text, respectively
We use  the mean average precision (mAP) [N,NN,NN] to measure the  performance of different retrieval methods, which is defined  as the mean of all queries’ average precision, �� , defined  as follows:  �� = N  �  �∑  �=N  ����(�) � ���(�) (NN)  where� is the number of relevant instances in the retrieved  set, ����(�) denotes the precision of the top � retrieved set, and ���(�) is an indicator of relevance of a given rank (which is set to N if relevant and 0 otherwise)
Here, we  consider two samples similar as long as there is at least  one similar label
In our experiments, we use � = N00 for the NUS-WIDE and Wiki dataset, and � = N00 for the IAPRTCNN
Generally, mAP measures the discriminative  learning ability of different cross-modal retrieval methods,  where a higher mAP indicates better retrieval performance
 Nhttp://imageclef/photodata
Nhttp://imageclef/SIAPRdata
Nhttp://lms.comp.nus.edu.sg/research/NUS-WIDE.htm  Because the IAPRTCNN and NUS-WIDE dataset have  multiple labels for each sample, it is important that a ranking metric is also evaluated
Hence, we also evaluate the  Normalized Discounted Cumulative Gain (NDCG), and Average Cumulative Gain (ACG)
For a given query sample  x� , these criterions are defined as follows:  ����@� = N  �  �∑  �=N  N�� − N  log(N + �) (NN)  ���@� = N  �  �∑  �=N  �� (NN)  where � is the normalized constant, �� is the similarity level  of the �th sample, and � is the number of retrieved samples  in the ranking list
�� represents a ranking level valued � is  the query and �-th sample in gallery share � similar labels,  and valued zero if they do not share any label
The NDCG  evaluates the ranking by penalizing errors in higher ranked  items more strongly, while ACG takes the average of the  similarity levels of data within the retrieved samples
 Implementation Details: Our deep architecture and experiments were implemented under the MatConvNet [NN]  framework
For the fusion network, the image hashing  network used the pre-trained CNN-F from [N] as our initial convolution and pooling layers up to FCN, and stack  a number of new FC layers with dimensions of [N0NN → N00 → N00] for all datasets, while the text hashing net- work is designed with fully-connected networks and use  the pre-processed text features, given by each experiment, as input
We set the FC layers as [N0 → N00 → N00], [NNNN → N00 → N00], and [N000 → N00 → N00], for the Wiki, IAPRTCNN, and NUS-WIDE dataset, respectively
For the latent network which fuses the output of image  and text network, we used FC layers with dimensions of  [N00 → N00 → �]
For the modality-specific network- s, we use the similar image and text networks except that  a the top FC layer would have a size of N × � because of the splitting done during latent variable sampling
We perform end-to-end learning by having the learning rate at the  new fully connected layers to be 0.0N
To avoid overfitting and ruining the representative abstract features already  learned during the pre-training, we reduce the learning rate  of the remaining convolution and FC layers to be 0.000N
 For both image and text network, we used the ReLU activationN as the nonlinear activation function for the new fully  connected layers except for the last layer
We use the hyperbolic tangent (tanh) function for the top layer of the latent  network because it is able to squeeze the representation to  a {-N,N} range which ensures that the quantization loss can be reduced as much as possible
The parameters in the new  fully connected layers are initialized using the Xavier iniNReLU is a nonlinear transformation f(x) = max(0, x) [N0]  N0NN    Table N
mAP performance of different cross-modal hashing methods on different datasets, where images were used as query samples and  texts/tags were employed as gallery samples, respectively
 Wiki IAPRTCNN NUS-WIDE  Method NN bits NN bits NN bits NNN bits NN bits NN bits NN bits NNN bits NN bits NN bits NN bits NNN bits  CVH [NN] 0.NNNN 0.N0NN 0.NNNN 0.NNN0 0.NNN0 0.NN0N 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNNN 0.NNNN  CCA-ITQ [N] 0.NNNN 0.NNNN 0.N0NN 0.NNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NN00 0.NNN0 0.NNNN 0.NNNN  PDH [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.NN0N 0.NNN0 0.NNNN 0.NNNN 0.NNNN 0.NNNN  LSSH [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNN0 0.NNN0 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNN0 0.NNNN  CMFH [N] 0.NNNN 0.NNNN 0.NNNN 0.NNN0 0.NN0N 0.NNNN 0.N0NN 0.NNNN 0.NNNN 0.NN0N 0.NNNN 0.NNNN  SCM [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNNN 0.NNN0 0.NNNN 0.NNNN 0.NNNN  SePH - km [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NN00 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  DisCMH [NN] 0.NNNN 0.NNNN 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NN0N  CMDVH 0.NNNN 0.NNN0 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N00N 0.NN0N 0.NN0N 0.NNNN 0.NN0N 0.NNN0  Table N
mAP performance of different cross-modal hashing methods on different datasets, where texts/tags were used as query samples  and images were employed as gallery samples, respectively
 Wiki IAPRTCNN NUS-WIDE  Method NN bits NN bits NN bits NNN bits NN bits NN bits NN bits NNN bits NN bits NN bits NN bits NNN bits  CVH [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNN0 0.NNNN 0.NNN0 0.NNNN 0.NNNN 0.NN0N  CCA-ITQ [N] 0.NNNN 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNNN 0.NN0N 0.NNN0  PDH [NN] 0.NNNN 0.NNNN 0.NNNNN 0.NNNNN 0.NNN0 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NN0N 0.NNNN 0.N0NN  LSSH [NN] 0.N0NN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  CMFH [N] 0.NNNN 0.NN0N 0.NNNN 0.NN0N 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NN0N  SCM [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNN0  SePH - km [NN] 0.N00N 0.NNNN 0.N0NN 0.NNNN 0.NN0N 0.NNN0 0.NN0N 0.NNN0 0.NN0N 0.NNNN 0.N0NN 0.N0NN  DisCMH [NN] 0.NNNN 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.NNN0 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  CMDVH 0.NNN0 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNN0 0.NNNN 0.NN0N 0.NNNN  Table N
mAP performance of different deep cross-modal hashing  methods on different datasets
 IAPRTCNN  Method NN bits NN bits NN bits NNN bits  � → � DNH-C [NN] 0.NNN0 0.NNNN 0.NN0N 0.NNNN  DVSH [N] 0.NNNN 0.NNNN 0.NNNN 0.NNNN  CMDVH 0.NNNN 0.NNNN 0.N00N 0.NN0N  � → � DNH-C [NN] 0.NNNN 0.NNNN 0.NN0N 0.N0NN  DVSH [N] 0.N0NN 0.NNNN 0.NN0N 0.NNNN  CMDVH 0.NNNN 0.NNNN 0.N0NN 0.NNNN  NUSWIDE  Method NN bits NN bits NN bits NNN bits  � → � CAH [N] 0.NNN0 0.N0NN 0.NN0N 0.NNNN  DCMH [N0] 0.NNNN 0.NNNN 0.NNN0 CMDVH 0.NN0N 0.NNNN 0.NN0N 0.NNN0  � → � CAH [N] 0.N0NN 0.NNNN 0.NNNN 0.NN00  DCMH [N0] 0.NNNN 0.NNNN 0.NN0N CMDVH 0.NNN0 0.NNNN 0.NN0N 0.NNNN  tialization [N]N
The momentum, and weight decay were set  to 0.N, and 0.000N, respectively
In our experiments, the  parameters �N and � were set to 0.N and 0.N, respectively,  which were obtained by cross-validation on the Wiki dataset  using NN bits
 N.N
Experimental Results  Comparisons with State-of-the-art Cross-Modal  Hashing Methods: We compared our CMDVH with the  different state-of-the-art cross-modal hashing methods  which can be grouped to unsupervised (CVH, PDH,  N W = �  [  − √  N  ���+���� , √  N  ���+����  ]  where W ∈ ℝ���×����  CCA-ITQ, LSSH, CMFH) and supervised (SCM, SePH,  DisCMH).N To have a fair comparison because they are  shallow methods, we make use of CNN features extracted  at the FCN layer for the images from the pre-trained model  initially used by our CMDH method
Also, to maximize  the learning potential of each dataset, we made use of  the gallery samples as training data to learn the hashing  functions
During retrieval, methods that employ unified  binary code learning (LSSH, CMFH, SePH, DisCMH)  similar to CMDH use the learned binary code as gallery  set, while other methods (CVH, PDH, CCA-ITQ, SCM)  use the learned hash function to obtain the binary codes for  the gallery set
Tables N and N show the mAP performance  by Hamming Ranking
It can be observed that our method  provided the best performance compared to the shallow  cross-modal hashing methods
This may be because our  DCNN model captured the nonlinearities of the raw data  due to several nonlinear transformations
Although SePH  also performed nonlinear transformations, it was done  explicitly through kernels which cannot really maximize  the information from raw data
The DisCMH method gave  competitive results with our CMHN method at lower bits,  but did not consistently improve as the bit size increased
 This may be because it performed linear projection which  may have limited the binary code mapping
In addition,  a larger performance gap can be seen in the IARPTCNN  and NUSWIDE experiments most probably due to larger  NAuthors provided their codes except for DisCMH in which we implemented ourselves
 N0NN    NN NN NN NNN 0.NN  0.N  0.NN  0.N  0.NN  0.N  Number of bits  N D  C G   @  N  0 0        CMFH  LSSH  SePH  DisCMH  CMDVH  (a) IAPRTCNN (� → � )  NN NN NN NNN  0.N  0.N  0.N  0.N  0.N  Number of bits  N D  C G   @  N  0 0        CMFH  LSSH  SePH  DisCMH  CMDVH  (b) IAPRTCNN (� → �)  NN NN NN NNN 0.NN  0.N  0.NN  0.N  0.NN  0.N  Number of bits  N D  C G   @  N  0 0        CMFH  LSSH  SePH  DisCMH  CMDVH  (c) NUSWIDE (� → � )  NN NN NN NNN 0.N  0.N  0.N  0.N  0.N  Number of bits  N D  C G   @  N  0 0        CMFH  LSSH  SePH  DisCMH  CMDVH  (d) NUSWIDE (� → �)  Figure N
NDCG performance of different cross-modal hashing methods for the IAPRTCNN and NUSWIDE database
 NN NN NN NNN  0.N  0.N  N  N.N  Number of bits  A C  G  @   N 0 0        CMFH  LSSH  SePH  DisCMH  CMDVH  (a) IAPRTCNN (� → � )  NN NN NN NNN  0.N  0.N  N  N.N  Number of bits  A C  G  @   N 0 0        CMFH  LSSH  SePH  DisCMH  CMDVH  (b) IAPRTCNN (� → �)  NN NN NN NNN  0.N  0.N  0.N  0.N  N  N.N  Number of bits  A C  G  @   N 0 0        CMFH  LSSH  SePH  DisCMH  CMDVH  (c) NUSWIDE (� → � )  NN NN NN NNN 0.N  0.N  0.N  0.N  N  N.N  N.N  Number of bits  A C  G  @   N 0 0        CMFH  LSSH  SePH  DisCMH  CMDVH  (d) NUSWIDE (� → �)  Figure N
ACG performance of different cross-modal hashing methods for the IAPRTCNN and NUSWIDE database
 training data which hashing network training fully utilized
 Figures N- N show the NDCG and ACG performance
 Unlike other methods that gave the same weight if samples  have at least one similar label between them during training, it can be seen that our method shows the best results by  a large margin which shows that our method addressed the  ranking problem well by exploiting the label information  fully
 Comparisons with Current Deep Cross-Modal Hashing Methods: We also compared our method with current deep cross-modal hashing methods as shown in Table N.N0 It can be seen, that our model gave best results,  using the shared binary code as gallery for the two benchmark datasets
This may be due to several reasons; First,  the CAH method still used handcrafted image features as  input for their deep networks while our method performed  a complete network learning from raw images
Second, the  DCMH method performed end-to-end learning but exploited the label information directly to the neural code output of the hash networks, and not the binary code which  may have lead to some approximation loss
Finally, DVSH  and DNH-C both performed end-to-end supervised metricbased network training in the form of cosine hinge loss and  triplet ranking loss, respectively, which may not fully obtain  discriminative binary codes compared to our classificationbased hinge loss learning
 Empirical Analysis: We also investigated variants of  our CMDVH method to see the importance of each aspect  of our architecture and learning method
CMDVHN ignores  the latent network in the cross-modal fusion network which  assumes that simply combining the outputs of the image  N0Results are obtained from the respective author’s papers
We used the  same experimental setup as mentioned in their papers
 Table N
mAP performance of different variants of our CMDVH  method on the NUS-WIDE dataset
 Method NN bits NN bits NN bits NNN bits  � → � CMDVHN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  CMDVHN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  CMDVH 0.NN0N 0.NNNN 0.NN0N 0.NNN0  � → � CMDVHN 0.NNN0 0.NN0N 0.NNNN 0.NN0N  CMDVHN 0.NNNN 0.NNN0 0.NNNN 0.NNNN  CMDVH 0.NNN0 0.NNNN 0.NN0N 0.NNNN  and text network would be representative enough for binary  code inference
CMDVHN ignores the probabilistic interpretation of the modality-specific network and simply learn  the binary codes from a negative log likelihood loss
Table N  shows the performance of these variants on the NUS-WIDE  database
We see that a fusion network is still important to  perform the nonlinear transformation to make the learned  codes more representative
 N
Conclusion  In this paper, we have proposed a cross-modal deep  variational hashing (CMDVH) for scalable multimedia retrieval
Our method learns a fusion network to learn binary codes from cross-modal training pairs which exploits  class label information, which learn a generative modalityspecific hash network for the out-of-sample extension
Experimental results on three multimedia retrieval datasets  have shown the effectiveness of the proposed approach
 Acknowledgements  This work was supported in part by the National Key Research and Development Program of China under Grant N0NNYFBN00N00N and the National Natural Science FounN0NN    dation of China under Grant NNNNNN0N
This research was also carried out at the Rapid-Rich Object Search (ROSE) Lab at the Nanyang Technological University, Singapore
The ROSE Lab is supported by the Infocomm Media Devel- opment Authority, Singapore  References  [N] M
M
Bronstein, A
M
Bronstein, F
Michel, and N
Paragios
Data fusion through cross-modality metric learning using similarity-sensitive hashing
In CVPR, pages NNNN–NN0N,  N0N0
N, N, N  [N] Y
Cao, M
Long, J
Wang, Q
Yang, and P
S
Yu
Deep  visual-semantic hashing for cross-modal retrieval
In KDD,  pages N–N0, N0NN
N, N, N  [N] Y
Cao, M
Long, J
Wang, and H
Zhu
Correlation autoencoder hashing for supervised cross-modal search
In ICMR,  pages NNN–N0N, N0NN
N, N  [N] K
Chatfield, K
Simonyan, A
Vedaldi, and A
Zisserman
 Return of the devil in the details: Delving deep into convolutional nets
In BMVC, N0NN
N  [N] G
Ding, Y
Guo, and J
Zhou
Collective matrix factorization  hashing for multimodal data
In CVPR, pages N0NN–N0N0,  N0NN
N, N, N  [N] X
Glorot and Y
Bengio
Understanding the difficulty of  training deep feedforward neural networks
In ICAIS, pages  NNN–NNN, N0N0
N  [N] Y
Gong, Q
Ke, M
Isard, and S
Lazebnik
A multi-view embedding space for modeling internet images, tags, and their  semantics
IJCV, N0N(N):NN0–NNN, N0NN
N  [N] Y
Gong and S
Lazebnik
Iterative quantization: A procrustean approach to learning binary codes
In CVPR, pages  NNN–NNN, N0NN
N, N  [N] K
Jiang, Q
Que, and B
Kulis
Revisiting kernelized  locality-sensitive hashing for improved large-scale image retrieval
In CVPR, pages N–N0, N0NN
N  [N0] Q.-Y
Jiang and W.-J
Li
Deep cross-modal hashing
arXiv  preprint arXiv:NN0N.0NNNN, pages N–NN, N0NN
N, N  [NN] C
Kang, S
Xiang, S
Liao, C
Xu, and C
Pan
Learning  consistent feature representation for cross-modal multimedia  retrieval
TMM, NN(N):NN0–NNN, N0NN
N  [NN] D
P
Kingma and M
Welling
Auto-encoding variational  bayes
In ICLR, pages N–NN, N0NN
N, N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, pages N0NN–NN0N, N0NN
N  [NN] S
Kumar and R
Udupa
Learning hash functions for crossview similarity search
In IJCAI, volume NN, pages NNN0 –  NNNN, N0NN
N, N, N  [NN] H
Lai, Y
Pan, Y
Liu, and S
Yan
Simultaneous feature  learning and hash coding with deep neural networks
In  CVPR, pages NNN0–NNNN, N0NN
N  [NN] C
Leng, J
Wu, J
Cheng, X
Bai, and H
Lu
Online sketching hashing
In CVPR, pages NN0N–NNNN, N0NN
N  [NN] Z
Lin, G
Ding, M
Hu, and J
Wang
Semantics-preserving  hashing for cross-view retrieval
In CVPR, pages NNNN–NNNN,  N0NN
N, N, N  [NN] W
Liu, C
Mu, S
Kumar, and S.-F
Chang
Discrete graph  hashing
In NIPS, pages NNNN–NNNN, N0NN
N  [NN] J
Masci, M
M
Bronstein, A
M
Bronstein, and J
Schmidhuber
Multimodal similarity-preserving hashing
TPAMI,  NN(N):NNN–NN0, N0NN
N  [N0] V
Nair and G
E
Hinton
Rectified linear units improve  restricted boltzmann machines
In ICML, pages N0N–NNN,  N0N0
N  [NN] J
Ngiam, A
Khosla, M
Kim, J
Nam, H
Lee, and A
Y
Ng
 Multimodal deep learning
In ICML, pages NNN–NNN, N0NN
 N  [NN] N
Rasiwasia, J
Costa Pereira, E
Coviello, G
Doyle, G
R
 Lanckriet, R
Levy, and N
Vasconcelos
A new approach to  cross-modal multimedia retrieval
In ACM MM, pages NNN–  NN0, N0N0
N  [NN] M
Rastegari, J
Choi, S
Fakhraei, D
Hal, and L
Davis
 Predictable dual-view hashing
In ICML, pages NNNN–NNNN,  N0NN
N, N  [NN] F
Shen, C
Shen, W
Liu, and H
Shen
Supervised discrete  hashing
In CVPR, pages NN–NN, N0NN
N  [NN] C
Szegedy, A
Toshev, and D
Erhan
Deep neural networks  for object detection
In NIPS, pages NNNN–NNNN, N0NN
N  [NN] A
Vedaldi and K
Lenc
Matconvnet: Convolutional neural  networks for matlab
In ACM MM, pages NNN–NNN, N0NN
N  [NN] C
Wang, H
Yang, and C
Meinel
A deep semantic framework for multimodal representation learning
Multimedia  Tools and Applications, pages N–NN, N0NN
N  [NN] D
Wang, X
Gao, X
Wang, and L
He
Semantic topic multimodal hashing for cross-media retrieval
In IJCAI, pages  NNN0–NNNN, N0NN
N  [NN] J
Wang, S
Kumar, and S.-F
Chang
Semi-supervised hashing for large-scale search
TPAMI, NN(NN):NNNN–NN0N, N0NN
 N  [N0] K
Wang, R
He, L
Wang, W
Wang, and T
Tan
Joint feature  selection and subspace learning for cross-modal retrieval
TPAMI, NN(N0):N0N0–N0NN, N0NN
N  [NN] K
Wang, R
He, W
Wang, L
Wang, and T
Tan
Learning  coupled feature spaces for cross-modal matching
In ICCV,  pages N0NN–N0NN, N0NN
N  [NN] W
Wang, X
Yang, B
C
Ooi, D
Zhang, and Y
Zhuang
 Effective deep learning-based multi-modal retrieval
VLDB,  NN(N):NN–N0N, N0NN
N  [NN] Y
Wei, Y
Zhao, C
Lu, S
Wei, L
Liu, Z
Zhu, and S
Yan
 Cross-modal retrieval with cnn visual features: A new baseline
TSCVT, NN(N):NNN–NN0, N0NN
N  [NN] Y
Weiss, A
Torralba, and R
Fergus
Spectral hashing
In  NIPS, pages NNNN–NNN0, N00N
N  [NN] X
Xu, F
Shen, Y
Yang, and H
T
Shen
Discriminant crossmodal hashing
In ICMR, pages N0N–N0N, N0NN
N, N, N  [NN] D
Zhang and W.-J
Li
Large-scale supervised multimodal  hashing with semantic correlation maximization
In AAAI,  pages NNNN–NNNN, N0NN
N, N, N  [NN] J
Zhou, G
Ding, and Y
Guo
Latent semantic sparse hashing for cross-modal similarity search
In ACM SIGIR, pages  NNN–NNN, N0NN
N, N, N  [NN] X
Zhu, Z
Huang, H
T
Shen, and X
Zhao
Linear crossmodal hashing for efficient multimedia search
In ACM MM,  pages NNN–NNN, N0NN
N  N0NNSituation Recognition With Graph Neural Networks   Situation Recognition with Graph Neural Networks  Ruiyu LiN, Makarand TapaswiN, Renjie LiaoN, Jiaya JiaN,N, Raquel UrtasunN,N,N, Sanja FidlerN,N  NThe Chinese University of Hong Kong, NUniversity of Toronto, NYoutu Lab, Tencent NUber Advanced Technologies Group, NVector Institute  ryli@cse.cuhk.edu.hk, {makarand,rjliao,urtasun,fidler}@cs.toronto.edu, leojiaN@gmail.com  Abstract  We address the problem of recognizing situations in images
Given an image, the task is to predict the most  salient verb (action), and fill its semantic roles such as  who is performing the action, what is the source and target of the action, etc
Different verbs have different roles  (e.g
attacking has weapon), and each role can take on  many possible values (nouns)
We propose a model based  on Graph Neural Networks that allows us to efficiently  capture joint dependencies between roles using neural networks defined on a graph
Experiments with different graph  connectivities show that our approach that propagates information between roles significantly outperforms existing  work, as well as multiple baselines
We obtain roughly NN% improvement over previous work in predicting the full  situation
We also provide a thorough qualitative analysis  of our model and influence of different roles in the verbs
 N
Introduction  Object [NN, NN, NN], action [NN, N0], and scene classification [N0, NN] have come a long way, with performance  in some of these tasks almost reaching human agreement
 However, in many real world applications such as robotics  we need a much more detailed understanding of the scene
 For example, knowing that an image depicts a repairing  action is not sufficient to understand what is really happening in the scene
We thus need additional information such  as the person repairing the house, and the tool that is used
 Several datasets have recently been collected for such  detailed understanding [NN, NN, NN]
In [NN], the Visual  Genome dataset was built containing detailed relationships  between objects
A subset of the scenes were further annotated with scene graphs [NN] to capture both unary (e.g
attributes) and pairwise (e.g
relative spatial info) object relationships
Recently, Yatskar et al
[NN] extended this idea  to actions by labeling action frames where a frame consists  of a fixed set of roles that define the action
Fig
N shows a  frame for action repairing
The challenge then consists  VERB:  REPAIRING  PROBLEM  Part  Pipe  Wheel⋯ ITEM  AC  Wall  Car⋯ TOOL  Screwdriver  Wrench  Knife⋯ PLACE  Outdoors  Bathroom  Field⋯ AGENT  Worker  Plumber  Soldier⋯ Figure N
Understanding an image involves more than just predicting the most salient action
We need to know who is performing  this action, what tools (s)he may be using, etc
Situation recognition is a structured prediction task that aims to predict the verb  and its frame that consists of multiple role-noun pairs
The figure  shows a glimpse of our model that uses a graph to model dependencies between the verb and its roles
 of assigning values (nouns) to these roles based on the image content
The number of different role types, their possible values, as well as the number of actions are very large,  making it a very challenging prediction task
As shown in  Fig
N, the same verb can appear in very different image  contexts, and nouns that fill the roles are vastly different
 In [NN], the authors proposed a Conditional Random  Field (CRF) to model dependencies between verb-rolenoun pairs
In particular, a neural network was trained in  an end-to-end fashion to both, predict the unary potentials  for verbs and nouns, and to perform inference in the CRF
 While their model captured the dependency between the  verb and role-noun pairs, dependencies between the roles  were not modeled explicitly
 In this paper, we aim to jointly reason about verbs and  their roles using a Graph Neural Network (GNN), a generalization of graphical models to neural networks
A GNN  defines observation and output at each node in the graph,  NNNNN    RIDING  ROLE AGENT VEHICLE PLACE ROLE AGENT VEHICLE PLACE  VALUE MAN HORSE OUTSIDE VALUE DOG SKATEBOARD SIDEWALK  Figure N
Images corresponding to the same verb can be quite different in their content involving verb roles
This makes situation  recognition difficult
 and propagates messages along the edges in a recurrent  manner
In particular, we exploit the GNNs to also model  dependencies between roles and predict a consistent structured output
We explore different connectivity structures  among the role nodes, and show that our approach significantly improves performance over existing work
In addition, we compare with strong baseline methods using Recurrent Neural Networks (RNNs) that have been shown to  work well on joint prediction tasks, such as semantic [NN]  and object instance [N] segmentation, as well as on group  activity recognition [N]
We also visualize the learned models to further investigate dependencies between roles
 N
Related Work  Situation recognition generalizes action recognition to  include actors, objects, and location in the activity
There  has been work to combine activity recognition with scene  or object labels [N, NN, NN, NN]
In [NN, NN], visual semantic  role labeling tasks were proposed where datasets are built to  study action along with localization of people and objects
 In another line of work, Yatskar et al
[NN] created the imSitu  dataset that uses linguistic resources from FrameNet [N0]  and WordNet [NN] to associate images not only with verbs,  but also with specific role-noun pairs that describe the verb  with more details
As a baseline approach, in [NN], a Conditional Random Field (CRF) jointly models prediction of  the verb and verb-role-noun triplets
Further, considering  that the large output space and sparse training data could be  problematic, a tensor composition function was used [NN]  to share nouns across different roles
The authors also proposed to augment the training data by searching images using query phrases built from the structured situation
 Different from these methods, our work focuses on explicitly modeling dependencies between roles for each verb  through the use of different neural architectures
 Understanding Images
There is a surge of interest in  joint vision and language tasks in recent years
Visual Question Answering in images and videos [N, NN] aims to answer  questions related to image or video content
In image captioning [NN, NN, NN, NN], a natural language sentence is generated to describe the image
Approaches for these tasks  often use the CNN-RNN pipelines to provide a caption, or  a correct answer to a specific question
Dependencies between verbs and nouns are typically being implicitly learned  with the RNN
An alternative is to list all important objects  with their attributes and relationships
Johnson et al
[NN]  created scene graphs, which are being used for visual relationship detection [NN, N0, NN] tasks
In [NN], the authors  exploit scene graphs to generate image captions
 In Natural Language Processing (NLP), semantic role  labeling [NN, NN, N0, NN, NN, NN] involves annotating a  sentence with thematic or semantic roles
Building upon  resources from NLP, and leveraging collections such as  FrameNet [N0] and WordNet [NN], visual semantic role labeling, or situation recognition, aims to interpret details for  one particular action with verb-role-noun pairs
 Graph Neural Networks
There are a few different ways  for applying neural networks to graph-structured data
We  divide them into two categories
The first group defines convolutions on graphs
Approaches like [N, N, NN] utilized the  graph Laplacian and applied CNNs to spectral domain
Differently, Duvenaud et al
[N] designed a special hash function such that a CNN can be used on the original graphs
 The second group applies feed-forward neural networks  to every node of the graph recurrently
Information is propagated through the network by dynamically updating the  hidden state of each node based on their history and incoming messages from their neighborhood
The Graph Neural  Network (GNN) proposed by [NN] utilized multi-layer perceptrons (MLP) to update the hidden state
However, their  learning algorithm is restrictive due to the contraction map  assumption
In the following work, the Gated Graph Neural  Network (GGNN) [NN] used a recurrent gating function [N]  to perform the update, and effectively learned model parameters using back-propagation through time (BPTT)
 Other work [NN, NN] designed special update functions  based on the LSTM [NN] cell and applied the model to treestructured or general graph data
In [NN], knowledge graphs  and GGNNs are used for image classification
Here we use  GGNNs for situation recognition
 N
Graph-based Neural Models for Situation  Recognition  Task Definition
Situation recognition as per the imSitu  dataset [NN] assumes a discrete set of verbs V , nouns N , roles R, and frames F 
The verb and its corresponding frame that contains roles are obtained from FrameNet [N0],  while nouns come from WordNet [NN]
Each verb v ∈ V is associated with a frame f ∈ F that contains a set of seman- tic roles Ef 
Each role e ∈ Ef is paired with a noun value  NNNN    WHEELING  CARRIER  AGENT PLACE  ITEM  FARMING  FARMER  ITEM  PLACETOOL  GROUND  Figure N
The architecture of fully-connected roles GGNN
The  undirected edges between all roles of a verb-frame allows to fully  capture the dependencies between them
 ne ∈ N ∪{∅}
Here, ∅ indicates that the noun is unknown or not applicable
A set of semantic roles and their nouns is  called a realized frame, denoted as Rf = {(e, ne) : e ∈ Ef},  where each role is with a noun
 Given an image, the task is to predict the structured  situation S = (v,Rf ), specified by a verb v ∈ V and its corresponding realized frame Rf 
For example, as  shown on the right of Fig
N, the verb riding is associated with three role-noun pairs, i.e., {agent:dog,  vehicle:surfboard, place:sidewalk}
 N.N
Graph Neural Network  The verb and semantic roles of a situation depend on  each other
For example, in the verb carrying, the roles  agent and agent-part are tightly linked with the item  being carried
Small items can be carried by hand, while  heavy items may be carried on the back
We propose modeling these dependencies through a graph G = (A,B)
The nodes in our graph a ∈ A are of two types of verb or role, and take unique values of V or N , respectively
Since each image in the dataset is associated with one unique verb,  every graph has a single verb node
Edges in the graph  b = (a′, a) encode dependencies between role-role or verb- role pairs, and can be directed or undirected
Fig
N shows  an example of such a graph where verb and role nodes are  connected to each other
 Background
Modeling structure and learning representation on graphs have prior work
Gated Graph Neural Networks (GGNNs) [NN] is one approach that learns the representation of a graph, which is then used to predict nodeor graph-level output
Each node of a GGNN is associated  with a hidden state vector that is updated in a recurrent fashion
At each time step, the hidden state of a node is updated  based on its history and incoming messages from its neighbors
These updates are applied simultaneously to all nodes  in the graph at each propagation step
The hidden states after T propagation steps are used to predict the output
In  contrast, a standard unrolled RNN only moves information  in one direction and updates one “node” per time step
 GGNN for Situation Recognition
We adopt the GGNN  framework to recognize situations in images
Each image i  is associated with one verb v that corresponds to a frame f  with a set of roles Ef 
We instantiate a graph Gf for each image that consists of one verb node, and |Ef | (number of roles associated with the frame) role nodes
To capture the  dependency between roles to the full extent, we propose creating undirected edges between all pairs of roles
Fig
N  shows two example graph structures of this type
We explore other edge configurations in the evaluation
 To initialize the hidden states for each node, we use features derived from the image
In particular, for every image  i, we compute representations φv(i) and φn(i) using the penultimate fully-connected layer of two convolutional neural network (CNN) pre-trained to predict verbs and nouns,  respectively
We initialize the hidden states h ∈ RD of the verb node av and role node ae as  h0av = g(Wivφv(i)) (N)  h0ae = g(Winφn(i)⊙Wee⊙Wv v̂) , (N)  where v̂ ∈ {0, N}|V| corresponds to a one-hot encoding of the predicted verb and e ∈ {0, N}|R| is a one-hot encoding of the role that the node ae corresponds to
Wv ∈ R  D×|V|  is the verb embedding matrix, and We ∈ R D×|R| is the  role embedding matrix
Wiv and Win are parameters that  transform image features to the space of hidden representations
⊙ corresponds to element-wise multiplication, and g(·) is a non-linear function such as tanh(·) or ReLU (g(x) = max(0, x))
We normalize the initialized hidden states to unit-norm prior to propagation
 For any node a, at each time step, the aggregation of  incoming messages at time t is determined by the hidden  states of its neighbors a′:  xta = ∑  (a′,a)∈B  Wph t−N a′ + bp 
(N)  Note that we use a shared linear layer of weights Wp and  biases bp to compute incoming messages across all nodes
 After aggregating the messages, the hidden state of the  node is updated through a gating mechanism similar to the  Gated Recurrent Unit [N, NN] as follows:  zta = σ(Wzx t a + Uzh  t−N a + bz) ,  rta = σ(Wrx t a + Urh  t−N a + br) ,  h̃ta = tanh(Whx (t) a + Uh(r  t a ⊙ h  t−N a ) + bh) ,  hta = (N− z t a)⊙ h  t−N a + z  t a ⊙ h̃  t a 
(N)  This allows each node to softly combine the influence of the  aggregated incoming message and its own memory
Wz ,  Uz , bz , Wr, Ur, br, Wh, Uh, and bh are the weights and  biases of the update function
 Output and Learning
We run T propagation steps
After propagation, we extract node-level outputs from GGNN  NNNN    image  feature  h0 hN hN hN  VEHICLEAGENTPLACE  …  OUTSIDE MAN BIKERIDING  Figure N
The architecture of chain RNN for verb riding
The  time-steps at which different roles are predicted needs to be decided manually, and has an influence on the performance
 to predict the verb and nouns
Specifically, for each image,  we predict the verb and a set of nouns for each role associated with the verb frame using a softmax layer:  pv = σ(Whvhav + bhv) (N)  pe:n = σ(Whnhae + bhn) 
(N)  Note that the softmax function σ is applied across the class  space for verbs V and nouns N 
pe:n can be treated as the probability of assigning noun n to role e
 Each image i in the imSitu dataset comes with three sets  of annotations (from three annotators) for the nouns
During  training, we accumulate the cross-entropy loss at verb and  noun nodes for every annotation as  L = ∑  i  N ∑  j=N  (  yv log(pv) + N  |Ef |  ∑  e  ye:n log(pe:n) )  , (N)  where yv and ye:n correspond to the ground-truth verb for  image i and the ground-truth noun for role e of the image,  respectively
Different to the Soft-OR loss in [NN], we encourage the model to predict all three annotations for each  image
We use back-propagation through time (BPTT) [NN]  to train the model
 Inference
At test time, our approach first predicts the  verb v̂ = arg maxv pv to choose a corresponding frame f and obtain the set of associated roles Ef 
We then propagate information among role nodes and choose the highest  scoring noun n̂e = arg maxn pe:n for each role
Thus our predicted situation is  Ŝ = (v̂, {(e, n̂e) : e ∈ Ef}) 
(N)  To reduce reliance on the quality of verb prediction, we explore beam search over verbs as discussed in Experiments
 N.N
Simpler Graph Architectures  An alternative to model dependencies between nodes is  to use recurrent neural networks (RNN)
Here, situation  recognition can be considered as a sequential prediction  problem of choosing the verb and corresponding noun-role  pairs
The hidden state of the RNN carries information  across the verb and noun-role pairs, and the input at each  time-step dictates what the RNN should predict
 FASTENING EDUCATING  AGENT PLACE  DESTINATIONITEMTOOL CONNECTOR  TEACHER  STUDENT SUBJECT  PLACE  Figure N
The architecture of tree-structured RNN
Like the Chain  RNN, verb prediction is at the root of the tree, and semantic roles  agent-like and place are parents of all other roles
 Chain RNN
An unrolled RNN can be seen as a special  case of a GGNN, where nodes form a chain with directed  edges between them
However, there are a few notable differences, wherein the nodes receive information only once  from their (left) neighbor
In addition, the nodes do not perform T steps of propagation among each other and predict  output immediately after the information arrives
 In the standard chain structure of a RNN, we need to  manually specify the order of the verb and roles
As the  choice of the verb dictates the set of roles in the frame, we  predict the verb at the first time step
We observe that the  imSitu dataset and any verb-frame in general, commonly  consist of place and agent-like roles (e.g
semantic role  teacher can be considered as the agent for the verb  teaching)
We thus predict place and agent roles  as the second and third roles in the chain N
We make all  other roles for the frame to follow subsequently in descending order of the number of times they occur across all verbframes
Fig
N shows an example of such a model
 For a fair comparison to the fully connected roles  GGNN, we employ the GRU update in our RNN
The input to the hidden states matches node initialization (Eqs
N  and N)
We follow the same scheme for predicting the output (linear layer with softmax), and train the model with the  same cross-entropy loss
 Tree-structured RNN
As mentioned above, the place  and agent semantic roles occur more frequently
We propose a structure where they have a larger chance to influence prediction of other roles
In particular, we create a  tree-structured RNN [NN] where the hidden states first predict the verb, followed by agent and place, and all other  roles
Fig
N shows examples of resulting structures
The tree-structured RNN can be deemed as a special case  of GGNN, where nodes have the following directed edges:  B = {(av, a ′) : a′ ∈ Z} ∪ {(a′, a) : a′ ∈ Z, a ∈ Ef\Z} , (N)  where Z = {agent,place}, and Ef\Z represents all roles in that frame other than agent and place
Similar  to the chain RNN, we use GRU update and follow the same  learning and inference procedures
 NPredicting place requires a more global view of the image compared  to agent
Changing the order to verb → agent → place → 


results in N.N% drop of performance
 NNNN    Method top-N predicted verb top-N predicted verbs ground truth verbs  verb value value-all verb value value-all value value-all mean  N Unaries NN.NN NN.NN NN.NN NN.NN NN.NN N0.NN NN.NN NN.NN NN.0N  N Unaries, BS=N0 NN.NN NN.NN NN.0N NN.NN NN.NN N0.NN NN.NN NN.NN NN.NN  N FC Graph, T=N NN.NN NN.NN NN.0N NN.N0 NN.NN NN.NN NN.NN NN.NN NN.NN  N FC Graph, T=N NN.NN NN.0N NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  N FC Graph, T=N NN.NN NN.NN NN.NN NN.NN NN.0N NN.NN NN.NN NN.NN NN.NN  N FC Graph, T=N, BS=N0 NN.N0 NN.NN NN.N0 NN.NN NN.NN NN.0N NN.NN NN.NN NN.NN  N FC Graph, T=N, BS=N0, vOH NN.NN NN.NN NN.NN NN.N0 NN.NN NN.NN NN.NN NN.0N NN.NN  N FC Graph, T=N, BS=N0, vOH, g=ReLU NN.NN NN.NN NN.N0 NN.NN NN.NN N0.NN NN.NN NN.NN NN.NN  N FC Graph, T=N, BS=N0, vOH, Soft-OR NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN N0.NN N0.NN  Table N
Situation prediction results on the development set
We compare several variants of our fully-connected roles model to show the  improvements achieved at every step
T refers to the number of time-steps of propagation in the fully connected roles GGNN (FC Graph)
 BS=N0 indicates the use of beam-search with beam-width of N0
vOH (verb, one-hot) is included when the embedding of the predicted  verb is used to initialize the hidden state of the role nodes
g=ReLU refers to the non-linear function used after initialization
All other  rows use g=tanh(·)
Finally, Soft-OR refers to the loss function used in [NN]
Best performance is in bold and second-best is italicized
 N
Evaluation  We evaluate our methods on the imSitu dataset [NN] and  use the standard splits with NNk, NNk, and NNk images for  the train, development, and test subsets, respectively
Each  image in imSitu is associated with one verb and three annotations for the role-noun pairs
 We follow [NN] and report three metrics: (i) verb: the  verb prediction performance; (ii) value: the semantic verbrole-value tuple prediction performance that is considered  to be correct if it matches any of the three ground truth annotators; and (iii) value-all: the performance when the entire situation is correct and all the semantic verb-role-value  pairs match at least one ground truth annotation
 N.N
Implementation Details  Image Representations
We adopt two pre-trained VGGNN CNNs [NN] for extracting image features by removing the  last fully-connected and softmax layers, and fine-tuning all  weights
The first CNN (φv(i)) is trained to predict verbs, and second CNN (φn(i)) predicts the top K most frequent nouns (K = N000 cover about NN% of nouns) in the dataset
 Unaries
Creating a graph with no edges, or equivalently  with T = 0 steps of propagation corresponds to using the initialized features to perform prediction
We refer to this  approach as Unaries, which will be used as the simplest  baseline to showcase the benefit of modeling dependencies  between the roles
 Learning
We implement the proposed models in  Torch [N]
The network is trained using RMSProp [NN] with  mini-batches of NNN samples
We choose the hidden state  dimension D = N0NN, and train image (Wiv,Win), verb (Wv) and role (We) embeddings
The image features are  extracted before training the GGNN or RNN models
 The initial learning rate is N0−N and starts to decay after N0 epochs by a factor of 0.NN
We use dropout with a probability of 0.N on the output prediction layer (c.f 
Eqs
N and N) and clip the gradients to range (−N, N)
 Mapping agent Roles
The imSitu dataset [NN] has situations for N0N verbs
Among them, we notice that NN  verbs do not have the semantic role agent but instead with  roles of similar meaning (e.g
verb educating has role  teacher)
We map these alternative roles to agentwhen  determining their position in the RNN architecture
Such a  mapping is not used for the fully connected GGNN model
 Variable Number of Roles
A verb has a maximum of N  roles associated with it
We implement our proposed model  with fixed-size graphs involving N nodes
To deal with verbs  with less than N roles, we zero the hidden states at each  time-step of propagation, making them not receive or send  any information
 N.N
Results  We first present a quantitative analysis comparing different variants of our proposed model
We then evaluate the  performance of different architectures, and compare results  with state-of-the-art approaches
 Ablative Analysis A detailed study of the GGNN model  with fully connected roles (referred to as FC Graph) is  shown in Table N
An important hyper-parameter for the  GGNN model is the number of propagation steps T 
We  found that the performance increases by a small amount  when increasing T , and saturates soon (in rows N, N, and N)
 We believe that this is due to the use of a fully-connected  graph, and all nodes sharing most of the information at  the first-step propagation
Nevertheless, the propagation is  important, as revealed in the comparison between Unaries  (T = 0) from row N and T = N in row N
We obtain a mean improvement of N.N% in all metrics
 During test we have the option of using beam search,  where we hold B best verb predictions and compute the  NNNN    top-N predicted verb top-N predicted verbs ground truth verbs  verb value value-all verb value value-all value value-all mean  N Unaries NN.NN NN.NN NN.0N NN.NN NN.NN N0.NN NN.NN NN.NN NN.NN  N Chain RNN NN.NN NN.NN NN.NN NN.0N NN.NN NN.N0 NN.NN NN.NN NN.NN  N Tree-structured RNN NN.NN NN.NN NN.0N NN.NN NN.NN NN.NN N0.NN N0.NN NN.NN  N Chain GGNN, T=N NN.NN NN.NN NN.0N NN.NN NN.NN NN.NN NN.N0 N0.NN N0.NN  N Tree-structured GGNN, T=N NN.NN NN.NN NN.NN NN.NN NN.NN N0.NN NN.NN NN.0N NN.NN  N Fully-connected GGNN, T=N NN.NN NN.NN NN.NN NN.N0 NN.NN NN.NN NN.NN NN.0N NN.NN  Table N
Situtation prediction results on the development set for models with different graph structures
All models use beam search,  predicted verb embedding, and g = tanh(·)
Best performance is highlighted in bold, and second-best in each table section is italicized
 top-N predicted verb top-N predicted verbs ground truth verbs  verb value value-all verb value value-all value value-all mean  d ev  CNN+CRF [NN] NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.N0 NN.N0 NN.NN  Tensor Composition [NN] NN.NN NN.NN NN.NN NN.NN NN.N0 NN.0N NN.NN NN.NN NN.0N  Tensor Composition + DataAug [NN] NN.N0 NN.NN NN.NN NN.NN NN.NN NN.NN N0.N0 NN.NN NN.NN  Chain RNN NN.NN NN.NN NN.NN NN.0N NN.NN NN.N0 NN.NN NN.NN NN.NN  Fully-connected Graph NN.NN NN.NN NN.NN NN.N0 NN.NN NN.NN NN.NN NN.0N NN.NN  te st  CNN+CRF [NN] NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  Tensor Composition [NN] NN.NN NN.NN NN.NN N0.NN NN.NN NN.00 NN.N0 NN.NN NN.NN  Tensor Composition + DataAug [NN] NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN N0.NN NN.NN NN.NN  Chain RNN NN.NN NN.NN NN.NN NN.0N NN.NN NN.NN NN.NN NN.NN NN.NN  Fully-connected Graph NN.NN NN.NN NN.NN NN.N0 NN.NN NN.NN NN.NN NN.NN NN.N0  Table N
We compare situation prediction results on the development and test sets against state-of-the-art models
Each model was run on  the test set only once
Our model shows significant improvement in the top-N prediction on all metrics, and performs better than a baseline  that uses data augmentation
The performance improvement on the value-all metric is important for applications, such as captioning and  QA
Best performance is highlighted in bold, and second-best is italicized
 role-noun predictions for each of the corresponding graphs  (frames)
Finally, we select the top prediction using the  highest log-probability across all B options
We use a beam  width of B = N0 in our experiments, which yields small im- provement
Rows N and N of Table N show the improvement  using beam search on a graph without propagation
Rows N  and N show the benefit after multiple steps of propagation
 Rows N and N of Table N demonstrate the impact of using embeddings of the predicted verb (vOH) to initialize  the role nodes’ hidden states in Eq
(N)
Notable improvement is obtained when using the ground-truth verb (N-N%)
 The value-all for the top-N predicted verb increases from  NN.N0% to NN.NN%
We also tested different non-linear functions for initialization, i.e., tanh (row N) or ReLU (row N), however, the impact is almost negligible
We thus use tanh for all experiments
 Finally, comparing rows N and N of Table N reveals that  our loss function to predict all annotations in Eq
(N) performs slightly better than the Soft-OR loss that aims to fit at  least one of the annotations [NN]
 Baseline RNNs
Table N summarizes the results with different structures on the dev set
As expected, Unaries perform consistently worse than models with information propSPRINKLING  AGENT PLACE ITEM SOURCE DEST
 Unaries PERSON KITCHEN MEAT HAND HAND  RNN PERSON KITCHEN FOOD FINGER PIZZA  FC Graph PERSON KITCHEN CHEESE HAND PIZZA  FISHING  AGENT PLACE SOURCE TOOL  Unaries MAN RIVER - FISHING  RNN MAN OUTDOORS BODY FISHING  FC Graph MAN RIVER RIVER FISHING  Figure N
Example images with their predictions listed from all  methods
Roles are marked with a blue background, and predicted  nouns are in green boxes when correct, and red when wrong
Using the FC Graph corrects mistakes made by the Unaries or Chain  RNN prediction models
 agation between nodes on the value and value-all metrics
 The Tree-structured RNN provides a N% boost in value-all for top-N predicted verb, while the Chain RNN provides a  N.N% improvement
Owing to the better connectivity be- tween the roles in a Chain RNN (especially place and  agent), we observe better performance compared to the  NNNN    Tree-structured RNN
Note that as the RNNs are trained  jointly to predict both verbs and nouns, and as the noun  gradients dominate, the verb prediction takes a hit
 Different Graph Structures
We can also use chain or  tree-structured graphs in GGNN
Along with the FC graph  in row N of Table N, rows N and N present the results for  different GGNN structures
They show that connecting  roles with each other is critical and sharing information  helps
Interestingly, the Chain GGNN needs more propagation steps (T=N), as it takes time for the left-most and  right-most nodes to share information
Smaller values of  T are possible when nodes are well-connected as in Treestructured (T=N) or FC Graph (T=N)
Fig
N presents prediction from all models for two images
The FC Graph is  able to reason about associating cheese and pizza rather  than sprinkling meat or food on it
 Comparison with State-of-the-art
We compare the performance of our models against state-of-the-art on both the  dev and test sets in Table N
Our CNN predicts the verb  well
Beam search leads to even better performance (N-N%  higher) in verb prediction
We note that Tensor Composition  + DataAug actually uses more data to train models
Nevertheless, we achieve the best performance on all metrics  when using the top-N predicted verb
 Another advantage of our model is in improvement for  the value-all metric
It yields +N% when using the groundtruth verb, +N% with top-N predicted verbs, and +N.N% with  top-N predicted verb, compared with the baseline without  data augmentation
Interestingly, even with data augmentation, we outperform [NN] by N-N% in value-all for top-N  predicted verb
This property attributes to information sharing between role nodes, which helps in correcting errors and  better predicts frames
Note that value-all is an important  metric to measure a full understanding of the image
Models with higher value-all will likely lead to better captioning  or question-answering results
 N.N
Further Discussion  We delve deeper into our model and discuss why the FC  Graph outperforms baselines
 Learned Structure
A key emphasis of this model is  on information propagation between roles
In Fig
N, we  present the norms of the propagation matrices
Each element in the matrix P (a′, a) is the norm of the incom- ing message from role a′ to a averaged across all images  (in dev set) at the first time-step, i.e., ‖xt=N(a′,a)‖ regarding Eq
(N)
In this example, tool is important for the verb  fastening and influences all other roles, while agent  and obstacle influence roles in jumping
 Wrong Verb Predictions
We present a few examples of  top scoring results where the verb prediction is wrong in  des t
 age nt  pla ce  item  con n
 too l  age nt pla  ce item des  tina tion  con nec  tor  too l  FASTENING  age nt  pla ce  src 
 obs t
 des t
 age nt  pla ce sou  rce obs  tac le  des tina  tion  JUMPING  Figure N
We present the “amount” of information that is propagated between roles for two verbs along with sample images
Blue  corresponds to high, and green to zero
Each element of the matrix corresponds to the norm of the incoming message from different roles (normalized column sum to N)
Left: verb fastening  needs to pay attention to the tool used
Right: important components to describe jumping are the agent and obstacles  along the path
 GT: FISHING  AGENT PLACE TOOL SOURCE  MAN BOAT FISHING LAKE  PRED: CATCHING  AGENT PLACE TOOL CAUGHTITEM  MAN BOAT BODY FISHING  GT: SLOUCHING  AGENT PLACE CONTACT  WOMAN OFFICE CHAIR  PRED: SITTING  AGENT PLACE CONTACT  WOMAN OFFICE CHAIR  GT: SHELVING  AGENT PLACE ITEM DESTINATION  WOMAN LIBRARY BOOK BOOKSHELF  PRED: BROWSING  AGENT PLACE GOALITEM  WOMAN LIBRARY BOOK  Figure N
Images with ground-truth and top-N predictions from  the development set
Roles are marked with blue background
 Ground-truth (GT) nouns are in yellow and predicted (PRED)  nouns with green when correct, or red when wrong
Although the  predicted verb is different from the ground-truth, it is very plausible
Some of the verbs refer to the same frame (e.g
sitting and  slouching), and contain the same set of roles, which our model  is able to correctly infer
 Fig
N
Note that in fact these predicted verbs are plausible options for the given images
The metric value treats  them as wrong, and yet we can correctly predict the rolenoun pairs
One example is the middle one of slouching  vs
sitting
Fig
N (bottom) shows that choosing a different verb might lead to the selection of different roles  (goalitem vs
item, destination)
Nevertheless,  predicting book for browsing is a good choice
 NNNN    DYEING  AGENT PERSON  PLACE OUTDOORS  MATERIAL FABRIC  DYE RED  LEAKING  SUBSTANCE WATER  PLACE OUTSIDE  SOURCE PIPE  DESTINATION LAND  DRUMMING  AGENT MAN  PLACE ROOM  TOOL DRUMSTICK  ITEM DRUM  DOUSING  AGENT MAN  PLACE OUTDOORS  LIQUID WATER  UNDERGOER MAN  MILKING  AGENT FARMER  PLACE OUTDOORS  TOOL COW  SOURCE COW  DESTINATION BUCKET  CLINGING  AGENT MONKEY  PLACE OUTDOORS  CLUNGTO MONKEY  CAMPING  AGENT PEOPLE  PLACE FOREST  SHELTER TENT  OVERFLOWING  AGENT RUBBISH  PLACE OUTDOORS  SOURCE ASHCAN  PAWING  AGENT CAT  PLACE OUTDOORS  AGENTPART PAW  PAWEDITEM FENCE  PICKING  AGENT WOMAN  PLACE OUTDOORS  CROP APPLE  SOURCE TREE  HUGGING  AGENT MAN  PLACE OUTDOORS  HUGGED MAN  AGENTPART ARM  TAXIING  AGENT AIRPLANE  PLACE AIRPORT  GROUND RUNWAY  Figure N
Images with top-N predictions from the development set
For all samples, the predicted verb is correct, shown below the image  in bold
Roles are marked with a blue background, and predicted nouns are in green when correct, and red when wrong
Top row: We are  able to correctly predict the situation (verb and all role-noun pairs) for all samples
Bottom row: The first three samples contain errors in  prediction (e.g
the agent for the verb pawing is clearly a dog)
However, the latter three samples are in fact correct predictions that are  not found in the ground-truth annotations (e.g
people are in fact camping in the forest)
 Predictions with Correct Verb
Fig
N shows several examples of prediction obtained by FC Graph, where the predicted verb matches the ground-truth one
The top row  corresponds to samples where the metric value-all scores  correctly as all role-noun pairs are correct
Note that the  roles are closely related (e.g
(agent, clungto) and  (material, dye)) and help each other choose the correct nouns
In the bottom row, we show some failure  cases in predicting role-noun pairs
First, the model favors predicting place as outdoor (a majority of place is  outdoor in the training set)
Second, for the sample with  verb picking, we predict the crop as apple, which  appears NN times in the dataset compared with cotton  that appears NN times
Providing more training samples  (e.g
[NN]) could help remedy such issues
 In the latter three samples of the bottom row, although  the model makes reasonable predictions, they do not match  the ground-truth
For example, the ground-truth annotation  for the verb taxiing is agent:jet and for the verb  camping is agent:persons
Therefore, even though  each image comes with three annotations, synonymous  nouns and verbs make the task still challenging
 N
Conclusion  We presented an approach for recognizing situations in  images that involves predicting the correct verb along with  its corresponding frame consisting of role-noun pairs
Our  Graph Neural Network (GNN) approach explicitly models dependencies between verb and roles, allowing nouns  to inform each other
On a benchmark dataset imSitu, we  achieved ∼N.N% accuracy improvement on a metric that evaluates correctness of the entire frame (value-all)
We  presented analysis of our model, demonstrating the need to  capture the dependencies between roles, and compared it  with RNN models and other related solutions
 N
Acknowledgements  This work is in part supported by a grant from the Research  Grants Council of the Hong Kong SAR (project No
NNNNNN)
We  also acknowledge support from NSERC, and GPU donations from  NVIDIA
 NNN0    References  [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra, C
L
 Zitnick, and D
Parikh
VQA: Visual Question Answering
 In ICCV, N0NN
N  [N] J
Bruna, W
Zaremba, A
Szlam, and Y
LeCun
Spectral  networks and locally connected networks on graphs
ICLR,  N0NN
N  [N] L
Castrejon, K
Kundu, R
Urtasun, and S
Fidler
Annotating object instances with a polygon-rnn
In CVPR, N0NN
 N  [N] K
Cho, B
van Merriënboer, C
Gulcehre, B
Dzmitry,  F
Bougares, H
Schwenk, and Y
Bengio
Learning Phrase  Representations using RNN Encoder-Decoder for Statistical  Machine Translation
In EMNLP, N0NN
N, N  [N] R
Collobert, K
Kavukcuoglu, and C
Farabet
TorchN: A  matlab-like environment for machine learning
In BigLearn,  NIPS Workshop, N0NN
N  [N] M
Defferrard, X
Bresson, and P
Vandergheynst
Convolutional neural networks on graphs with fast localized spectral  filtering
In NIPS, N0NN
N  [N] V
Delaitre, I
Laptev, and J
Sivic
Recognizing human actions in still images: a study of bag-of-features and partbased representations
In BMVC, N0N0
N  [N] Z
Deng, A
Vahdat, H
Hu, and G
Mori
Structured Inference Machines: Recurrent Neural Networks for Analyzing  Relations in Group Activity Recognition
In CVPR, N0NN
N  [N] D
K
Duvenaud, D
Maclaurin, J
Iparraguirre, R
Bombarell, T
Hirzel, A
Aspuru-Guzik, and R
P
Adams
Convolutional networks on graphs for learning molecular fingerprints
In NIPS, N0NN
N  [N0] C
J
Fillmore, C
R
Johnson, and M
R
L
Petruck
Background to FrameNet
International Journal of Lexicography,  NN(N):NNN–NN0, N00N
N  [NN] H
Fürstenau and M
Lapata
Graph Alignment for SemiSupervised Semantic Role Labeling
In EMNLP, N00N
N  [NN] A
Gupta and L
S
Davis
Beyond Nouns: Exploiting  Prepositions and Comparative Adjectives for Learning Visual Classifiers
In ECCV, N00N
N  [NN] S
Gupta and J
Malik
Visual Semantic Role Labeling
 arXiv:NN0N.0NNNN, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep Residual Learning  for Image Recognition
In CVPR, N0NN
N  [NN] G
Hinton, N
Srivastava, and K
Swersky
Lecture Na  overview of mini-batch gradient descent
N  [NN] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural computation, N(N):NNNN–NNN0, NNNN
N  [NN] J
Johnson, R
Krishna, M
Stark, L.-J
Li, D
A
Shamma,  M
S
Bernstein, and L
Fei-Fei
Image Retrieval using Scene  Graphs
In CVPR, N0NN
N, N  [NN] D
Jurafsky and J
H
Martin
Speech and Language Processing, chapter NN
Semantic Role Labeling
N, draft edition,  N0NN
N  [NN] A
Karpathy and L
Fei-Fei
Deep Visual-Semantic Alignments for Generating Image Descriptions
In CVPR, N0NN
 N  [N0] P
Kingsbury and M
Palmer
From TreeBank to PropBank
 N00N
N  [NN] T
N
Kipf and M
Welling
Semi-supervised classification  with graph convolutional networks
ICLR, N0NN
N  [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L.-J
Li, D
A
Shamma, M
Bernstein, and L
Fei-Fei
Visual Genome: Connecting Language  and Vision Using Crowdsourced Dense Image Annotations
 arXiv:NN0N.0NNNN, N0NN
N  [NN] Y
Li, D
Tarlow, M
Brockschmidt, and R
Zemel
Gated  Graph Sequence Neural Networks
In ICLR, N0NN
N, N  [NN] X
Liang, X
Shen, J
Feng, L
Lin, and S
Yan
Semantic  object parsing with graph lstm
In ECCV, N0NN
N  [NN] D
Lin, S
Fidler, C
Kong, and R
Urtasun
Generating multisentence lingual descriptions of indoor scenes
In BMVC,  N0NN
N  [NN] H
Ling and S
Fidler
Teaching machines to describe images  via natural language feedback
In arXiv:NN0N.00NN0, N0NN
 N  [NN] C
Lu, R
Krishna, M
Bernstein, and L
Fei-Fei
Visual Relationship Detection with Language Priors
In ECCV, N0NN
 N, N  [NN] K
Marino, R
Salakhutdinov, and A
Gupta
The more  you know: Using knowledge graphs for image classification
 CVPR, N0NN
N  [NN] G
A
Miller
WordNet: A Lexical Database for English
 Communications of the ACM, NN(NN):NN–NN, NNNN
N  [N0] B
A
Plummer, A
Mallya, C
M
Cervantes, and  J
Hockenmaier
Phrase Localization and Visual Relationship Detection with Comprehension Linguistic Cues
 arXiv:NNNN.0NNNN, N0NN
N  [NN] M
R
Ronchi and P
Perona
Describing Common Human  Visual Actions in Images
In BMVC, N0NN
N  [NN] M
Roth and M
Lapata
Neural Semantic Role Labeling with  Dependency Path Embeddings
In ACL, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual  Recognition Challenge
IJCV, NNN(N):NNN–NNN, N0NN
N  [NN] F
Scarselli, M
Gori, A
C
Tsoi, M
Hagenbuchner, and  G
Monfardini
The Graph Neural Network Model
IEEE  Transactions on Neural Networks, N0(N):NN–N0, N00N
N  [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In NIPS, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 N, N  [NN] K
S
Tai, R
Socher, and C
D
Manning
Improved semantic  representations from tree-structured long short-term memory  networks
ACL, N0NN
N, N  [NN] M
Tapaswi, Y
Zhu, R
Stiefelhagen, A
Torralba, R
Urtasun, and S
Fidler
MovieQA: Understanding Stories in  Movies through Question-Answering
In CVPR, N0NN
N  [NN] O
Vinyals, A
Toshev, S
Bengio, and Dumitru Erhan
Show  and Tell: A Neural Image Caption Generator
In CVPR,  N0NN
N  [N0] H
Wang, A
Kläser, C
Schmid, and C.-L
Liu
Action recognition by dense trajectories
In CVPR, N0NN
N  NNNN    [NN] P
J
Werbos
Generalization of backpropagation with application to a recurrent gas market model
Neural networks,  N(N):NNN–NNN, NNNN
N  [NN] K
Xu, J
L
Ba, R
Kiros, K
Cho, A
Courville, R
Salakhutdinov, R
S
Zemel, and Y
Bengio
Show, Attend and  Tell: Neural Image Caption Generation with Visual Attention
JMLR, N0NN
N  [NN] S
Yang, Q
Gao, C
Liu, C
Xiong, S.-C
Zhu, and J
Y
Chai
 Grounded Semantic Role Labeling
In NAACL, N0NN
N  [NN] B
Yao and L
Fei-Fei
Grouplet: A Structured Image Representation for Recognizing Human and Object Interactions
 In CVPR, N0N0
N  [NN] B
Yao, X
Jiang, A
Khosla, A
L
Lin, L
Guibas, and L
FeiFei
Human Action Recognition by Learning Bases of Action Attributes and Parts
In ICCV, N0NN
N  [NN] M
Yatskar, V
Ordonez, L
Zettlemoyer, and A
Farhadi
 Commonly Uncommon: Semantic Sparsity in Situation  Recognition
In CVPR, N0NN
N, N, N, N, N  [NN] M
Yatskar, L
Zettlemoyer, and A
Farhadi
Situation Recognition: Visual Semantic Role Labeling for Image Understanding
In CVPR, N0NN
N, N, N, N, N  [NN] H
Zhang, Z
Kyaw, S.-F
Chang, and T.-S
Chua
Visual  Translation Embedding Network for Visual Relation Detection
arXiv:NN0N.0NNNN, N0NN
N  [NN] S
Zheng, S
Jayasumana, B
Romera-Paredes, V
Vineet,  Z
Su, D
Du, C
Huang, and P
H
S
Torr
Conditional Random Fields as Recurrent Neural Networks
In ICCV, N0NN
 N  [N0] B
Zhou, A
Khosla, A
Lapedriza, A
Torralba, and A
Oliva
 Places: An Image Database for Deep Scene Understanding
 arXiv:NNN0.0N0NN, N0NN
N  [NN] B
Zhou, A
Lapedriza, J
Xiao, A
Torralba, and A
Oliva
 Learning Deep Features for Scene Recognition using Places  Database
In NIPS, N0NN
N  [NN] J
Zhou and W
Xu
End-to-end Learning of Semantic Role  Labeling using Recurrent Neural Networks
In ACL, N0NN
N  NNNNAnnArbor: Approximate Nearest Neighbors Using Arborescence Coding   AnnArbor: Approximate Nearest Neighbors Using Arborescence Coding  Artem Babenko  Yandex, Moscow  National Research University  Higher School of Economics, Moscow  artem.babenko@phystech.edu  Victor Lempitsky  Skolkovo Insitute of Science and Technology  (Skoltech), Moscow  lempitsky@skoltech.ru  Abstract  To compress large datasets of high-dimensional descriptors, modern quantization schemes learn multiple codebooks and then represent individual descriptors as combinations of codewords
Once the codebooks are learned,  these schemes encode descriptors independently
In contrast to that, we present a new coding scheme that arranges  dataset descriptors into a set of arborescence graphs, and  then encodes non-root descriptors by quantizing their displacements with respect to their parent nodes
By optimizing the structure of arborescences, our coding scheme can  decrease the quantization error considerably, while incurring only minimal overhead on the memory footprint and  the speed of nearest neighbor search in the compressed  dataset compared to the independent quantization
The advantage of the proposed scheme is demonstrated in a series  of experiments with datasets of SIFT and deep descriptors
 N
Introduction  Visual search and other computer vision applications are  routinely dealing with million or billion-scale datasets of  visual descriptors corresponding to images and/or image  parts
Lossy compression of such descriptor datasets that  reduce their memory footprint and increase the search speed  have therefore become an active area of research
Currently,  approaches based on (non-binary) quantizations [NN, NN, NN,  N, NN, N] achieve the best compression error-compression  ratio trade-off, while also permitting efficient computation  of scalar products and squared distances between uncompressed queries and compressed descriptor sets using lookup tables
For million-scale datasets, the look-up tables allow fast exhaustive search that scans through entire datasets  in a matter of milliseconds
 Existing quantization approaches represent dataset descriptors as combinations of codeword vectors that come  from different codebooks
The codebooks are invariably  adapted to the dataset (or its hold-out part) through the optimization process, so that statistical regularities of the descriptor distribution can be exploited for better coding accuracy
Importantly, once the codebooks have been learned,  existing quantization schemes apply independent coding to  each of the dataset descriptors
 In this work, we propose the approach that brings further improvement in terms of coding accuracy on top of  the existing descriptor coding techniques, while incurring  small memory and search time overheads
The improvement comes as we consider joint coding of descriptors in  the dataset that goes beyond codebook learning
Our approach (arborescence coding) avoids direct coding of the  majority of the dataset vectors, and instead focuses on coding relative displacements between nearby vectors
A similar idea underlies predictive coding [NN], however arborescences coding goes beyond predictive coding by selecting  sets of parent-children pairs that are most suitable for predictive quantization
 The main idea of our approach is simple (Figure N)
 During coding, the optimization process splits the dataset  into a set of arborescence graphs (i.e
directed trees), with  individual descriptors being the vertices of those arborescences
For each arborescence, the topology (structure), the  absolute position of the root, and the relative displacements  along the arcs are encoded and stored using quantization  techniques
When computing the scalar product with the  query vector, the scalar product between the query and the  root vector is computed first, and the scalar products with  other vectors are computed in a breadth-first manner
Such  breadth-first scan process can then benefit from the standard look-up table tricks used by quantization methods [NN]  thanks to the additivity of the scalar product
Arborescence  coding therefore does not incur significant reductions in  search speed compared to the base quantization algorithm
 Crucially, arborescence coding makes the topology (including the number) of the arborescences part of the optimization process during the encoding of the dataset
In  the process of optimization, individual descriptors are free  NNNNN    Figure N: Independent (traditional) coding vs
Arborescence coding
Left – a set of descriptors (black dots) are encoded  using the codebook of eight codewords (red)
Each descriptor gets assigned to the closest codeword
This is a standard,  independent (given the codebook) coding approach resulting in large coding errors (thin solid lines)
Middle – arborescence  coding splits the dataset into a set of arborescence graphs (the roots of arborescences are highlighted in red)
Right – following  the structure of these arborescences, the coding uses one four-word codebook to encode root descriptors (red circles), and  another four-word codebook to encode displacements along arcs in the arborescences (the codewords are shown as green,  magenta, orange, blue vectors)
By coordinating the choice of arborescence topology and the codebooks, arborescence coding  creates reconstructions (circles) that result in much lower coding errors (thin lines) than independent coding, while still using  one codeword per descriptor and eight different codewords for the dataset
The cost of storing arborescence topology is small  and independent of space dimensionality
Note: in this ND toy example, single codebook quantizations are used
In high  dimensions, both plain coding and arborescence coding can benefit from multi-codebook quantization methods (e.g
product  quantization [NN]) 
 to choose whether to become roots (and to be encoded directly) or to become non-root nodes (and to be coded relative to some parent descriptor)
The choice of the parent  is driven by the (greedy) desire to minimize the coding error
Importantly, our approach can be used on top of almost  any existing quantization scheme [NN, N0, NN, NN, N, NN, N]  or, in fact, any other vector compression schemes such as  generative binary hashing [N0]
The optimization process  in arborescence coding can be initialized with the “trivial”  state where each desriptor forms a separate arborescence  and is therefore coded independently
As the subsequent  optimization is guaranteed not to increase the coding errors  of individual vectors, our approach is guaranteed to achieve  same or lower compression error compared to the base coding algorithm
 Alongside the full-fledged version of our approach (arborescence coding), we also consider the simpler version  of the approach where all arborescences are restriced to be  star-shaped (star quantization)
In a number of experiments  on datasets of various nature (SIFTs [NN] and deep descriptors), we show that both arborescence coding and star coding bring consistent improvements in terms of coding error (which directly translates into the accuracy of nearest  neighbor search) over the base quantization scheme, which  in our experiments is optimized product quantization (OPQ)  [NN, NN]
 Below, we cover the related work in Section N
We then  introduce the general principles of arborescence coding in  Section N
In Section N, we discuss the specific instantiation  of arborescence coding on top of optimized product quantization
We conclude with the experimental validation in  Section N and the discussion in Section N
 N
Related Work  The plethora of recently proposed quantization methods  include product quantization [NN], residual vector quantization [N0], optimized product quantization [NN, NN], additive  quantization [N], composite quantization [NN], tree quantization [N]
All of these approaches can be used as base  methods for arborescence coding or star coding
In our experiments, we use optimized product quantization [NN, NN]  because of its appealing balance between the speed and the  accuracy of the encoding process
 Arborescence coding and in particular star coding are  in some ways reminiscent of the bi-layer coding approach  used in the IVFADC [NN] and Multi-D-ADC systems [N]
 Both systems are motivated by the indexing task for very  large scale datasets, as they split the descriptor space into  disjoint cells, and encode the displacements of individual  points w.r.t
cell centroids
The Multi-D-ADC system thus  uses two separate codebook sets, one to encode the centroids and one to encode the displacements (while the IVFADC system stores the centroids directly)
Arborescence  coding also maintains difference codebooks for root encoding and descriptor encoding
However, unlike IVFADC and  Multi-D-ADC that pick centroids in a separate optimizaNNNN    tion process and automatically assign each descriptor to the  nearest centroid, arborescence coding makes the optimization over possible arborescence topology part of the encoding process
In the experiments, we compare arborescence  coding with the Multi-D-ADC system in a comparable setting, and find such optimization advantageous
Star coding  is also related to the locally-optimized product quantization  system that also uses multiple OPQ codebooks [NN]
 Another method that perform non-independent compression of descriptor sets is [N] that is targeted towards very  strong compression of low to medium-dimensional data,  and is not competitive with quantization-based approaches  for the code lengths that we consider (e.g
eight bytes per  vector or more)
 Compression of unordered sets is also studied in the data  compression community
Many of such studies are focused  on sets of simple objects such as integers [NN, NN, NN, N]  or real numbers [NN], whereas we are interested in compression of sets of high-dimensional descriptors
A popular idea for set coding is seeking optimal permutation of the  entries (re-ordering) that permits efficient predictive coding
Re-oredering has been applied to e.g
image pixels [N]  and binary strings [NN]
Finding optimal order involves (approximate) solution to the travelling salesman problem
The  resulting Hamiltonian path can be regarded as a very large  arborescence spanning the entire dataset, and therefore arborescence coding can be regarded as a generalization of  re-ordering
 N
Arborescence coding  In this section, we introduce arborescence coding and its  variant, star coding, in their general form
The next section  then discusses the particular instantiation of arborescence  coding on top of optimized product quantization [NN, NN]
 The variants of arborescence coding on top of other quantization schemes can then be derived analogously
 Assume that a dataset X = {xN,xN, 


,xN} of D- dimensional vectors is given
Arborescence coding is a  lossy compression scheme that for each vector xi in the  dataset encodes either its absolute position in the descriptor space, or its relative displacement w.r.t
a certain other  descriptor
We denote the code stored for the i-th descriptor ti and we denote with pi the index of its parent
Parent-child relations are constrained to form a set of arborescences (i.e
 directed tree graphs)
If the i-th descriptor is the root of its arborescence, then we use the convention pi=0
Arbores- cences that consist of single descriptors are allowed
 We also consider a particular variant of arborescence  coding (star coding), which corresponds to the case when  all arborescences have depth at most one, i.e
no non-root  descriptors are allowed to have children
Below, ‘arborescence coding’ refers to all variants including star coding,  unless stated otherwise
 Arborescence coding requires that two decoding operations are defined that allow to recover the reconstruction yi for every descriptor xi
The first decoder d  0 with parameters θ0 reconstructs the roots of the arborescences:  yi = d 0(ti; θ  0), if pi = 0 
(N)  The coding process then picks the code ti and the parame- ters of the encoder θ0 to ensure that yi ≈ xi for root de- scriptors
 The second decoder d∆ with parameters θ∆ reconstructs the displacements zi from the reconstructions of parents to  their children:  zi = yi − ypi = d ∆(ti; θ  ∆), s.t
pi > 0 
(N)  The coding process then picks the code ti and the parame- ters of the encoder θ∆ so that zi ≈ xi−ypi 
 Encoding process
The dataset can be encoded by minimizing the overall squared reconstruction error
This optimization task has the following formulation:  minimize θ0,θ∆,P,T,y  ∑  i: pi=0  ‖d0(ti; θ 0)− xi‖  N+  ∑  i: pi>0  ‖ypi + d ∆(ti; θ  ∆)− xi‖ N (N)  subject to yi =  {  d0(ti; θ 0), if pi = 0  ypi + d ∆(ti; θ  ∆), if pi > 0 
 In (N) P denotes the set (vector) {pN, pN, 


, pN}, which defines the topology of arborescences, T denotes the set of codes {tN, tN, 


, tN}
Performing optimization (N) pro- cess is usually hard, only an approximate (local) minimum  can be found, and a reasonable initialization procedure is  usually required
In Section N.N, we discuss the optimization for arborescence coding based on optimized product  quantization scheme
 Decoding process
The approximations to the original  dataset can be recovered by applying formulas (N) and (N)
 To decode all descriptors from a certain arborescence, we  first decode the root descriptor using (N)
We then proceed  along the arborescence
For the ith descriptor, we take the reconstruction ypi of its parent, recover the displacement  vector zi using (N), and get the descriptor reconstruction as  yi = ypi + zi
Fast search
For the majority of quantization schemes,  search for descriptors with high scalar product or low  squared distance to a certain query descriptor q does not  require explicit decoding (N)-(N)
Instead, the quantization  schemes usually provide the way to quickly estimate the  scalar product Π0i (q, ti; θ 0) = 〈q, d0(ti; θ  0)〉 = 〈q,yi〉 using look-up tables precomputed once for the given q  and the parameters θ0 [NN]
Likewise, the scalar product Π∆i (q, ti; θ  ∆) = 〈q, d∆(ti; θ ∆)〉 = 〈q, zi〉 between the  query and the encoded displacement zi can be estimated  NNNN    without the explicit reconstruction of zi
The scalar product between the query and the encoded vectors can then be  quickly evaluated by traversing an arborescence from the  root to the leaves:  〈q,yi〉 =  {  Π0i (q, ti; θ 0), if pi = 0  〈q,ypi〉+Π ∆ i (q, ti; θ  ∆), if pi > 0 
(N)  The squared Euclidean difference can then be estimated  using the formula  ‖q− yi‖ N = ‖q‖N + ‖yi‖  N − N〈q,yi〉 
(N)  While the third term in (N) can be estimated using (N), the  scalar term ‖yi‖ N is trickier to handle
In practice, we store  a coarse estimate of ‖yi‖ N (one-byte quantization) along  with (inside) every descriptor code ti
Note that some of the quantization schemas (such as additive quantization [N],  tree quantization [N]) have to store such estimate anyways  (if fast nearest neighbor search is desired), so that the requirement to store the quantized squared norm does not incur additional memory costs over these schemes
 Memory overhead
In general, the memory footprint of  arborescence coding consists of the parameters θ0 and θ∆, the codes ti, and the topology information, which is needed to infer the parents pi during decoding or fast search
The memory spent on the topology information thus constitutes  overhead over the base quantization scheme
If stored directly, the indices pi take ⌈logN n⌉ bits each, which is sig- nificant for many interesting scenarios
 Fortunately, simple tricks can allow to store arborescence topology at a much lower cost
For that, the descriptors can be re-ordered, so that arborescences are stored sequentially (descriptors forming the same arborescence are  stored contiguously)
Furthermore, within each arborescence, the descriptors can be reordered following breadthfirst order
Then to recover the topology it is sufficient to  store the number of children with every descriptor
These  numbers follow a very low-entropy distribution (upto N-N  bits in all our experiments), which is a very low overhead  compared to reasonable code sizes for most practical purposes
 For star coding, the overhead can be made negligible as  follows
We store stars contiguously, further ordering stars  by the number of elements in them
Then, the only information that needs to be stored in order to recover the topology  is the maximal star size and the number of stars of each  size, which is at most few hundred bytes per dataset for any  reasonable dataset
 N
Arborescence coding using OPQ  We now discuss a particular instantiation of arborescence  coding when the decoders (N) and (N) use optimized product  quantization (OPQ) [NN, NN]
 We first recap OPQ using the notation introduced above
 In the OPQ scheme, a vector is encoded as a rotated  concatenation of M codewords coming from M differ- ent codebooks
The parameters for the decoders can thus  be written as: θ0 = {R0, C0N , C 0 N , 


, C  0 M} and θ  ∆ = {R∆, C∆N , C  ∆ N , 


, C  ∆ M}, where R  0 and R∆ are the D×D orthogonal matrices, and each of the codebooks C0j , C  ∆ j  contains K codeword D/M -dimensional vectors
We de- note c0j,k the k-th codeword in the j-th codebook for the first decoder
In our implementation, we keep the two rotation matrices the same: R = R0 = R∆
Using two differ- ent rotation matrices is possible, but leads to more complex  encoding process
 The code ti for each vector is then an M -tuple of code- word indices tNi , t  N i , 


, t  M i in the respective codebooks,  each of them being an integer between N and K
The de- coding of root descriptors (N) then takes the form:  yi = R [c 0 N,tN  i , c0N,tN  i , 


, c0  M,tM i  ], if pi = 0 , (N)  where square brackets denote concatenation
Likewise, the  decoding of displacements (N) then takes the form:  zi = R [c ∆ N,tN  i , c∆N,tN  i , 


, c∆  M,tM i  ], if pi > 0 , (N)  The fast search procedure discussed above then uses the  look-up tables L0(j, k) = 〈RTq, c0j,k〉 and L ∆(j, k) =  〈RTq, c∆j,k〉 that are precomputed for a given query (after  rotating it by RT=R−N)
Using these tables, the scalar product of the query q with yi (for root nodes) or zi (for  non-root nodes) can be evaluated by M look-ups in the ta- bles and M − N scalar additions
 N.N
Optimizing the encoding  We now discuss the encoding process (N) in the case of  OPQ
The following groups of variables are part of the optimization: the arborescence topology P , the code tuples T , the codebooks C, and the rotation matrix R
As common in the quantization schemes, optimization proceeds by alternations (group-coordinate descent)
At each update stage, one  group of variables is updated, while others are kept fixed
 We now go through the update stages
 Updating rotation
The updates for rotation can be performed by finding optimal rotation δR of the dataset X that minimizes the squared distance (N) and applying the update  R := δRTR to the current rotation
The update δR can be found using Procrustes analysis as detailed in [NN, NN]
In  the remaining updates,we can remove matrix R from con- sideration by applying the rotation RT to the dataset X , effectively reducing our quantizers to unoptimized product  quantizers [NN]
We apply this trick to simplify the derivations below
 Updating codebooks
When all other variables are  fixed, yi can be expressed as a linear function of the codeword entries
Consider the f -th dimension in the reconNNNN    struction yi
Let us assume that it corresponds to the l- th dimension in the m-th chunk of dimensions that OPQ- splits the D dimensions into
In other words, let f = l + (m− N) D  M 
Then the f -th dimension of the reconstruction yi can be found as:  yi[f ] = c 0 m,tm  r(i) [l] +  ∑  j∈r(i)→i  c ∆ m,tm  j [l] , (N)  where square brackets denote the dimension indexing, r(i) denotes the root index of the arborescence that the i-th de- scriptor belongs to, and r(i) → i denotes the set of indices in the path from the root to the i-th descriptor (excluding the root)
 Plugging the unrolled expression (N) into the objective  (N) results in a least-squares problem over the entries of the  codebooks
The problem decomoses over different dimensions, with each of the resulting D least-squares problems having NK variables (c0m,k[l] and c  ∆ m,k[l] for k ∈ N..K)
 Solving these problems then gives the optimal codebook entries (given the other variables fixed)
 Updating topology and codes
Finally, we discuss the  updates to the topology (i.e
the variables pi and the codes ti)
We perform these updates sequentially, at each time changing the variables pi and ti for a single i
In other words, we iterate over descriptors one-by-one, and allow  each of them to improve its reconstruction error by simultaneously choosing a different parent and encoding the displacement to this parent or becoming a root and encoding  its absolute position
 The change of pi and/or ti changes the reconstruction yi, which also results in the change of reconstructions for all  descendants in the arborescence
Since we want to perform  updates efficiently and with the guarantee that the squared  reconstruction error does not increase, we skip all descriptors with children during the updates
 To further speed-up the updates, for i-th descriptor xi we only consider k = N0 descriptors with the closest re- construction yj as potential parents
When performing star  quantization, we only consider descriptors that are currently  roots
For each potential parent j, we consider the vector xi − yj , assess the error of its optimal product quantization using codebooks C∆, assess the error of product quantiza- tion of xi using the codebook C  0, and pick the encoding  variant with the smallest error
 N.N
Initializating the encoding  The iterative updates discussed above are guaranteed to  not increase the reconstruction error, and given time will  converge to a certain configuration
This configuration,  however, is not guaranteed and most certainly will not be  a global minimum to the reconstruction error
Therefore,  the accuracy of the resulting encoding depends on the initialization
 We use the following initialization approach
We initialize all parent variables pi to zero making them roots, and initialize all other variables by effectively performing OPQ
 At this point, our reconstruction corresponds to OPQ
Since  the reconstruction error is guaranteed to not increase in the  further optimization steps and in subsequent optimization  updates, the squared error of arborescence coding (or star  coding) is guaranteed to be same or lower compared to  OPQ
 We then initialize the codebooks C∆ by running product quantization on the random subset of displacements from yj to xi, such that yj is one of the k = N0 nearest neighbors of xi (among all reconstructions y)
 Finally, we update the parameters pi and ti as discussed in Section N.N with one additional heuristics
During the  first update only, we visit the descriptors in the order of increasing OPQ reconstruction error
During this traversal,  we prohibit descriptor to choose parents among yet unvisited descriptors, which have higher OPQ reconstruction error
As a result, every descriptor is childless by the moment it is visited, which gives it an opportunity to choose  a parent (among more “affluent” descriptors with lower reconstruction error) and to decrease its own reconstruction  error (recall, that in our optimization algorithm discussed in  Section N.N only childless descriptors are allowed to switch  parents)
 N
Experiments  In this section, we present experimental evaluation of arborescence coding and star coding
In the experiments, we  encode the datasets using the new coding schemes, as well  as several baselines
In the majority of the experiments and  unless noted otherwise, we simplify the experimental setup  and optimize parameters directly on the “test” dataset rather  than “learning” them on a hold-out dataset of similar nature
While we do not evaluate generalization capabilities,  we still aim to compare methods with similar number of encoding parameters, making our comparisons valid
In the  final experiment, we demonstrate that the relative performance of coding schemes remains approximately the same,  when coding parameters are learned on the hold-out dataset
 Datasets
We consider the following four datasets:  • SIFTNM [NN] is a dataset of million SIFT vectors [NN], which are highly structured gradient-based descriptors,  extracted from natural image keypoints with the holdout set of N0.000 queries
 • DEEPNM and DEEPN0M datasets contain deep de- scriptors, which are computed from the activations of  deep neural networks
In general, deep descriptors are  emerging as the new state-of-the-art in retrieval
Here,  we use the first million and the first ten million vectors from the dataset of NN-dimensional deep descriptors introduced in [N]
 NNNN    SIFTNM, ≈ N bytes SIFTNM, ≈ NN bytes  DEEPNM, ≈ N bytes DEEPNM, ≈ NN bytes  DEEPN0M, ≈ N bytes DEEPN0M, ≈ NN bytes  San Francisco, ≈ N bytes San Francisco, ≈ NN bytes Figure N: Mean squared compression errors on four datasets for different methods and memory budgets
Arborescence  coding (red) provides considerably smaller errors comparing to the baselines except for SIFTNM (NN bytes), where TwinOPQ  performs best
We also show the average compression errors for different classes of nodes within arborescence coding (leaves  with parents, intermediate nodes that have both a parent and children nodes, roots with children, singletons)
Compression  errors for the leaves and the intermediate nodes are much smaller than for the singletones and the roots
The most of the  points in arborescence coding are the leaves or the intermediate nodes (see the distributions of classes in the pie charts),  which leads to arborescence coding having smaller compression error overall
 • SFLD (San Francisco Landmark dataset) [N] contains a database of N.N million images of buildings in San Francisco collected with a vehicle-mounted camera
 We compute NNN-dimensional deep SPoC descriptor [N] for all images
 Coding methods
We evaluate arborescence and star  codings introduced in this paper
We invariably use the size  of codebooks K = NNN as is done in most other quantiza- tion works, since it lead to small look-up tables and convenient one-byte code entries ti,k
We consider two different codebook numbers M = N, NN (much bigger M is less in- teresting, because the performance of all methods start to  saturate, and extremely small M such as M = N leads to an impractically poor compression)
The size of the codes  ti is thus either N or NN bytes, plus a few bits (less than one byte) needed to encode the number of children in the case of  general arborescence coding (but not star coding)
On top  of that, an additional byte is needed if fast nearest neighbor  search using (N) is to be performed
 We also consider three baselines
Our first baseline is  “vanilla” optimized product quantization (OPQ) [NN, NN]  with the same number of codebooks M and the same code- book size K leading to same code length as star coding (al- though fast NN search does not need length encoding in this  case)
Our second baseline (TwinOPQ) is a variant of OPQ  that uses two sets of codebooks (sharing the same rotation  matrix), so that each descriptor is encoded by one of the two  sets
At each iteration, after the codebooks and the rotation  matrix are re-estimated, a descriptor can switch to the other  codebook set, if such switch results in a lower compression  error
TwinOPQ has the same number of learnable parameters as our systems (apart from the arborescence structure)
 Our third and strongest baseline is the Multi-D-ADC [N],  which has clear similarities with star coding (as well as  NNN0    ≈ N bytes per vector ≈ NN bytes per vector  Method R@N R@N R@NN R@NN R@NNN R@N R@N R@NN R@NN R@NNN  SIFTNM  OPQ 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN Twin OPQ 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN 0.NNN 0.NNN 0.NNN  Multi-D-ADC 0.NNN 0.NN0 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN StarC 0.N0N 0.NNN 0.N0N 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN N.0  ArborC 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN N.0  DEEPNM  OPQ 0.NNN 0.NNN 0.NN0 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN Twin OPQ 0.NNN 0.NNN 0.NN0 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  Multi-D-ADC 0.N0N 0.NNN 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN 0.NNN 0.NNN 0.NNN StarC 0.NNN 0.NN0 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN 0.NNN 0.NNN 0.NNN  ArborC 0.NNN 0.NNN 0.NN0 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  DEEPN0M  OPQ 0.NNN 0.NN0 0.NNN 0.NNN 0.NNN 0.N0N 0.NN0 0.NNN 0.NNN 0.NNN Twin OPQ 0.NNN 0.N0N 0.NNN 0.NNN 0.N0N 0.NNN 0.N00 0.NNN 0.NN0 0.NNN  Multi-D-ADC 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN 0.NN0 0.NNN 0.NNN 0.NN0 0.NNN StarC 0.N0N 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  ArborC 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.N0N 0.N0N 0.NNN 0.NNN 0.NNN  Table N: Euclidean nearest neighbor search accuracy based on different compression methods for three datasets with the  different code lengths
The standard Recall@k measure (the probability of the true nearest neighbor being retrieved) is used to compare the compression methods
Arborescence coding performance is uniformly higher than for the baselines on  datasets of deep features, while on SIFTNM (NN bytes) TwinOPQ is better for small k
 an earlier IVFADC system [NN])
Multi-D-ADC first uses  “coarse-level” OPQ with M ′ = N large codebooks (K ′ = NNN for SIFTNM and DEEPNM, K ′ = N0NN for DEEPN0M, and K ′ = NNN for SFLD)
K ′ was chosen to allow a slightly more memory for the size of the coarse-level table in MultiD-ADC than the amount of memory spent on arborescence  topologies in arborescence coding
Each descriptor is then  assigned to the closest “centroid” out of K ′N variants cor- responding to different combinations of coarse-level codewords, and then the displacement w.r.t
the centroid is encoded using product quantization (in the rotated system)  with the same M and K as in our method
If fast exhaus- tive ANN search through the dataset is desired then MultiD-ADC also requires storing the quantized length with each  descriptor (an alternative is to either re-compute look-up tables for each visited non-empty cell or to store the tables of  scalar products between coarse-level codebooks and finelevel codebooks)
Overall, the memory footprint of MultiD-ADC in our comparisons is very close to the footprint of  arborescence coding and slightly larger than the footprint of  star coding
 N.N
Results  We compare different compression schemes using the  following two metrics
The mean squared reconstruction  error (MSRE) directly measures the reconstruction accuracy attained by different methods
For each dataset, we  also consider a hold-out set of query vectors, and consider  how well is the nearest neighbor search in the compressed  dataset able to recover the true nearest neighbor
As is  common for this task, we report recall@k measure (for  k = N, N, NN, NN, NNN), which is the probability that the true  nearest neighbor is among k closest neighbors in the com- pressed dataset
Two compression levels (≈ N, NN bytes per vector) were evaluated
 Compression error
The average compression errors on  four datasets obtained by the different coding methods are  presented in Figure N
Star coding and arborescence coding  provide significant improvements in the encoding accuracy
 The improvement over baselines are uniform everywhere  except the setting M = NN for SIFT-NM
In particular, the compression error is reduced by upto N0% on deep datasets
On the dataset of SIFT descriptors the TwinOPQ baseline  provides smaller error, that, probably, reflects the fact that  the SIFT data is a favourable case for (O)PQ methods due to  its histogram-based construction process
We also demonstrate the average compression errors for different classes of  points in arborescence coding
Each point in arborescence  coding belongs to the one of four classes depending on their  role in the arborescence structure
The singleton points do  not have a parent node and children nodes
The roots have  children nodes and do not have parent nodes
The leaves  do not have children nodes (but have parents) and the intermediate nodes have both a parent and children nodes
The  distributions of descriptors over classes are shown in the pie  charts in Figure N
Note, that the leaves and the intermediate nodes are compressed with much smaller errors than the  nodes without parents
Interestingly, the compression errors  for singletons and roots can be higher than average baseline  errors, but as their numbers is relatively small (about N0NN%) the encoding accuracy of the whole dataset is higher
 Approximate nearest neighbor search
Here we evaluate different coding schemes for nearest neighbor search in  compressed databases
The recall@k values obtained with  NNNN    ≈ N bytes per vector ≈ NN bytes per vector  Method R@N R@N R@N R@N0 R@N0 R@N0 R@N R@N R@N R@N0 R@N0 R@N0  San Francisco Landmark  Uncompressed 0.N0N 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN 0.N0N 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN Twin OPQ 0.NNN 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  Multi-D-ADC 0.NNN 0.NNN 0.NNN 0.NNN 0.N0N 0.N0N 0.NNN 0.N00 0.NNN 0.NNN 0.NNN 0.NNN StarC 0.NNN 0.NNN 0.N0N 0.NNN 0.NN0 0.N0N 0.NNN 0.N0N 0.NNN 0.NNN 0.N0N 0.NNN  ArborC 0.NNN 0.NN0 0.NNN 0.NNN 0.NNN 0.N0N 0.NN0 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN  Table N: The average recall w.r.t
ground truth matches obtained with retrieval from the San Francisco database compressed  with different methods and code lengths
Images in the database are presented by NNN-dimensional SPoC descriptors[N]
The performance of arborescence coding is uniformly higher than for baselines for both memory budgets and all lengths of  candidate lists
 different compression schemes of the search databases are  presented in Table N
As can be observed, the higher encoding accuracy results in higher search performance
Arborescence coding provides considerable improvement over  baselines for deep descriptors and perform best in general  except for SIFT-NM (NN bytes), where the TwinOPQ baseline is better for small k
Landmark recognition
We also apply the proposed  methods to the problem of visual localization
We compressed the set of SPoC descriptors of SFLD database images with different coding schemes and produce the list of  candidate matches for each of the uncompressed query images
Then for different methods we compare the mean recall w.r.t
the ground truth matches that are hand-labeled for  each query
In this experiment the database contains both  PCI and PFI images and GPS data is not used (for more  details see the protocol in [N0])
The recall values for different number of candidates are presented in Table N
The  advantage of arborescence coding is uniform for both compression levels and different lengths of candidate lists
 Timings
The most computationally expensive part of  the AnnArbor encoding is the calculation of the topology  and the codes (corresponding to the variables pi and ti) given codebooks and rotation matrix
In our experiments  the encoding of one million points with unoptimized Python  code requires NN minutes on Xeon EN CPU
The update of  codebooks and rotation matrix during learning is typically  much faster, e.g
on SIFTNM/DEEPNM one update requires  four minutes
These timings are obtained with the singlethread implementation of the initialization procedure (section N.N) while the other parts use N0 CPU threads
 Hold-out set encoding
Finally, to confirm the ability of  the AnnArbor scheme to generalize to new datasets, when  the parameters are learned on hold-out data, we performed  the following experiment
We took the last one million  points from the DEEPN0M dataset and encoded them with  the parameters obtained by training on DEEPNM (N bytes)  that does not overlap with the test set
The results in the  Table N demonstrate that the MSRE increase on the holdout set for Star and Arborescence Coding is on par with the  baselines and the relative-performance of the methods on  the hold-out set is the same as on the train set
 Method OPQ TwinOPQ Multi-D-ADC StarC ArborC  In-sample  MSRE 0.NN0 0.NNN 0.NNN 0.N0N 0.NNN  Out-of-sample  MSRE 0.NNN 0.NNN 0.NNN 0.N0N 0.NNN  Table N: The encoding mean-squared reconstruction error obtained with the different coding schemes trained on  the test and hold-out sets
The first row corresponds to  the setup where the coding parameters are trained on the  DEEPNM and the same dataset is encoded
The second row  presents MSRE obtained by the coding schemes trained on  DEEPNM, while the test set consists of the last one million points from the DEEPN0M which does not overlap  with DEEPNM
Eight bytes encoding is used in both setups
 The relative performance of coding schemes is the same between the two lines
 N
Discussion  We have presented a new descriptor coding scheme (arborescence coding) and its variant (star coding)
The new  schemes can be implemented on top of almost any of the existing quantization methods (and, in fact, almost any vector  compression methods), and are able to reduce the coding  error of the underlying method considerably by arranging  descriptors into arborescence graphs and coding the relative displacements rather than absolute positions
The encoded datasets still permit efficient search using look up tables, while the memory overhead that is required to store  the topology of the arborescences is very small
 To the best of our understanding, the source of the considerable reduction of error within arborescence coding is  the ability of descriptors to choose their parents among large  pools of potential parents
Even though the distribution of  displacements between neighbors may not be much easier  to model than the distribution of absolute positions (as even  nearest neighbors in the high-dimensional space are usually  far away), each descriptor can get encoded with low error  if only one neighbor (not necessarily the nearest one) corresponds to the displacement with low quantization error
 Future work involves implementations of arborescence  coding and star coding on top of other quantization  schemes, as well as combination of arborescence coding  with indexing approaches
 NNNN    Acknowledgement
VL is supported by the MES RF  grant NN.NNN.NN.000N
 References  [N] R
Arandjelovic and A
Zisserman
Extremely low bitrate nearest neighbor search using a set compression tree
 TPAMI, NN(NN):NNNN–NN0N, N0NN
 [N] A
Babenko and V
Lempitsky
The inverted multi-index
In  Proc
CVPR, N0NN
 [N] A
Babenko and V
S
Lempitsky
Additive quantization for  extreme vector compression
In Proc
CVPR, N0NN
 [N] A
Babenko and V
S
Lempitsky
Aggregating deep convolutional features for image retrieval
In Proc
ICCV, N0NN
 [N] A
Babenko and V
S
Lempitsky
Tree quantization for largescale similarity search and classification
In Proc
CVPR,  N0NN
 [N] A
Babenko and V
S
Lempitsky
Efficient indexing of  billion-scale datasets of deep descriptors
In Proc
CVPR,  N0NN
 [N] S
Battiato, G
Gallo, G
Impoco, and F
Stanco
An efficient re-indexing algorithm for color-mapped images
IEEE  Transactions on Image processing, NN(NN):NNNN–NNNN, N00N
 [N] V
Chandrasekhar, Y
Reznik, G
Takacs, D
M
Chen, S
S
 Tsai, R
Grzeszczuk, and B
Girod
Compressing feature sets  with digital search trees
In Proc.ICCV Workshops, N0NN
 [N] D
M
Chen, G
Baatz, K
Köser, S
S
Tsai, R
Vedantham,  T
Pylvänäinen, K
Roimela, X
Chen, J
Bach, M
Pollefeys,  B
Girod, and R
Grzeszczuk
City-scale landmark identification on mobile devices
In Proc
CVPR, N0NN
 [N0] Y
Chen, T
Guan, and C
Wang
Approximate nearest neighbor search by residual vector quantization
In Sensors, N0N0
 [NN] V
Cuperman and A
Gersho
Vector predictive coding of  speech at NN kbits/s
IEEE Transactions on Communications,  NN(N):NNN–NNN, NNNN
 [NN] T
Ge, K
He, Q
Ke, and J
Sun
Optimized product quantization for approximate nearest neighbor search
In Proc
 CVPR, N0NN
 [NN] V
Gripon, M
Rabbat, V
Skachek, and W
J
Gross
Compressing multisets using tries
In Information Theory Workshop (ITW), N0NN
 [NN] H
Jégou, M
Douze, and C
Schmid
Product quantization  for nearest neighbor search
TPAMI, NN(N), N0NN
 [NN] D
Johnson, S
Krishnan, J
Chhugani, S
Kumar, and  S
Venkatasubramanian
Compressing large boolean matrices using reordering techniques
In vldb, N00N
 [NN] Y
Kalantidis and Y
Avrithis
Locally optimized product  quantization for approximate nearest neighbor search
In  Proc
CVPR, N0NN
 [NN] D
G
Lowe
Distinctive image features from scale-invariant  keypoints
International Journal of Computer Vision,  N0(N):NN–NN0, N00N
 [NN] M
Norouzi and D
J
Fleet
Cartesian k-means
In Proc
 CVPR, N0NN
 [NN] Y
A
Reznik
Coding of sets of words
In Proc
DCC, N0NN
 [N0] R
Salakhutdinov and G
Hinton
Semantic hashing
International Journal of Approximate Reasoning, N0(N):NNN–NNN,  N00N
 [NN] L
R
Varshney and V
K
Goyal
Ordered and disordered  source coding
In Proc
UCSD Workshop Inform
Theory Its  Applications, N00N
 [NN] T
Zhang, C
Du, and J
Wang
Composite quantization for  approximate nearest neighbor search
In Proc
ICML, N0NN
 NNNNObject-Level Proposals   Object-level Proposals  Jianxiang MaN Anlong MingN Zilong HuangN Xinggang WangN Yu Zhou N,∗  NBeijing University of Posts and Telecommunications NHuazhong University of Science and Technology  {jianxiangma,minganlong,yuzhou}@bupt.edu.cn {hzl,xgwang}@hust.edu.cn  Abstract  Edge and surface are two fundamental visual elements  of an object
The majority of existing object proposal approaches utilize edge or edge-like cues to rank candidates,  while we consider that the surface cue containing the ND  characteristic of objects should be captured effectively for  proposals, which has been rarely discussed before
In this  paper, an object-level proposal model is presented, which  constructs an occlusion-based objectness taking the surface  cue into account
Specifically, the better detection of occlusion edges is focused on to enrich the surface cue into proposals, namely, the occlusion-dominated fusion and  normalization criterion are designed to obtain the approximately overall contour information, to enhance the occlusion edge map at utmost and thus boost proposals
Experimental results on the PASCAL VOC N00N and MS COCO  N0NN dataset demonstrate the effectiveness of our approach,  which achieves around N% improvement on the average recall than Edge Boxes at N000 proposals and also leads to a  modest gain on the performance of object detection
 N
Introduction  Object proposal aims to generate a certain amount of  candidate bounding boxes to determine the potential objects  and their locations in an image, which is widely applied to  many visual tasks for pre-processing, e.g., object detection  [NN], [NN], segmentation [N], [NN], object discovery [NN],  and ND match [N]
Due to the great practicability, it has  been a significant research recently
 As Perception of the Visual World writes, “The elementary impressions of a visual world are those of surface and  edge.” Indeed, edge and surface are fundamental to perceive  everything in vision, including objects
Most of existing approaches utilize the edge or edge-like cues to generate proposals, but the surface cue has been rarely discussed
The  main reason is that achieving the high-level surface cue in  ∗ Corresponding Author
 (d)  (b)(a)  (c)  Figure N
The formation of occlusion and contour
(a) a natural  image, (b) the surface of the object cow and its projection, (c)  the contour of the cow formed by occlusion edges in red, (b) the  detailed occlusion edges
 an unsupervised manner is a challenging task
 From the perspective of optics [NN], the smooth surface  of an object presented on the ND image forms the complete  and continuous contour, which is produced by the occlusion  events in the ND space [NN]
Fig.N illustrates the formation  of occlusion and contour
In (b), the optical rays (orange  lines) project the object cow in the ND space onto the background, and then its surface is delineated by contour in the  ND image
It is observed that the contour just occurs at the  boundary where the surface of the cow occludes the background
Formally, the contour is composed of a set of occlusion edges
In this paper, an occlusion edge is an edge signalling depth discontinuity between regions, and the edge  is called the basic edge for clarity
As shown in Fig.N (c)  and (d), the red occlusion edge is essentially a blue basic  edge between two junctions, circled in yellow
Moreover,  discontinuous occlusion edges form the complete contour,  corresponding to the object surface
Consequently, occluNNNN    (a) (c)(b)  Figure N
The comparative results of proposals, where the top  corresponds to Edge Boxes [N0] while the bottom is our approach  with the surface cue added
(a) the proposals, where blue boxes  are the best candidates for the found ground truth boxes in green,  and the red are missing ones, (b) the edge maps, (c) the detailed  edges contained in the green and red boxes in (b)
 sion edges are employed to capture the surface cue
Based  on the discussion above, the contour produced by connecting occlusion edges reflects the boundary of object surface  in the ND space, which is similar to [NN]
Thus, a novel  model based on the occlusion edges is presented to obtain  the surface cue effectively to boost the performance of proposals
 The comparative performance is illustrated in Fig.N,  where the top is the result of Edge Boxes [N0], and the bottom corresponds to our method with the surface cue introduced
It is observed that the edge map of Edge Boxes in  (b) produces weak and discontinuous response, which poorly delineates objects, e.g., the big boat in the green box and  the small boat in the red box are not coherently complete,  leading to the loss of proposals and their localization accuracy in (a)
However, our informative occlusion edge map  generated from the surface cue appears more consistent, and  strengthens the objects’ contours corresponding to their surface in the ND world, which depicts objects more saliently  for proposals
As shown in (b) and (c), the occlusion edges  capturing the surface cue obviously contribute to the comprehensive and accurate discovery of objects, e.g., the small  boat is found with our edge map, and the big boat is localized more precisely than Edge Boxes
 In this paper, a novel object-level approach of proposal  generation is presented, where the surface cue is considered in the form of occlusion edge
To this end, occlusion edge detection is firstly demanded and formulated as  a supervised learning task
Based on the work in [NN], the  edge cues are extracted to form feature samples and the kernel ridge regression is applied to acquire the occlusion  edge map
Moreover, a novel sparsity induced optimization  objective with Huber loss [NN] is proposed to dynamically  select a set of proper training samples, i.e., the basis
To  further enrich the surface cue for proposals, an occlusiondominated fusion is designed to obtain the overall contour  information, namely, a more reliable occlusion edge map
 In addition, it is observed that normalization is beneficial to  most of proposals for small objects
Hence, a specific normalization criterion is proposed to measure its effect and  determine whether the normalization should be done or not,  which improves the occlusion edge map at utmost
 In summary, our contributions lie in:  N
An object-level proposal approach is presented with  the surface cue considered
To the best of our knowledge, this is the first paper to introduce the surface cue  into proposals
 N
Occlusion edges are novelly utilized for the capture of  object surface cue to enhance proposals, and a whole  occlusion-based framework is constructed for the better occlusion edge map and corresponding objectness
 N
In contrast with Edge Boxes, our approach achieves  N% improvement on the average recall at N000 proposals, which also leads to a modest gain on the performance of object detection
 N
Related work  In general, two main categories may be distinguished for  object proposals: window scoring approaches and grouping approaches
The former utilize a set of sampled windows to score and sort them based on the likelihood of containing an object to remove proposals with low rankings,  e.g., Objectness [N] combines several image cues measuring characteristics of objects in a Bayesian framework, Bing  [N] proposes a simple and powerful feature called binarized  normed gradients to improve the search for objects using  objectness scores
The latter usually partition an image into multiple patches and merge them with specific criteria to  generate candidate region proposals, e.g., Selective Search  [NN] combines the strength of both an exhaustive search and  segmentation, CPMC [N] exploits multiple graph-cut based  segmentations with multiple foreground seeds and biases to  propose objects, and MCG [N] develops the multiple hierarchical work by combinatorially grouping regions
However,  these state-of-the-art methods rarely consider the ND cues  of object surface, while our object-level method takes the  surface cue into account
In addition, recent deep learning  based works achieve excellent performance for proposals,  e.g., Deep Mask [N0] and Sharp Mask [NN], but they may be  at the cost of efficiency
 N.N
Reviewing edge boxes  Since our object-level approach closely depends on occlusion edges, we deeply review Edge Boxes [N0], which  defines the specific objectness score based on an edge map  NNNN    to model the observation
Here, we thoroughly probe into  the deficiency of the basic edge map [N] utilized in it:  • The basic edge map involves many tiny edges with weak response, so some relatively large candidate boxes  containing them are likely to obtain higher scores than real  yet small ones, which brings the great difficulty to small object proposals
In Fig.N (b), the boat contained in the green  dashed box is composed of weak edges, leading to the lower  score, while other weak edges contained in a relatively large  box score higher than the boat, so it is more likely to be an  object, e.g., the red dashed box, which is a false judgement
 • Most of the basic edges are incomplete and weakly continuous, making the objects with large aspect ratio hard  to find
The reason is that the candidate boxes intersecting  the weak edges achieve the higher score ranking, e.g., for  the train in the green dashed box in Fig.N (b), the response  gets so weak in the half that the red box truncating its weak  edges acquires a higher score than the whole train
 Therefore, we attempt to address the issues above and  improve the edge response to promote proposals, namely,  enhance the response and consistency of object contours  and weaken or remove false contours to obtain a more reasonable edge map, i.e., the occlusion edge map
 N
Object-level proposals  Since occlusion effectively captures the surface cue, we  focus on occlusion estimation and occlusion-based objectness to propose objects
However, occlusion edges in the  complex scenarios are hard to detect completely, which  needs further improvement
Considering that basic edges  provide overall yet weak response, an occlusion-dominated  fusion is elaborately for a more reliable occlusion edge map  to compensate for the lost surface cue
 N.N
Occlusion edge response  With the edge representation in [NN], F ∈ RU×N denotes the sample matrix with N training edges, each of which has U dimensional features
However, such immense and mis- cellaneous samples greatly increase the complexity when  training the occlusion edge detector, so a set of basic samples are necessary to accelerate learning and boost accuracy
 Specifically, the basis matrix B ∈ RU×M is learnt to rep- resent the original samples approximately and as exactly as  possible, namely, F ≈ BS, where S ∈ RM×N is the coeffi- cient matrix for B, M is the number of basis and M ≪ N 
[NN] employs the Mean Shift Clustering [N] to obtain the  cluster centers as representative samples, which are fixed  and may include some noises
To avoid the adverse effects  of them, dynamic basis learning is novelly introduced
 Motivated by the sparse coding [NN], we present a sparsity induced optimization objective with the Huber loss [NN],  (a) (b) (c)  Figure N
(a) the natural images, with ground truth boxes shown  in green, (b) the basic edge maps, with ground truth boxes shown  in green and false proposal boxes shown in red, (c) the occlusion  edge maps with our approach
 which is formulated as:  min B,S  ∑  i  ∑  j  Hα(rij) + μ‖S‖N  s.t
B ≥ 0, S ≥ 0, ‖bi‖ N N ≤ d, ∀ i = N, ...,M  (N)  where d is a constant and ||S||N = ∑M  i=N  ∑N  j=N |si,j | de- notes the ℓN-norm of the matrix
The residue ri,j = fi,j − bi·s·j indicates the reconstruction error of each dimension
 Hα(·) denotes the Huber loss function with a parameter α
which is defined as:  Hα(r) =  {  rN/N |r| < α  α|r| − αN/N |r| ≥ α (N)  According to Eq.(N), if the residue |r| < α, representing the normal samples, the objective is the ℓN-regularized loss
Otherwise, it means that there may exist noises, i.e., the  edges are useless for reconstruction, and hence the objective is the ℓN-regularized loss, which is insensitive to large errors
Therefore, the Huber loss is robust to accommodate noises caused by the arbitrary of the edges, while ||S||N encourages each edge to be approximated by a sparse combination of the basis
 Taking Eq.(N) into consideration, Eq.(N) can be approximately converted to the weighted least square problem with  sparsity and non-negativity constraints:  min B,S  N  N W ⊙ ‖F − BS‖NF + μ‖S‖N  s.t
B ≥ 0, S ≥ 0, ‖bi‖ N N ≤ d, ∀ i = N, ...,M  (N)  where ⊙ is the Hadamard product of matrices, and W can be interpreted as the weight matrix of the residue r
Given the pth iteration of the optimization procedure, each element of W is defined as:  wpij =  ⎧  ⎨  ⎩  N |rpij | < α α  |rpij | |rpij | ≥ α  (N)  NNNN    Aiming to solve the optimization problem in Eq.(N), we  alternate between updating B and S
Fixing B, with the nonnegativity constraint to S, the objective is similar to the sparse coding, and thus the update rule in [NN] is employed  to optimize S
In turn, fixing S, the formula is reduced  to a conventional weighted least square problem with nonnegativity constraint, which can be done efficiently by the  Lagrange dual in [NN]
 With the optimized training samples B∗, the kernel ridge  regression in [NN] is utilized to learn the occlusion classifier, which has a simple closed form solution, i.e., v = (K + γI)−NL, where K = κ(B∗,B∗) is a kernel matrix calculated by the kernel function κ, γ is the regularization parameter, I is an identity matrix and L is the occlusion label vector
When testing, we calculate regression values for  each edge with the trained classifier and only positive ones  are retained, which is stated as:  ce = max(0, vκ(B ∗, fe)) (N)  where fe is the feature vector of the edge e
Consequently, the occlusion confidence cp of each edge pixel p ∈ e is ce
Then, the occlusion edge map is constructed by assigning  corresponding confidence cp to each edge pixel p, denot- ed by the matrix Ec
Note that the following edge maps with response known are obtained in the same way
As  seen in Fig.N (c), the occlusion edge maps strengthen the  edge response of small objects, e.g., the boat, and remove  a certain amount of irrelevant edges
Moreover, the edges  of objects are more continuous and complete in comparison  with edge response in Fig.N (b), which can delineate objects  more saliently and contribute to finding proper proposals,  e.g., the boundary of long train
 N.N
Occlusion-based objectness  Since the contour produced by connecting occlusion  edges reflects the boundaries of object surface in the ND  space, the discovery of objects directly from them seems  so simple
Unfortunately, due to the complexity of natural scenes, e.g., the similar appearance to the background  or the heavy shading, occlusion edges cannot be correctly  and completely detected
However, our object-level model is no need of strictly closed and continuous occlusion  edges to propose objects
With the rough outline of objects,  the objectness of each box b is evaluated directly based on the degree of overlapping between occlusion edges and box  boundaries, which is formulated as:  Γ(b) = ∑  e∈b−bo  Ce − ∑  e∈Ob  sin θ(e,b)Ce (N)  where bo is the inner box with half size centered in b, and Ce is the sum of occlusion edge confidence ce for all pixels in the edge e
Ob is the set of occlusion edges overlap- ping the box b’s boundary, which can be obtained efficient- ly with the two data structures in [N0]
θ(e,b) ∈ [0  ◦, N0◦] is  (a) (c)(b)  (d) (f)(e)  θ(e,b)  Figure N
(a) an object in the image, (b) the occlusion edge map  with improper proposals in the three boxes, where the warmer  color corresponds to the higher occlusion confidence, (c) the occlusion edge map with the best proposal in the large yellow box,  which obtains a relatively high score ranking, (d), (e) and (f) respectively correspond to the three proposal boxes in (b)
 the angle between e and its intersecting box boundary, and the weight sin θ(e,b) can be interpreted as the dissimilarity between them
Note that the score should be accordingly  scaled like Edge Boxes
 According to Eq.(N), the objectness is mainly related to  several factors: the number and confidence of occlusion  edges straddling the box’s boundary, the occlusion edges  included in the inner box, and the orientation disparity between box’s boundary and occlusion edges at the neighbourhood of the box
As shown in Fig.N, the large yellow  proposal box in (c) obtains a higher score than the three  boxes in (b)
Firstly, the box’s boundary is almost tangent to the detected occlusion edges of the bird, meaning that  the proposal covers the object along its boundary approximately, i.e., θ(e, b) is small as marked in (c)
Secondly, the inner yellow box hardly contains occlusion edges, which  indicates its interior appearance is coherent and it is more  likely to be an object
Hence, despite the contour of the  bird is not complete, we still propose it correctly with our  occlusion-based model
In contrast, (d), (e) and (f) illustrate several typical improper proposals, corresponding to  the white, green and pink boxes in (b) respectively
The red  points in (d) shows all intersection angles between occlusion edges and the box’s boundary are large, which is much  likely that the white box in (b) truncates the object and thus  the score is degraded
The green box in (e) is better than  (d) because some of the intersection angles are relatively small
For the large pink box in (f), although the boundary  of the box is consistent with occlusion edges like (c), the  inner pink box contains many occlusion edges with large  confidence, which means there may exist a more suitable  proposal box to represent an object
 NNNN    (a) (c)(b)  Figure N
(a) the image whose occlusion edges are hard to detect completely, (b) the occlusion edge map Ec, (c) the improved  occlusion edge map Ẽ c  with the lost response circled in yellow 
 N.N
Occlusion-dominated fusion  Considering that the basic edges can compensate for  the detailed response lost in occlusion map, an occlusiondominated fusion is introduced to further promote occlusion  edge map, which can make our occlusion-based objectness  more reliable
Specifically, due to the informative ND characteristic of occlusion, we novelly regard occlusion confidence as the weighting term to dominate the fusion, which  is formulated as:  Ẽ c = Ec ⊙ Ec + (Ecm − E  c)⊙ Eg (N)  where Ecm is the matrix filled with the maximum confidence  of all occlusion edges in the image
Eg , Ec and Ẽ c  respectively represent the basic edge, occlusion edge and improved occlusion edge maps constructed like Section N.N
 According to Eq.(N), for a certain edge pixel, we obtain  the corresponding response in the edge map matrix with its  location i and j, i.e., Ẽcij = E c ijE  c ij + (E  c m(ij) − E  c ij)E  g ij 
 Thus, the occlusion response Ecij is the weight between E c ij  and Egij , which can adjust the response based on both edge  and surface cue
For instance, a large Ecij makes Ẽ c ij prefer  Ecij itself, while a small one places more weight on E g ij to  compensate for the lost response
As shown in Fig.N, the  improved occlusion edge map Ẽ c  not only further enhances  the occlusion edges of real objects, but also recovers weak  yet necessary response of ambiguous boundaries lost in Ec,  e.g., the masts of ships in (a) are too narrow to be detected in  the occlusion edge map (b), but with the supplement of basic edges, the occlusion edge map in (c) provides the weak  response for them, circled in yellow, and thus can obviously  contribute to precise discovery of ships
 However, some small objects with low score rankings  are still difficult to find
To tackle this issue, we normalize the improved occlusion edge map into [0, N], namely,  Ẽ c  n = Ẽ c ⊘ Ẽ  c  m, where ⊘ is the element-wise division and  Ẽ c  m is similar to E c m
For these small objects, the normalization can diminish their score distance to obvious objects, which promotes their rankings and makes them easier to  find, but it may risk losing some informative response
Thus  a specific normalization criterion is designed to measure its  N.NNN  0  N.NNN  0.0NN  N.N0N  0.N0N  N.NNN  0.0NN  0.N0N  0.NNN  (a) (b) (c)  Figure N
(a) the natural images, the top image is in need of normalization, while the bottom one is not, (b) the ground truth boxes with maximum and minimum scores obtained by Ẽ c  , (c) the  ground truth boxes with maximum and minimum scores obtained  by Ẽ c  n 
 effect
Given an image, a score ratio g is defined as:  g = maxb∈ΩΓ(b)  minb∈ΩΓ(b) + ε (N)  where Ω is the set containing all ground truth boxes, Γ(b) is the objectness in Eq.(N) and ε is a sufficiently small number for smoothing
Eq.(N) is the ratio of maximum objectness  to minimum objectness among ground truth boxes, which  partly reflects the score distance between small and obvious  objects
Based on Ẽ c  and Ẽ c  n, we can calculate the ratios  gc and gcn respectively with Eq.(N)
Then the normalization criterion is stated as:  E =  {  Ẽ c  n g c > gcn  Ẽ c  gc ≤ gcn (N)  where gc > gcn means normalization works, otherwise it  fails to reduce the distance so that Ẽ c  is unchanged
Fig.N  illustrates the two situations, where the top image is in need  of normalization, while the bottom one is not
Scores of  all ground truth boxes are marked near the green boxes in  (b) and (c) respectively, and the corresponding score ratios  are obtained based on Eq.(N)
For the top image, gc ≫ gcn indicates normalization is much significant to the image
 But for the bottom one, gc < gcn means Ẽ c  is more effective
 N
Experiments  In this section, we mainly evaluate the performance of  our approach on the PASCAL VOC N00N dataset [N0]
Referring to the experimental settings of Edge Boxes [N0], we  employ the training and validation sets to report the results  on variants of our algorithm, while the test set is used for  contrast with state-of-the-art approaches
For each dataset,  we measure the results with three proposal metrics: Firstly,  we set the Intersection over Union (IoU) threshold to 0.N  and vary the number of object proposals from N0 to N0000
 NNNN    (a) (b) (c)  0.N 0.N 0.N 0.N 0.N N 0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  N  IoU overlap threshold  re ca  ll  proposals=N000        N0 N  N0 N  N0 N  N0 N  0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  N IoU between [0.N,N]  # proposals  av er  ag e   re ca  ll        N0 N  N0 N  N0 N  N0 N  0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  N IoU=0.N  # proposals  re ca  ll        Edge Boxes Occlusion Occlusion + basic edges Ours  Figure N
Comparison on variants of our approach on the PASCAL VOC N00N dataset
(a) recall versus number of proposals given IoU =  0.N, (b) recall versus IoU overlap threshold given N000 proposals, (c) average recall versus number of proposals between IoU 0.N to N
 (a) (b) (c)  0.N 0.N 0.N 0.N 0.N N 0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  N  IoU overlap threshold  re ca  ll  proposals=N000        N0 N  N0 N  N0 N  N0 N  0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  N IoU between [0.N,N]  # proposals  av er  ag e   re ca  ll        SSearch Geodesic CPMC MCG Edge Boxes Ours  N0 N  N0 N  N0 N  N0 N  0  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  N IoU=0.N  # proposals  re ca  ll        Figure N
Comparison of our approach with state-of-the-art hand-crafted methods on the PASCAL VOC N00N dataset
(a) recall versus  number of proposals given IoU = 0.N, (b) recall versus IoU overlap threshold given N000 proposals, (c) average recall versus number of  proposals between IoU 0.N to N
 Secondly, given N000 proposals, the IoU threshold ranges  from 0.N to N
Thirdly, the average recall (AR) between IoU  0.N to N is introduced in [NN] to summarize proposal performance across IoU thresholds, varying from N0 to N0000 proposals too
In addition, we make comparison on the capability of variants of our algorithm to propose specific objects,  and finally explore the effects of different existing methods  on object detection
Recently, MS COCO has become the  mainstream dataset for object proposal, especially for deep  learning based works, thus some additional experiments are  done on the MS COCO N0NN dataset [NN]
 N.N
Comparison on variants of the approach  First, we make comparison on variants of our approach,  and the results are shown in Fig.N
Edge Boxes is regarded  as the baseline, the second variant utilizes our occlusionbased objectness with primary occlusion edge map and the  third employs the occlusion-dominated fusion
The final is  our whole method including normalization criterion
It is  observed that our occlusion-based objectness is better than  Edge Boxes
The reason is that occlusion edges indicate the  ND discontinuity of object surface and provide the informative surface cue not involved in edges, which increases the  accuracy of proposals and makes localization more precise
 Then, when we properly add basic edges to improve occlusion edge map, the recall further rises
Even though for less  proposals or higher IoU threshold, the performance is better than the methods with single edge map
Finally, with  our normalization criterion introduced, there is still a modest gain in accuracy, which demonstrates that the refinement  of occlusion edge response with our approach is effective to  promote object proposals
 NNNN    Edge Boxes (I) (II) Ours  Small size 0.NNN 0.NNN 0.N0N 0.NNN  Aspect ratio 0.NNN 0.NNN 0.NNN 0.NNN  Table N
Comparison results of recall on the specific objects for  N000 proposals and IoU 0.N on the PASCAL VOC N00N dataset
 (I) is the occlusion-based objectness only with occlusion edges
 (II) utilizes the occlusion-dominated fusion
Ours is the whole  framework with the normalization criterion added based on (II)
 Additionally, as mentioned in Section N.N, Edge Boxes  based on the basic edges perform poorly for some specific  objects, e.g., the objects with large aspect ratio or extremely  small size
Thus, we compare the ability of different edge  maps when proposing these difficult objects
In the experiment, a ground truth box b in the image I is defined as a small object if area(b) < 0.0N ∗ area(I), while the box with its aspect ratio more than N is also considered
Table N illustrates the results of recall for N000 proposals and  IoU 0.N
Small objects are too difficult to discover, but the  occlusion-based objectness with primary occlusion edges  (I) obtains N% gain in contrast with Edge Boxes
When  we further improve the occlusion edge map with occlusiondominated fusion (II), the performance gets better
Finally,  with the normalization criterion added, our whole approach  achieves N% improvement than Edge Boxes, which demonstrates the superior ability of our method to propose small  objects
For the objects with large aspect ratio, the promotion is more significant
The recall of occlusion exceeds N%  than Edge Boxes, which indicates that occlusion effectively  preserve the integrality of the objects
Similarly, when we  refine the occlusion edge map with occlusion-dominated fusion and normalization criterion, the results also obtain the  corresponding rise
Above all, our final occlusion edge map  achieves N0% improvement than Edge Boxes for the objects  with large aspect ratio
 Note that the pipeline of the presented method is the  same as Edge Boxes, which only contains two parts: occlusion edge detection and object proposal generation with  the occlusion edge map, namely, both of us train the specific  edge detectors for supervised edge detection, and then use  similar window scoring mechanism for unsupervised object  proposal
For the PASCAL VOC N00N dataset, the runtime  of Edge Boxes is 0.NNs, while ours is 0.Ns, which is nearly  as efficient as Edge Boxes, but achieves much higher recall  when fixing the number of proposals
 N.N
Comparison with hand-crafted approaches  In Fig.N, several hand-crafted methods are selected from  [NN] to evaluate our approach, where their competing results  are provided
Selective Search (SSearch) [NN] and Geodesic  [NN] achieve promising accuracy and perform similarly for  the three metrics
Both of them fall behind at a small numN0 0  N0 N  N0 N  N0 N  0  0.N  0.N  0.N  0.N  0.N  0.N  IoU between [0.N,N]  # proposals  av er  ag e   re ca  ll       SSearch  Edge Boxes  MCG  Deep Mask  Sharp Mask  Ours  Figure N
Comparison results of average recall for N, N0, N00,  N000 proposals between typical hand-crafted and deep learning  based methods on the MS COCO N0NN validation set
 ber of proposals but rise rapidly with candidates increasing, especially at the larger number of proposals and higher  IoU values, and Selective Search gets much powerful for  proposals when the amount is very large (about > N000)
CPMC [N] obtains relatively few yet high-quality proposals
In contrast, Edge Boxes [N0], MCG [N] and our objectlevel approach achieve superior results of recall as a whole,  and ours is the best
Both Edge Boxes and our approach  perform well at the small or large number of proposals
 However, due to the low localization accuracy of window  scoring mechanism, their results get worse than grouping  methods when IoU > 0.N, which adversely affects the aver- age performance
MCG has a comparatively strong ability  to propose objects and localize them precisely, leading to  the competitive average accuracy across all proposals, as  shown in Fig.N (c)
Nevertheless, when we introduce the  informative surface cue into proposals, both the quality and  localization precision are enhanced, and thus the average  recall exceeds around N% than Edge Boxes at N000 proposals
Moreover, our approach outperforms MCG on overall  performance, which demonstrates the effectiveness of our  refined occlusion edge map
 N.N
Comparison with deep learning based ap- proaches  Due to the powerful capability of feature extraction and  well-designed structure of convolutional networks, recent deep learning based works like Deep Mask [N0] and Sharp  Mask [NN] achieve excellent accuracy, and thus outperform hand-crafted methods
As shown in Fig.N, we compare  the average recall (AR) between IoU 0.N and N for N, N0,  N00, N000 proposals on the MS COCO N0NN validation set,  which is larger and more diverse
It is observed that our  method still outperforms other hand-crafted ones, but Deep  NNNN    aero bicycle bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mean  CPMC NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.0 NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Geodesic NN.N NN.0 NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N NN.N N0.N N0.N NN.N  SSearch N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N  MCG NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N  Edge Boxes NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N  Ours NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N  Table N
Fast R-CNN (model M) detection results (AP) on the PASCAL VOC N00N dataset, where mean average precision is listed at the  end
Bold numbers indicate the best proposal method per class
Our approach is better than other state-of-the-art methods for the majority  of objects, and achieve the best mean performance of detection
 0.N0NN  0.N0NN  0.NNNN  0.NNNN  0.NNNN  0.N0NN  0.N0NN  0.NNNN0.NNNN  N.NN.NN  N.NNN  0.NNNN  0.NNNN  0.NNN  N.NNN  N.NN  0.NNNN  0.NNNN  N.NNN  N.0NN 0.N0NN  0.NNNN  N.NNN  N.N0N  N.NNN  N.NNN  N.NNN  N.NNNN.NNN N.NNN  N.NNN  N.NNN  N.0N  0.NNNN 0.NNNN  0.NNN  0.NNN 0.NNN 0.NN  0.NNN  Figure N0
The results of our approach with N000 proposals and  IoU threshold of 0.N
Ground truth bounding boxes are shown in  green and red, where red boxes indicate the objects are not found
 Blue bounding boxes with their obtained scores nearby are the  generated object proposals close to green ground truth boxes
 Mask and Sharp Mask perform better than hand-crafted  methods, including ours
However, our method is entirely based on the unsupervised hand-crafted features and has  comparable strengths
Firstly, it reflects a good tradeoff between recall and speed for object proposal, which takes  less time than most deep learning based methods per image
 Secondly, it does not require fully-labeled training images,  and is easier to be generalized to work on other unlabeled  data, compared with supervised deep learning methods
 N.N
Proposals for object detection  Proposals are commonly applied to object detection,  whose precision is related to the average recall and localization accuracy of candidate boxes
Hence, we consider the  well-known object detector, the Fast R-CNN [NN]
After obtaining N000 proposals with our approach, the Fast R-CNN  is trained on the training and validation sets of the PASCAL  VOC N00N dataset, and then detect objects on the test set
 For efficiency, proposals generated by different methods start from the same pre-trained VGG-M network [N]
Table N  shows the per-class Fast R-CNN detection results of diverse  approaches, as well as mean average precision (mAP)
Selective Search, MCG and Edge Boxes achieve comparable  accuracy results because their proposals are relatively highquality
Geodesic and CPMC perform a little bit worse
 In contrast, due to the improvements of the average recall  and localization accuracy, our approach obtains the best results for the majority of objects and thus the highest mAP  among these methods, which demonstrates that the highquality proposals generated with our approach can be further utilized for effective object detection
 Finally, qualitative results of our proposal method are  shown in Fig.N0
Due to the introduced surface cue with  improved occlusion edges, our approach almost discovers  diverse objects effectively in various scenes, including the  objects with large aspect ratio or small size, which are difficult to find only with the basic edge map
 N
Conclusion  This paper presents a novel object-level proposal model,  where the occlusion-based objectness captures the surface  cue reflecting abundant ND characteristic of objects with occlusion edges
Specifically, to obtain the high-quality occlusion edge map for proposal, an optimization objective with  Huber loss is first constructed to select proper samples for  occlusion detection
Then an occlusion-dominated fusion is  elaborately designed, with specific normalization criterion  added, to further promote the occlusion edges and proposals
Experiments on the PASCAL VOC N00N and MS COCO N0NN dataset demonstrate the superiority of our method  over state-of-the-art methods, especially N% improvement  on the average recall at N000 proposals than Edge Boxes
 N
Acknowledgement  This work was supported by the National Natural Science Foundation of China (NSFC) under grants UNNNNNN0  and NNN0NNNN, Young Elite Scientists Sponsorship Program by CAST (No
YESSN0NN00NN), the Fundamental  Research Funds for the Central Universities under grants  N0NNRCNN and N0NNRCNN, and the National Key Technology Research and Development Program of the Ministry of Science and Technology of China under grant  N0NNBAKNNB0N
 NNNN    References  [N] B
Alexe, T
Deselares, and V
Ferrari
Measuring the objectness of image windows
IEEE Transactions on Pattern Analysis and Machine Intelligence, NN(NN):NNNN–NN0N, N0NN
 [N] P
Arbelaez, J
Pont-Tuset, J
Barron, F
Marqus, and J
Malik
Multiscale combinatorial grouping
In CVPR, pages  NNN–NNN, N0NN
 [N] X
Bai, S
Bai, Z
Zhu, and L
Latecki
Nd shape matching  via two layer coding
IEEE Transactions on Pattern Analysis  and Machine Intelligence, NN(NN):NNNN–NNNN, N0NN
 [N] J
Carreira and C
Sminchisescu
Cpmc: Automatic object  segmentation using constrained parametric min-cuts
IEEE  Transactions on Pattern Analysis and Machine Intelligence,  NN(N):NNNN–NNNN, N0NN
 [N] K
Chatfield, K
Simonyan, A
Vedaldi, and A
Zisserman
 Return of the devil in the details: Delving deep into convolutional nets
In BMVC, N0NN
 [N] M
Cheng, Z
Zhang, W
Lin, and P
Torr
Bing: Binarized  normed gradients for objectness estimation at N00fps
In  CVPR, pages NNNN–NNNN, N0NN
 [N] D
Comaniciu and P
Meer
Mean shift: A robust approach  toward feature space analysis
IEEE Transactions on Pattern  Analysis and Machine Intelligence, NN(N):N0N–NNN, N00N
 [N] J
Dai, K
He, and J
Sun
Convolutional feature masking for  joint object and stuff segmentation
In CVPR, N0NN
 [N] P
Dollar and C
L
Zitnick
Fast edge detection using structured forests
IEEE Transactions on Pattern Analysis and  Machine Intelligence, NN(N):NNNN–NNN0, N0NN
 [N0] M
Everingham
The pascal visual object classes (voc) challenge
International Journal of Computer Vision, NN(N):N0N–  NNN, N0N0
 [NN] R
Girshick
Fast r-cnn
In ICCV, pages NNN0–NNNN, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Spatial pyramid pooling  in deep convolutional networks for visual recognition
IEEE  Transactions on Pattern Analysis and Machine Intelligence,  NN(N):NN0N–NNNN, N0NN
 [NN] J
Hosang, R
Benenson, P
Dollar, and B
Schiele
What  makes for effective detection proposals? IEEE Transactions  on Pattern Analysis and Machine Intelligence, NN(N):NNN–  NN0, N0NN
 [NN] P
Huber
Robust estimation of a location parameter
The  Annals of Mathematical Statistics, NN(N):NN–N0N, NNNN
 [NN] C
Kading, A
Freytag, E
Rodner, P
Bodesheim, and J
Denzler
Active learning and discovery of object categories in the  presence of unnameable instances
In CVPR, N0NN
 [NN] P
Krahenbuhl and V
Koltun
Geodesic object proposals
In  ECCV, pages NNN–NNN, N0NN
 [NN] H
Lee, A
Battle, R
Raina, and A
Y
Ng
Efficient sparse  coding algorithms
In NIPS, N00N
 [NN] T
Lin, M
Maire, S
Belongie, L
Bourdev, R
Girshick,  J
Hays, P
Perona, D
Ramanan, C
L
Zitnick, and P
Dollar
Microsoft coco: Common objects in context
In ECCV,  pages NN0–NNN, N0NN
 [NN] A
Ming, T
Wu, J
Ma, F
Sun, and Y
Zhou
Monocular  depth-ordering reasoning with occlusion edge detection and  couple layers inference
IEEE Intelligent Systems, NN(N):NN–  NN, N0NN
 [N0] P
Pinheiro, R
Collobert, and P
Dollar
Learning to segment  object candidates
In NIPS, N0NN
 [NN] P
Pinheiro, T
Lin, R
Collobert, and P
Dollar
Learning to  refine object segments
In ECCV, N0NN
 [NN] W
Shen, X
Wang, Y
Wang, X
Bai, and Z
Zhang
Deepcontour: A deep convolutional feature learned by positivesharing loss for contour detection
In CVPR, pages NNNN–  NNNN, N0NN
 [NN] R
Szeliski
Computer Vision: Algorithms and Applications
 Springer-Verlag New York, N0NN
 [NN] J
Uijlings, K
van de Sande, T
Gevers, and A
Smeulders
Selective search for object recognition
International Journal  of Computer Vision, N0N(N):NNN–NNN, N0NN
 [NN] R
Vaillant and O
D
Faugeras
Using extremal boundaries  for N-d object modeling
IEEE Transactions on Pattern Analysis and Machine Intelligence, NN(N):NNN–NNN, NNNN
 [NN] N
Wang, J
Wang, and D
Yeung
Online robust non-negative  dictionary learning for visual tracking
In ICCV, pages NNN–  NNN, N0NN
 [NN] J
Wright, A
Y
Yang, A
Ganesh, S
S
Sastry, and Y
Ma
 Robust face recognition via sparse representation
IEEE  Transactions on Pattern Analysis and Machine Intelligence,  NN(N):NN0–NNN, N00N
 [NN] Y
Zhang, X
Chen, J
Li, C
Wang, and C
Xia
Semantic  object segmentation via detection in weakly labeled video
 In CVPR, pages NNNN–NNNN, N0NN
 [NN] Y
Zhu, R
Urtasun, R
Salakhutdinov, and S
Fidler
segdeepm: Exploiting segmentation and context in deep neural networks for object detection
In CVPR, pages NN0N–NNNN,  N0NN
 [N0] C
L
Zitnick and P
Dollar
Edge boxes: Locating object  proposals from edges
In ECCV, pages NNN–N0N, N0NN
 NNNNWordSup: Exploiting Word Annotations for Character Based Text Detection   WordSup: Exploiting Word Annotations for Character based Text Detection  Han HuN∗ Chengquan ZhangN∗ Yuxuan LuoN Yuzhuo WangN Junyu HanN Errui DingN  Microsoft Research AsiaN IDL, Baidu ResearchN  hanhu@microsoft.com {zhangchengquan,luoyuxuan,wangyuzhuo,hanjunyu,dingerrui}@baidu.com  Abstract  Imagery texts are usually organized as a hierarchy of  several visual elements, i.e
characters, words, text lines  and text blocks
Among these elements, character is the  most basic one for various languages such as Western, Chinese, Japanese, mathematical expression and etc
It is natural and convenient to construct a common text detection engine based on character detectors
However, training character detectors requires a vast of location annotated characters, which are expensive to obtain
Actually, the existing  real text datasets are mostly annotated in word or line level
 To remedy this dilemma, we propose a weakly supervised  framework that can utilize word annotations, either in tight  quadrangles or the more loose bounding boxes, for character detector training
When applied in scene text detection,  we are thus able to train a robust character detector by exploiting word annotations in the rich large-scale real scene  text datasets, e.g
ICDARNN [NN] and COCO-text [NN]
The  character detector acts as a key role in the pipeline of our  text detection engine
It achieves the state-of-the-art performance on several challenging scene text detection benchmarks
We also demonstrate the flexibility of our pipeline  by various scenarios, including deformed text detection and  math expression recognition
 N
Introduction  Understanding optical texts has a long history dating  back to the early twentieth century [NN]
For a long time,  the attempts were made for texts of a few languages captured by very special devices, e.g
scanned English documents
With the growing popularity of smart phones, there  have been increasing demands for reading texts of various  languages captured under different scenarios
 We are interested in developing a common text extraction engine for various languages and scenarios
The first  step is to localize texts
It is not easy
Firstly, languages  differ in organization structures
For an example, English  ∗Equal contribution
This work is done when Han Hu is at IDL, Baidu  Research
 characters  words  text-line/ structure  Figure N: The visual hierarchies for various language texts under different scenarios
Different languages and scenarios may differ  in hierarchy, but they are all formed by a basic element, character
 texts include visual blank separation between words, while  Chinese not
For another example, regular human language  texts are organized sequentially, while math expressions are  structural
Secondly, texts may differ in visual shapes and  distortions according to the individual scenarios
Nevertheless, all optical texts share one common property that they  are all formed by characters, as illustrated in Fig
N
It is  natural and convenient that we base a common text detection framework on character detection
 When characters are localized, we can then determine  the structure of texts in a bottom-up manner
The atomicity  and universality of characters enable structure analysis for  various languages and scenarios, e.g., oriented / deformed  text lines and structural math expression recognition (see  representative samples in Fig
N)
 The training of character detectors require a vast of location annotated characters
However, annotating character locations is very inconvenient and expensive, because  the characters are small, easily gluing with each other and  blurry
Actually, most existing large scale real text image  datasets are labeled coarsely in word level, as illustrated in  Table N
 In this paper, we propose a weakly supervised learning  framework to address the problem of lacking real characNNNN0    Dataset # im # word Real/Synth
Anno
 ICDARNN [N0] NNN N,NNN Real char  ICDARNN [NN] N,N00 ∼NNK Real word SVT [NN] NN0 NNN Real word  COCO-Text [NN] ∼NNK ∼NNNK Real word IIIT Nk-word [N0] N.A
N000 Real word  CharN0K [NN] N.A
NM Synth
char  VGG SynthText [N] N00K - Synth
char  Table N: Popular datasets and their properties
Nearly all median and large scale real datasets are annotated in word level
 ter level annotations
It utilizes word annotations as supervision source to train the character detectors
Specifically,  two alternative steps are iterated to gradually refine both the  character center mask and the character model, as illustrated  in Fig
N
By applying this framework, we are able to train  a robust character model by exploiting rich samples in several large scale challenging datasets, e.g
ICDARNN [NN]  and COCO-Text [NN]
 The character model acts as a key module to our text detection pipeline
When applied to challenging scene texts, it  achieves the state-of-the-art performance on several benchmarks, i.e
ICDARNN [N0], ICDARNN [NN] and COCOText [NN]
It is also proved applicable on various scenarios,  including deformed text line extraction and structural math  expression recognition
 N.N
Related Works  There have been numerous approaches for text detection
According to the basic elements they rely on, the approaches can be roughly grouped into five categories:  Character based As mentioned earlier, character is a natural choice to build common detection engines
Nearly all  existing character based methods rely on synthetic datasets  for training [NN, N, N0, NN, NN, NN], because of lacking character level annotated real data
However, synthetic data cannot have a full coverage of characters from various scenes,  limiting the model’s performance in representing challenging real scene texts
Actually, none of the current top methods for the popular ICDARNN benchmark [NN] are based on  character detection
Recently, some sophisticated synthetic  technologies [N] have been invented that the synthetic text  images look more “real”
Nevertheless, real text images are  still indispensable in training more robust character models,  as we will show in our experiments
 Our pipeline is also character induced, but by incorporating a weakly supervised framework, we are able to exploit word annotations in several large scale real datasets to  strengthen the character model
Using this model as a key to  our pipeline, we achieve the state-of-the-art performance on  several challenging scene text detection benchmarks
The  Update Mask  Update Network  Detection Network  Figure N: Illustration of our word supervision training approach for a character model
Two alternative steps are conducted: giving  the current model, compute a response map which is then used  together with word annotations to get a character center mask (red  and green points); giving the character center mask, supervise the  training of character model
 pipeline is flexible for various scenarios such as deformed  texts and structural math expressions
 Text Line based Text line based methods directly estimate line models
These methods are widely adopted in the  field of document analysis [NN], where article layout provides strong priors
They are hard to be applied for nondocument scenarios
 Word based A merit of these methods is that the modern  object detection frameworks, such as faster RCNN [NN] and  SSD [NN], can be conveniently adjusted [NN, N, NN, N0]
Yet,  these methods are limited to languages which have word  representation and visual separation between the words
 Component based Early component or word fragment  based methods [NN, NN, NN, NN, NN, NN, NN, NN] extract candidate text fragments by some manually designed features,  e.g
MSER [N] and SWT [N], and then determine whether  the fragments are real text or not
These methods once led  some popular competitions for well focused texts, e.g
ICDARNN [N0]
However, the performance of these methods  heavily degrades when applied to more challenging scenarios such as ICDARNN [NN] where texts are captured accidentally
In addition, as long as some texts are missed by the  manually designed features, they would never be recalled in  the subsequent steps
 Recently, some component based methods [NN, NN, N,  NN, NN] attempt to learn text components by CNN feature  learning
The components are either representative pixels [NN, NN, N] or segment boxes [NN, NN]
These methods  can learn from word annotations
In addition, text component is also a basic visual element, which may also benefit  a common text detection engine
Nevertheless, our method  takes advantages over these methods in the following aspects: first, characters provide stronger cues, e.g., character  NNNN    Character	  Detector	  Character	  Grouping	  Line	EsNmaNon	  and	RecNficaNon	  Word	  SeparaNon	  Input	  Images  Character	  Grouping	  Non-sequenNal	  RecogniNon	  Text	  Lines	 Words	  Text	Blocks	  Recognized	Texts	  Text	Structure	Analysis		  Figure N: Our pipeline
There are two modules, character detector and text structure analysis
The pipeline is flexible for various  scenarios ascribed to the atomicity and universality of characters
 scales and center locations, for the subsequent text structure analysis module; second, character is a semantic element, while component not
Thus our method is applicable  to problems where direct character recognition is needed,  e.g
match expression; third, our method can utilize loose  word annotations for training, e.g
bounding box annotations in the COCO-Text dataset [NN]
This is because our  method can refine character center labels during training
 For the above component based methods, their noisy labels  are fixed which may harm training
 N
Our Approach  N.N
Pipeline  The pipeline of our approach is illustrated in Fig
N
 Given an image, we first detect characters on it
This  module is shared by various languages and scenarios
Its  performance is crucial for the whole pipeline
Instead  of using synthetic character data alone for training, we  strengthen it by exploiting word annotations from real scene  text datasets
The details of our basic character detector and  the word supervision method are presented in Section N.N  and Section N.N, respectively
 Then the detected characters are fed to a text structure  analysis module, which is application dependent
We handle several typical scenarios
First is the sequential line, a  widely used text structure
We propose a unified method to  extract all of the horizontal, orientated and deformed text  lines
English text lines are optionally separated into words  for word based text recognition methods
Math expression  recognition is another scenario, where characters are nonsequential
We first recognize all the detected characters  and then recover structures connecting characters/symbols  [NN]
Details for text structure analysis are presented in Section N.N
 N.N
Basic Character Detector  The fully convolution neural network is adopted, which  has seen good performance on general object detection, e.g.,  SSD [NN] and DenseBox [NN]
Nevertheless, to be applied  for characters, several factors need to be taken into account
 First, characters may vary a lot in size on different images
 NNN deconv  + NNN conv  NNN deconv  +  NNN conv  loss  input patch NxNNNxNNN convN_N~convN_N  convN_N NNNxNNxNN  convN_N NNNxNNxNN  convN_N NNNxNNxNN  features NNNxNNxNN  predictions NkxNNxNN  labels NkxNNxNN  NNNxNNxNN  NNNxNNxNN  Figure N: Our basic detection network
The network inherits from the VGGNN network model [NN]
 Some characters may be very small, e.g., N0 × N0 pixels in an NM pixel image
Second, texts may appear in very different scenarios, such as captured documents, street scenes,  advertising posters and etc, which makes the backgrounds  distribute in a large space
 To cope with the character size problem, we use feature maps with higher resolution to generate character responses
Specifically, it is N/N of the original image, other than N/NN or N/NN used in general object detec- tion [NN, NN]
Cues from deeper stages with coarser resolutions are merged for better representation power
We  adopt the method in FPN [NN] for such purpose, which  uses an eltsum layer to merge features from two stages with  N× resolution difference
It requires less parameters than other methods, e.g., [NN, NN, NN], for producing the same  number of feature maps
See Fig
N as an illustration
 The network inherits from the VGGNN network model [NN]
 convN features are N× up-sampled by deconvolution and merged with convN features by an eltsum layer
The eltsumed convN-convN features are merged with convN features  in the same way
The produced feature maps are used for  both text/non-text classification and bounding box regression
Nk = (N + N)k score maps are generated, with N for text/non-text classification, N for bounding box regression, and k indicating the number of anchors
We use k = N anchors, representing characters with diagonal lengths of  NN pixels, NN pixels and NN pixels (on the NNN × NNN input patch), respectively
The characters with diagonal lengths  of 0.N ∼ N.N against the anchor’s are regarded positive
 To ease the background variation problem, we adopt a  two-level hard negative example mining approach for training
First is online hard negative mining [NN]
All positives are used for loss computation
For negatives, only top  scored ones are used in loss computation that the ratio between negatives and positives is at most N : N
Second is hard patch mining
During training, we test all training images every N0k iterations to find false positives (using the current character model)
These false positives will be more  likely sampled in the successive mini-batch sampling proNNNN    cedure
 Training NN NNN × NNN patches are randomly cropped from training images to form a mini-batch
N0% of the patches include characters
These positive patches are  cropped from training images according to a randomly selected character and anchor, with some degree of translation/scale perturbation
The other N0% are randomly cropped but with no texts
After N0k iterations, we start to apply the hard patch mining procedure
For negative  patches, half training patches will be hard ones which  should include the current detected false positives
 Inference We conduct multi-scale test for an image
The  used scales are N{0,−N,−N,−N,−N,−N}, respectively
Since only down-sampling scales are involved, the computation  overhead is afforded, at about N.N times, compared to single-scale test
NMS with IoU threshold of 0.N is con- ducted to produce the final characters
It should be noted  that multi-scale testing is indispensable for our basic detector, since we use anchors with only N scales
Explor- ing more efficient basic detector without the need for multiscale testing will be our future work
 N.N
Learning From Word Annotations  Overview As illustrated in Table N, most real text image  datasets are annotated in word level, i.e
ICDARNN and  COCO-Text
Each word is attached with a quadrangle (e.g
 ICDARNN) or a bounding box (e.g
COCO-Text) which  tightly surrounds it, as well as a word category
In this paper, we suppose at least the bounding box of each word is  available
If further a quadrangle or the word category is  given, we use them to strengthen our word supervising procedure
 Our approach is inspired by [N] which successfully  learns object segments from bounding box annotations
It  is illustrated as Fig
N
Two alternative steps are conducted:  given a character model, automatically generate the character mask according to a word annotation; given a character  mask, update the character network
These two steps are  alternative in each network iteration
During the training,  the character masks and the network are both gradually improved
 It is worth noting that the above procedure is only involved in network training
The inference is the same as in  Section N.N
 Character Mask Generation During forward and backward of each mini-batch, the first step is to generate character masks using the current character model and word annotations, as illustrated in Fig
N (bottom)
First, we make forward using the current character model and get a set of candidate characters inside the annotated word bounding box
 We select real characters from these candidate characters by  maximizing score,  s = w · sN + (N− w) · sN = w · area(Bchars)  area(Banno) + (N− w) · (N− λN  λN ),  (N)  where Bchars represents the bounding box of selected char- acters; Banno represents the annotated word bounding box; area(·) denotes the area operator; λN and λN are the largest and second largest eigenvalues of a covariance matrix C, computed by center coordinates of selected characters; w is a weight balancing the two score terms
We find the learning is insensitive to the choice of w, and it is set as 0.N by default
The first term of Eq
(N), sN, favors larger coverage of selected characters to the annotated word bounding box,  while the second one, sN, prefers that all characters locate on a straight line
 We use a similar approach as in [NN] to approximately  maximize Eq
(N)
Firstly, a maximum spanning tree [N],  M , is constructed from the character graph G, which is built by the k-nn of all candidate characters with pair weights defined by their spatial distances and the current text/nontext scores,  wij = exp(− d(i, j)  d ) · (ti + tj), (N)  where d is the average of all distances between k-nn nodes; ti denotes the current text/non-text score of candidate i
It is obvious that eliminating an edge in M equals to partitioning the characters into two groups
For each partitioning, we  choose the group with larger score according to (N), and run  the partitioning procedure greedily and recursively until the  score (N) does not rise
 When a tight quadrangle or character number is given,  we can further improve generated character mask: for the  former, replacing computation of sN in Eq
(N) by area ra- tio of quadrangles; for the latter, adding a term to Eq
(N)  that the mask prefers equal character number compared to  ground truth
 Character Network Updating The generated character  mask can be used as ground truth to supervise network  training
We define a loss L̃ such that more reliable masks  contribute more, as,  L̃ = sL, (N)  where L represents a combination of the confidence loss  and localization loss commonly used in modern object detection frameworks [NN, NN]; s is the score computed by Eq
(N)
 Fig
N shows the gradually updated character masks during training
During training, the character model is gradually improved
 NNNN    word annotation  & initial chars initial epoch #N epoch #N epoch #N0 epoch #N0 chars at   epoch #N0, #N0  Figure N: Updated character responses and the corresponding character masks during word supervised training on ICDARNN datasets
The initial model in the second column is trained by Nk warmup iterations on synthetic data alone
The N ∼ Nth columns are responses during the word supervised training, where the epoch number means for ICDARNN datasets
For illustration, we use bounding box annotations  rather than the original quadrangles in training
Both the responses and character candidates are colored by their scores (indexed by  colormap in Matlab)
 N.N
Text Structure Analysis  Given characters extracted by the methods in Section N.N  and N.N, we conduct text structure analysis for various scenarios, e.g., text lines, words, text blocks, math expressions,  and etc
Fig
N illustrates our text structure analysis methods  for these typical text structures
For text line based applications, we propose a method which can handle arbitrarily  deformed lines
The first step is to group characters
Then a  line model is estimated to describe the line
With the model,  we rectify the text line, which is usually required by modern sequential text recognition systems
Optionally, we separate lines into words
This is not necessary, but enables  word based text recognition methods
 Characters can also be employed for text block extraction, e.g., document layout analysis [NN], and nonsequential text recognition, e.g., math expression recognition [N0]
 In the following, we briefly describe techniques used for  extracting text lines, which are frequently used in our experiments
More details can be found in appendix
 Character Grouping We adapt the method in [NN] to  group characters into text lines or blocks
Given characters  with score larger than a threshold, [NN] first builds a k-nn graph with each node denoting a character candidate
Unary  and pairwise costs are defined on the graph to achieve clustering
The unary costs model relations between characters  and the text category, e.g
character scores
The pairwise  costs model relations between two characters, e.g
spatial  and scale distances
A greedy min-cost flow algorithm is  then conducted to obtain all character groups (see [NN] for  details)
 The method in [NN] is designed for horizontal text lines  only
To be applied in oriented and deformed text lines,  we introduce a higher-order cost which models relations between three characters
To reserve the efficiency of pairwise  graph, we use character pairs instead of characters as graph  nodes
The character pairs are spatially close characters  with high scores and small spatial/scale distances
Then the  unary and pairwise costs in the old graph can be modeled as  unary costs in the new graph, while the higher order costs,  e.g
angle distance, can be modeled as pairwise costs in the  new graph
The same as in [NN], We then conduct a greedy  min-cost flow algorithm on the new graph to achieve character grouping
It can handle oriented and deformed text  lines, ascribed to the introduction of higher-order costs
 Line Model Estimation and Rectification For each  character group, we fit three text line models with increasing complexity
First is 0-order model
Text lines are either horizontal or vertical
Second is N-order model
Text  lines can be arbitrarily orientated
Last is a piecewise linear  model, where a restricted polygon is adopted to represent a  text line
 A model selection approach is conducted to choose a  model with best balance between fitting accuracy and model  complexity
Given the estimated model, we rectify the text  line using thin plate spline (TPS) [N] method, where the vertexes of the text line model are used as control points
 Word partition Some text recognition systems can process only word inputs
To enable usage of such systems,  we optionally separate text lines into words
An LSTM [N]  based word blank detection method is applied on the rectified text line
Words are separated accordingly
 NNNN    N
Experiments  In this section, we first do ablation studies on synthetic  data where character level annotations are provided
Both  our basic detector and the word supervision approach are  evaluated
Then we apply our character induced text detection pipeline on scene text benchmarks
Finally, we show  its applications to various scenarios
 N.N
Datasets and Evaluation  Four datasets are used in the experiments:  • VGG SynthText-part
The VGG SynthText  datasets [N] consist of N00,000 images, generated by a  synthetic engine proposed in [N]
The images have detailed character-level annotations
For experiment efficiency, we randomly select N0,000 images for training  and N,000 images for validation
This subset is referred  to as VGG SynthText-part
 • ICDARNN
The ICDARNN datasets [N0] are from  the ICDAR N0NN Robust Reading Competition, with  NNN natural images for training and NNN for testing
 The texts are annotated with character-level bounding boxes, and they are mostly horizontal and well focused
 • ICDARNN
The ICDARNN datasets [N0] are from the  ICDAR N0NN Robust Reading Competition, with N000 natural images for training and N00 for testing
The images are acquired using Google Glass and the texts  accidentally appear in the scene without user’s prior  intention
All the texts are annotated with word-level  quadrangles
 • COCO-Text
The COCO-Text [NN] is a large scale  dataset with NN,NNN images for training and N0,000  for testing
The original images are from Microsoft  COCO dataset
 The VGG SynthText-part is mainly used for ablation  experiments
Both character-level and word-level evaluations are conducted by using the PASCAL VOC style criterion (≥ 0.N Intersection-over-Union for a positive detec- tion)
For benchmark experiments on ICDARNN, ICDARNN  and COCO-Text, the evaluation protocols provided by the  datasets themselves are adopted
Specifically, for ICDARNN  and ICDARNN, we use the online evaluation system provided with the datasets
For COCO-Text, the protocol provided by the dataset is used for evaluation
 N.N
Implementation Details  The VGGNN model pretrained on the ILSVRC CLSLOC dataset [NN] is adopted for all experiments
 Given different datasets, we train three character models
 The first is trained by synthetic character data alone, i.e
N0k  0.N 0.N 0.N 0.N 0.N 0.N N  pr ec  is io  n  0.N  0.N  0.N  0.N  N  recall  Faster RCNN  our (N0k, one) our (Nk, two)  our (N0k, two)  our (Nk+NNk word, two) SSD  Figure N: Character detection performance of our basic detec- tion network, the faster RCNN and SSD methods on the VGG  SynthText-part datasets
Four variants of our method are presented
The first term in brackets indicates the used supervision  source
The second term indicates the used hard negative mining  strategy, with “one” representing one-level hard negative mining  and “two” representing two-level hard negative mining
 training images from VGG SynthText-part datasets
Second  is trained on Nk ICDARNN training images plus N0k VGG  SynthText-part
N0% are sampled from ICDARNN and the others are sampled from VGG SynthText-part
The third  is trained on COCO and VGG SynthText-part, with minibatch also sampled half-half from the two datasets
These  three models are dubbed as “VGGNN-synth”, “VGGNNsynth-icdar” and “VGGNN-synth-coco”, respectively
 We use SGD with a mini-batch size of NN on N GPUs (NN per GPU)
A total of N0k iterations are performed for all  models
For the “VGGNN-synth” model, N0k are at a learning rate of 0.00N, and the other N0k at 0.000N
For other models, Nk iterations with VGG SynthText-part character  supervision alone are first run for warming up
The learning rate is 0.00N at this stage
Then NNk and N0k iterations are conducted using both character and word supervision at  learning rates of 0.00N and 0.000N, respectively
The weight decay is set as 0.000N and the momentum as 0.N
 For experiments on ICDARNN, ICDARNN and COCOtext, the text line generation and word partition approaches  introduced in Section N.N are applied to produce word localizations, which are required for evaluation of these benchmark datasets
For fair comparison, we tune hyperparameters of the line generation algorithm on a small fraction of  training images, i.e
N0, for all character models
 N.N
Experiments on Synthetic Data  The VGG SynthText-part datasets are used
 NNNN    Evaluation of the Basic Character Detector We first  compare the proposed basic detection network presented in  Section N.N with the state-of-the-art algorithms in the field  of general object detection, e.g
faster RCNN [NN] and SSD  [NN]
For faster RCNN and SSD, we directly use the codes  provided by the authors
 Fig
N illustrates the precision-recalls of our basic network, faster RCNN and SSD on character detection, respectively
The main difference between our character network with the state-of-the-art general object detectors is  that the feature maps used to produce character responses  is finer than that of general object detectors (N/N vs
N/NN), while maintaining sufficient representation power by merging cues from deeper stages
The large gap between our  basic network and general object detector demonstrates that  reserving resolution is crucial for character detection
The  two-level hard negative mining during training is also a plus  that the second level hard patch mining can bring a moderate gain, as shown in Fig
N
 Evaluation of Word Supervision Approach Three models are trained
The first is trained by randomly selected  N,000 images using character supervision
The second is  trained using character supervision of all the N0k images
 The third is trained using N,000 character supervision images and NN,000 word supervision images
The training approaches are similar to those in N.N
 From Fig
N, it can be seen that the word-supervised  model performs superior to Nk characters trained model and  the performance degradation against the full N0k characters  trained model is insignificant, demonstrating the effectiveness of our word supervision approach in exploiting weak  word annotations for character model training
 N.N
Experiments on Scene Text Benchmarks  We apply our text detection approach on three real  challenging scene text benchmarks: ICDARNN [N0], ICDARNN [NN] and COCO-Text [NN]
These benchmarks are  all based on word-level evaluation
Hence, the text line  generation and word partition methods are involved
In the  line model estimation step, we only use 0-order and N-order  models as nearly all text lines have up to orientation deformation
 Table N, N and N show the performances of different methods on the ICDARNN, ICDARNN and COCO-Text  datasets
Our approach outperforms previous state-of-theart methods by a large margin
 On ICDARNN, we achieve N0.NN% F-measure, which is  N.NN% higher than the second best one, i.e
CTPN [NN]
 On the more challenging ICDARNN datasets, images are  more likely to suffer from blurry, perspective distortion, extreme illumination, and etc
Our best model achieves a fmeasure of NN.NN%, with a large margin over the previous  Method Recall Precision F-measure  MCLAB-FCN [NN] NN.NN NN.N0 NN.N0  Yao et al
[NN] N0.NN NN.NN NN.NN  Gupta et al.[N] NN.N NN.0 NN.0  Zhu et al
[NN] NN.NN NN.N0 NN.NN  CTPN [NN] NN.NN NN.NN NN.NN  our (VGGNN-synth) NN.NN NN.NN NN.NN  our (VGGNN-synth-icdar) NN.NN NN.NN N0.NN  Table N: Performances of different methods on ICDARNN using the DetEval criterion (%)
 Method Recall Precision F-measure  MCLAB-FCN [NN] NN.0N N0.NN NN.NN  CTPN [NN] NN.NN NN.NN N0.NN  Yao et al
[NN] NN.NN NN.N0 NN.NN  SCUT-DMPNet [NN] NN.NN NN.NN N0.NN  RRPN-N [NN] NN.NN NN.NN N0.NN  our (VGGNN-synth) NN.NN NN.NN NN.NN  our (VGGNN-synth-icdar) NN.0N NN.NN NN.NN  Table N: Performances of different methods on ICDARNN (%)
 best method [NN] (NN.NN% vs
N0.NN%)
Comparing our approach using different character models, VGG-synth-icdar  performs much better than the VGG-synth model (NN.NN%  vs
NN.NN%)
VGG-synth-icdar only adds Nk training image  compared to the VGG-synth model (N0k training images)
 This indicates that the gain is from more real data, other  than more data
 On COCO, our best model achieves N0.N%, NN.N% and  NN.N% in recall, precision and F-measure, respectively
It  takes over Yao’s method by N.N% in total F-measure
VGGsynth-coco also performs much better than the VGG-synth  model, demonstrating the introduction of real text images  helps a lot in training better character models
 Fig
N illustrates some detection samples from the ICDARNN, ICDARNN and COCO-Text test images
By exploiting rich word annotations from real text image datasets,  our model becomes more robust and can thus successfully  detect various challenging texts, e.g
blurry, perspective distortion, handwritten/art fonts, extreme illumination and etc.,  which are hard to be synthesized using computers
 Computational Time For a N00×N00 image, the charac- ter network takes about N00ms using an Nvidia Tesla KN0  GPU
The text line generation and word partition procedures together take about N0ms using a NGHz CPU
 N.N
Applied to Various Scenarios  We apply our pipeline to various challenging scenarios,  including advertising images, deformed document texts and  math expressions
A character model is trained by a privately collected text image datasets about these scenarios,  NNNN    Figure N: Sample qualitative results using the VGGNN-synth model (top) and the models trained by word supervision (bottom) on the benchmark scene text datasets
Yellow and red rectangles illustrate the correctly and wrongly detected text lines, respectively
 Figure N: Applied to various scenarios
The top row shows detected characters, with colors indicating character scores (indexed by colormap in Matlab)
The bottom row shows results of structure analysis
 Method Recall Precision F-measure  A [NN] NN.N NN.NN NN.NN  B [NN] N0.N NN.NN NN.NN  C [NN] N.N NN.NN N.NN  Yao et al.[NN] NN.N NN.N NN.N  our (VGGNN-synth) NN.N NN.N NN.N  our (VGGNN-synth-coco) N0.N NN.N NN.N  Table N: Performance of different methods on the COCO-Text (%)
Notice that the annotations are obtained under the participation of method A, B and C
It is thus not fair to be compared with  these methods
Yet, they are listed here for reference
 consisting of Nk character-level annotated images and N0k line-level annotated images (only images with straight text  lines are involved)
The training approach is similar as in  Section N.N
Text lines are generated by the approach in  Section N.N
Fig
N illustrates the character detection (top  row) and text line generation (bottom row) results on some  representative images
Our approach can handle text lines  with various languages and extreme deformations
It is also  worth noting that Chinese has a vast number of character  classes, where some of them may not be seen by the Nk character-level annotated images
However, we empirically  found that the initial model can still help recovering center  masks of many unseen characters given only text line annotations
One possible reason is that the unseen chracters  may share similar substructures or strokes with the characters seen by the initial model
 We also show application for math expression recognition (see the last column of Fig
N)
Math expressions are  non-sequential, and hence sequential text recognition technique is not applicable
Given detected characters, we can  recognize each of them, producing a set of math symbols
 N
Conclusion  Character based text detection methods are flexible to be  applied in various scenarios
We present a weakly supervised approach to enable the use of real word-level annotated text images for training
We show the representation  power of character models can be significantly strengthened
Extensive experiments demonstrate the effectiveness  of our weakly supervised approach and the flexibility of our  text detection pipeline
 NNNN    References  [N] A
V
Aho, J
E
Hopcroft, and J
D
Ullman
Data Structures  and Algorithms
Addison-Wesley, NNNN
 [N] F
L
Bookstein
Principal warps: Thin-plate splines and the  decomposition of deformations
IEEE TPAMI, NN(N):NNN–  NNN, NNNN
 [N] H
Chen, S
S
Tsai, G
Schroth, D
M
Chen, R
Grzeszczuk,  and B
Girod
Robust text detection in natural images with  edge-enhanced maximally stable extremal regions
In ICIP,  pages NN0N–NNNN, N0NN
 [N] J
Dai, K
He, and J
Sun
Boxsup: Exploiting bounding  boxes to supervise convolutional networks for semantic segmentation
In Proceedings of the IEEE International Conference on Computer Vision, pages NNNN–NNNN, N0NN
 [N] B
Epshtein, E
Ofek, and Y
Wexler
Detecting text in natural  scenes with stroke width transform
In CVPR, pages NNNN–  NNN0, N0N0
 [N] F
A
Gers, J
Schmidhuber, and F
Cummins
Learning to  forget: Continual prediction with lstm
Neural computation,  NN(N0):NNNN–NNNN, N000
 [N] A
Gupta, A
Vedaldi, and A
Zisserman
Synthetic data for  text localisation in natural images
In CVPR, N0NN
 [N] T
He, W
Huang, Y
Qiao, and J
Yao
Accurate text localization in natural image with cascaded convolutional text  network
CoRR, abs/NN0N.0NNNN, N0NN
 [N] T
He, W
Huang, Y
Qiao, and J
Yao
Text-attentional convolutional neural network for scene text detection
IEEE TIP,  NN(N):NNNN–NNNN, N0NN
 [N0] W
He, Y
Luo, F
Yin, H
Hu, J
Han, E
Ding, and C.-L
 Liu
Context-aware mathematical expression recognition:  An end-to-end framework and a benchmark
In ICPR, N0NN
 [NN] W
He, Y
Luo, F
Yin, H
Hu, J
Han, E
Ding, and C
L
 Liu
Context-aware mathematical expression recognition:  An end-to-end framework and a benchmark
In Pattern  Recognition (ICPR), N0NN NNrd International Conference on,  N0NN
 [NN] L
Huang, Y
Yang, Y
Deng, and Y
Yu
DenseBox: Unifying landmark localization with end to end object detection
 CoRR, abs/NN0N.0NNNN, N0NN
 [NN] W
Huang, Z
Lin, J
Yang, and J
Wang
Text localization  in natural images using stroke feature transform and text covariance descriptors
In ICCV, pages NNNN–NNNN, N0NN
 [NN] W
Huang, Y
Qiao, and X
Tang
Robust scene text detection with convolution neural network induced mser trees
In  ECCV, pages NNN–NNN, N0NN
 [NN] M
Jaderberg, K
Simonyan, A
Vedaldi, and A
Zisserman
 Synthetic data and artificial neural networks for natural scene  text recognition
ArXiv e-prints, N0NN
 [NN] M
Jaderberg, K
Simonyan, A
Vedaldi, and A
Zisserman
 Reading text in the wild with convolutional neural networks
 IJCV, NNN(N):N–N0, N0NN
 [NN] M
Jaderberg, A
Vedaldi, and A
Zisserman
Deep features  for text spotting
In ECCV, pages NNN–NNN, N0NN
 [NN] L
Kang, Y
Li, and D
Doermann
Orientation robust text  line detection in natural images
In CVPR, pages N0NN–N0NN,  N0NN
 [NN] D
Karatzas, L
Gomez-Bigorda, A
Nicolaou, S
Ghosh,  A
Bagdanov, M
Iwamura, J
Matas, L
Neumann, V
R
 Chandrasekhar, S
Lu, et al
Icdar N0NN competition on robust reading
In ICDAR, pages NNNN–NNN0
IEEE, N0NN
 [N0] D
Karatzas, F
Shafait, S
Uchida, M
Iwamura, L
G
i Bigorda, S
R
Mestre, J
Mas, D
F
Mota, J
A
Almazan, and  L
P
de las Heras
Icdar N0NN robust reading competition
In  ICDAR, pages NNNN–NNNN
IEEE, N0NN
 [NN] T
Kong, A
Yao, Y
Chen, and F
Sun
Hypernet: towards accurate region proposal generation and joint object detection
 In CVPR, pages NNN–NNN, N0NN
 [NN] Y
Li, W
Jia, C
Shen, and A
van den Hengel
Characterness:  An indicator of text in the wild
IEEE TIP, NN(N):NNNN–NNNN,  N0NN
 [NN] T.-Y
Lin, P
Dollár, R
Girshick, K
He, B
Hariharan, and  S
Belongie
Feature pyramid networks for object detection
 In CVPR, N0NN
 [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
E
Reed,  C
Fu, and A
C
Berg
SSD: single shot multibox detector
 In ECCV, pages NN–NN, N0NN
 [NN] Y
Liu and L
Jin
Deep matching prior network: Toward tighter multi-oriented text detection
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, pages NNNN–  NNN0, N0NN
 [NN] J
Ma, W
Shao, H
Ye, L
Wang, H
Wang, Y
Zheng, and  X
Xue
Arbitrary-oriented scene text detection via rotation  proposals
In CoRR, abs/NN0N.0NNNN, N0NN
 [NN] S
Mao, A
Rosenfeld, and T
Kanungo
Document structure  analysis algorithms: a literature survey
In Electronic Imaging N00N, pages NNN–N0N
International Society for Optics  and Photonics, N00N
 [NN] G
Meng, Z
Huang, Y
Song, S
Xiang, and C
Pan
Extraction of virtual baselines from distorted document images using curvilinear projection
In ICCV, pages NNNN–NNNN, N0NN
 [N0] A
Mishra, K
Alahari, and C
Jawahar
Scene text recognition using higher order language priors
In BMVC
BMVA,  N0NN
 [NN] S
Ren, K
He, R
B
Girshick, and J
Sun
Faster R-CNN:  towards real-time object detection with region proposal networks
In NIPS, pages NN–NN, N0NN
 [NN] O
Ronneberger, P
Fischer, and T
Brox
U-net: Convolutional networks for biomedical image segmentation
In MICCAI, pages NNN–NNN
Springer, N0NN
 [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
S
Bernstein,  A
C
Berg, and F
Li
Imagenet large scale visual recognition  challenge
IJCV, NNN(N):NNN–NNN, N0NN
 [NN] H
F
Schantz
History of OCR, optical character recognition
Recognition Technologies Users Association, NNNN
 [NN] B
Shi, X
Bai, and S
Belongie
Detecting oriented text  in natural images by linking segments
arXiv preprint  arXiv:NN0N.0NNN0, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
CoRR,  abs/NN0N.NNNN, N0NN
 NNNN    [NN] S
Tian, Y
Pan, C
Huang, S
Lu, K
Yu, and C
Lim Tan
 Text flow: A unified text detection system in natural scene  images
In ICCV, pages NNNN–NNNN, N0NN
 [NN] Z
Tian, W
Huang, T
He, P
He, and Y
Qiao
Detecting text  in natural image with connectionist text proposal network
In  ECCV, pages NN–NN, N0NN
 [NN] A
Veit, T
Matera, L
Neumann, J
Matas, and S
Belongie
Coco-text: Dataset and benchmark for text detection and recognition in natural images
In arXiv preprint  arXiv:NN0N.0NNN0, N0NN
 [N0] K
Wang, B
Babenko, and S
Belongie
End-to-End scene  text recognition
In ICCV, pages NNNN–NNNN, N0NN
 [NN] K
Wang and S
Belongie
Word spotting in the wild
In  ECCV, pages NNN–N0N, N0N0
 [NN] T
Wang, D
J
Wu, A
Coates, and A
Y
Ng
End-to-End text  recognition with convolutional neural networks
In ICPR,  pages NN0N–NN0N, N0NN
 [NN] C
Yao, X
Bai, W
Liu, Y
Ma, and Z
Tu
Detecting texts  of arbitrary orientations in natural images
In CVPR, pages  N0NN–N0N0, N0NN
 [NN] C
Yao, X
Bai, N
Sang, X
Zhou, S
Zhou, and Z
Cao
 Scene text detection via holistic, multi-channel prediction
 CoRR, abs/NN0N.0N00N, N0NN
 [NN] F
Yin and C
Liu
Handwritten chinese text line segmentation by clustering with distance metric learning
Pattern  Recognition, NN(NN):NNNN–NNNN, N00N
 [NN] X.-C
Yin, W.-Y
Pei, J
Zhang, and H.-W
Hao
Multiorientation scene text detection with adaptive clustering
 IEEE TPAMI, NN(N):NNN0–NNNN, N0NN
 [NN] X.-C
Yin, X
Yin, K
Huang, and H.-W
Hao
Robust text  detection in natural scene images
IEEE TPAMI, NN(N):NN0–  NNN, N0NN
 [NN] Z
Zhang, W
Shen, C
Yao, and X
Bai
Symmetry-based text  line detection in natural scenes
In CVPR, pages NNNN–NNNN,  N0NN
 [NN] Z
Zhang, C
Zhang, W
Shen, C
Yao, W
Liu, and X
Bai
 Multi-oriented text detection with fully convolutional networks
In CVPR, N0NN
 [N0] X
Zhou, C
Yao, H
Wen, Y
Wang, S
Zhou, W
He, and  J
Liang
East: An efficient and accurate scene text detector
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] S
Zhu and R
Zanibbi
A text detection system for natural scenes with convolutional feature learning and cascaded  classification
In CVPR, pages NNN–NNN, N0NN
 NNNNExtreme Clicking for Efficient Object Annotation   Extreme clicking for efficient object annotation  Dim P
PapadopoulosN Jasper R
R
UijlingsN Frank KellerN Vittorio FerrariN,N  dim.papadopoulos@ed.ac.uk jrru@google.com keller@inf.ed.ac.uk vferrari@inf.ed.ac.uk  NUniversity of Edinburgh NGoogle Research  Abstract  Manually annotating object bounding boxes is central  to building computer vision datasets, and it is very time  consuming (annotating ILSVRC [NN] took NNs for one highquality box [NN])
It involves clicking on imaginary corners  of a tight box around the object
This is difficult as these  corners are often outside the actual object and several adjustments are required to obtain a tight box
We propose  extreme clicking instead: we ask the annotator to click on  four physical points on the object: the top, bottom, left- and  right-most points
This task is more natural and these points  are easy to find
We crowd-source extreme point annotations  for PASCAL VOC N00N and N0NN and show that (N) annotation time is only Ns per box, N× faster than the traditional way of drawing boxes [NN]; (N) the quality of the boxes is  as good as the original ground-truth drawn the traditional  way; (N) detectors trained on our annotations are as accurate as those trained on the original ground-truth
Moreover, our extreme clicking strategy not only yields box coordinates, but also four accurate boundary points
We show  (N) how to incorporate them into GrabCut to obtain more  accurate segmentations than those delivered when initializing it from bounding boxes; (N) semantic segmentations  models trained on these segmentations outperform those  trained on segmentations derived from bounding boxes
 N
Introduction  Drawing the bounding boxes traditionally used for object  detection is very expensive
The PASCAL VOC bounding  boxes were obtained by organizing an “annotation party”  where expert annotators were gathered in one place to create high quality annotations [NN]
But crowdsourcing is essential for creating larger datasets: Su et al
[NN] developed  an efficient protocol to annotate high-quality boxes using  Amazon Mechanical Turk (AMT)
They report NN% efficiency gains over consensus-based approaches (which collect multiple annotations to ensure quality) [NN, N0]
However, even this efficient protocol requires NNs to annotate  one box (more details in Sec
N)
 (b) (a)   Submit   Figure N
Annotating an instance of motorbike: (a) The conventional way of drawing a bounding box
(b) Our proposed extreme  clicking scheme
 Why does it take so long to draw a bounding box? Fig Na  shows the typical process [NN, NN, NN, NN, NN, NN]
First  the annotator clicks on a corner of an imaginary rectangle  tightly enclosing the object (say the bottom-right corner)
 This is challenging, as these corners are typically not on  the object
Hence the annotator needs to find the relevant  extreme points of the object (the bottom point and the rightmost point) and adjust the x- and y-coordinates of the corner  to match them
After this, the annotator clicks and drags the  mouse to the diagonally opposite corner
This involves the  same process of x- and y-adjustment, but now based on a  visible rectangle
After the rectangle is adjusted, the annotator clicks again
He/she can make further adjustments by  clicking on the sides of the rectangle and dragging them until the box is tight on the object
Finally, the annotator clicks  a “submit” button
 From a cognitive perspective, the above process is suboptimal
The three steps (clicking on the first corner, dragging to the second corner, adjusting the sides) effectively  constitute three distinct tasks
Each task requires attention  to different parts of the object and using the mouse differently
In effect, the annotator is constantly task-switching, a  process that is cognitively demanding and is correlated with  increased response times and errors rates [NN, NN]
FurtherNNNN0    more, the process involves a substantial amount of mental  imagery: the rectangle to be drawn is imaginary, and so are  the corner points
Mental imagery also has a cognitive cost,  e.g
in mental rotation experiments, response time is proportional to rotation angle [NN, NN]
 In this paper we propose an annotation scheme which  avoids task switching and mental imagery, resulting in  greatly improved efficiency
We call our scheme extreme  clicking: we ask the annotator to click on four extreme  points of the object, i.e
points belonging to the top, bottom, left-most, and right-most parts of the object (Fig Nb)
 This has several advantages: (N) Extreme points are not  imaginary, but are well-defined physical points on the object, which makes them easy to locate
(N) No rectangle is  involved, neither real nor imaginary
This further reduces  mental imagery, and avoids the need for detailed instructions defining the notion of a bounding box
(N) Only a  single task is performed by the annotator thus avoiding task  switching
(N) No separate box adjustment step is required
 (N) No “submit” button is necessary; annotation terminates  after four clicks
 Additionally, extreme clicking provides more information than just box coordinates: we get four points on the  actual object boundary
We demonstrate how to incorporate  them into GrabCut [NN], to deliver more accurate segmentations than when initializing it from bounding boxes [NN]
In  particular, GrabCut relies heavily on the initialization of the  object appearance model (e.g
[NN, NN, NN]) and on which  pixels are clamped to be object/backgound
When using just  a bounding box, the object appearance model is initialized  from all pixels within the box (e.g
[NN, NN, NN])
Moreover,  it typically helps to clamp a smaller central region to be object [NN]
Instead, we first expand our four object boundary  points to an estimate of the whole contour of the object
We  use this estimate to initialize the GrabCut object appearance  model
Furthermore, we skeletonize the estimate and clamp  the resulting pixels to be object
 We perform extensive experiments on PASCAL VOC  N00N and N0NN using crowd-sourced annotations which  demonstrate: (N) extreme clicking only takes Ns seconds  per box, N× faster than the traditional way of drawing boxes [NN]; (N) extreme clicking leads to high-quality boxes  on a par with the original ground-truth boxes drawn the traditional way; (N) detectors trained on boxes generated using extreme clicking perform as well as those trained on  the original ground-truth; (N) incorporating extreme points  into GrabCut [NN] improve object segmentations by N%-N%  mIoU over initializing it from bounding boxes; (N) semantic segmentations models trained on segmentations derived  from extreme clicking outperform those trained on segmentations generated from bounding boxes by N.N% mIoU
 N
Related work  Time to draw a bounding box
The time required to  draw a bounding box varies depending on several factors,  including the quality of the boxes and the crowdsourcing  protocol used
In this paper, as an authoritative reference  we use the protocol of [NN] which was used to annotate  ILSVRC [NN]
It was designed to produce high-quality  bounding boxes with little human annotation time on Amazon Mechanical Turk
They report the following median  times for annotating an object of a given class in an image [NN]: NN.Ns for drawing one box, N.0s for verifying its  quality, and N.Ns for checking whether there are other objects of the same class yet to be annotated
Since we only  consider annotating one object per class per image, we use  NN.Ns+N.0s = NN.Ns as the reference time
This is a conser- vative estimate: when taking into account that some boxes  are rejected and need to be re-drawn, the median time increases to NNs
If we use average times instead of medians,  the cost raises further to NNNs
 Note how both PASCAL VOC and ILSVRC have images of comparable difficulty and come with ground-truth  box annotations of similar high quality [NN], justifying our  choice of NNs reference time
Papers reporting faster timings [NN, NN] aim for lower-quality boxes (e.g
the official  annotator instructions of [NN] show an example box which  is not tight around the object)
We compare to [NN] in Sec
N
 Reducing annotation time for training object detectors
 Weakly-supervised object localization techniques (WSOL)  can be used to train object detectors from image-level labels only (without bounding boxes) [N, NN, NN, NN, NN]
This  setting is very cheap in terms of annotation time, but it produces lower quality object detectors, typically performing  at only about half the level of accuracy achieved by training  from bounding boxes [N, NN, NN, NN, NN]
 Training object class detectors from videos could bypass  the need for manual bounding boxes, as the motion of the  objects facilitates their automatic localization [NN, N0, NN]
 However, because of the domain adaptation problem, these  detectors are still quite weak compared to ones trained on  manually annotated still images [NN]
Alternative types of  supervision information such as eye-tracking data [NN, NN],  text from news articles or web pages [NN, NN], or even movie  scripts [N] have also been explored
Papadopoulos et al
[NN]  propose a scheme for training object class detectors which  only requires annotators to verify bounding boxes generated  automatically by the learning algorithm
We compare our  extreme clicking scheme to state-of-the-art WSOL [N], and  to [NN] in Sec
N
 (Interactive) object segmentation
Object segmentations are significantly more expensive to obtain than bounding boxes
The creators of the SBD dataset [NN] merged  five annotations per instance, resulting in a total time of  NNNs per instance
For COCO [NN], NNs per instance were  NNNN    Submit   Y Qualification   Main task   Y   Return   N N   Y User   qualified ?   N   Feedback   Pass?   Pass?   Annotator training Annotating images Instructions   Figure N
The workflow of our crowd-sourcing protocol for collecting extreme click annotations on images
The annotators read a  set of instructions and then go through an interactive training stage that consists of a qualification test at the end of which they receive a  detailed feedback on how well they performed
Annotators who successfully pass the test can proceed to the annotation stage
In case of  failure, they are allowed to repeat the test as many times as they want until they succeed
 required for drawing object polygons, excluding verifying correctness and possibly redrawing
To reduce annotation time many interactive segmentation techniques have  been proposed, which require the user to input either a  bounding box around the object [NN, NN, NN], or scribbles  [N, NN, NN, NN, NN, NN, N0, NN, NN, NN], or clicks [NN, NN]
 Most of this work is based on the seminal GrabCut algorithm [NN], which iteratively alternates between estimating appearance models (typically Gaussian Mixture Models [N]) and refining the segmentation using graph cuts [N]
 The user input is typically used to initialize the appearance  model and to clamp some pixels to background
In this paper, we incorporate extreme clicks into GrabCut [NN], improving the appearance model initialization and automatically selecting good seed pixels to clamp to object
 N
Collecting extreme clicks  In this section, we describe our crowd-sourcing framework for collecting extreme click annotations (Fig
N)
Annotators read a simple set of instructions (sec
N.N) and then  go through an interactive training stage (sec
N.N)
Those  who successfully pass the training stage can proceed to the  annotation stage (sec
N.N)
 N.N
Instructions  The annotators are given an image and the name of a  target object class
They are instructed to click on four extreme points (top, bottom, left-most, right-most) on the visible part of any object of this class
They can click the points  in any order
In order to let annotators know approximately  how long the task will take, we suggest a total time of N0s  for all four clicks
This is an upper bound on the expected  annotation time that we estimated from a small pilot study
 Note that our instructions are extremely simple, much  simpler than those necessary to explain how to draw a  bounding box in the traditional way (e.g
[NN, NN])
They are  also simpler than instructions required for verifying whether  a displayed bounding box is correct [NN, NN, NN]
That requires the annotator to imagine a perfect box on the object,  and to mentally compare it to the displayed one
 N.N
Annotator training  After reading the instructions, the annotators go through  the training stage
They have to complete a qualification  test, at the end of which they receive detailed feedback on  how well they performed
Annotators who successfully  pass this test can proceed to the annotation stage
In case  of failure, annotators can repeat the test until they succeed
 Qualification test
A qualification test is a good mechanism for enhancing the quality of crowd-sourcing data and  for filtering out bad annotators and spammers [N, NN, NN,  NN]
Some annotators do not pay attention to the instructions or do not even read them
Qualification tests have been  successfully used to collect image labels, object bounding  boxes, and segmentations for some of the most popular  datasets (e.g., COCO [NN] and Imagenet [NN, NN])
 The qualification test is designed to mimic our main task  of clicking on the extreme points of objects
We show the  annotator a sequence of N different images with the same object class and ask them to carry out the extreme clicking  task
 Feedback
The qualification test uses a small pool of images with ground-truth segmentation masks for the objects,  which we employ to automatically validate the annotator’s  clicks and to provide feedback (Fig
N, middle part)
We  take a small set of qualification images from a different  dataset than the one that we annotate
 In the following, we explain the validation procedure for  the top click (the other three cases are analogous)
We ask  the annotator to click on a top point on the object, but this  point is not necessarily uniquely defined
Depending on the  object shape, there may be multiple points that are equivalent, up to some tolerance margin (e.g
the top of the dog’s  head in fig
N, top row)
Clearly, clicking on any of these  points is correct
The area in which we accept the annotator’s click is derived from the segmentation mask
First,  we find the pixels with the highest y-coordinate in it (there  might be multiple such pixels)
Then, we select all pixels in  NNNN    qualification image   extreme areas of   segmentation mask   accepted areas of   qualification image   Figure N
Qualification test
(Left) Qualification test examples of  the dog and cat class
(Middle) The figure-ground segmentation  masks we use to evaluate annotators’ extreme clicks during the  training stage
The pixels of the four extreme areas of the mask are  marked with colors
(Right) The accepted areas for each extreme  click and the click positions as we display them to the annotators  as feedback
 the mask with y-coordinates within N0 pixels of any of these top pixels (red area in Fig
N, middle column)
Finally, we  also include in the accepted area all image pixels within N0 pixels of any of the selected pixels in the segmentation mask  (Fig
N, right column)
Thus the accepted area includes all  top pixels in the mask, plus a tolerance region around them,  both inside and outside the mask
 After the annotators finish the qualification test, they receive a feedback page with all the examples they annotated
 For each image, we display the annotator’s four clicks, and  the accepted areas for each click (Fig
N right column)
 Success or failure
The annotators pass the qualification  test if all their clicks on all N qualification images are inside  the accepted areas
Those that pass the test are recorded as  qualified annotators and can proceed to the main annotation  stage
A qualified annotator never has to retake the qualification test
In case of failure, annotators are allowed to  repeat the test as many times as they want
The combination of automatically providing rich feedback and allowing  annotators to repeat the test makes the training stage interactive and highly effective
Annotators that have reached the  desired level of quality can be expected to keep it throughout the annotation [N0]
 N.N
Annotating images  In the annotation stage, annotators are asked to annotate  small batches of N0 consecutive images
To increase anno- tation efficiency, the target class for all the images within  a batch is the same
This means annotators do not have to  re-read the class name for every image and can use their  prior knowledge of the class to find it rapidly in the image [NN]
More generally, it avoids task-switching which is  well-known to increase response time and decrease accuracy [NN, NN]
 Quality control
Quality control is a common process  when crowd-sourcing image annotations [N, NN, NN, NN, NN,  N0, NN, NN, N0]
We control the quality of the annotation by  hiding one evaluation image for which we have a groundtruth segmentation inside a N0-image batch, and monitor the  annotator’s accuracy on it (golden question)
Annotators  that fail to click inside the accepted areas on this evaluation  image are not able to submit the task
We do not do any  post-processing rejection of the submitting data
 N
Object segmentation from extreme clicks  Extreme clicking results not only in high-quality bounding box annotations, but also in four accurate object boundary points
In this section we explain how we use these  boundary points to improve the creation of segmentation  masks from bounding boxes
 We cast the problem of segmenting an object instance  in image I as a pixel labeling problem
Each pixel p ∈ I  should be labeled as either object (lp = N) or background (lp = 0)
A labeling L of all pixels represents the segmented object
Similar to [NN], we employ a binary pairwise energy  function E defined over the pixels and their labels
 E(L) = ∑  p  U(lp) + ∑  p,q  V (lp, lq) (N)  U is a unary potential that evaluates how likely a pixel p is  to take label lp according to the object and background appearance models, while the pairwise potential V encourages  smoothness by penalizing neighboring pixels taking different labels
 Initial object surface estimate from extreme clicks
For  GrabCut to work well, it is important to have a good initial  estimate of the object surface to initialize the appearance  model
Additionally, it helps to clamp certain pixels to object [NN]
We show how the four collected object boundary  points can be exploited to do both
 In particular, for each pair of consecutive extreme clicks  (e.g
leftmost-to-top, or top-to-rightmost) we find the path  connecting them which is most likely to belong to the object boundary
For this purpose we first apply a strong edge  detector [NN] to obtain a boundary probability ep ∈ [0, N] for every pixel p of the image (second row of Fig
N)
We  then define the best boundary path between two consecutive extreme clicks as the shortest path whose minimum  edge-response is the highest (third row of Fig
N, magenta)
 We found this objective function to work better than others,  such as minimizing ∑  p (N−ep) for pixels p on the path
The  resulting object boundary paths yield an initial estimate of  the object outlines
 We use the surface within the boundary estimates (shown  in green in the third row of Fig
N) to initialize the object appearance model used for U in Eq
(N)
Furthermore, from  NNNN    Figure N
Visualization of input cues and output of GrabCut
First row shows input with annotator’s extreme clicks
Second row shows  output of edge detector [NN]
Third row shows our inputs for GrabCut: the pixels used to create background appearance model (red), the  pixels used to create the object appearance model (bright green), the initial boundary estimate (magenta), and the skeleton pixels which we  clamp to have the object label (dark green)
Fourth row shows the output of GrabCut when using our new inputs, while the last row shows  the output when using only a bounding box
 this surface we obtain a skeleton using standard morphology (shown in dark green in third row of Fig
N)
This skeleton is very likely to be object, so we clamp its pixel-labels  to be object (ls = N for all pixels s on the skeleton)
 Appearance model
As in classic GrabCut [NN], the appearance model consists of two GMMs, one for the object  (used when lp = N) and one for the background (used when lp = 0)
Each GMM has five components, where each is a full-covariance Gaussian over the RGB color space
 Traditional interactive segmentation techniques [NN, NN,  NN] start from a manually drawn bounding box and estimate the initial appearance models from all pixels inside  the box (object model) and all pixels outside it (background  model)
However, this may be suboptimal: since we are  trying to segment the object within the box, intuitively only  the immediate background is relevant, not the whole image
 Indeed, we improved results by using a small ring around  the bounding box for initializing the background model (see  third row Fig
N in red)
Furthermore, not all pixels within  the box belong to the object
But given only a bounding box  as input, the best is to still use the whole box to initialize  the object model
Therefore, in our baseline GrabCut implementation, the background model is initialized from the  immediate background and the object model is initialized  from all pixels within the box
 However, because we have extreme clicks we can do better
We use them to obtain an initial object surface estimate  (described above) from which we initialize the object appearance model
Fig
N illustrates how this improves the  unary potentials U resulting from the appearance models
 Clamping pixels
GrabCut sometimes decides to label all  pixels either as object or background
To prevent this, one  can clamp some pixels to a certain label
For the background, all pixels outside the bounding box are typically  clamped to background
For the object, one possible approach is to clamp a small area in the center of the box [NN]
 However, there is no guarantee that the center of the box is  on the object, as many objects are not convex
Moreover,  the size of the area to be clamped is not easy to set
 In this paper, we estimate the pixels to be clamped by  skeletonizing the object surface estimate derived from our  extreme clicks (described above)
In Sec
N we show how  our proposed object appearance model initialization and  clamping scheme affect the final segmentation quality
 Pairwise potential V 
The summation over (p, q) in (N) is defined on an eight-connected pixel grid
Usually, this  penalty depends on the RGB difference between pixels, being smaller in regions of high contrast [N, N, NN, NN, NN, NN]
 In this paper, we instead use the sum of the edge responses  of the two pixels given by the edge detector [NN]
In Sec
N  NNNN    in it  ia l  A  p     ex tr  em e   cl ic  k s   in it  ia l  A  p     b o  u n  d in  g  b  o x     Figure N
Posterior probability of pixels belonging to object
For  both rows the background appearance model is created by using an  area outside the initial box (see Fig
N)
In the first row the object  model is created using the area inside the initial box
In the second  row the object model is created from the object surface estimated  using extreme clicks (Fig
N, third row in light-green)
Predictions  from the appearance model using extreme clicks are visibly better
 we evaluate both pairwise potentials and show how they affect the final segmentation
 Optimization
After the initial estimation of appearance  models, we follow [NN] and alternate between finding the  optimal segmentation L given the appearance models, and  updating the appearance models given the segmentation
 The first step is solved globally optimally by minimizing (N)  using graph-cuts [N], as our pairwise potentials are submodular
The second step simply fits GMMs to labeled pixels
 N
Extreme Clicking Results  We implement our annotation scheme on Amazon Mechanical Turk (AMT) and collect extreme click annotations  for both the trainval set of PASCAL VOC N00N [N0] (N0NN  images) and the training set of PASCAL VOC N0NN [NN]  (NNNN images), which contain N0 object categories
For every image we annotate a single instance per class (if present  in the image), which enables direct comparison to other  methods described below
We compare methods both in  terms of efficiency and quality
 Compared methods
Our main comparisons are to the  existing ground-truth bounding boxes of PASCAL VOC
As  discussed in Sec
N, we use NN.Ns as the reference time necessary to produce one such high quality bounding box by  drawing it the traditional way [NN]
 At the other extreme, it is possible to obtain lower quality  bounding boxes automatically at zero extra costs by using  weakly supervised methods, which only input image-level  labels
We compare to the recent method of [N]
 We also compare to two methods which strike a trade-off  between accuracy and efficiency [NN, NN]
In [NN], manual  box drawing is part of a complex computer-assisted annotation system
Papadopoulos et al
[NN] propose an annotation  scheme that only requires annotators to verify boxes automatically generated by a learning algorithm
Importantly,  both [NN, NN] report both annotation time and quality, enabling proper comparisons
 Evaluation measures
For evaluating efficiency we report time measurements, both in terms of annotating the  whole dataset and per instance
 We evaluate the quality of bounding boxes with respect  to the PASCAL VOC ground-truth
We do this with respect  to the ground-truth bounding boxes (GT Boxes), but also  with respect to bounding boxes which we fit to the groundtruth segmentations (GT SegBoxes)
We quantify quality  by intersection-over-union (IoU) [NN], where we measure  the percentage of bounding boxes we annotated per object  class with IoU greater than 0.N and 0.N, and then take the  mean over all classes (IoU>0.N, IoU>0.N)
In addition, we  calculate the average IoU for all instances of a class and  take the mean over all classes (mIoU)
 As an additional measure of accuracy we measure detector performance using Fast-RCNN [NN], trained either on  our extreme click boxes or on the PASCAL GT Boxes
 N.N
Results on quality and efficiency  PASCAL ground-truth boxes vs
extreme clicks
Table N reports the results
Having two sets of groundtruth boxes enables us to measure the agreement among  the expert annotators that created PASCAL
Comparing GT  Boxes and GT SegBoxes reveals this agreement to be at  NN% mIoU on VOC N00N
Moreover, NN% of all GT Boxes  have IoU > 0.N with their corresponding GT SegBox
This shows that the ground-truth annotations are highly consistent, and these metrics represent the quality of the groundtruth itself
Similar findings apply to VOC N0NN
 Interestingly, the boxes derived from our extreme clicks  achieve equally high metrics, when compared to the PASCAL ground-truth annotations
Therefore our extreme click  annotations yield boxes with a quality within the agreement  among expert-annotators using the traditional way of drawing
To get a better feeling for such quality, if we perturb  each of the four coordinates of the GT Boxes by N pixels,  the resulting boxes also have NN% mIoU with the unperturbed annotations
 To further demonstrate the quality of extreme clicking,  we train Fast-RCNN [NN] using either PASCAL GT Boxes  or extreme click boxes
We train on PASCAL VOC N00Ns  trainval set and test on its test set, then we train on VOC  N0NNs train and test on its val set
We experiment using  AlexNet [NN] and VGGNN [NN]
Performance when training from GT Boxes or from our boxes is identical on both  datasets and using both base networks
 Annotation efficieny
In terms of annotation efficiency,  extreme clicks are N× cheaper: N.0s instead of NN.Ns
This demonstrates that extreme clicking costs only a fraction of  the annotation time of the widely used box-drawing protocol [NN, NN, NN, NN, NN], without any compromise on quality
 Human verification [NN] vs
extreme clicks
Table N  compares extreme clicks to human verification [NN] on VOC  NNNN    Annotation quality w.r.t
GT SegBoxes Detector performance (mAP) Annotation time  Dataset Annotation approach mIoU IoU>0.N IoU>0.N AlexNet VGGNN dataset (h) instance (s)  PASCAL Extreme clicks NN NN NN NN NN NN.N N.0  VOC N00N PASCAL GT Boxes NN NN NN NN NN N0.0 NN.N  PASCAL Extreme clicks NN NN NN NN NN NN.N N.N  VOC N0NN PASCAL GT Boxes NN N0 NN NN NN NN.N NN.N  Table N
Comparison of extreme clicking and PASCAL VOC ground-truth
 Annotation quality w.r.t
GT Boxes Detector performance (mAP) Annotation time  Dataset Annotation approach mIoU IoU>0.N IoU>0.N AlexNet VGGNN dataset (h) instance (s)  Extreme clicks NN NN NN NN NN NN.N N.0  PASCAL VOC Human verification [NN] – – NN N0 NN N.N N.N  N00N WSOL: Bilen and Vedaldi [N] – – NN NN NN 0 0  ILSVRC (subset) box drawing in [NN] – NN – – – – NN.N  Table N
Comparison of extreme clicking and alternative fast annotation approaches
 N00N
While verification is N.N× faster, our bounding boxes are much more accurate (NN% correct at IoU>0.N, compared to NN% for [NN])
Additionally, detector performance  at test time is N%-N% mAP higher for extreme clicking
 Weak supervision vs
extreme clicks
Weakly supervised methods are extremely cheap in human supervision  time
However, the recent work [N] reports NN% mAP using VGGNN, which is only about half the result brought by  extreme clicking (NN% mAP, Table N)
 Box drawing [NN] vs
extreme clicks
Finally, we compare to [NN] in Table N
This is an approximate comparison  as measurements of their box-drawing component are done  on an unspecified subset of ILSVRC N0NN
However, as  ILSVRC and PASCAL VOC are comparable in both quality  of annotations and difficulty of the dataset [NN], this comparison is representative
In [NN] they report NN.Ns for drawing a bounding box, where NN% of the drawn boxes have  an IoU>0.N with the ground-truth box
This suggests that  bounding boxes can be drawn faster than reported in [NN]  but this comes with a significant drop in quality
In contrast, extreme clicking costs Ns per box and NN%-NN% of  those boxes have IoU>0.N
Hence our protocol to annotate  bounding boxes is both faster and more accurate
 N.N
Additional analysis Per-click response-time
We examine the mean response  time per click during extreme clicking
Interestingly, the  first click on an object takes about N.Ns, while subsequent  clicks take about N.Ns
This is because the annotator needs  to find the object in the image before they can make the  first click
Interestingly, Ns visual search is consistent with  earlier findings [NN, NN]
 Influence of qualification test and quality control
We  conducted three crowd-sourcing experiments on N00 trainval images of PASCAL VOC N00N to test the influence of  using a qualification test and quality control
We report the  quality of the bounding boxes derived from extreme clicks  in Tab
N
Using a qualification test vastly improves annotation quality (from NN.N% to NN.N% mIoU)
The quality control brings a smaller further improvement to NN.N% mIoU
 Actual Cost
We paid the annotators $0.NN to annotate a batch of N0 images which, based on our timings, is about  Qualification test Quality control mIoU IoU>0.N  NN.N NN.0  X NN.N NN.0  X X NN.N NN.N  Table N
Influence of the qualification test and quality control on  the accuracy of extreme click annotations (on N00 images from  PASCAL VOC N00N)
 $N.N per hour
The total cost for annotating the whole train- val set of PASCAL VOC N00N and the training set of PASCAL VOC N0NN was $NNN and $NNN, respectively
 N
Results on Object Segmentation  This section demonstrates that one can improve segmentation from a bounding box by using also the boundary  points which we obtain from extreme clicking
 N.N
Results on PASCAL VOC  Datasets and Evaluation
We perform experiments on  VOC N00N and VOC N0NN
The trainval set of the segmentation task of VOC N00N consists of NNN images with groundtruth segmentation masks of N0 classes
For VOC N0NN, we  evaluate on the training set, using as reference ground-truth  the augmented masks set by [NN] (NNNN images)
 To evaluate the output object segmentations, for every  class we compute the intersection over union (IoU) between  the predicted and ground-truth segmentation mask, and report the mean IoU over all object classes (mIoU)
Some pixels in VOC N00N are labeled as ‘unknown’ and are excluded  from evalutation
For these experiments we use structured  edge forests [NN] to predict object boundaries, which is  trained on BSDN00 [N]
 GrabCut from PASCAL VOC GT Boxes
We start with  establishing our baseline by using GrabCut on the original  GT Boxes of VOC (for which no boundary points are available)
Since applying [NN] directly leads to rather poor performance on VOC N00N (NN.N% mIoU), we first optimize  GrabCut on this dataset using methods discussed in Sec
N
 Our optimized model has the following properties: the object appearance model is initialized from all pixels within  the box
The background appearance model is initialized  from a small ring around the box which has twice the area  NNNN    of the bounding box
A small rectangular core centered  within the box whose area is a quarter of the area of the  box is clamped to be object
All pixels outside the box are  clamped to be background
As pairwise potential, instead of  using standard RGB differences, we use the summed edge  responses of [NN] of the corresponding pixels
All modifications together substantially improve results to NN.N% mIoU  on VOC N00N
We then run GrabCut again on VOC N0NN  using the exact same settings optimized for VOC N00N, obtaining NN.0% mIoU
 GrabCut from extreme clicking
Thanks to our extreme  clicking annotations, we also have object boundary points
 Starting from the optimized GrabCut settings established  in the previous paragraph, we make use of these boundary  points to (N) initialize a better object appearance model, and  (N) choose better pixels to clamp to object
As described  in Sec
N, we use the extreme clicks to estimate an initial  contour of the object by following predicted object boundaries [NN]
We use the surface bounded by this contour estimate to initialize the appearance model
We also skeletonize this surface and clamp the resulting pixels to be object
The resulting model yields NN.N% mIoU on VOC N00N  and NN.N% on VOC N0NN
This is an improvement of N.N%  (VOC N00N) and N.N% (VOC N0NN) over the strong baseline  we built
Fig
N shows qualitative results comparing GrabCut segmentations starting from GT Boxes (last row) and  those based on our extreme clicking annotations (secondlast row)
 N.N
Results on the GrabCut dataset  We also conducted an experiment on the Grabcut  dataset [NN], consisting of only N0 images
The standard  evaluation measure is the error rate in terms of the percentage of mislabelled pixels
For this experiment, we simulate  the extreme click annotation by using the extreme points of  the ground-truth segmentation masks of the images
 When we perform GrabCut from bounding boxes, we  obtain an error rate of N%
When using additionally the  boundary points from simulated extreme clicking, we obtain N.N% error, an improvement of N.N%
This again  demonstrates that boundary points contain useful information over bounding boxes alone for this task
 For completeness, we note that the state-of-the-art  method on this dataset has N.N% error [NN]
This method  uses a framework of superpixels and Multiple Instance  Learning to turn a bounding box into a segmentation mask
 In this paper we build on a much simpler segmentation  framework (GrabCut)
We believe that incorporating our  extreme clicks into [NN] would bring further improvements
 N.N
Training a semantic segmentation model  We now explore training a modern deep learning system  for semantic segmentation from the segmentations derived  Full Segments from Segments from  supervision GT Boxes extreme clicks  mIoU NN.N NN.N NN.N  Table N
Segmentation performance on the val set of PASCAL  VOC N0NN dataset using different types of annotations
 from extreme clicking
We train DeepLab [N0, NN] based  on VGG-NN [NN] on the VOC N0NN train set (N,NNN images)  and then we test on its val set (N,NNN images)
We measure  performance using the standard mIoU measure (Tab
N)
We  compare our approach to full supervision by training on the  same images but using the ground-truth, manually drawn  object segmentations (one instance per class per image, for  fair comparison)
We also compare to training on segmentations generated from GT Boxes
 Full supervision yields NN.N% mIoU, which is our upper  bound
As a reference, training on manual segmentations  for all instances in the dataset yields NN.N% mIoU
This is  N.N% lower than in [NN] since they train from train+val using  the extra annotations by [NN] (N0.Nk images)
 Segments from GT Boxes result in NN.N% mIoU
 Segments from extreme clicks lead to NN.N% mIoU
This  means our extreme clicking segmentations lead to a +N.N%  mIoU improvement over those generated from bounding  boxes
Moreover, our result is only -N.N% mIoU below the  fully supervised case (given the same total number of training samples)
 N
Conclusions  We presented an alternative to the common way of drawing bounding boxes, which involves clicking on imaginary  corners of an imaginary box
Our alternative is extreme  clicking: we ask annotators to click on the top, bottom, leftand right-most points of an object, which are well-defined  physical points
We demonstrate that our method delivers bounding boxes that are as good as traditional drawing,  while taking just Ns per annotation
To achieve this same  level of quality, traditional drawing needs NN.Ns [NN]
Hence  our method cuts annotation costs by a factor N× without any compromise on quality
 In addition, extreme clicking leads to more than just  a box: we also obtain accurate object boundary points
 To demonstrate their usefulness we incorporate them into  GrabCut, and show that they leads to better object segmentations than when initializing it from the bounding box  alone
Finally, we have shown that semantic segmentation models trained on these segmentations perform close  to those trained with manually drawn segmentations (when  given the same total number of samples)
 Acknowledgement
This work was supported by the  ERC Starting Grant “VisCul”
 NNNN    References  [N] M
Andriluka, L
Pishchulin, P
Gehler, and B
Schiele
Nd  human pose estimation: New benchmark and state of the art  analysis
In CVPR, N0NN
N  [N] P
Arbeláez, M
Maire, C
Fowlkes, and J
Malik
Contour  detection and hierarchical image segmentation
IEEE Trans
 on PAMI, N0NN
N  [N] X
Bai and G
Sapiro
Geodesic matting: A framework for  fast interactive image and video segmentation and matting
 IJCV, N00N
N  [N] A
Bearman, O
Russakovsky, V
Ferrari, and L
Fei-Fei
 What’s the point: Semantic segmentation with point supervision
In ECCV, N0NN
N  [N] H
Bilen and A
Vedaldi
Weakly supervised deep detection  networks
In CVPR, N0NN
N, N, N  [N] A
Blake, C
Rother, M
Brown, P
Perez, and P
Torr
Interactive image segmentation using an adaptive GMMRF model
 In ECCV, N00N
N, N  [N] P
Bojanowski, F
Bach, I
Laptev, J
Ponce, C
Schmid, and  J
Sivic
Finding actors and actions in movies
In ICCV,  N0NN
N  [N] Y
Boykov and M
P
Jolly
Interactive graph cuts for optimal  boundary and region segmentation of objects in N-D images
 In ICCV, N00N
N  [N] Y
Boykov and V
Kolmogorov
An experimental comparison  of min-cut/max-flow algorithms for energy minimization in  vision
IEEE Trans
on PAMI, NN(N):NNNN–NNNN, N00N
N, N  [N0] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected CRFs
In ICLR, N0NN
 N  [NN] R
Cinbis, J
Verbeek, and C
Schmid
Weakly supervised  object localization with multi-fold multiple instance learning
IEEE Trans
on PAMI, N0NN
N  [NN] CrowdFlower: https://www.crowdflower.com/
 Crowdflower bounding box annotation tool
https://  www.youtube.com/watch?v=lUIUN_HWNIc, N0NN
 N, N  [NN] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
Feifei
ImageNet: A large-scale hierarchical image database
In  CVPR, N00N
N  [NN] T
Deselaers, B
Alexe, and V
Ferrari
Weakly supervised localization and learning with generic knowledge
IJCV, N0NN
 N  [NN] P
Dollar and C
Zitnick
Structured forests for fast edge detection
In ICCV, N0NN
N, N, N, N  [NN] O
Duchenne, J.-Y
Audibert, R
Keriven, J
Ponce, and  F
Ségonne
Segmentation by transduction
In CVPR, N00N
 N  [NN] P
Duygulu, K
Barnard, N
de Freitas, and D
Forsyth
Object  recognition as machine translation: Learning a lexicon for a  fixed image vocabulary
In ECCV, N00N
N  [NN] K
A
Ehinger, B
Hidalgo-Sotelo, A
Torralba, and A
Olivia
 Modelling search for people in N00 scenes: A combined  source model of eye guidance
Visual Cognition, N00N
N  [NN] I
Endres, A
Farhadi, D
Hoiem, and D
A
Forsyth
The benefits and challenges of collecting richer object annotations
 In DeepVision workshop at CVPR, N0N0
N  [N0] M
Everingham, L
Van Gool, C
Williams, J
Winn, and  A
Zisserman
The PASCAL Visual Object Classes Challenge N00N Results, N00N
N  [NN] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn, and  A
Zisserman
The PASCAL Visual Object Classes (VOC)  Challenge
IJCV, N0N0
N, N  [NN] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn,  and A
Zisserman
The PASCAL Visual Object Classes  Challenge N0NN (VOCN0NN) Results
http://www.pascalnetwork.org/challenges/VOC/vocN0NN/workshop/index.html,  N0NN
N  [NN] V
Ferrari, M
Marin, and A
Zisserman
Progressive search  space reduction for human pose estimation
In CVPR, N00N
 N, N  [NN] D
Freedman and T
Zhang
Interactive graph cut based segmentation with shape priors
In CVPR, N00N
N  [NN] R
Girshick
Fast R-CNN
In ICCV, N0NN
N  [NN] L
Grady
Random walks for image segmentation
IEEE  Trans
on PAMI, NN(NN):NNNN–NNNN, N00N
N  [NN] V
Gulshan, C
Rother, A
Criminisi, A
Blake, and A
Zisserman
Geodesic star convexity for interactive image segmentation
In CVPR, N0N0
N, N  [NN] A
Gupta and L
Davis
Beyond nouns: Exploiting prepositions and comparators for learning visual classifiers
In  ECCV, N00N
N  [NN] B
Hariharan, P
Arbeláez, L
Bourdev, S
Maji, and J
Malik
 Semantic contours from inverse detectors
In ICCV, N0NN
N,  N, N  [N0] K
Hata, R
Krishna, L
Fei-Fei, and M
Bernstain
A glimpse  far into the future: Understanding long-term crowd worker  accuracy
In CSCW, N0NN
N  [NN] S
Jain and K
Grauman
Click carving: Segmenting objects in video with point clicks
In Proceedings of the Fourth  AAAI Conference on Human Computation and Crowdsourcing, N0NN
N  [NN] S
D
Jain and K
Grauman
Predicting sufficient annotation  strength for interactive foreground segmentation
In ICCV,  N0NN
N, N  [NN] S
Johnson and M
Everingham
Learning effective human  pose estimation from inaccurate annotation
In CVPR, N0NN
 N  [NN] V
Kalogeiton, V
Ferrari, and C
Schmid
Analysing domain  shift factors between videos and images for object detection
 IEEE Trans
on PAMI, N0NN
N  [NN] S
M
Kosslyn, W
L
Thompson, I
J
Kim, and N
M
Alpert
 Topographic representations of mental images in primary visual cortex
Nature, NNN(NNNN):NNN–NNN, NNNN
N  [NN] A
Kovashka and K
Grauman
Discovering attribute shades  of meaning with the crowd
IJCV, N0NN
N  [NN] J
Krause, M
Stark, J
Deng, and L
Fei-Fei
Nd object representations for fine-grained categorization
In ICCV Workshop on ND Representation and Recognition, N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, N0NN
N  NNNN  https://www.crowdflower.com/ https://www.youtube.com/watch?v=lUIUN_HWNIc https://www.youtube.com/watch?v=lUIUN_HWNIc   [NN] D
Kuettel and V
Ferrari
Figure-ground segmentation by  transferring window masks
In CVPR, N0NN
N, N  [N0] K
Kumar Singh, F
Xiao, and Y
Jae Lee
Track and transfer:  Watching videos to simulate strong human supervision for  weakly-supervised object detection
In CVPR, N0NN
N  [NN] A
Kuznetsova, S
J
Hwang, B
Rosenhahn, and L
Sigal
 Expanding object detector’s horizon: Incremental learning  framework for object detection in videos
In CVPR, N0NN
N  [NN] V
Lempitsky, P
Kohli, C
Rother, and T
Sharp
Image segmentation with a bounding box prior
In ICCV, N00N
N,  N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
Zitnick
Microsoft COCO: Common objects in context
In ECCV, N0NN
N, N, N  [NN] S
Mathe and C
Sminchisescu
Action from still image  dataset and inverse optimal control to learn task specific visual scanpaths
In NIPS, N0NN
N  [NN] S
Monsell
Task switching
Trends in Cognitive Sciences,  N(N):NNN–NN0, N00N
N, N  [NN] D
P
Papadopoulos, A
D
F
Clarke, F
Keller, and V
Ferrari
 Training object class detectors from eye tracking data
In  ECCV, N0NN
N, N  [NN] D
P
Papadopoulos, J
R
R
Uijlings, F
Keller, and V
Ferrari
We don’t need no bounding-boxes: Training object class  detectors using only human verification
In CVPR, N0NN
N,  N, N, N  [NN] G
Papandreou, L.-C
Chen, K
Murphy, and A
L
Yuille
 Weakly- and semi-supervised learning of a deep convolutional network for semantic image segmentation
In ICCV,  N0NN
N  [NN] A
Prest, C
Leistner, J
Civera, C
Schmid, and V
Ferrari
Learning object class detectors from weakly annotated  video
In CVPR, N0NN
N  [N0] B
L
Price, B
Morse, and S
Cohen
Geodesic graph cut for  interactive image segmentation
In CVPR, N0N0
N  [NN] C
Rother, V
Kolmogorov, and A
Blake
Grabcut: Interactive foreground extraction using iterated graph cuts
In  SIGGRAPH, N00N
N, N, N, N, N, N, N  [NN] J
S
Rubinstein, D
E
Meyer, and J
E
Evans
Executive  control of cognitive processes in task switching
Journal of  Experimental Psychololgy: Human Perception and Performance, NN(N):NNN–NNN, N00N
N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
Berg, and L
Fei-Fei
ImageNet large scale visual recognition challenge
IJCV, N0NN
N, N, N, N, N  [NN] O
Russakovsky, L.-J
Li, and L
Fei-Fei
Best of both  worlds: human-machine collaboration for object annotation
 In CVPR, N0NN
N, N, N, N, N  [NN] O
Russakovsky, Y
Lin, K
Yu, and L
Fei-Fei
Objectcentric spatial pooling for image classification
In ECCV,  N0NN
N  [NN] B
C
Russell, K
P
Murphy, and W
T
Freeman
LabelMe:  a database and web-based tool for image annotation
IJCV,  N00N
N  [NN] R
N
Shepard and J
Metzler
Mental rotation of threedimensional objects
Science, NNN(NNNN):N0N–N0N, NNNN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 N, N  [NN] P
Siva and T
Xiang
Weakly supervised object detector  learning with model drift detection
In ICCV, N0NN
N  [N0] A
Sorokin and D
Forsyth
Utility data annotation with amazon mechanical turk
In Workshop at CVPR, N00N
N, N  [NN] SpareN/Mighty AI: https://app.spareN.com/  fives
Bounding box drawing instruction video
https:  //www.youtube.com/watch?v=NSZyFJiMGOw,  N0NN
N, N  [NN] H
Su, J
Deng, and L
Fei-Fei
Crowdsourcing annotations  for visual object detection
In AAAI Human Computation  Workshop, N0NN
N, N, N, N, N, N, N  [NN] A
Torralba, A
Oliva, M
Castelhano, and J
M
Henderson
 Contextual guidance of attention in natural scenes: The role  of global features on object search
Psychological Review,  NNN(N):NNN–NNN, N00N
N  [NN] O
Veksler
Star shape prior for graph-cut image segmentation
In ECCV, N00N
N, N  [NN] S
Vicente, V
Kolmogorov, and C
Rother
Graph cut based  image segmentation with connectivity priors
In CVPR,  N00N
N  [NN] C
Vondrick, D
Patterson, and D
Ramanan
Efficiently scaling up crowdsourced video annotation
IJCV, N0NN
N  [NN] C
Wang, W
Ren, J
Zhang, K
Huang, and S
Maybank
 Large-scale weakly supervised object localization via latent  category learning
IEEE Transactions on Image Processing,  NN(N):NNNN–NNNN, N0NN
N  [NN] J
Wang and M
Cohen
An iterative optimization approach  for unified image segmentation and matting
In ICCV, N00N
 N  [NN] T
Wang, B
Han, and J
Collomosse
Touchcut: Fast image and video segmentation using single-touch interaction
 CVIU, N0NN
N  [N0] P
Welinder, S
Branson, P
Perona, and S
J
Belongie
The  multidimensional wisdom of crowds
In NIPS, N0N0
N  [NN] J
Wu, Y
Zhao, J.-Y
Zhu, S
Luo, and Z
Tu
Milcut: A  sweeping line multiple instance learning paradigm for interactive image segmentation
In CVPR, N0NN
N, N, N  [NN] W
Yang, J
Cai, J
Zheng, and J
Luo
User-friendly interactive image segmentation through unified combinatorial user  inputs
IEEE Transactions on Image Processing, N0N0
N  NNNN  https://app.spareN.com/fives https://app.spareN.com/fives https://www.youtube.com/watch?v=NSZyFJiMGOw https://www.youtube.com/watch?v=NSZyFJiMGOwRoomNet: End-To-End Room Layout Estimation   RoomNet: End-to-End Room Layout Estimation  Chen-Yu Lee Vijay Badrinarayanan Tomasz Malisiewicz Andrew Rabinovich  Magic Leap, Inc
 {clee,vbadrinarayanan,tmalisiewicz,arabinovich}@magicleap.com  Abstract  This paper focuses on the task of room layout estimation from a monocular RGB image
Prior works break the  problem into two sub-tasks: semantic segmentation of floor,  walls, ceiling to produce layout hypotheses, followed by an  iterative optimization step to rank these hypotheses
 In contrast, we adopt a more direct formulation of this  problem as one of estimating an ordered set of room layout  keypoints
The room layout and the corresponding segmentation is completely specified given the locations of these ordered keypoints
We predict the locations of the room layout  keypoints using RoomNet, an end-to-end trainable encoderdecoder network
On the challenging benchmark datasets  Hedau and LSUN, we achieve state-of-the-art performance  along with N00× to N00× speedup compared to the most recent work
Additionally, we present optional extensions to  the RoomNet architecture such as including recurrent computations and memory units to refine the keypoint locations  under the same parametric capacity
 N
Introduction  Room layout estimation from a monocular image, which  aims to delineate a ND boxy representation of an indoor  scene, is an essential step for a wide variety of computer  vision tasks, and has recently received great attention from  several applications
These include indoor navigation [NN],  scene reconstruction/rendering [NN], and augmented reality [NN, NN, N0]
 The field of room layout estimation has been primarily  focused on using bottom-up image features such as local  color, texture, and edge cues followed by vanishing point  detection
A separate post-processing stage is used to clean  up feature outliers and generate/rank a large set of room  layout hypotheses with structured SVMs or conditional random fields (CRFs) [NN, NN, NN, NN, NN]
In principle, the ND  reconstruction of the room layout can be obtained (up to  scale) with knowledge of the ND layout and the vanishing  points
However, in practice, the accuracy of the final layout prediction often largely depends on the quality of the  Feature  Extraction   End-to-End  Trainable  RoomNet   N	 N	 N	  N	  N	 N	  Layout Hypotheses  Generation/Ranking   Vanishing Point  Detection (a)   (b)   Figure N
(a) Typical multi-step pipeline for room layout estimation
(b) Room layout estimation with RoomNet is direct and simple: run RoomNet, extract a set of room layout keypoints, and  connect the keypoints in a specific order to obtain the layout
 extracted low-level image features, which in itself is susceptible to local noise, scene clutter and occlusion
 Recently, with the rapid advances in deep convolutional neural networks (CNNs) for semantic segmentation  [N, NN, NN, N], researchers have been exploring the possibility of using such CNNs for room layout estimation
More  specifically, Mallya et al
[NN] first train a fully convolutional network (FCN) [NN] model to produce “informative  edge maps” that replace hand engineered low-level image  feature extraction
The predicted edge maps are then used  to sample vanishing lines for layout hypotheses generation  and ranking
Dasgupta et al
[N] use the FCN to learn semantic surface labels such as left wall, front wall, right wall,  ceiling, and ground
Then connected components and hole  filling techniques are used to refine the raw per pixel prediction of the FCN, followed by the classic vanishing point/line  sampling methods to produce room layouts
However, despite the improved results, these methods use CNNs to generate a new set of “low-level” features and fall short of exploiting the end-to-end learning ability of CNNs
In other  words, the raw CNN predictions need to be post-processed  by an expensive hypotheses testing stage to produce the final layout
This, for example, takes the pipeline of Dasgupta et al
[N] N0 seconds to process each frame
 In this work, we address the problem top-down by diNNNN    N	  N	  N	  N	 N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	  N	 N	  N	  N	 N	  Type 0 Type N Type N Type N Type N Type N   N	 N	  N	 N	  N	  N	  N	  N	  N	 N	  N	 N	  N	  N	  Type N Type N Type N Type N Type N0   Figure N
Definition of room layout types
The type is indexed from 0 to N0 as in [N0]
The number on each keypoint defines the specific  order of points saved in the ground truth
For a given room type, the ordering of keypoints specifies their connectivities
 rectly training CNNs to infer both the room layout corners  (keypoints) and room type
Once the room type is inferred  and the corresponding set of ordered keypoints are localized, we can connect them in a specific order to obtain the  ND spatial room layout
The proposed method, RoomNet,  is direct and simple as illustrated in Figure N: The network  takes an input image of size NN0 × NN0, processes it through  a convolutional encoder-decoder architecture, extracts a set  of room layout keypoints, and then simply connects the obtained keypoints in a specific order to draw a room layout
 The semantic segmentation of the layout surfaces is simply  obtainable as a consequence of this connectivity
 Overall, we make several contributions in this paper: (N)  reformulate the task of room layout estimation as a keypoint localization problem that can be directly addressed  using CNNs, (N) a custom designed convolutional encoderdecoder network, RoomNet, for parametrically efficient and  effective joint keypoint regression and room layout type  classification, and (N) state-of-the-art performance on challenging benchmarks Hedau [NN] and LSUN [N0] along with  N00× to N00× speedup compared to the most recent work
 N
RoomNet  N.N
Keypoint-based room layout representaiton  To design an effective room layout estimation system, it  is important to choose a proper target output representation  that is end-to-end trainable and can be inferred efficiently
 Intuitively, one can assign geometric context classes (floor,  walls, and ceiling) to each pixel, and then try to obtain room  layout keypoints and boundaries based on the pixel-wised  labels
However, it is non-trivial to derive layout keypoints  and boundaries from the raw pixel output
In contrast, if  we can design a model that directly outputs a set of ordered  room layout keypoint locations, it is then trivial to obtain  both keypoint-based and pixel-based room layout representations
 Another important property of using a keypoint-based  representation is that it eliminates the ambiguity in the  pixel-based representation
Researchers have shown that  CNNs often have difficulty distinguishing between different surface identities
For instance, CNNs can be confused  between the front wall class and the right wall class, and  thereby output irregular or mixed pixel-wise labels within  the same surface – this is well illustrated by Figure N and  N from [N]
This phenomenon also largely undermines the  overall room layout estimation performance
 Hence, we propose to use a keypoint-based room layout  representation to train our model
Figure N shows a list of  room types with their respective keypoint definition as defined by [N0]
These NN room layouts cover most of the  possible situations under typical camera poses and common  cuboid representations under “Manhattan world assumption” [N]
Once the trained model predicts correct keypoint  locations with an associated room type, we can then simply connect these points in a specific order to produce boxy  room layout representation
 N.N
Architecture of RoomNet  We design a CNN to delineate room layout structure using ND keypoints
The input to the network is a single RGB  image and the output of the network is a set of ND keypoints  in a specific order with an associated room type
 Keypoint estimation The base network architecture for  keypoint estimation is inspired by the recent successes in  the field of semantic segmentation [NN, NN, N]
Here we  adopt the SegNet architecture proposed by Badrinarayanan  et al
[N, N] with modifications
Initially designed for segmentation, the SegNet framework consists of encoder and  decoder sub-networks – the encoder of the SegNet maps an  input image to lower resolution feature maps, and then the  role of the decoder is to upsample the low resolution encoded feature maps to full input resolution for pixel-wise  classification
In particular, the decoder uses pooling indices computed in the max-pooling step of the corresponding encoder to perform non-linear upsampling
This eliminates the need for learning to upsample
The upsampled  maps are sparse and are convolved with trainable filters to  produce dense feature map
This architecture has proven to  provide good performance with competitive inference time  and efficient memory usage as compared to other recent semantic segmentation architectures
 The base architecture of RoomNet adopts essentially the  same convolutional encoder-decoder network as in SegNet
 It takes an image of an indoor scene and directly outputs a  set of ND room layout keypoints to recover the room layout  structure
Each keypoint ground truth is represented by a  ND Gaussian heatmap centered at the true keypoint location  as one of the channels in the output layer.N The encoderdecoder architecture processes the information flow through  bottleneck layers, enforcing it to implicitly model the relaNWe color-code and visualize multiple keypoint heatmaps in a single  ND image in Figure N, Figure N and the rest of the paper
 NNNN    NNN   NNN   NNN   NNN   NN0   NN0   N0   N0 N0 N0   NNN  NNN   NNN   N0   N0   N   Conv
Layer  NxNxNN   NxNxNN  Maxpool Layer   NxN-s-N   Conv
Layer  NxNxNNN   NxNxNNN  Maxpool Layer   NxN-s-N   Conv
Layer  NxNxNNN   NxNxNNN  NxNxNNN   Maxpool Layer   NxN-s-N  Dropout-0.N   Conv
Layer  NxNxNNN   NxNxNNN  NxNxNNN   Maxpool Layer   NxN-s-N  Dropout-0.N   Conv
Layer  NxNxNNN   NxNxNNN  NxNxNNN   Maxpool Layer   NxN-s-N  Dropout-0.N   Upsampling  Conv
Layer   NxNxNNN  NxNxNNN   NxNxNNN   Dropout-0.N   Upsampling  Conv
Layer   NxNxNNN  NxNxNNN   NxNxNNN   Conv
Layer  NxNxNN   NxNxNN   Pooling Indices   Recurrent structure   NN0 NN0 N0   N0   N0   N0   N0 N0   N0NN NNN NN   …   N0   N0   N N   Type 0 Type N Type N0   Room Type Learning/Inference   …   NN   Figure N
An illustration of the RoomNet base architecture
A decoder upsamples its input using the transferred pooling indices from  its encoder to produce sparse feature maps followed by a several convolutional layers with trainable filter banks to densify the feature  responses
The final decoder output keypoint heatmaps are fed to a regressor with Euclidean losses
A side head with N fully-connected  layers is attached to the bottleneck layer and used to train/predict the room type class label, which is then used to select the associated set  of keypoint heatmaps
The full model of RoomNet with recurrent encoder-decoder (center dashed line block) further performs keypoint  refinement as shown in Figure N (b) and N
 tionship among the keypoints that encode the ND structure  of the room layout
 The decoder of the RoomNet upsamples the feature  maps from the bottleneck layer with spatial dimension N0  × N0 to N0 × N0 instead of the full resolution NN0 × NN0  as shown in Figure N
This is because we empirically  found that using the proposed ND keypoint-based representation can already model the room layout effectively at N0  × N0 scale (results are similar as compared to training decoder sub-network at full resolution)
Using this “trimmed”  decoder sub-network also significantly reduces the memory/time cost during both training and testing due to the  high computation cost of convolution at higher resolutions
 Extending to multiple room types The aforementioned  keypoint estimation framework serves as a basic room layout estimation system for one particular room type
To generalize this approach for multiple room types, one possible  solution is to train one network per class as in the Single  Image ND Interpreter Network of Wu et al
[NN]
However,  in order to maximize efficiency, we design the RoomNet  to be fast from the ground up
Encouraged by the recent  object detection works YOLO [NN] and SSD [NN] that utilize a single neural network to predict bounding boxes and  class probabilities directly from full images in one evaluation, our proposed RoomNet similarly predicts room layout  keypoints and the associated room type with respect to the  input image in one forward pass
To achieve this goal, we  increase the number of channels in the output layer to match  the total number of keypoints for all NN room types (total NN  keypoints for NN room types derived from Figure N), and we  also add a side head with fully connected layers to the bottleneck layer (the layer where usually used for image classification) for room type prediction as shown in Figure N
 We denote a training example as (I,y, t), where y stands for the ground truth coordinates of the k keypoints  with room type t for the input image I 
At training stage,  we use the Euclidean loss as the cost function for layout  keypoint heatmap regression and use the cross-entropy loss  for the room type prediction
Given the keypoint heatmap  regressor ϕ (output from the decoder sub-network), and the  room type classifier ψ (output from the fully-connected side  head layer), we can then optimize the following loss function: ∑  k  ✶ keypoint  k,t ‖Gk(y)− ϕk(I)‖ N − λ  ∑  c  ✶ room c,t log(ψc(I)) (N)  where ✶ keypoint  k,t denotes if keypoint k appears in ground truth  room type t, ✶roomc,t denotes if room type index c equals to  the ground truth room type t, G is a Gaussian centered at  y and the weight term λ is set to N by cross validation
 The first term in the loss function compares the predicted  heatmaps to ground-truth heatmaps synthesized for each  keypoint separately
The ground truth for each keypoint  heatmap is a ND Gaussian centered on the true keypoint location with standard deviation of N pixels as in the common  practice in recent keypoint regression works [NN, NN, N, NN]
 The second term in the loss function encourages the side  head fully-connected layers to produce a high confidence  value with respect to the correct room type class label
 Note that one forward pass of the proposed architecture  will produce keypoint heatmaps for all room types
However, the loss function only penalizes Euclidean regression  error if the keypoint k is present for the ground truth room  NNNN    type t in the current input image I , effectively using the  predicted room type indices to select the corresponding set  of keypoint heatmaps to update the regressor
The same  strategy applies at the test stage i.e
the predicted room type  is used to select the corresponding set of keypoint heatmaps  in the final output
 RoomNet extension for keypoint refinement Recurrent  neural networks (RNNs) and its variant Long Short-Term  Memory (LSTM) [NN] have proven to be extremely effective models when dealing with sequential data
Since then,  researchers have been exploring the use of recurrent structures for static input format as well, such as recurrent convolutional layers [NN] and convLSTM layers [NN]
 Recently, more sophisticated iterative/recurrent architectures have been proposed for ND static input, such as FCN  with CRF-RNN [NN], iterative error feedback networks [N],  recurrent CNNs [N], stacked encoder-decoder [NN], and recurrent encoder-decoder networks [NN, NN]
These evidence  show that adopting the “time series” concept when modeling a static input can also significantly improve the ability  of the network to integrate contextual information and to  reduce prediction error
 Motivated by the aforementioned successes, we extend our base RoomNet architecture by making the central  encoder-decoder component (see center dashed line block  in Figure N) recurrent
Specifically, we propose a memory augmented recurrent encoder-decoder (MRED) structure (see Figure N (b)) whose goal is to mimic the behavior  of a typical recurrent neural network (Figure N (a)) in order  to refine the predicted keypoint heatmaps over “time” – the  artificial time steps created by the recurrent structure
 Each layer in this MRED structure shares the same  weight matrices through different time steps that convolve  (denoted as ∗ symbol) with the incoming feature maps from  the previous prediction hl(t − N) at time step t − N in the same layer l and the current input hl−N(t) at time step t in the previous layer l− N, generating output at time step t as:  hl(t) =  {  σ(wcurrent l  ∗ hl−N(t) + bl) , t = 0  σ(wcurrent l  ∗ hl−N(t) +w previous  l ∗ hl(t− N) + bl) , t > 0  (N)  where wcurrentl and w previous  l are the input and feed-forward  weights for layer l
bl is the bias for layer l
σ is the ReLU  activation function [N0]
 Figure N (b) demonstrates the overall process of the information flow during forward- and backward- propagations through depth and time within the recurrent encoderdecoder structure
The advantages of using the proposed  MRED architecture are: (N) exploiting the contextual and  structural knowledge among keypoints iteratively through  hidden/memory units (that have not been explored in recurrent convolutional encoder-decoder structure) and (N)  weight sharing of the convolutional layers in the recurrent  encoder-decoder, resulting in a much deeper network with a  Memory Augmented  Recurrent Encoder-Decoder   =	  Forward pass   Unrolled backward pass  Backward pass   Feature  Maps   Feature  Maps   =	 R   x0   y0   (a)   (b)   RNN R   xN   yN   R   xN   yN   R   xt   yt   h l-N   (0) hl-N (N) hl-N (N)   h l  (0) hl (N) hl (N) hl (t)   h l-N   (t)   Figure N
Illustration of unrolled (N iterations) version of (a) a  RNN and (b) the proposed memory augmented recurrent encoderdecoder architecture that mimics the behavior of a RNN but which  is designed for a static input
Both structures have hidden units  to store previous activations that help the inference at the current  time step
 Without  Keypoint   Refinement   Image   With  Keypoint   Refinement   (a)   (b)   Figure N
Room layout keypoint estimation from a single image (a)  without refinement and (b) with refinement
Keypoint heatmaps  from multiple channels are color-coded and shown in a single ND  image for visualization purposes
The keypoint refinement step  produces more concentrated and cleaner heatmaps and removes  some false positives
 fixed number of parameters
After refinement, the heatmaps  of keypoints are much cleaner as shown in Figure N
It is  also interesting to observe the mistakes made early on and  corrected later by the network (see third and fourth columns  in Figure N)
We analyze the performance with and without  the keypoint refinement step in Section N.N, and we also  evaluate different encoder-decoder variants in Section N
 Deep supervision through time When applying stacked,  iterative, or recurrent convolutional structures, each layer  in the network receives gradients across more layers or/and  time steps, resulting in models that are much harder to train
 For instance, the iterative error feedback network [N] reNNNN    quires multi-stage training and the stacked encoder-decoder  structure in [NN] uses intermediate supervision at the end of  each encoder-decoder even when batch normalization [NN]  is used
Following the practices in [NN, NN], we extend the  idea by injecting supervision at the end of each time step
 The same loss function L is applied to all the time steps as  demonstrated in Figure N
Section N.N and Table N provide  details of the analysis and effect of the deep supervision  through time
 N
Experiments  N.N
Datasets  We evaluate the proposed RoomNet framework on two  challenging benchmark datasets: Hedau [NN] dataset and  Large-scale Scene Understanding Challenge (LSUN) room  layout dataset [N0]
The Hedau dataset contains N0N training, NN validation, and N0N test images that are collected  from the web and from LabelMe [NN]
The LSUN dataset  consists of N000 training, NNN validation, and N000 test images that are sampled from SUN database [NN]
We follow the same experimental setup as Dasgupta et al
[N]
We  rescale all input images to NN0 × NN0 pixels and train our  network from scratch on the LSUN training set only
All  experimental results are computed using the LSUN room  layout challenge toolkit [N0] on the original image scales
 N.N
Implementation details  The input to the network is an RGB image of resolution NN0 × NN0 and the output is the room layout keypoint  heatmaps of resolution N0 × N0 with an associated room  type class label
We apply the backpropagation through  time (BPTT) algorithm to train the models with batch size  N0 SGD, 0.N dropout rate, 0.N momentum, and 0.000N  weight decay
Initial learning rate is 0.0000N and decreased  by a factor of N twice at epoch NN0 and N00, respectively
 All variants use the same scheme with NNN total epochs
 The encoder and decoder weights are all initialized using  the technique described in He et al
[NN]
Batch normalization [NN] and ReLU [N0] activation function are also used after each convolutional layer to improve the training process
 We apply horizontal flipping of input images during training  as the only data augmentation
The system is implemented  in the open source deep learning framework Caffe [N0]
 In addition, a ground truth keypoint heatmap has zero  value (background) for most of its area and only a small  portion of it corresponds to the Gaussian distribution (foreground associated with actual keypoint location)
The output of the network therefore tends to converges to zero due  to the imbalance between foreground and background distributions
For this reason, it is crucial to weight the gradients based on the ratio between foreground and background  area for each keypoint heatmap
In our experiment, we  degrade the gradients of background pixels by multiplying  L    x  L 0   L N   L N   x  (a) (b)   =	  L    x  =	  L    x  t=0 t=N t=N t=0 t=N t=N   Figure N
Illustration of the proposed memory augmented recurrent  encoder-decoder architecture (a) without deep supervision through  time and (b) with deep supervision through time
 them with a factor of 0.N and found this makes training significantly more stable
 Training from scratch takes about N0 hours on N NVIDIA  Titan X GPUs
One forward inference of the full model  (RoomNet recurrent N-iter) takes NN ms on a single GPU
 For generating final test predictions we run both the original input and a flipped version of the image through the network and average the heatmaps together (accounting for a  0.NN% average improvement on keypoint error and a 0.NN%  average improvement on pixel error) as in [NN]
The keypoint location is chosen to be the max activating location of  the corresponding heatmap
 N.N
Results  Two standard room layout estimation evaluation metrices are: (N) pixel error: pixelwise error between the predicted surface labels and ground truth labels, and (N) keypoint error: average Euclidean distance between the predicted keypoint and annotated keypoint locations, normalized by the image diagonal length
 Accuracy We summarize the performance on both datasets  in Table N and N
The previous best method is the two-step  framework (per pixel CNN-based segmentation with a separate hypotheses ranking approach) Dasgupta et al
[N]
The  proposed RoomNet significantly improves upon the previous results on both keypoint error and pixel error, achieving  state-of-the-art performance N
 To decouple the performance gains due to external data,  we also prepare results of fine-tuning the RoomNet from  a SUN [NN] pre-trained model (on semantic segmentation  task) and this achieves N.0N% keypoint error and N.0N%  pixel error as compared of method in [NN]N with N.NN% keypoint error and N.NN% pixel error on LSUN dataset
 Runtime and complexity Efficiency evaluation on the input image size of NN0 × NN0 is shown in Table N
Our  full model (RoomNet recurrent N iteration) achieves N00×  speedup compares to the previous best method in [N], and  NThe side head room type classifier obtained NN.N% accuracy on LSUN  dataset
NThe multi-step method in [NN] utilizes additional Hedau+ [NN] training  set and fine-tunes from NYUDvN RGBD [NN] pre-trained models
 NNNN    Method Pixel Error (%)  Hedau et al
(N00N) [NN] NN.N0  Del Pero et al
(N0NN) [N] NN.N0  Gupta et al
(N0N0) [NN] NN.N0  Zhao et al
(N0NN) [NN] NN.N0  Ramalingam et al
(N0NN) [NN] NN.NN  Mallya et al
(N0NN) [NN] NN.NN  Schwing et al
(N0NN) [N0] NN.N  Del Pero et al
(N0NN) [N] NN.N  Dasgupta et al
(N0NN) [N] N.NN  RoomNet recurrent N-iter (ours) N.NN  Table N
Performance on Hedau dataset [NN]
We outperform the  previous best result in [N] using the proposed end-to-end trainable  RoomNet
 Method Keypoint Error (%) Pixel Error (%)  Hedau et al
(N00N) [NN] NN.NN NN.NN  Mallya et al
(N0NN) [NN] NN.0N NN.NN  Dasgupta et al
(N0NN) [N] N.N0 N0.NN  RoomNet recurrent N-iter (ours) N.N0 N.NN  Table N
Performance on LSUN dataset [N0]
We outperform the  previous best result in [N] on both keypoint and pixel errors using  the proposed end-to-end trainable RoomNet
 Method FPS  Del Pero et al
(N0NN) [N] 0.00N  Dasgupta et al
(N0NN) [N] 0.0N  RoomNet recurrent N-iter N.NN  RoomNet recurrent N-iter N.NN  RoomNet basic NN.NN  Table N
Runtime evaluation on an input size of NN0×NN0
The proposed RoomNet full model (N-iter) achieves N00× speedup and the basic RoomNet model achieves N00× speedup than the previ- ous best method in [N]
 the base RoomNet without recurrent structure (RoomNet  basic) achieves N00× speedup
Note that the timing is for  two forward passes as described earlier
Using either one  of the proposed architecture can provide significant inference time reduction and an improved accuracy as shown in  Table N
 N.N
Analyzing RoomNet  In this section, we empirically investigate the effect  of each component in the proposed architecture with the  LSUN dataset as our running example
 Recurrent vs direct prediction Table N shows the effectiveness of extending the RoomNet-basic architecture to a  memory augmented recurrent encoder-decoder networks
 We observed that more iterations led to lower error rates  Model Keypoint Error (%) Pixel Error (%)  RoomNet basic N.NN N0.NN  RoomNet recurrent N-iter N.NN N.NN  RoomNet recurrent N-iter N.N0 N.NN  Table N
The impact of keypoint refinement step (see Section N.N)  using the proposed memory augmented recurrent encoder-decoder  architecture on LSUN dataset [N0]
 Model Keypoint Error (%) Pixel Error (%)  RoomNet recurrent N-iter  - w/o deep supervision through time N.NN N0.NN  - w/ deep supervision through time N.NN N.NN  RoomNet recurrent N-iter  - w/o deep supervision through time N.NN N0.NN  - w/ deep supervision through time N.N0 N.NN  Table N
The impact of deep supervision through time on LSUN  dataset [N0] for RoomNets with N and N recurrent iterations
 on both keypoint error and pixel error: the RoomNet with  recurrent structure that iteratively regresses to correct keypoint locations achieves N.N% keypoint error and N.NN pixel  error as compared to the RoomNet without recurrent structure which achieves N.NN% keypoint error and N0.NN pixel  error
No further significant performance improvement is  observed after N iterations
Notice that the improvement  essentially came from the same parametric capacity within  the networks since the weights of convolutional layers are  shared across iterations
 Importance of deep supervision through time When applying a recurrent structure with encoder-decoder architectures, each layer in the network receives gradients not only  across depth but also through time steps between the input and the final objective function during training
It is  therefore of interest to investigate the effect of adding auxiliary loss functions at different time steps
Table N demonstrates the impact of deep supervision through time using  RoomNet with N and N recurrent iterations
We observed  immediate reduction in both keypoint error and pixel error  by adding auxiliary losses for both cases
This can be understood by the fact that the learning problem with deep supervision is much easier [NN, NN] through different time steps
 It is also interesting to point out that RoomNet N-iter performs worse than RoomNet N-iter when deep supervision  through time is not applied
This is rectified when deep  supervision through time is applied
Overall, we validate  that with more iterations in the recurrent structure, there is  a stronger need to apply deep supervision through time to  successfully train the proposed architecture
 Qualitative results We show qualitative results of the proposed RoomNet in Figure N
When the image is clean and  NNN0    Input RoomNet Output Ground Truth   (a)   (b)   (c)   (d)   (e)   (f)   (g)   Figure N
The RoomNet predictions and the corresponding ground truth on LSUN dataset
The proposed architecture takes a RGB input  (first column) and produces room layout keypoint heatmaps (second column)
The final keypoints are obtained by extracting the location  with maximum response from the heatmaps
The third and fourth columns show a boxy room layout representation by simply connecting  obtained keypoints in a specific order as in Figure N
The fifth and sixth columns show the ground truth
Our algorithm is robust to keypoint  occlusion by objects (ex: tables, chairs, beds)
 the room layout boundaries/corners are not occluded, our  algorithm can recover the boxy room layout representation  with high accuracy
Our framework is also robust to keypoint occlusion by objects (ex: tables, chairs, beds), demonstrated in Figure N (b)(c)(d)(f)
The major failure cases are  when room layout boundaries are barely visible (Figure N  (a)(c)) or when there are more than one plausible room layout explanations for a given image of a scene (Figure N  (b)(d))
 N
Discussion  Alternative encoder-decoders We provide an evaluation  of alternative encoder-decoder architectures for the room  layout estimation task including: (a) a vanilla encoderdecoder (RoomNet basic), (b) stacked encoder-decoder,  (c) stacked encoder-decoder with skip-connections; (d)  encoder-decoder with feedback; (e) memory augmented recurrent encoder-decoder (RoomNet full); (f) memory augmented recurrent encoder-decoder with feedback
Figure N  illustrates the N different network configurations that are  evaluated here
We emphasize that our intention is not to put  each encoder-decoder variant in competition, but to provide  an illustrative comparison of the relative benefits of different configurations for the task being addressed here
Table N shows the performance of different variants on LSUN  dataset
 The comparison of (a) and (b) variants indicates that  stacking encoder-decoder networks can further improve the  performance, as the network is enforced to learn the spatial  structure of the room layout keypoints implicitly by placing  constraints on multiple bottleneck layers
 However, adding skip connections [NN, NN] as in (c) does  NNNN    Input RoomNet Output Ground Truth   (a)   (b)   (c)   (d)   Figure N
The ambigous cases where the RoomNet predictions do not match the human-annotated ground truth
The first column is the  input image, the second column is predicted keypoint heatmaps, the third and fourth columns are obtained boxy representation, and the  fifth and sixth columns show the ground truth
 (a) (b) (c) (d) (e) (f)   Direct forward connection  Forward connection through hidden units   Figure N
Illustration of different encoder-decoder architecture  configurations: (a) vanilla encoder-decoder; (b) stacked encoderdecoder; (c) stacked encoder-decoder with skip-connections; (d)  encoder-decoder with feedback; (e) memory augmented recurrent encoder-decoder; (f) memory augmented recurrent encoderdecoder with feedback
 not improve the performance for this task
This could be  because the size of the training set (thousands) is not as  large as other datasets (millions) that have been evaluated  on, therefore skipping layers is not necessary for the specific dataset
 Adding a feedback loop, implemented as a concatenation  of input and previous prediction as a new input [NN, NN] for  the same encoder-decoder network as in (d) improves the  performance
At each iteration, the network has access to  the thus-far sub-optimal prediction along with the original  input to help inference at the current time step
 Making an encoder-decoder recurrent with memory units  (e) to behave as a RNN obtains the lowest keypoint error  and pixel error (our full RoomNet model)
The lateral connections in the recurrent encoder-decoder allow the network  to carry information forward and help prediction at future  time steps
Finally, adding a feedback loop to the memory  Model Keypoint Error (%) Pixel Error (%)  Vanilla enc-dec (RoomNet basic) N.NN N0.NN  Stacked enc-dec N.NN N0.NN  Stacked enc-dec with skip connect
N.0N N0.NN  Enc-dec w/ feedback N.NN N0.N0  Recurrent enc-dec (RoomNet full) N.N0 N.NN  Recurrent enc-dec w/ feedback N.NN N.NN  Table N
Evaluation of encoder-decoder (enc-dec) variants on  LSUN dataset [N0]
Note that recurrent encoder-decoders use N  iteration time steps
 augmented recurrent encoder-decoder (f) does not improve  the results
It is possible that using the memory augmented  structure (e) can already store previous hidden state information well without feedback
Note that weight matrices of  the encoder-decoder are not shared in configurations (b) and  (c) but shared in configurations (d), (e), and (f), resulting in  more parametrically efficient architectures
 N
Conclusion  We presented a simple and direct formulation of room  layout estimation as a keypoint localization problem
We  showed that our RoomNet architecture and its extensions  can be trained end-to-end to perform accurate and efficient  room layout estimation
The proposed approach stands out  from a large body of work using geometry inspired multistep processing pipelines
In the future, we would like to  adopt gating mechanism [NN] to allow incoming signal to  alter the state of recurrent units and extend RoomNet to sequential data for building room layout maps  NNNN    References  [N] V
Badrinarayanan, A
Kendall, and R
Cipolla
Segnet: A  deep convolutional encoder-decoder architecture for image  segmentation
arXiv:NNNN.00NNN, N0NN
 [N] V
Badrinarayanan, A
Kendall, and R
Cipolla
Segnet: A  deep convolutional encoder-decoder architecture for scene  segmentation
TPAMI, N0NN
 [N] V
Belagiannis and A
Zisserman
Recurrent human pose  estimation
arXiv:NN0N.0NNNN, N0NN
 [N] J
Carreira, P
Agrawal, K
Fragkiadaki, and J
Malik
Human pose estimation with iterative error feedback
In CVPR,  N0NN
 [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected crfs
In ICLR, N0NN
 [N] J
M
Coughlan and A
L
Yuille
The manhattan world  assumption: Regularities in scene statistics which enable  bayesian inference
In NIPS, N000
 [N] S
Dasgupta, K
Fang, K
Chen, and S
Savarese
Delay:  Robust spatial layout estimation for cluttered indoor scenes
 In CVPR, N0NN
 [N] L
Del Pero, J
Bowdish, D
Fried, B
Kermgard, E
Hartley, and K
Barnard
Bayesian geometric modeling of indoor  scenes
In CVPR, N0NN
 [N] L
Del Pero, J
Bowdish, B
Kermgard, E
Hartley, and  K
Barnard
Understanding bayesian rooms using composite  Nd object models
In CVPR, N0NN
 [N0] D
DeTone, T
Malisiewicz, and A
Rabinovich
Deep image  homography estimation
arXiv:NN0N.0NNNN, N0NN
 [NN] A
Gupta, M
Hebert, T
Kanade, and D
M
Blei
Estimating spatial layout of rooms using volumetric reasoning about  objects and surfaces
In NIPS, N0N0
 [NN] S
Gupta, P
Arbelaez, and J
Malik
Perceptual organization and recognition of indoor scenes from rgb-d images
In  CVPR, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Delving deep into  rectifiers: Surpassing human-level performance on imagenet  classification
In ICCV, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
 [NN] V
Hedau, D
Hoiem, and D
Forsyth
Recovering the spatial  layout of cluttered rooms
In ICCV, N00N
 [NN] V
Hedau, D
Hoiem, and D
Forsyth
Recovering free space  of indoor scenes from a single image
In CVPR, N0NN
 [NN] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural computation, NNNN
 [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
In  ICML, N0NN
 [NN] H
Izadinia, Q
Shan, and S
M
Seitz
ImNcad
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [N0] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional  architecture for fast feature embedding
In ACM MM, N0NN
 [NN] C.-Y
Lee, P
W
Gallagher, and Z
Tu
Generalizing pooling  functions in convolutional neural networks: Mixed, gated,  and tree
In AISTATS, N0NN
 [NN] C.-Y
Lee and S
Osindero
Recursive recurrent nets with  attention modeling for ocr in the wild
In CVPR, N0NN
 [NN] C.-Y
Lee, S
Xie, P
W
Gallagher, Z
Zhang, and Z
Tu
 Deeply-supervised nets
In AISTATS, N0NN
 [NN] M
Liang and X
Hu
Recurrent convolutional neural network  for object recognition
In CVPR, N0NN
 [NN] C
Liu, A
G
Schwing, K
Kundu, R
Urtasun, and S
Fidler
 RentNd: Floor-plan priors for monocular layout estimation
 In CVPR, N0NN
 [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.-Y
 Fu, and A
C
Berg
Ssd: Single shot multibox detector
In  ECCV, N0NN
 [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
 [NN] A
Mallya and S
Lazebnik
Learning informative edge maps  for indoor scene layout prediction
In ICCV, N0NN
 [NN] P
Mirowski, R
Pascanu, F
Viola, H
Soyer, A
Ballard,  A
Banino, M
Denil, R
Goroshin, L
Sifre, K
Kavukcuoglu,  et al
Learning to navigate in complex environments
In  ICLR, N0NN
 [N0] V
Nair and G
E
Hinton
Rectified linear units improve restricted boltzmann machines
In ICML, N0N0
 [NN] A
Newell, K
Yang, and J
Deng
Stacked hourglass networks for human pose estimation
In ECCV, N0NN
 [NN] H
Noh, S
Hong, and B
Han
Learning deconvolution network for semantic segmentation
In ICCV, N0NN
 [NN] M
Oberweger, P
Wohlhart, and V
Lepetit
Training a feedback loop for hand pose estimation
In ICCV, N0NN
 [NN] X
Peng, R
S
Feris, X
Wang, and D
N
Metaxas
A recurrent encoder-decoder network for sequential face alignment
 In ECCV, N0NN
 [NN] T
Pfister, J
Charles, and A
Zisserman
Flowing convnets  for human pose estimation in videos
In ICCV, N0NN
 [NN] S
Ramalingam, J
K
Pillai, A
Jain, and Y
Taguchi
Manhattan junction catalogue for spatial reasoning of indoor scenes
 In CVPR, N0NN
 [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In  CVPR, N0NN
 [NN] Y
Ren, C
Chen, S
Li, and C.-C
J
Kuo
A coarse-to-fine  indoor layout estimation (cfile) method
In ACCV, N0NN
 [NN] B
C
Russell, A
Torralba, K
P
Murphy, and W
T
Freeman
 Labelme: a database and web-based tool for image annotation
IJCV, N00N
 [N0] A
G
Schwing, T
Hazan, M
Pollefeys, and R
Urtasun
Efficient structured prediction for Nd indoor scene understanding
In CVPR, N0NN
 [NN] S
Song, S
P
Lichtenberg, and J
Xiao
Sun rgb-d: A rgb-d  scene understanding benchmark suite
In CVPR, N0NN
 [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
 [NN] J
J
Tompson, A
Jain, Y
LeCun, and C
Bregler
Joint training of a convolutional network and a graphical model for  human pose estimation
In NIPS, N0NN
 [NN] Z
Tu
Auto-context and its application to high-level vision  tasks
In CVPR, N00N
 NNNN    [NN] J
Wu, T
Xue, J
J
Lim, Y
Tian, J
B
Tenenbaum, A
Torralba, and W
T
Freeman
Single image Nd interpreter network
In ECCV, N0NN
 [NN] J
Xiao and Y
Furukawa
Reconstructing the worlds museums
IJCV, N0NN
 [NN] J
Xiao, J
Hays, K
A
Ehinger, A
Oliva, and A
Torralba
 Sun database: Large-scale scene recognition from abbey to  zoo
In CVPR, N0N0
 [NN] S
Xingjian, Z
Chen, H
Wang, D.-Y
Yeung, W.-K
Wong,  and W.-c
Woo
Convolutional lstm network: A machine  learning approach for precipitation nowcasting
In NIPS,  N0NN
 [NN] J
Zhang, C
Kan, A
G
Schwing, and R
Urtasun
Estimating the Nd layout of indoor scenes and its clutter from depth  sensors
In ICCV, N0NN
 [N0] Y
Zhang, F
Yu, S
Song, P
Xu, A
Seff, and J
Xiao
Largescale scene understanding challenge: Room layout estimation, N0NN
 [NN] Y
Zhao and S.-C
Zhu
Scene parsing by integrating function, geometry and appearance models
In CVPR, N0NN
 [NN] S
Zheng, S
Jayasumana, B
Romera-Paredes, V
Vineet,  Z
Su, D
Du, C
Huang, and P
H
Torr
Conditional random fields as recurrent neural networks
In CVPR, N0NN
 NNNNMutual Enhancement for Detection of Multiple Logos in Sports Videos   Mutual Enhancement for Detection of Multiple Logos in Sports Videos  Yuan LiaoN, Xiaoqing LuN, Chengcui ZhangN, Yongtao WangN, Zhi TangN  NInstitute of Computer Science and Technology, Peking University, Beijing, China NDepartment of Computer Science, University of Alabama at Birmingham, USA  {liao yuan, lvxiaoqing, wangyongtao, tangzhi}@pku.edu.cn, czhang0N@uab.edu  Abstract  Detecting logo frequency and duration in sports videos  provides sponsors an effective way to evaluate their advertising efforts
However, general-purposed object detection methods cannot address all the challenges in sports  videos
In this paper, we propose a mutual-enhanced approach that can improve the detection of a logo through the  information obtained from other simultaneously occurred  logos
In a Fast-RCNN-based framework, we first introduce  a homogeneity-enhanced re-ranking method by analyzing  the characteristics of homogeneous logos in each frame, including type repetition, color consistency, and mutual exclusion
Different from conventional enhance mechanism  that improves the weak proposals with the dominant proposals, our mutual method can also enhance the relatively significant proposals with weak proposals
Mutual enhancement is also included in our frame propagation mechanism  that improves logo detection by utilizing the continuity of  logos across frames
We use a tennis video dataset and an  associated logo collection for detection evaluation
Experiments show that the proposed method outperforms existing  methods with a higher accuracy
 N
Introduction  Logo detection in images and videos has been gaining  considerable attention in the last decade, with many applications such as traffic-control systems and measurement of  brand exposure
Detecting and classifying logos in videos  are valuable for business analysis, especially for television  advertisers
The frequency and duration of logos allow  sponsors to evaluate the effect of their advertising efforts
 Detection requires localizing objects within an image
 Existing researches on logo detection and recognition have  made great achievements in still image [N, N, N0, NN, NN]
 Most logo databases adopted by retrieval systems consist of  still images only [NN, NN, NN, NN]
The logos in still images  are generally clear and likely to be captured from a front  facing view
However in videos, due to zooming, panning  Figure N
Challenges in logo detection
The logos from the same  sponsor, (a), (b), and (c) have different layouts and deformation in  tennis videos
The logos in neighboring frames (d), (e), and (f),  suffer from motion blur and partial occlusion
 and changing in camera exposures, the chrominance level  changes frequently, and the logos are often deformed and  blurred
Consequently, the logo detection for videos faces  many more challenges
 Logo detection could be considered as a sub problem of  object detection
The existing approaches [N, N, NN] generally consist of two important modules: detecting candidate  object regions and classifying those regions
Besides the  common challenges in object detection in videos, the detection of sponsor logos faces at least three specific obstacles
First, as shown in Figures N(a), (b) and (c), according  to the requirement of sponsors, the same sponsor’s logos  may exhibit different layouts and thus cause higher innercluster differences than ordinary objects; second, multiple  instances of the same logo sometimes appear simultaneously but with various visual qualities, as shown in Figures  N(d), (e), and (f)
Traditional methods can hardly detect  all of them simultaneously
Furthermore, as a result of the  panning and zooming of the camera, consecutive frames  can produce different degrees of fuzziness, and the detection in fuzzy frames can be eased by utilizing relevant information from adjacent frames
To overcome the abovementioned obstacles, we propose a logo detection framework based on a mutual enhancement mechanism aiming  NNNNNNNN    to improve the detection of a logo leveraging the information obtained from other simultaneously occurring logos
In a Fast-RCNN-based framework, we introduce a  homogeneity-enhanced re-ranking method by analyzing the  characteristics of logos in videos to improve the region proposal accuracy in frames, including the type repetition of  appearance, color consistency and mutual exclusion
As our  most import contribution, this technique can be applied to  other kinds of videos that contain multiple homogenous objects appearing simultaneously but with various visual qualities
Different from the conventional enhance mechanism  that improves the weak proposals with the dominant proposals, our method enables all the proposals in one frame  to mutually enhance each other, including improving those  relatively significant or non-obvious proposals with weak  proposals by their common characteristics and the potential  alignment information
Moreover, a frame propagation enhancement method is also presented to assist the detection  in contiguous frames
 The remainder of this paper is structured as follows
 Section N introduces the related works of logo recognition  and detection
In Section N, the framework of the proposed method is presented
The homogeneity-enhanced  re-ranking is detailed in Section N
The propagatingenhancement and control mechanism is introduced in Section N
Section N introduces the dataset used in this work,  and presents the experimental results
Section N concludes  this paper
 N
Related Works  Previous works generally establish detection models  with key-point representations commendably capturing specific patterns present in graphic logos, such as key-pointbased detectors and bag-of-words models [NN, NN, NN, NN,  N0, NN, NN, NN]
These methods extract local features such  as SIFT [NN] or HoG [N] from images, cluster and quantize these features into visual words and finally measure the  similarity between a test image and a logo image according to these visual words
For instance, Romberg et al
[NN]  proposed a shape representation built with found key-points  and their respective SIFT representation for scalable logo  recognition in images
In [NN], bundles of SIFT features are  built from local regions around each key-point to index specific graphic logo patterns
In [NN], an improved topological  constraint, which considers the relative position between a  key-point and its neighbor points, is proposed to reduce the  number of mismatched key-points
 In recent years, deep learning has shown its good performance in object detection
State-of-the-art deep learningbased object detection networks such as R-CNN [N0] depend on region proposal algorithms to hypothesize object  locations
Further improvements Fast R-CNN [N] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck
Region  Proposal Network (RPN) [NN] shares full-image convolutional features with the detection network, thus enabling  nearly cost-free region proposals
There are also a lot of  attempts to apply deep learning in logo detection, and obtain excellent results [N, NN, N0]
Bianco et al
[N] involves  the selection of candidate subwindows using an unsupervised segmentation algorithm, and the SVM-based classification of such candidate regions uses features computed  by a CNN
Oliveira et al
[N0] adopts the transfer learning  to leverage powerful Convolutional Neural Network models  trained with large-scale datasets and repurposes them in the  context of graphic logo detection
 Video object tracking is another task close to video logo  detection
Object tracking (especially single object tracking) refers to tasks that estimate the object state in subsequent frames with a given initial object state in the first  frame
Tracking-by-detection methods gradually become  the main-stream in the field of object tracking because of its  outstanding performance [N, N, NN, NN, NN]
Wang and Yeung [NN] propose a framework of offline training and online  fine-tuning, which largely solved the problem of insufficient  training samples
While the majority of tracking targets are  foreground, logos in sports videos generally belong to background
Therefore, it is difficult to distinguish logos from  other background by movement analysis
Besides, the logos drift in and out of the camera frequently, which makes  the detection of logos in sport videos more challenging than  typical tracking tasks
 Some previous efforts attempt to recognize logos in  videos [N, N, NN]
Richard et al
[N] propose string based  template matching to recognize logos in video stills
Chattopadhyay and Sinha [N] propose a system to automatically  recognize the logos from sports videos for channel hyperlinking in the client end
 Despite the past efforts in video logo detection, the overall accuracy is still far from satisfactory
Different types of  sports videos have their own unique characteristics
For example, in soccer videos, logos may look small and blurry in  high-angle bleacher shots, while in tennis videos, logos may  appear large in size but more likely to be occluded
(Figure  N(f))
Rich interframe information in videos has also been  underutilized
In this paper, we focus on the characteristics in sports videos, utilize the mutual information in tennis  videos and the enhanced deep learning to detect trademarks  in videos
 N
The Proposed Framework  Deep Convolution Neural Networks (CNNs) are increasingly used in objection detection
A typical CNN-based  framework for logo detection [N0] generally consists of  video preprocessing, candidate logo proposition and CNNbased classification modules
In this paper, the proposed  NNNNNNNN    Figure N
Video logo detection framework
The proposed video logo detection framework contains two major modules: proposal re-ranking  and frame propagation
(a) Detection method decision
If the similarity between two frames is less than the threshold, we regard the coming  frame as a shot boundary, then (b) is performed, otherwise (e) is performed
(b) When a shot boundary is detected, a few frames at the  beginning of the shot are processed by region proposal and RCNN
The proposal rerank is operated to adjust the ranking of detection  results
Detection results of these frames are utilized to propagate the subsequent frames
(c) Motion estimation module which predicts  the next location of a logo using LSTM
The dotted red boxes are the logo location in the previous frame, and the solid red boxes are  the location predicted
The yellow arrows show the translation of centers
(d) Bounding box regression is utilized to adjust that of each  predicted logo
(e) The process of propagation
 framework for logo detection in sports videos is different  from the conventional approaches
As shown in Figure N,  two phases, mutual-enhancement-based logo proposal reranking in frames and motion-estimation-based propagation  in video clips, are introduced in our framework
The reranking method analyzes the characteristic of homogeneous  logos within the frame to improve the accuracy of logo  proposals, and the motion-estimation-based propagation enhances the detection in contiguous frames
The workflow of  the method is described briefly as follows:  At the beginning of a shot, we detect logos in the first  several frames with Fast-RCNN and mutual-enhancementbased re-ranking
Then we collect the detection results and  utilize them to initialize the relay potency for each detected  potential logo region
With the locations and the information of potential logos in previous frames obtained, we  perform motion-estimation-based propagation to predict the  new positions of potential logos in the current frame
A  similarity verification step is then performed to select those  qualified prediction results as the detection results of the  current frame, and the relay potencies are updated for these  results
When the relay potency of a potential logo decreases to 0, the propagation of this logo will be discontinued
During this process, we keep comparing the difference between the previous frame and the current one
If  the magnitude of change exceeds a relatively conservative  threshold, the current propagation will be canceled, and the  detection with Fast-RCNN will be restarted from the current  frame, just as that for the start of a shot
 In this study, we adopt the Fast-RCNN-based object detection model introduced by Ross Girshick [N] to generate  logo candidates
First, a region proposal algorithm [NN]  is used to select category-independent region proposals for  further classification
Then feature extraction is performed  using a CNN for each proposed region
These regions are  then classified using a softmax classifier with fully connected layers
After classification, each region is assigned  a confidence score
However, a fixed confidence threshold cannot accommodate the diversity of visual quality in  a video
To mutually enhance all possible proposals, we  screen the proposed logos with a re-ranking method by analyzing the homogeneous logos occurring simultaneously,  including the type repetition of appearance, color consistency and mutual exclusion (more details are provided in  Section N)
 When the location of a logo is obtained in one frame,  the corresponding location of its repetition in the next  frame can be predicted utilizing the continuity between adjacent frames [NN, NN, NN, NN]
In this paper, the motionestimation-based propagation enhances the detection in  contiguous frames, which is described in Section N
 N
Mutual Enhancement for Proposals  Fast-RCNN has shown excellent performance in  general-purposed object detection
However, it suffers from  low recall in logo detection [N, NN, N0]
One of the primary  reasons is the camera panning, which blurs logo regions and  leads to low confidences
In this section, we present a filtering phase that re-ranks the proposals according to their  NNNNNNNN    local context, including the type repetition of appearance,  color consistency and mutual exclusion
 In a frame set K generated from a video clip, for the  i-th frame image Fi ∈ K, we utilize the Fast-RCNN-based object detection model to generate logo proposals in it, and  then bag proposals as a candidate set Pi,  Pi = {P N i ,P  N i , 


,P  j i , 


,P  ni i }  where P j i is the j-th logo proposal of frame Fi, and ni is the  total number of proposals in frame Fi
The corresponding  confidence of proposal P j i is defined as Φ  j i , and the category of proposal P j i is C  j i ∈ {N, N, 


,m}, where m is the  number of categories
 To improve the accuracy of proposal, we notice that the  confidence Φ j i could be adjusted on the basis of context  analysis
First, it is common in sport videos that several  instances of the same logo appear simultaneously but with  different visual quality, and the low confidence of a proposal can be promoted when more proposals of the same  type have been detected
We denote this factor as ρ j i , which  will be explained in Subsection N.N
Second, different from  randomly-appearing objects, such as pedestrians and vehicles, the logos in sports videos seldom overlap each other
 We adopt an aggressive strategy to punish the overlapped  proposals in Subsection N.N, and define this factor as τ j i 
 As a consequence, the adjustment of Φ j i is estimated based  on the homogeneous promotion and inhomogeneous exclusion, as follows:  Φ̂ j i = Φ  j i +W · [ρ  j i , τ  j i ]  T (N)  where W = [wN, wN] are the weight parameters, and Φ̂ j i is  the updated value of confidence
 N.N
Proposal Promotion with Homogeneous Sib- lings  As shown in Figure N, it is common in sport videos that  several instances of the same logo appear simultaneously,  but their visual qualities may vary due to their individual  perspective or distance to the camera
In a general FastRCNN framework, the decision about whether a region contains an object depends on one or more threshold
In our  framework, a sibling-similarity-based strategy is proposed  to improve the proposal selection
The proposals with high  confidence are firstly chosen as delegates
And then more  proposals in the same category of the delegate can obtain eligibility based on their co-appearance with a delegate
Such  promotion also takes into account other characteristic similarities among the candidate regions in addition to the class  information
Last but not the least, the layout information  of the qualified proposals can reversely enhance all the proposals including the delegates according to the regularity  degree of their spatial arrangement
 We denote the delegates as the most salient logo proposals among a group of logo proposals that appear simultaneously in a frame
They are selected with a relatively high  confidence
This step is implemented by introducing a sign  indicator S j i for each proposal P  j i with the following formulas:  S j i =  { N, Φji ≥ δh  0, otherwise (N)  δh = θ + ε  where, θ is the experimental threshold in conventional network, and the proposals with S j i = N are chosen as delegates
ε is a two-way correction for the threshold adjustment, which is discussed in Section N.N
 For each category of proposals in the current frame, we  choose the maximum confidence among the delegates in the  category as a recommendation factor Φ̃:  Φ̃ j i = max{Φ  k i |(S  k i > 0) ∧ (C  k i = C  j i )}  k ∈ {N, N, 


, ni} (N)  We utilize the delegates and their category information to  promote the detection of weak proposals and to remove unrelated proposals
First of all, a dynamic threshold is defined as:  δ l,C  j i = θ − ε× exp(Φ̃ji − N) (N)  The relatively low threshold δ l,C  j i  of the category C j i enables the maximum inclusion of the logo proposals of the  same category
The adjustment ε × exp(Φ̃ji − N) of the threshold depends on the confidence of the most significant  delegate
 In many situations, the logos that could not be detected,  even when other homogeneous logos in the same frame are  obtained, are occluded by players or become blurred due to  camera movement
To enhance the identification of these  weak candidates, other characteristic similarities between  the available regions of those and the delegates should be  taken into account
In this paper, we adopt the color consistency to implement further enhancement, because it remains relatively stable in the cases of a slightly occluded  logo and a blurred logo
Moreover, many instances in the  same logo category can have different layouts, but they still  have a relatively high consistency of color
For each class  of logos, we smooth each training sample by Rolling Guidance Filter [NN] to extract dominant color information, calculate the color histogram of the smoothed training sample, and compute the average histogram as the class template ColorHistTemplateClass
For each logo proposal,  we compare its color histogram with its corresponding class  template
We denote the color consistency λ j i as  λ j i =  ∑ I (HN − H̄N)(HN − H̄N)√∑  I (HN − H̄N)N  ∑ I (HN − H̄N)N  (N)  where HN is the color histogram of P j i , HN is the color histogram template of C j i , and  H̄k = N  N  ∑ j Hk(j)  NNNNNNNN    N is the number of bins in color histogram, and λ j i is  the correlation similarity of color histograms between a  logo proposal P j i and the corresponding class template  ColorHistTemplate C  j  i 
Combining the two factors, sibling  co-occurrence and color consistency, we can update the criteria for promotion of logo proposals as:  ρ̃ j i = S  j i × exp(  Φ j i − δl,C j  i  Φ̃ j i  )− α× λji (N)  At last, a largely ignored characteristic when multiple logos  appear in the same frame, i.e., the regularity of their spatial arrangement, can be utilized to verify or enhance all the  proposals
In a typical sports video, the relative positions  among the logos occurring simultaneously are not random,  such as aligned in one direction
In this paper, we extract  the local alignment information between two adjacent proposals to adjust their confidence
Suppose Pki is the nearest  neighbor of the current proposal P j i within a predefined radius of neighborhood, the alinement σ between them can be  estimated with the following formula
 σ = min(|△x|, |△y|)  max(dN, dN) (N)  where, △x and △y are the coordinate differences between the two proposal centers
dN and dN are the diagonals of P ji and Pki , respectively
If σ is small enough, the two proposals can be considered aligned in one direction
The adjustment for the current proposal can be expressed as:  ρ j i =  { ρ̃ j i × exp(N− σ), σ < ξ  ρ̃ j i , otherwise  (N)  where ξ is a threshold for judging whether an alignment exists
The spatial-relationship-based enhancement is mutual  and can apply to all the proposals, including improving the  delegates with the weak proposals
 N.N
Exclusion with Inhomogeneous Siblings  Unlike typical objects in videos, such as pedestrians and  vehicles, logo instances in sports videos rarely overlap each  other
The observation motivates us to punish inhomogeneous overlapped proposals
In our approach, when two or  more proposals are overlapped, the proposal with the highest confidence is selected as the dominate proposal
Simultaneously, the other overlapped proposals in the same region are severely punished
The penalty of an individual  proposal is commensurate with the degree of overlapping  according to the following formulas
 τ j i =  { 0 if C ji = C  k i  N− eOReg(P j  i ,Pki ) otherwise  (N)  where OReg(P ji ,P k i ) =  P j i ∩ P  k i  P j i ∪ P  k i  OReg(P ji ,P k i ) indicates the overlap degree between the  logo proposal P j i and the dominant proposal P  k i 
As overlap increases, the penalty increases rapidly
 N
Mutual Enhancement via Frame Propagation  It is more efficient to predict the logo positions on the  basis of temporal correlation than to detect them on every  frame individually, since most logos in the same shot may  have small variation between adjacent frames
Meanwhile,  the detection in fuzzy frames could be assisted by their adjacent frames
Several works have been proposed to dig and  utilize the relationships between frames [N, N, NN, NN]
In  our framework, we first simply use long short-term memory  (LSTM) network to predict the location in coming frames,  then take advantages of the time continuity to enhance the  logo detection with a propagation mechanism
 N.N
Motion-Estimation-based Prediction  Motion estimation is one of the widely used methods  for object detection and tracking in the field of video retrieval and surveillance [NN]
When we detect and collect  the logo proposals in previous frames, we also record the  center {x ji , y j i } of each proposal P  j i 
Let the center sequence of P j proposal be:  Xj = {xjN, x j N, 


, x  j i}, Y  j = {yjN, y j N, 


, y  j i }  We train a LSTM on the set of center sequences for predicting the center {xji+N, y j i+N} of P  j i+N
Then, a region which  centers at {xji+N, y j i+N} and has the same size as P  j i is regarded as P j i+N
 To verify the reliability of the propagated P j i+N, we calculate the similarity between P j i and P  j i+N based on the  mean absolute deviation (MAD) cost
Suppose B is one  of the proposal blocks and B ′ is the corresponding block in  the next frame
The MAD value between them is calculated  as:  MAD(B ,B ′) = N  w × h  w∑  x=N  h∑  y=N  |B(x, y)− B ′(x, y)|  (N0)  where w and h are the width and height of the block region,  respectively, B(x, y) and B ′(x, y) are the pixels in the cur- rent block and the referenced block, respectively
 The distance between P j i and estimated P  j i+N can also be  calculated by MAD 
 D j i = MAD(P  j i ,P  j i+N) (NN)  and the similarity of two adjacent frames is defined as:  Ψ(Fi, Fi+N) = N  ni  ni∑  j=N  (N−Dji ) (NN)  where ni is the total number of proposals in frame Fi
 NNNNNNN0    At the end of each frame propagation, a bounding box  regression is adopted to adjust P j i+N
 N.N
Control Mechanism for Propagation  Before a frame is processed, a control mechanism is  adopted to make sure a right decision is made to adopt either  the propagation or the detection with Fast-RCNN
 First, different than traditional methods that firstly divide a video into shots, our method automatically decides  whether to perform detection by Fast-RCNN or by propagation based on the frame continuity
Once the propagation starts, the proposed method keeps on comparing the  difference between the current frame and the next one
The  frame similarity Ψ(Fi, Fi+N) is compared with a relatively high threshold θs
When Ψ(Fi, Fi+N) > θs, the two adja- cent frames are considered to be in the same shot and the  propagation can continue
If Ψ(Fi, Fi+N) ≤ θs, the prop- agation will be discontinued, and the detection with FastRCNN will be restarted from the next frame
The significant camera changes, such as shot switch, fast panning and  zooming, may lead to a low Ψ(Fi, Fi+N) and a Fast-RCNN will be adopted to avoid problematic propagation
 Moreover, a group of relay potency, Ri, is introduced to  describe the propagation ability of proposals in Pi
 Ri = {R N i ,R  N i , 


,R  j i , 


,R  ni i }  where, i denotes the i-th frame, j denotes the j-th proposal  in Fi
R j i is initialized according to Φ  j i when the propagation procedure starts or re-starts
During the propagation,  the relay potency decreases gradually
On each iteration of  the propagation, R j i is transferred to the new proposal in the  next frame, but part of its energy is lost according to the dissimilarity between the corresponding proposals in adjacent  frames
 R j i+N = R  j i −D  j i  (NN)  When the relay potency drops to zero, the corresponding  proposal also has to withdraw from propagation
When all  the proposals run out of potency, the propagation will terminate and the detection with Fast-RCNN will restart
Obviously, significant camera changes lead to fast attenuation of  relay potency, which also avoids problematic propagation
 N
Experiments  In our experiments, we firstly evaluate the proposed  method in commonly used logo recognition image datasets,  and then evaluate the logo detection method in a sports  video dataset
 N.N
Dataset  We evaluate the proposed re-ranking method in two  challenging and commonly used logo recognition datasets,  FlickrLogos-NN[NN] and FlickrLogos-NN[NN], both of which  Figure N
Some examples of detection results in the proposed logo  image dataset
Logos in the green boxes are the detected results,  and the detected categories are tagged around the green box
 Figure N
(a): The comparison of different ε in three image  datasets
(b): The comparison of different γ in the video dataset
 are specifically designed for real-world logo recognition
 FlickrLogos-NN is a collection of photos showing NN different logo brands, which contains N,NN0 images
FlickrLogosNN contains more than four thousand images
They are established for the evaluation of logo retrieval and multi-class  logo detection/recognition systems
 Since there is still no public video logo dataset, we collected a set of tennis videos, annotating each frame in the  training set, and established a dataset for the detection evaluation
The dataset contains N0 different tennis video clips  (more than twenty five thousand frames) including various  camera motions such as switch of camera views, panning,  zooming, and rotating, and the resulted object blurring and  occlusion
The logos appear both in the background and  on players’ and staff’s clothes
We annotated the positions of the top N0 high frequency categories of logos in  videos
Meanwhile, we collect a corresponding logo recognition image dataset
Figure N shows some examples of  logos in this image dataset
The new dataset contains the  same N0 logo categories and about N00 images for each category
We also labeled the position of each logo in the image  dataset
To evaluate the proposed approach, we randomly  select N0% of data for training, and N0% for testing, respec- tively
 NNNNNNNN    Logo DPM DTC FRCN FasterRCNN Proposed  adidas N.NN NN.N NN.N N0.N NN.NN  aldi NN.N NN.N NN.N NN.N NN.N  apple N0.0 N.NN NN.N NN.N NN.NN  becks N/A NN.N NN.N NN.N NN.NN  bmw N/A NN.N N0.0 NN.N NN.0N  carlsberg N.N NN.N NN.N NN.N NN.NN  chimay N.N N0.0 NN.N NN.N NN.NN  cocacola NN.N NN.N NN.0 NN.N NN.NN  corona NN.N N00.0 NN.N NN.0 NN.NN  dhl 0.0N NN.N NN.N NN.N NN.NN  erdinger NN.N NN.0 N0.N NN.N NN.NN  esso NN.N NN.N NN.N N0.N NN.NN  fedex N.NN NN.N NN.N NN.0 NN.NN  ferrari NN.N NN.N N0.0 NN.N NN.NN  ford NN.N NN.0 NN.N NN.N NN.NN  fosters NN.N N0.0 NN.N NN.N NN.NN  google NN.N NN.N NN.N NN.0 NN.NN  guiness NN.N NN.N NN.N N0.N NN.NN  heineken N.NN NN.N NN.N NN.N NN.NN  HP N/A N0.N N/A NN.N NN.N0  milka 0.NN NN.N NN.N NN.N NN.NN  nvidia N.NN NN.N N0.N NN.0 NN.NN  paulaner NN.0 NN.N NN.N NN.0 N00.00  pepsi N.NN N/A NN.N NN.0 NN.NN  rittersport N.N NN.N NN.0 NN.0 NN.NN  shell NN.N NN.N NN.N NN.0 NN.NN  singha N0.0 NN.N NN.N NN.N NN.NN  starbucks NN.N NN.N NN.N NN.0 NN.N0  stellaartois NN.N NN.N NN.N N0.N N0.00  texaco NN.N NN.N NN.N NN.N NN.NN  tsingtao NN.N NN.N NN.N N0.N NN.NN  ups NN.N NN.N NN.N N0.N NN.N0  MAP NN.N NN.N NN.N NN.N NN.NN  Table N
Comparison of MAPs(%) in each class and average MAPs(%)
 N.N
Parameter Setting  The most commonly used experimental threshold θ in  conventional network is 0.N [N, N0, NN]
After testing the  effect of different ε in three training data, we select 0.NN for ε as an empirical value
The parameters wN and wN in  Eq
N are assigned 0.N and 0.N, respectively
The parameter  ξ in Eq
N is assigned 0.NN
The comparison results are  shown in Figure N(a)
For the initial frame set size γ for  Fast-RCNN, we test the FN values with different γ in video  dataset
Experiments show that the detection performance  converges when γ reaches N0, as shown in Figure N(b)
 N.N
Evaluation in Still Frames  We pre-train a basic VGGNN network with the ImageNet  dataset, fine-tune and test the model in the three datasets  separately
Training data is augmented by rotating, flipMethod Precision Recall FN  FRCN[N] 0.N 0.NN 0.NN  FRCN[N]+ρ (without λ) 0.NN 0.NN 0.NN  FRCN[N]+ρ (with λ) 0.NN 0.NN 0.NN  Proposed 0.NN 0.NN 0.NN  Table N
Comparison of results in FlickrLogos-NN
 DataSet Method Precision Recall FN  FlickrLogos-NN FRCN[N] 0.N0N 0.NNN 0.NNN  Proposed 0.NNN 0.NN0 0.NNN  FlickrLogos-NN FRCN[N] 0.NNN 0.NNN 0.NNN  Proposed 0.NNN 0.NNN 0.NNN  Our own  logo Dataset  FRCN[N] 0.NNN 0.NNN 0.NNN  Proposed 0.NNN 0.NNN 0.NN0  Table N
Comparison of results in the three logo dataset
 ping, blurring and sharpening
Because of the wide use  of FlickrLogos-NN in logo detection, we compare the reranking method on this dataset with several other logo detection methods, and then show the comparison of the proposed method with the baseline[N] in all the three datasets
 For FlickrLogos-NN, we compare the MAP values with  DPM [N] which detects logos based on mixtures of multiscale deformable part models, DTC [NN] which uses improved topological constraint for SIFT features, FRCN [N]  and Faster-RCNN [NN] which use conventional RCNN to  detect logos
Table N shows the comparison results with  different detection algorithms
The proposed method obviously has a higher average MAP than the other methods,  and gains the highest MAPs in NN categories (out of NN)
 Table N shows the effectiveness of each re-ranking factor
The factor ρ is effective in reducing the false negatives  caused by blurred logos but introduces more false positives
 The use of color consistency and inhomogeneous exclusion  on top of ρ obviously boosts up the overall precision without undermining the recall
 The FlickrLogos-NN dataset is labeled mainly for classification without location information
So we annotate  the data of FlickrLogos-NN manually
Since there are very  few repetitions in FlickrLogos-NN, the benefit of mutualenhancement is less significant
Results in Table N show  that the proposed method outperforms conventional FastRCNN in FlickrLogos-NN and our own logo dataset
Figure N illustrates some examples of detection results in our  own dataset
 N.N
Evaluation in Videos  Figure N shows some comparison in the video dataset
 When the similarity between adjacent frames obtained by  Eq
NN is less than the threshold, the logo candidates will be  propagated
Otherwise, Fast-RCNN detection will restart to  generate new logo candidates
 NNNNNNNN    (a) Detection results in a frame sequence with blur and camera zooming  (b) Detection results in a frame sequence with occlusion and camera’s translation  (c) Detection results in a frame sequence with camera’s rotation  Figure N
Detection results in video sequences with various camera changes
 Method Precision Recall FN  FRCN[N] 0.NNN 0.NNN 0.NNN  FRCN[N]+re-ranking 0.NNN 0.NNN 0.N0N  FRCN[N]+propogation 0.NNN 0.NNN 0.NNN  Proposed 0.NNN 0.NNN 0.NNN  Table N
Comparison of results in the video dataset
 We compare the propagation method with FastRCNN [N]
The values of precision, recall, and FN are listed  in Table N
The re-ranking method gains a higher recall,  and propagation further improves the precision
Overall, the  proposed method has a higher FN
It shows that the characteristics of homogenous logos are indeed powerful features  of sports video
 Our experiments on football and basketball videos show  that the proposed method also outperforms the existing  methods
However, none of the detection results on these  videos are as good as the result on tennis videos
This is  because that there are more challenging problems, such as  small logos in long shots and the incompleteness due to frequent occlusions
 To measure the time efficiency of our method, we evaluate various stages of the proposed method in the proposed  video dataset which contains NN,NNN frames, and achieve  a performance about 0.NNs/frame
We obtain the most potential regions by propagation rather than Fast-RCNN detection in similar contiguous frames, which greatly reduces  the region proposing and detection time
Meanwhile, frame  propagation finds more true positive regions and discards  more false positive regions for classification
For the fuzzy  frames, frame propagation utilizes the location of the object  in contiguous frames to predict the location in the current  frame, which helps obtain more true positive regions compared with those methods that consider only still frames
 Besides, the relay potency of false positive regions decreases rapidly, which leads to the corresponding regions to  be removed immediately and further reduces the processing  time
 N
Conclusion  In this study, we propose an integrative, effective, and  efficient framework for logo detection in sports videos
The  framework efficiently combines Fast-RCNN-based object  detection with frame propagation
Based on the analysis of  the characteristics of logos, proposal re-ranking, prediction,  and verification schemes are extensively investigated and  evaluated
The proposed approach shows consistent performance improvement over existing logo detections methods  for sports videos
However, several challenges still remain  to be addressed in the future
For instance, the extraction  and utilization of texts in logos, rich variations in logo layout, and more tracking information could be considered
 N
Acknowledgments  This work is supported by National Natural Science  Foundation of China under Grant NNNNN0NN
 NNNNNNNN    References  [N] S
Bianco, M
Buzzelli, D
Mazzini, and R
Schettini
Logo  recognition using cnn features
In International Conference  on Image Analysis and Processing, pages NNN–NNN
Springer,  N0NN
 [N] R
Boia and C
Florea
Homographic class template for  logo localization and recognition
In Iberian Conference  on Pattern Recognition and Image Analysis, pages NNN–NNN
 Springer, N0NN
 [N] T
Chattopadhyay and A
Sinha
Recognition of trademarks  from sports videos for channel hyperlinking in consumer  end
In N00N IEEE NNth International Symposium on Consumer Electronics, pages NNN–NNN
IEEE, N00N
 [N] N
Dalal and B
Triggs
Histograms of oriented gradients for human detection
In N00N IEEE Computer Society Conference on Computer Vision and Pattern Recognition  (CVPR’0N), volume N, pages NNN–NNN
IEEE, N00N
 [N] M
Danelljan, G
Hager, F
Shahbaz Khan, and M
Felsberg
Convolutional features for correlation filter based visual tracking
In Proceedings of the IEEE International Conference on Computer Vision Workshops, pages NN–NN, N0NN
 [N] M
Danelljan, A
Robinson, F
S
Khan, and M
Felsberg
 Beyond correlation filters: Learning continuous convolution  operators for visual tracking
In European Conference on  Computer Vision, pages NNN–NNN
Springer, N0NN
 [N] R
J
den Hollander and A
Hanjalic
Logo recognition in  video stills by string matching
In Image Processing, N00N
 ICIP N00N
Proceedings
N00N International Conference on,  volume N, pages III–NNN
IEEE, N00N
 [N] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and D
Ramanan
Object detection with discriminatively trained partbased models
IEEE transactions on pattern analysis and  machine intelligence, NN(N):NNNN–NNNN, N0N0
 [N] R
Girshick
Fast r-cnn
In Proceedings of the IEEE International Conference on Computer Vision, pages NNN0–NNNN,  N0NN
 [N0] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In Proceedings of the IEEE conference on  computer vision and pattern recognition, pages NN0–NNN,  N0NN
 [NN] J
F
Henriques, R
Caseiro, P
Martins, and J
Batista
Highspeed tracking with kernelized correlation filters
IEEE  Transactions on Pattern Analysis and Machine Intelligence,  NN(N):NNN–NNN, N0NN
 [NN] S
C
Hoi, X
Wu, H
Liu, Y
Wu, H
Wang, H
Xue, and  Q
Wu
Logo-net: Large-scale deep logo detection and brand  recognition with deep region-based convolutional networks
 arXiv preprint arXiv:NNNN.0NNNN, N0NN
 [NN] R
Jain and D
S
Doermann
Logo retrieval in document images
In Document Analysis Systems, pages NNN–NNN, N0NN
 [NN] Y
Kalantidis, L
G
Pueyo, M
Trevisiol, R
van Zwol, and  Y
Avrithis
Scalable triangulation-based logo recognition
 In Proceedings of the Nst ACM International Conference on  Multimedia Retrieval, page N0
ACM, N0NN
 [NN] B
Kovar and A
Hanjalic
Logo detection and classification in a sport video: video indexing for sponsorship revenue  control
In Electronic Imaging N00N, pages NNN–NNN
International Society for Optics and Photonics, N00N
 [NN] L
Li, X
Wang, W
Zhang, G
Yang, and G
Hu
Detecting removed object from video with stationary background
 In International Conference on Digital Forensics and Watermaking, N0NN
 [NN] D
G
Lowe
Distinctive image features from scaleinvariant keypoints
International journal of computer vision, N0(N):NN–NN0, N00N
 [NN] H
Nam and B
Han
Learning multi-domain convolutional  neural networks for visual tracking
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NN0N, N0NN
 [NN] H
Nisar, A
S
Malik, and T
S
Choi
Content adaptive  fast motion estimation based on spatio-temporal homogeneity analysis and motion classification
Pattern Recognition  Letters, NN(N):NN–NN, N0NN
 [N0] G
Oliveira, X
Frazão, A
Pimentel, and B
Ribeiro
Automatic graphic logo detection via fast region-based convolutional networks
arXiv preprint arXiv:NN0N.0N0NN, N0NN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  Advances in neural information processing systems, pages  NN–NN, N0NN
 [NN] J
Revaud, M
Douze, and C
Schmid
Correlation-based  burstiness for logo retrieval
In Proceedings of the N0th  ACM international conference on Multimedia, pages NNN–  NNN
ACM, N0NN
 [NN] S
Romberg and R
Lienhart
Bundle min-hashing for logo  recognition
In Proceedings of the Nrd ACM conference  on International conference on multimedia retrieval, pages  NNN–NN0
ACM, N0NN
 [NN] S
Romberg, L
G
Pueyo, R
Lienhart, and R
Van Zwol
 Scalable logo recognition in real-world images
In Proceedings of the Nst ACM International Conference on Multimedia  Retrieval, page NN
ACM, N0NN
 [NN] A
Saha, J
Mukherjee, and S
Sural
A neighborhood elimination approach for block matching in motion estimation
 Signal Processing Image Communication, NN(NCN):NNN–  NNN, N0NN
 [NN] H
Sahbi, L
Ballan, G
Serra, and A
Del Bimbo
Contextdependent logo matching and recognition
IEEE Transactions on Image Processing, NN(N):N0NN–N0NN, N0NN
 [NN] P
Shenolikar and S
Narote
Different approaches for motion estimation
In Control, Automation, Communication and  Energy Conservation, N00N
INCACEC N00N
N00N International Conference on, pages N–N
IEEE, N00N
 [NN] P
Tang and Y
Peng
Logo recognition via improved topological constraint
In International Conference on Multimedia  Modeling, pages NN0–NNN
Springer, N0NN
 [NN] J
R
R
Uijlings, K
E
A
V
D
Sande, T
Gevers, and  A
W
M
Smeulders
Selective search for object recognition
 International Journal of Computer Vision, N0N(N):NNN–NNN,  N0NN
 [N0] C
Wan, Z
Zhao, X
Guo, and A
Cai
Tree-based shape descriptor for scalable logo detection
In Visual Communications and Image Processing (VCIP), N0NN, pages N–N
IEEE,  N0NN
 NNNNNNNN    [NN] J
Wang, Q
Liu, L
Duan, H
Lu, and C
Xu
Automatic tv  logo detection, tracking and removal in broadcast video
In  Advances in Multimedia Modeling, International Multimedia Modeling Conference, MMM N00N, Singapore, January  N-NN, N00N
Proceedings, pages NN–NN, N00N
 [NN] N
Wang and D.-Y
Yeung
Learning a deep compact image representation for visual tracking
In Advances in neural  information processing systems, pages N0N–NNN, N0NN
 [NN] X
Wu and K
Kashino
Image retrieval based on spatial context with relaxed gabriel graph pyramid
In N0NN IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages NNNN–NNNN
IEEE, N0NN
 [NN] W
Q
Yan, J
Wang, and M
S
Kankanhalli
Automatic video  logo detection and removal
Multimedia Systems, N0(N):NNN–  NNN, N00N
 [NN] B
Zhang, L
Wang, Z
Wang, Y
Qiao, and H
Wang
Realtime action recognition with enhanced motion vector cnns
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
 [NN] Q
Zhang, X
Shen, L
Xu, and J
Jia
Rolling guidance filter
 In European Conference on Computer Vision, pages NNN–  NN0
Springer, N0NN
 NNN0NNNNGeneralized Orderless Pooling Performs Implicit Salient Matching   Generalized orderless pooling performs implicit salient matching  Marcel SimonN, Yang GaoN, Trevor DarrellN, Joachim DenzlerN, Erik RodnerN  N Computer Vision Group, University of Jena, Germany N EECS, UC Berkeley, USA N Corporate Research and Technology, Carl Zeiss AG  {marcel.simon, joachim.denzler}@uni-jena.de {yg, trevor}@eecs.berkeley.edu  Abstract  Most recent CNN architectures use average pooling as  a final feature encoding step
In the field of fine-grained  recognition, however, recent global representations like bilinear pooling offer improved performance
In this paper,  we generalize average and bilinear pooling to “α-pooling”,  allowing for learning the pooling strategy during training
 In addition, we present a novel way to visualize decisions  made by these approaches
We identify parts of training  images having the highest influence on the prediction of  a given test image
This allows for justifying decisions  to users and also for analyzing the influence of semantic  parts
For example, we can show that the higher capacity  VGGNN model focuses much more on the bird’s head than,  e.g., the lower-capacity VGG-M model when recognizing  fine-grained bird categories
Both contributions allow us to  analyze the difference when moving between average and  bilinear pooling
In addition, experiments show that our  generalized approach can outperform both across a variety  of standard datasets
 N
Introduction  Deep architectures are characterized by interleaved convolution layers to compute intermediate features and pooling layers to aggregate information
Inspired by recent results in fine-grained recognition [NN, N0] showing that certain pooling strategies offer equivalent performance as classic models involving explicit correspondence, we investigate here a new pooling layer generalization for deep neural networks suitable for both fine-grained and more generic  recognition tasks
 Fine-grained recognition developed from a niche research field into a popular topic with numerous applications,  ranging from automated monitoring of animal species [N]  to fine-grained recognition of cloth types [N]
The defining property of fine-grained recognition is that all possible object categories share a similar object structure and  hence similar object parts
Since the objects do not sigFigure N
We present the novel pooling strategy α-pooling, which  replaces the final average pooling or bilinear pooling layer in  CNNs
It allows for a smooth combination of average and bilinear  pooling techniques
The optimal pooling strategy can be learned  during training to adapt to the properties of the task
In addition,  we present a novel way to visualize predictions of α-pooling-based  classification decisions
It allows in particular for analyzing incorrect classification decisions, which is an important addition to all  widely used orderless pooling strategies
 nificantly differ in the overall shape, subtle differences in  the appearance of an object part can likely make the difference between two classes
For example, one of the most  popular fine-grained tasks is bird species recognition
All  birds have the basic body structure with beak, head, throat,  belly, wings as well as tail parts, and two species might difNNNN0    fer only in the presence or absence of a yellow shade around  the eyes
 Most approaches in the past five years concentrated on  exploiting this extra knowledge about the shared general  object structure
Usually, the objects are described by the  appearance of different parts
This explicit modeling of the  appearance of object parts is intuitive and natural
While  explicit part modeling greatly outperformed off-the-shelf  CNNs, the recently presented second-order or bilinear pooling [NN] gives a similar performance boost at the expense of  an understandable model
 Our paper presents a novel approach which has both  state-of-the-art performance and allows for clear justification of the classification decision using visualization of influential training image regions
Classification accuracy on  this task is reaching human level performance and hence we  additionally focus on making classification decisions more  understandable and explainable
We present an approach  which can show for each evaluated image why the decision  was made by referring to the most influential training image  regions
Average pooling is mainly used in generic recognition
while bilinear pooling has its largest benefits in finegrained recognition: our approach allows for understanding  and generalizing the relationship between the two – a crucial step for further research
 Our first contribution is a novel generalization and parametric representation of the commonly used average and  bilinear pooling
This representation allows for a smooth  combination of these first-order and second-order operators
The framework provides both a novel conceptual understanding of the relationship of the methods and offers a  new operating point with consistent improvement in terms  of accuracy
 The second contribution is an analysis of the learned optimal pooling strategy during training
Our parametrized  pooling scheme is differentiable and hence can be integrated  into an end-to-end-learnable pipeline
We show that the  learned pooling scheme is related to the classification task  it is trained on
In addition, a pooling scheme half-way  between average and bilinear pooling seems to achieve the  highest accuracy on several benchmark datasets
 Our third contribution is a novel way to visually justify  a classification decision of a specific image to a user
It is  complementary to our novel pooling scheme and hence also  applicable to the previous pooling schemes average and bilinear pooling
Both classifier parameters and local feature  matches are considered to identify training image regions of  highest influence
 Finally, our fourth contribution is an approach for quantifying the influence of semantic parts in a classification  decision
In contrast to previous work, we consider both  the classifier parameters and the saliency
We show that  the CNN’s way of classifying objects increasingly diverges  from the human way, i.e
CNNs base most of their decisions  on one object part instead of using a broad set of object attributes
In particular, we show that more complex CNN  models like VGGNN focus much more on the bird’s head  compared to less complex ones like VGG-M
We also show  that a similar shift can be seen when moving from average  pooled features to bilinear features encoding
 After reviewing related work in the following section,  Sect
N will present our novel α-pooling formulation, which  generalizes average and bilinear pooling into a single framework
Sect
N will then investigate the relationship between  generalized orderless pooling and pairwise matching and  present an approach for visualizing a classification decision
 This is followed by the experiments and a discussion about  the trade-offs between implicit and explicit pose normalization for fine-grained recognition in Sect
N and N
 N
Related work  Our work is related to several topics in the area of computer vision
This includes pooling techniques, match kernels, bilinear encoding, and visualizations for CNNs
 Pooling techniques and match kernels The presented  α-pooling is related to other pooling techniques, which aggregate a set of local features into a single feature vector
Besides the commonly used average pooling, fullyconnected layers, and maximum pooling, several new approaches have been developed in the last years
Zeiler et  al
[NN] randomly pick in each channel an element according to a multinomial distribution, which is defined by the activations themselves
Motivated by their success with handcrafted features, Fisher vector [NN, NN] and VLAD encoding [NN] applied on top of the last convolutional layer have  been evaluated as well
The idea of spatial pyramids was  used by He et al
[NN] in order to improve recognition performance
In contrast to these techniques, feature encoding  based on α-pooling shows a significantly higher accuracy  in fine-grained applications
Lin et al
[NN, NN] presents  bilinear pooling, which is a special case of α-pooling
It  has its largest benefits in fine-grained tasks
As shown in  the experiments, learning the right mix of average and bilinear pooling improves results especially in tasks besides  fine-grained
 The relationship of average pooling and pairwise matching of local features was presented by Bo et al
[N] as an  efficient encoding for matching a set of local features
This  formulation was also briefly discussed in [N0] and used for  deriving an explicit feature transformation which approximates bilinear pooling
Bilinear encoding was first mentioned by Tenenbaum et al
[NN] and used, for example, by  Carreira et al
[N] and Lin et al
[NN] for image recognition  tasks
Furthermore, the recent work of Murray et al
[NN]  also analyzes orderless pooling approaches and proposes a  technique to normalize the contribution of each local deNNNNN    scriptor to resulting kernel values
In contrast, we show  how the individual contributions can be used either for visualizing the classification decisions and for understanding  the differences between generic and fine-grained tasks
 Justifying classifier predictions for an image Especially Sect
N is related to visualization techniques for information processing in CNNs
Most of the previous works focused on the primal view of the feature representation
This  means they analyze the feature representations by looking  only at a single image
Zeiler et al
[NN] identify image patterns which cause high activations of selected channels of  a convolutional layer
Yosinksi et al
[NN] try to generate  input patterns which lead to a maximal activation of certain  units
Bach et al
[N] visualize areas important to the classification decision with layer-wise relevance propagation
In  contrast to the majority of these works, we focus on the dual  (or kernel) view of image classification
While a visualization for a single image looks interesting at the first sight, it  does not allow for understanding which parts of an image  are compared with which parts of the training images
In  other words, these visualizations look only at the image itself and are omitting the relationship to the training data
 For example, while the bird’s head might be an attentive  region in the visualization techniques mentioned above, a  system might still compare this head with some unrelated  areas in other images
Our approach allows for a clearer  understanding about which pairs of training and test image  regions contribute to a classification decision
 Zhang et al
[NN] tackle a related idea for the case of  explicit part detectors
They use the prediction score of a  SVM classifier for each part to identify the most important  patches for a selected part detector
We extend this idea to  orderless-pooled features which do not originate from explicit part detections
 N
From generic to fine-grained classification:  generalized α-pooling  Fine-grained applications like bird recognition and more  generic image classification tasks like ImageNet have traditionally been two related but clearly separate fields with  their own specialized approaches
While the general CNN  architecture is shared, its usage differs
In this work, we  focus on two state-of-the-art feature encoding: global average and bilinear pooling
While bilinear pooling shows the  largest benefits in fine-grained applications, average pooling is the most commonly chosen final pooling step in literally all state-of-the-art CNN architectures
In this section,  we show the connection between these two worlds
We  present the novel generalization α-pooling, which allows  for a continuous transition between average and bilinear  pooling
The right mixture is learned with back-propagation  from data in training, which allows for adapting to the specific tasks
In addition, the results will allow us to investigate which mixture of pooling approaches is best suited for  which application, and what makes fine-grained recognition  different from generic image classification
 Generalized α-pooling We propose a novel generalization of the common average and bilinear pooling as used in  deep networks, which we call α-pooling
Let (f, g, C) de- note a classification model
f : (I, i) N→ yi ∈ R  D denotes  a local feature descriptor mapping from input image I and location i to a vector with length D, which describes this  region
g : {yi | i = N, 


, n} N→ z ∈ R M is a pooling  scheme which aggregates n local features to a single global  image description of length M 
In our case, M = DN and is compressed using [N0]
Finally, C is a classifier
In a common CNN like VGGNN, f corresponds to the first part  of a CNN up to the last convolutional layer, g are two fully  connected layers and C is the final classifier
 An α-pooling-model is then defined by (f, galpha, C), where  galpha({yi} n  i=N) = v  (  N  n  n ∑  i=N  alpha-prod(yi, α)  )  (N)  and  alpha-prod(yi, α) = (sgn(yi) ◦ |yi| α−N)yTi , (N)  where v(·) is the vectorization function, and sgn(·), · ◦ ·, | · |, and ·α denote the element-wise signum, product, ab- solute value and exponentiation function, and α is a model  parameter
α has a significant influence on the pooling due  to its role as an exponent
The optimal value is learned with  back-propagation
For numerical stability, we add a small  constant ǫ > 0 to |yi| when calculating the power and when calculating the logarithm
In our experiments, learning α  was stable
 Special cases Average pooling is a common final feature  encoding step in most state-of-the-art CNN architectures  like ResNet [NN] or Inception [NN]
The combination [NN]  of CNN feature maps and bilinear pooling [NN, N] is one  of the current state-of-the-art approaches in the fine-grained  area
Both approaches are a special case of α-pooling
 For α = N and yi ≥ 0 we obtain alpha-prod(yi, N) = I ·yTi 
Hence g  alpha calculates a matrix in which each row is  the mean vector
This mean vector is identical to the one obtained in common average pooling
The vectorization v(·) turns the resulting matrix into a concatention of identical  mean vectors
 In case of α = N the mean outer product of yi is calculated, which is equivalent to bilinear pooling:  alpha-prod(yi, N) = yiy T i 
Therefore, α-pooling allows for  estimating the type of pooling necessary for a particular task  by learning α directly from data
 α-pooling can continuously shift between average and  bilinear pooling, which opens a great variety of opportunities
It shows a connection between both that was to the best  NNNNN    of our knowledge previously unknown
Furthermore, and  even more important, all following contributions are also  applicable to these two commonly used pooling techniques
 They allow for analyzing and understanding differences between both special cases
 N
Understanding decisions of α-pooling  In this section, we give a “deep” insight into the class  of α-pooled features, which includes average and bilinear  pooling as well as shown in the last section
We make use  of the formulation as pairwise matching of local features,  which allows for visualizing both the gist of the representation and resulting classification decisions
We use the  techniques presented in this section to analyze the effects  of α-pooled features as we move between generic and finegrained classification tasks
To simplify the notation, we  will focus in this section on the case that all local features  y are non-negative
This is the case for all features used in  the experiments
All observations apply to the generic case  in an analogous manner
 Interpreting decisions using most influential regions  While an impressive classification accuracy can be achieved  with orderless pooling, one of its main drawbacks is the  difficulty of interpreting classification decisions
This applies especially to fine-grained tasks, since the difference  between two categories might not be clear even for a human  expert
Furthermore, there is a need to analyze false automatic predictions, to understand error cases and advance  algorithms
 In this section, we use the formulation of α-pooling as  pairwise matching to visualize classification decisions
It is  based on finding locations with high influence on the decision
We show how to find the most relevant training image regions and show that even implicit part modeling approaches are well suited for visualizing decisions
 To show this, we calculate the linear kernel between the  vectors zk and z̃ℓ, which induces a kernel between Yk = {yi}  n i=N and Yℓ = {ỹj}  n j=N as follows:  〈zk, z̃ℓ〉 ∝ 〈v (  n ∑  i=N  y α−N i y  T i  )  , v (  n ∑  j=N  ỹ α−N j ỹ  T j  )  〉  = tr ((  n ∑  i=N  y α−N i y  T i  )T( n ∑  j=N  ỹ α−N j ỹ  T j  ))  = ∑  i,j  〈yi, ỹj〉 · 〈y α−N i , ỹ  α−N j 〉, (N)  where we ignored normalizing with respect to n for brevity
 Please note, that this derivation also reveals that the difference between bilinear and average pooling is only the  quadratic transformation of the scalar product between two  feature vectors yi and ỹj 
 If we use a single fully-connected layer after bilinear  pooling and a suitable loss, the resulting score for a single  class is given up to a constant as: N ∑  k=N  βk〈zk, z̃〉 = N ∑  k=N  ∑  i,j  βk · 〈yi,k, ỹj〉〈y α−N i,k , ỹ  α−N j 〉,  (N)  where βk are the weights of each training image given by  the dual representation of the last layer and N is the number  of training samples
zk is the α-pooled feature of the k-th  training image and calculated using the local features yi,k
 A match between a region j in the test example and  region i of a training example k is defined by the triplet  (k, i, j)
The influence of the triplet on the final score is given by the product  γk,i,j = βk · 〈yi,k, ỹj〉〈y α−N i,k , ỹ  α−N j 〉 
(N)  Therefore, we can visualize the regions with the highest influence on the classification decisions by showing the ones  with the highest corresponding γk,i,j 
This calculation can  be done efficiently also on large datasets with the main limitation being the memory for storing the feature maps
 Figure N depicts a classification visualization for test images from four different datasets
In the bottom left of each  block, the test image is shown
The test image is surrounded  by the five most relevant training image regions
They are  picked by first selecting the training images with the highest  influence defined by the aggregated γk,i,j over all locations  i, j of the test and training image
In each training image,  the highest γk,i,j is shown using an arrow and a relative influence
The relative influence is defined by γk,i,j normalized by the aggregated γk,i,j over the test and all positive  training image regions, i.e
images supporting the classification decision
Please note, that γk,i,j >= 0 for positive training samples as each element in y is greater or equal 0
 Since multiple similar triplets occur, we use non-maximum  suppression and group triplets with a small normalized distance of less than 0.NN
As can be seen, this visualization of the classification decision is intuitive and reveals the high  impact of a few small parts of the training images
 Measuring the contribution of semantic parts We are  also interested whether human-defined semantic parts contribute significantly to decisions
Figure N shows the contribution of individual bird body parts for classification on  CUBN00-N0NN [NN]
For each test image, we obtain the ten  most related training image similar to before
We divide the  local feature into groups belonging to the bird’s head, belly,  and background and compute the sum of the squared inner  products between these regions
As can be seen, on average, NN% of the VGGNN [NN] prediction is caused by the  comparison of the bird’s heads
In contrast for VGG-M [N],  the background plays the most significant role with a contribution of NN%
This shows that the deeper network VGGNN  focuses much more on the bird instead of the background
 Relationship to salient regions We show that orderless  pooling cannot just be rephrased as a correspondence kerNNNNN    Correct  N N .N %  N N .N %  N0 .N %  NN.0 %  N.N%  N .N %  N .N %  N .0 %  N
N%  N.N%  Incorrect  N .N %  N .N %  N
N%  N.N%  N.N% N 0 .N %  N 0 .N %  N
N%  N.N%  N.N%  N 0 .N %  N 0 .N %  N
N%  N.N%  N.N%  N .N %  N .N %  N .N %  N.N%  N.N%  Figure N
Visualization of the most influential image regions for  the classification decision as defined in Eq
N: The large image in  the bottom left corner is the test image and the surrounding images  are crops of the training examples with highly relevant image regions
Percentages show the relative impact on the final decision
 The lower four images show incorrect classifications
 nel [N0] but also as implicitly performing salient matching
 Eq
N calculates the linear kernel between the pooled features showing that it induces a kernel between each pair of  local features
We can now show a further direct relation to  a simple matching of local features in two images by rewritHe ad  Be lly BG  Train image location  Head  Belly  BGT e st   i m  a g e  l o ca  ti o n NN% N% N%  N% NN% N%  N% N% NN%  VGG-M   He ad  Be lly BG  Train image location  NN% N% N%  N% NN% N%  N% N% N%  VGG-NN  0.0N  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  0.N0  Figure N
Contribution of different bird parts to the classification decision on CUBN00-N0NN comparing VGG-M and VGGNN  without fine-tuning
For each semantic part in a given test image  (rows), we compute the sum of inner products to another semantic part in a training image (columns)
This statistic is normalized  and averaged over all test images
The plots show that for VGG-M,  NN% of the classification decision can be attributed to the comparison of background elements
In contrast, the comparison of the  bird’s head is most important for VGGNN
 Figure N
Visualization of the pairs of most similar local features  using LN-distance
The thicker and whiter the line, the more similar are the features
Blue thin lines denote low similarity
We only  show matchings larger than N0% of the maximum matching in this  case
 ing the scalar products as:  〈yi, ỹj〉 ∝ ∑  i,j  (  ‖yi‖ N + ‖yj‖  N − ‖yi − yj‖ N )  , (N)  where the Euclidean distance between two features appears
The kernel output is therefore high if the feature vectors are highly similar (small Euclidean distance) especially  for pairs (i, j) characterized by individual high Euclidean norms
In Figure N, we visualize the Euclidean norms of the  feature vectors in convN N extracted with VGGNN
The input size was increased to NNN× NNN similar to [NN, N0] and the output of convN N after activation and before pooling  was used
Hence the local features have a spatial resolution  of NN × NN
In the first column, feature similarity was de- fined by the lowest LN-distance between local features
The  second column shows the magnitude of all local features  normalized to the highest norm in both feature maps
The  third and fourth column show the implicit matchings using  NNNNN    the inner product and the squared inner product as similarity measure, as it is used in average and bilinear pooling
 We only show matchings larger than N0% of the maximum  matching in this case
As can be observed when comparing the plot for the norm and the matching, areas with a  high magnitude of the features also correspond to salient regions
This is indeed reasonable since the “matching cost”  in Eq
(N) should focus on the relevant object itself and not  on background elements
 Focusing pairwise similarities by increasing α Similar  to [NN], we apply pooling directly after the ReLU activation following the last convolutional layer
Therefore, all  scalar products between these features are positive
Hence,  summands with a high scalar product are emphasized dramatically for large values of α and in particular also for bilinear pooling
Increasing α therefore leads to kernel values  likely based on only a few relevant pairs (i, j)
This fact is illustrated in the last two rows of Figure N, where we only  showed the pairs with an inner product larger N0% of the highest one for both average and bilinear pooling
 N
Experimental Analysis  In our experiments, we focus on analyzing the difference  between average and bilinear pooling for image recognition
 We make use of our novel α-pooling presented in Sect
N
 First, we show that it achieves state-of-the-art results in both  generic and fine-grained image recognition
α and hence  the pooling strategy is learned
Second, based on deep neural nets learned on both kinds of datasets, we analyze distinguishing properties using the visualization techniques of  Sect
N
We discuss the relationship of α with dataset granularity, classification decisions, and implicit pose normalization
Hence we manually set α in this second part
 Accuracy of α-pooling We evaluate both training from  scratch and fine-tuning using a network pre-trained on the  ILSVRC N0NN dataset [NN]
For training from scratch, we  use the VGG-M [N] architecture and replace the last pooling and the two fully-connected layers before the classifier  with α-pooling
Batch normalization [NN] is used after convolutions as well as after the α-pooling
In addition, we  use dropout with a probability of p = 0.N to reduce overfit- ting and improve generalization
Compact bilinear encoding [N0] is used to reduce the dimensionality of the outer  product to NNNN
The learning rate starts at 0.0NN and follows an exponential decay after every epoch
The batch size  is NNN
The results on ILSVRC N0NN (N000 classes, N.N million images) are shown in Figure N
We plot both the validation accuracy during the first twenty epochs of the training  as well as the final top-N single crop accuracy
The network  converges faster at only small additional computation cost  and reaches a higher final accuracy compared to the original  VGG-M with batch normalization
 For fine-tuning, we use VGG-M [N] and VGGNN [NN]  0 N N0 NN N0  Epochs  0.0  0.N  0.N  0.N  0.N  0.N  0.N  T o p -N  A c c u ra  c y  VGG-M  α-pooling  Approach Top-N accuracy  (Single crop)  VGG-M [N] NN.N%  Bilinear [NN] NN.N%  Our work:  VGG-M w/ α-pool NN.N%  Figure N
Accuracy on ILSVRC N0NN for VGG-M
The left plot  shows validation accuracy over the first twenty trained epochs
 VGG-M denotes the original architecture and α-pooling the novel  generalized pooling technique
α is learned from data
 Table N
Accuracy on several datasets with α-pooling using the  multi-scale variant
No ground-truth part or bounding box annotations were used
α is learned from data
 Dataset CUBN00-N0NN Aircraft N0 actions  classes / images N00 / NNk NN / N0k N0 / N.Nk  Previous NN.0% [NN] NN.N% [N] NN.0% [NN]  NN.0% [NN] NN.0% [NN] N0.N% [N]  NN.N% [NN] N0.N% [NN] NN.N% [NN]  Special case: bilinear [NN] NN.N% NN.N% Learned strategy (Ours) NN.N% NN.N% NN.0%  pre-trained on ILSVRC N0NN
We replace the last pooling and the two fully connected layers before the classifier with the novel α-pooling encoding
We follow [NN, N0]  and add a signed square root as well as LN-normalization  layer before the classifier
Pooling is done across two scales  with the smaller side of the image being NNN and NN0 pixels  long
Two-step fine-tuning [N] is used, where the last linear  layer is trained first with 0.0N times the usual weight decay and the whole network is trained afterwards with the usual  weight decay of 0.000N
The learning rate is fixed at 0.00N with a batch size of eight
α is learned from data
 The results for CUBN00-N0NN birds [NN], FGVCAircraft [N0] and Stanford N0 actions [N0] can be seen in  Table N
We achieve higher top-N accuracy for all datasets  compared to previous work
For fine-grained datasets  like birds and aircraft, we slightly improve the results of  [NN, N0], which is due to α = N being close to the learned α for this dataset
Our approach also shows a high accuracy  on datasets besides traditional fine-grained tasks as shown  by the actions dataset, where we achieve NN.0% accuracy  compared to NN.N% reported in [NN]
 Ranking dataset granularity wrt
α As mentioned before, the main purpose of the experiments is to analyze the  differences between average and bilinear pooling
In particular, we are interested in why average pooling lacks accuracy in fine-grained while bilinear reaches state-of-the-art
 The presented α-pooling allows for a smooth transition  between average and bilinear pooling
In this paragraph, we  analyze the relationship of the parameter α and the granularity of the dataset
The results using VGGNN can be seen  NNNNN    N.0 N.N N.0 N.N N.0 N.N N.0  α  0.NN  0.NN  0.NN  0.NN  N.00  N o rm  a liz  e d  a c c u ra  c y  MIT Scenes NN  Stanford N0 actions  CUBN00-N0NN  N.0 N.N N.0 N.N N.0 N.N N.0  α  0.NN  0.NN  0.NN  0.NN  N.00  N o rm  a liz  e d  a c c u ra  c y  Average across datasets  Figure N
Influence of α using VGGNN without fine-tuning
α = N  corresponds to average pooling and α = N to bilinear pooling
α  is manually set in this experiment
 in Figure N
α has been manually set to values in [N.0, N.0] and the accuracy without fine-tuning is plotted
The input  resolution was increased to NNN × NNN as done in previous work [NN, N0]
The accuracy is normalized to N for easier  comparison of different datasets
It seems each dataset requires a different type of pooling
If the datasets are ordered  by the value of α, which gives the highest validation accuracy, the order is as follows: MIT Scenes NN, N0 actions,  and CUBN00-N0NN with α = N.N, N.N, and N.N, respec- tively
This seems to suggest that the more we move from  generic to fine-grained classification, the higher is the value  of α
In addition, larger values of alpha are still good for  fine-grained while accuracy drops quickly for generic tasks
 Hence focusing the classification on few correspondences  seems a good strategy for fine-grained while it lowers accuracy on generic tasks
VGG-M shows a similar trend
 Classification visualization versus α Sect
N presents a  novel way to visualize classification decisions for feature  representations based on α-pooling
We are now interested  in the change of classification decision reasoning with respect to α
Figure N shows the classification visualization  for two sample test images from CUBN00-N0NN and MIT  scenes NN
For each test image, we show the visualization  for α = N and α = N
While α = N causes a fairly equal contribution of multiple training image regions to the decision, α = N pushes the importance of the first images
For example, the contribution of the most relevant training image region grows from NN.N% to NN.N% in the bird image
A  statistical analysis is shown in the supplementary material
 Relevance of semantic parts versus α A second way to  analyze the pairwise matching induced by α-pooling is to  quantify the matchings between semantic parts
We evaluated on CUBN00-N0NN using ground-truth part annotations
 A ground-truth segmentation of the bird’s head and belly  was generated based on these annotations and used to assign feature locations in convN N of VGGNN to bird head,  body, and background
Afterwards, for each test image, the  kernel between all features of the bird’s head in the test and  a training image is aggregated
This is done for all pairs of  regions and for the N0 most relevant training images
The  obtained statistics are averaged over all test images
 α = N  N N .N %  N 0 .N %  N .N %  N.N%  N.N%  N N .N %  N 0 .N %  N .0 %  N
N%  N.N%  α = N  N N .N %  N 0 .N % N  .N %  N.N%  N.N% N N .N %  N 0 .N %  N 0 .N %  N
N%  N.0%  Figure N
Influence of α on the classification decisions
As in Figure N, we visualize the most relevant image regions for the classification decision
A larger α increases the importance of the most  influencial regions in the training images
Hence the decision is  based on few important regions
α is manually set
 He ad  Be lly BG  Train image location  Head  Belly  BGT e st   i m  a g e  l o ca  ti o n NN% N% N%  N% NN% N%  N% N% NN%  α=N (average pooling)  He ad  Be lly BG  Train image location  NN% NN% N%  N% NN% N%  N% N% N%  α=N
N  He ad  Be lly BG  Train image location  NN% N% N%  N% NN% N%  N% N% N%  α=N (bilinear pooling)  0.0N  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  0.N0  Figure N
Influence of α on the contribution of different bird body  parts to the classification decision on CUBN00-N0NN
The higher  the value of α, the higher is the influence of the actual bird body  parts to the classification decision
α is manually set to {N, N.N, N}
 First, we analyze the influence of α on the contribution of  different body parts
Figure N shows the results for VGGNN  without fine-tuning when α is manually set to {N, N.N, N}
It seems that larger values of α focus the classification decision on actual body parts
The contribution of the bird’s  head to the classification decision shifts from N% (α = N) to NN% (α = N)
This observation matches our previous in- terpretation that larger values for α focus the classification  decision on fewer discriminative pairs of local features
 Second, we are also interested in the effect of fine-tuning  on classification decisions
Figure N depicts the results for  VGGNN and α set to N
Fine-tuning shifts the focus towards the bird’s head, while especially the influence of background decreases
α-pooling is one of the few approaches  which allow quantifying the influence of semantic parts
 NNNNN    He ad  Be lly BG  Train image location  Head  Belly  BGT e st   i m  a g e  l o ca  ti o n NN% N% N%  N% NN% N%  N% N% N%  VGGNN w/o finetuning  He ad  Be lly BG  Train image location  Head  Belly  BGT e st   i m  a g e  l o ca  ti o n NN% N% N%  N% NN% N%  N% N% N%  VGGNN w/ finetuning  0.0N  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  0.N0  0.NN  Figure N
Influence of fine-tuning on the contribution of different  bird body parts to the classification decision on CUBN00-N0NN
 As can be seen, the bird’s head gains influence at the cost of background areas
α is set to N.0
 N
Discussion  Fine-grained tasks are about focusing on a few relevant  areas Our in-depth analysis revealed that a high accuracy for fine-grained recognition can be achieved when only  a few relevant areas are compared with each other by implicit salient matching
In terms of α-pooling, this corresponds to a higher value of the parameter α
It also explains  why bilinear pooling showed a large performance gain for  fine-grained tasks [NN]: the corresponding α = N increases the influence of highly related features
On the other hand,  in generic image classification tasks like scene recognition,  the general appearance seems more important and hence a  lower value of α is better suited
Our experiments showed  that α = N.N is a good trade-off for a wide range of classi- fication datasets and hence is a good starting point
If finetuning is used, α is learned and adapts to the best value
 Implicit matching vs
explicit pose normalization  Most approaches for fine-grained recognition assumes objects consists of a few parts [NN, N, NN]
It is common belief  that part-based in contrast to global descriptors allow for  better representations of objects appearing in diverse poses
 In contrast, our analysis reveals that state-of-the-art  global representations perform an implicit matching of several different image regions
Compared to explicit partbased models, they are not limited by a fixed number of  parts learned from the data or utilized during classification
 Our α-pooling strategy can even learn how much a classification decision should rely on a few rather than a large  number of matchings
As argued in the last paragraph, the  intuition that fine-grained recognition tasks are about “detecting a small set of image regions that matter” is right
 However, the consequence that explicit part-based models  are the solution is questionable
Rather than designing yet  another part-based model, representations should be developed that lead to an even better implicit matching
 Kernel view of classification decisions We argue that  the kernel view of classification decisions is a valuable tool  for understanding and analyzing different feature encoding
 Pooling on Individual Scales ...[ [...[ [ ...[ [[pool(    ),pool(    ),pool(    )]  R e  su lt  in g  Im p  lic it  M a  tc h  in g  Pooling on Multiple Scales  pool(            )...[ ..
..
[  Figure N0
Illustration of different techniques to deal with multiple  scales and their resulting implicit matching
Directly pooling over  multiple scales allows for implicit matching across scales
 We used the kernel view in the previous sections to show  that a larger value of α focuses the classification decision on  the most relevant pairs of local features
This understanding also allowed us to visualize classification decisions by  using matchings to the most relevant training images
However, there are even more possible ways to exploit this in  future work
For example, we can derive a feature matching  over multiple scales in a theoretically sound way
Previous  work often handled multiple scales by extracting crops at  different scales and averaging the decision values across all  crops [NN, NN]
While this gives an improvement, a theoretical justification is missing
In contrast, if we perform  α-pooling across all local features extracted from all scales  of the input image, the kernel view reveals that this relates  to a matching of local features across all possible combinations of locations and scales of two images, see Figure N0
 To summarize, while kernel functions are rarely explicitly  used in state-of-the-art approaches, they can be useful for  both understanding and designing new approaches
 N
Conclusions  In this paper, we propose a novel generalization of average and bilinear pooling called α-pooling
Our approach  has both state-of-the-art performance and a clear justification of predictions
It allows for a smooth transition between average and bilinear pooling, and to higher-order  pooling, allowing for understanding the connection between  these operating points
We find that in practice our method  learns that an intermediate strategy between average and bilinear pooling offers the best performance on several finegrained classification tasks
In addition, a novel way for  visualizing classification decision is presented showing the  most influential training image regions for a decision
Furthermore, we quantify the contributions of semantic parts in  a classification decision based on these influential regions
 Acknowledgments Part of this research was supported  by grant RO N0NN/N-N of the German Research Foundation  (DFG)
The authors thank Nvidia for GPU donations
 NNNNN    References  [N] S
Bach, A
Binder, G
Montavon, F
Klauschen, K.-R
 Müller, and W
Samek
On pixel-wise explanations for nonlinear classifier decisions by layer-wise relevance propagation
PloS one, N0(N), N0NN
N  [N] L
Bo and C
Sminchisescu
Efficient match kernel between  sets of features for visual recognition
In NIPS, pages NNN–  NNN
Curran Associates, Inc., N00N
N  [N] S
Branson, G
V
Horn, S
J
Belongie, and P
Perona
Bird  species categorization using pose normalized deep convolutional nets
CoRR, abs/NN0N.NNNN, N0NN
N, N  [N] S
Cai, L
Zhang, W
Zuo, and X
Feng
A probabilistic collaborative representation based approach for pattern classification
In CVPR, pages NNN0–NNNN, N0NN
N  [N] J
Carreira, R
Caseiro, J
Batista, and C
Sminchisescu
Semantic segmentation with second-order pooling
In ECCV  (N), volume NNNN of Lecture Notes in Computer Science,  pages NN0–NNN
Springer, N0NN
N, N  [N] Y
Chai, V
S
Lempitsky, and A
Zisserman
Symbiotic segmentation and part localization for fine-grained categorization
In ICCV, pages NNN–NNN, N0NN
N  [N] K
Chatfield, K
Simonyan, A
Vedaldi, and A
Zisserman
 Return of the devil in the details: Delving deep into convolutional nets
In BMVC
BMVA Press, N0NN
N, N  [N] W
Di, C
Wah, A
Bhardwaj, R
Piramuthu, and N
Sundaresan
Style finder: Fine-grained clothing style detection and  retrieval
In ICCV Workshops, pages N–NN, N0NN
N  [N] A
Freytag, E
Rodner, M
Simon, A
Loos, H
Khl, and  J
Denzler
Chimpanzee faces in the wild: Log-euclidean  cnns for predicting identities and attributes of primates
In  GCPR, N0NN
N  [N0] Y
Gao, O
Beijbom, N
Zhang, and T
Darrell
Compact  bilinear pooling
CoRR, abs/NNNN.0N0NN, N0NN
N, N, N, N, N,  N  [NN] Y
Gong, L
Wang, R
Guo, and S
Lazebnik
Multi-scale  orderless pooling of deep convolutional activation features
 In ECCV (N), volume NNNN of Lecture Notes in Computer  Science, pages NNN–N0N
Springer, N0NN
N  [NN] P
H
Gosselin, N
Murray, H
Jégou, and F
Perronnin
Revisiting the fisher vector for fine-grained classification
Pattern  Recognition Letters, NN:NN–NN, N0NN
N  [NN] P
H
Gosselin, N
Murray, H
Jégou, and F
Perronnin
Revisiting the fisher vector for fine-grained classification
Pattern  Recognition Letters, NN:NN–NN, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Spatial pyramid pooling in deep convolutional networks for visual recognition
In  ECCV (N), volume NNNN of Lecture Notes in Computer Science, pages NNN–NNN
Springer, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
CoRR, abs/NNNN.0NNNN, N0NN
N, N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
In  ICML, volume NN of JMLR Workshop and Conference Proceedings, pages NNN–NNN
JMLR.org, N0NN
N  [NN] J
Krause, H
Jin, J
Yang, and F
Li
Fine-grained recognition  without part annotations
In CVPR, pages NNNN–NNNN, N0NN
 N  [NN] T
Lin and S
Maji
Visualizing and understanding deep texture representations
In CVPR, pages NNNN–NNNN, N0NN
N,  N  [NN] T
Lin, A
Roy Chowdhury, and S
Maji
Bilinear CNN models for fine-grained visual recognition
In ICCV, pages NNNN–  NNNN
IEEE Computer Society, N0NN
N, N, N, N, N, N, N  [N0] S
Maji, E
Rahtu, J
Kannala, M
B
Blaschko, and  A
Vedaldi
Fine-grained visual classification of aircraft
 CoRR, abs/NN0N.NNNN, N0NN
N  [NN] N
Murray, H
Jegou, F
Perronnin, and A
Zisserman
Interferences in match kernels
IEEE Transactions on Pattern  Analysis and Machine Intelligence, N0NN
N  [NN] A
Rosenfeld and S
Ullman
Visual concept recognition and  localization via iterative introspection
In ACCV (N), pages  NNN–NNN, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
S
Bernstein,  A
C
Berg, and F
Li
Imagenet large scale visual recognition  challenge
IJCV, NNN(N):NNN–NNN, N0NN
N  [NN] M
Simon and E
Rodner
Neural activation constellations:  Unsupervised part model discovery with convolutional networks
In ICCV, pages NNNN–NNNN
IEEE Computer Society,  N0NN
N, N  [NN] M
Simon, E
Rodner, and J
Denzler
Part detector discovery  in deep convolutional neural networks
In ACCV (N), volume  N00N of Lecture Notes in Computer Science, pages NNN–NNN
 Springer, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
CoRR,  abs/NN0N.NNNN, N0NN
N, N  [NN] C
Szegedy, S
Ioffe, and V
Vanhoucke
Inception-vN,  inception-resnet and the impact of residual connections on  learning
CoRR, abs/NN0N.0NNNN, N0NN
N, N  [NN] J
B
Tenenbaum and W
T
Freeman
Separating style  and content with bilinear models
Neural Computation,  NN(N):NNNN–NNNN, N000
N, N  [NN] C
Wah, S
Branson, P
Welinder, P
Perona, and S
Belongie
 The Caltech-UCSD Birds-N00-N0NN Dataset
Technical Report CNS-TR-N0NN-00N, California Institute of Technology,  N0NN
N, N  [N0] B
Yao, X
Jiang, A
Khosla, A
L
Lin, L
J
Guibas, and  F
Li
Human action recognition by learning bases of action  attributes and parts
In ICCV, pages NNNN–NNNN, N0NN
N  [NN] J
Yosinski, J
Clune, A
Nguyen, T
Fuchs, and H
Lipson
 Understanding neural networks through deep visualization
 In Deep Learning Workshop, ICML, N0NN
N  [NN] M
D
Zeiler and R
Fergus
Stochastic pooling for regularization of deep convolutional neural networks
CoRR,  abs/NN0N.NNNN, N0NN
N  [NN] M
D
Zeiler and R
Fergus
Visualizing and understanding  convolutional networks
In ECCV (N), volume NNNN of Lecture Notes in Computer Science, pages NNN–NNN
Springer,  N0NN
N  [NN] X
Zhang, H
Xiong, W
Zhou, W
Lin, and Q
Tian
Picking  deep filter responses for fine-grained image recognition
In  CVPR, June N0NN
N  NNNNN    [NN] Y
Zhang, X.-S
Wei, J
Wu, J
Cai, J
Lu, V.-A
Nguyen, and  M
N
Do
Weakly supervised fine-grained categorization  with part-based image representation
IEEE Transactions on  Image Processing, NN(N):NNNN–NNNN, N0NN
N  [NN] B
Zhou, A
Khosla, À
Lapedriza, A
Oliva, and A
Torralba
Learning deep features for discriminative localization
 In CVPR, pages NNNN–NNNN, N0NN
N  N0NNNNLocally-Transferred Fisher Vectors for Texture Classification   Locally-Transferred Fisher Vectors for Texture Classification  Yang SongN, Fan ZhangN, Qing LiN, Heng HuangN, Lauren J
O’DonnellN, Weidong CaiN  NSchool of Information Technologies, University of Sydney, Australia  NBrigham and Women’s Hospital, Harvard Medical School, USA  NElectrical and Computer Engineering Department, University of Pittsburgh, USA  Abstract  Texture classification has been extensively studied in  computer vision
Recent research shows that the combination of Fisher vector (FV) encoding and convolutional  neural network (CNN) provides significant improvement in  texture classification over the previous feature representation methods
However, by truncating the CNN model at  the last convolutional layer, the CNN-based FV descriptors  would not incorporate the full capability of neural networks  in feature learning
In this study, we propose that we can  further transform the CNN-based FV descriptors in a neural  network model to obtain more discriminative feature representations
In particular, we design a locally-transferred  Fisher vector (LFV) method, which involves a multi-layer  neural network model containing locally connected layers  to transform the input FV descriptors with filters of locally  shared weights
The network is optimized based on the  hinge loss of classification, and transferred FV descriptors  are then used for image classification
Our results on three  challenging texture image datasets show improved performance over the state-of-the-art approaches
 N
Introduction  Texture is a fundamental component in visual recognition
The study of texture, especially feature representation  of textures, has evolved over the years from basic statistical  features, to the most recent methods based on deep learning
 Among the numerous representation methods, we are particularly interested in the feature encoding aspect
While the  earlier studies have mainly used the bag-of-words (BOW)  model and its variations [NN, NN, NN, NN, N0, NN, NN], encoding via Fisher vectors (FV) has become the dominant  approach in texture classification [NN, N, NN, N]
 Similar to BOW, FV encoding aggregates the local-level  features into the image-level representation
The main  uniqueness of FV encoding is the soft assignment of GausFigure N
With the VGG-VD model, FV-CNN descriptor is computed by FV encoding of the local features from the last convolutional layer
We design the LFV model, in a multi-layer neural  network construct, to further transform the FV-CNN descriptor to  a more discriminative LFV descriptor
 sian components and the computation of first and second  order difference vectors
In addition, while typically the  dense scale-invariant feature transform (DSIFT) features  are the local features used with FV encoding [NN, N, NN],  the recent approach has shown that the local features from a  convolutional neural network (CNN) model could produce  more discriminative FV descriptors [N]
In particular, this  study proposes a FV-CNN descriptor, which is computed  by FV encoding of the local features extracted from the  last convolutional layer of the VGG-VD model (very deep  CNN model with NN layers) [NN] pretrained on ImageNet
 This FV-CNN descriptor shows large improvement over the  more standard FV-DSIFT descriptor [N, NN]
 Also, for texture classification, this FV-CNN descriptor shows higher classification performance than FC-CNN,  which is the descriptor obtained from the penultimate fully  connected layer of the CNN [N]
Moreover, we find that  even if the pretrained VGG-VD model is fine-tuned on the  texture image dataset, the fine-tuned FC-CNN descriptors  are still less discriminative than the FV-CNN descriptors
 These observations indicate that FV encoding is more efNNNNN    fective than the encoding by the fully connected layers in  the CNN pipeline
We suggest that the main reason of this  advantage is that the GMM model used in FV encoding  provides an explicit feature space modeling and this has a  higher generalization capability to better capture the complex feature space
 However, with FV-CNN, the benefit of CNN is not fully  utilized since it is truncated at the last convolutional layer
 To better incorporate the learning capability of a CNN  model, there is a trend to create end-to-end learning by  mimicking the handcrafted encoding in a CNN model
For  example, in the NetVLAD model [N], customized layers are  inserted in place of the fully connected layers to generate a  descriptor similar to the VLAD encoding
However, our experiments show that this NetVLAD model is less effective  than FV-CNN descriptors in the texture classification problem
We find that besides the reason that VLAD encodes  only first order differences, the classification performance  of NetVLAD is also limited by the design of the fully connected layer connecting the high-dimensional VLAD descriptor with the softmax loss layer
 In this work, we consider that since the multi-layer neural network model (with fully connected layers) is very different from the GMM construct, both algorithms (FV encoding and neural network) could discover complementary  information to represent the images effectively
Therefore,  it could be helpful to integrate the FV encoding with a neural network model, rather than using a single model in place  of the other, so that the advantages of both algorithms would  be incorporated
We expect that the integrated model would  generate descriptors with higher discriminative power
 We thus design a locally-transferred Fisher vector (LFV)  method to further transform the FV-CNN descriptor in a  neural network model (as shown in Figure N)
Briefly, we  design a multi-layer neural network model, with the FVCNN descriptors as the input layer and a final layer representing the hinge loss of classification
The intermediate  layers comprise a locally connected layer, with local filters that transform the input data into a lower dimension
 The filter weights are shared locally so that the data transform is performed differently on the sub-regions of the FVCNN descriptor
Compared to FV-CNN, this LFV method  helps to integrate the benefit of discriminative neural network in feature learning
Also when compared to end-toend learning, the capability of FV encoding in representing  the complex feature space is retained by keeping the FVCNN component
Therefore, instead of attempting to use  a single CNN model to encompass the benefits of both FV  encoding and neural network, it becomes a simpler problem to design the additional neural network model on top of  FV-CNN descriptors
 We performed experiments on three texture image  datasets, including the KTH-TIPSN dataset [N], the Flickr  Material Dataset (FMD) [N0], and the Describable Texture Datasets (DTD) [N]
We demonstrate improved performance over the recent approaches [N, NN]
 N.N
Related work  The current state-of-the-art approaches for texture classification include the one with FV-CNN descriptors [N] and  the bilinear CNN (B-CNN) model [NN]
Both approaches  use the pretrained VGG-VD model as the base network,  but with different encoding techniques, i.e
FV versus bilinear encoding
The two encoding techniques provide similar  classification performance with FV-CNN having a smaller  feature dimension
 When applying the pretrained VGG-VD model to the  texture image datasets, it could be intuitive to consider finetuning the model first on the specific dataset [N, NN, NN]
For  FV-CNN and B-CNN models, the fine-tuning needs to be  conducted down to the convolutional layers to take effect
 However, it is reported in [NN] that fine-tuning the VGG-VD  model on the texture image datasets leads to negligible performance difference
This could be due to the small number of images available for training in the texture datasets
 The B-CNN model also has the advantage of an end-to-end  learning capability with its neural network construct
However, such learning requires a large image dataset and has  only been performed on ImageNet [NN]
 A particularly interesting end-to-end learning model is  the NetVLAD [N]
In this model, the outputs from the last  convolutional layer are used as input to the VLAD layer,  which contains learnable parameters and can be computed  with convolution and softmax operations
The model is  however designed for place recognition
When applied to  texture images, we find that the classification performance  is lower than FV-CNN, partly due to the formulation of only  first order differences
Another study proposes a FisherNet  model, which adds layers with similar effects to FV encoding, incorporating both first and second order differences  [NN]
However, this model is quite complicated requiring  an explicit patch generation layer, rather than using the local features from the convolutional layers
Another model,  namely HistNet, is recently proposed to simulate the histogram / BOW encoding in the CNN model [NN]
However,  without the first and second order difference information,  such a network might not be suitable for texture classification problems
 There are also other ways to improve the FV descriptors
 For example, dimensionality reduction with a large margin  construct is designed and shows improvement in face recognition over the high-dimensional FV descriptor [NN]
Also,  with deep Fisher networks [NN], multiple Fisher layers are  stacked and combined with a global layer to produce the final descriptor, and discriminative dimensionality reduction  is learned in each layer
In another study [NN], the GausNNNN    Figure N
Our LFV model comprises the input layer, locally connected layer, local normalization layer, ReLU layer, and the hinge loss  layer
The input layer is the FV-CNN descriptor
The output at the ReLU layer is the LFV descriptor generated
 sian parameters are integrated into the SVM learning objective to achieve end-to-end learning of both FV encoding  and SVM classification
In addition, approximate Fisher  kernel [N] is designed to incorporate latent variable modeling into Fisher encoding, so that local features need not  be identically and independently distributed (iid)
An intranormalization technique [N], which is originally proposed  for the VLAD descriptor, has also been applied to FV descriptors recently [N0]
With this technique, each feature  block is individually normalized to reduce the bursty effect  in the descriptor encoding
These approaches are however  less coupled with the CNN model and not designed for texture image classification
 N
Preliminary  FV encoding computes an image-level descriptor by aggregating the local patch-level features
The key step in FV  encoding is to generate a Gaussian mixture model (GMM)  with K components from the local features of the training set
To obtain the FV descriptor of an image, the local features in this image are soft assigned to each Gaussian component
Then based on the soft assignment, the average first  and second order differences between the local features and  K Gaussian components are computed and concatenated to produce the FV descriptor
 In this study, we focus on the FV-CNN descriptor
Given  an image I and the VGG-VD model pretrained on Ima- geNet, the NNN-dimensional local features are derived from  the last convolutional layer of the VGG-VD model
These  local features of training images are then pooled together  to generate the GMM model, and encoded accordingly to  produce the FV-CNN descriptor
The dimension of the FVCNN descriptor h is NKD, with D = NNN and K is set to NN following the approach used in [N]
 N
Locally transferring Fisher vectors  We design the LFV method in a multi-layer neural network model
Figure N gives an overview of our model,  which comprises five layers
The first input layer is simply  the FV-CNN descriptor
In a CNN sense, this input layer  has a size of N× N× (NKD)×N , with N as the batch size during training
We denote the nth input vector in the batch as h(n)
 The second layer is a locally connected layer
It consists  of NK filters, with each filter of DN neurons
Each filter is fully connected to a section of D inputs in the input layer, and produces DN outputs
Formally, the output fN(n, i) ∈ R  DN corresponding to the input h(n) from the ith filter is computed as:  fN(n, i) = WN(i)h(n, i) + bN(i) (N)  where h(n, i) ∈ RD is the ith section in the input vector h(n), WN(i) ∈ R  DN×D is the weight matrix of the ith fil- ter, and bN(i) ∈ R  DN is the bias vector
Also, to reduce the  number of parameters, we choose to have every four consecutive filters share the same weights, hence there are a  total of NK/N unique filters in this layer
The total output dimension of the second layer is N×N×(NKDN)×N 
Note that with DN set to NN, this layer effectively condenses the FV descriptor to a lower dimension
 The third layer is a local normalization layer
Each output fN(n, i) from the second layer is LN normalized so that the various sections have the same importance in the transferred descriptor
The fourth layer is a ReLU layer, with  ReLU activation applied to the N×N×(NKDN)×N dimen- sional output of the previous layer
We denote the output of  the input h(n) at the fourth layer as fN(n), which can be summarized as:  fN(n) = ReLU({‖fN(n, N)‖N, 


, ‖fN(n, NK)‖N})
(N)  This fN(n) is then the transferred FV descriptor LFV from our model
 The last layer is the loss layer, which gives a loss value  of classification based on the output fN from the previous layer
We define this layer with the hinge loss
Specifically,  assume that the dataset contains L image classes
A one- versus-all multi-class linear-kernel classification model is  formulated, with one weight vector wl ∈ R NKDN for each  NNNN    class l ∈ {N, 


, L}
The loss value ε is computed as:  N  N  L∑  l=N  wT l wl+C  L∑  l=N  N∑  n=N  max(N−wT l fN(n)λ(n, l), 0) (N)  where λ(n, l) = N if the nth input vector h(n) belongs to class l and λ(n, l) = −N otherwise
Minimizing this loss value at the last layer is thus analogous to minimizing the  margin in an SVM classifier
 N.N
Design explanation  In our network design, we consider the second layer conceptually similar to a fully connected layer in the VGG-VD  model, which is useful for transforming the input data to a  lower dimension
However, we choose to use the locally  connected structure rather than fully connected, since we  consider that it would be difficult to have a single filter that  would effectively transform the long FV descriptors
By  using the local filters, varying feature patterns could potentially be explored in different sub-regions of the FV descriptors, and the collective results from local filters could  improve the overall results
Also, we set the section size as  NNN, which is the dimension of the local feature
Each filter thus corresponds to the mean or variance vector of one  Gaussian component in the GMM model
Furthermore, although we could have one filter for each NNN-dimensional  section, the amount of learnable parameters would be huge  and overfitting would be a problem for the small size of  dataset
We thus experimented with a number of strategies  to merge filters with weight sharing
We found that the simple technique of having a common filter for every four consecutive sections could provide good performance
 For the loss layer, we suggest that since LFV descriptors  will be finally classified using linear-kernel SVM, the commonly used softmax loss function is not well aligned with  the SVM classification objective
We thus choose to use an  SVM formulation in this loss layer based on the standard  hinge loss
This design is similar to the method in [NN],  but we explicitly define the multi-class classification loss
 In addition, while it is reported in [NN] that the LN-SVM  formulation (squared sum of losses) provides better performance than LN-SVM (linear sum of losses), we found that  LN-SVM is more effective in the texture image classification problem
 Overall, by transferring the FV descriptor using the proposed model, the benefits of FV encoding and discriminative learning of neural network are integrated in a simple  manner
We also keep the network design simple with minimal layers to reduce the network complexity and the risk of  overfitting
We do however suggest that it could be possible  to further enhance the network with varying configurations  (e.g
more locally connected layers and a different DN), es- pecially if the method is applied to a different dataset
 N.N
Parameter learning  The forward and backward passes of the locally connected layer can be implemented by a combination of NK/N standard fully connected neural networks to learn the parameters WN and bN
The input data to each network is of size N×N×D×N and the output is of size N×N×DN×N 
The combination of all individual outputs then gives a total  dimension of N× N× (NKDN)×N 
Standard implementa- tion is also used for the LN normalization and ReLU layers
 For the loss layer, the loss function can be differentiated  with respect to fN(n) and wl to obtain the derivatives for backpropagation
In particular, we obtain the following:  ∂ε  ∂fN(n) = −C  L∑  l=N  λ(n, l)wlN(N > w T  l fN(n)λ(n, l)) (N)  and  ∂ε  ∂wl = wl − C  N∑  n=N  λ(n, l)fN(n)N(N > w T  l fN(n)λ(n, l))  (N)  where the regularization parameter C is set to 0.N
The parameters WN, bN, and wl are initialized by treating  the local filters as individual networks and training them  separately based on the sections of FV-CNN descriptors
In  other words, we create NK separate networks, with each one used to train one filter as the initial values; and we found  such an initialization process to be particularly useful for  the FMD dataset
This initialization process leads to considerable improvement in classification results over the random initialization
In addition, we also found that adding a  dropout layer with rate 0.N before the loss layer can further  reduce the feature redundancy and improve the final classification result slightly
This is thus incorporated into the  network when learning parameters
 N
Experiments  N.N
Datasets and implementation  We used three texture image datasets for experiments
 The KTH-TIPSN dataset contains NNNN images from NN material classes, with each class of NNN images
The images in  each class are divided into four samples of different scales
 Following the standard protocol, one sample is used for  training and three samples are used for testing during each  split
The FMD dataset contains N000 images from N0 material classes with each class of N00 images
During experiments, half of the images are randomly selected for training and the other half for testing
The DTD dataset contains NNN0 images from NN texture classes, with each class  having NN0 images
Unlike KTH-TIPSN and FMD, the images in DTD have varying sizes
DTD is also considered as  the most challenging dataset since it contains images in the  NNNN    Table N
The classification accuracies (%), comparing our LFV method with FV-CNN [N], FV-CNN computed with fine-tuned VGG-VD  model (backpropagation to the last convolutional layer), FV descriptor generated with end-to-end CNN learning similar to the NetVLAD  model (backpropagation to the FV layer), and B-CNN [NN]
Linear-kernel SVM classification is performed with all compared approaches
 Dataset Our LFV FV-CNN Fine-tuned FV End-to-end FV B-CNN  KTH-TIPSN NN.N±N.N NN.N±N.N N0.N±N.N NN.N±N.N N0.N±N.N  FMD NN.N±N.N NN.N±N.N NN.N±N.N NN.N±N.N N0.N±N.N  DTD NN.N±N.0 NN.N±N.N NN.N±N.0 NN.N±N.N NN.N±N.0  wild
For DTD, the training / testing splits published with  the dataset are used, and within each split, N/N of the images  are used for training and N/N for testing
For all datasets,  four splits of training and testing are conducted, and the  mean accuracy is used as the performance metric
 When generating the FV-CNN descriptors, we follow the  approach in [N]
The images are scaled to multiple sizes,  with scales of Ns, s = −N,−N.N, 


, N.N, and the VGG- VD model (with NN layers) is applied to each scale
The  local features from the last convolutional layer are pooled  together to generate a GMM with K = NN Gaussian com- ponents
The resultant FV-CNN descriptor is then NKD = NNNNN dimensional
This high-dimensional FV-CNN de- scriptor is then input to the LFV model to obtain the transferred descriptors
The learning rates of the various layers  are set to 0.0N and the batch size N is set to N0
The LFV model provides a discriminative dimensionality reduction  and reduces the descriptor dimension to NKDN = NNNN
Linear-kernel SVM is finally used to classify the LFV descriptors
Our code was implemented based on VLFeat [NN]  and MatConvNet [NN] libraries
 N.N
Compared approaches  For performance comparison, we evaluated the following approaches
For all approaches, VGG-VD is used as the  base model, and linear-kernel SVM is used as the classifier
 Pretrained model
FV-CNN descriptors are generated  with the VGG-VD model pretrained on ImageNet
This is  the same approach proposed in [N], and also the input to our  LFV model
 Fine-tuned model
FV-CNN descriptors are also computed by first fine-tuning the VGG-VD model on the texture  image dataset
The fine-tuning is performed in a standard  manner with the backpropagation stopped at various convolutional layers
 End-to-end learning of FV descriptor
We also experiment with an end-to-end CNN-based learning method  to derive the FV descriptors
To do this, we modify the  NetVLAD model to replace the VLAD layer with an FV  layer while keeping all the other layers unchanged
Also,  a fully connected layer of L neurons (L being the num- ber of image classes) and a softmax loss layer are appended  at the end of the NetVLAD model for parameter learning
 The FV layer is constructed following the design in [NN]
 Briefly, in the FV layer, a weight vector wk and bias vector bk are defined corresponding to each Gaussian component k
The first and second order difference vectors are com- puted using element-wise product and sum operations between the weight vector, local feature, and bias vector
This  layer is differentiable and hence can be embedded into the  CNN model
Note that the model is initialized using the  pretrained VGG-VD model, and the resultant FV descriptor  is also NKD dimensional
Include FC-CNN
As reported in [N], the FC-CNN descriptor provided much lower results than FV-CNN, but can  be concatenated with FV-CNN to obtain a more discriminative feature representation
We also evaluated the classification performance by concatenating FC-CNN with our LFV  descriptor
For this concatenation, the N0NN-dimensional  FC-CNN descriptor obtained from the penultimate layer of  VGG-VD is transformed using a model similar to LFV, but  with FC-CNN as the input, and the section size D is set to NN (a convenient number)
We found that this transformed  FC-CNN descriptor gives better classification results than  simply concatenating the original FC-CNN descriptor
 B-CNN
The B-CNN encoding is also used to obtain  the image descriptors
Similar to FV-CNN, the images are  scaled to multiple scales and the features from different  scales are pooled together
 Dimension reduced descriptor
Since our IFV descriptor effectively reduces the feature dimension of the original  FV-CNN descriptor, we also compare with the other dimensionality reduction algorithms, including principal component analysis (PCA), linear discriminant analysis (LDA),  the compact bilinear pooling designed to reduce the B-CNN  descriptor [N], and a simple fully connected layer in place  of the locally connected layer in our LFV model
 N.N
Results  Table N lists the classification results using (i) original FV-CNN obtained using VGG-VD pretrained on ImageNet, (ii) FV-CNN from fine-tuned VGG-VD model, (iii)  FV descriptor with end-to-end learning, (iv) B-CNN, and  (v) our LFV model
The results show that our LFV method  NNNN    Table N
The classification accuracies (%) of LFV and LFV combined with FC-CNN
 Dataset LFV LFV + FC-CNN  KTH-TIPSN NN.N±N.N NN.N±N.N  FMD NN.N±N.N NN.N±N.N  DTD NN.N±N.0 NN.N±N.N  Figure N
Classification accuracies (%) comparing our LFV  method with SVM classification on original FV-CNN descriptor,  and the dimension reduced FV-CNN descriptor using PCA, LDA,  and the neural network model with a fully connected (FC) layer in  place of the locally connected layer
 achieved the highest classification performance on all three  datasets
Compared to the current state of the art (FV-CNN  and B-CNN), our method provides the larger improvement  on the FMD dataset than the KTH-TIPSN and DTD datasets
 We suggest that this difference in improvements could be  partly affected by the number of image classes
The hinge  loss function would normally better model the differentiation when the number of image classes is small (e.g
N0  classes in FMD)
 Also, it is interesting to see that the fine-tuned FV-CNN  actually gives lower accuracy than the original FV-CNN
 This undesirable effect of fine-tuning could be due to the  small number of images available for training
Note that the  results given in the table are from backpropagation only to  the last convolutional layer
If lower convolutional layers  are also fine-tuned, similar or worse results are obtained
 In addition, the end-to-end learning of FV descriptors results in the lowest performance
This indicates that when  the training data is limited, the generalization capability  of GMM is more effective than the supervised learning in  CNN in representing the complex feature space
We do  however suggest that it might be possible to further enhance  the result with the end-to-end learning approach, with more  thorough experiments on the design of the training method  with data augmentation or multi-scale handling
This is  however beyond the scope of this study
 When the FC-CNN descriptor is concatenated with the  LFV descriptor, the classification performance is further  Table N
The classification accuracies (%) of LFV and the compact  bilinear pooling (CBP) [N]
The results of CBP are taken from [N],  based on two algorithms (RM & TS)
Since the CBP method was  evaluated using N/N of images for training and N/N for testing, for  fair comparison, we also use this setup here to evaluate LFV
Note  that both LFV and CBP have the same feature dimension of NNNN
 Dataset LFV CBP-RM CBP-TS  DTD NN.N±N.0 NN.N NN.N  Figure N
Classification accuracies (%) of our LFV method when  different numbers of local filters have shared weights
For example, P = N is the default setting, meaning every four consecutive  filters have the same weights
 Figure N
Classification accuracies (%) comparing our LFV  method with using softmax as the loss layer, and performing intranormalization on the FV-CNN descriptor
 improved on all three datasets, as shown in Table N
Recall  that this FC-CNN descriptor is the transformed descriptor  based on the same LFV model (with different parameters)
 This result also indicates that our LFV model is not limited to transforming FV descriptors but can be extended to  apply to different high-dimensional feature vectors
In addition, our LFV model has a similar number of parameters  to the VGG-F model [N]
However, the ImageNet pretrained  and fine-tuned VGG-F model provided less than N0% accuracy on texture classification, hence further demonstrating  the advantages of using FV-CNN and our LFV descriptors
 Figure N shows the various results comparing our LFV  method with the other dimensionality reduction techniques
 For PCA and LDA, the feature dimension is reduced to the  maximum possible dimension when using such techniques
 For FC, to restrict the network size, we set the fully conNNNN    Figure N
Example images from the KTH-TIPSN dataset
With our LFV method, the ‘aluminium’ and ‘lettuce leaf’ image classes are the  best classified classes (around NN.N% recall), while the ‘wool’ and ‘cotton’ classes are worst classified (around NN.N% and N0.N% recall,  respectively)
The red border indicates images that are misclassified
 Figure N
Example images from the FMD dataset
With our LFV method, the ‘foliage’ and ‘water’ image classes are the best classified  classes (around NN% and NN% recall, respectively), while the ‘metal’ and ‘fabric’ classes are worst classified (around NN% and NN% recall,  respectively)
The red border indicates images that are misclassified
 nected layer to have N0NN neurons
The results show that  PCA does not affect the classification performance, indicating that there is indeed a large degree of redundancy in the  FV-CNN descriptor that could be effectively removed
It  is interesting that LDA results in some improvement in the  classification performance, hence LDA could be a better alternative than SVM for classifying the FV-CNN descriptors
 The FC approach gives the lowest classification accuracy,  demonstrating the necessity of using the locally connected  layer instead of fully connected layer when transforming the  descriptors
In addition, recently a compact bilinear pooling  (CBP) method [N] was proposed to reduce the dimension of  the B-CNN feature
The method includes two similar algorithms, RM and TS, and the results on the DTD dataset are  reported
The two CBP algorithms and our LFV method  all reduce the feature dimension to NNNN
Our evaluation  shows that our LFV method outperforms CBP, as shown in  Table N
These results demonstrate that our LFV method  can be regarded as an effective discriminative dimensionality reduction algorithm, based on the supervised learning  with a multi-layer neural network model
 We note that an important parameter in our method is  the number of local filters of shared weights
We denote  this number as P 
By default, we specify that every four (P = N) consecutive local filters have the same weights
This is mainly to reduce the network size
Figure N shows  the effect of this P value on the classification performance
The classification result tends to increase slightly when  P = N or P = N is used
However, the training complexity and time required also increase with smaller P settings
On the other hand, P = N means too many local filters have shared weights, and the classification result is reduced considerably
Overall, we suggest that P = N is a well balanced choice when designing the network model
 We also evaluated using the standard softmax function  for the loss layer instead of our SVM loss, with an additional fully-connected layer ahead of the softmax layer
As  shown in Figure N, the softmax loss provides on average  0.N% lower accuracy than the SVM loss, indicating the benefit of using an SVM loss function
In addition, we consider  that our local transformation of the FV-CNN descriptor is  conceptually related to the intra-normalization technique on  VLAD [N], since in both approaches the transformation /  normalization is performed on individual sections of the  NNNN    Figure N
Example images from the DTD dataset
With our LFV method, the ‘chequered’, ‘studded’, ‘potholed’, and ‘knitted’ image  classes are the best classified classes (around NN.N%, NN.N%, NN.0%, and NN.N% recall, respectively), while the ‘blotchy’, ‘bumpy’, ’pitted’,  and ’stained’ classes are worst classified (around NN.0%, NN.N%, N0.0%, and N0.0% recall, respectively)
The red border indicates images  that are misclassified
 descriptor
Therefore, we also evaluated our LFV method  against the intra-normalization technique
As shown in Figure N, compared to the original FV-CNN descriptor, the  intra-normalization technique decreases the classification  accuracy on the KTH-TIPSN dataset by about N% and provides a small improvement on the DTD dataset only, while  our LFV method achieves consistent enhancement over FVCNN on all three datasets
This demonstrates the advantage  of having a supervised learning-based transformation rather  than a predefined normalization
 Figures N, N, and N show example images of the classification results
Take the KTH-TIPSN dataset for example
 The aluminium and lettuce leaf classes are visually distinctive from the other classes and hence exhibit excellent classification performance
The lowest classification accuracy  was obtained for the wool class, which is often misclassified as cotton or linen classes due to the similar visual characteristics among these fabric classes
For the FMD dataset,  it can be seen that although the images in the foliage class  also exhibit large visual variation, our method could effectively identify the distinguishing pattern of the leaves and  the classification performance for this class is high
 The main computational expensive process is the application of the CNN model to compute the local features at  multiple scales, requiring about N seconds per image
After the CNN local features are computed, the encoding of  Fisher vectors need less than N minute for each dataset
 Therefore, for a test image at run time, there is little additional cost to compute the FV-CNN descriptor compared to  obtain a CNN feature at the last fully connected layer
The  training of local filters in LFV needs about N00 epochs on  each dataset, and the training time varies depending on the  size of the data
For example, on the largest DTD dataset,  the training takes about N0 minutes with CPU Core iN and  GPU GeForce GTX NNN
 N
Conclusions  We present a texture image classification method in this  paper
Our method, called the locally-transferred Fisher  vector (LFV), transforms the FV-CNN descriptor in a multilayer neural network model to obtain a more discriminative  feature representation
The LFV model comprises a locally  connected layer with filters of locally shared weights and a  hinge loss layer representing the SVM classification objective
With the LFV model, the benefits of FV encoding and  neural network are integrated in a simple and effective manner, and the resultant LFV descriptor has a lower dimension  than the FV-CNN descriptor
Our method is evaluated on  three texture image datasets including KTH-TIPSN, FMD,  and DTD
The results show that our LFV descriptors provide higher classification performance than the state-of-theart approaches based on FV-CNN and B-CNN descriptors
 We also demonstrate that LFV is more effective than finetuning or end-to-end learning of FV-CNN descriptors
 NNNN    References  [N] R
Arandjelovic, P
Gronat, A
Torii, T
Pajdla, and J
Sivic
 NetVLAD: CNN architecture for weakly supervised place  recognition
CVPR, pages NNNN–NN0N, N0NN
N  [N] R
Arandjelovic and A
Zisserman
All about VLAD
CVPR,  pages NNNN–NNNN, N0NN
N, N  [N] Y
Aytar and A
Zisserman
Tabula rasa: model transfer for  object category detection
ICCV, pages NNNN–NNNN, N0NN
N  [N] B
Caputo, E
Hayman, and P
Mallikarjuna
Class-specific  material categorisation
ICCV, pages NNNN–NN0N, N00N
N  [N] K
Chatfield, K
Simonyan, A
Vedaldi, and A
Zisserman
 Return of the devil in the details: delving deep into convolutional nets
BMVC, pages N–NN, N0NN
N  [N] M
Cimpoi, S
Maji, I
Kokkinos, S
Mohamed, and  A
Vedaldi
Describing textures in the wild
CVPR, pages  NN0N–NNNN, N0NN
N, N  [N] M
Cimpoi, S
Maji, and A
Vedaldi
Deep filter banks for  texture recognition and segmentation
CVPR, pages NNNN–  NNNN, N0NN
N, N, N, N  [N] R
G
Cinbis, J
Verbeek, and C
Schmid
Approximate fisher  kernels of non-iid image models for image categorization
 IEEE Trans
Pattern Anal
Mach
Intell., NN(N):N0NN–N0NN,  N0NN
N  [N] Y
Gao, O
Beijbom, N
Zhang, and T
Darrell
Compact  bilinear pooling
CVPR, pages NNN–NNN, N0NN
N, N, N  [N0] L
W
A
v
d
H
C
W
L
Liu, C
Shen
Encoding high dimensional local features by sparse coding based fisher vectors
NIPS, pages N–N, N0NN
N  [NN] S
Lazebnik, C
Schmid, and J
Ponce
A sparse texture representation using local affine regions
IEEE Trans
Pattern  Anal
Mach
Intell., NN(N):NNNN–NNNN, N00N
N  [NN] T
Lin and S
Maji
Visualizing and understanding deep texture representations
CVPR, pages NNNN–NNNN, N0NN
N, N  [NN] L
Liu, P
Fieguth, G
Kuang, and H
Zha
Sorted random projections for robust texture classification
ICCV, pages NNN–  NNN, N0NN
N  [NN] J
Malik, S
Belongie, T
Leung, and J
Shi
Contour and  texture analysis for image segmentation
Int
J
Comput
Vis.,  NN(N):N–NN, N00N
N  [NN] M
Oquab, L
Bottou, I
Laptev, and J
Sivic
Learning and  transferring mid-level image representations using convolutional neural networks
CVPR, pages N–N0, N0NN
N  [NN] W
Ouyang, X
Wang, C
Zhang, and X
Yang
Factors  in finetuning deep model for oject detection with long-tail  deistribution
CVPR, pages NNN–NNN, N0NN
N  [NN] F
Perronnin, J
Sanchez, and T
Mensink
Improving the  fisher kernel for large-scale image classification
ECCV,  pages NNN–NNN, N0N0
N  [NN] Y
Quan, Y
Xu, Y
Sun, and Y
Luo
Lacunarity analysis on  image patterns for texture classification
CVPR, pages NN0–  NNN, N0NN
N  [NN] L
Sharan, C
Liu, R
Rosenholtz, and E
H
Adelson
Recognizing materials using perceptually inspired features
Int
 J
Comput
Vis., N0N(N):NNN–NNN, N0NN
N  [N0] L
Sharan, R
Rosenholtz, and E
H
Adelson
Material perception: what can you see in a brief glance? Journal of  Vision, N(N):NNN, N00N
N  [NN] G
Sharma, S
ul Hussain, and F
Jurie
Local higher-order  statistics (lhs) for texture categorization and facial analysis
 ECCV, pages N–NN, N0NN
N  [NN] K
Simonyan and A
V
an A
Zisserman
Deep fisher networks for large-scale image classification
NIPS, pages NNN–  NNN, N0NN
N  [NN] K
Simonyan, O
M
Parkhi, A
Vedaldi, and A
Zisserman
 Fisher vector faces in the wild
In: BMVC,, pages N–NN,  N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
ICLR,  arXiv:NN0N.NNNN, N0NN
N  [NN] Y
Song, W
Cai, Q
Li, F
Zhang, D
Feng, and H
Huang
 Fusing subcategory probabilities for texture classification
 CVPR, pages NN0N–NNNN, N0NN
N  [NN] Y
Song, Q
Li, D
Feng, J
Zou, and W
Cai
Texture image  classification with discriminative neural networks
Computational Visual Media, N(N):NNN–NNN, N0NN
N  [NN] V
Sydorov, M
Sakurada, and C
H
Lampert
Deep fisher  kernels - end to end learning of the fisher kernel GMM parameters
CVPR, pages NN0N–NN0N, N0NN
N  [NN] P
Tang, X
Wang, B
Shi, X
Bai, W
Liu, and Z
Tu
Deep  FisherNet for object classification
arXiv:NN0N.00NNN, N0NN
 N, N  [NN] Y
Tang
Deep learning with linear support vector machines
 ICML Workshop, pages N–N, N0NN
N  [N0] R
Timofte and L
J
V
Gool
A training-free classifiation  framework for textures, writers, and materials
BMVC, pages  N–NN, N0NN
N  [NN] A
Vedaldi and B
Fulkerson
Vlfeat: an open and portable library of computer vision algorithms
ACM MM, pages NNNN–  NNNN, N0N0
N  [NN] A
Vedaldi and K
Lenc
Matconvnet – convolutional neural  networks for matlab
ACM MM, pages NNN–NNN, N0NN
N  [NN] Z
Wang, H
Li, W
Ouyang, and X
Wang
Learnable histogram: statistical context features for deep neural networks
 ECCV, pages NNN–NNN, N0NN
N  [NN] J
Zhang, M
Marszalek, S
Lazebnik, and C
Schmid
Local  features and kernels for classification of texture and object  categories
Int
J
Comput
Vis., NN(N):NNN–NNN, N00N
N  NNN0SSH: Single Stage Headless Face Detector   SSH: Single Stage Headless Face Detector  Mahyar Najibi* Pouya Samangouei* Rama Chellappa Larry S
Davis  University of Maryland  najibi@cs.umd.edu {pouya,rama,lsd}@umiacs.umd.edu  Abstract  We introduce the Single Stage Headless (SSH) face detector
Unlike two stage proposal-classification detectors,  SSH detects faces in a single stage directly from the early  convolutional layers in a classification network
SSH is  headless
That is, it is able to achieve state-of-the-art results while removing the “head” of its underlying classification network – i.e
all fully connected layers in the VGG-NN  which contains a large number of parameters
Additionally,  instead of relying on an image pyramid to detect faces with  various scales, SSH is scale-invariant by design
We simultaneously detect faces with different scales in a single forward pass of the network, but from different layers
These  properties make SSH fast and light-weight
Surprisingly,  with a headless VGG-NN, SSH beats the ResNet-N0N-based  state-of-the-art on the WIDER dataset
Even though, unlike the current state-of-the-art, SSH does not use an image  pyramid and is NX faster
Moreover, if an image pyramid is deployed, our light-weight network achieves state-of-theart on all subsets of the WIDER dataset, improving the AP  by N.N%
SSH also reaches state-of-the-art results on the FDDB and Pascal-Faces datasets while using a small input  size, leading to a speed of N0 frames/second on a GPU
 N
Introduction  Face detection is a crucial step in various problems involving verification, identification, expression analysis, etc
 From the Viola-Jones [NN] detector to recent work by Hu  et al
[N], the performance of face detectors has been improved dramatically
However, detecting small faces is still  considered a challenging task
The recent introduction of  the WIDER face dataset [NN], containing a large number  of small faces, exposed the performance gap between humans and current face detectors
The problem becomes  more challenging when the speed and memory efficiency  of the detectors are taken into account
The best performing face detectors are usually slow and have high memory  *Authors contributed equally  Figure N: SSH is able to detect various face sizes in a single  CNN feed-forward pass and without employing an image  pyramid in ∼ 0.N second for an image with size N00×NN00 on a GPU
 foot-prints (e.g
[N] takes more than N second to process an image, see Section N.N) partly due to the huge number of  parameters as well as the way robustness to scale or incorporation of context are addressed
 State-of-the-art CNN-based detectors convert image  classification networks into two-stage detection systems  [N, NN]
In the first stage, early convolutional feature maps  are used to propose a set of candidate object boxes
In the  second stage, the remaining layers of the classification networks (e.g
fcN~N in VGG-NN [NN]), which we refer to as the  network “head”, are deployed to extract local features for  these candidates and classify them
The head in the classification networks can be computationally expensive (e.g
the  network head contains ∼ NN0M parameters in VGG-NN and ∼ NNM parameters in ResNet-N0N)
Moreover, in the two stage detectors, the computation must be performed for all  proposed candidate boxes
 Very recently, Hu et al
[N] showed state-of-the-art results on the WIDER face detection benchmark by using a  similar approach to the Region Proposal Networks (RPN)  [NN] to directly detect faces
Robustness to input scale is  achieved by introducing an image pyramid as an integral  NNNN    part of the method
However, it involves processing an input pyramid with an up-sampling scale up to N000 pixels per side and passing each level to a very deep network which  increased inference time
 In this paper, we introduce the Single Stage Headless  (SSH) face detector
SSH performs detection in a single  stage
Like RPN [NN], the early feature maps in a classification network are used to regress a set of predefined anchors towards faces
However, unlike two-stage detectors,  the final classification takes place together with regressing  the anchors
SSH is headless
It is able to achieve stateof-the-art results while removing the head of its underlying  network (i.e
all fully connected layers in VGG-NN), leading  to a light-weight detector
Finally, SSH is scale-invariant  by design
Instead of relying on an external multi-scale  pyramid as input, inspired by [NN], SSH detects faces from  various depths of the underlying network
This is achieved  by placing an efficient convolutional detection module on  top of the layers with different strides, each of which is  trained for an appropriate range of face scales
Surprisingly, SSH based on a headless VGG-NN, not only outperforms the best-reported VGG-NN by a large margin but also  beats the current ResNet-N0N-based state-of-the-art method  on the WIDER face detection dataset
Unlike the current  state-of-the-art, SSH does not deploy an input pyramid and  is N times faster
If an input pyramid is used with SSH as well, our light-weight VGG-NN-based detector outperforms the best reported ResNet-N0N [N] on all three subsets  of the WIDER dataset and improves the mean average precision by N% and N.N% on the validation and the test set respectively
SSH also achieves state-of-the-art results on  the FDDB and Pascal-Faces datasets with a relatively small  input size, leading to a speed of N0 frames/second
 The rest of the paper is organized as follows
Section N  provides an overview of the related works
Section N introduces the proposed method
Section N presents the experiments and Section N concludes the paper
 N
Related Works  N.N
Face Detection  Prior to the re-emergence of convolutional neural networks (CNN), different machine learning algorithms were  developed to improve face detection performance [NN, NN,  N0, NN, NN, N, NN]
However, following the success of these  networks in classification tasks [N], they were applied to  detection as well [N]
Face detectors based on CNNs significantly closed the performance gap between human and  artificial detectors [NN, NN, NN, NN, N]
However, the introduction of the challenging WIDER dataset [NN], containing  a large number of small faces, re-highlighted this gap
To  improve performance, CMS-RCNN [NN] changed the Faster  R-CNN object detector [NN] to incorporate context information
Very recently, Hu et al
proposed a face detection  method based on proposal networks which achieves stateof-the-art results on this dataset [N]
However, in addition  to skip connections, an input pyramid is processed by rescaling the image to different sizes, leading to slow detection speeds
In contrast, SSH is able to process multiple  face scales simultaneously in a single forward pass of the  network, which reduces inference time noticeably
 N.N
Single Stage Detectors and Proposal Networks  The idea of detecting and localizing objects in a single  stage has been previously studied for general object detection
SSD [NN] and YOLO [NN] perform detection and classification simultaneously by classifying a fixed grid of boxes  and regressing them towards objects
G-CNN [NN] models detection as a piece-wise regression problem and iteratively pushes an initial multi-scale grid of boxes towards objects while classifying them
However, current state-of-theart methods on the challenging MS-COCO object detection  benchmark are based on two-stage detectors[NN]
SSH is a  single stage detector; it detects faces directly from the early  convolutional layers without requiring a proposal stage
 Although SSH is a detector, it is more similar to the object proposal algorithms which are used as the first stage in  detection pipelines
These algorithms generally regress a  fixed set of anchors towards objects and assign an objectness score to each of them
MultiBox [NN] deploys clustering to define anchors
RPN [NN], on the other hand, defines  anchors as a dense grid of boxes with various scales and aspect ratios, centered at every location in the input feature  map
SSH uses similar strategies, but to localize and at the  same time detect, faces
 N.N
Scale Invariance and Context Modeling  Being scale invariant is important for detecting faces in  unconstrained settings
For generic object detection, [N, NN]  deploy feature maps of earlier convolutional layers to detect small objects
Recently, [NN] used skip connections  in the same way as [NN] and employed multiple shared  RPN and classifier heads from different convolutional layers
For face detection, CMS-RCNN [NN] used the same  idea as [N, NN] and added skip connections to the Faster  RCNN [NN]
[N] creates a pyramid of images and processes  each separately to detect faces of different sizes
In contrast, SSH is capable of detecting faces at different scales  in a single forward pass of the network without creating an  image pyramid
We employ skip connections in a similar  fashion as [NN, NN], and train three detection modules jointly  from the convolutional layers with different strides to detect  small, medium, and large faces
 In two stage object detectors, context is usually modeled  by enlarging the window around proposals [NN]
[N] models context by deploying a recurrent neural network
For  NNNN    face detection, CMS-RCNN [NN] utilizes a larger window  with the cost of duplicating the classification head
This increases the memory requirement as well as detection time
 SSH uses simple convolutional layers to achieve the same  larger window effect, leading to more efficient context modeling
 N
Proposed Method  SSH is designed to decrease inference time, have a low  memory foot-print, and be scale-invariant
SSH is a singlestage detector; i.e
instead of dividing the detection task into  bounding box proposal and classification, it performs classification together with localization from the global information extracted from the convolutional layers
We empirically show that in this way, SSH can remove the “head” of  its underlying network while achieving state-of-the-art face  detection accuracy
Moreover, SSH is scale-invariant by design and can incorporate context efficiently
 N.N
General Architecture  Figure N shows the general architecture of SSH
It is a  fully convolutional network which localizes and classifies  faces early on by adding a detection module on top of feature maps with strides of N, NN, and NN, depicted as MN, MN, and MN respectively
The detection module consists of a convolutional binary classifier and a regressor for detecting faces and localizing them respectively
 To solve the localization sub-problem, as in [NN, NN, NN],  SSH regresses a set of predefined bounding boxes called anchors, to the ground-truth faces
We employ a similar strategy to the RPN [NN] to form the anchor set
We define the  anchors in a dense overlapping sliding window fashion
At  each sliding window location, K anchors are defined which  have the same center as that window and different scales
 However, unlike RPN, we only consider anchors with aspect ratio of one to reduce the number of anchor boxes
We  noticed in our experiments that having various aspect ratios  does not have a noticeable impact on face detection precision
More formally, if the feature map connected to the  detection module Mi has a size of Wi × Hi, there would be Wi ×Hi ×Ki anchors with aspect ratio one and scales {SN  i , SN  i , 


SKi  i }
 For the detection module, a set of convolutional layers  are deployed to extract features for face detection and localization as depicted in Figure N
This includes a simple  context module to increase the effective receptive field as  discussed in section N.N
The number of output channels  of the context module, (i.e
“X” in Figures N and N) is set  to NNN for detection module MN and NNN for modules MN and MN
Finally, two convolutional layers perform bound- ing box regression and classification
At each convolution  location in Mi, the classifier decides whether the windows at the filter’s center and corresponding to each of the scales  {Sk i }K k=N  contains a face
A N× N convolutional layer with N ×K output channels is used as the classifier
For the re- gressor branch, another N×N convolutional layer with N×K output channels is deployed
At each location during the  convolution, the regressor predicts the required change in  scale and translation to match each of the positive anchors  to faces
 N.N
Scale-Invariance Design  In unconstrained settings, faces in images have varying  scales
Although forming a multi-scale input pyramid and  performing several forward passes during inference, as in  [N], makes it possible to detect faces with different scales, it  is slow
In contrast, SSH detects large and small faces simultaneously in a single forward pass of the network
Inspired  by [NN], we detect faces from three different convolutional  layers of our network using detection modules MN,MN, and MN
These modules have strides of N, NN, and NN and are designed to detect small, medium, and large faces respectively
 More precisely, the detection module MN performs de- tection from the convN-N layer in VGG-NN
Although it is  possible to place the detection module MN directly on top of convN-N, we use the feature map fusion which was previously deployed for semantic segmentation [NN], and generic  object detection [NN]
However, to decrease the memory  consumption of the model, the number of channels in the  feature map is reduced from NNN to NNN using N × N con- volutions
The convN-N feature maps are up-sampled and  summed up with the convN-N features, followed by a N × N convolutional layer
We used bilinear up-sampling in the  fusion process
For detecting larger faces, a max-pooling  layer with stride of N is added on top of the convN-N layer to increase its stride to NN
The detection module MN is placed on top of this newly added layer
 During the training phase, each detection module Mi is trained to detect faces from a target scale range as discussed in N.N
During inference, the predicted boxes from  the different scales are joined together followed by NonMaximum Suppression (NMS) to form the final detections
 N.N
Context Module  In two-stage detectors, it is common to incorporate context by enlarging the window around the candidate proposals
SSH mimics this strategy by means of simple convolutional layers
Figure N shows the context layers which  are integrated into the detection modules
Since anchors are  classified and regressed in a convolutional manner, applying  a larger filter resembles increasing the window size around  proposals in a two-stage detector
To this end, we use N× N and N × N filters in our context module
Modeling the con- text in this way increases the receptive field proportional to  the stride of the corresponding layer and as a result the tarNNNN    convN_N  Max pool   N/N  Detection   Module MN  Detection   Module MN  Dim Red   �×�  R e L  U  Dim Red   �×�  Bilinear   Upsampling Eltwise  Sum  Conv   �×�  Detection   Module MN  N N  N  C  h a  n n  e ls  NNN Channels  NNN Channels  NNN Channels  Scores  Boxes  Scores  Boxes  Scores  Boxes  C o  n v  s  N -N ~ N -N  R e L  U  R e L  U  NNN Channels  NNN Channels  Figure N: The network architecture of SSH
 � Channels  � Channels  C o  n v     � × �  � / � × � / � × � �  R eg  
 O  u tp  u t  C o  n v     � × �  � / � × � / � × � �  C ls  
 S  co re  s  Context   Module  Conv   �×�  Concat  R e L  U  Figure N: SSH detection module
 Concat  Conv   �×�  Conv   �×�  Conv   �×�  R e L  U R  e L  U  R e L  U  �/N Channels �/N Channels  �/N Channels Conv   �×�  R e L  U  Figure N: SSH context module
 get scale of each detection module
To reduce the number  of parameters, we use a similar approach as [NN] and deploy  sequential N×N filters instead of larger convolutional filters
The number of output channels of the detection module (i.e
 “X” in Figure N) is set to NNN for MN and NNN for modules MN and MN
It should be noted that our detection mod- ule together with its context filters uses fewer of parameters  compared to the module deployed for proposal generation  in [NN]
Although, more efficient, we empirically found that  the context module improves the mean average precision on  the WIDER validation dataset by more than half a percent
 N.N
Training  We use stochastic gradient descent with momentum and  weight decay for training the network
As discussed in section N.N, we place three detection modules on layers with  different strides to detect faces with different scales
Consequently, our network has three multi-task losses for the  classification and regression branches in each of these modules as discussed in Section N.N.N
To specialize each of  the three detection modules for a specific range of scales,  we only back-propagate the loss for the anchors which are  assigned to faces in the corresponding range
This is implemented by distributing the anchors based on their size  to these three modules (i.e
smaller anchors are assigned to  MN compared to MN, and MN)
An anchor is assigned to a ground-truth face if and only if it has a higher IoU than  0.N
This is in contrast to the methods based on Faster R- CNN which assign to each ground-truth at least one anchor  with the highest IoU
Thus, we do not back-propagate the  loss through the network for ground-truth faces inconsistent  with the anchor sizes of a module
 N.N.N Loss function  SSH has a multi-task loss
This loss can be formulated as  follows:  ∑  k  N  N c k  ∑  i∈Ak  ℓc(pi, gi)+  λ ∑  k  N  Nr k  ∑  i∈Ak  I(gi = N)ℓr(bi, ti) (N)  where ℓc is the face classification loss
We use standard  multinomial logistic loss as ℓc
The index k goes over the  SSH detection modules M = {Mk} K N  and Ak represents the set of anchors defined in Mk
The predicted category for the i’th anchor in Mk and its assigned ground-truth la- bel are denoted as pi and gi respectively
As discussed in  Section N.N, an anchor is assigned to a ground-truth bounding box if and only if it has an IoU greater than a threshold  (i.e
0.N)
As in [NN], negative labels are assigned to anchors  with IoU less than a predefined threshold (i.e
0.N) with any  ground-truth bounding box
N c k  is the number of anchors  in module Mk which participate in the classification loss computation
 ℓr represents the bounding box regression loss
Following [N, N, NN], we parameterize the regression space  NNNN    with a log-space shift in the box dimensions and a scaleinvariant translation and use smooth ℓN loss as ℓr
In this  parametrized space, pi represents the predicted four dimensional translation and scale shift and ti is its assigned  ground-truth regression target for the i’th anchor in module Mk
I(.) is the indicator function that limits the re- gression loss only to the positively assigned anchors, and  Nr k =  ∑ i∈Ak  I(gi = N)
 N.N
Online hard negative and positive mining  We use online negative and positive mining (OHEM) for  training SSH as described in [NN]
However, OHEM is applied to each of the detection modules (Mk) separately
That is, for each module Mk, we select the negative an- chors with the highest scores and the positive anchors with  the lowest scores with respect to the weights of the network at that iteration to form our mini-batch
Also, since  the number of negative anchors is more than the positives,  following [N], NN% of the mini-batch is reserved for the pos- itive anchors
As empirically shown in Section N.N, OHEM  has an important role in the success of SSH which removes  the fully connected layers out of the VGG-NN network
 N
Experiments  N.N
Experimental Setup  All models are trained on N GPUs in parallel using stochastic gradient descent
We use a mini-batch of N images
Our networks are fine-tuned for NNK iterations starting from a pre-trained ImageNet classification network
 Following [N], we fix the initial convolutions up to convN-N
 The learning rate is initially set to 0.0N and drops by a factor of N0 after NNK iterations
We set momentum to 0.N, and weight decay to Ne−N
Anchors with IoU> 0.N are assigned to positive class and anchors which have an IoU< 0.N with all ground-truth faces are assigned to the background class
 For anchor generation, we use scales {N, N} in MN, {N, N} in MN, and {NN, NN} in MN with a base anchor size of NN pixels
All anchors have aspect ratio of one
During training, NNN detections per module is selected for each image
During inference, each module outputs N000 best scoring anchors as detections and NMS with a threshold of 0.N is performed on the outputs of all modules together
 N.N
Datasets  WIDER dataset[NN]: This dataset contains NN, N0N im- ages with NNN, N0N annotated faces, NNN, NNN of which are in the train set, NN, NNN in the validation set and the rest are in the test set
The validation and test set are divided into  “easy”, “medium”, and “hard” subsets cumulatively (i.e
the  “hard” set contains all images)
This is one of the most challenging public face datasets mainly due to the wide variety  of face scales and occlusion
We train all models on the  Table N: Comparison of SSH with top performing methods on the validation set of the WIDER dataset
 Method easy medium hard  CMS-RCNN [NN] NN.N NN.N NN.N  HR(VGG-NN)+Pyramid [N] NN.N NN.N NN.N  HR(ResNet-N0N)+Pyramid [N] NN.N NN.0 N0.N  SSH(VGG-NN) NN.N N0.N NN.N  SSH(VGG-NN)+Pyramid NN.N NN.N NN.N  train set of the WIDER dataset and evaluate on the validation and test sets
Ablation studies are performed on the the  validation set (i.e
“hard” subset)
 FDDB[N]: FDDB contains NNNN images and NNNN annotated faces
We use this dataset only for testing
 Pascal Faces[N0]: Pascal Faces is a subset of the Pascal  VOC dataset [N] and contains NNN images annotated for face  detection
We use this dataset only to evaluate our method
 N.N
WIDER Dataset Result  We compare SSH with HR [N], CMS-RCNN [NN], Multitask Cascade CNN [NN], LDCF [N0], Faceness [NN], and  Multiscale Cascade CNN [NN]
When reporting SSH without an image pyramid, we rescale the shortest side of  the image up to NN00 pixels while keeping the largest side below NN00 pixels without changing the aspect ratio
SSH+Pyramid is our method when we apply SSH to a pyramid of input images
Like HR, a four level image pyramid  is deployed
To form the pyramid, the image is first scaled  to have a shortest side of up to N00 pixels and the longest side less than NN00 pixels
Then, we scale the image to have min sizes of N00, N00, NN00, and NN00 pixels in the pyramid
All modules detect faces on all pyramid levels, except MN which is not applied to the largest level
 Table N compares SSH with best performing methods on  the WIDER validation set
SSH without using an image  pyramid and based on the VGG-NN network outperforms  the VGG-NN version of HR by N.N%, N.N%, and N.N% in “easy”, “medium”, and “hard” subsets respectively
Surprisingly, SSH also outperforms HR based on ResNet-N0N  on the whole dataset (i.e
“hard” subset) by 0.N
In con- trast HR deploys an image pyramid
Using an image pyramid, SSH based on a light VGG-NN model, outperforms the  ResNet-N0N version of HR by a large margin, increasing the  state-of-the-art on this dataset by ∼ N%
The precision-recall curves on the test set is presented in  Figure N
We submitted the detections of SSH with an image pyramid only once for evaluation
As can be seen, SSH  based on a headless VGG-NN, outperforms the prior methods on all subsets, increasing the state-of-the-art by N.N%
 N.N
FDDB and Pascal Faces Results  In these datasets, we resize the shortest side of the input to N00 pixels while keeping the larger side less than  NNNN    0 0.N 0.N 0.N 0.N N Recall  0  0.N  0.N  0.N  0.N  N  Pr ec  is io  n  SSH(VGG-NN)-0.NNN HR(ResNet-N0N)-0.NNN CMS-RCNN-0.N0N Multitask Cascade-0.NNN LDCF+-0.NNN Multiscale Cascade-0.NNN Faceness-0.NNN  (a) Easy  0 0.N 0.N 0.N 0.N N Recall  0  0.N  0.N  0.N  0.N  N  Pr ec  is io  n  SSH(VGG-NN)-0.NNN HR(ResNet-N0N)-0.NN0 CMS-RCNN-0.NNN Multitask Cascade-0.NN0 LDCF+-0.NNN Multiscale Cascade-0.NNN Faceness-0.N0N  (b) Medium  0 0.N 0.N 0.N 0.N N Recall  0  0.N  0.N  0.N  0.N  N  Pr ec  is io  n  SSH(VGG-NN)-0.NNN HR(ResNet-N0N)-0.NNN CMS-RCNN-0.NNN Multitask Cascade-0.N0N LDCF+-0.NNN Multiscale Cascade-0.N00 Faceness-0.NNN  (c) Hard  Figure N: Comparison among the methods on the test set of WIDER face detection benchmark
 0 N00 N00 N00 N00 N000 False positives  0.0  0.N  0.N  0.N  0.N  N.0  Tr ue   p os  it iv  e  ra  te  SSH (0.NNN) HR-ER* (0.NNN) HR (0.NN0) MTCNN (0.NNN) Faster R-CNN (0.NNN) DPNMFD (0.NNN) Faceness (0.N0N) ConvNd (0.N0N) HeadHunter (0.NNN)  (a) FDDB discrete score
 0 N00 N00 N00 N00 N000 False positives  0.0  0.N  0.N  0.N  0.N  N.0  Tr ue   p os  it iv  e  ra  te  HR-ER* (0.NN0) ConvNd (0.NNN) SSH (0.NNN) Faceness (0.NNN) Faster R-CNN (0.NNN) MTCNN (0.N0N) HeadHunter (0.N0N) HR (0.NNN) DPNMFD (0.NNN)  (b) FDDB continuous score
 0.N 0.N 0.N 0.N 0.N N.0 Recall  0.N  0.N  0.N  0.N  0.N  N.0  Pr ec  is io  n  SSH (AP NN.NN) Faster R-CNN (AP NN.NN) HyperFace (AP NN.NN) Faceness (AP NN.NN) HeadHunter (AP NN.NN)  (c) Pascal-Faces
 Figure N: Comparison among the methods on FDDB and Pascal-Faces datasets
(*Note that unlike SSH, HR-ER is also  trained on the FDDB dataset in a N0-Fold Cross Validation fashion.)  N00 pixels, leading to an inference speed of more than  N0 frames/sec
We compare SSH with HR[N], HR-ER[N], ConvND[NN], Faceness[NN], Faster R-CNN(VGG-NN)[NN],  MTCNN[NN], DPNMFD[NN], and Headhunter[NN]
Figures  Na and Nb show the ROC curves with respect to the discrete  and continuous measures on the FDDB dataset respectively
 It should be noted that HR-ER also uses FDDB as a training data in a N0-fold cross validation fashion
Moreover, HR-ER and ConvND both generate ellipses to decrease the  localization error
In contrast, SSH does not use FDDB for  training, and is evaluated on this dataset out-of-the-box by  generating bounding boxes
However, as can be seen, SSH  outperforms all other methods with respect to the discrete  score
Compare to HR, SSH improved the results by N.N% and N.N% with respect to the continuous and discrete scores
 We also compare SSH with Faster R-CNN(VGGNN)[NN], HyperFace[NN], Headhunter[NN], and Faceness[NN] on the Pascal-Faces dataset
As shown in Figure  Nc, SSH achieves state-of-the-art results on this dataset
 N.N
Timing  SSH performs face detection in a single stage while removing all fully-connected layers from the VGG-NN network
This makes SSH an efficient detection algorithm
 Table N shows the inference time with respect to different  input sizes
We report average time on the WIDER validaTable N: SSH inference time with respect to different input sizes
 Max Size N00× N00 N00× N000 N00× NN00 NN00× NN00  Time NN ms NN ms N0N ms NNN ms  tion set
Timing are performed on a NVIDIA Quadro PN000  GPU
In column with max size m × M , the shortest side of the images are resized to “m” pixels while keeping the  longest side less than “M” pixels
As shown in section N.N,  and N.N, SSH outperforms HR on all datasets without an image pyramid
On WIDER we resize the image to the last  column and as a result detection takes NNN ms/image
In contrast, HR has a runtime of N0N0 ms/image, more than NX slower
As mentioned in Section N.N, a maximum input size of N00×N00 is enough for SSH to achieve state-of-the- art performance on FDDB and Pascal-Faces, with a detection speed of N0 frames/sec
If an image pyramid is used, the runtime would be dominated by the largest scale
 N.N
Ablation study: Scale-invariant design  As discussed in Section N.N, SSH uses each of its detections modules, {Mi} N  i=N , to detect faces in a certain range  of scales from layers with different strides
To better understand the impact of these design choices, we compare the  results of SSH with and without multiple detection modNNN0    0 0.N 0.N 0.N 0.N N Recall  0  0.N  0.N  0.N  0.N  N  Pr ec  is io  n  SSH-0.NNN SSH-OnlyMN-0.NNN  (a) Effect of multi-scale design
 0 0.N 0.N 0.N 0.N N Recall  0  0.N  0.N  0.N  0.N  N  Pr ec  is io  n  SSH-0.NNN SSH-NoOHEM-0.NNN  (b) Effect of OHEM
 0 0.N 0.N 0.N 0.N N Recall  0  0.N  0.N  0.N  0.N  N  Pr ec  is io  n  SSH-0.NNN SSH-NoFusion-0.N0N  (c) Effect of feature fusion
 0 0.N 0.N 0.N 0.N N Recall  0  0.N  0.N  0.N  0.N  N  Pr ec  is io  n  SSH-0.NNN SSH-FinerScales-0.NNN  (d) Effect of increasing #anchors
 Figure N: Ablation studies
All experiments are reported on the Wider Validation set
 Table N: The effect of input size on average precision
 Max Size N00× N000 N00× NN00 NN00× NN00 NN00× NN00  AP NN.N NN.N NN.N NN.0  ules
That is, we remove {MN,MN} and only detect faces with MN from convN-N in VGG-NN
However, for fair com- parison, all anchor scales in {MN,MN} are moved to MN (i.e
we use ∪N  i=N Si in MN)
Other parameters remain the  same
We refer to this simpler method as ”SSH-OnlyMN”
 As shown in Figure Na, by removing the multiple detection  modules from SSH, the AP significantly drops by ∼ NN.N% on the hard subset which contains smaller faces
Although  SSH does not deploy the expensive head of its underlying  network, results suggest that having independent simple detection modules from different layers of the network is an  effective strategy for scale-invariance
 N.N
Ablation study: The effect of input size  The input size can affect face detection precision, especially for small faces
Table N shows the AP of SSH on  the WIDER validation set when it is trained and evaluated  with different input sizes
Even at a maximum input size of  N00×NN00, SSH outperforms HR-VGGNN, which up-scales images up to N000 pixels, by N.N%, showing the effective- ness of our scale-invariant design for detecting small faces
 N.N
Ablation study: The effect of OHEM  As discussed in Section N.N, we apply hard negative and  positive mining (OHEM) to select anchors for each of our  detection modules
To show its role, we train SSH, with  and without OHEM
All other factors are the same
Figure  Nb shows the results
Clearly, OHEM is important for the  success of our light-weight detection method which does  not use the pre-trained head of the VGG-NN network
 N.N
Ablation study: The effect of feature fusion  In SSH, to form the input features for detection module MN, the outputs of convN-N and convN-N are fused to- gether
Figure Nc, shows the effectiveness of this design  choice
Although it does not have a noticeable computational overhead, as illustrated, it improves the AP on the  WIDER validation set
 N.N0
Ablation study: Selection of anchor scales  As mentioned in Section N.N, SSH uses SN = {N, N}, SN = {N, N}, SN = {NN, NN} as anchor scale sets
Fig- ure Nd compares SSH with its slight variant which uses  SN = {0.NN, 0.N, N, N, N}, SN = {N, N, N, N0, NN}, SN = {NN, N0, NN, NN, NN}
Although using a finer scale set leads to a slower inference, it also reduces the AP due to the increase in the number of False Positives
 N.NN
Qualitative Results  Figure N shows some qualitative results on the Wider validation set
The colors encode the score of the classifier
 Green and blue represent score N.0 and 0.N respectively
 N
Conclusion  We introduced the SSH detector, a fast and lightweight  face detector that, unlike two-stage proposal/classification  approaches, detects faces in a single stage
SSH localizes  and detects faces simultaneously from the early convolutional layers in a classification network
SSH is able to  achieve state-of-the-art results without using the “head” of  its underlying classification network (i.e
fc layers in VGGNN)
Moreover, instead of processing an input pyramid,  SSH is designed to be scale-invariant while detecting different face scales in a single forward pass of the network
 SSH achieves state-of-the-art performance on the challenging WIDER dataset as well as FDDB and Pascal-Faces  while reducing the detection time considerably
 Acknowledgement This research is based upon work supported by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity  (IARPA), via IARPA R&D Contract No
N0NN-NN0NNN000NN
The  views and conclusions contained herein are those of the authors  and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of the  ODNI, IARPA, or the U.S
Government
The U.S
Government is  authorized to reproduce and distribute reprints for Governmental  purposes notwithstanding any copyright annotation thereon
 NNNN    Figure N: Qualitative results of SSH on the validation set of the WIDER dataset
Green and blue represent a classification  score of N.0 and 0.N respectively
 NNNN    References  [N] S
Bell, C
Lawrence Zitnick, K
Bala, and R
Girshick
 Inside-outside net: Detecting objects in context with skip  pooling and recurrent neural networks
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N  [N] D
Chen, S
Ren, Y
Wei, X
Cao, and J
Sun
Joint cascade  face detection and alignment
In European Conference on  Computer Vision, pages N0N–NNN
Springer, N0NN
N  [N] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
International journal of computer vision, NN(N):N0N–  NNN, N0N0
N  [N] R
Girshick
Fast r-cnn
In Proceedings of the IEEE International Conference on Computer Vision, pages NNN0–NNNN,  N0NN
N, N  [N] R
Girshick
Fast r-cnn
In Proceedings of the IEEE International Conference on Computer Vision, pages NNN0–NNNN,  N0NN
N  [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In Proceedings of the IEEE conference on  computer vision and pattern recognition, pages NN0–NNN,  N0NN
N, N  [N] P
Hu and D
Ramanan
Finding tiny faces
Proceedings  of the IEEE Conference on Computer Vision and Pattern  Recognition, N0NN
N, N, N, N, N  [N] V
Jain and E
G
Learned-Miller
Fddb: A benchmark for  face detection in unconstrained settings
UMass Amherst  Technical Report, N0N0
N  [N] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in neural information processing systems, pages  N0NN–NN0N, N0NN
N  [N0] H
Li, G
Hua, Z
Lin, J
Brandt, and J
Yang
Probabilistic elastic part model for unsupervised face detector adaptation
In Proceedings of the IEEE International Conference  on Computer Vision, pages NNN–N00, N0NN
N  [NN] H
Li, Z
Lin, J
Brandt, X
Shen, and G
Hua
Efficient  boosted exemplar-based face detection
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNN0, N0NN
N  [NN] H
Li, Z
Lin, X
Shen, J
Brandt, and G
Hua
A convolutional neural network cascade for face detection
In Proceedings of the IEEE Conference on Computer Vision and Pattern  Recognition, pages NNNN–NNNN, N0NN
N  [NN] Y
Li, B
Sun, T
Wu, and Y
Wang
face detection with endto-end integration of a convnet and a Nd model
In European  Conference on Computer Vision, pages NN0–NNN
Springer,  N0NN
N  [NN] T.-Y
Lin, P
Dollár, R
Girshick, K
He, B
Hariharan, and  S
Belongie
Feature pyramid networks for object detection
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, N0NN
N, N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European Conference on Computer Vision, pages NN0–NNN
Springer, N0NN
N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.Y
Fu, and A
C
Berg
Ssd: Single shot multibox detector
 In European Conference on Computer Vision, pages NN–NN
 Springer, N0NN
N  [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNN0, N0NN
N, N  [NN] M
Mathias, R
Benenson, M
Pedersoli, and L
Van Gool
 Face detection without bells and whistles
In European Conference on Computer Vision, pages NN0–NNN
Springer, N0NN
 N, N  [NN] M
Najibi, M
Rastegari, and L
S
Davis
G-cnn: an iterative  grid based object detector
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NNNN–NNNN, N0NN
N, N  [N0] E
Ohn-Bar and M
M
Trivedi
To boost or not to boost? on  the limits of boosted trees for object detection
NNrd International Conference on Pattern Recognition, N0NN
N  [NN] R
Ranjan, V
M
Patel, and R
Chellappa
A deep pyramid  deformable part model for face detection
In Nth International Conference on Biometrics Theory, Applications and  Systems (BTAS), pages N–N
IEEE, N0NN
N  [NN] R
Ranjan, V
M
Patel, and R
Chellappa
Hyperface: A deep  multi-task learning framework for face detection, landmark  localization, pose estimation, and gender recognition
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNN–NNN, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  Advances in neural information processing systems, pages  NN–NN, N0NN
N, N, N, N, N  [NN] A
Shrivastava, A
Gupta, and R
Girshick
Training regionbased object detectors with online hard example mining
In  Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNN–NNN, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In Proceedings of the IEEE  Conference on Computer Vision and Pattern Recognition,  pages N–N, N0NN
N  [NN] C
Szegedy, S
Reed, D
Erhan, D
Anguelov, and S
Ioffe
 Scalable, high-quality object detection
arXiv preprint  arXiv:NNNN.NNNN, N0NN
N, N  [NN] P
Viola and M
Jones
Rapid object detection using a boosted  cascade of simple features
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, volume N, pages I–I
IEEE, N00N
N, N  NNNN    [N0] J
Yan, X
Zhang, Z
Lei, and S
Z
Li
Face detection by structural models
Image and Vision Computing,  NN(N0):NN0–NNN, N0NN
N  [NN] B
Yang, J
Yan, Z
Lei, and S
Z
Li
Aggregate channel features for multi-view face detection
In IEEE International  Joint Conference on Biometrics (IJCB), pages N–N
IEEE,  N0NN
N  [NN] B
Yang, J
Yan, Z
Lei, and S
Z
Li
Convolutional channel  features
In Proceedings of the IEEE international conference on computer vision, pages NN–N0, N0NN
N  [NN] S
Yang, P
Luo, C.-C
Loy, and X
Tang
From facial parts  responses to face detection: A deep learning approach
In  Proceedings of the IEEE International Conference on Computer Vision, pages NNNN–NNNN, N0NN
N  [NN] S
Yang, P
Luo, C.-C
Loy, and X
Tang
From facial parts  responses to face detection: A deep learning approach
In  Proceedings of the IEEE International Conference on Computer Vision, pages NNNN–NNNN, N0NN
N, N  [NN] S
Yang, P
Luo, C.-C
Loy, and X
Tang
Wider face: A  face detection benchmark
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NNNN–NNNN, N0NN
N, N, N  [NN] S
Zagoruyko, A
Lerer, T.-Y
Lin, P
O
Pinheiro, S
Gross,  S
Chintala, and P
Dollár
A multipath network for object  detection
arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] K
Zhang, Z
Zhang, Z
Li, and Y
Qiao
Joint face detection  and alignment using multitask cascaded convolutional networks
IEEE Signal Processing Letters, NN(N0):NNNN–NN0N,  N0NN
N, N  [NN] C
Zhu, Y
Zheng, K
Luu, and M
Savvides
Cms-rcnn: contextual multi-scale region-based cnn for unconstrained face  detection
pages NN–NN, N0NN
N, N, N  [NN] X
Zhu and D
Ramanan
Face detection, pose estimation,  and landmark localization in the wild
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN
IEEE, N0NN
N  NNNNBoosting Image Captioning With Attributes   Boosting Image Captioning with Attributes∗  Ting Yao †, Yingwei Pan ‡, Yehao Li §, Zhaofan Qiu ‡, and Tao Mei †  † Microsoft Research, Beijing, China ‡ University of Science and Technology of China, Hefei, China  § Sun Yat-Sen University, Guangzhou, China  {tiyao, tmei}@microsoft.com, {panyw.ustc, yehaoli.sysu, zhaofanqiu}@gmail.com  Abstract  Automatically describing an image with a natural language has been an emerging challenge in both fields of  computer vision and natural language processing
In this  paper, we present Long Short-Term Memory with Attributes  (LSTM-A) - a novel architecture that integrates attributes  into the successful Convolutional Neural Networks (CNNs)  plus Recurrent Neural Networks (RNNs) image captioning  framework, by training them in an end-to-end manner
Particularly, the learning of attributes is strengthened by integrating inter-attribute correlations into Multiple Instance  Learning (MIL)
To incorporate attributes into captioning,  we construct variants of architectures by feeding image  representations and attributes into RNNs in different ways  to explore the mutual but also fuzzy relationship between  them
Extensive experiments are conducted on COCO image captioning dataset and our framework shows clear improvements when compared to state-of-the-art deep models
More remarkably, we obtain METEOR/CIDEr-D of  NN.N%/N00.N% on testing data of widely used and publicly  available splits in [N0] when extracting image representations by GoogleNet and achieve superior performance on  COCO captioning Leaderboard
 N
Introduction  Accelerated by tremendous increase in Internet bandwidth and proliferation of sensor-rich mobile devices, image data has been generated, published and spread explosively, becoming an indispensable part of today’s big data
This has encouraged the development of advanced techniques for a broad range of image understanding applications
A fundamental issue that underlies the success of  these technological advances is the recognition [N, NN, NN]
 Recently, researchers have strived to automatically describe  ∗This work was performed when Yingwei Pan, Yehao Li and Zhaofan Qiu were visiting Microsoft Research as research interns
 the content of an image with a complete and natural sentence, which has a great potential impact for instance on  robotic vision or helping visually impaired people
Nevertheless, this problem is very challenging, as description generation model should capture not only the objects/scenes presented in the image, but also be capable of expressing how  the objects/scenes relate to each other in a nature sentence
 The main inspiration of recent attempts on this problem  [N, NN, NN] are from the advances by using RNNs in machine  translation [NN], which is to translate a text from one language (e.g., English) to another (e.g., Chinese)
The basic  idea is to perform a sequence to sequence learning for translation, where an encoder RNN reads the input sequential  sentence, one word at a time till the end of the sentence and  then a decoder RNN is exploited to generate the sentence  in target language, one word at each time step
Following  this philosophy, it is natural to employ a CNN instead of the  encoder RNN for image captioning, which is regarded as an  image encoder to produce image representations
 While encouraging performances are reported, these CNN plus RNN image captioning methods translate directly  from image representations to language, without explicitly  taking more high-level semantic information from images  into account
On the other hand, attributes are properties  observed in images with rich semantic cues and have been  proved to be effective in visual recognition [NN]
Therefore,  a valid question is how to incorporate high-level image attributes into CNN plus RNN image captioning architecture  as complementary knowledge in addition to image representations
We investigate particularly in this paper the architectures by exploiting the mutual relationship between  image representations and attributes for enhancing image  description generation
Specifically, to better demonstrate  the impact of simultaneously utilizing the two kinds of representations, we devise variants of architectures by feeding  them into RNN in different placements and moments, e.g.,  leveraging only attributes, inserting image representations  first and then attributes or vice versa, and inputting image  representations/attributes once or at each time step
MoreNNNN    over, considering attributes are vital to our proposal, we endow the Multiple Instance Learning (MIL) framework with  more power of exploring inter-attribute correlations
 The main contribution of this work is the proposal of attribute augmented architectures by integrating the attributes  into CNN plus RNN image captioning framework, which  is a problem not yet fully understood in the literature
By  leveraging more knowledge for building richer representations and description models, our work takes a further step  forward to enhance image captioning
More importantly,  the utilization of attributes also has a great potential to be an  elegant solution of generating open-vocabulary sentences,  making image captioning system really practical
 N
Related Work  The research on image captioning has proceeded along three different dimensions: template-based methods  [NN, NN, NN], search-based approaches [N, N], and languagebased models [N, NN, NN, NN, NN, NN, NN]
 The template-based methods firstly align each sentence  fragments (e.g., subject, verb, object) with detected words  from image content and then generate the sentence with predefined language templates
Obviously, most of them highly depend on the templates of sentence and always generate  sentences with syntactical structure
For example, Kulkarni  et al
employ Conditional Random Field (CRF) model to  predict labeling based on the detected objects, attributes and  prepositions, and then generate sentence with a template by  filling in slots with the most likely labeling [NN]
Similarly,  Yang et al
utilize HMM to select the best objects, scenes,  verbs, and prepositions with the highest log-likelihood ratio  for template-based sentence generation in [NN]
 Search-based approaches “generate” sentence for an image by selecting the most semantically similar sentences  from sentence pool or directly copying sentences from other  visually similar images
This direction indeed can achieve  human-level descriptions as all sentences are from existing  human-generated sentences
The need to collect humangenerated sentences, however, makes the sentence pool hard  to be scaled up
Moreover, the approaches in this dimension  cannot generate novel descriptions
For instance, in [N], an  intermediate meaning space based on the triplet of object, action, and scene is proposed to measure the similarity  between image and sentence, where the top sentences are  regarded as the generated sentences for the target image
 Recently, a simple k-nearest neighbor retrieval model is u- tilized in [N] and the best or consensus caption is selected  from the returned candidate captions, which even performs  as well as several state-of-the-art language-based models
 Different from template-based and search-based models,  language-based models aim to learn the probability distribution in the common space of visual content and textual sentence to generate novel sentences with more flexible syntactical structures
In this direction, recent works explore such  probability distribution mainly by using neural networks for  image captioning
For instance, in [NN], Vinyals et al
propose an end-to-end neural networks architecture by utilizing LSTM to generate sentence for an image, which is further incorporated with attention mechanism in [NN] to automatically focus on salient objects when generating corresponding words
More recently, in [NN], high-level concepts/attributes are shown to obtain clear improvements on image captioning when injected into existing state-of-the-art  RNN-based model and such attributes are further utilized  as semantic attention [NN] to enhance image captioning
In  another work by Yao et al
[NN], attribute/object detectors  are developed and leveraged into image captioning to describe novel objects
 In short, our work in this paper belongs to the languagebased models
Different from most of the aforementioned  language-based models which mainly focus on sentence  generation by solely depending on image representations  [N, NN, NN, NN] or attributes [NN], our work contributes by  studying not only jointly exploiting image representations  and attributes for image captioning, but also how the architecture can be better devised by exploring mutual relationship in between
It is also worth noting that [NN] also  involves attributes for image captioning
Ours is fundamentally different in the way that [NN] is as a result of utilizing  attributes to model semantic attention to the locally previous words, as opposed to holistically employing attributes  as a kind of complementary representations in this work
 N
Boosting Image Captioning with Attributes  In this paper, we devise our CNN plus RNN architectures  to generate descriptions for images under the umbrella of  additionally incorporating the detected high-level attributes
 Specifically, we begin this section by presenting the problem formulation
Then, an attributes prediction method by  further exploring inter-attribute correlations is provided
Finally, five variants of our image captioning frameworks with  attributes are investigated and discussed
 N.N
Problem Formulation  Suppose we have an image I to be described by a tex- tual sentence S , where S = {wN, wN, ..., wNs} consist- ing of Ns words
Let I ∈ R  Dv and wt ∈ R Ds denote the Dv-dimensional image representations of the im- age I and the Ds-dimensional textual features of the t-th word in sentence S , respectively
Furthermore, we have feature vector A ∈ RDa to represent the probability dis- tribution over all the high-level attributes A for image I , where A = {aN, aN, ..., aDa} consisting of Da attributes in the whole image captioning dataset
More details about  how we mine and represent the attributes will be elaborated in Section N.N
Taking inspiration from the recent sucNNNN    cesses of probabilistic sequence models leveraged in statistical machine translation [NN] and image/video captioning [N0, NN, NN], we aim to formulate our image captioning  models in an end-to-end fashion based on RNNs which encode the given image and/or its detected attributes into a  fixed dimensional vector and then decode it to the target  output sentence
Hence, the sentence generation problem  we explore here can be formulated by minimizing the following energy loss function as  E(I,A,S) = − log Pr (S|I,A), (N)  which is the negative log probability of the correct textual  sentence given the image and detected attributes
 Since the model produces one word in the sentence at  each time step, it is natural to apply chain rule to model the  joint probability over the sequential words
Thus, the log probability of the sentence is given by the sum of the log probabilities over the word and can be expressed as  log Pr (S|I,A) =  Ns ∑  t=N  log Pr (wt| I,A,w0, 


,wt−N)
(N)  By minimizing this loss, the contextual relationship among  the words in the sentence can be guaranteed given the image  and its detected attributes
 We formulate this task as a variable-length sequence to  sequence problem and model the parametric distribution  Pr (wt| I,A,w0, 


,wt−N) in Eq.(N) with Long Short- Term Memory (LSTM) network, which is a widely used  type of RNN and can capture long-term information in the  sequential data by mapping sequences to sequences
 N.N
Attributes Prediction  An image generally contains not only multiple semantic  attributes but also the interactions between the attributes
 To detect attributes from images, one way is to train Fully Convolutional Networks (FCNs) by using the weaklysupervised multi-label classification approach of Multiple  Instance Learning (MIL) in [N]
This method can easily  predict the attributes probability distribution over massive  attributes, but leaving the inherent semantic correlations between attributes unexploited as all the attributes detectors  are learnt independently
To further explore the semantic correlations between attributes, a new MIL-based model with Inter-Attributes Correlations (MIL-IAC) is devised
 Technically, for an attribute aj , one image I is regarded as a positive bag of regions (instances) if aj exists in image I’s ground-truth sentences, and negative bag otherwise
By inputting all the bags into a noisy-OR MIL model [N], the  probability of the bag bI which contains attribute aj is mea- sured on the probabilities of all the regions in the bag:  Pr aj  I = N −  ∏  ri∈bI  (  N − p aj  i  )  , (N)  where p aj i is the probability of the attribute aj predicted by  region ri
We calculate p aj i through a sigmoid layer after the  last convolutional layer in the fully convolutional network:  p aj  i =  N  N + e −T⊤  j ri  , (N)  where Tj ∈ R Dt denotes the detection parameter matrix in  sigmoid layer for measuring the prediction score of j-th at- tribute aj and ri is the corresponding representation for im- age region ri
In particular, the dimension of convolutional activations from the last convolutional layer is x × x ×Dt and Dt represents the representation dimension of each re- gion, resulting in x × x response map which preserves the spatial dependency of the image
Then, a cross entropy loss  is calculated based on the probabilities of all the attributes  at the top of the whole FCNs architecture as  lc(I) = −  Da ∑  j=N  [  I(Cj=N) log (  Pr aj  I  )  + (N − I(Cj=N)) log (  N − Pr aj  I  )  ]  , (N)  where Pr aj I is measured as in Eq.(N), the indicator function  Icondition = N if condition is true; otherwise Icondition = 0, and Cj denotes the j-th element in attributes label vector C
Note that each element of the attributes label vector C ∈ {0, N}Da is an attribute indicator
The indicator is N if the image contains this attribute otherwise the indicator is 0
 Inspired by the idea of structure preservation or manifold  regularization in [NN, NN], the inter-attribute correlation here  is integrated into the learning of attributes detector as a regularizer in the sigmoid layer to further explore the inherent  semantic correlations between attributes
This regularizer  indicates that the detectors, i.e., detection parameter matrices in sigmoid layer, of semantically relevant attributes  should be similar
The estimation of the underlying semantic correlations can be measured by the appropriate pairwise  similarity between attributes
Specifically, the regularization of inter-attribute correlations could be given by  la(T) =  Da ∑  m,n=N  Smn‖Tm − Tn‖ N , (N)  where S ∈ RDa×Da is the affinity matrix defined on the at- tributes, T ∈ RDt×Da is the whole detection parameter ma- trix in sigmoid layer, and Tm denotes the m-th column of T representing the detection parameter matrix for attribute  am
It is reasonable to minimize Eq.(N), since it will incur a heavy penalty if the distance between two similar detection parameter matrices is very far
There are many ways  of defining the affinity matrix S
Here, we calculate the  elements through the normalized cosine similarity between  two attributes: Smn =  am · an  ‖am‖ ‖an‖ , (N)  where am is a N00-dimensional word representation gener- ated from wordNvector neural network [NN] for attribute am
Please note that each cosine similarity score Smn is further  linearly normalized into the range of [0, N]
 NNNN    AttributesImage  LSTM LSTM  Attributes Image  LSTM LSTM  Attributes  LSTM  w0  LSTM  wN  wN  LSTM  wN  wNs-N  LSTM  wNs  ..
 Attributes  LSTM  Image  w0  LSTM  wN  wN  LSTM  wN  wNs-N  LSTM  wNs  ...LSTM  Attributes  Image  w0  LSTM  wN  wN  LSTM  wN  wNs-N  LSTM  wNs  ..
 Figure N
Five variants of our LSTM-A framework (better viewed in color)
 By defining the graph Laplacian L = D − S, where D is a diagonal matrix with its elements defined as Dmm =∑  n Smn, Eq.(N) can be rewritten as  la(T) = tr(TLT ⊤ )
(N)  By minimizing this term, the inherent semantic corrections  between attributes could be preserved in the learnt attributes  detectors
The overall objective function integrates the cross  entropy loss in Eq.(N) on the image set I and inter-attribute correlations regularization in Eq.(N)
Hence, we obtain the  following overall loss objective:  l = λ ∑  I∈I  lc(I) + (N − λ) la(T), (N)  where λ is the tradeoff parameter
After optimizing the w- hole FCN architecture with the overall loss objective in Eq.(N), we complete the learning of our MIL-IAC attributes  prediction model and treat its final prediction scores on all  the attributes as A
 N.N
Long Short-Term Memory with Attributes  Unlike the existing image captioning models in [N, NN]  which solely encode image representations for sentence  generation, our proposed Long Short-Term Memory with  Attributes (LSTM-A) model additionally integrates the detected high-level attributes into LSTM
We devise five variants of LSTM-A for involvement of two design purposes
 The first purpose is about where to feed attributes into LSTM and three architectures, i.e., LSTM-AN (leveraging only  attributes), LSTM-AN (inserting image representations first)  and LSTM-AN (feeding attributes first), are derived from  this view
The second is about when to input attributes or  image representations into LSTM and we design LSTMAN (inputting image representations at each time step) and  LSTM-AN (inputting attributes at each time step) for this  purpose
An overview of LSTM-A is depicted in Figure N
 N.N.N LSTM-AN (Leveraging only Attributes)  Given the detected attributes, one natural way is to directly inject the attributes as representations at the initial time  to inform the LSTM about the high-level attributes
This  kind of architecture with only attributes input is named  as LSTM-AN
It is also worth noting that the attributesbased model in [NN] is similar to LSTM-AN and can be regarded as one special case of our LSTM-A
Given the attribute representations A and the corresponding sentence  W ≡ [w0,wN, ...,wNs ], the LSTM updating procedure in LSTM-AN is as  x −N  = TaA,  x t = Tswt, and h  t = f  (  x t )  , t ∈ {0, 


, Ns − N} , (N0)  where De is the dimensionality of LSTM input, Ta ∈ R  De×Da and Ts ∈ R De×Ds is the transformation matrix  for attribute representations and textual features of word,  respectively, f is the updating function within LSTM unit, and ht is the cell output of the LSTM unit
Please note that  for the input sentence W ≡ [w0,wN, ...,wNs ], we take w0 as the start sign word to inform the beginning of sentence  and wNs as the end sign word which indicates the end of  sentence
Both of the special sign words are included in our  vocabulary
Most specifically, at the initial time step, the  attribute representations are transformed as the input to LSTM, and then in the next steps, word embedding xt will be  input into the LSTM along with the previous step’s hidden  state ht−N
In each time step (except the initial step), we use  the LSTM cell output ht to predict the next word through a  softmax layer
 N.N.N LSTM-AN (Inserting image first)  To further leverage both image representations and highlevel attributes in the encoding stage of our LSTM-A, we  design the second architecture LSTM-AN by treating both  of them as atoms in the input sequence to LSTM
Specifically, at the initial step, the image representations I are  firstly transformed into LSTM to inform the LSTM about  the image content, followed by the attribute representations  A which are encoded into LSTM at the next time step to  inform the high-level attributes
Then, LSTM decodes each  NNNN    output word based on previous word xt and previous step’s  hidden state ht−N, which is similar to the decoding stage in  LSTM-AN
The LSTM updating procedure in LSTM-AN is  designed as  x −N  = TvI and x −N  = TaA,  x t = Tswt, and h  t = f  (  x t )  , t ∈ {0, 


, Ns − N} , (NN)  where Tv ∈ R De×Dv is the transformation matrix for image representations
 N.N.N LSTM-AN (Feeding attributes first)  The third design LSTM-AN is similar to LSTM-AN as  both designs utilize image representations and high-level  attributes to form the input sequence to LSTM in the encoding stage, except that the orders of encoding are different
In LSTM-AN, the attribute representations are firstly  encoded into LSTM and then the image representations are  transformed into LSTM at the second time step
The whole  LSTM updating procedure in LSTM-AN is as  x −N  = TaA and x −N  = TvI,  x t = Tswt, and h  t = f  (  x t )  , t ∈ {0, 


, Ns − N} 
(NN)  N.N.N LSTM-AN (Inputting image each time step)  Different from the former three designed architectures  which mainly inject high-level attributes and image representations at the encoding stage of LSTM, we next modify  the decoding stage in our LSTM-A by additionally incorporating image representations or high-level attributes
More  specifically, in LSTM-AN, the attribute representations are  injected once at the initial step to inform the LSTM about  the high-level attributes, and then image representations are  fed at each time step as an extra input to LSTM to emphasize the image content frequently among memory cells in  LSTM
Hence, the LSTM updating procedure in LSTM-AN is:  x −N  = TaA,  x t = Tswt + TvI, and h  t = f  (  x t )  , t ∈ {0, 


, Ns − N} 
(NN)  N.N.N LSTM-AN (Inputting attributes each time step)  The last design LSTM-AN is similar to LSTM-AN except  that it firstly encodes image representations and then feeds attribute representations as an additional input to LSTM  at each step in decoding stage to emphasize the high-level  attributes frequently
Accordingly, the LSTM updating procedure in LSTM-AN is as  x −N  = TvI,  x t = Tswt + TaA, and h  t = f  (  x t )  , t ∈ {0, 


, Ns − N} 
(NN)  N
Experiments  We conducted the experiments and evaluated our approaches on COCO captioning dataset (COCO) [NN]
 N.N
Dataset and Experimental Settings  The dataset, COCO, is the most popular benchmark for  image captioning, which contains NN,NNN training images  and N0,N0N validation images
There are N human-annotated  descriptions per image
As the annotations of the official  testing set are not publicly available, we follow the widely  used settings in [NN, NN] and take NN,NNN images for training,  N,000 for validation and N,000 for testing
 Data Preprocessing
Following [N0], we convert all the  descriptions in training set to lower case and discard rare  words which occur less than N times, resulting in the final  vocabulary with N,NNN unique words in COCO dataset
 Features and Parameter Settings
Each word in the  sentence is represented as “one-hot” vector (binary index  vector in a vocabulary)
For image representations, we  take the output of N,0NN-way poolN/N × N sN layer from GoogleNet [NN] pre-trained on Imagenet ILSVRCNN dataset  [NN]
For attribute representations, we select N,000 most  common words on COCO as the high-level attributes and  train our MIL-IAC attributes prediction model purely on the  training data of COCO, resulting in the final N,000-way vector of probabilities of attributes
The tradeoff parameter λ is empirically set as 0.N
The dimension of the input and hidden layers in LSTM of LSTM-A are both set to N,0NN
 Implementation Details
We mainly implement our image captioning models based on Caffe [N], which is one  of widely adopted deep learning frameworks
Specifically, with an initial learning rate 0.0N and mini-batch size of  N,0NN, the objective value can decrease to NN% of the initial  loss and reach a reasonable result after N0,000 iterations
 Testing Strategies
For sentence generation in testing  stage, we adopt the beam search strategy which selects the  top-k best sentences at each time step and considers them as the candidates to generate new top-k best sentences at the next time step
The beam size k is empirically set to N
 Evaluation Metrics
For the evaluation of our proposed  models, we adopt five types of metrics: BLEU@N [NN], METEOR [N], ROUGE-L [NN], CIDEr-D [N0] and SPICE  [N]
All the metrics are computed by using the codesN released by COCO Evaluation Server [N]
 N.N
Compared Approaches  To verify the merit of our LSTM-A models, we compared the following state-of-the-art methods
 (N) NIC & LSTM [NN]: NIC is the standard RNN-based  model which only injects image into LSTM at the initial  time step
We directly extract results reported in [NN] and  Nhttps://github.com/tylin/coco-caption  NNNN  https://github.com/tylin/coco-caption   Table N
Performance of our proposed models and other state-of-the-art methods on COCO, where B@N , M, R, C and S are short for  BLEU@N , METEOR, ROUGE-L, CIDEr-D and SPICE scores
All values are reported as percentage (%)
 Model B@N B@N B@N B@N M R C S  NIC [NN] NN.N NN.N N0.N N0.N - - - LRCN [N] NN.N NN.N NN.0 NN.N NN.N N0.N NN.N NN.N  HA [NN] NN.N N0.N NN.N NN NN - - SA [NN] N0.N NN.N NN.N NN.N NN.N - - ATT [NN] N0.N NN.N N0.N N0.N NN.N - - SC [NN] NN NN.N N0.N NN.N NN.N - NN.N LSTM [NN] NN.N NN.N NN NN.N NN.N N0.N NN.N NN  LSTM-AN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  LSTM-AN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  LSTM-AN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  LSTM-AN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  LSTM-AN NN.N NN.N NN.0 NN.N NN.N NN.0 N00.N NN.N  LSTM-A∗ NN.N NN.N NN.N NN.N NN.N NN.N NN0.N NN.N  name this run as NIC
Moreover, for fair comparison, we  also include our implementation of NIC, named as LSTM
 (N) LRCN [N]: LRCN inputs both image representations  and previous word into LSTM at each time step
 (N) Hard-Attention (HA) & Soft-Attention (SA) [NN]: Spatial attention on convolutional features of an image is incorporated into the encoder-decoder framework through two kinds of mechanisms: N) “hard” stochastic attention equivalently by reinforce learning and N) “soft” deterministic  attention with standard back-propagation
 (N) ATT [NN]: ATT utilizes attributes as semantic attention to combine image and attributes in RNN for captioning
 (N) Sentence-Condition (SC) [NN]: Sentence-condition  exploits text-conditional semantic attention to generate semantic guidance for sentence generation by conditioning  image features on current text content
 (N) MSR Captivator [N]: MSR Captivator employs both  Multimodal Recurrent Neural Network (MRNN) and Maximum Entropy Language Model (MELM) [N] for sentence  generation
Deep Multimodal Similarity Model (DMSM)  [N] is further exploited for sentence re-ranking
 (N) CaptionBot [NN]: CaptionBot is a publicly image captioning systemN which is mainly built on vision models by  using Deep residual networks (ResNets) [N] to detect visual  concepts, MELM [N] language model for sentence generation and DMSM [N] for caption ranking
 (N) LSTM-A: LSTM-AN ∼ LSTM-AN are five variants derived from our proposed LSTM-A framework
In addition, LSTM-A∗ is an oracle run that inputs ground-truth attributes into the LSTM-AN architecture
 N.N
Performance Comparison on COCO  Table N shows the performances of different models on COCO image captioning dataset
It is worth noting that the performances of different approaches here  Nhttps://www.captionbot.ai  are based on different image representations
Specifically, VGG architecture [NN] is utilized as image feature extractor in the methods of Hard-Attention & Soft-Attention  and Sentence-Condition, while GoogleNet [NN] is exploited in NIC, LRCN, ATT, LSTM and our LSTM-A
In view  that the GoogleNet and VGG features are comparable, we  compare directly with results
Overall, the results across  eight evaluation metrics consistently indicate that our proposed LSTM-A exhibits better performance than all the  state-of-the-art techniques including non-attention models  (NIC, LSTM, LRCN) and attention-based methods (HardAttention, Soft-Attention, ATT, Sentence-Condition)
In  particular, the CIDEr-D and SPICE can achieve N00.N% and  NN.N%, respectively, when extracting image representations by GoogleNet
LSTM-AN inputting only high-level attributes as representations makes the relative improvement  over LSTM which feeds into image representations instead  by NN.N%, N.N%, N.N%, NN.N% and NN.N% in BLEU@N, METEOR, ROUGR-L, CIDEr-D and SPICE, respectively
 The results basically indicate the advantage of exploiting  high-level attributes than image representations for image  captioning
Furthermore, by additionally incorporating attributes to LSTM model, LSTM-AN, LSTM-AN and LSTMAN lead to a performance boost, indicating that image representations and attributes are complementary and thus have  mutual reinforcement for image captioning
Similar in spirit, LSTM-AN improves LRCN by further taking attributes  into account
There is a significant performance gap between ATT and LSTM-AN
Though both runs involve the  utilization of image representations and attributes, they are  fundamentally different in the way that the performance of  ATT is as a result of modulating the strength of attention on  attributes to the previous words, and LSTM-AN is by employing attributes as auxiliary knowledge to complement  image representations
This somewhat reveals the weakness of semantic attention model, where the prediction errors will accumulate along the generated sequence
 NNNN  https://www.captionbot.ai   Table N
Leaderboard of the published state-of-the-art image captioning models on the online COCO testing server, where B@N , M, R,  and C are short for BLEU@N , METEOR, ROUGE-L, and CIDEr-D scores
All values are reported as percentage (%)
 Model B@N B@N B@N B@N M R C  cN cN0 cN cN0 cN cN0 cN cN0 cN cN0 cN cN0 cN cN0  LSTM-AN (Ours) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN NN.N NN.N N0.N NNN NNN  Watson Multimodal [NN] NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NNN.N NNN.N  G-RMI(PG-SPIDEr-TAG) [NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0N.N N0N.N  MetaMind/VT GT [NN] NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 N0.N N0N.N N0N.N  reviewnet [NN] NN.0 N0.0 NN.0 NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  ATT [NN] NN.N N0 NN.N NN.N NN.N N0.N NN.N NN.N NN NN.N NN.N NN.N NN.N NN.N  Google [NN] NN.N NN.N NN.N N0.N N0.N NN.N N0.N NN.N NN.N NN.N NN NN.N NN.N NN.N  MSR Captivator [N] NN.N N0.N NN.N NN.N N0.N NN N0.N N0.N NN.N NN.N NN.N NN NN.N NN.N  Table N
BLEU@N, METEOR, ROUGE-L, CIDEr-D, and SPICE  scores of our proposed LSTM-AN with attributes learnt by different attributes prediction models on COCO
 Model B@N M R C S  Fine-tune N0.N NN.N NN.N NN.N NN.N  MIL [N] NN.N NN.N NN.N NN.N NN.N  MIL-IAC NN.N NN.N NN.N NN.N NN.N  Compared to LSTM-AN, LSTM-AN which is augmented by integrating image representations performs better, but  the performances are lower than LSTM-AN
The results  indicate that LSTM-AN, in comparison, benefits from the  mechanism of first feeding high-level attributes into LSTM  instead of starting from inserting image representations in  LSTM-AN
The chance that a good start point can be attained and lead to performance gain is better
LSTM-AN feeding the image representations at each time step yields  inferior performances to LSTM-AN, which only inputs image representations once
We speculate that this may because the noise in the image can be explicitly accumulated  and thus the network overfits more easily
In contrast, the  performances of LSTM-AN which feeds attributes at each  time step show the improvements on LSTM-AN
The results  demonstrate that the high-level attributes are more accurate  and easily translated into human understandable sentence
 Among the five proposed LSTM-A architectures, LSTMAN achieves the best performances in terms of BLEU@N  and METEOR, while LSTM-AN performs the best in other six evaluation metrics
The performances of the oracle  run LSTM-A∗ could be regarded as the upper bound of employing attributes in our framework and lead to large performance gain against LSTM-AN
Such an upper bound enables us to obtain more insights on the factor accounting for  the success of the current attribute augmented architecture  and also provides guidance to future research in this direction
More specifically, the results, on one hand, indicate  the advantage and great potential of leveraging attributes  for boosting image captioning, and on the other, suggest  that more efforts are further required towards mining and  representing attributes more effectively
 N.N
Evaluation of Attributes Prediction Model  We further verify the effectiveness of our MIL-IAC attributes prediction model
We compared two baselines here
 One is to directly fine-tune the VGG architecture with cross  entropy loss for attributes prediction, named as Fine-tune,  and the other, namely MIL, exploits a weakly supervised  MIL model [N] based on VGG to learn region-based detectors for attributes
Table N compares the sentence generation  performances of our LSTM-AN model with attributes learnt  by different attributes prediction models on COCO dataset
 Compared to Fine-tune, MIL method using region-based  detectors consistently exhibits better performance across different evaluation metric
Moreover, by additionally exploring the inter-attribute correlations in MIL framework,  our proposed MIL-IAC leads to larger performance gains
 N.N
Performance on COCO Online Testing Server  We also submitted our best run in terms of METEOR,  i.e., LSTM-AN, to online COCO testing server and evaluated the performance on official testing set
Table N shows the  performance Leaderboard on official testing image set with  N reference captions (cN) and N0 reference captions (cN0)
 Please note that here we utilize the outputs of N,0NN-way  poolN layer from ResNet-NNN as image representations and train the attribute detectors by ResNet-NNN in our final submission
Moreover, inspired by [NN], we adopt the policy  gradient optimization to specifically boost CIDEr-D performance
The latest top-N performing methods which have  been officially published are included in the table
Compared to the top performing methods on the leaderboard, our  proposed LSTM-AN achieves the best performances across  all the evaluation metrics on both cN and cN0 testing sets
 N.N
Human Evaluation  To better understand how satisfactory are the sentences  generated from different methods, we also conducted a  human study to compare our LSTM-AN against three approaches, i.e., CaptionBot, LRCN and LSTM
A total number of NN evaluators (N females and N males) from different education backgrounds, including computer science (N),  business (N), linguistics (N) and engineering (N), are invited  and a subset of NK images is randomly selected from testing  set for the subjective evaluation
The evaluation process is  as follows
All the evaluators are organized into two groups
 We show the first group all the four sentences generated by  each approach plus five human-annotated sentences and ask  NN00    Generated Sentences:  LSTM: a man riding a skateboard down a street  CaptionBot:  I think it's a group of people walking down the   road
 LSTM-AN: a man walking down a street with a herd of sheep  Ground Truth:     a man walks while a large number of sheep follow    a man leading a herd of sheep down the sheep   the man is walking a herd of sheep on the road  through a town  Attributes:  sheep: 0.NNN  herd: 0.NNN  street: 0.N0N  walking: 0.N0N  road: 0.NNN  man: 0.NNN  standing: 0.NN0  animals: 0.NNN  Generated Sentences:  LSTM: a group of people standing around a market  CaptionBot:  I think it's a bunch of yellow flowers
 LSTM-AN: a group of people standing around a bunch of   bananas  Ground Truth:     bunches of bananas for sale at an outdoor market     a person at a table filled with bananas   there are many bananas layer across this table at a  farmers market  Attributes:  bananas: N  people: 0.NNN  market: 0.N0N  standing: 0.NNN  outdoor: 0.NNN  blue: 0.NNN  large: 0.N0N  table: 0.NNN  Generated Sentences:  LSTM: a cell phone sitting on top of a table  CaptionBot:  I think it's a laptop that is on the phone
 LSTM-AN: a person holding a cell phone in front of a laptop  Ground Truth:     a smart phone being held up in front of a lap top     the person is holding his cell phone while on his laptop   someone holding a cell phone in front of a laptop  Attributes:  phone: 0.NNN  cell: 0.NNN  computer: 0.NNN    laptop: 0.NNN  keyboard: 0.NNN   screen: 0.NNN holding: 0.N0N  person: 0.NNN  Generated Sentences:  LSTM: a group of people flying kites in the sky  CaptionBot:  I think it's a plane is flying over the water
 LSTM-AN: a red and white plane flying over a body of water  Ground Truth:     a plane with water skies for landing gear coming in for  a landing at a lake      a plane flying through a sky above a lake   a red and white plane is flying over some water  Attributes:  flying: 0.NNN  airplane: 0.NNN  plane: 0.NNN    water: 0.NNN  red: 0.NNN  lake: 0.NNN  white: 0.NNN  sky: 0.NNN  Attributes:  boat: N  water: 0.NNN  man: 0.NNN    riding: 0.NNN  dog: 0.NNN  small: 0.NNN  person: 0.NNN  river: 0.NNN  Generated Sentences:  LSTM: a group of people on a boat in the water  CaptionBot: I think it's a man with a small boat in a body of   water
 LSTM-AN: a man and a dog on a boat in the water  Ground Truth:     an image of a man in a boat with a dog    a person on a rowboat with a dalmatian dog on the boat   old woman rowing a boat with a dog  Figure N
Attributes and sentences generation results on COCO
The attributes are detected by our attributes prediction model and the  output sentences are generated by N) LSTM, N) CaptionBotN, N) our LSTM-AN and N) Ground Truth: three ground truth sentences
 Table N
User study on two criteria: MN - percentage of captions  generated by different methods that are evaluated as better/equal to  human caption; MN - percentage of captions that pass Turing Test
Human LSTM-AN CaptionBot LSTM LRCN  MN - NN.N NN.N NN.N NN.N  MN N0.N NN.N NN.N NN.N NN.N  them the question: Do the systems produce captions resembling human-generated sentences? In contrast, we show the  second group once only one sentence generated by different approach or human annotation and they are asked: Can  you determine whether the given sentence has been generated by a system or by a human being? From evaluators’  responses, we calculate two metrics: N) MN: percentage of  captions that are evaluated as better or equal to human caption; N) MN: percentage of captions that pass the Turing  Test
Table N lists the result of the user study
Overall, our  LSTM-AN is clearly the winner for all two criteria
In particular, the percentage achieves NN.N% and NN.N% in terms  of MN and MN, respectively, making the absolute improvement over the best competitor CaptionBot by N.N% and N%
 N.N
Qualitative Analysis  Figure N showcases a few sentence examples generated  by different methods, the detected high-level attributes, and  human-annotated ground truth sentences
From these exemplar results, it is easy to see that all of these automatic  methods can generate somewhat relevant sentences, while  our proposed LSTM-AN can predict more relevant keywords by jointly exploiting high-level attributes and image representations for image captioning
For example, compared  to subject term “a group of people” and “a man” in the sentence generated by LSTM and CaptionBot respectively, “a  man and a dog” in our LSTM-AN is more precise to describe the image content in the first image, since the keyword “dog” is one of the detected attributes and directly  injected into LSTM to guide the sentence generation
Similarly, verb term “holding” which is also detected as one  high-level attribute presents the fourth image more exactly
Moreover, our LSTM-AN generate more descriptive sentence by enriching the semantics with high-level attributes
 For instance, with the detected adjective “red,” the generated sentence “a red and white plane flying over a body of  water” of the fifth image depicts the image content more  comprehensive
We refer the readers to supplementary materials for more examples
 N
Discussions and Conclusions  We have presented Long Short-Term Memory with Attributes (LSTM-A) architectures which explores both image  representations and high-level attributes for image captioning
Particularly, we detect attributes by additionally exploring the inter-attribute correlations in the Multiple Instance  Learning framework and study the problem of augmenting high-level attributes from images to complement image representations for enhancing sentence generation
To  verify our claim, we have devised variants of architectures  by modifying the placement and moment, where and when  to feed into the two kinds of representations
Experiments  conducted on COCO image captioning dataset validate our  proposal and analysis
Performance improvements are observed when comparing to other captioning techniques
 Our future works are as follows
First, more attributes  will be learnt from large-scale image benchmarks, e.g.,  YFCC-N00M dataset, and integrated into image captioning
 Second, how to generate free-form and open-vocabulary  sentences with the learnt attributes is also expected
 NN0N    References  [N] P
Anderson, B
Fernando, M
Johnson, and S
Gould
Spice:  Semantic propositional image caption evaluation
In ECCV,  N0NN
 [N] S
Banerjee and A
Lavie
Meteor: An automatic metric for  mt evaluation with improved correlation with human judgments
In ACL workshop, N00N
 [N] X
Chen, H
Fang, T.-Y
Lin, R
Vedantam, S
Gupta, P
Dollár, and C
L
Zitnick
Microsoft COCO captions: Data collection and evaluation server
arXiv preprint arXiv:NN0N.00NNN, N0NN
 [N] J
Devlin, H
Cheng, H
Fang, S
Gupta, L
Deng, X
He,  G
Zweig, and M
Mitchell
Language models for image captioning: The quirks and what works
In ACL, N0NN
 [N] J
Donahue, L
Anne Hendricks, S
Guadarrama,  M
Rohrbach, S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual  recognition and description
In CVPR, N0NN
 [N] H
Fang, S
Gupta, et al
From captions to visual concepts  and back
In CVPR, N0NN
 [N] A
Farhadi, M
Hejrati, M
A
Sadeghi, P
Young,  C
Rashtchian, J
Hockenmaier, and D
Forsyth
Every picture tells a story: Generating sentences from images
In ECCV, N0N0
 [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
 [N] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional  architecture for fast feature embedding
In MM, N0NN
 [N0] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, N0NN
 [NN] G
Kulkarni, V
Premraj, et al
Babytalk: Understanding and  generating simple image descriptions
IEEE Trans
on PAMI,  N0NN
 [NN] C.-Y
Lin
Rouge: A package for automatic evaluation of  summaries
In ACL Workshop, N00N
 [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV, N0NN
 [NN] S
Liu, Z
Zhu, N
Ye, S
Guadarrama, and K
Murphy
Optimization of image description metrics using policy gradient  methods
arXiv preprint arXiv:NNNN.00NN0, N0NN
 [NN] J
Lu, C
Xiong, D
Parikh, and R
Socher
Knowing when  to look: Adaptive attention via a visual sentinel for image  captioning
In CVPR, N0NN
 [NN] J
Mao, W
Xu, Y
Yang, J
Wang, and A
L
Yuille
Explain  images with multimodal recurrent neural networks
In NIPS  Workshop on Deep Learning, N0NN
 [NN] T
Mikolov, K
Chen, G
Corrado, and J
Dean
Efficient  estimation of word representations in vector space
In ICLR  workshop, N0NN
 [NN] M
Mitchell, X
Han, et al
Midge: Generating image descriptions from computer vision detections
In EACL, N0NN
 [NN] Y
Pan, Y
Li, T
Yao, T
Mei, H
Li, and Y
Rui
Learning  deep intrinsic video representation by exploring temporal coherence and graph structure
In IJCAI, N0NN
 [N0] Y
Pan, T
Mei, T
Yao, H
Li, and Y
Rui
Jointly modeling  embedding and translation to bridge video and language
In  CVPR, N0NN
 [NN] Y
Pan, T
Yao, H
Li, and T
Mei
Video captioning with  transferred semantic attributes
In CVPR, N0NN
 [NN] K
Papineni, S
Roukos, T
Ward, and W.-J
Zhu
Bleu: a  method for automatic evaluation of machine translation
In  ACL, N00N
 [NN] D
Parikh and K
Grauman
Relative attributes
In ICCV,  N0NN
 [NN] S
J
Rennie, E
Marcheret, Y
Mroueh, J
Ross, and V
Goel
 Self-critical sequence training for image captioning
arXiv  preprint arXiv:NNNN.00NNN, N0NN
 [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual  Recognition Challenge
IJCV, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 [NN] I
Sutskever, O
Vinyals, and Q
V
Le
Sequence to sequence  learning with neural networks
In NIPS, N0NN
 [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
 [NN] K
Tran, X
He, L
Zhang, J
Sun, C
Carapcea, C
Thrasher,  C
Buehler, and C
Sienkiewicz
Rich image captioning in  the wild
arXiv preprint arXiv:NN0N.0N0NN, N0NN
 [N0] R
Vedantam, C
Lawrence Zitnick, and D
Parikh
Cider:  Consensus-based image description evaluation
In CVPR,  N0NN
 [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In CVPR, N0NN
 [NN] Q
Wu, C
Shen, L
Liu, A
Dick, and A
v
d
Hengel
What  value do explicit high level concepts have in vision to language problems? In CVPR, N0NN
 [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
Courville, R
Salakhudinov, R
Zemel, and Y
Bengio
Show, attend and tell: Neural  image caption generation with visual attention
In ICML,  N0NN
 [NN] Y
Yang, C
L
Teo, H
Daumé III, and Y
Aloimonos
 Corpus-guided sentence generation of natural images
In  EMNLP, N0NN
 [NN] Z
Yang, Y
Yuan, Y
Wu, W
W
Cohen, and R
R
Salakhutdinov
Review networks for caption generation
In NIPS,  N0NN
 [NN] T
Yao, Y
Pan, Y
Li, and T
Mei
Incorporating copying  mechanism in image captioning for learning novel objects
 In CVPR, N0NN
 [NN] T
Yao, Y
Pan, C.-W
Ngo, H
Li, and T
Mei
Semisupervised domain adaptation with subspace learning for visual recognition
In CVPR, N0NN
 [NN] Q
You, H
Jin, Z
Wang, C
Fang, and J
Luo
Image captioning with semantic attention
In CVPR, N0NN
 [NN] L
Zhou, C
Xu, P
Koch, and J
J
Corso
Image caption  generation with text-conditional semantic attention
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
 NN0NIlluminating Pedestrians via Simultaneous Detection & Segmentation   Illuminating Pedestrians via Simultaneous Detection & Segmentation  Garrick Brazil, Xi Yin, Xiaoming Liu  Michigan State University, East Lansing, MI NNNNN  {brazilga, yinxiN, liuxm}@msu.edu  Abstract  Pedestrian detection is a critical problem in computer vision with significant impact on safety in urban autonomous  driving
In this work, we explore how semantic segmentation can be used to boost pedestrian detection accuracy  while having little to no impact on network efficiency
We  propose a segmentation infusion network to enable joint  supervision on semantic segmentation and pedestrian detection
When placed properly, the additional supervision  helps guide features in shared layers to become more sophisticated and helpful for the downstream pedestrian detector
Using this approach, we find weakly annotated boxes  to be sufficient for considerable performance gains
We provide an in-depth analysis to demonstrate how shared layers  are shaped by the segmentation supervision
In doing so, we  show that the resulting feature maps become more semantically meaningful and robust to shape and occlusion
Overall, our simultaneous detection and segmentation framework achieves a considerable gain over the state-of-the-art  on the Caltech pedestrian dataset, competitive performance  on KITTI, and executes N× faster than competitive methods
 N
Introduction  Pedestrian detection from an image is a core capability of  computer vision, due to its applications such as autonomous  driving and robotics [NN]
It is also a long-standing vision  problem because of its distinct challenges including low  resolution, occlusion, cloth variations, etc [N0]
There are  two central approaches for detecting pedestrians: object detection [N, NN] and semantic segmentation [N, N]
The two  approaches are highly related by nature but have their own  strengths and weaknesses
For instance, object detection is  designed to perform well at localizing distinct objects but  typically provides little information on object boundaries
 In contrast, semantic segmentation does well at distinguishing pixel-wise boundaries among classes but struggles to  separate objects within the same class
 Intuitively, we expect that knowledge from either task  will make the other substantially easier
This has been  Figure N
Detection results on the Caltech test set (left), feature  map visualization from the RPN of conventional Faster R-CNN  (middle), and feature map visualization of SDS-RCNN (right)
 Notice that our feature map substantially illuminates the pedestrian shape while suppressing the background region, both of  which make positive impact to downstream pedestrian detection
 demonstrated for generic object detection, since having segmentation masks of objects would clearly facilitate detection
For example, Fidler et al
[NN] utilize predicted segmentation masks to boost object detection performance via  a deformable part-based model
Hariharan et al
[NN] show  how segmentation masks generated from MCG [N] can be  used to mask background regions and thus simplify detection
Dai et al
[N] utilize the two tasks in a N-stage cascaded network consisting of box regression, foreground segmentation, and classification
Their architecture allows each task  to share features and feed into one another
 In contrast, the pairing of these two tasks is rarely studied  in pedestrian detection, despite the recent advances [N, NN,  NN]
This is due in part to the lack of pixel-wise annotations  available in classic pedestrian datasets such as Caltech [N]  and KITTI [NN], unlike the detailed segmentation labels in  the COCO [NN] dataset for generic object detection
With  the release of Cityscapes [N], a high quality dataset for urban semantic segmentation, it is expected that substantial  research efforts will be on how to leverage semantic segmentation to boost the performance of pedestrian detection,  which is the core problem to be studied in this paper
 Given this objective, we start by presenting a competitive two-stage baseline framework of pedestrian detection  deriving from RPN+BF [NN] and Faster R-CNN [NN]
We  contribute a number of key changes to enable the secondstage classifier to specialize in stricter supervision and additionally fuse the refined scores with the first stage RPN
 These changes alone lead to state-of-the-art performance on  NNN0    the Caltech benchmark
We further present a simple, but  surprisingly powerful, scheme to utilize multi-task learning  on pedestrian detection and semantic segmentation
Specifically, we infuse the semantic segmentation mask into shared  layers using a segmentation infusion layer in both stages of  our network
We term our approach as “simultaneous detection and segmentation R-CNN (SDS-RCNN)”
We provide  an in-depth analysis on the effects of joint training by examining the shared feature maps, e.g., Fig
N
Through infusion, the shared feature maps begin to illuminate pedestrian  regions
Further, since we infuse the semantic features during training only, the network efficiency at inference is unaffected
We demonstrate the effectiveness of SDS-RCNN  by reporting considerable improvement (NN% relative re- duction of the error) over the published state-of-the-art on  Caltech [N], competitive performance on KITTI [NN], and a  runtime roughly N× faster than competitive methods
In summary our contributions are as follows:  ⋄ Improved baseline derived from [NN, NN] by enforcing stricter supervision in the second-stage classification  network, and further fusing scores between stages
 ⋄ A multi-task infusion framework for joint supervision on pedestrian detection and semantic segmentation,  with the goal of illuminating pedestrians in shared feature maps and easing downstream classification
 ⋄ We achieve the new state-of-the-art performance on Caltech pedestrian dataset, competitive performance  on KITTI, and obtain N× faster runtime
 N
Prior work  Object Detection: Deep convolution neural networks have  had extensive success in the domain of object detection
 Notably, derivations of Fast [NN] and Faster R-CNN [NN] are  widely used in both generic object detection [N, NN, NN] and  pedestrian detection [NN, NN, NN]
Faster R-CNN consists of  two key components: a region proposal network (RPN) and  a classification sub-network
The RPN works as a sliding  window detector by determining the objectness across a set  of predefined anchors (box shapes defined by aspect ratio  and scale) at each spatial location of an image
After object  proposals are generated, the second stage classifier determines the precise class each object belongs to
Faster RCNN has been shown to reach state-of-the-art performance  on the PASCAL VOC N0NN [NN] dataset for generic object detection and continues to serve as a frequent baseline  framework for a variety of related problems [NN, NN, NN, N0]
 Pedestrian Detection: Pedestrian detection is one of the  most extensively studied problems in object detection due to  its real-world significance
The most notable challenges are  caused by small scale, pose variations, cyclists, and occlusion [N0]
For instance, in the Caltech pedestrian dataset [N]  N0% of pedestrians are occluded in at least one frame
 The top performing approaches on the Caltech pedestrian benchmark are variations of Fast or Faster R-CNN
 SA-FastRCNN [NN] and MS-CNN [N] reach competitive  performance by directly addressing the scale problem using  specialized multi-scale networks integrated into Fast and  Faster R-CNN respectively
Furthermore, RPN+BF [NN]  shows that the RPN of Faster R-CNN performs well as  a standalone detector while the downstream classifier degrades performance due to collapsing bins of small-scale  pedestrians
By using higher resolution features and replacing the downstream classifier with a boosted forest,  RPN+BF is able to alleviate the problem and achieve N.NN% miss rate on the Caltech reasonable [N] setting
F-DNN [N0]  also uses a derivation of the Faster R-CNN framework
 Rather then using a single downstream classifier, F-DNN  fuses multiple parallel classifiers including ResNet [NN] and  GoogLeNet [NN] using soft-reject and further incorporates  multiple training datasets to achieve N.NN% miss rate on the Caltech reasonable setting
The majority of top performing  approaches utilize some form of a RPN, whose scores are  typically discarded after selecting the proposals
In contrast,  our work shows that fusing the score with the second stage  network can lead to substantial performance improvement
 Simultaneous Detection & Segmentation: There are two  lines of research on simultaneous detection and segmentation
The first aims to improve the performance of  both tasks, and formulates a problem commonly known as  instance-aware semantic segmentation [N]
Hariharan et  al
[NN] predict segmentation masks using MCG [N] then get  object instances using “slow” R-CNN [NN] on masked image proposals
Dai et al
[N] achieve high performance on  instance segmentation using an extension of Faster R-CNN  in a N-stage cascaded network including mask supervision
 The second aims to explicitly improve object detection  by using segmentation as a strong cue
Early work on  the topic by Fidler et al
[NN] demonstrates how semantic  segmentation masks can be used to extract strong features  for improved object detection via a deformable part-based  model
Du et al
[N0] use segmentation as a strong cue in  their F-DNN+SS framework
Given the segmentation mask  predicted by a third parallel network, their ensemble network uses the mask in a post-processing manner to suppress background proposals, and pushes performance on  the Caltech pedestrian dataset from N.NN% to N.NN% miss rate
However, the segmentation network degrades the efficiency of F-DNN+SS from 0.N0 to N.NN seconds per image, and requires multiple GPUs at inference
In contrast, our  novel framework infuses the semantic segmentation masks  into shared feature maps and thus does not require a separate segmentation network, which outperforms [N0] in both  accuracy and network efficiency
Furthermore, our use of  weak box-based segmentation masks addresses the issue of  lacking pixel-wise segmentation annotations in [N, NN]
 NNNN    se g     co n  v N  -N    co n  v N  -N    co n  v N  -N    co n  v N  -N    co n  v N  -N    p ro  p o  sa l   co n  v N  -N    co n  v N  -N    co n  v N  -N    co n  v N  -N    co n  v N  -N    cl s   b b  o x    F C  N    VGG-NN   VGG-NN   +   se g     RPN BCN   F C  N    Figure N
Overview of the proposed SDS-RCNN framework
The segmentation layer infuses semantic features into shared convN-N layers  of each stage, thus illuminating pedestrians and easing downstream pedestrian detection (proposal layers in RPN, and FCN-N in BCN)
 N
Proposed method  Our proposed architecture consists of two key stages:  a region proposal network (RPN) to generate candidate  bounding boxes and corresponding scores, and a binary  classification network (BCN) to refine their scores
In both  stages, we propose a semantic segmentation infusion layer  with the objective of making downstream classification a  substantially easier task
The infusion layer aims to encode  semantic masks into shared feature maps which naturally  serve as strong cues for pedestrian classification
Due to the  impressive performance of the RPN as a standalone detector, we elect to fuse the scores between stages rather than  discarding them as done in prior work [N, N0, NN, NN]
An  overview of the SDS-RCNN framework is depicted in Fig
N  N.N
Region Proposal Network  The RPN aims to propose a set of bounding boxes with  associated confidence scores around potential pedestrians
 We adopt the RPN of Faster R-CNN [NN] following the settings in [NN]
We tailor the RPN for pedestrain detection  by configuring Na = N anchors with a fixed aspect ratio of 0.NN and spanning a scale range from NN – NN0 pixels, cor- responding to the pedestrain statistics of Caltech [N]
Since  each anchor box acts as a sliding window detector across a  pooled image space, there are Np = Na × W fs  × H fs  total  pedestrian proposals, where fs corresponds to the feature  stride of the network
Hence, each proposal box i corresponds to an anchor and a spatial location of image I
 The RPN architecture uses convN-N from VGG-NN [NN]  as the backbone
Following [NN], we attach a proposal feature extraction layer to the end of the network with two sibling output layers for box classification (cls) and bounding  box regression (bbox)
We further add a segmentation infusion layer to convN as detailed in Sec
N.N
 For every proposal box i, the RPN aims to minimize the  following joint loss function with three terms:  L = λc ∑  i  Lc(ci, ĉi) + λr ∑  i  Lr(ti, t̂i) + λsLs
(N)  The first term is the classification loss Lc, which is a softmax logistic loss over two classes (pedestrian vs
background)
We use the standard labeling policy which considers a proposal box at location i to be pedestrian (ci = N) if it has at least 0.N Intersection over Union (IoU) with a ground truth pedestrian box, and otherwise background (ci = 0)
The second term seeks to improve localization via bounding box regression, which learns a transformation for each  proposal box to the nearest pedestrian ground truth
Specifically, we use Lr(ti, t̂i) = R(ti − t̂i) where R is the robust (smooth LN) loss defined in [NN]
The bounding box transformation is defined as a N-tuple consisting of shifts in x, y and scales in w, h denoted as t = [tx, ty, tw, th]
The third term Ls is the segmentation loss presented in Sec
N.N
 In order to reduce multiple detections of the same pedestrian, we apply non-maximum suppression (NMS) greedily  to all pairs of proposals after the transformations have been  applied
We use an IoU threshold of 0.N for NMS
 We train the RPN in the Caffe [N0] framework using  SGD with a learning rate of 0.00N, momentum of 0.N, and mini-batch of N full-image
During training, we randomly sample NN0 proposals per image at a ratio of N:N for pedes- trian and background proposals to help alleviate the class  imbalance
All other proposals are treated as ignore
We  initialize convN-N from a VGG-NN model pretrained on ImageNet [N], and all remaining layers randomly
Our network has four max-pooling layers (within convN-N), hence  fs = NN
In our experiments, we regularize our multi-task loss terms by setting λc = λs = N, λr = N
 NNNN    Without Padding  Padding N0%              (a)                     (b)                          (a)                          (b)   Figure N
Example proposal masks with and without padding
 There is no discernible difference between the non-padded masks  of well-localized (a) and poorly localized (b) proposals
 N.N
Binary Classification Network  The BCN aims to perform pedestrian classification over  the proposals of the RPN
For generic object detection, the  BCN usually uses the downstream classifier of Faster RCNN by sharing convN-N with the RPN, but was shown  by [NN] to degrade pedestrian detection accuracy
Thus,  we choose to construct a separate network using VGG-NN
 The primary advantage of a separate network is to allow the  BCN freedom to specialize in the types of “harder” samples left over from the RPN
While sharing computation is  highly desirable for the sake of efficiency, the shared networks are more predestined to predict similar scores which  are redundant when fused
Therefore, rather than cropping  and warping a shared feature space, our BCN directly crops  the top Nb proposals from the RGB input image
 For each proposal image i, the BCN aims to minimize  the following joint loss function with two terms:  L = λc ∑  i  wiLc(ci, ĉi) + λsLs
(N)  Similar to RPN, the first term is the classification loss Lc where ci is the class label for the ith proposal
A costsensitive weight wi is used to give precedence to detect  large pedestrians over small pedestrians
There are two key  motivations for this weighting policy
First, large pedestrians typically imply close proximity and are thus significantly more important to detect
Secondly, we presume that  features of large pedestrians may be more helpful for detecting small pedestrians
We define the weighting function  given the ith proposal with height hi and a pre-computed  mean height h̄ as wi = N + hi h̄  
The second term is the  segmentation loss presented in Sec
N.N
 We make a number of significant contributions to the  BCN
First, we change the labeling policy to encourage  higher precision and further diversification from the RPN
 We enforce a stricter labeling policy, requiring a proposal  to have IoU > 0.N with a ground truth pedestrian box to be considered pedestrian (ci = N), and otherwise background (ci = 0)
This encourages the network to suppress poorly localized proposals and reduces false positives in the form  of double detections
Secondly, we choose to fuse the scores  of the BCN with the confidence scores of the RPN at test  time
Since our design explicitly encourages the two stages  convN   RPN Baseline   conv_proposal RGB convN   RPN + Weak Segmentation   conv_proposal   Figure N
Feature map visualizations of convN and the proposal  layer for the baseline RPN (left) and the RPN infused with weak  segmentation supervision (right)
 to diversify, we expect the classification characteristics of  each network to be complementary when fused
We fuse  the scores at the feature level prior to softmax
Formally,  the fused score for the ith proposal, given the predicted N- class scores from the RPN = {ĉri0, ĉ  r iN} and BCN = {ĉ  b i0,  ĉbiN} is computed via the following softmax function:  ĉi = e(ĉ  r  iN +ĉb  iN )  e(ĉ r  iN +ĉb  iN ) + e(ĉ  r  i0 +ĉb  i0 ) 
(N)  In effect, the fused scores become more confident when  the stages agree, and otherwise lean towards the dominant  score
Thus, it is ideal for each network to diversify in its  classification capabilities such that at least one network may  be very confident for each proposal
 For a modest improvement to efficiency, we remove the  poolN layer from the VGG-NN architecture then adjust the  input size to NNN × NNN to keep the fully-connected layers intact
This is a fair trade-off since most pedestrian heights  fall in the range of N0− N0 pixels [N]
Hence, small pedes- trian proposals are upscaled by a factor of ∼N×, allowing space for finer discrimination
We further propose to pad  each proposal by N0% on all sides to provide background context and avoid partial detections, as shown in Fig
N
 We train the BCN in the Caffe [N0] framework using  the same settings as the RPN
We initialize convN-N from  the trained RPN model, and all remaining layers randomly
 During training, we set Nb = N0
During inference, we set Nb = NN for a moderate improvement to efficiency
We regularize the multi-task loss by setting λc = λs = N
 NNNN    N x N Downsample   mask box mask box   Full resolution   m a  sk    b o  x    Full Resolution Downsampled   RGB   Figure N
Visualization of the similarity between pixel-wise segmentation masks (from Cityscapes [N]) and weak box-based masks  when downsampled in both the BCN (top) and RPN (bottom)
 N.N
Simultaneous Detection & Segmentation  We approach simultaneous detection and segmentation  with the motivation to make our downstream pedestrian detection task easier
We propose a segmentation infusion  layer trained on weakly annotated pedestrian boxes which  illuminate pedestrians in the shared feature maps preceding  the classification layers
We integrate the infusion layer into  both stages of our SDS-RCNN framework
 Segmentation Infusion Layer: The segmentation infusion  layer aims to output two masks indicating the likelihood of  residing on pedestrian or background segments
We choose  to use only a single layer and a N × N kernel so the im- pact on the shared layers will be as high as possible
This  forces the network to directly infuse semantic features into  shared feature maps, as visualized in Fig
N
A deeper network could achieve higher segmentation accuracy but will  infer less from shared layers and diminish the overall impact on the downstream pedestrian classification
Further,  we choose to attach the infusion layer to convN since it is  the deepest layer which precedes both the proposal layers  of the RPN and the fully connected layers of the BCN
 Formally, the final loss term Ls of both the RPN and  BCN is a softmax logistic loss over two classes (pedestrian  vs
background), applied to each location i, where wi is the  cost-sensitive weight introduced in N.N:  λs ∑  i  wiLs(Si, Ŝi)
(N)  We choose to levereage the abundance of bounding box  annotations available in popular pedestrian datasets (e.g.,  Caltech [N], KITTI [NN]) by forming weak segmentation  ground truth masks
Each mask S ∈ RW×H is generated by labeling all pedestrian box regions as Si = N, and oth- erwise background Si = 0
In most cases, box-based an- notations would be considered too noisy for semantic segmentation
However, since we place the infusion layer at  convN, which has been pooled significantly, the differences  between box-based annotations and pixel-wise annotations  diminish rapidly w.r.t
the pedestrian height (Fig
N)
For  example, in the Caltech dataset NN% of pedestrians are less than N0 pixels tall, which corresponds to N × N pixels at convN of the RPN
Further, each of the BCN proposals are  pooled to N × N at convN
Hence, pixel-wise annotations may not offer a significant advantage over boxes at the high  levels of pooling our networks undertake
 Benefits Over Detection: A significant advantage of segmentation supervision over detection is its simplicity
For  detection, sensitive hyperparamters must be set, such as  anchor selection and IoU thresholds used for labeling and  NMS
If the chosen anchor scales are too sparse or the IoU  threshold is too high, certain ground truths that fall near the  midpoint of two anchors could be missed or receive low  supervision
In contrast, semantic segmentation treats all  ground truths indiscriminate of how well the pedestrian’s  shape or occlusion-level matches the chosen set of anchors
 In theory, the incorporation of semantic segmentation infusion may help reduce the sensitivity of convN-N to such hyperparamters
Furthermore, the segmentation supervision  is especially beneficial for the second stage BCN, which on  its own would only know if a pedestrian is present
The infusion of semantic segmentation features inform the BCN  where the pedestrian is, which is critical for differentiating  poorly vs
well-localized proposals
 N
Experiments  We evaluate our proposed SDS-RCNN on popular  datasets including Caltech [N] and KITTI [NN]
We perform  comprehensive analysis and ablation experiments using the  Caltech dataset
We refer to our collective method as SDSRCNN and our region proposal network as SDS-RPN
We  show the performance curves compared to the state-of-theart pedestrian detectors on Caltech in Fig
N
We further  report a comprehensive overview across datasets in Table N
 N.N
Benchmark Comparison  Caltech: The Caltech dataset [N] contains ∼NN0K pedes- trian bounding box annotations across N0 hours of urban driving
The log average miss rate sampled against a false  positive per image (FPPI) range of [N0−N, N00] is used for measuring performance
A minimum IoU threshold of 0.N is required for a detected box to match with a ground truth  box
For training, we sample from the standard training set  according to CaltechN0×[NN], which contains NN,NNN train- ing images
We evaluate on the standard N,0NN images in the Caltech N× test set using the reasonable [N] setting, which only considers pedestrians with at least N0 pixels in height and with less than NN% occlusion
 NNNN    Figure N
Comparison of SDS-RCNN with the state-of-the-art  methods on the Caltech dataset using the reasonable setting
 SDS-RCNN achieves an impressive N.NN% miss rate
The performance gain is a relative improvement of NN% compared to the best published method RPN+BF (N.NN%)
In Fig
N, we show the ROC plot of miss rate against FPPI  for the current top performing methods reported on Caltech
 We further report our performance using just SDS-RPN  (without cost-sensitive weighting, Sec
N.N) on Caltech as  shown in Table N
The RPN performs quite well by itself, reaching N.NN% miss rate while processing images at roughly N× the speed of competitive methods
Our RPN is already on par with other top detectors, which themselves  contain a RPN
Moreover, the network significantly outperforms other standalone RPNs such as in [NN] (NN.N%)
Hence, the RPN can be leveraged by other researchers to  build better detectors in the future
 KITTI: The KITTI dataset [NN] contains ∼N0K annotations of cars, pedestrians, and cyclists
Since our focus is on  pedestrian detection, we continue to use only the pedestrian  class for training and evaluation
The mean Average Precision (mAP) [NN] sampled across a recall range of [0, N] is used to measure performance
We use the standard training  set of N,NNN images and evaluate on the designated test set of N,NNN images
Our method reaches a score of NN.0N mAP on the moderate setting for the pedestrian class
Surprisingly,  we observe that many models which perform well on Caltech do not generalize well to KITTI, as detailed in Table N
 We expect this is due to both sensitivity to hyperparameters  and the smaller training set of KITTI (∼N× smaller than CaltechN0×)
MS-CNN [N] is the current top performing method for pedestrian detection on KITTI
Aside from the  novelty as a multi-scale object detector, MS-CNN augments  the KITTI dataset by random cropping and scaling
Thus,  incorporating data augmentation could alleviate the smaller  Method Caltech KITTI Runtime  DeepParts [NN] NN.NN NN.NN Ns CompACT-Deep [N] NN.NN NN.NN Ns MS-CNN [N] N.NN NN.N0 0.Ns SA-FastRCNN [NN] N.NN NN.0N 0.NNs RPN+BF [NN] N.NN NN.NN 0.N0s F-DNN [N0] N.NN - 0.N0s F-DNN+SS [N0] N.NN - N.NNs  SDS-RPN (ours) N.NN - 0.NNs SDS-RCNN (ours) N.NN NN.0N 0.NNs  Table N
Comprehensive comparison of SDS-RCNN with other  state-of-the-art methods showing the Caltech miss rate, KITTI  mAP score, and runtime performance
 training set and lead to better generalization across datasets
 Furthermore, as described in the ablation study of Sec
N.N,  our weak segmentation supervision primarily improves the  detection of unusual shapes and poses (e.g., cyclists, people  sitting, bent over)
However, in the KITTI evaluation, the  person sitting class is ignored and cyclists are counted as  false positives, hence such advantages are less helpful
 Efficiency: The runtime performance of SDS-RCNN takes  ∼0.NNs/image
We use images of size NN0× NN0 pixels and a single Titan X GPU for computation
The efficiency of  SDS-RCNN surpasses the current state-of-the-art methods  for pedestrian detection, often by a factor of N×
Compared to F-DNN+SS [N0], which also utilizes segmentation cues,  our method executes ∼N0× faster
The next fastest runtime is F-DNN, which takes 0.N0s/image with the caveat of re- quiring multiple GPUs to process networks in parallel
Further, our SDS-RPN method achieves very competitive accuracy while only taking 0.NNs/image (frequently ∼N× faster than competitive methods using a single GPU)
 N.N
Ablation Study  In this section, we evaluate how each significant component of our network contributes to performance using the  reasonable set of Caltech [N]
First, we examine the impact of four components: weak segmentation supervision,  proposal padding, cost-sensitive weighting, and stricter supervision
For each experiment, we start with SDS-RCNN  and disable one component at a time as summarized in Table N
For simplicity, we disable components globally when  applicable
Then we provide detailed discussion on the benefits of stage-wise fusion and comprehensively report the  RPN, BCN, and fused performances for all experiments
Finally, since our BCN is designed to not share features with  the RPN, we closely examine how sharing weights between  stages impacts network diversification and efficiency
 Weak Segmentation: The infusion of semantic features  into shared layers is the most critical component of SDSRCNN
The fused miss rate degrades by a full N.0N% when  NNNN    Figure N
Example error sources which are corrected by infusing semantic segmentation into shared layers
Row N shows the test images  from CaltechN×
Row N shows a visualization of the RPN proposal layer using the baseline network which fails on these examples
Row N shows a visualization of the proposal layer from SDS-RCNN, which corrects the errors
Collectively, occlusion and unusual poses of  pedestrians (sitting, cyclist, bent over) make up for NN% of the corrections, suggesting that the the segmentation supervision naturally  informs the shared features on robust pedestrian parts and shape information
 Component Disabled RPN BCN Fusion  proposal padding N0.NN NN.0N N.NN cost-sensitive N.NN NN.NN N.NN strict supervision N0.NN NN.NN N.NN weak segmentation NN.NN NN.NN N0.NN  SDS-RCNN N0.NN N0.NN N.NN  Table N
Ablation experiments evaluated using the Caltech test set
 Each ablation experiment reports the miss rate for the RPN, BCN,  and fused score with one component disabled at a time
 the segmentation supervision is disabled, while both individual stages degrade similarly
To better understand the  types of improvements gained by weak segmentation, we  perform a failure analysis between SDS-RCNN and the  “baseline” (non-weak segmentation) network
For analysis, we examine the NN pedestrian cases which are missed when weak segmentation is disabled, but corrected otherwise
Example error corrections are shown in Fig
N
We  find that ∼NN% of corrected pedestrians are at least par- tially occluded
Further, we find that ∼NN% are pedestri- ans in unusual poses (e.g., sitting, cycling, or bent over)
 Hence, the feature maps infused with semantic features become more robust to atypical pedestrian shapes
These benefits are likely gained by semantic segmentation having indiscriminant coverage of all pedestrians, unlike object detection which requires specific alignment between pedestrians and anchor shapes
A similar advantage could be gained  for object detection by expanding the coverage of anchors,  but at the cost of computational complexity
 Proposal Padding: While padding proposals is an intuitive design choice to provide background context (Fig
N),  the benefit in practice is minor
Specifically, when proposal padding is disabled, the fused performance only worsens from N.NN% to N.NN% miss rate
Interestingly, pro- posal padding remains critical for the individual BCN performance, which degrades heavily from N0.NN% to NN.0N% without padding
The low sensitivty of the fused score to  padding suggests that the RPN is already capable of localizing and differentiating between partial and full-pedestrians,  thus improving the BCN in this respect is less significant
 Cost-sensitive: The cost-sensitive weighting scheme used  to regularize the importance of large pedestrians over small  pedestrians has an interesting effect on SDS-RCNN
When  the cost-sensitive weighting is disabled, the RPN performance actually improves to an impressive N.NN% miss rate
In contrast, without cost-sensitive weighting the BCN degrades heavily, while the fused score degrades mildly
A  logical explanation is that imposing a precedence on a single scale is counter-intuitive to the RPN achieving high  recall across all scales
Further, the RPN has the freedom to learn scale-dependent features, unlike the BCN  which warps to a fixed size for every proposal
Hence,  the BCN can gain significant boost when encouraged to focus on large pedestrian features, which may be more scaleindependent than features of small pedestrians
 Strict Supervision: Using a stricter labeling policy while  training the BCN has a substantial impact on the performance of both the BCN and fused scores
Recall that the  strict labeling policy requires a box to have IoU > 0.N to be considered foreground, while the standard policy requires  IoU > 0.N
When the stricter labeling policy is reduced to the standard policy, the fused performance degrades by  N.NN%
Further, the individual BCN degrades by N.NN%, which is on par with the degradation observed when weak  NNNN    Figure N
Visualization of the diversification between the RPN and  BCN classification characteristics
We plot only boxes which the  RPN and BCN of SDS-RCNN disagree on using a threshold of  0.N
The BCN drastically reduces false positives of the RPN, while  the RPN corrects many missed detections by the BCN
 segmentation is disabled
We examine the failure cases of  the strict versus non-strict BCN and observe that the false  positives caused by double detections reduce by ∼NN%
Hence, the stricter policy enables more aggressive suppression of poorly localized boxes and therefore reduces double  detections produced as localization errors of the RPN
 Stage Fusion: The power of stage-wise fusion relies on the  assumption that the each network will diversify in their classification characteristics
Our design explicitly encourages  this diversification by using separate labeling policies and  training distributions for the RPN and BCN
Table N shows  that although fusion is useful in every case, it is difficult to  anticipate how well any two stages will perform when fused  without examining their specific strengths and weaknesses
 To better understand this effect, we visualize how fusion behaves when the RPN and BCN disagree (Fig
N)
 We consider only boxes for which the RPN and BCN disagree using a decision threshold of 0.N
We notice that both networks agree on the majority of boxes (∼N0K), but ob- serve an interesting trend when they disagree
The visualization clearly shows that the RPN tends to predict a significant amount of background proposals with high scores,  which are corrected after being fused with the BCN scores
 The inverse is true for disagreements among the foreground,  where fusion is able to correct the majority of pedestrians  boxes given low scores by the BCN
It is clear that whenever  the two networks disagree, the fused result tends toward the  true score for more than ∼N0% of the conflicts
 Sharing Features: Since we choose to train a separate RPN  and BCN, without sharing features, we conduct comprehensive experiments using different levels of stage-wise sharing  in order to understand the value of diversification as a tradeoff to efficiency
We adopt the Faster R-CNN feature sharing scheme with five variations differing at the point of sharing (convN-N) as detailed in Table N
In each experiment, we  keep all layers of the BCN except those before and including the shared layer
Doing so keeps the effective depth of  the BCN unchanged
For example, if the shared layer is  Shared Layer BCN MR Fused MR Runtime  convN NN.NN N0.NN 0.NNs convN NN.NN N0.NN 0.NNs convN NN.NN N.NN 0.NNs convN NN.NN N.NN 0.NNs convN NN.0N N.NN 0.NNs  RGB N0.NN N.NN 0.NNs  Table N
Stage-wise sharing experiments which demonstrate the  trade-off of runtime efficiency and accuracy, using the Caltech  dataset
As sharing is increased from RGB (no sharing) to convN,  both the BCN and Fused miss rate (MR) become less effective
 convN then we replace convN-N of the BCN with a RoIPooling layer connected to convN of the RPN
We configure the  RoIPooling layer to pool to the resolution of the BCN at the  shared layer (e.g., convN → NN× NN, convN→ N× N)
We observe that as the amount of sharing is increased,  the overall fused performance degrades quickly
Overall,  the results suggest that forcing the networks to share feature maps lowers their freedom to diversify and complement in fusion
In other words, the more the networks share  the more susceptible they become to redundancies
Further,  sharing features up to convN becomes slower than no stagewise sharing (e.g., RGB)
This is caused by the increased  number of channels and higher resolution feature map of  convN (e.g., NN0 × NN0 × NN), which need to be cropped and warped
Compared to sharing feature maps with convN,  using no sharing results in a very minor slow down of 0.0N seconds while providing a N.N0% improvement to miss rate
Hence, our network design favors maximum precision for a  reasonable trade-off in efficiency, and obtains speeds generally N× faster than competitive methods
 N
Conclusion  We present a multi-task infusion framework for joint supervision on pedestrian detection and semantic segmentation
The segmentation infusion layer results in more sophisticated shared feature maps which tend to illuminate  pedestrians and make downstream pedestrian detection easier
We analyze how infusing segmentation masks into  feature maps helps correct pedestrian detection errors
In  doing so, we observe that the network becomes more robust to pedestrian poses and occlusion compared to without
We further demonstrate the effectiveness of fusing  stage-wise scores and encouraging network diversification  between stages, such that the second stage classifier can  learn a stricter filter to suppress background proposals and  become more robust to poorly localized boxes
In our SDSRCNN framework, we report new state-of-the-art performance on the Caltech pedestrian dataset (NN% relative re- duction in error), achieve competitive results on the KITTI  dataset, and obtain an impressive runtime approximately  N× faster than competitive methods
 NNNN    References  [N] P
Arbeláez, J
Pont-Tuset, J
T
Barron, F
Marques, and  J
Malik
Multiscale combinatorial grouping
In Proceedings of the IEEE Conference on Computer Vision and Pattern  Recognition, pages NNN–NNN, N0NN
N, N  [N] Z
Cai, Q
Fan, R
S
Feris, and N
Vasconcelos
A unified  multi-scale deep convolutional neural network for fast object detection
In European Conference on Computer Vision,  pages NNN–NN0
Springer, N0NN
N, N, N, N  [N] Z
Cai, M
Saberian, and N
Vasconcelos
Learning  complexity-aware cascades for deep pedestrian detection
In  Proceedings of the IEEE International Conference on Computer Vision, pages NNNN–NNNN, N0NN
N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Deeplab: Semantic image segmentation with  deep convolutional nets, atrous convolution, and fully connected crfs
arXiv preprint arXiv:NN0N.00NNN, N0NN
N  [N] M
Cordts, M
Omran, S
Ramos, T
Rehfeld, M
Enzweiler,  R
Benenson, U
Franke, S
Roth, and B
Schiele
The  cityscapes dataset for semantic urban scene understanding
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N, N  [N] J
Dai, K
He, and J
Sun
Instance-aware semantic segmentation via multi-task network cascades
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNN0–NNNN, N0NN
N, N  [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
 In Computer Vision and Pattern Recognition, N00N
CVPR  N00N
IEEE Conference on, pages NNN–NNN
IEEE, N00N
N  [N] P
Dollár, C
Wojek, B
Schiele, and P
Perona
Pedestrian  detection: A benchmark
In Computer Vision and Pattern  Recognition, N00N
CVPR N00N
IEEE Conference on, pages  N0N–NNN
IEEE, N00N
N, N, N, N, N, N  [N] P
Dollar, C
Wojek, B
Schiele, and P
Perona
Pedestrian detection: An evaluation of the state of the art
IEEE  transactions on pattern analysis and machine intelligence,  NN(N):NNN–NNN, N0NN
N, N  [N0] X
Du, M
El-Khamy, J
Lee, and L
S
Davis
Fused  dnn: A deep neural network fusion approach to fast and robust pedestrian detection
arXiv preprint arXiv:NNN0.0NNNN,  N0NN
N, N, N  [NN] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn,  and A
Zisserman
The PASCAL Visual Object Classes  Challenge N0NN (VOCN0NN) Results
http://www.pascalnetwork.org/challenges/VOC/vocN0NN/workshop/index.html
 N  [NN] M
Everingham, L
Van Gool, C
K
I
Williams, J
Winn,  and A
Zisserman
The PASCAL Visual Object Classes  Challenge N0NN (VOCN0NN) Results
http://www.pascalnetwork.org/challenges/VOC/vocN0NN/workshop/index.html
 N  [NN] S
Fidler, R
Mottaghi, A
Yuille, and R
Urtasun
Bottom-up  segmentation for top-down detection
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NN0N, N0NN
N, N  [NN] A
Geiger, P
Lenz, and R
Urtasun
Are we ready for autonomous driving? the kitti vision benchmark suite
In  Conference on Computer Vision and Pattern Recognition  (CVPR), N0NN
N, N, N, N  [NN] S
Gidaris and N
Komodakis
Object detection via a multiregion and semantic segmentation-aware cnn model
In Proceedings of the IEEE International Conference on Computer  Vision, pages NNNN–NNNN, N0NN
N  [NN] R
Girshick
Fast r-cnn
In Proceedings of the IEEE International Conference on Computer Vision, pages NNN0–NNNN,  N0NN
N, N  [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In Proceedings of the IEEE conference on  computer vision and pattern recognition, pages NN0–NNN,  N0NN
N  [NN] B
Hariharan, P
Arbeláez, R
Girshick, and J
Malik
Simultaneous detection and segmentation
In European Conference on Computer Vision, pages NNN–NNN
Springer, N0NN
N,  N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NN0–NNN, N0NN
N  [N0] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional architecture for fast feature embedding
arXiv preprint  arXiv:NN0N.N0NN, N0NN
N, N  [NN] J
Li, X
Liang, S
Shen, T
Xu, J
Feng, and S
Yan
Scaleaware fast r-cnn for pedestrian detection
arXiv preprint  arXiv:NNN0.0NNN0, N0NN
N, N, N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European Conference on Computer Vision, pages NN0–NNN
Springer, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  Advances in neural information processing systems, pages  NN–NN, N0NN
N, N, N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In Proceedings of the IEEE  Conference on Computer Vision and Pattern Recognition,  pages N–N, N0NN
N  [NN] Y
Tian, P
Luo, X
Wang, and X
Tang
Deep learning strong  parts for pedestrian detection
In Proceedings of the IEEE  International Conference on Computer Vision, pages NN0N–  NNNN, N0NN
N, N  [NN] S
Tsogkas, I
Kokkinos, G
Papandreou, and A
Vedaldi
 Deep learning for semantic part segmentation with high-level  guidance
arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] F
Yang, W
Choi, and Y
Lin
Exploit all the layers: Fast  and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers
In Proceedings of the  NNNN    IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N  [NN] L
Zhang, L
Lin, X
Liang, and K
He
Is faster rcnn doing well for pedestrian detection? arXiv preprint  arXiv:NN0N.0N0NN, N0NN
N, N, N, N, N  [N0] S
Zhang, R
Benenson, M
Omran, J
Hosang, and  B
Schiele
How far are we from solving pedestrian detection? In Proceedings of the IEEE Conference on Computer  Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N,  N  [NN] S
Zhang, R
Benenson, and B
Schiele
Filtered channel features for pedestrian detection
In Computer Vision and Pattern Recognition (CVPR), N0NN IEEE Conference on, pages  NNNN–NNN0
IEEE, N0NN
N  NNNNSoft-NMS -- Improving Object Detection With One Line of Code   Soft-NMS – Improving Object Detection With One Line of Code  Navaneeth Bodla* Bharat Singh* Rama Chellappa Larry S
Davis  Center For Automation Research, University of Maryland, College Park  {nbodla,bharat,rama,lsd}@umiacs.umd.edu  Abstract  Non-maximum suppression is an integral part of the object detection pipeline
First, it sorts all detection boxes on  the basis of their scores
The detection box M with the maximum score is selected and all other detection boxes  with a significant overlap (using a pre-defined threshold)  with M are suppressed
This process is recursively applied on the remaining boxes
As per the design of the algorithm,  if an object lies within the predefined overlap threshold, it  leads to a miss
To this end, we propose Soft-NMS, an algorithm which decays the detection scores of all other objects  as a continuous function of their overlap with M
Hence, no object is eliminated in this process
Soft-NMS obtains  consistent improvements for the coco-style mAP metric on  standard datasets like PASCAL VOC N00N (N.N% for both RFCN and Faster-RCNN) and MS-COCO (N.N% for R-FCN  and N.N% for Faster-RCNN) by just changing the NMS algorithm without any additional hyper-parameters
Using  Deformable-RFCN, Soft-NMS improves state-of-the-art in  object detection from NN.N% to N0.N% with a single model
 Further, the computational complexity of Soft-NMS is the  same as traditional NMS and hence it can be efficiently  implemented
Since Soft-NMS does not require any extra  training and is simple to implement, it can be easily integrated into any object detection pipeline
Code for SoftNMS is publicly available on GitHub http://bit.ly/  NnJLNMu
 N
Introduction  Object detection is a fundamental problem in computer  vision in which an algorithm generates bounding boxes for  specified object categories and assigns them classification  scores
It has many practical applications in autonomous  driving [N, N], video/image indexing [NN, NN], surveillance  [N, NN] etc
Hence, any new component proposed for the  object detection pipeline should not create a computational  *The first two authors contributed equally to this paper
 0.NN	  0.N0	  0.0?	  0.N?	  Figure N
This image has two confident horse detections (shown  in red and green) which have a score of 0.NN and 0.N respectively
 The green detection box has a significant overlap with the red one
 Is it better to suppress the green box altogether and assign it a score  of 0 or a slightly lower score of 0.N?  bottleneck, otherwise it will be conveniently “ignored’ in  practical implementations
Moreover, if a complex module  is introduced which requires re-training of models which  leads to a little improvement in performance, it will also be  ignored
However, if a simple module can improve performance without requiring any re-training of existing models,  it would be widely adopted
To this end, we present a soft  non-maximum suppression algorithm, as an alternative to  the traditional NMS algorithm in the current object detection pipeline
 Traditional object detection pipelines [N, N] employ a  multi-scale sliding window based approach which assigns  foreground/background scores for each class on the basis  of features computed in each window
However, neighboring windows often have correlated scores (which increases  false positives), so non-maximum suppression is used as  a post-processing step to obtain final detections
With the  advent of deep learning, the sliding window approach was  replaced with category independent region proposals generated using a convolutional neural network
In state-ofthe-art detectors, these proposals are input to a classification sub-network which assigns them class specific scores  NNNNN  http://bit.ly/NnJLNMu http://bit.ly/NnJLNMu   Input : B = {bN, .., bN}, S = {sN, .., sN}, Nt B is the list of initial detection boxes S contains corresponding detection scores Nt is the NMS threshold  begin  D ← {} while B N= empty do  m ← argmax S M ← bm D ← D  S  M; B ← B −M for bi in B do  if iou(M, bi) ≥ Nt then B ← B − bi; S ← S − si  end  si ← sif(iou(M, bi))  end  end  return D,S end  NMS  Soft-NMS  Figure N
The pseudo code in red is replaced with the one in green  in Soft-NMS
We propose to revise the detection scores by scaling  them as a linear or Gaussian function of overlap
 [NN, NN]
Another parallel regression sub-network refines  the position of these proposals
This refinement process improves localization for objects, but also leads to cluttered  detections as multiple proposals often get regressed to the  same region of interest (RoI)
Hence, even in state-of-the-art  detectors, non-maximum suppression is used to obtain the  final set of detections as it significantly reduces the number  of false positives
 Non-maximum suppression starts with a list of detection  boxes B with scores S 
After selecting the detection with the maximum score M, it removes it from the set B and appends it to the set of final detections D
It also removes any box which has an overlap greater than a threshold Nt with M in the set B
This process is repeated for remain- ing boxes B
A major issue with non-maximum suppres- sion is that it sets the score for neighboring detections to  zero
Thus, if an object was actually present in that overlap  threshold, it would be missed and this would lead to a drop  in average precision
However, if we lower the detection  scores as a function of its overlap with M, it would still be in the ranked list, although with a lower confidence
We  show an illustration of the problem in Fig N
 Using this intuition, we propose a single line modification to the traditional greedy NMS algorithm in which we  decrease the detection scores as an increasing function of  overlap instead of setting the score to zero as in NMS
Intuitively, if a bounding box has a very high overlap with M, it should be assigned a very low score, while if it has a low  overlap, it can maintain its original detection score
This  Soft-NMS algorithm is shown in Figure N
Soft-NMS leads  to noticeable improvements in average precision measured  over multiple overlap thresholds for state-of-the-object detectors on standard datasets like PASCAL VOC and MSCOCO
Since Soft-NMS does not require any extra-training  and is simple to implement, it can be easily integrated in the  object detection pipeline
 N
Related Work  NMS has been an integral part of many detection algorithms in computer vision for almost N0 years
It was first  employed in edge detection techniques [NN]
Subsequently,  it has been applied to multiple tasks like feature point detection [NN, NN, N0], face detection [NN] and object detection  [N, N, N0]
In edge detection, NMS performs edge thinning  to remove spurious responses [NN, N, NN]
In feature point  detectors [NN], NMS is effective in performing local thresholding to obtain unique feature point detections
In face detection [NN], NMS is performed by partitioning boundingboxes into disjoint subsets using an overlap criterion
The  final detections are obtained by averaging the co-ordinates  of the detection boxes in the set
For human detection, Dalal  and Triggs [N] demonstrated that a greedy NMS algorithm,  where a bounding box with the maximum detection score  is selected and its neighboring boxes are suppressed using a pre-defined overlap threshold improves performance  over the approach used for face detection [NN]
Since then,  greedy NMS has been the de-facto algorithm used in object  detection [N, N0, NN, NN]
 It is surprising that this component of the detection  pipeline has remained untouched for more than a decade
 Greedy NMS still obtains the best performance when average precision (AP) is used as an evaluation metric and is  therefore employed in state-of-the-art detectors [NN, NN]
A  few learning-based methods have been proposed as an alternative to greedy NMS which obtain good performance for  object class detection [N, NN, NN]
For example, [NN] first  computes overlap between each pair of detection boxes
It  then performs affinity propagation clustering to select exemplars for each cluster which represent the final detection  boxes
A multi-class version of this algorithm is proposed in  [NN]
However, object class detection is a different problem,  where object instances of all classes are evaluated simultaneously per image
Hence, we need to select a threshold for  all classes and generate a fixed set of boxes
Since different thresholds may be suitable for different applications, in  generic object detection, average precision is computed using a ranked list of all object instances in a particular class
 Therefore, greedy NMS performs favourably to these algoNNNN    NMS	  Figure N
In object detection, first category independent region  proposals are generated
These region proposals are then assigned  a score for each class label using a classification network and their  positions are updated slightly using a regression network
Finally,  non-maximum-suppression is applied to obtain detections
 rithms on generic object detection metrics
 In another line of work, for detecting salient objects, a  proposal subset optimization algorithm was proposed [N0]  as an alternative to greedy NMS
It performs a MAP-based  subset optimization to jointly optimize the number and locations of detection windows
In salient object detection, the  algorithm is expected to only find salient objects and not all  objects
So, this problem is also different from generic object detection and again greedy NMS performs favourably  when performance on object detection metrics is measured
 For special cases like pedestrian detection, a quadratic unconstrained binary optimization (QUBO) solution was proposed which uses detection scores as a unary potential and  overlap between detections as a pairwise potential to obtain the optimal subset of detection boxes [NN]
Like greedy  NMS, QUBO also applies a hard threshold to suppress detection boxes, which is different from Soft-NMS
In another learning-based framework for pedestrian detection, a  determinantal point process was combined with individualness prediction scores to optimally select final detections  [NN]
To the best of our knowledge, for generic object detection, greedy NMS is still the strongest baseline on challenging object detection datasets like PASCAL VOC and  MS-COCO
 N
Background  We briefly describe the object-detection pipeline used in  state-of-the-art object detectors in this section
During inference, an object detection network performs a sequence  of convolution operations on an image using a deep convolutional neural network (CNN)
The network bifurcates  into two branches at a layer L — one branch generates re- gion proposals while the other performs classification and  regression by pooling convolutional features inside RoIs  generated by the proposal network
The proposal network  generates classification scores and regression offsets for anchor boxes of multiple scales and aspect ratios placed at  each pixel in the convolutional feature map [NN]
It then  ranks these anchor boxes and selects the top K (≈ N000) anchors to which the bounding box regression offsets are  added to obtain image level co-ordinates for each anchor
 Greedy non-maximum suppression is applied to top K an- chors which eventually generates region proposals N
 The classification network generates classification and  regression scores for each proposal generated by the proposal network
Since there is no constraint in the network  which forces it to generate a unique RoI for an object, multiple proposals may correspond to the same object
Hence,  other than the first correct bounding-box, all other boxes on  the same object would generate false positives
To alleviate  this problem, non-maximum-suppression is performed on  detection boxes of each class independently, with a specified overlap threshold
Since the number of detections is  typically small and can be further reduced by pruning detections which fall below a very small threshold, applying  non-maximum suppression at this stage is not computationally expensive
We present an alternative approach to this  non-maximum suppression algorithm in the object detection pipeline
An overview of the object detection pipeline  is shown in Fig N
 N
Soft-NMS  Current detection evaluation criteria emphasise precise  localization and measure average precision of detection  boxes at multiple overlap thresholds (ranging from 0.N to  0.NN)
Therefore, applying NMS with a low threshold like  0.N could lead to a drop in average precision when the overlap criterion during evaluation for a true positive is 0.N (we  refer to the detection evaluation threshold as Ot from here on)
This is because, there could be a detection box bi which is very close to an object (within 0.N overlap), but  had a slightly lower score than M (M did not cover the object), thus bi gets suppressed by a low Nt
The likeli- hood of such a case would increase as the overlap threshold  criterion is increased
Therefore, suppressing all nearby detection boxes with a low Nt would increase the miss-rate
 Also, using a high Nt like 0.N would increase false posi- tives when Ot is lower and would hence drop precision av- eraged over multiple thresholds
The increase in false positives would be much higher than the increase in true positives for this case because the number of objects is typically  NWe do not replace this non-maximum suppression in the object detection pipeline
 NNNN    much smaller than the number of RoIs generated by a detector
Therefore, using a high NMS threshold is also not  optimal
 To overcome these difficulties, we revisit the NMS algorithm in greater detail
The pruning step in the NMS algorithm can be written as a re-scoring function as follows,  si =  {  si, iou(M, bi) < Nt  0, iou(M, bi) ≥ Nt ,  Hence, NMS sets a hard threshold while deciding what  should be kept or removed from the neighborhood of M
Suppose, instead, we decay the classification score of a box  bi which has a high overlap with M, rather than suppress- ing it altogether
If bi contains an object not covered by M, it won’t lead to a miss at a lower detection threshold
However, if bi does not cover any other object (while M covers an object), and even after decaying its score it ranks  above true detections, it would still generate a false positive
 Therefore, NMS should take the following conditions into  account,  • Score of neighboring detections should be decreased to an extent that they have a smaller likelihood of increasing the false positive rate, while being above obvious  false positives in the ranked list of detections
 • Removing neighboring detections altogether with a low NMS threshold would be sub-optimal and would  increase the miss-rate when evaluation is performed at  high overlap thresholds
 • Average precision measured over a range of overlap thresholds would drop when a high NMS threshold is  used
 We evaluate these conditions through experiments in  Section N.N
 Rescoring Functions for Soft-NMS: Decaying the  scores of other detection boxes which have an overlap with  M seems to be a promising approach for improving NMS
It is also clear that scores for detection boxes which have  a higher overlap with M should be decayed more, as they have a higher likelihood of being false positives
Hence, we  propose to update the pruning step with the following rule,  si =  {  si, iou(M, bi) < Nt  si(N− iou(M, bi)), iou(M, bi) ≥ Nt ,  The above function would decay the scores of detections  above a threshold Nt as a linear function of overlap with M
Hence, detection boxes which are far away from M would not be affected and those which are very close would  be assigned a greater penalty
 However, it is not continuous in terms of overlap and  a sudden penalty is applied when a NMS threshold of Nt is reached
It would be ideal if the penalty function was  continuous, otherwise it could lead to abrupt changes to  the ranked list of detections
A continuous penalty function should have no penalty when there is no overlap and  very high penalty at a high overlap
Also, when the overlap is low, it should increase the penalty gradually, as M should not affect the scores of boxes which have a very low  overlap with it
However, when overlap of a box bi with M becomes close to one, bi should be significantly penal- ized
Taking this into consideration, we propose to update  the pruning step with a Gaussian penalty function as follows,  si = sie −  iou(M,bi) N  σ , ∀bi /∈ D  This update rule is applied in each iteration and scores of  all remaining detection boxes are updated
 The Soft-NMS algorithm is formally described in Figure N, where f(iou(M, bi))) is the overlap based weight- ing function
The computational complexity of each step  in Soft-NMS is O(N), where N is the number of detec- tion boxes
This is because scores for all detection boxes  which have an overlap with M are updated
So, for N de- tection boxes, the computational complexity for Soft-NMS  is O(NN), which is the same as traditional greedy-NMS
Since NMS is not applied on all detection boxes (boxes with  a minimum threshold are pruned in each iteration), this step  is not computationally expensive and hence does not affect  the running time of current detectors
 Note that Soft-NMS is also a greedy algorithm and does  not find the globally optimal re-scoring of detection boxes
 Re-scoring of detection boxes is performed in a greedy fashion and hence those detections which have a high local score  are not suppressed
However, Soft-NMS is a generalized  version of non-maximum suppression and traditional NMS  is a special case of it with a discontinuous binary weighting function
Apart from the two proposed functions, other  functions with more parameters can also be explored with  Soft-NMS which take overlap and detection scores into account
For example, instances of the generalized logistic  function like the Gompertz function can be used, but such  functions would increase the number of hyper-parameters
 N
Datasets and Evaluation  We perform experiments on two datasets, PASCAL VOC  [N] and MS-COCO [NN]
The Pascal dataset has N0 object  categories, while the MS-COCO dataset has N0 object categories
We choose the VOC N00N test partition to measure  performance
For the MS-COCO dataset, sensitivity analysis is conducted on a publicly available minival set of N,000  images
We also show results on the test-dev partition on  NNNN    Method Training data Testing data AP 0.N:0.NN AP @ 0.N AP small AP medium AP large Recall @ N0 Recall @ N00  R-FCN [NN] train+valNNk test-dev NN.N NN.N NN.N NN.N NN.0 NN.N NN.N  R-FCN + S-NMS G train+valNNk test-dev NN.N NN.N NN.N NN.N NN.N NN.N NN.0  R-FCN + S-NMS L train+valNNk test-dev NN.N NN.N NN.N NN.0 NN.N NN.0 NN.0  F-RCNN [NN] train+valNNk test-dev NN.N NN.N N.N NN.N NN.N NN.N NN.N  F-RCNN + S-NMS G train+valNNk test-dev NN.N NN.N N.N NN.N NN.N NN.N NN.N  F-RCNN + S-NMS L train+valNNk test-dev NN.N NN.N N.N NN.N NN.N N0.N NN.N  D-RFCN [N] trainval test-dev NN.N NN.N NN.N N0.N NN.N NN.N NN.N  D-RFCN S-NMS G trainval test-dev NN.N N0.N NN.N NN.N NN.N N0.N NN.N  D-RFCN + MST trainval test-dev NN.N NN.N NN.N NN.N NN.N N0.N NN.N  D-RFCN + MST + S-NMS G trainval test-dev N0.N NN.N NN.N NN.N NN.N NN.N N0.N  Table N
Results on MS-COCO test-dev set for R-FCN, D-RFCN and Faster-RCNN (F-RCNN) which use NMS as baseline and our  proposed Soft-NMS method
G denotes Gaussian weighting and L denotes linear weighting
MST denotes multi-scale testing
 Method AP AP@0.N areo bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv  [NN]+NMS NN.N N0.0 NN.N NN.N NN.N NN.N NN.N N0.N N0.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N  [NN]+S-NMS G NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N  [NN]+S-NMS L NN.N NN.N N0.N NN.N NN.N NN.0 NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.0 NN.N  [NN]+NMS NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.0 NN.N NN.0 NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  [NN]+S-NMS G NN.N N0.0 NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 N0.N N0.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  [NN]+S-NMS L NN.N N0.0 NN.N NN.N NN.N N0.0 NN.N NN.N NN.N NN.0 NN.N NN.N N0.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  Table N
Results on Pascal VOC N00N test set for off-the-shelf standard object detectors which use NMS as baseline and our proposed  Soft-NMS method
Note that COCO-style evaluation is used
 the MS-COCO dataset which consists of N0,NNN images
 To evaluate our method, we experimented with three  state-of-the-art detectors, namely, Faster-RCNN [NN], RFCN [NN] and Deformable-RFCN
For the PASCAL  dataset, we selected publicly available pre-trained models provided by the authors
The Faster-RCNN detector was trained on VOC N00N train set while the R-FCN  detector was trained on VOC N00N and N0NN
For MSCOCO also, we use the publicly available model for FasterRCNN
However, since there was no publicly available  model trained on MS-COCO for R-FCN, we trained our  own model in Caffe [NN] starting from a ResNet-N0N CNN  architecture [NN]
Simple modifications like N scales for  RPN anchors, a minimum image size of N00, NN images  per minibatch and NNN ROIs per image were used
Training  was done on N GPUs in parallel
Note that our implementation obtains N.N% better accuracy than that reported in [NN]  without using multi-scale training or testing
Hence, this is  a strong baseline for R-FCN on MS-COCO
Both these detectors use a default NMS threshold of 0.N
In the sensitivity  analysis section, we also vary this parameter and show results
We also trained deformable R-FCN with the same settings
At a threshold of N0e-N, using N CPU threads, it takes  0.0Ns per image for N0 classes
After each iteration, detections which fall below the threshold are discarded
This  reduces computation time
At N0e-N, run time is 0.00N seconds on a single core
We set maximum detections per image to N00 on MS-COCO and the evaluation server selects  the top N00 detections per class for generating metrics (we  confirmed that the coco evaluation server was not selecting  top N00 scoring detections per image till June N0NN)
Setting maximum detections to N00 reduces coco-style AP by  0.N
 N
Experiments  In this section, we show comparative results and perform  sensitivity analysis to show robustness of Soft-NMS compared to traditional NMS
We also conduct specific experiments to understand why and where does Soft-NMS perform better compared to traditional NMS
 N.N
Results  In Table N we compare R-FCN and Faster-RCNN with  traditional non-maximum suppression and Soft-NMS on  MS-COCO
We set Nt to 0.N when using the linear weight- ing function and σ to 0.N with the Gaussian weighting function
It is clear that Soft-NMS (using both Gaussian  and linear weighting function) improves performance in all  cases, especially when AP is computed at multiple overlap thresholds and averaged
For example, we obtain an  improvement of N.N% and N.N% respectively for R-FCN  and Faster-RCNN, which is significant for the MS-COCO  dataset
Note that we obtain this improvement by just  changing the NMS algorithm and hence it can be applied  easily on multiple detectors with minimal changes
We perform the same experiments on the PASCAL VOC N00N test  set, shown in Table N
We also report average precision  averaged over multiple overlap thresholds like MS-COCO
 Even on PASCAL VOC N00N, Soft-NMS obtains an improvement of N.N% for both Faster-RCNN and R-FCN
For  detectors like SSD [NN] and YOLOvN [NN] which are not  proposal based, with the linear function, Soft-NMS only  obtains an improvement of 0.N%
This is because proposal  NNNN    Nt AP @ 0.N AP @ 0.N AP @ 0.N AP @ 0.N σ AP @ 0.N AP @ 0.N AP @ 0.N AP @ 0.N 0.N 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N 0.NN0N 0.NNNN 0.NNNN 0.NNNN  0.N 0.NNNN 0.NNN0 0.NNN0 0.NNNN 0.N 0.NNNN 0.NNNN 0.NNN0 0.NNNN  0.N 0.NNNN 0.NN0N 0.NNNN 0.NNNN 0.N 0.NNNN 0.NNNN 0.NNNN 0.NNNN  0.N 0.NNNN 0.NNN0 0.NNNN 0.NNNN 0.N 0.NNNN 0.NNNN 0.N00N 0.NNNN  0.N 0.NNNN 0.NNNN 0.NNN0 0.NNNN 0.N 0.NNNN 0.NNNN 0.NNNN 0.NNN0  0.N 0.NNNN 0.N0NN 0.NNNN 0.NNN0 N.N 0.NNNN 0.NNNN 0.NNNN 0.NNNN Table N
Sensitivity Analysis across multiple overlap thresholds Nt and parameters σ for NMS and Soft-NMS using R-FCN on coco  minival
Best performance at each Ot is marked in bold for each method
 0.0 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N  Nt  or σ  0.N0  0.NN  0.NN  0.NN  0.NN  0.N0  0.NN  0.NN  0.NN  0.NN  A P  [ 0 N :0 .N N ]  Sensitivity to hyper parameter R-FCN σ Faster-RCNN σ  R-FCN Nt Faster-RCNN Nt  Figure N
R-FCN Sensitivity to hyper parameters σ (Soft-NMS)  and Nt (NMS)  based detectors have higher recall and hence Soft-NMS has  more potential to improve recall at higher Ot
 From here on, in all experiments, when we refer to SoftNMS, it uses the Gaussian weighting function
In Fig N,  we also show per-class improvement on MS-COCO
It is  interesting to observe that Soft-NMS when applied on RFCN improves maximum performance for animals which  are found in a herd like zebra, giraffe, sheep, elephant, horse  by N-N%, while there is little gain for objects like toaster,  sports ball, hair drier which are less likely to co-occur in  the same image
 N.N
Sensitivity Analysis  Soft-NMS has a σ parameter and traditional NMS has an overlap threshold parameter Nt
We vary these param- eters and measure average precision on the minival set of  MS-COCO set for each detector, see Fig N
Note that AP is  stable between 0.N to 0.N and drops significantly outside this  range for both detectors
The variation in AP in this range is  around 0.NN% for traditional NMS
Soft-NMS obtains better performance than NMS from a range between 0.N to 0.N
 Its performance is stable from 0.N to 0.N and better by ∼N% for each detector even on the best NMS threshold selected  by us on the coco-minival set
In all our experiments, we set  σ to 0.N, even though a σ value of 0.N seems to give better performance on the coco minival set
This is because we  conducted the sensitivity analysis experiments later on and  a difference of 0.N% was not significant
 N.N
When does Soft-NMS work better?  Localization Performance Average precision alone  does not explain us clearly when Soft-NMS obtains significant gains in performance
Hence, we present average  precision of NMS and Soft-NMS when measured at different overlap thresholds
We also vary the NMS and SoftNMS hyper-parameters to understand the characteristics of  both these algorithms
From Table N, we can infer that average precision decreases as NMS threshold is increased
 Although it is the case that for a large Ot, a high Nt obtains slightly better performance compared to a lower Nt — AP does not drop significantly when a lower Nt is used
On the other hand, using a high Nt leads to significant drop in AP at lower Ot and hence when AP is averaged at multiple thresholds, we observe a performance drop
Therefore, a  better performance using a higher Nt does not generalize to lower values of Ot for traditional NMS
 However, when we vary σ for Soft-NMS, we observe a different characteristic
Table N shows that even when  we obtain better performance at higher Ot, performance at lower Ot does not drop
Further, we observe that Soft-NMS performs significantly better (∼N%) than traditional NMS irrespective of the value of the selected Nt at higher Ot
Also, the best AP for any hyper-parameter (Nt or σ) for a selected Ot is always better for Soft-NMS
This compari- son makes it very clear that across all parameter settings,  the best σ parameter for Soft-NMS performs better than a hard threshold Nt selected in traditional NMS
Further, when performance across all thresholds is averaged, since a  single parameter setting in Soft-NMS works well at multiple values of Ot, overall performance gain is amplified
As expected, low values of σ perform better at lower Ot and higher values of sigma perform better at higher Ot
Unlike NMS, where higher values of Nt lead to very little improve- ment in AP, higher values of σ lead to significant improve- ment in AP at a higher Ot
Therefore, a larger σ can be used  NNNN    N0 N0 N0 N0 N0 N0 N0 N0 N0  Recall  0  N0  N0  N0  N0  N00  P re ci si o n  Precision vs Recall at Ot  = 0.N  NMS  Soft-NMS  N0 N0 N0 N0 N0 N0 N0 N0 N0  Recall  0  N0  N0  N0  N0  N0  N0  N0  N0  N0  P re ci si o n  Precision vs Recall at Ot  = 0.N  NMS  Soft-NMS  N0 N0 N0 N0 N0 N0 N0 N0 N0  Recall  0  N0  N0  N0  N0  N0  N0  N0  P re ci si o n  Precision vs Recall at Ot  = 0.N  NMS  Soft-NMS  Figure N
R-FCN : Precision vs Recall at multiple overlap thresholds Ot  to improve performance of the detector for better localization which is not the case with NMS, as a larger Nt obtains very little improvement
 Precision vs Recall Finally, we would like to also know  at what recall values is Soft-NMS performing better than  NMS at different Ot
Note that we re-score the detection scores and assign them lower scores, so we do not expect  precision to improve at a lower recall
However, as Ot and recall is increased, Soft-NMS obtains significant gains in  precision
This is because, traditional NMS assigns a zero  score to all boxes which have an overlap greater than Nt with M
Hence, many boxes are missed and therefore pre- cision does not increase at higher values of recall
SoftNMS re-scores neighboring boxes instead of suppressing  them altogether which leads to improvement in precision at  higher values of recall
Also, Soft-NMS obtains significant  improvement even for lower values of recall at higher values of Ot because near misses are more likely to happen in this setting
 N.N
Qualitative Results  We show a few qualitative results in Fig N using a detection threshold of 0.NN for images from the COCO-validation  set
The R-FCN detector was used to generate detections
It  is interesting to observe that Soft-NMS helps in cases when  bad detections (false positives) have a small overlap with a  good detection (true positive) and also when they have a low  overlap with a good detection
For example, in the street  image (No.N), a large wide bounding box spanning multiple people is suppressed because it had a small overlap with  multiple detection boxes with a higher score than it
Hence,  its score was reduced multiple times because of which it  was suppressed
We observe a similar behaviour in image  No.N
In the beach image (No.N), the score for the larger  bounding box near the woman’s handbag is suppressed below 0.NN
We also see that a false positive near the bowl in  the kitchen image (No.N) is suppressed
In other cases, like  for zebra, horse and giraffe images (images N,N,N and NN),  the detection boxes get suppressed with NMS while SoftNMS assigns a slightly lower score for neighboring boxes  0 N N N  zebra  giraffe  elephant  sheep  horse  motorcycle  toilet  person  tennis	racket  bicycle  umbrella  suitcase  bed  cow  dog  teddy	bear  cat  baseball	bat  broccoli  train  airplane  wine	glass  pizza  couch  skateboard  car  carrot  banana  stop	sign  bear  keyboard  refrigerator  surfboard  sandwich  laptop  sink  donut  tie  kite  orange  clock  fork  chair  bird  bus  boat  apple  remote  oven  toothbrush  fire	hydrant  skis  bottle  hot	dog  dining	table  microwave  book  truck  traffic	light  parking	meter  bowl  cake  potted	plant  tv  backpack  cup  spoon  mouse  bench  handbag  snowboard  knife  cell	phone  scissors  frisbee  baseball	glove  vase  sports	ball  hair	drier  toaster  ⋙⋙⋙  ⋙⋙⋙  ⋙⋙⋙  0 N N N  zebra  giraffe  toilet  cat  elephant  cow  motorcycle  person  tennis	racket  broccoli  suitcase  horse  sheep  bed  teddy	bear  train  bear  bicycle  umbrella  pizza  clock  keyboard  car  dog  snowboard  banana  sink  couch  baseball	bat  airplane  skateboard  surfboard  carrot  hot	dog  wine	glass  sandwich  oven  refrigerator  truck  bird  parking	meter  mouse  fork  potted	plant  bus  donut  laptop  boat  kite  fire	hydrant  chair  traffic	light  skis  frisbee  microwave  bench  tie  dining	table  remote  bottle  cup  orange  toothbrush  book  stop	sign  baseball	glove  bowl  tv  cell	phone  vase  scissors  backpack  apple  sports	ball  knife  toaster  cake  spoon  handbag  hair	drier  ⋙⋙⋙  Figure N
Per class improvement in AP for MS-COCO using SoftNMS for R-FCN is shown in the left and for Faster-RCNN is  shown on the right
Green bars indicate improvements beyond  N %  NNNN    because of which we are able to detect true positives above  a detection threshold of 0.NN
 References  [N] J
Canny
A computational approach to edge detection
IEEE  Transactions on pattern analysis and machine intelligence,  (N):NNN–NNN, NNNN
N  [N] R
T
Collins, A
J
Lipton, T
Kanade, H
Fujiyoshi, D
Duggins, Y
Tsin, D
Tolliver, N
Enomoto, O
Hasegawa, P
Burt,  N0  N N N  N  N  N  N  N  N  N0  NN NN  NN  NN  NN NN NN  NN  NN  NN  NN  NN NN  NN NN  N0  Figure N
Qualitative results: Image pairs are shown in which the left image with blue bounding boxes is for traditional NMS, while the  right image with red bounding boxes is for Soft-NMS
Examples above the blue line are successful cases and below are failure cases
Even  in failure cases, Soft-NMS assigns a significantly lower score than the bounding box with a higher score
Note that for image No
NN, the  classes in consideration are for “person” and not “dog”, similarly in NN it is “bench” and in NN it is “potted plant”
 NNNN    et al
A system for video surveillance and monitoring
N000
 N  [N] J
Dai, H
Qi, Y
Xiong, Y
Li, G
Zhang, H
Hu, and  Y
Wei
Deformable convolutional networks
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [N] N
Dalal and B
Triggs
Histograms of oriented gradients for  human detection
In Computer Vision and Pattern Recognition, N00N
CVPR N00N
IEEE Computer Society Conference  on, volume N, pages NNN–NNN
IEEE, N00N
N, N  [N] C
Desai, D
Ramanan, and C
C
Fowlkes
Discriminative  models for multi-class object layout
International journal  of computer vision, NN(N):N–NN, N0NN
N  [N] P
Dollár, C
Wojek, B
Schiele, and P
Perona
Pedestrian  detection: A benchmark
In Computer Vision and Pattern  Recognition, N00N
CVPR N00N
IEEE Conference on, pages  N0N–NNN
IEEE, N00N
N  [N] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
International journal of computer vision, NN(N):N0N–  NNN, N0N0
N  [N] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and D
Ramanan
Object detection with discriminatively trained partbased models
IEEE transactions on pattern analysis and  machine intelligence, NN(N):NNNN–NNNN, N0N0
N, N  [N] A
Geiger, P
Lenz, C
Stiller, and R
Urtasun
Vision meets  robotics: The kitti dataset
The International Journal of  Robotics Research, NN(NN):NNNN–NNNN, N0NN
N  [N0] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In Proceedings of the IEEE conference on  computer vision and pattern recognition, pages NN0–NNN,  N0NN
N  [NN] I
Haritaoglu, D
Harwood, and L
S
Davis
W/sup N:  real-time surveillance of people and their activities
IEEE  Transactions on pattern analysis and machine intelligence,  NN(N):N0N–NN0, N000
N  [NN] C
Harris and M
Stephens
A combined corner and edge  detector
In Alvey vision conference, volume NN, pages N0–  NNNN
Citeseer, NNNN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NN0–NNN, N0NN
N  [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional architecture for fast feature embedding
In Proceedings of the NNnd ACM international conference on Multimedia, pages NNN–NNN
ACM, N0NN
N  [NN] D
Lee, G
Cha, M.-H
Yang, and S
Oh
Individualness and  determinantal point processes for pedestrian detection
In  European Conference on Computer Vision, pages NN0–NNN
 Springer, N0NN
N  [NN] Y
Li, K
He, J
Sun, et al
R-fcn: Object detection via regionbased fully convolutional networks
In Advances in Neural  Information Processing Systems, pages NNN–NNN, N0NN
N, N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European Conference on Computer Vision, pages NN0–NNN
Springer, N0NN
N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.Y
Fu, and A
C
Berg
Ssd: Single shot multibox detector
 In European Conference on Computer Vision, pages NN–NN
 Springer, N0NN
N  [NN] D
G
Lowe
Distinctive image features from scaleinvariant keypoints
International journal of computer vision, N0(N):NN–NN0, N00N
N  [N0] K
Mikolajczyk and C
Schmid
Scale & affine invariant interest point detectors
International journal of computer vision, N0(N):NN–NN, N00N
N  [NN] D
Mrowca, M
Rohrbach, J
Hoffman, R
Hu, K
Saenko,  and T
Darrell
Spatial semantic regularisation for large scale  object detection
In Proceedings of the IEEE international  conference on computer vision, pages N00N–N0NN, N0NN
N  [NN] J
Philbin, O
Chum, M
Isard, J
Sivic, and A
Zisserman
Object retrieval with large vocabularies and fast spatial matching
In Computer Vision and Pattern Recognition, N00N
CVPR’0N
IEEE Conference on, pages N–N
IEEE,  N00N
N  [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNN–NNN, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  Advances in neural information processing systems, pages  NN–NN, N0NN
N, N, N  [NN] A
Rosenfeld and M
Thurston
Edge and curve detection  for visual scene analysis
IEEE Transactions on computers,  N00(N):NNN–NNN, NNNN
N  [NN] R
Rothe, M
Guillaumin, and L
Van Gool
Non-maximum  suppression for object detection by passing messages between windows
In Asian Conference on Computer Vision,  pages NN0–N0N
Springer, N0NN
N  [NN] S
Rujikietgumjorn and R
T
Collins
Optimized pedestrian  detection for multiple and occluded people
In Proceedings  of the IEEE Conference on Computer Vision and Pattern  Recognition, pages NNN0–NNNN, N0NN
N  [NN] J
Sivic, A
Zisserman, et al
Video google: A text retrieval  approach to object matching in videos
In iccv, volume N,  pages NNN0–NNNN, N00N
N  [NN] P
Viola and M
Jones
Rapid object detection using a boosted  cascade of simple features
In Computer Vision and Pattern  Recognition, N00N
CVPR N00N
Proceedings of the N00N  IEEE Computer Society Conference on, volume N, pages I–I
 IEEE, N00N
N  [N0] J
Zhang, S
Sclaroff, Z
Lin, X
Shen, B
Price, and R
Mech
 Unconstrained salient object detection via proposal subset optimization
In Proceedings of the IEEE Conference  on Computer Vision and Pattern Recognition, pages NNNN–  NNNN, N0NN
N  [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object  proposals from edges
In European Conference on Computer  Vision, pages NNN–N0N
Springer, N0NN
N  NNNNDeeper, Broader and Artier Domain Generalization   Deeper, Broader and Artier Domain Generalization  Da Li Yongxin Yang Yi-Zhe Song Timothy M
Hospedales  Queen Mary University of London University of Edinburgh  {da.li, yongxin.yang, yizhe.song}@qmul.ac.uk, t.hospedales@ed.ac.uk  Abstract  The problem of domain generalization is to learn from  multiple training domains, and extract a domain-agnostic  model that can then be applied to an unseen domain
Domain generalization (DG) has a clear motivation in contexts where there are target domains with distinct characteristics, yet sparse data for training
For example recognition in sketch images, which are distinctly more abstract and  rarer than photos
Nevertheless, DG methods have primarily been evaluated on photo-only benchmarks focusing on  alleviating the dataset bias where both problems of domain  distinctiveness and data sparsity can be minimal
We argue that these benchmarks are overly straightforward, and  show that simple deep learning baselines perform surprisingly well on them
 In this paper, we make two main contributions: Firstly,  we build upon the favorable domain shift-robust properties of deep learning methods, and develop a low-rank parameterized CNN model for end-to-end DG learning
Secondly, we develop a DG benchmark dataset covering photo,  sketch, cartoon and painting domains
This is both more  practically relevant, and harder (bigger domain shift) than  existing benchmarks
The results show that our method outperforms existing DG alternatives, and our dataset provides  a more significant DG challenge to drive future research
 N
Introduction  Learning models that can bridge train-test domain-shift  is a topical issue in computer vision and beyond
In vision this has been motivated recently by the observation of  significant bias across popular datasets [NN], and the poor  performance of state-of-the-art models when applied across  datasets
Existing approaches can broadly be categorized  into domain adaptation (DA) methods, that use (un)labeled  target data to adapt source model(s) to a specific target domain [NN]; and domain generalization (DG) approaches,  that learn a domain agnostic model from multiple sources  that can be applied to any target domain [NN, N0]
While DA  has been more commonly studied, DG is the more valuable  Photo Art painting Sketch  T ra  in T e s t  V L C  S C  a lt e c h -O  ff ic  e    Train Test  Cartoon  Domain shift  Figure N: Contrast between prior Caltech Office and VLCS  datasets versus our new PACS dataset
The domain generalization task is to recognize categories in an unseen testing  domain
PACS provides more diverse domains with bigger  more challenging domain-shifts between them
 yet challenging setting, as it does not require acquisition of  a large target domain set for off-line analysis to drive adaptation
Such data may not even exist if the target domain  is sparse
Instead it aims to produce a more human-like  model, where there is a deeper semantic sharing across different domains – a dog is a dog no matter if it is depicted in  the form of a photo, cartoon, painting, or indeed, a sketch
 The most popular existing DA/DG benchmarks define  domains as photos of objects spanning different camera  types [NN], or datasets collected with different composition biases [NN]
While these benchmarks provide a good  start, we argue that they are neither well motivated nor  hard enough to drive the field
Motivation: The constituent  domains/datasets in existing benchmarks are based upon  conventional photos, albeit with different camera types or  composition bias
However there exist enough photos, that  NNNN    one could in principle collect enough target domain-specific  data to train a good model, or enough diverse data to cover  all domains and minimize bias (thus negating the need  for DA)
A more compelling motivation is domains where  the total available images is fundamentally constrained,  such as for particular styles of art [N, NN], and sketches  [NN, N, NN, NN]
Compared to photos, there may simply  not be enough examples of a given art style to train a good  model, even if we are willing to spend the effort
Difficulty:  The camera type and bias differences between domains in  existing benchmarks are already partially bridged by contemporary Deep features [N, NN], thus questioning the need  for DA or DG methods
In this paper, we show that multidomain deep learning provides a very simple but highly effective approach to DG that outperforms existing purposedesigned methods
 To address these limitations, we provide a harder and  better motivated benchmark dataset PACS, consisting of  images from photo (P), art painting (A), cartoon (C), and  sketch (S) domains
This benchmark carries two important  advancements over prior examples: (i) it extends the previously photo-only setting in DA/DG research, and uniquely  includes domains that are maximally distinct from each  other, spanning a wide spectrum of visual abstraction, from  photos that are the least abstract to human sketches which  are the most abstract; (ii) it is more reflective of a real-world  task where a target domain (such as sketch) is intrinsically  sparse, and so DG from a more abundant domain (such as  photos) is really necessary
As illustrated qualitatively in  Fig
N, the benchmark is harder, as the domains are visually more distinct than in prior datasets
We explore these  differences quantitatively in Sec
N.N
 There have been a variety of prior approaches to DG  based on SVM [NN, N0], subspace learning [NN], metric  learning [N], and autoencoders [N0]
Despite their differences, most of these have looked at fixed shallow features
 In this paper, we address the question of how end-to-end  learning of deep features impacts the DG setting
Our deep  learning approach trains on multiple source domains, and  extracts both domain agnostic features (e.g., convolutional  kernels), and classifier (e.g., final FC layer) for transfer to  a new target domain
This approach can be seen as a deep  multi-class generalization of the shallow binary Undo Bias  method [NN], which takes the form of a dynamically parameterized deep neural network [NN]
However, the resulting  number of parameters grows linearly with the number of  source domains (of which ultimately, we expect many for  DG), increasing overfitting risk
To address this we develop  a low-rank parameterized neural network which reduces the  number of parameters
Furthermore the low-rank approach  provides an additional route to knowledge sharing besides  through explicit parameterization
In particular it has the  further benefit of automatically modeling how related the  different domains are (e.g., perhaps sketch is similar to cartoon; and cartoon is similar to painting), and also how the  degree of sharing should vary at each layer of the CNN
 To summarize our contributions: Firstly, we highlight  the weaknesses of existing methods (they lose to a simple  deep learning baseline) and datasets (their domain shift is  small)
Second, we introduce a new, better motivated, and  more challenging DG benchmark
Finally, we develop a  novel DG method based on low-rank parameterized CNNs  that shows favorable performance compared to prior work
 N
Related work  Domain Generalization Despite different methodological tools (SVM, subspace learning, autoencoders, etc), existing methods approach DG based on a few different intuitions
One is to project the data to a new domain invariant representation where the differences between training domains is minimized [NN, N0], with the intuition that  such a space will also be good for an unseen testing domain
Another intuition is to predict which known domain  a testing sample seems most relevant to, and use that classifier [N0]
Finally, there is the idea of generating a domain  agnostic classifier, for example by asserting that each training domain’s classifier is the sum of a domain-specific and  domain-agnostic weight vector [NN]
The resulting domainagnostic weight vector can then be extracted and applied to  held out domains
Our approach lies in this latter category
 However, prior work in this area has dealt with shallow, linear models only
We show how to extend this intuition to  end-to-end learning in CNNs, while limiting the resulting  parameter growth, and making the sharing structure richer  than an unweighted sum
 There has been more extensive work on CNN models for  domain adaptation, with methods developed for encouraging CNN layers to learn transferable features [N, NN]
However, these studies have typically not addressed our domain  generalization setting
Moreover, as analysis has shown that  the transferability of different layers in CNNs varies significantly [NN], these studies have had carefully hand designed  the CNN sharing structure to address their particular DA  problems
In our benchmark, this is harder, as the gaps between our more diverse domains are unknown and likely  to be more variable
However, our low-rank modeling approach provides the benefit of automatically estimating both  the per-domain and per-layer sharing strength
 Domain Generalization is also related to learning to  learn
Learning to learn methods aim to learn not just specific concepts or skills, but learning algorithms or problem agnostic biases that improve generalization [N0, NN, N]
 Similarly DG is to extract common knowledge from source  domains that applies to unseen target domains
Thus our  method can be seen as a simple learning to learn method for  NNNN    the DG setting
Different from few-shot learning [N, NN],  DG is a zero-shot problem as performance is immediately  evaluated on the target domain with no further learning
 Neural Network Methods Our DG method is related to  parameterized neural networks [N, NN], in that the parameters are set based on external metadata
In our case, based  on a description of the current domain, rather than an instance [N], or additional sensor [NN]
It is also related to lowrank neural network models, typically used to compress  [NN] and speed up [NN] CNNs, and have very recently been  explored for cross-category CNN knowledge transfer [NN]
 In our case we exploit this idea both for compression – but  across rather than within domains [NN], as well as for crossdomain (rather than cross-category [NN]) knowledge sharing
Different domains can share parameters via common  latent factors
[N] also addresses the DG setting, but learns  shared parameters based on image reconstruction, whereas  ours is learned via paramaterizing each domain’s CNN
As  a parameterized neural network, our approach also differs  from all those other low-rank methods [NN, NN, NN], which  have a fixed parameterization
 N.N
Benchmarks and Datasets  DG Benchmarks The most popular DG benchmarks are:  ‘Office’ [NN] (containing Amazon/Webcam/DSLR images),  later extended to include a fourth Caltech N0N domain  [NN] (OfficeCaltech) and Pascal N00N, LabelMe, Caltech,  SUN0N (VLCS) [NN, NN]
The domains within Office relate  to different camera types, and the others are created by the  biases of different data collection procedures [NN]
Despite  the famous analysis of dataset bias [NN] that motivated the  creation of the VLCS benchmark, it was later shown that the  domain shift is much smaller with recent deep features [N]
 Thus recent DG studies have used deep features [N0], to obtain better results
Nevertheless, we show that a very simple  baseline of fine-tuning deep features on multiple source domains performs comparably or better than prior DG methods
This motivates our design of a CNN-based DG method,  as well as our new dataset (Fig N) which has greater domain  shift than the prior benchmarks
Our dataset draws on nonphotorealistic and abstract visual domains which provide a  better motivated example of the sort of relatively sparse data  domain where DG would be of practical value
 Non-photorealistic Image Analysis Non-photorealistic  image analysis is a growing subfield of computer vision  that extends the conventional photo-only setting of vision  research to include other visual depictions (often more abstract) such as paintings and sketches
Typical tasks include  instance-level matching between sketch-photo [NN, NN], and  art-photo domains [N], and transferring of object recognizers trained on photos to detect objects in art [N, NN]
Most  prior work focuses on two domains (such as photo and  painting [N, NN], or photo and sketch [NN, NN])
Studies have  investigated simple ‘blind’ transfer between domains [N],  learning cross-domain projections [NN, N], or engineering  structured models for matching [NN]
Thus, in contrast to  our DG setting, prior non-photorealistic analyses fall into  either cross-domain instance matching, or domain adaptation settings
To create our benchmark, we aggregate multiple domains including paintings, cartoons and sketches,  and define a comprehensive domain-generalization benchmark covering a wide spectrum of visual abstraction based  upon these
Thus in contrast to prior DG benchmarks, our  domain-shifts are bigger and more challenging
 N
Methodology  Assume we observe S domains, and the ith domain contains Ni labeled instances {(x (i) j , y  (i) j )}  Ni j=N where x  (i) j is the  input data (e.g., an image) for which we assume they are  of the same size among all domains (e.g., all images are  cropped into the same size), and y (i) j ∈ {N 


C} is the  class label
We assume the label space is consistent across  domains
The objective of DG is to learn a domain agnostic  model which can be applied to unseen domains in the future
In contrast to domain adaptation, we can not access  the labeled or unlabeled examples from those domains to  which the model is eventually applied
So the model is supposed to extract the domain agnostic knowledge within the  observed domains
In the training stage, we will minimize  the empirical error for all observed domains,  argmin ΘN,ΘN,...,ΘS  N  S  S∑  i=N  N  Ni  Ni∑  j=N  ℓ(ŷ (i) j , y  (i) j ) (N)  where ℓ is the loss function that measures the error between  the predicted label ŷ and the true label y, and prediction  is carried out by a function ŷ (i) j = f(x  (i) j |Θi) parameterized by Θi
A straightforward approach to finding a do- main agnostic model is to assume Θ∗ = ΘN = ΘN = · · · = ΘS , i.e., there exists a universal model Θ∗
Doing so we literally ignore the domain difference
Alternatively,  Undo-Bias [NN] considers linear models, and assumes that  the parameter (a D-dimensional vector when x ∈ RD) for the ith domain is in the form Θ(i) = Θ(0) + ∆(i), where Θ(0) can be seen as a domain agnostic model that benefits all domains, and ∆(i) is a domain specific bias term
Conceptually, Θ(0) can also serve as the classifier for any unseen domains
[NN] showed that (for linear models) Θ(0) is better than the universal model Θ∗ trained by  argmin Θ∗  N S  ∑S i=N  N Ni  ∑Ni j=N ℓ(Θ  T ∗ x (i) j , y  (i) j ) in terms of testing performance on unseen domains
However we show  that for deep networks, a universal model f(x|Θ∗) is a strong baseline that requires improved methodology to beat
 NNNN    N.N
Parameterized Neural Network for DG  To extend the idea of Undo-Bias [NN] into the neural network context, it is more convenient to think Θ(i) is gener- ated from a function g(z(i)|Θ) parameterized by Θ
Here z(i) is a binary vector encoding of the ith domain with two  properties: (i) it is of length S + N where S is the number of observed domains; (ii) it always has only two units activated (being one): the ith unit active for the ith domain and  the last unit active for all domains
Formally, the objective  function becomes,  argmin Θ  N  S  S∑  i=N  N  Ni  Ni∑  j=N  ℓ(ŷ (i) j , y  (i) j ) (N)  where ŷ (i) j = f(x  (i) j |Θi) = f(x  (i) j |g(z  (i)|Θ))
 To reproduce Undo-Bias [NN], we can stack all parameters in a column-wise fashion to form Θ, i.e., Θ = [∆(N),∆(N), 


,∆(S),Θ(0)], and choose the g(·) function to be linear mapping: g(z(i)|Θ) = Θz(i)
 From linear to multi-linear The method as described so  far generates the model parameter in the form of vector thus  it is only suitable for single-out setting (univariate regression or binary classification)
To generate higher order parameters, we use a multi-linear model, where Θ is (Nrd order or higher) tensor
E.g., to generate a weighting matrix for a  fully-connected layer in neural network, we can use  W (i) FC = g(z  (i)|W) = W ×N z (i) (N)  Here ×N is the inner product between tensor and vector along tensor’s Nrd axis
For example if W is the weight matrix of size H × C (i.e., the number of input neurons is H and the number of output neurons is C) then W is a H × C × (S + N) tensor
 If we need to generate the parameter for a convolutional  layer of size DN×DN×FN×FN (Height×Width×Depth× Filter Number), then we use:  W (i) CONV = g(z  (i)|W) = W ×N z (i) (N)  where W is a Nth order tensor of size DN×DN×FN×FN× (S + N)
 Domain generalization Using one such parameter generating function per layer, we can dynamically generate the  weights at every layer of a CNN based on the encoded vector of every domain
In this approach, knowledge sharing  is realized through the last (bias) bit in the encoding of z
 I.e., every weight tensor for a given domain is the sum of a  domain specific tensor and a (shared) domain agnostic tensor
For generalization to an unseen domain, we apply the  one-hot, bias-only, vector z∗ = [0, 0, 


, 0, N] to synthesize a domain agnostic CNN
 N.N
Low rank parameterized CNNs  The method as described so far has two limitations: (i)  the required parameters to learn now grow linearly in the  number of domains (which we eventually hope to be large  to achieve good DG), and (ii) the sharing structure is very  prescribed: every parameter is an equally weighted sum of  its domain agnostic and domain-specific bias partners
 To alleviate these two issues, we place a structural constraint on W 
Motivated by the well-known Tucker decom- position [NN], we assume that the M -order tensor W is syn- thesized as:  W = G ×N UN · · · ×M UM (N)  where G is a KN × 

.KM sized low-rank core tensor, and Um are Km × Dm matrices (note that DM = S + N)
By controlling the ranks KN 

.KM we can effec- tively reduce the number of parameters to learn
By learning {G, UN 


UM} instead of W , the number of param- eters is reduced from (DN × · · · × DM−N × (S + N)) to  (KN× 

.KM )+ ∑M−N  m=N Dm×Km+KM × (S+N)
Be- sides, UM produces a KM -dimensional dense vector that  guides how to linearly combine the shared factors, which  is much more informative than the original case of equally  weighted sum
 Given a tensor W the Tucker problem can be solved via high-order singular value decomposition (HO-SVD) [NN]
 G = W ×N U T N · · · ×M U  T M (N)  where Un is the U matrix from the SVD of the the mode-n  flattening of W 
However, note that aside from (optionally) performing this once for initialization, we do not perform  this costly HO-SVD operation during learning
 Inference and Learning To make predictions for a particular domain, we synthesize a concrete CNN by multiplying out the parameters {G, UN, 


, UM} after that do- ing an inner product with the corresponding domain’s z
 This CNN can then be used to classify an input instance x
 Since our method does not introduce any non-differentiable  functions, we can use standard back-propagation to learn  {G, UN, 


, UM} for every layer
 For our model there are hyperparameters – Tucker rank  [KN 

.KM ] – that can potentially be set at each layer
We sidestep the need to set all of these, by using the strategy  of decomposing the stack of (ImageNet pre-trained) single  domain models plus one agnostic domain model through  Tucker decomposition, and then applying a reconstruction  error threshold of ǫ = N0% for the HO-SVD in Eq N
This effectively determines all rank values via one ‘sharing  strength’ hyperparameter ǫ
 NNNN    N
Experiments  N.N
New Domain Generalization Dataset: PACS  Our PACS DG dataset is created by intersecting the  classes found in CaltechNNN (Photo), Sketchy (Photo,  Sketch) [NN], TU-Berlin (Sketch) [N] and Google Images  (Art painting, Cartoon, Photo)
Our dataset and code, together with latest results using alternative state-of-the-art  base networks, can be found at: http://sketchx
 eecs.qmul.ac.uk/
 PACS: Our new benchmark includes N domains (Photo,  Sketch, Cartoon, Painting), and N common categories ‘dog’,  ‘elephant’, ‘giraffe’
‘guitar’, ‘horse’, ‘house’, ‘person’
 The total number of images is NNNN
 N.N
Characterizing Benchmarks’ Domain Shifts  We first perform a preliminary analysis to contrast the  domain shift within our PACS dataset to that of prior popular datasets such as VLCS
We make this contrast from both  a feature space and a classifier performance perspective
 Feature Space Analysis Given the DG setting of training on source domains and applying to held out test domain(s), we measure the shift between source and target domains based on the Kullback-Leibler divergence as:  Dshift(D s, Dt) = N  m×n  n∑ i  m∑ j  λiKLD(D s i ||D  t j), where n  and m are the number of source and target domains, and λi weights the i th source domain, to account for data imbalance
To encode each domain as a probability, we calculate  the mean DECAFN representation over instances and then  apply softmax normalization
 Classifier Performance Analysis We also compare the  datasets by the margin between multiclass classification  accuracy of within-domain learning, and a simple crossdomain baseline of training a CNN on all the source domains before testing on the held out target domain (as we  shall see later, this baseline is very competitive)
Assuming within-domain learning performance is an upper bound,  then this difference indicates the space which a DG method  has to make a contribution, and hence roughly reflects size  of the domain-shift/difficulty of the DG task
 Results Fig
N(a) shows the average domain-shift in terms  of KLD across all choices of held out domain in our new  PACS benchmark, compared with the VLCS benchmark  [NN]
Clearly the domain shift is significantly higher in our  new benchmark, as is visually intuitive from the illustrative  examples in Fig
N
To provide a qualitative summarization,  we also show the distribution of features in our PACS compared to VLCS in Fig
N(b,c) as visualized by a N dimensional t-SNE [NN] plot, where the features are categorized  and colored by their associated domain
From this result,  we can see that the VLCS data are generally hard to separate by domain, while our PACS data are much more separated by domain
This illustrates the greater degree of shift  between the domains in PACS over VLCS
 We next explore the domain shifts from a model-, rather  than feature-centric perspective
Fig
Na summarizes the  within-domain and across-domain performance for each domain within PACS and VLCS benchmarks
The average  drop in performance due to cross-domain transfer is N0.N% for PACS versus N0.0% for VLCS
This shows that the scope for contribution of DG/DA in our PACS is double that  of VLCS, and illustrates the greater relevance and challenge  of the PACS benchmark
 N.N
Domain Generalization Experiments  N.N.N Datasets and Settings  We evaluate our proposed method on two datasets: VLCS,  and our proposed PACS dataset
VLCS [NN] aggregates  photos from Caltech, LabelMe, Pascal VOC N00N and  SUN0N
It provides a N-way multiclass benchmark on  the five common classes: ’bird’,’car’,’chair’,’dog’ and ’person’
Our PACS (described in Sec
N.N) with N classes from  Photo, Sketch, Cartoon, Painting domains
All results are  evaluated by multi-class accuracy, following [N0]
We explore features including Classic SIFT features (for direct  comparison with earlier work), DECAF pre-extracted deep  features following [N0], and ENE end-to-end CNN learning
 Settings: For our method in ENE configuration, we use  the ImageNet pre-trained AlexNet CNN, fine-tuned with  multi-domain learning on the training domains
On VLCS,  we follow the train-test split strategy from [N0]
Our initial learning rate is Ne-N and batch size is NN for each training domain
We use the best performed model on validation to do the test after tuning the model for NNk iterations
 On PACS, we split the images from training domains to N  (train) : N (val) and test on the whole held-out domain
Recall that our model uses a N-hot encoding of z to parameterize the CNN
The domain-specific vs agnostic ‘prior’ can  be set by varying the ratio ρ of the elements in the N-hot  coding
For training we use ρ = 0.N, so z = {[0, 0, 0.N, N], [0, 0.N, 0, N], ...}
For DG testing we use z = [0, 0, 0, N]
 Baselines: We evaluate our contributions by comparison  with number of alternatives including variants designed to  reveal insights, and state of the art competitors:  Ours-MLP: Our DG method applied to a N hidden layer  multi-layer perception
For use with pre-extracted features
 Ours-Full: Our full low-rank parameterized CNN trained  end-to-end on images
SVM: Linear SVM, applied on the  aggregation of data from all source domains
Deep-All:  Pretrained Alexnet CNN [NN], fine-tuned on the aggregation of all source domains
Undo-Bias: Modifies traditional SVM to include a domain-specific and global weight  vector which can be extracted for DG [NN]
The original  NNNN  http://sketchx.eecs.qmul.ac.uk/ http://sketchx.eecs.qmul.ac.uk/   VLCS PACS 0  0.N  0.N  0.N  0.N  N  0.0N  0.NN  (a) Average KLD  between domains
 Caltech  LabelMe  Pacal  SUN  (b) The feature distribution of VLCS  Art painting Cartoon Photo Sketch  (c) The feature distribution of PACS  Figure N: Evaluation of domain shift in different domain generalization benchmarks
 P A C S Avg 0  N0  N0  N0  N0  N00  A c c u  ra c y  PACS  Within Across  V L C S Avg 0  N0  N0  N0  N0  N00  A c c u  ra c y  VLCS  Within Across  N N N N Avg 0  N  N0  NN  N0  NN  N0  M a  rg in   W it h  in -A  c ro  s s  Cross-domain accuracy loss  PACS VLCS  (a) Measuring domain-shift by within versus across domain accuracy
Left: Our PACS, Middle:  VLCS
Right: Distribution of margins between within and across domain accuracy
 Co nv  N Co  nv N Co  nv N Co  nv N Co  nv N  FC N  FC N  FC N  0  N  N  N  R a n k  Sharing Strength by Layer  Sketch  Cartoon  Art  Photo  Avg  (b) Per-layer rank (inverse sharing  strength) after learning for different  held out domains
 Figure N: Cross-domain similarity (a) and learned sharing strength by layer (b)
 Undo-Bias is a binary classifier (BC)
We also implement a  multi-class (MC) generalization
uDICA: A kernel based  method learning a subspace to minimize the dissimilarity  between domains [NN]N
UML: Structural metric learning  algorithm learn a low-bias distance metric for classification tasks [N]
LRE-SVM: Exploits latent domains, and  a nuclear-norm based regularizer on the likelihood matrix  of exemplar-SVM [N0]
NHNN: N hidden layer neural network
MTAE-NHNN: NHNN with multi-task auto encoder  [N0]
D-MTAE-NHNN: NHNN with de-noising multi-task  auto encoder [N0]
DSN: The domain separation network  learns specific and shared models for the source and target  domains [N]
We re-purpose the original DSN from the domain adaptation to the DG task
Note that DSN is already  shown to outperform the related [N]
 N.N.N VLCS Benchmark  Classic Benchmark - Binary Classification with Shallow  Features Since our approach to extracting a domain invariant model is related to the intuition in Undo Bias [NN],  we first evaluate our methodology by performing a direct  comparison against Undo Bias
We use the same NNNN diNLike [N0], we found sDICA to be worse than uDICA, so excluded it
 mensional VLCS SIFT-BOW featuresN from [NN], and compare Our-MLP using one RELU hidden layer with N0NN  neurons
For direct comparison, we apply Our-MLP in  a N-vs-All manner as per Undo-Bias
The results in Table N show that without exploiting the benefit of end-to-end  learning, our approach still performs favorably compared  to Undo Bias
This is due to (i) our low-rank modeling of  domain-specific and domain-agnostic knowledge, and (ii)  the generalization of doing so in a multi-layer network
 Multi-class recognition with Deep Learning In this experiment we continue to analyze the VLCS benchmark,  but from a multiclass classification perspective
We compare existing DG methods (Undo-Bias [NN], UML [N],  LRE-SVM [N0], uDICA [NN], MTAE+NHNN [N0], DMTAE+NHNN [N0]) against baselines (NHNN, SVM, Deep)  and our methods Ours-MLP/Ours-Full
For the other methods besides Deep-All and Ours-Full, we follow [N0] and use  pre-extracted DECAFN features N [N]
For Deep and OursFull, we fine-tune the CNN on the source domains
 From the results in Table N, we make the following obNhttp://undoingbias.csail.mit.edu/ Nhttp://www.cs.dartmouth.edu/˜chenfang/proj_  page/FXR_iccvNN/index.php  NNNN  http://undoingbias.csail.mit.edu/ http://www.cs.dartmouth.edu/~chenfang/proj_page/FXR_iccvNN/index.php http://www.cs.dartmouth.edu/~chenfang/proj_page/FXR_iccvNN/index.php   Unseen domain Bird Car Chair Dog Person  Undo bias Ours-MLP Undo bias Ours-MLP Undo bias Ours-MLP Undo bias Ours-MLP Undo bias Ours-MLP  Caltech NN.0N N0.NN NN.N0 NN.NN N.NN NN.NN N.NN N.N0 N0.NN NN.NN  LabelMe NN.0N NN.NN NN.NN NN.0N N.NN N.NN N.NN N.0N NN.NN NN.00  Pascal NN.NN NN.NN NN.NN NN.NN N0.0N NN.NN NN.NN NN.NN NN.NN NN.NN  Sun 0.NN N.0N N0.NN NN.NN NN.NN NN.N0 N.NN N.NN NN.N0 NN.NN  Mean AP % NN.NN NN.NN N0.NN NN.NN N0.0N NN.NN N.NN N.NN NN.0N NN.NN  Table N: Comparison against Undo-Bias [NN] on the VLCS benchmark using classic SIFT-BOW features, and our shallow  model Ours-MLP
Average precision (%) and mean average precision (%) of binary N-v-all classification in unseen domains
 Unseen domain Image N→ Deep Feature N→ Classifier Image N→ ENE  SVM NHNN Undo-Bias[NN] uDICA[NN] UML[N] LRE-SVM[N0] MTAE+NHNN[N0] D-MTAE+NHNN[N0] Ours-MLP Deep-All Ours-Full  Caltech NN.NN NN.NN NN.N0 NN.N0 NN.NN NN.NN N0.NN NN.0N NN.NN NN.N0 NN.NN  LabelMe NN.NN NN.N0 NN.0N NN.NN NN.N0 NN.NN NN.NN N0.NN NN.NN NN.NN NN.NN  Pascal NN.NN NN.N0 NN.NN NN.NN NN.NN N0.NN NN.0N NN.N0 NN.NN NN.NN NN.NN  Sun NN.0N NN.NN NN.NN NN.NN NN.NN NN.NN N0.N0 NN.NN NN.NN NN.NN NN.NN  Ave.% NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.N0 NN.NN NN.0N NN.NN  Table N: Comparison of features and state of the art on the VLCS benchmark
Multi-class accuracy (%)
 servations: (i) Given the fixed DECAFN feature, most prior  DG methods improve on vanilla SVM, and D-MTAE [N0] is  the best of these
(ii) Ours-MLP outperforms NHNN, which  uses the same type of architecture and the same feature
 This margin is due to our low-rank domain-generalization  approach
(iii) The very simple baseline of fine-tuning a  deep model on the aggregation of source domains (DeepAll) performs surprisingly well and actually outperforms all  the prior DG methods
(iii) Ours-Full outperforms Deep-All  slightly
This small margin is understandable
Our model  does have more parameters to learn than Deep-All, despite  the low rank; and the cost of doing this is not justified by the  relatively small domain gap between the VLCS datasets
 N.N.N Our PACS benchmark  We compare baselines (SVM, NHNN) and prior methods  (LRE-SVM [N0], D-MTAE+NHNN [N0], uDICA [NN]) using DECAFN features against Deep-ALL, DSN [N] and  Ours-Full using end-to-end learning
From the results in Table N we make the observations: (i) uDICA and D-MTAENHNN are the best prior DG models, and DSN is also effective despite being designed for DA
While uDICA scores  well overall, this is mostly due to very high performance on  the photo domain
This is understandable as in that condition DICA uses unaltered DECAFN features tuned for photo  recognition
It is also the least useful direction for DG, as  photos are already abundant
(ii) As for the VLCS benchmark, Deep-ALL again performs well
(iii) However OursFull performs best overall by combining the robustness of a  CNN architecture with an explicit DG mechanism
 Ablation Study: To investigate the contributions of each  components in our framework, we compare the following  variants: Tuning-Last: Trains on all sources followed by direct application to the target
But fine-tunes the final FC  layer only
NHE-Last: Fine-tunes the final FC layer, and  uses our tensor weight generation (Eq
N) based on N-hot  encoding for multidomain learning, before transferring the  shared model component to the target
But without low  rank factorisation
NHE+Decomp-Last: Uses N-hot encoding based weight synthesis, and low-rank decomposition of  the final layer (Eq
N)
Ours-Full: Uses N-hot encoding and  low-rank modeling on every layer in the CNN
 From the results, we can see that each component  helps: (i) NHE-Last outperforms Tuning-Last, demonstrating the ability of our tensor weight generator to synthesize domain agnostic models for a multiclass classifier
 (ii) NHE+Decomp-Last outperforms NHE-Last, demonstrating the value of our low-rank tensor modeling of the  weight generator parameters
(iii) Ours-Full outperforms  NHE+Decomp-Last, demonstrating the value of performing  these DG strategies at every layer of the network
 N.N
Further Analysis  Learned Layer-wise Sharing Strength An interesting  property of our approach is that, unlike some other deep  learning methods [N, NN] it does not require manual specification of the cross-domain sharing structure at each layer of  the CNN; and unlike Undo Bias [NN] it can choose how to  share more flexibly through the rank choice at each layer
 We can observe the estimated sharing structure at each  layer by performing Tucker decomposition to factorize the  tuned model under a specified reconstruction error threshold (ǫ = 0.00N)
The resulting domain-rank at each layer reveals the sharing strength
The rank per-layer for each  held-out domain in PACS is shown in Fig
Nb
Here there  are three training domains, so the maximum rank is N and  the minimum rank is N
Intuitively, the results show heavily  shared ConvN-ConvN layers, and low-sharing in FCN-FCN  NNNN    Real image Photo AgnosticArt painting Cartoon Sketch  h o rs  e g ir  a ff e  h o u s e  Figure N: Visualization of the preferred images of output neurons ‘horse’, ‘giraffe’ and ‘house’ in the domains of the PACS  dataset
Left: real images
Middle: synthesized images for PACS domains
Right: synthesized images for agnostic domain
 Unseen domain Image N→ Deep Feature N→ Classifier Image N→ ENE  SVM NHNN uDICA [NN] LRE-SVM [N0] D-MTAE+NHNN [N0] Ours-MLP Deep-All DSN [N] Ours-Full  Art painting NN.NN NN.N0 NN.NN NN.NN N0.NN NN.N0 NN.N0 NN.NN NN.NN  Cartoon NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  Photo NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.N0 NN.NN NN.N0  Sketch NN.NN N0.NN NN.NN NN.NN NN.NN N0.NN NN.0N NN.NN NN.NN  Ave.% NN.NN NN.NN NN.00 NN.NN NN.NN NN.NN NN.0N NN.NN NN.NN  Table N: Evaluation % of classification on PACS
Multi-class accuracy (%)
 Unseen domain Ablation Study  Tuning-Last NHE-Last NHE+Decom-Last Ours-Full  Art painting NN.NN NN.N0 NN.NN NN.NN  Cartoon NN.NN NN.N0 NN.NN NN.NN  Photo NN.NN NN.NN NN.NN NN.N0  Sketch NN.NN NN.NN NN.NN NN.NN  Ave.% NN.N0 NN.NN NN.N0 NN.NN  Table N: Ablation study
Multi-class accuracy (%)
 layers
The middle layers ConvN and ConvN have different  sharing strength according to which domains provide the  source set
For example, in Conv N, when Sketch is unseen,  the other domains are relatively similar so can have greater  sharing, compared to when Sketch is included as a seen domain
This is intuitive as Sketch is the most different from  the other three domains
This flexible ability to determine  sharing strength is a key property of our model
 Visualization To visualize the preferences of our multidomain network, we apply the DGN-AM [NN] method to  synthesize the preferred input images for our model when  parameterized (via the domain descriptor z) to one specific domain versus the abstract domain-agnostic factor
 This visualization is imperfect because [NN] is trained using a photo-domain, and most of our domains are nonphotographic art
Nevertheless, from Fig
N the synthesis  for Photo domain seem to be the most concrete, while the  Sketch/Cartoon/Painting domains are more abstract
 N
Conclusion  We presented a new dataset and deep learning-based  method for domain generalization
Our PACS (Photo-ArtCartoon-Sketch) dataset is aligned with a practical application of domain generalization, and we showed it has more  challenging domain shift than prior datasets, making it suitable to drive the field in future
Our new domain generalization method integrates the idea of learning a domainagnostic classifier with a robust deep learning approach for  end-to-end learning of domain generalization
The result  performs comparably or better than prior approaches
 NNNN    References  [N] L
Bertinetto, J
F
Henriques, J
Valmadre, P
H
S
Torr,  and A
Vedaldi
Learning feed-forward one-shot learners
 In NIPS, N0NN
N  [N] K
Bousmalis, G
Trigeorgis, N
Silberman, D
Krishnan, and  D
Erhan
Domain separation networks
In NIPS, N0NN
N, N,  N, N  [N] E
J
Crowley, O
M
Parkhi, and A
Zisserman
Face painting: querying art with photos
In BMVC, N0NN
N  [N] J
Donahue, Y
Jia, O
Vinyals, J
Hoffman, N
Zhang,  E
Tzeng, and T
Darrell
Decaf: A deep convolutional activation feature for generic visual recognition
In ICML, N0NN
 N, N, N  [N] A
Z
E
J
Crowley
The art of detection
In ECCV Workshop  on Computer Vision for Art Analysis, N0NN
N, N  [N] M
Eitz, J
Hays, and M
Alexa
How do humans sketch  objects? TOG, N0NN
N, N  [N] C
Fang, Y
Xu, and D
N
Rockmore
Unbiased metric learning: On the utilization of multiple datasets and web images  for softening bias
In ICCV, N0NN
N, N, N  [N] C
Finn, P
Abbeel, and S
Levine
Model-agnostic metalearning for fast adaptation of deep networks
In ICML,  N0NN
N, N  [N] Y
Ganin and V
Lempitsky
Unsupervised domain adaptation  by backpropagation
In ICML, N0NN
N, N, N  [N0] M
Ghifary, W
Bastiaan Kleijn, M
Zhang, and D
Balduzzi
 Domain generalization for object recognition with multi-task  autoencoders
In ICCV, N0NN
N, N, N, N, N, N, N  [NN] B
Gong, Y
Shi, F
Sha, and K
Grauman
Geodesic flow  kernel for unsupervised domain adaptation
In CVPR, N0NN
 N  [NN] A
Khosla, T
Zhou, T
Malisiewicz, A
Efros, and A
Torralba
Undoing the damage of dataset bias
In ECCV, N0NN
 N, N, N, N, N, N, N  [NN] Y.-D
Kim, E
Park, S
Yoo, T
Choi, L
Yang, and D
Shin
 Compression of deep convolutional neural networks for fast  and low power mobile applications
In ICLR, N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, N0NN
N  [NN] L
D
Lathauwer, B
D
Moor, and J
Vandewalle
A multilinear singular value decomposition
SIMAX, N000
N  [NN] V
Lebedev, Y
Ganin, M
Rakhuba, I
Oseledets, and V
Lempitsky
Speeding-up convolutional neural networks using  fine-tuned cp-decomposition
In ICLR, N0NN
N  [NN] M
Long, Y
Cao, J
Wang, and M
I
Jordan
Learning transferable features with deep adaptation networks
In ICML,  N0NN
N, N  [NN] L
v
d
Maaten and G
Hinton
Visualizing data using t-sne
 JMLR, N00N
N  [NN] K
Muandet, D
Balduzzi, and B
Scholkopf
Domain generalization via invariant feature representation
In ICML, N0NN
 N, N, N, N  [N0] T
Munkhdalai and H
Yu
Meta networks
In ICML, N0NN
N  [NN] A
M
Nguyen, A
Dosovitskiy, J
Yosinski, T
Brox, and  J
Clune
Synthesizing the preferred inputs for neurons in  neural networks via deep generator networks
In NIPS, N0NN
 N  [NN] S
Ravi and H
Larochelle
Optimization as a model for fewshot learning
In ICLR, N0NN
N, N  [NN] K
Saenko, B
Kulis, M
Fritz, and T
Darrell
Adapting visual category models to new domains
In ECCV, N0N0
N,  N  [NN] P
Sangkloy, N
Burnell, C
Ham, and J
Hays
The sketchy  database: learning to retrieve badly drawn bunnies
TOG,  N0NN
N, N  [NN] O
Sigaud, C
Masson, D
Filliat, and F
Stulp
Gated networks: an inventory
arXiv, N0NN
N, N  [NN] J
Song, Y
Qian, Y.-Z
Song, T
Xiang, and T
Hospedales
 Deep spatial-semantic attention for fine-grained sketchbased image retrieval
In ICCV, N0NN
N  [NN] A
Torralba and A
A
Efros
Unbiased look at dataset bias
 In CVPR, N0NN
N, N, N  [NN] L
R
Tucker
Some mathematical notes on three-mode factor  analysis
Psychometrika, NNNN
N  [NN] Q
Wu, H
Cai, and P
Hall
Learning graphs to model visual  objects across different depictive styles
In ECCV
N0NN
N,  N  [N0] Z
Xu, W
Li, L
Niu, and D
Xu
Exploiting low-rank  structure from latent domains for domain generalization
In  ECCV, N0NN
N, N, N, N  [NN] Y
Yang and T
M
Hospedales
Deep multi-task representation learning: A tensor factorisation approach
In ICLR,  N0NN
N  [NN] J
Yosinski, J
Clune, Y
Bengio, and H
Lipson
How transferable are features in deep neural networks? In NIPS, N0NN
 N  [NN] Q
Yu, F
Liu, Y.-Z
Song, T
Xiang, T
M
Hospedales, and  C
C
Loy
Sketch me that shoe
In CVPR, N0NN
N, N  [NN] Q
Yu, Y
Yang, F
Liu, Y.-Z
Song, T
Xiang, and T
M
 Hospedales
Sketch-a-net: A deep neural network that beats  humans
In IJCV, N0NN
N  NNN0Learning Spatio-Temporal Representation With Pseudo-ND Residual Networks   Learning Spatio-Temporal Representation with Pseudo-ND Residual Networks ∗  Zhaofan Qiu †, Ting Yao ‡, and Tao Mei ‡  † University of Science and Technology of China, Hefei, China ‡ Microsoft Research, Beijing, China  zhaofanqiu@gmail.com, {tiyao, tmei}@microsoft.com  Abstract  Convolutional Neural Networks (CNN) have been regarded as a powerful class of models for image recognition problems
Nevertheless, it is not trivial when utilizing  a CNN for learning spatio-temporal video representation
 A few studies have shown that performing ND convolutions  is a rewarding approach to capture both spatial and temporal dimensions in videos
However, the development of a  very deep ND CNN from scratch results in expensive computational cost and memory demand
A valid question is  why not recycle off-the-shelf ND networks for a ND CNN
In  this paper, we devise multiple variants of bottleneck building blocks in a residual learning framework by simulating  N× N× N convolutions with N× N× N convolutional filters on spatial domain (equivalent to ND CNN) plus N × N × N convolutions to construct temporal connections on adjacent  feature maps in time
Furthermore, we propose a new architecture, named Pseudo-ND Residual Net (PND ResNet),  that exploits all the variants of blocks but composes each  in different placement of ResNet, following the philosophy  that enhancing structural diversity with going deep could  improve the power of neural networks
Our PND ResNet  achieves clear improvements on Sports-NM video classification dataset against ND CNN and frame-based ND CNN by  N.N% and N.N%, respectively
We further examine the generalization performance of video representation produced by  our pre-trained PND ResNet on five different benchmarks and three different tasks, demonstrating superior performances over several state-of-the-art techniques
 N
Introduction  Today’s digital contents are inherently multimedia: text, audio, image, video and so on
Images and videos, in  particular, become a new way of communication between  Internet users with the proliferation of sensor-rich mobile  ∗This work was performed when Zhaofan Qiu was visiting Mi- crosoft Research as a research intern
 PND ResNet ResNet CND  NN.N%  NN.N%  NN.N%  Video hit@N  Method Depth Model  size  CND NN NNNMB  ResNet NNN NNNMB  PND ResNet NNN NNNMB  Figure N
Comparisons of different models on Sports-NM dataset  in terms of accuracy, model size and the number of layers
 devices
This has encouraged the development of advanced  techniques for a broad range of multimedia understanding  applications
A fundamental progress that underlies the success of these technological advances is representation learning
Recently, the rise of Convolutional Neural Networks  (CNN) convincingly demonstrates high capability of learning visual representation especially in image domain
For  instance, an ensemble of residual nets [N] achieves N.NN%  top-N error on the ImageNet test set, which is even lower  than N.N% of the reported human-level performance
Nevertheless, video is a temporal sequence of frames with large  variations and complexities, resulting in difficulty in learning a powerful and generic spatio-temporal representation
 One natural way to encode spatio-temporal information  in videos is to extend the convolution kernels in CNN from  ND to ND and train a brand new ND CNN
As such, the networks have access not only the visual appearance present  in each video frame, but also the temporal evolution across  consecutive frames
While encouraging performances are  reported in recent studies [N, NN, NN], the training of ND CNN is very computationally expensive and the model size also  has a quadratic growth compared to ND CNN
Take a widely adopted NN-layer ND CNN, i.e., CND [NN] networks, as an  example, the model size reaches NNNMB which is even larger than that (NNNMB) of a NNN-layer ND ResNet (ResNetNNN) [N], making it extremely difficult to train a very deep  ND CNN
More importantly, directly fine-tuning ResNetNNN with frames in Sports-NM dataset [N0] may achieve better accuracy than CND trained on videos from scratch as  shown in Figure N
Another alternative solution of producing spatio-temporal video representation is to utilize poolNNNN    ing strategy or Recurrent Neural Networks (RNN) over the  representations of frames, which are often the activations  of a ND CNN’s last pooling layer or fully-connected layers
 This category of approaches, however, only build temporal  connections on the high-level features at the top layer while  leaving the correlations in the low-level forms, e.g., corners  or edges at the bottom layers, not fully exploited
 We demonstrate in this paper that the above limitations  can be mitigated by devising a family of bottleneck building  blocks that leverages both spatial and temporal convolutional filters
Specifically, the key component in each block is a  combination of one N × N × N convolutional layer and one layer of N × N × N convolutions in a parallel or cascaded fashion, that takes the place of a standard N × N × N con- volutional layer
As such, the model size is significantly  reduced and the advantages of pre-learnt ND CNN in image domain could also be fully leveraged by initializing the  N×N×N convolutional filters with N×N convolutions in ND CNN
Furthermore, we propose a novel Pseudo-ND Residual Net (PND ResNet) that composes each designed block  in different placement throughout a whole ResNet-like architecture to enhance the structural diversity of the network
 As a result, the temporal connections in our PND ResNet are  constructed at every level from bottom to top and the learnt  video representations encapsulate information related to objects, scenes and actions in videos, making them generic for  various video analysis tasks
 The main contribution of this work is the proposal of a  family of bottleneck building blocks that simulates ND convolutions in an economic and effective way
This also leads  to the elegant view of how different blocks should be placed  for learning very deep networks and a new PND ResNet is  presented for video representation learning
Through an extensive set of experiments, we demonstrate that our PND  ResNet outperforms several state-of-the-art models on five  different benchmarks and three different tasks
 N
Related Work  We briefly group the methods for video representation learning into two categories: hand-crafted and deep  learning-based methods
 Hand-crafted representation learning methods usually start by detecting spatio-temporal interest points and then  describe these points with local representations
In this  scheme, Space-Time Interest Points (STIP) [NN], Histogram  of Gradient and Histogram of Optical Flow [NN], ND Histogram of Gradient [NN] and SIFT-ND [NN] are proposed by  extending representations from image domain to measure  the temporal dimension of ND volumes
Recently, Wang et  al
propose dense trajectory features, which densely sample  local patches from each frame at different scales and then  track them in a dense optical flow field [NN]
 The most recent approaches for video representation  learning are to devise deep architectures
Karparthy et al
 stack CNN-based frame-level representations in a fixed size  of windows and then leverage spatio-temporal convolutions for learning video representation [N0]
In [NN], the famous two-stream architecture is devised by applying two  CNN architectures separately on visual frames and staked  optical flows
This architecture is further extended by exploiting multi-granular structure [NN, NN, NN], convolutional  fusion [N], key-volume mining [NN] and temporal segment  networks [NN] for video representation learning
In the work  by Wang et al
[NN], the local ConvNet responses over the  spatio-temporal tubes centered at the trajectories are pooled  as the video descriptors
Fisher Vector [N0] is then used  to encode these local descriptors to a global video representation
Recently, the LSTM-RNN networks have been  successfully employed for modeling temporal dynamics in  videos
In [N, NN], temporal pooling and stacked LSTM network are leveraged to combine frame-level (optical flow images) representation and discover long-term temporal relationships for learning a more robust video representation
 Srivastava et al
[NN] further formulate the video representation learning task as an autoencoder model based on the  encoder and decoder LSTMs
 It can be observed that most aforementioned deep  learning-based methods treat video as a frame/optical flow  image sequence for video representation learning while  leaving the temporal evolution across consecutive frames  not fully exploited
To tackle this problem, ND CNN proposed by Ji et al
[N] is one of the earlier works to directly learn the spatio-temporal representation of a short video  clip
Later in [NN], Tran et al
devise a widely adopted NNlayer ND CNN (CND) for learning video representation over  NN-frame video clips in the context of large-scale supervised  video datasets, and temporal convolutions across longer  clips (N00 frames) are further exploited in [NN]
However,  the capacity of existing ND CNN architectures is extremely limited with expensive computational cost and memory  demand, making it hard to train a very deep ND CNN
Our  method is different that we not only propose the idea of simulating ND convolutions with ND spatial convolutions plus  ND temporal connections which is more economic, but also  integrate this design into a deep residual learning framework for video representation learning
 N
PND Blocks and PND ResNet  In this section we firstly define the ND convolutions for  video representation learning which can be naturally decoupled into ND spatial convolutions to encode spatial information and ND temporal convolutional filters for temporal dimension
Then, a new family of bottleneck building blocks,  namely Pseudo-ND (PND), to leverage both spatial and temporal convolutional filters is devised in the residual learning  framework
Finally, we develop a novel Pseudo-ND ResiduNNNN    +  S  T  (a) PND-A  +  S T  (b) PND-B  +  S  T  (c) PND-C  Figure N
Three designs of Pseudo-ND blocks
 al Net (PND ResNet) composing each PND block at different  placement in ResNet-like architecture and further compare  its several variants through experimental studies in terms of  both performance and time efficiency
 N.N
ND Convolutions  Given a video clip with the size of c×l×h×w where c, l,  h and w denotes the number of channels, clip length, height  and width of each frame, respectively, the most natural way  to encode the spatio-temporal information is to utilize ND  convolutions [N, NN]
ND convolutions simultaneously model the spatial information like ND filters and construct temporal connections across frames
For simplicity, we denote  the size of ND convolutional filters as d× k × k where d is  the temporal depth of kernel and k is the kernel spatial size
 Hence, suppose we have ND convolutional filters with size  of N×N×N, it can be naturally decoupled into N×N×N con- volutional filters equivalent to ND CNN on spatial domain  and N× N× N convolutional filters like ND CNN tailored to temporal domain
Such decoupled ND convolutions can be  regarded as a Pseudo ND CNN, which not only reduces the  model size significantly, but also enables the pre-training of  ND CNN from image data, endowing Pseudo ND CNN more  power of leveraging the knowledge of scenes and objects  learnt from images
 N.N
Pseudo-ND Blocks  Inspired by the recent successes of Residual Networks (ResNet) [N] in numerous challenging image recognition  tasks, we develop a new family of building modules named  Pseudo-ND (PND) blocks to replace ND Residual Units in  ResNet, pursuing spatio-temporal encoding in ResNet-like  architectures for videos
Next, we will recall the basic design of Residual Units in ResNet, followed by presenting  how to devise our PND blocks
The bottleneck building architecture on each PND block is finally elaborated
 Residual Units
ResNet consists of many staked Residual Units and each Residual Unit could be generally given by  xt+N = h (xt) + F (xt) , (N)  where xt and xt+N denote the input and output of the tth Residual Unit, h (xt) = xt is an identity mapping and  F is a non-linear residual function
Hence, Eq.(N) can be  rewritten as  (I+ F) · xt = xt + F · xt := xt + F (xt) = xt+N, (N)  where F · xt represents the result of performing residual  function F over xt
The main idea of ResNet is to learn the  additive residual function F with reference to the unit inputs  xt which is realized through a shortcut connection, instead  of directly learning unreferenced non-linear functions
 PND Blocks design
To develop each ND Residual Unit in ResNet into ND architectures for encoding spatiotemporal video information, we modify the basic Residual Unit in ResNet following the principle of Pseudo ND  as introduced in Section N.N and devise several Pseudo-ND  Blocks
The modification is not straightforward for involvement of two design issues
The first issue is about whether  the modules of ND filters on spatial dimension (S) and ND  filters on temporal domain (T) should directly or indirectly  influence each other
Direct influence within the two types  of filters means that the output of spatial ND filters is connected as the input to the temporal ND filters (i.e., in a cascaded manner)
Indirect influence between the two filters  decouples the connection such that each kind of filters is on  a different path of the network (i.e., in a parallel fashion)
 The second issue is whether the two kinds of filters should  both directly influence the final output
As such, direct influence in this context denotes that the output of each type  of filters should be directly connected to the final output
 Based on the two design issues, we derive three different  PND blocks as depicted in Figure N, respectively, named as  PND-A to PND-C
Detailed comparisons about their architectures are provided as following:  (N) PND-A: The first design considers stacked architecture by making temporal ND filters (T) follow spatial ND  filters (S) in a cascaded manner
Hence, the two kinds of  filters can directly influence each other in the same path and  only the temporal ND filters are directly connected to the  final output, which could be generally given by  (I+T · S) · xt := xt +T (S (xt)) = xt+N
(N)  (N) PND-B: The second design is similar to the first one  except that indirect influence between two filters are adopted and both filters are at different pathways in a parallel  fashion
Although there is no direct influence between S  and T, both of them are directly accumulated into the final  output, which could be expressed as  (I+ S+T) · xt := xt + S (xt) +T (xt) = xt+N
(N)  (N) PND-C: The last design is a compromise between  PND-A and PND-B, by simultaneously building the direct  influences among S, T and the final output
Specifically,  to enable the direct connection between S and final output  based on the cascaded PND-A architecture, we establish a  NNNN    +  NxN conv  NxN conv  NxN conv  ReLU  ReLU  ReLU  (a) Residual Unit [N]  +  NxNxN conv  NxNxN conv  NxNxN conv  ReLU  ReLU  NxNxN conv  ReLU  ReLU  (b) PND-A  +  NxNxN conv  NxNxN conv  ReLU  ReLU  NxNxN conv NxNxN conv  + ReLU  ReLU  (c) PND-B  +  NxNxN conv  NxNxN conv  NxNxN conv  ReLU  ReLU  NxNxN conv  ReLU +  ReLU  (d) PND-C  Figure N
Bottleneck building blocks of Residual Unit and our Pseudo-ND
 shortcut connection from S to the final output, making the  output xt+N as  (I+ S+T · S) ·xt := xt +S (xt) +T (S (xt)) = xt+N
(N)  Bottleneck architectures
When specifying the architecture of ND Residual Unit, the basic ND block is modified  with a bottleneck design for reducing the computation complexity
In particular, as shown in Figure N(a), instead of a  single spatial ND filters (N × N convolutions), the Residual Unit adopts a stack of N layers including N × N, N × N, and N×N convolutions, where the first and last N×N convolution- al layers are applied for reducing and restoring dimensions  of input sample, respectively
Such bottleneck design makes  the middle N × N convolutions as a bottleneck with smaller input and output dimensions
Thus, we follow this elegant recipe and utilize the bottleneck design to implement our  proposed PND blocks
Similar in spirit, for each PND block  which purely consists of one spatial ND filters (N × N × N convolutions) and one temporal ND filters (N× N× N convo- lutions), we additionally place two N × N × N convolutions at both ends of the path, which are responsible for reducing and then increasing the dimensions
Accordingly, the  dimensions of the input and output of both the spatial ND  and temporal ND filters are reduced with this bottleneck design
The detailed bottleneck building architectures on all  the three PND blocks are illustrated in Figure N(b) to N(d)
 N.N
Pseudo-ND ResNet  In order to verify the merit of the three PND blocks,  we first develop three PND ResNet variants, i.e., PND-A  ResNet, PND-B ResNet and PND-C ResNet by replacing  all the Residual Units in a N0-layer ResNet (ResNet-N0)  [N] with one certain kind of PND block, respectively
The  comparisons of performance and time efficiency between  the basic ResNet-N0 and the three PND ResNet variants are  presented
Then, a complete version of PND ResNet is proposed by mixing all the three PND blocks from the viewpoint of structural diversity
 Table N
Comparisons of ResNet-N0 and different Pseudo-ND  ResNet variants in terms of model size, speed, and accuracy on  UCFN0N (splitN)
The speed is reported on one NVidia KN0 GPU
 Method Model size Speed Accuracy  ResNet-N0 NNMB NN.0 frame/s N0.N%  PND-A ResNet NNMB N.0 clip/s NN.N%  PND-B ResNet NNMB N.N clip/s NN.N%  PND-C ResNet NNMB N.N clip/s NN.0%  PND ResNet NNMB N.N clip/s NN.N%  Comparisons between PND ResNet variants
The  comparisons are conducted on UCFN0N [NN] video action recognition dataset
Specifically, the architecture of  ResNet-N0 is fine-tuned on UCFN0N video data
We set the  input as NNN× NNN image which is randomly cropped from the resized NN0 × NN0 video frame
Moreover, following [NN], we freeze the parameters of all Batch Normalization  layers except for the first one and add an extra dropout layer with 0.N dropout rate to reduce the effect of over-fitting
After fine-tuning ResNet-N0, the networks will predict one  score for each frame and the video-level prediction score  is calculated by averaging all frame-level scores
The architectures of three PND ResNet variants are all initialized  with ResNet-N0 except for the additional temporal convolutions and are further fine-tuned on UCFN0N
For each  PND ResNet variant, the dimension of input video clip is  set as NN × NN0 × NN0 which is randomly cropped from the resized non-overlapped NN-frame clip with the size of  NN × NNN × NNN
Each frame/clip is randomly horizontally flipped for data augmentation
In the training stage, we set  each mini-batch as NNN frames/clips, which are implemented with multiple GPUs in parallel
The network parameters  are optimized by standard SGD and the initial learning rate  is set as 0.00N, which is divided by N0 after every NK iterations
The training is stopped after N.NK iterations
 Table N shows the performance and time efficiency of  ResNet-N0 and our Pseudo-ND ResNet variants on UCFN0N
 Overall, all the three PND ResNet variants (i.e., PND-A  ResNet, PND-B ResNet and PND-C ResNet) exhibit better  performance than ResNet-N0 with only a small increase in  NNNN    Table N
Comparisons in terms of pre-train data, clip length, Top-N clip-level accuracy and Top-N&N video-level accuracy on Sports-NM
 Method Pre-train Data Clip Length Clip hit@N Video hit@N Video hit@N  Deep Video (Single Frame) [N0] ImageNetNK N NN.N% NN.N% NN.N%  Deep Video (Slow Fusion) [N0] ImageNetNK N0 NN.N% N0.N% N0.N%  Convolutional Pooling [NN] ImageNetNK NN0 N0.N% NN.N% N0.N%  CND [NN] – NN NN.N% N0.0% NN.N%  CND [NN] INN0K NN NN.N% NN.N% NN.N%  ResNet-NNN [N] ImageNetNK N NN.N% NN.N% NN.N%  PND ResNet (ours) ImageNetNK NN NN.N% NN.N% NN.N%  PND-A PND-B PND-C PND-A PND-B PND-C..
 Figure N
PND ResNet by interleaving PND-A, PND-B and PND-C
 model size
The results basically indicate the advantage of  exploring spatio-temporal information by our PND blocks
 Moreover, the speed of our PND ResNet variants is very fast  and could reach N.N ∼ N.0 clips per second
 Mixing different PND Blocks
Further inspired from the  recent success of pursuing structural diversity in the design  of very deep networks [NN], we devise a complete version  of PND ResNet by mixing different PND blocks in the architecture to enhance structural diversity, as depicted in Figure  N
Particularly, we replace Residual Units with a chain of  our PND blocks in the order PND-A→PND-B→PND-C
Table N also details the performance and speed of the complete  PND ResNet
By additionally pursuing structural diversity,  PND ResNet makes the absolute improvement over PND-A  ResNet, PND-B ResNet and PND-C ResNet by 0.N%, N.N%  and N.N% in accuracy respectively, indicating that enhancing structural diversity with going deep could improve the  power of neural networks
 N
Spatio-Temporal Representation Learning  We further validate the complete design of our PND  ResNet on a deeper NNN-layer ResNet [N] and then produce a generic spatio-temporal video representation
The  learning of PND ResNet here was conducted on Sports-NM  dataset [N0], which is one of the largest video classification  benchmark
It roughly contains about N.NN million videos  annotated with NNN Sports labels
There are NK-NK videos  per label and approximately N% of the videos are with more  than one label
Please also note that about N.N% video URLs were dead when we downloaded the videos
Hence, we  conducted the experiments on the remaining N.0N million  videos and followed the official split, i.e., N0%, N0% and  N0% for training, validation and test set, respectively
 Network Training
For efficient training on the large  Sports-NM training set, we randomly select five N-second  short videos from each video in the set
During training, the  settings of data augmentation and mini-batch are the same  as those in Section N.N except that the dropout rate is set  as 0.N
The learning rate is also initialized as 0.00N, and divided by N0 after every N0K iterations
The optimization  will be complete after NN0K batches
 Network Testing
We evaluate the performance of the  learnt PND ResNet by measuring video/clip classification  accuracy on the test set
Specifically, we randomly sample  N0 clips from each video and adopt a single center crop per  clip, which is propagated through the network to obtain a  clip-level prediction score
The video-level score is computed by averaging all the clip-level scores of a video
 We compare the following approaches for performance  evaluation: (N) Deep Video (Single Frame) and (Slow Fusion) [N0]
The former performs a CNN which is similar to  the architecture in [NN] on one single frame from each clip  to predict a clip-level score and fuses multiple frames in  each clip with different temporal extent throughout the network to achieve the clip-level prediction
(N) Convolutional  Pooling [NN] exploits max-pooling over the final convolutional layer of GoogleNet [N0] across each clip’s frames
 (N) CND [NN] utilizes ND convolutions on a clip volume to  model the temporal information and the whole architecture  could be trained on Sports-NM dataset from scratch or finetuned from the pre-trained model on INN0K internal dataset  collected in [NN]
(N) ResNet-NNN [N]
In this run, a NNNlayer ResNet is fine-tuned and employed on one frame from  each clip to produce a clip-level score
 The performances and comparisons are summarized in  Table N
Overall, our PND ResNet leads to a performance  boost against ResNet-NNN (ND CNN) and CND (ND CNN)  by N.N% and N.N% in terms of top-N video-level accuracy,  respectively
The results basically indicate the advantage of  exploring spatio-temporal information by decomposing ND  learning into ND convolutions in spatial space and ND operations in temporal dimension
As expected, Deep Video  (Slow Fusion) fusing temporal information throughout the  networks exhibits better performance than Deep Video (Single Frame) which exploits only one single frame
Though  the three runs of Deep Video (Slow Fusion), Convolutional Pooling and our PND ResNet all capitalizes on temporal  fusion, they are fundamentally different in the way of performing temporal connections
The performance of Deep  Video (Slow Fusion) is as a result of carrying out temporal convolutions on spatial convolutions to compute activations, while Convolutional Pooling is by simply maxpooling the outputs of final convolutional layer across temporal frames
As indicated by the results, our PND ResNet  employing different combinations of spatial and temporal  NNNN    motorcycle   racing  boxing  tai chi  horizontal bar  Figure N
Visualization of class knowledge inside PND ResNet model by using DeepDraw [N]
Four categories, i.e., tai chi, horizontal bar,  motorcycle racing and boxing, are selected for visualization
 convolutions improves Deep Video (Slow Fusion)
This  somewhat indicates that PND ResNet is benefited from the  principle of structural diversity in network design
It is also  not surprise that the performances of PND ResNet are still  lower than Convolutional Pooling which performs temporal pooling on NN0 frames’ clips with frame rate of N fps,  making the clip length over NN0s
In contrast, we take NN  consecutive frames as a basic unit which only covers less  than 0.Ns but has strong spatio-temporal connections, making our PND ResNet with better generalization capability
 Figure N further visualizes the insights in the learnt PND  ResNet model
Following [NN], we adopt DeepDraw toolbox [N], which conducts iterative gradient ascent on the input clip of white noises
During learning, it evaluates the  model’s violation of class label and back-propagates the  gradients to modify the input clip
Thus, the final generated input clip could be regarded as the visualization of class  knowledge inside PND ResNet
We select four categories,  i.e., tai chi, horizontal bar, motorcycle racing and boxing,  for visualization
As illustrated in the figure, PND ResNet  model could capture both spatial visual patterns and temporal motion
Take the category of tai chi as an example, our  model generates a video clip in which a person is displaying  different poses, depicting the process of this action
 PND ResNet Representation
After training our PND  ResNet architecture on Sports-NM dataset, the networks  could be utilized as a generic representation extractor for  any video analysis tasks
Given a video, we select N0 video  clips and each clip is with NN-frame long
Each video clip  is then input into the learnt PND ResNet architecture and  the N,0NN dimensional activations of poolN layer are output  as the representation of this clip
Finally, all the clip-level  representations in a video are averaged to produce a N,0NN  dimensional video representation
We refer to this representation as PND ResNet representation in the following evaluations unless otherwise stated
 N
Video Representation Evaluation  Next, we evaluate our PND ResNet video representation on three different tasks and five popular datasets, i.e.,  UCFN0N [NN], ActivityNet [N], ASLAN [NN], YUPENN [N]  and Dynamic Scene [NN]
UCFN0N and ActivityNet are two  of the most popular video action recognition benchmarks
 UCFN0N consists of NN,NN0 videos from N0N action categories
Three training/test splits are provided by the dataset  organisers and each split in UCFN0N includes about N.NK  training and N.NK test videos
The ActivityNet dataset is  a large-scale video benchmark for human activity understanding
The latest released version of the dataset (vN.N)  is exploited, which contains NN,NNN videos from N00 activity categories
The NN,NNN videos are divided into N0,0NN,  N,NNN and N,0NN videos for training, validation and test set,  respectively
It is also worth noting that the labels of test  set are not publicly available and thus the performances on  ActivityNet dataset are all reported on validation set
 ASLAN is a dataset on action similarity labeling task,  which is to predict the similarity between videos
The  dataset includes N,NNN videos from NNN action categories
 We follow the strategy of N0-fold cross validation with the  official data splits on this set
Furthermore, YUPENN and  Dynamic Scene are two sets for the scenario of scene recognition
In between, YUPENN is comprised of NN scene categories each containing N0 videos
Dynamic Scene consists  of NN scene classes with N0 videos per class
The training and test procedures on both datasets follow the standard  leave-one-video-out protocol
 Comparison with the state-of-the-art
We first compare with several state-of-the-art techniques in the context of video action recognition on three splits of UCFN0N  and ActivityNet validation set
The performance comparisons are summarized in Table N and N, respectively
We  briefly group the approaches on UCFN0N into three categories: end-to-end CNN architectures which are fine-tuned  on UCFN0N in the upper rows, CNN-based video representation extractors with linear SVM classifier in the middle  rows and approaches fused with IDT in the bottom rows
 It is worth noting that most recent end-to-end CNN architectures on UCFN0N often employ and fuse two or multiple types of inputs, e.g., frame, optical flow or even audio
 Hence, the performances by exploiting only video frames  and late fusing the scores on two inputs of video frames  plus optical flow are both reported
As shown in Table N,  NNNN    Table N
Performance comparisons with the state-of-the-art methods on UCFN0N (N splits)
TSN: Temporal Segment Networks [NN]; TDD: Trajectory-pooled Deep-convolutional Descriptor  [NN]; IDT: Improved Dense Trajectory [NN]
We group the approaches into three categories, i.e., end-to-end CNN architectures  which are fine-tuned on UCFN0N at the top, CNN-based video representation extractors with linear SVM classifier in the middle and  approaches fused with IDT at the bottom
For the methods in the  first direction, we report the performance of only taking frames  and frames plus optical flow (in brackets) as inputs, respectively
 Method Accuracy  End-to-end CNN architecture with fine-tuning  Two-stream ConvNet [NN] NN.0% (NN.0%)  Factorized ST-ConvNet [NN] NN.N% (NN.N%)  Two-stream + LSTM [NN] NN.N% (NN.N%)  Two-stream fusion [N] NN.N% (NN.N%)  Long-term temporal ConvNet [NN] NN.N% (NN.N%)  Key-volume mining CNN [NN] NN.N% (NN.N%)  ST-ResNet [N] NN.N% (NN.N%)  TSN [NN] NN.N% (NN.0%)  CNN-based representation extractor + linear SVM  CND [NN] NN.N%  ResNet-NNN NN.N%  PND ResNet NN.N%  Method fusion with IDT  IDT [NN] NN.N%  CND + IDT [NN] N0.N%  TDD + IDT [NN] NN.N%  ResNet-NNN + IDT NN.0%  PND ResNet + IDT NN.N%  the accuracy of PND ResNet can achieve NN.N%, making  the absolute improvement over the best competitor TSN on  the only frame input and ResNet-NNN in the first and second category by N.N% and N.N%, respectively
Compared to  [NN] which operates LSTM over high-level representations  of frames to explore temporal information, PND ResNet is  benefited from the temporal connections throughout the whole architecture and outperforms [NN]
PND ResNet with  only frame input is still superior to [NN, NN, NN] when fusing the results on the inputs of both frame and optical flow
 The results also consistently indicate that fusing two kinds  of inputs (performances in brackets) leads to apparent improvement compared to only using video frames
This motivates us to learn PND ResNet architecture with other types  of inputs in our future works
Furthermore, PND ResNet  utilizing ND spatial convolutions plus ND temporal convolutions exhibits significantly better performance than CND  which directly uses ND spatio-temporal convolutions
By  combining with IDT [NN] which are hand-crafted features,  the performance will boost up to NN.N%
In addition, by performing the recent state-of-the-art encoding method [NN] on  the activations of resNc layer in PND ResNet, the accuracy  can achieve N0.N%, making the improvement over the global representation from poolN layer in PND ResNet by N.N%
 The results across different evaluation metrics constantTable N
Performance comparisons in terms of Top-N&Top-N classification accuracy, and mean AP on ActivityNet validation set
A  linear SVM classifier is learnt on each feature
Method Top-N Top-N MAP  IDT [NN] NN.N0% NN.NN% NN.NN%  CND [NN] NN.N0% NN.NN% NN.NN%  VGG NN [NN] NN.NN% NN.N0% N0.NN%  ResNet-NNN [N] NN.NN% NN.NN% NN.NN%  PND ResNet NN.NN% NN.NN% NN.NN%  Table N
Action similarity labeling performances on ASLAN  benchmark
STIP: Space-Time Interest Points; MIP: Motion Interchange Patterns; FV: Fisher Vector
 Method Model Accuracy AUC  STIP [NN] linear N0.N% NN.N%  MIP [NN] metric NN.N% NN.N%  IDT+FV [NN] metric NN.N% NN.N%  CND [NN] linear NN.N% NN.N%  ResNet-NNN [N] linear N0.N% NN.N%  PND ResNet linear N0.N% NN.N%  ly indicate that video representation produced by our PND  ResNet attains a performance boost against baselines on  ActivityNet validation set, as shown in Table N
Specifically, PND ResNet outperforms IDT, CND, VGG NN and  ResNet-NNN by N0.N%, N.N%, N.N% and N.N% in terms of  Top-N accuracy, respectively
There is also a large performance gap between CND and ResNet-NNN
This is mainly  due to data shift that the categories in ActivityNet are mostly human activities in daily life, which are quite different  from those sport-related data in Sports-NM benchmark, resulting in not satisfying performance by CND learnt purely  on Sports-NM data
Instead, ResNet-NNN trained on ImageNet image data is found to be more helpful in this case
 PND ResNet which pre-trains ND spatial convolutions on  image data and learns ND temporal convolutions on video  data fully leverages the knowledge from two domains, successfully boosting up the performance
 The second task is action similarity labeling challenge,  which is to answer a binary question of “does a pair of  videos present the same action?” Following the experimental settings in [NN, NN], we extract the outputs of four layers  in PND ResNet, i.e., prob, poolN, resNc and resNbNN layer as  four types of representation for each NN-frame video clip
 The video-level representation is then obtained by averaging all clip-level representations
Given each video pair, we  calculate NN different similarities on each type of video representation and thus generate a NN-dimensional vector for  each pair
An LN normalization is implemented on the NNd vector and a binary classifier is trained by using linear  SVM
The performance comparisons on ASLAN are shown  in Table N
Overall, PND ResNet performs consistently better than both hand-crafted features and CNN-based representations across the performance metric of accuracy and  area under ROC curve (AUC)
In general, CNN-based representations exhibits better accuracy than hand-crafted feaNNNN    Table N
The accuracy performance of scene recognition on Dynamic Scene and YUPENN sets
Method Dynamic Scene YUPENN  [N] NN.N% N0.N%  [N] NN.N% NN.N%  CND [NN] NN.N% NN.N%  ResNet-NNN [N] NN.N% NN.N%  PND ResNet NN.N% NN.N%  N00 N00 N00 N00 N0 N0  Feature dimension  N0  N0  N0  N0  N0  N0  P e rc  e n  ta g  e  o  n  a  cc u  ra cy  IDT  CND  ResNet-NNN  PND ResNet  Figure N
The accuracy of video representation learnt by different  architectures with different dimensions
The performances reported in this figure are on UCFN0N (N splits)
 tures
Unlike the observations on action recognition task,  CND significantly outperforms ResNet-NNN on the scenario of action similarity labeling
We speculate that this may  be the result of difficulty in interpreting the similarity between videos based on the ResNet-NNN model learnt purely  on image domain
In contrast, the video representation extracted by CND which is trained on video data potentially  has higher capability to distinguish between videos
At this  point, improvements are also observed in PND ResNet
This  again indicates that PND ResNet is endowed with the advantages of both CND and ResNet-NNN by pre-training ND spatial convolutions on image data and learning ND temporal  connections on video data
 The third experiment was conducted on scene recognition
Table N shows the accuracy of different methods
 PND ResNet outperforms the state-of-the-art hand-crafted  features [N] by NN.N% and N.N% on Dynamic Scene and  YUPENN benchmark, respectively
Compared to CND and  ResNet-NNN, PND ResNet makes the absolute improvements  by N.N% and 0.N% on YUPENN, respectively
 The effect of representation dimension
Figure N compares the accuracy of video representation with different dimensions on UCFN0N by performing Principal Components  Analysis on the original features of IDT, ResNet-NNN, CND  and PND ResNet
Overall, video representation learnt by  PND ResNet consistently outperforms others at each dimension from N00 to N0
In general, higher dimensional representation provide better accuracy
An interesting observation is that the performance of ResNet-NNN decreases more  sharply than that of CND and PND ResNet when reducing  the representation dimension
This somewhat reveals the  weakness of ResNet-NNN in generating video representa(a) ResNet-NNN (b) PND ResNet  Figure N
Video representation embedding visualizations of  ResNet-NNN and PND ResNet on UCFN0N using t-SNE [NN]
Each  video is visualized as one point and colors denote different actions
 tion, which is originated from domain gap that ResNet-NNN  is learnt purely on image data and may degrade the representational capability on videos especially when the feature  dimension is very low
PND ResNet, in comparison, is benefited from the exploration of knowledge from both image  and video domain, making the learnt video representation  more robust to the change of dimension
 Video representation embedding visualization
Figure N further shows the t-SNE [NN] visualization of embedding of video representation learnt by ResNet-NNN and PND  ResNet
Specifically, we randomly select N0K videos from  UCFN0N and the video-level representation is then projected  into N-dimensional space using t-SNE
It is clear that video  representations by PND ResNet are better semantically separated than those of ResNet-NNN
 N
Conclusion  We have presented Pseudo-ND Residual Net (PND  ResNet) architecture which aims to learn spatio-temporal  video representation in deep networks
Particularly, we study the problem of simplifying ND convolutions with ND  filters on spatial dimension plus ND temporal connections
 To verify our claim, we have devised variants of bottleneck  building blocks for combining the ND spatial and ND temporal convolutions, and integrated them into a residual learning framework at different placements for structural diversity purpose
The PND ResNet architecture learnt on SportsNM dataset validate our proposal and analysis
Experiments  conducted on five datasets in the context of video action  recognition, action similarity labeling and scene recognition  also demonstrate the effectiveness and generalization of the  spatio-temporal video representation produced by our PND  ResNet
Performance improvements are clearly observed  when comparing to other feature learning techniques
 Our future works are as follows
First, attention mechanism will be incorporated into our PND ResNet for further enhancing representation learning
Second, an elaborated study will be conducted on how the performance of  PND ResNet is affected when increasing the frames in each  video clip in the training
Third, we will extend PND ResNet  learning to other types of inputs, e.g., optical flow or audio
 NNN0    References  [N] Deep draw
https://github.com/auduno/deepdraw
N  [N] F
Caba Heilbron, V
Escorcia, B
Ghanem, and J
Carlos Niebles
Activitynet: A large-scale video benchmark for  human activity understanding
In CVPR, N0NN
N  [N] K
Derpanis, M
Lecce, K
Daniildis, and R
Wildes
Dynamic scene understanding: The role of orientation features  in space and time in scene classification
In CVPR, N0NN
N,  N  [N] C
Feichtenhofer, A
Pinz, and R
Wildes
Spatiotemporal  residual networks for video action recognition
In NIPS,  N0NN
N  [N] C
Feichtenhofer, A
Pinz, and R
P
Wildes
Bags of spacetime energies for dynamic scene recognition
In CVPR, N0NN
 N  [N] C
Feichtenhofer, A
Pinz, and A
Zisserman
Convolutional  two-stream network fusion for video action recognition
In  CVPR, N0NN
N, N  [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N, N, N, N, N, N  [N] S
Ji, W
Xu, M
Yang, and K
Yu
Nd convolutional neural networks for human action recognition
IEEE Trans
on  PAMI, NN, N0NN
N, N, N  [N] Y.-G
Jiang, Z
Wu, J
Wang, X
Xue, and S.-F
Chang
Exploiting feature and class relationships in video categorization with regularized deep neural networks
IEEE Trans
on  PAMI, N0NN
N  [N0] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In CVPR, N0NN
N, N, N  [NN] A
Klaser, M
Marszałek, and C
Schmid
A spatio-temporal  descriptor based on Nd-gradients
In BMVC, N00N
N  [NN] O
Kliper-Gross, Y
Gurovich, T
Hassner, and L
Wolf
Motion interchange patterns for action recognition in unconstrained videos
In ECCV, N0NN
N  [NN] O
Kliper-Gross, T
Hassner, and L
Wolf
The action similarity labeling challenge
IEEE Trans
on PAMI, NN(N), N0NN
 N, N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, N0NN
N  [NN] I
Laptev
On space-time interest points
International journal of computer vision, NN(N-N):N0N–NNN, N00N
N  [NN] I
Laptev, M
Marszalek, C
Schmid, and B
Rozenfeld
 Learning realistic human actions from movies
In CVPR,  N00N
N  [NN] Q
Li, Z
Qiu, T
Yao, T
Mei, Y
Rui, and J
Luo
Action  recognition by learning deep multi-granular spatio-temporal  video representation
In ICMR, N0NN
N  [NN] Q
Li, Z
Qiu, T
Yao, T
Mei, Y
Rui, and J
Luo
Learning hierarchical video representation for action recognition
 International Journal of Multimedia Information Retrieval,  pages N–NN, N0NN
N  [NN] X
Peng, Y
Qiao, Q
Peng, and Q
Wang
Large margin  dimensionality reduction for action similarity labeling
IEEE  Signal Processing Letters, NN(N):N0NN–N0NN, N0NN
N  [N0] F
Perronnin, J
Sánchez, and T
Mensink
Improving the  fisher kernel for large-scale image classification
In ECCV,  N0N0
N  [NN] Z
Qiu, Q
Li, T
Yao, T
Mei, and Y
Rui
Msr asia msm at  thumos challenge N0NN
In CVPR workshop, N0NN
N  [NN] Z
Qiu, T
Yao, and T
Mei
Deep quantization: Encoding  convolutional activations with deep generative model
In  CVPR, N0NN
N  [NN] P
Scovanner, S
Ali, and M
Shah
A N-dimensional sift descriptor and its application to action recognition
In ACM  MM, N00N
N  [NN] N
Shroff, P
Turaga, and R
Chellappa
Moving vistas: Exploiting motion for describing scenes
In CVPR, N0N0
N  [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In NIPS, N0NN
N,  N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 N  [NN] K
Soomro, A
R
Zamir, and M
Shah
UCFN0N: A dataset  of N0N human action classes from videos in the wild
CRCVTR-NN-0N, N0NN
N, N  [NN] N
Srivastava, E
Mansimov, and R
Salakhutdinov
Unsupervised learning of video representations using lstms
In  ICML, N0NN
N  [NN] L
Sun, K
Jia, D.-Y
Yeung, and B
E
Shi
Human action  recognition using factorized spatio-temporal convolutional  networks
In ICCV, N0NN
N  [N0] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
N  [NN] D
Tran, L
Bourdev, R
Fergus, L
Torresani, and M
Paluri
 Learning spatiotemporal features with Nd convolutional networks
In ICCV, N0NN
N, N, N, N, N, N  [NN] L
van der Maaten and G
Hinton
Visualizing data using  t-sne
JMLR, N00N
N  [NN] G
Varol, I
Laptev, and C
Schmid
Long-term temporal  convolutions for action recognition
In arXiv preprint arXiv:NN0N.0NNNN, N0NN
N, N, N  [NN] H
Wang and C
Schmid
Action recognition with improved  trajectories
In ICCV, N0NN
N, N  [NN] L
Wang, Y
Qiao, and X
Tang
Action recognition with  trajectory-pooled deep-convolutional descriptors
In CVPR,  N0NN
N, N  [NN] L
Wang, Y
Xiong, Z
Wang, Y
Qiao, D
Lin, X
Tang, and  L
Van Gool
Temporal segment networks: towards good  practices for deep action recognition
In ECCV, N0NN
N, N,  N, N  [NN] J
Yue-Hei Ng, M
Hausknecht, S
Vijayanarasimhan,  O
Vinyals, R
Monga, and G
Toderici
Beyond short snippets: Deep networks for video classification
In CVPR,  N0NN
N, N, N  [NN] X
Zhang, Z
Li, C
C
Loy, and D
Lin
Polynet: A pursuit  of structural diversity in very deep networks
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
N  [NN] W
Zhu, J
Hu, G
Sun, X
Cao, and Y
Qiao
A key volume  mining deep framework for action recognition
In CVPR,  N0NN
N, N  NNNN  https://github.com/auduno/deepdrawHashNet: Deep Learning to Hash by Continuation   HashNet: Deep Learning to Hash by Continuation∗  Zhangjie Cao†, Mingsheng Long†, Jianmin Wang†, and Philip S
Yu†‡  †KLiss, MOE; NEL-BDS; TNList; School of Software, Tsinghua University, China ‡University of Illinois at Chicago, IL, USA  caozhangjieNN@gmail.com {mingsheng,jimwang}@tsinghua.edu.cn psyu@uic.edu  Abstract  Learning to hash has been widely applied to approximate nearest neighbor search for large-scale multimedia retrieval, due to its computation efficiency and retrieval quality
Deep learning to hash, which improves retrieval quality  by end-to-end representation learning and hash encoding,  has received increasing attention recently
Subject to the illposed gradient difficulty in the optimization with sign activations, existing deep learning to hash methods need to first  learn continuous representations and then generate binary  hash codes in a separated binarization step, which suffer  from substantial loss of retrieval quality
This work presents  HashNet, a novel deep architecture for deep learning to  hash by continuation method with convergence guarantees,  which learns exactly binary hash codes from imbalanced  similarity data
The key idea is to attack the ill-posed gradient problem in optimizing deep networks with non-smooth  binary activations by continuation method, in which we begin from learning an easier network with smoothed activation function and let it evolve during the training, until it  eventually goes back to being the original, difficult to optimize, deep network with the sign activation function
Comprehensive empirical evidence shows that HashNet can generate exactly binary hash codes and yield state-of-the-art  multimedia retrieval performance on standard benchmarks
 N
Introduction  In the big data era, large-scale and high-dimensional media data has been pervasive in search engines and social networks
To guarantee retrieval quality and computation efficiency, approximate nearest neighbors (ANN) search has attracted increasing attention
Parallel to the traditional indexing methods [NN], another advantageous solution is hashing methods [NN], which transform high-dimensional media  data into compact binary codes and generate similar binary  codes for similar data items
In this paper, we will focus  on learning to hash methods [NN] that build data-dependent  ∗Corresponding author: M
Long (mingsheng@tsinghua.edu.cn)
 hash encoding schemes for efficient image retrieval, which  have shown better performance than data-independent hashing methods, e.g
Locality-Sensitive Hashing (LSH) [N0]
 Many learning to hash methods have been proposed to  enable efficient ANN search by Hamming ranking of compact binary hash codes [NN, NN, N0, N, NN, NN, NN, NN, NN, NN]
 Recently, deep learning to hash methods [N0, N0, NN, N, NN,  NN, NN] have shown that end-to-end learning of feature representation and hash coding can be more effective using  deep neural networks [NN, N], which can naturally encode  any nonlinear hash functions
These deep learning to hash  methods have shown state-of-the-art performance on many  benchmarks
In particular, it proves crucial to jointly learn  similarity-preserving representations and control quantization error of binarizing continuous representations to binary  codes [NN, NN, NN, NN]
However, a key disadvantage of these  deep learning to hash methods is that they need to first learn  continuous deep representations, which are binarized into  hash codes in a separated post-step of sign thresholding
By  continuous relaxation, i.e
solving the discrete optimization  of hash codes with continuous optimization, all these methods essentially solve an optimization problem that deviates  significantly from the hashing objective as they cannot learn  exactly binary hash codes in their optimization procedure
 Hence, existing deep hashing methods may fail to generate  compact binary hash codes for efficient similarity retrieval
 There are two key challenges to enabling deep learning  to hash truly end-to-end
First, converting deep representations, which are continuous in nature, to exactly binary hash  codes, we need to adopt the sign function h = sgn (z) as activation function when generating binary hash codes using similarity-preserving learning in deep neural networks
 However, the gradient of the sign function is zero for all  nonzero inputs, which will make standard back-propagation  infeasible
This is known as the ill-posed gradient problem,  which is the key difficulty in training deep neural networks  via back-propagation [NN]
Second, the similarity information is usually very sparse in real retrieval systems, i.e., the  number of similar pairs is much smaller than the number  of dissimilar pairs
This will result in the data imbalance  NNN0N    problem, making similarity-preserving learning ineffective
 Optimizing deep networks with sign activation remains an  open problem and a key challenge for deep learning to hash
 This work presents HashNet, a new architecture for deep  learning to hash by continuation with convergence guarantees, which addresses the ill-posed gradient and data imbalance problems in an end-to-end framework of deep feature  learning and binary hash encoding
Specifically, we attack  the ill-posed gradient problem in the non-convex optimization of the deep networks with non-smooth sign activation  by the continuation methods [N], which address a complex  optimization problem by smoothing the original function,  turning it into a different problem that is easier to optimize
 By gradually reducing the amount of smoothing during the  training, it results in a sequence of optimization problems  converging to the original optimization problem
A novel  weighted pairwise cross-entropy loss function is designed  for similarity-preserving learning from imbalanced similarity relationships
Comprehensive experiments testify that  HashNet can generate exactly binary hash codes and yield  state-of-the-art retrieval performance on standard datasets
 N
Related Work  Existing learning to hash methods can be organized into  two categories: unsupervised hashing and supervised hashing
We refer readers to [NN] for a comprehensive survey
 Unsupervised hashing methods learn hash functions that  encode data points to binary codes by training from unlabeled data
Typical learning criteria include reconstruction  error minimization [NN, NN, NN] and graph learning[NN, NN]
 While unsupervised methods are more general and can be  trained without semantic labels or relevance information,  they are subject to the semantic gap dilemma [NN] that highlevel semantic description of an object differs from lowlevel feature descriptors
Supervised methods can incorporate semantic labels or relevance information to mitigate the  semantic gap and improve the hashing quality significantly
 Typical supervised methods include Binary Reconstruction  Embedding (BRE) [NN], Minimal Loss Hashing (MLH) [N0]  and Hamming Distance Metric Learning [NN]
Supervised  Hashing with Kernels (KSH) [NN] generates hash codes by  minimizing the Hamming distances across similar pairs and  maximizing the Hamming distances across dissimilar pairs
 As deep convolutional neural network (CNN) [NN, NN]  yield breakthrough performance on many computer vision  tasks, deep learning to hash has attracted attention recently
 CNNH [N0] adopts a two-stage strategy in which the first  stage learns hash codes and the second stage learns a deep  network to map input images to the hash codes
DNNH [N0]  improved the two-stage CNNH with a simultaneous feature  learning and hash coding pipeline such that representations  and hash codes can be optimized in a joint learning process
 DHN [NN] further improves DNNH by a cross-entropy loss  and a quantization loss which preserve the pairwise similarity and control the quantization error simultaneously
DHN  obtains state-of-the-art performance on several benchmarks
 However, existing deep learning to hash methods only  learn continuous codes g and need a binarization post-step  to generate binary codes h
By continuous relaxation, these  methods essentially solve an optimization problem L(g) that deviates significantly from the hashing objective L(h), because they cannot keep the codes exactly binary after convergence
Denote by Q(g,h) the quantization error func- tion by binarizing continuous codes g into binary codes h
 Prior methods control the quantization error in two ways:  (a) minL(g) + Q(g,h) through continuous optimization [NN, NN]; (b) minL(h)+Q(g,h) through discrete optimiza- tion on L(h) but continuous optimization on Q(g,h) (the continuous optimization is used for out-of-sample extension  as discrete optimization cannot be extended to the test data)  [NN]
However, since Q(g,h) cannot be minimized to zero, there is a large gap between continuous codes and binary  codes
To directly optimize minL(h), we must adopt sign as the activation function within deep networks, which enables generation of exactly binary codes but introduces the  ill-posed gradient problem
This work is the first effort to  learn sign-activated deep networks by continuation method,  which can directly optimize L(h) for deep learning to hash
 N
HashNet  In similarity retrieval systems, we are given a training set  of N points {xi} N i=N, each represented by a D-dimensional  feature vector xi ∈ R D
Some pairs of points xi and xj are  provided with similarity labels sij , where sij = N if xi and xj are similar while sij = 0 if xi and xj are dissimilar
The goal of deep learning to hash is to learn nonlinear hash function f : x N→ h ∈ {−N, N} K  from input space RD to Hamming space {−N, N}K using deep neural networks, which encodes each point x into compact K-bit binary hash code h = f(x) such that the similarity information between the given pairs S can be preserved in the compact hash codes
In supervised hashing, the similarity set S = {sij} can be constructed from semantic labels of data points or relevance  feedback from click-through data in real retrieval systems
 To address the data imbalance and ill-posed gradient  problems in an end-to-end learning framework, this paper  presents HashNet, a novel architecture for deep learning to  hash by continuation, shown in Figure N
The architecture  accepts pairwise input images {(xi,xj , sij)} and processes them through an end-to-end pipeline of deep representation  learning and binary hash coding: (N) a convolutional network (CNN) for learning deep representation of each image  xi, (N) a fully-connected hash layer (fch) for transforming the deep representation into K-dimensional representation zi ∈ R  K , (N) a sign activation function h = sgn (z) for binarizing the K-dimensional representation zi into K-bit  NN0N    +N  -N  x  y  0  N  input CNNs fch sgn similarity  label  weighted  crossentropy   loss  -N -N 0 N N  -N  N  h=tanh(β b z)  h=tanh(β g z)  h=tanh(β o z)  z  h  Figure N
(left) The proposed HashNet for deep learning to hash by continuation, which is comprised of four key components: (N) Standard  convolutional neural network (CNN), e.g
AlexNet and ResNet, for learning deep image representations, (N) a fully-connected hash layer  (fch) for transforming the deep representation into K-dimensional representation, (N) a sign activation function (sgn) for binarizing the K-dimensional representation into K-bit binary hash code, and (N) a novel weighted cross-entropy loss for similarity-preserving learning  from sparse data
(right) Plot of smoothed responses of the sign function h = sgn (z): Red is the sign function, and blue, green and orange show functions h = tanh (βz) with bandwidths βb < βg < βo
The key property is limβ→∞ tanh (βz) = sgn (z)
Best viewed in color
 binary hash code hi ∈ {−N, N} K , and (N) a novel weighted  cross-entropy loss for similarity-preserving learning from  imbalanced data
We attack the ill-posed gradient problem  of the non-smooth activation function h = sgn (z) by con- tinuation, which starts with a smoothed activation function  y = tanh (βx) and becomes more non-smooth by increas- ing β as the training proceeds, until eventually goes back to the original, difficult to optimize, sign activation function
 N.N
Model Formulation  To perform deep learning to hash from imbalanced data,  we jointly preserve similarity information of pairwise images and generate binary hash codes by weighted maximum  likelihood [N]
For a pair of binary hash codes hi and hj ,  there exists a nice relationship between their Hamming distance distH(·, ·) and inner product 〈·, ·〉: distH (hi,hj) = N  N (K − 〈hi,hj〉)
Hence, the Hamming distance and inner  product can be used interchangeably for binary hash codes,  and we adopt inner product to quantify pairwise similarity
 Given the set of pairwise similarity labels S = {sij}, the Weighted Maximum Likelihood (WML) estimation of the  hash codes H = [hN, 


,hN ] for all N training points is  logP (S|H) = ∑  sij∈S  wij logP (sij |hi,hj), (N)  where P (S|H) is the weighted likelihood function, and wij is the weight for each training pair (xi,xj , sij), which is used to tackle the data imbalance problem by weighting the  training pairs according to the importance of misclassifying  that pair [N]
Since each similarity label in S can only be sij = N (similar) or sij = 0 (dissimilar), to account for the data imbalance between similar and dissimilar pairs, we set  wij = cij ·  {  |S| / |SN| , sij = N  |S| / |S0| , sij = 0 (N)  where SN = {sij ∈ S : sij = N} is the set of similar pairs and S0 = {sij ∈ S : sij = 0} is the set of dissimilar pairs;  cij is continuous similarity, i.e
cij = yi∩yj  yi∪yj if labels yi  and yj of xi and xj are given, cij = N if only sij is given
For each pair, P (sij |hi,hj) is the conditional probability of similarity label sij given a pair of hash codes hi and hj , which can be naturally defined as pairwise logistic function,  P (sij |hi,hj) =  {  σ (〈hi,hj〉) , sij = N  N− σ (〈hi,hj〉) , sij = 0  = σ(〈hi,hj〉) sij (N− σ (〈hi,hj〉))  N−sij  (N)  where σ (x) = N/(N + e−αx) is the adaptive sigmoid func- tion with hyper-parameter α to control its bandwidth
Note that the sigmoid function with larger α will have larger sat- uration zone where its gradient is zero
To perform more effective back-propagation, we usually require α < N, which is more effective than the typical setting of α = N
Similar to logistic regression, we can see in pairwise logistic regression that the smaller the Hamming distance distH (hi,hj) is, the larger the inner product 〈hi,hj〉 as well as the con- ditional probability P (N|hi,hj) will be, implying that pair hi and hj should be classified as similar; otherwise, the  larger the conditional probability P (0|hi,hj) will be, im- plying that pair hi and hj should be classified as dissimilar
 Hence, Equation (N) is a reasonable extension of the logistic  regression classifier to the pairwise classification scenario,  which is optimal for binary similarity labels sij ∈ {0, N}
By taking Equation (N) into WML estimation in Equation (N), we achieve the optimization problem of HashNet,  min Θ  ∑  sij∈S  wij (log (N + exp (α 〈hi,hj〉))− αsij 〈hi,hj〉),  (N)  where Θ denotes the set of all parameters in deep networks
Note that, HashNet directly uses the sign activation function  hi = sgn (zi) which converts the K-dimensional represen- tation to exactly binary hash codes, as shown in Figure N
 By optimizing the WML estimation in Equation (N), we can  NNN0    enable deep learning to hash from imbalanced data under  a statistically optimal framework
It is noteworthy that our  work is the first attempt that extends the WML estimation  from pointwise scenario to pairwise scenario
The HashNet  can jointly preserve similarity information of pairwise images and generate exactly binary hash codes
Different from  HashNet, previous deep-hashing methods need to first learn  continuous embeddings, which are binarized in a separated  step using the sign function
This will result in substantial  quantization errors and significant losses of retrieval quality
 N.N
Learning by Continuation  HashNet learns exactly binary hash codes by converting  the K-dimensional representation z of the hash layer fch, which is continuous in nature, to binary hash code h taking  values of either +N or −N
This binarization process can only be performed by taking the sign function h = sgn (z) as activation function on top of hash layer fch in HashNet,  h = sgn (z) =  {  +N, if z > 0  −N, otherwise (N)  Unfortunately, as the sign function is non-smooth and nonconvex, its gradient is zero for all nonzero inputs, and is illdefined at zero, which makes the standard back-propagation  infeasible for training deep networks
This is known as the  vanishing gradient problem, which has been a key difficulty  in training deep neural networks via back-propagation [NN]
 Many optimization methods have been proposed to circumvent the vanishing gradient problem and enable effective network training with back-propagation, including unsupervised pre-training [NN, N], dropout [NN], batch normalization [NN], and deep residual learning [NN]
In particular,  Rectifier Linear Unit (ReLU) [NN] activation function makes  deep networks much easier to train and enables end-to-end  learning algorithms
However, the sign activation function  is so ill-defined that all the above optimization methods will  fail
A very recent work, BinaryNet [N], focuses on training  deep networks with activations constrained to +N or −N
However, the training algorithm may be hard to converge as  the feed-forward pass uses the sign activation (sgn) but the back-propagation pass uses a hard tanh (Htanh) activation
Optimizing deep networks with sign activation remains an  open problem and a key challenge for deep learning to hash
 Algorithm N: Optimizing HashNet by Continuation  Input: A sequence N = β0 < βN < 


< βm = ∞ for stage t = 0 to m do  Train HashNet (N) with tanh(βtz) as activation Set converged HashNet as next stage initialization  end  Output: HashNet with sgn(z) as activation, βm → ∞  This paper attacks the problem of non-convex optimization of deep networks with non-smooth sign activation by  starting with a smoothed objective function which becomes  more non-smooth as the training proceeds
It is inspired by  recent studies in continuation methods [N], which address  a complex optimization problem by smoothing the original  function, turning it into a different problem that is easier to  optimize
By gradually reducing the amount of smoothing  during the training, it results in a sequence of optimization  problems converging to the original optimization problem
 Motivated by the continuation methods, we notice there exists a key relationship between the sign function and the  scaled tanh function in the concept of limit in mathematics,  limβ→∞ tanh (βz) = sgn (z) , (N)  where β > 0 is a scaling parameter
Increasing β, the scaled tanh function tanh(βz) will become more non-smooth and more saturated so that the deep networks using tanh(βz) as the activation function will be more difficult to optimize, as  in Figure N (right)
But fortunately, as β → ∞, the opti- mization problem will converge to the original deep learning to hash problem in (N) with sgn(z) activation function
 Using the continuation methods, we design an optimization method for HashNet in Algorithm N
As deep network  with tanh(z) as the activation function can be successfully trained, we start training HashNet with tanh(βtz) as the activation function, where β0 = N
For each stage t, after HashNet converges, we increase βt and train (i.e
fine-tune) HashNet by setting the converged network parameters as the  initialization for training the HashNet in the next stage
By  evolving tanh(βtz) with βt → ∞, the network will con- verge to HashNet with sgn(z) as activation function, which can generate exactly binary hash codes as we desire
The  efficacy of continuation in Algorithm N can be understood  as multi-stage pre-training, i.e., pre-training HashNet with  tanh(βtz) activation function is used to initialize HashNet with tanh(βt+Nz) activation function, which enables easier progressive training of HashNet as the network is becoming  non-smooth in later stages by βt → ∞
Using m = N0 we can already achieve fast convergence for training HashNet
 N.N
Convergence Analysis  We analyze that the continuation method in Algorithm N  decreases HashNet loss (N) in each stage and each iteration
 Let Lij = wij (log (N + exp (α 〈hi,hj〉))− αsij 〈hi,hj〉) and L =  ∑  sij∈S Lij , where hi ∈ {−N,+N}  K are binary  hash codes
Note that when optimizing HashNet by continuation in Algorithm N, the network activation in each stage  t is g = tanh(βtz), which is continuous in nature and will only become binary after convergence βt → ∞
Denote by Jij = wij (log (N + exp (α 〈gi, gj〉))− αsij 〈gi, gj〉) and J =  ∑  sij∈S Jij the true loss we optimize in Algorithm N,  NNNN    where gi ∈ R K and hi = sgn(gi)
Our results are two theorems, with proofs provided in the supplemental materials
 Theorem N
The HashNet loss L will not change across stages t and t+N with bandwidths switched from βt to βt+N
 Theorem N
Loss L decreases when optimizing loss J(g) by the stochastic gradient descent (SGD) within each stage
 N
Experiments  We conduct extensive experiments to evaluate HashNet  against several state-of-the-art hashing methods on three  standard benchmarks
Datasets and implementations are  available at http://github.com/thuml/HashNet
 N.N
Setup  The evaluation is conducted on three benchmark image  retrieval datasets: ImageNet, NUS-WIDE and MS COCO
 ImageNet is a benchmark image dataset for Large Scale  Visual Recognition Challenge (ILSVRC N0NN) [NN]
It contains over N.NM images in the training set and N0K images  in the validation set, where each image is single-labeled by  one of the N,000 categories
We randomly select N00 categories, use all the images of these categories in the training  set as the database, and use all the images in the validation  set as the queries; furthermore, we randomly select N00 images per category from the database as the training points
 NUS-WIDEN [N] is a public Web image dataset which  contains NNN,NNN images downloaded from Flickr.com
 Each image is manually annotated by some of the NN ground  truth concepts (categories) for evaluating retrieval models
 We follow similar experimental protocols as DHN [NN] and  randomly sample N,000 images as queries, with the remaining images used as the database; furthermore, we randomly  sample N0,000 images from the database as training points
 MS COCON [NN] is an image recognition, segmentation,  and captioning dataset
The current release contains NN,NNN  training images and N0,N0N validation images, where each  image is labeled by some of the N0 categories
After pruning  images with no category information, we obtain NN,NNNN  images by combining the training and validation images
 We randomly sample N,000 images as queries, with the rest  images used as the database; furthermore, we randomly  sample N0,000 images from the database as training points
 Following standard evaluation protocol as previous work  [N0, N0, NN], the similarity information for hash function  learning and for ground-truth evaluation is constructed from  image labels: if two images i and j share at least one label, they are similar and sij = N; otherwise, they are dissimilar and sij = 0
Note that, although we use the image labels to construct the similarity information, our proposed HashNet  Nhttp://lms.comp.nus.edu.sg/research/NUS-WIDE.htm Nhttp://mscoco.org  can learn hash codes when only the similarity information is  available
By constructing the training data in this way, the  ratio between the number of dissimilar pairs and the number  of similar pairs is roughly N00, N, and N for ImageNet, NUSWIDE, and MS COCO, respectively
These datasets exhibit  the data imbalance phenomenon and can be used to evaluate  different hashing methods under data imbalance scenario
 We compare retrieval performance of HashNet with ten  classical or state-of-the-art hashing methods: unsupervised  methods LSH [N0], SH [NN], ITQ [NN], supervised shallow  methods BRE [NN], KSH [NN], ITQ-CCA [NN], SDH [NN],  and supervised deep methods CNNH [N0], DNNH [N0],  DHN [NN]
We evaluate retrieval quality based on five standard evaluation metrics: Mean Average Precision (MAP),  Precision-Recall curves (PR), Precision curves within Hamming distance N (P@H=N), Precision curves with respect to  different numbers of top returned samples (P@N), and Histogram of learned codes without binarization
For fair comparison, all methods use identical training and test sets
We  adopt MAP@N000 for ImageNet as each category has N,N00  images, and adopt MAP@N000 for the other datasets [NN]
 For shallow hashing methods, we use DeCAFN features  [N] as input
For deep hashing methods, we use raw images  as input
We adopt the AlexNet architecture [NN] for all deep  hashing methods, and implement HashNet based on the  Caffe framework [NN]
We fine-tune convolutional layers  convN–convN and fully-connected layers fcN–fcN copied from the AlexNet model pre-trained on ImageNet N0NN and  train the hash layer fch, all through back-propagation
As the fch layer is trained from scratch, we set its learning rate to be N0 times that of the lower layers
We use mini-batch  stochastic gradient descent (SGD) with 0.N momentum and  the learning rate annealing strategy implemented in Caffe,  and cross-validate the learning rate from N0−N to N0−N with  a multiplicative step-size N0 N  N 
We fix the mini-batch size of  images as NNN and the weight decay parameter as 0.000N
 N.N
Results  The Mean Average Precision (MAP) results are shown in  Table N
HashNet substantially outperforms all comparison  methods
Specifically, compared to the best shallow hashing method using deep features as input, ITQ/ITQ-CCA, we  achieve absolute boosts of NN.N%, NN.N%, and N.N% in aver- age MAP for different bits on ImageNet, NUS-WIDE, and  MS COCO, respectively
Compared to the state-of-the-art  deep hashing method, DHN, we achieve absolute boosts of  NN.N%, N.N%, N.N% in average MAP for different bits on the three datasets, respectively
An interesting phenomenon is  that the performance boost of HashNet over DHN is significantly different across the three datasets
Specifically, the  performance boost on ImageNet is much larger than that on  NUS-WIDE and MS COCO by about N0%, which is very impressive
Recall that the ratio between the number of disNNNN  http://github.com/thuml/HashNet Flickr.com http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm http://mscoco.org   Table N
Mean Average Precision (MAP) of Hamming Ranking for Different Number of Bits on the Three Image Datasets  Method ImageNet NUS-WIDE MS COCO  NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits  HashNet 0.N0NN 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NN0N 0.NNNN  DHN [NN] 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNNN  DNNH [N0] 0.NN0N 0.NN0N 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.N0NN 0.N0NN  CNNH [N0] 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  SDH [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  KSH [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  ITQ-CCA [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N0NN  ITQ [NN] 0.NNNN 0.NNN0 0.NNN0 0.NNN0 0.N0NN 0.NNNN 0.NNN0 0.NNNN 0.NNNN 0.NNNN 0.NNN0 0.NNNN  BRE [NN] 0.0NNN 0.NNNN 0.NN00 0.NNNN 0.N0NN 0.NNN0 0.NNNN 0.NNNN 0.NNN0 0.NNNN 0.NN00 0.NNNN  SH [NN] 0.N0NN 0.NNN0 0.NNNN 0.NNNN 0.N0NN 0.NN0N 0.NNNN 0.NN0N 0.NNNN 0.N0NN 0.N0NN 0.NN0N  LSH [N0] 0.N00N 0.NNN0 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.N00N 0.NNNN 0.NNNN 0.NNN0 0.NNNN  Number of bits  N0 NN N0 NN N0 NN N0 NN N0  P re  c is  io n  0    0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  (a) Precision within Hamming radius N  Recall  0  0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N N    P re  c is  io n  0    0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  (b) Precision-recall curve @ NN bits  Number of top returned images N00 N00 N00 N00 N00 N00 N00 N00 N00 N000  P re  c is  io n  0.N  0.N  0.N  0.N  0.N HashNet DHN DNNH CNNH SDH ITQ-CCA KSH ITQ SH  (c) Precision curve w.r.t
top-N @ NN bits  Figure N
The experimental results of HashNet and comparison methods on the ImageNet dataset under three evaluation metrics
 Number of Bits  N0 NN N0 NN N0 NN N0 NN N0  P re  c is  io n  0    0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  (a) Precision within Hamming radius N  Recall  0  0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N N    P re  c is  io n  0.N  0.N  0.N  0.N  0.N  0.N  0.N  (b) Precision-recall curve @ NN bits  Number of top returned images N00 N00 N00 N00 N00 N00 N00 N00 N00 N000  P re  c is  io n  0.N  0.N  0.N  0.N HashNet DHN DNNH CNNH SDH ITQ-CCA KSH ITQ SH  (c) Precision curve w.r.t
top-N @ NN bits  Figure N
The experimental results of HashNet and comparison methods on the NUS-WIDE dataset under three evaluation metrics
 Number of bits  N0 NN N0 NN N0 NN N0 NN N0  P re  c is  io n  0    0.N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  (a) Precision within Hamming radius N  Recall  0  0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N N    P re  c is  io n  0.N  0.N  0.N  0.N  0.N  (b) Precision-recall curve @ NN bits  Number of top returned images N00 N00 N00 N00 N00 N00 N00 N00 N00 N000  P re  c is  io n  0.N  0.N  0.N  0.N HashNet DHN DNNH CNNH SDH ITQ-CCA KSH ITQ SH  (c) Precision curve w.r.t
top-N @ NN bits  Figure N
The experimental results of HashNet and comparison methods on the MS COCO dataset under three evaluation metrics
 NNNN    HashNet P@N0 N0%  DHN P@N0 N0%  Query Top N0 Retrieved Images  fire  engine  HashNet P@N0 N0%  DHN P@N0 N0%  buildings  HashNet P@N0 N0%  DHN P@N0 N0%  bicycle bed book  sports ball umbrella  ImageNet  NUS-WIDE  MS COCO  Figure N
Examples of top N0 retrieved images and precision@N0
 similar pairs and the number of similar pairs is roughly N00, N, and N for ImageNet, NUS-WIDE and MS COCO, respec- tively
This data imbalance problem substantially deteriorates the performance of hashing methods trained from pairwise data, including all the deep hashing methods
HashNet  enhances deep learning to hash from imbalanced dataset by  Weighted Maximum Likelihood (WML), which is a principled solution to tackling the data imbalance problem
This  lends it the superior performance on imbalanced datasets
 The performance in terms of Precision within Hamming  radius N (P@H=N) is very important for efficient retrieval  with binary hash codes since such Hamming ranking only  requires O(N) time for each query
As shown in Figures N(a), N(a) and N(a), HashNet achieves the highest P@H=N  results on all three datasets
In particular, P@H=N of HashNet with NN bits is better than that of DHN with any bits
 This validates that HashNet can learn more compact binary  codes than DHN
When using longer codes, the Hamming  space will become sparse and few data points fall within the  Hamming ball with radius N [N]
This is why most hashing  methods achieve best accuracy with moderate code lengths
 The retrieval performance on the three datasets in terms  of Precision-Recall curves (PR) and Precision curves with  respect to different numbers of top returned samples (P@N)  are shown in Figures N(b)∼N(b) and Figures N(c)∼N(c), re- spectively
HashNet outperforms comparison methods by  large margins
In particular, HashNet achieves much higher  precision at lower recall levels or when the number of top  results is small
This is desirable for precision-first retrieval,  which is widely implemented in practical systems
As an  intuitive illustration, Figure N shows that HashNet can yield  much more relevant and user-desired retrieval results
 Recent work [NN] studies two evaluation protocols for  supervised hashing: (N) supervised retrieval protocol where  queries and database have identical classes and (N) zero-shot  retrieval protocol where queries and database have different  classes
Some supervised hashing methods perform well in  Table N
MAP on ImageNet with Zero-Shot Retrieval Protocol [NN]  Method NN bits NN bits NN bits NN bits  HashNet 0.NNNN 0.NNNN 0.NNNN 0.NNNN  DHN [NN] 0.NNNN 0.NNNN 0.NNNN 0.NNNN  -N0 -N0 0 N0 N0 -N0  -N0  -N0  -N0  0  N0  N0  N0  N0  (a) HashNet  -N0 -N0 -N0 0 N0 N0 -N0  -N0  -N0  -N0  0  N0  N0  N0  N0  (b) DHN  Figure N
The t-SNE of hash codes learned by HashNet and DHN
 one protocol but poorly in another protocol
Table N shows  the MAP results on ImageNet dataset under the zero-shot  retrieval protocol, where HashNet substantially outperforms  DHN
Thus, HashNet works well under different protocols
 N.N
Empirical Analysis  Visualization of Hash Codes: We visualize the t-SNE  [N] of hash codes generated by HashNet and DHN on ImageNet in Figure N (for ease of visualization, we sample N0  categories)
We observe that the hash codes generated by  HashNet show clear discriminative structures in that different categories are well separated, while the hash codes generated by DHN do not show such discriminative structures
 This suggests that HashNet can learn more discriminative  hash codes than DHN for more effective similarity retrieval
 Ablation Study: We go deeper with the efficacy of the  weighted maximum likelihood and continuation methods
 We investigate three variants of HashNet: (N) HashNet+C,  variant using continuous similarity cij = yi∩yj  yi∪yj when image labels are given; (N) HashNet-W, variant using maximum likelihood instead of weighted maximum likelihood,  i.e
wij = N; (N) HashNet-sgn, variant using tanh() instead of sgn() as activation function to generate continuous codes and requiring a separated binarization step to generate hash  codes
We compare results of these variants in Table N
 By weighted maximum likelihood estimation, HashNet  outperforms HashNet-W by substantially large margins of  NN.N%, N.N% and 0.N% in average MAP for different bits on ImageNet, NUS-WIDE and MS COCO, respectively
The  standard maximum likelihood estimation has been widely  adopted in previous work [N0, NN]
However, this estimation does not account for the data imbalance, and may suffer  from performance drop when training data is highly imbalanced (e.g
ImageNet)
In contrast, the proposed weighted  maximum likelihood estimation (N) is a principled solution  to tackling the data imbalance problem by weighting the  training pairs according to the importance of misclassifying  that pair
Recall that MS COCO is a balanced dataset, hence  HashNet and HashNet-W may yield similar MAP results
 NNNN    Table N
Mean Average Precision (MAP) Results of HashNet and Its Variants, HashNet+C, HashNet-W, and HashNet-sgn on Three Datasets  Method ImageNet NUS-WIDE MS COCO  NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits NN bits  HashNet+C 0.N0NN 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.N0NN 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  HashNet 0.N0NN 0.NN0N 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NN0N 0.NNNN  HashNet-W 0.NNN0 0.NNNN 0.NNNN 0.NNNN 0.NN00 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN 0.NNNN  HashNet-sgn 0.NNNN 0.NNN0 0.NNNN 0.N0NN 0.NN0N 0.NNN0 0.NNNN 0.N0N0 0.NNNN 0.NNNN 0.N0NN 0.NNNN  Number of Iterations 0    N000 N000 N000 N000 N000 N000 N000 N000 N000 N0000  L o  s s  V  a lu  e  0    0.N  N    N.N  N    N.N  N    N.N  N    HashNet-sign HashNet+sign DHN-sign DHN+sign  (a) ImageNet  Number of Iterations 0    N000 N000 N000 N000 N000 N000 N000 N000 N000 N0000  L o  s s  V  a lu  e  0    0.N  N    N.N  N    N.N  N    N.N  HashNet-sign HashNet+sign DHN-sign DHN+sign  (b) NUS-WIDE  Number of Iterations 0    N000 N000 N000 N000 N000 N000 N000 N000 N000 N0000  L o  s s  V  a lu  e  0    0.N  N    N.N  N    N.N  N    N.N  HashNet-sign HashNet+sign DHN-sign DHN+sign  (c) COCO  Figure N
Losses of HashNet and DHN through training process
 By further considering continuous similarity (cij = yi∩yj  yi∪yj ),  HashNet+C achieves even better accuracy than HashNet
 By training HashNet with continuation, HashNet outperforms HashNet-sgn by substantial margins of N.N%, N.N% and N.0% in average MAP on ImageNet, NUS-WIDE, and MS COCO, respectively
Due to the ill-posed gradient problem, existing deep hashing methods cannot learn exactly binary hash codes using sgn() as activation function
Instead, they need to use surrogate functions of sgn(), e.g
tanh(), as the activation function and learn continuous codes, which  require a separated binarization step to generate hash codes
 The proposed continuation method is a principled solution  to deep learning to hash with sgn() as activation function, which learn lossless binary hash codes for accurate retrieval
 Loss Value Through Training Process: We compare  the change of loss values of HashNet and DHN through the  training process on ImageNet, NUS-WIDE and MSCOCO
 We display the loss values before (-sign) and after (+sign)  binarization, i.e
J(g) and L(h)
Figure N reveals three im- portant observations: (a) Both methods converge in terms  of the loss values before and after binarization, which validates the convergence analysis in Section N.N
(b) HashNet  converges with a much smaller training loss than DHN both  before and after binarization, which implies that HashNet  can preserve the similarity relationship in Hamming space  much better than DHN
(c) The two loss curves of HashNet  before and after binarization become close to each other and  overlap completely when convergence
This shows that the  continuation method enables HashNet to approach the true  loss defined on the exactly binary codes without continuous relaxation
But there is a large gap between two loss  curves of DHN, implying that DHN and similar methods  [NN, NN, NN] cannot learn exactly binary codes by minimizing quantization error of codes before and after binarization
 Histogram of Codes Without Binarization: As discussed previously, the proposed HashNet can learn exactly  HashNet 0 0.N N  F re  q u  e n  c y  0  N00  N000  NN00  N000  NN00  N000  DHN 0 0.N N  (a) ImageNet  HashNet 0 0.N N  F re  q u  e n  c y  0  N00  N000  NN00  N000  NN00  N000  NN00  N000  DHN 0 0.N N  (b) NUS-WIDE  HashNet 0 0.N N  F re  q u  e n  c y  0  N00  N000  NN00  N000  NN00  N000  DHN 0 0.N N  (c) COCO  Figure N
Histogram of non-binarized codes of HashNet and DHN
 binary hash codes while previous deep hashing methods can  only learn continuous codes and generate binary hash codes  by post-step sign thresholding
To verify this key property,  we plot the histograms of codes learned by HashNet and  DHN on the three datasets without post-step binarization
 The histograms can be plotted by evenly dividing [0, N] into N00 bins, and calculating the frequency of codes falling into  each bin
To make the histograms more readable, we show  absolute code values (x-axis) and squared root of frequency (y-axis)
Histograms in Figure N show that DHN can only generate continuous codes spanning across the whole range  of [0, N]
This implies that if we quantize these continuous codes into binary hash codes (taking values in {−N, N}) in a post-step, we may suffer from large quantization error especially for the codes near zero
On the contrary, the codes  of HashNet without binarization are already exactly binary
 N
Conclusion  This paper addressed deep learning to hash from imbalanced similarity data by the continuation method
The proposed HashNet can learn exactly binary hash codes by optimizing a novel weighted pairwise cross-entropy loss function in deep convolutional neural networks
HashNet can be  effectively trained by the proposed multi-stage pre-training  algorithm carefully crafted from the continuation method
 Comprehensive empirical evidence shows that HashNet can  generate exactly binary hash codes and yield state-of-the-art  multimedia retrieval performance on standard benchmarks
 N
Acknowledgments  This work was supported by the National Key R&D Program of China (No
N0NNYFBN000N0N), the National Natural Science Foundation of China (No
NNN0NNNN, NNNNN00N,  and NNNN0NNN), the National Sci.&Tech
Supporting Program (N0NNBAFNNB0N), and the Tsinghua TNList Projects
 NNNN    References  [N] E
L
Allgower and K
Georg
Numerical continuation methods: an introduction, volume NN
Springer  Science & Business Media, N0NN
N, N  [N] Y
Bengio, A
Courville, and P
Vincent
Representation learning: A review and new perspectives
IEEE  Transactions on Pattern Analysis and Machine Intelligence (TPAMI), NN(N):NNNN–NNNN, Aug N0NN
N  [N] Y
Bengio, P
Lamblin, D
Popovici, and  H
Larochelle
Greedy layer-wise training of  deep networks
In B
Schölkopf, J
C
Platt, and  T
Hoffman, editors, NIPS, pages NNN–NN0
MIT  Press, N00N
N  [N] T.-S
Chua, J
Tang, R
Hong, H
Li, Z
Luo, and Y.-T
 Zheng
Nus-wide: A real-world web image database  from national university of singapore
In ICMR
ACM,  N00N
N  [N] M
Courbariaux and Y
Bengio
Binarynet: Training deep neural networks with weights and activations  constrained to +N or -N
In NIPS, N0NN
N  [N] J
P
Dmochowski, P
Sajda, and L
C
Parra
Maximum likelihood in cost-sensitive learning: Model  specification, approximations, and upper bounds
 Journal of Machine Learning Research (JMLR),  NN(Dec):NNNN–NNNN, N0N0
N  [N] J
Donahue, Y
Jia, O
Vinyals, J
Hoffman, N
Zhang,  E
Tzeng, and T
Darrell
Decaf: A deep convolutional  activation feature for generic visual recognition
In  ICML, N0NN
N, N  [N] V
Erin Liong, J
Lu, G
Wang, P
Moulin, and J
Zhou
 Deep hashing for compact binary codes learning
In  CVPR, pages NNNN–NNNN
IEEE, N0NN
N  [N] D
J
Fleet, A
Punjani, and M
Norouzi
Fast search in  hamming space with multi-index hashing
In CVPR
 IEEE, N0NN
N, N  [N0] A
Gionis, P
Indyk, R
Motwani, et al
Similarity  search in high dimensions via hashing
In VLDB, volume NN, pages NNN–NNN
ACM, NNNN
N, N, N  [NN] Y
Gong, S
Kumar, H
Rowley, S
Lazebnik, et al
 Learning binary codes for high-dimensional data using bilinear projections
In CVPR, pages NNN–NNN
 IEEE, N0NN
N  [NN] Y
Gong and S
Lazebnik
Iterative quantization: A  procrustean approach to learning binary codes
In  CVPR, pages NNN–NNN, N0NN
N, N, N, N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual  learning for image recognition
CVPR, N0NN
N, N  [NN] G
E
Hinton, S
Osindero, and Y.-W
Teh
A fast learning algorithm for deep belief nets
Neural Computation, NN(N):NNNN–NNNN, N00N
N, N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating deep network training by reducing internal  covariate shift
In ICML, N0NN
N  [NN] H
Jegou, M
Douze, and C
Schmid
Product quantization for nearest neighbor search
IEEE Transactions on Pattern Analysis and Machine Intelligence  (TPAMI), NN(N):NNN–NNN, Jan N0NN
N  [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long,  R
Girshick, S
Guadarrama, and T
Darrell
Caffe:  Convolutional architecture for fast feature embedding
 In ACM Multimedia Conference
ACM, N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet classification with deep convolutional neural  networks
In NIPS, N0NN
N, N, N  [NN] B
Kulis and T
Darrell
Learning to hash with binary  reconstructive embeddings
In NIPS, pages N0NN–  N0N0, N00N
N, N, N, N  [N0] H
Lai, Y
Pan, Y
Liu, and S
Yan
Simultaneous feature learning and hash coding with deep neural networks
In CVPR
IEEE, N0NN
N, N, N, N  [NN] M
S
Lew, N
Sebe, C
Djeraba, and R
Jain
Contentbased multimedia information retrieval: State of the  art and challenges
ACM Transactions on Multimedia Computing, Communications, and Applications  (TOMM), N(N):N–NN, Feb
N00N
N  [NN] W.-J
Li, S
Wang, and W.-C
Kang
Feature learning  based deep supervised hashing with pairwise labels
 In IJCAI, N0NN
N, N, N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona,  D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft  coco: Common objects in context
In ECCV, pages  NN0–NNN
Springer, N0NN
N  [NN] H
Liu, R
Wang, S
Shan, and X
Chen
Deep supervised hashing for fast image retrieval
In CVPR, pages  N0NN–N0NN, N0NN
N, N, N  [NN] W
Liu, J
Wang, R
Ji, Y.-G
Jiang, and S.-F
Chang
 Supervised hashing with kernels
In CVPR
IEEE,  N0NN
N, N, N, N  [NN] W
Liu, J
Wang, S
Kumar, and S.-F
Chang
Hashing  with graphs
In ICML
ACM, N0NN
N  [NN] X
Liu, J
He, B
Lang, and S.-F
Chang
Hash bit  selection: a unified solution for selection problems in  hashing
In CVPR, pages NNN0–NNNN
IEEE, N0NN
N  [NN] C
Ma, I
W
Tsang, F
Peng, and C
Liu
Partial hash  update via hamming subspace learning
IEEE Transactions on Image Processing (TIP), NN(N):NNNN–NNNN,  N0NN
N  [NN] V
Nair and G
E
Hinton
Rectified linear units improve restricted boltzmann machines
In J
Fürnkranz  and T
Joachims, editors, ICML, pages N0N–NNN
Omnipress, N0N0
N  NNNN    [N0] M
Norouzi and D
M
Blei
Minimal loss hashing  for compact binary codes
In ICML, pages NNN–NN0
 ACM, N0NN
N, N  [NN] M
Norouzi, D
M
Blei, and R
R
Salakhutdinov
 Hamming distance metric learning
In NIPS, pages  N0NN–N0NN, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause,  S
Satheesh, S
Ma, Z
Huang, A
Karpathy, A
Khosla,  M
Bernstein, A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual Recognition Challenge
 International Journal of Computer Vision (IJCV),  NNN(N):NNN–NNN, N0NN
N  [NN] R
Salakhutdinov and G
E
Hinton
Learning a nonlinear embedding by preserving class neighbourhood  structure
In AISTATS, pages NNN–NNN, N00N
N  [NN] F
Shen, C
Shen, W
Liu, and H
Tao Shen
Supervised  discrete hashing
In CVPR
IEEE, June N0NN
N, N, N,  N  [NN] A
W
Smeulders, M
Worring, S
Santini, A
Gupta,  and R
Jain
Content-based image retrieval at the end  of the early years
IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI), NN(NN):NNNN–  NNN0, N000
N  [NN] N
Srivastava, G
Hinton, A
Krizhevsky, I
Sutskever,  and R
Salakhutdinov
Dropout: A simple way to prevent neural networks from overfitting
Journal of Machine Learning Research (JMLR), NN(N):NNNN–NNNN,  Jan
N0NN
N  [NN] J
Wang, S
Kumar, and S.-F
Chang
Semi-supervised  hashing for large-scale search
IEEE Transactions on  Pattern Analysis and Machine Intelligence (TPAMI),  NN(NN):NNNN–NN0N, N0NN
N  [NN] J
Wang, H
T
Shen, J
Song, and J
Ji
Hashing for  similarity search: A survey
Arxiv, N0NN
N, N  [NN] Y
Weiss, A
Torralba, and R
Fergus
Spectral hashing
In NIPS, N00N
N, N, N  [N0] R
Xia, Y
Pan, H
Lai, C
Liu, and S
Yan
Supervised  hashing for image retrieval via image representation  learning
In AAAI, pages NNNN–NNNN
AAAI, N0NN
N,  N, N, N, N  [NN] F
X
Yu, S
Kumar, Y
Gong, and S.-F
Chang
Circulant binary embedding
In ICML, pages NNN–NN0
 ACM, N0NN
N  [NN] P
Zhang, W
Zhang, W.-J
Li, and M
Guo
Supervised  hashing with latent factor models
In SIGIR, pages  NNN–NNN
ACM, N0NN
N  [NN] F
Zhao, Y
Huang, L
Wang, and T
Tan
Deep semantic ranking based hashing for multi-label image  retrieval
In CVPR, pages NNNN–NNNN, N0NN
N  [NN] H
Zhu, M
Long, J
Wang, and Y
Cao
Deep hashing network for efficient similarity retrieval
In AAAI
 AAAI, N0NN
N, N, N, N, N  NNNNScene Categorization With Spectral Features   Scene Categorization with Spectral Features  Salman H
KhanN,N, Munawar HayatN and Fatih PorikliN  NDataNN-CSIRO, NUniversity of Canberra, NAustralian National University  {salman.khan,fatih.porikli}@anu.edu.au, munawar.hayat@canberra.edu.au  Abstract  Spectral signatures of natural scenes were earlier found  to be distinctive for different scene types with varying spatial envelope properties such as openness, naturalness,  ruggedness, and symmetry
Recently, such handcrafted features have been outclassed by deep learning based representations
 This paper proposes a novel spectral description of convolution features, implemented efficiently as a unitary transformation within deep network architectures
To the best of  our knowledge, this is the first attempt to use deep learning  based spectral features explicitly for image classification  task
We show that the spectral transformation decorrelates  convolutional activations, which reduces co-adaptation between feature detections, thus acts as an effective regularizer
Our approach achieves significant improvements on  three large-scale scene-centric datasets (MIT-NN, SUN-NNN,  and Places-N0N)
Furthermore, we evaluated the proposed  approach on the attribute detection task where its superior  performance manifests its relevance to semantically meaningful characteristics of natural scenes
 N
Introduction  Scene recognition is a challenging task with a broad  range of applications in content-based image indexing and  retrieval systems
The knowledge about the scene category can also assist in context-aware object detection, action recognition, and scene understanding [NN, NN]
Spectral signature of an image has been shown to be distinctive and semantically meaningful for indoor and outdoor  scenes
Initial work from Oliva et al
[NN] used power spectrum as a global feature descriptor to characterize scenes
 Later, Torralba and Oliva proposed a spatial envelope model  that estimates the shape of a scene (a.k.a
‘the gist’) using  the statistics of both global and localized spectral information [NN, NN]
However, these global features work only for  categorization of scenes into a general set of classes (e.g.,  beach, highway, forest, and mountain) and fail to tackle  fine-grained scene classification, which involves discriminating highly confusing scene categories with subtle differences (e.g., bus station, train station, and airport)
 In this work, we propose to use spectral features obtained from intermediate convolutional layer activations of  -N00  -N00  Figure N: t-SNE visualization of gist and spectral features  for the MIT-NN indoor scene dataset
(Best viewed in color)  deep neural networks for scene classification (Fig
N)
We  demonstrate that these feature representations perform surprisingly well for the scene categorization task and result  in significant performance gains
Further, these spectral  features can be used to automatically tag scenes with semantically meaningful attributes (e.g., man-made, dense,  natural)
These attributes not only pertain to appearance  based characteristics but also relate to functional and material properties, illustrating that the learned spectral features  can capture meaningful information about a scene, closely  linked with the mid-level, human-interpretable attributes
 Such a global scene-centric representation can be computed  efficiently without involving segmentation, detection, and  grouping procedures
Therefore, it could assist in local image analysis or as an attention mechanism to focus on specific details in complex and cluttered scenes
It is noteworthy to point out that such a visual processing approach is  consistent with the remarkable ability of human visual perception which quickly identifies a scene in its first glance  and uses this information to selectively attend to the salient  scene details at a finer scale [N, NN]
 The proposed spectral features are derived from the  learned convolutional activations in a deep neural network  using an orthogonal unitary transformation
Orthogonal  transforms possess decorrelation properties, thus they tend  to concentrate feature energy into only a small number of  coefficients [N]
In terms of decorrelation and energy compaction, Karhunen-Loeve Transform (KLT) provides an optimal solution [N] by identifying the principle directions  (eigenvectors) of the data covariance matrix and projecting the data onto these orthogonal basis to achieve maximal  decorrelation (independence) and energy compaction (concentration)
However, a serious drawback of KLT is its high  NNNNN    computational cost (O(nN) complexity), which prevents its deployment to large-scale scene classification problems
 We show that for a large number of neurons, KLT can be  well approximated by the spectral-domain discrete Fourier  transforms
These approximations can be efficiently computed, thanks to the fast algorithms that utilize precomputed  basis functions
A beneficial consequence of a spectral  transformation is that it tends to regularize the deep network by reducing feature co-adaptations and therefore enhances its generalization ability
Previous literature demonstrates the significance of having uncorrelated and disentangled representations for supervised and unsupervised learning tasks [N, NN, NN]
Another major motivation is that the  human visual sensory mechanism also favors sparse and  non-redundant representations [NN]
 Deep neural networks with fixed parameters, such as the  wavelet scattering networks [N, NN, NN], have been reported  to perform efficiently for specific tasks
However, these  rigid architectures do not generalize and are outperformed  by data-driven features
As an alternative, we propose a  spectral transformation of Convolutional Neural Network  (CNN) activations on fixed basis vectors while learning  the rest of the network parameters from data
We demonstrate that the spectral transformation with fixed parameters  achieves better classification performance than the conventionally learned parameters
The resulting architecture not  only performs superior on task-specific learning problems  but also generalizes to transfer learning scenarios
 We report performance improvements on three largescale scene classification datasets, MIT-NN, SUN-NNN and  Places-N0N
Furthermore, our experiments on two attribute  datasets (SUN Attribute and Outdoor Scene Attribute) show  significant performance gains
In addition to the improved  classification performance, the spectral transformation does  not require any additional supervision or data dependent  statistics to enforce independence between feature detectors
Its integration within any CNN architecture is straightforward, and a unitary transformation could be achieved  with insignificant additional computation load during the  training and testing processes
 We review related approaches in § N
Our proposed fea- ture representation is described in § N and the experiments on scene classification and attribute recognition are summarized in § N and § N, respectively
A comprehensive ablation analysis is provided in § N.N as well
 N
Related Work  Scene Classification: Popular approaches reported in the  literature for scene classification use global descriptors  [NN, NN], mid-level distinctive parts [NN, NN], bag-of-word  style models [NN, N], and deep neural networks [NN, NN]
Recent best performing methods on scene recognition either  employ feature encoding approaches on CNN activations  [NN, NN] or leverage from large-scale scene-centric datasets  [NN] and feature jittering [N0]
In contrast to these works,  we introduce a simple and efficient solution to obtain highperforming spectral features within regular CNNs, which is  computationally inexpensive (in comparison to feature encoding methods), efficiently generalizable, and able to benefit from large-scale scene datasets
 Deep Networks: CNNs have obtained state-of-the-art performance on several key computer vision tasks including  classification [NN, NN], detection [NN, N0], localization [NN],  segmentation [NN], and retrieval [N]
Beginning with the  rudimentary LeNet model, several revamped and extended  architectures (e.g., AlexNet [NN], GoogLeNet [NN], VGGnet  [NN], and ResNet [NN]) have been proposed for image classification
More specialized models such as FCN [NN] and  R-CNN [NN] have been used for segmentation and localization
All of these models have been learned in a data-driven  manner from the raw data
In contrast, our work proposes  a simple spectral transformation layer that provides significant improvements, although its parameters do not require  learning
Previous works that use fixed or random parameter layers had limited generalization properties and suffered  from performance degradation [NN, N0, NN]
 Spectral Representations: According to the convolution  theorem, there exists an equivalence between convolution  operation in the spatial domain and point-wise multiplication in the spectral domain [NN]
Therefore, frequency  domain transforms have traditionally been considered in  deep networks to achieve computational gains [N, NN]
For  small convolution kernels (e.g., N×N in state-of-the-art VG- Gnet model) computational gain was found to be minimal  [NN, NN]
Similarly, spectral transforms along with hashing  techniques have been used to reduce the memory footprint  of the deep networks by eliminating redundancy [N0, NN]
 In this work, we show that spectral features are more powerful for classification than their counterparts in the spatial  domain
In this aspect, our work is relevant to the spectral pooling [NN] approach that improves the down sampling  process by retaining informative frequency coefficients
 Regularization: Feature co-adaptations are avoided in  [NN, NN] by randomly reducing neurons or their connections  to zero during the training process
Batch normalization  is another popular approach that indirectly improves generalization capacity by minimizing the internal covariance  shifts [NN]
There is also recent work on imposing sparsity  in network layers [NN] and using modified losses to avoid  imbalanced set representations [NN]
These methods, however, do not explicitly decorrelate individual feature detectors
Some recent approaches [NN, NN] employ covariance  based loss functions as regularizers to reduce co-adaptations  during the training process
Different from these works, our  approach uses a spectral transformation to decorrelate feature detectors without any explicit training regime
 NNNNN    N
Proposed Approach  KLT is one of the ideal choices in terms of signal decorrelation, data compression, and energy compaction
Given  a symmetric positive semi-definite data covariance matrix,  Cn ∈ R n×n, the KLT basis vectors (Φi) can be obtained by  solving the following eigenvalue problem:  (Cn − λiIn)Φi = 0, i ∈ [N, n] (N)  where, λi are the eigenvalues and I represents identity ma- trix
It is evident from Eq
N that the basis functions for  KLT can not be predetermined due to their dependence on  the data covariance matrix
Therefore, the diagonalization  of covariance matrix to generate KLT basis vectors is a computationally expensive process (especially when n is large)
For high dimensional data, the Discrete Fourier Transform (DFT) of the covariance matrix of a stationary firstorder Markov signal is asymptotically equivalent to its KLT  [NN]
Note that for a first-order Markov process, the data  covariance matrix has a symmetric Toeplitz structure which  is asymptotically equivalent to a circulant matrix for a large  n (it is well known that the eigenvectors of a circulant ma- trix are the basis of DFT)
Here, we first show that spectral  transformation of convolutional activations has a decorrelation effect
Note that the following analysis has particular  relevance to CNN activations which have high dimensionality and somewhat low correlation beforehand
 Suppose that Cn denotes the class of covariance matrices which model the correlation between n feature detectors in a fully-connected layer (ℓ) of the CNN:  Cn = cov(F ) = E[(F − E(F ))(F − E(F )) T ],  where, F ∈ Rn×m is the matrix comprising of n- dimensional feature vectors corresponding to m images
Since the convolutional activations are real valued, Cn rep- resents real symmetric matrices i.e., Cn=C  T n and Im(Cn)  N= 0n×n
The feature detectors generate m responses cor- responding to a given dataset X = {XN, 


, Xm}
Also, consider Tn ∈ R  n×n to be a class of Toeplitz  (constant-diagonal) matrices whose elements are defined  as, T i,jn = τi−j , where τi−j is a constant from the set {τN−n, 


, τn−N}
In the following theorem, we establish equivalence between the two classes of matrices, Cn and Tn, under certain conditions
 Theorem N.N
For a large number of feature detectors n in layer ℓ, the process is asymptotically weakly stationary such that: Cn ∼ Tn, where ∼ denotes asymptotic equivalence
 Proof
Asymptotic equivalence between Cn and Tn can be proved by satisfying the following two properties [NN]:  • The matrix classes Cn and Tn are bounded in terms of both operator and Hilbert-Schmidt norms (lemma N.N)
 NIm(·) denotes the imaginary part
 • The Hilbert-Schmidt norm of the matrix difference (Cn − Tn) vanishes when n is large (lemma N.N)
 We prove these properties below
 Lemma N.N
The matrix classes Cn and Tn are strongly bounded such that:  ‖ Cn ‖, ‖ Tn ‖< z <∞ (N)  Proof
Here, we only consider the matrix class Cn and note that similar arguments can be applied for matrix class Tn
Since, Cn is Hermitian, it’s operator norm is defined as:  ‖ Cn ‖= sup y∈Rn:〈y,y〉=N  yT Cny = max i  |λiC |
 We assume that the individual entries of the covariance matrix Cn are bounded: |C i,j n | ≤ u
Furthermore, the offdiagonal entries are smaller compared to the diagonal: ∣  ∣  ∣  ∣  Ci,jn  Ci,in  ∣  ∣  ∣  ∣  ≤ N : j N= i
 Using the Gershgorin circle theorem, we can see that the  eigenvalues of Cn are bounded by the Gershgorin discs,  s.t., max i  |λiC | < D(C i,i n ,  ∑  j N=i  |Ci,jn |)
 A matrix bounded in the operator norm is also bounded in  the Hilbert-Schmidt norm, since: |Cn| ≤‖ Cn ‖
Hence, Eq
N is the necessary and sufficient condition to satisfy the  first property
 Lemma N.N
The Hilbert-Schmidt norm of the matrix difference between Cn and Tn vanishes as n→ ∞, i.e.,  lim n→∞  |Cn − Tn| = 0
(N)  Proof
Since both Cn and Tn are Hermitian, we can decom- pose them as follows:  Cn = PnΣCP T n , Tn = QnΣTQ  T n 
 Here, Pn, Qn are the unitary transforms which diagonalize the covariance matrices Cn and Tn to:  ΣC = diag(λ i C), ΣT = diag(λ  i T ),  where, λiC and λ i T denote the eigen values, i ∈ [N, n]
Consider the matrix class Cn under the unitary transformation Qn to be:  An = Q T nCnQn = Q  T nPnΣCP  T n Qn
(N)  Approximating An as: Ãn = diag(An), the projection onto Pn can be defined as: ψQ[Cn] = PnÃnP  T n 
The asymptotic  equivalence can then be established considering the HilbertSchmidt norm of the difference between Cn and it’s projecNNNN0    Output Feature MapInput Feature Map Spectral Transformation Layer  ℎ × � × � N × � ℎ × � × � N × �N × � N × � N×N×  �  Figure N: As shown, the spectral transformation is implemented as a convolutional layer in the CNN model
The transformation can be used after a fully-connected layer (right) as a convolutional layer (left) as well
 tion ψQ[Cn] :  |Cn − ψQ[Cn]| =  √  √  √  √  N  n  n ∑  i=N  (  λiC N −  ˜ Ai,in  N )  
(N)  Here, λiC N  and ˜ Ai,in  N  can be represented in terms of matrix  product as follows:  n ∑  i=N  λiC N  = λCλC T ,  n ∑  i=N  ˜ Ai,in  N  = ÃnÃ T n = λCBnB  T n λC  T  where, λC is the vector of all eigenvalues of Cn and Bn = diag(QTnPn) = diag(〈qk,pk〉) s.t
k ∈ [N, n], where qk,pk are the columns of matrices Qn, Pn respectively
Substituting the above expressions in Eq.N:  |Cn − ψQ[Cn]| =  √  N  n  (  λC(In −BnBTn )λC T )  
(N)  Here, the eigenvalues are bounded (as per lemma N.N)
Furthermore, since the matrices Qn, Pn are unitary, the sum ∑n  i=N( ˜ Ai,in )N is also bounded and therefore:  lim n→∞  |Cn − ψQ[Cn]| = 0,  which proves the lemma
 Having the asymptotic equivalence established, we quote  two important results [N, NN]
The corollary N.N.N immediately follows from the lemma N.N and the WielandtHoffman theorem [NN] for Hermitian matrices, while the  corollary N.N.N follows from the lemma N.N [NN]
 Corollary N.N.N
If matrix classes Cn and Tn with eigenval- ues λiC and λ  i T are asymptotically equivalent, we have:  lim n→∞  N  n  n ∑  i=N  (λiC − λ i T ) = 0  Corollary N.N.N
If two matrix classes Cn and Tn with eigen-values λiC and λ  i T are asymptotically equivalent such  that λiC , λ i T have a lower bound z  ′ > 0, then:  lim n→∞  n  √  det Cn − n  √  det Tn = 0  Next, we describe the details of the spectral transformation used in our CNN model
 N.N
Spectral Transform  Having established the asymptotic equivalence between  the KLT and the DFT for a general class of discrete signals,  we investigate efficient ways to implement spectral transformation within a deep neural network
Although, fast algorithms for DFT computation are available (e.g., Fast Fourier  Transform), a complex Fourier transform seems less desirable because phase information is not much useful for classification
Furthermore, our experiments show that a closely  related real-valued DCT transform performs slightly better  in practice (see § N.N for details)
The spectral transform is implemented as a convolution layer which can be placed at  any level in the CNN architecture
Given an input activations tensor Yℓ−N from the ℓ− N layer, we have:  Y sℓ = Y t ℓ−N ∗ k  t,s ℓ , (N)  where ‘∗’ denotes the convolution operation and k denotes N × N dimensional filter which maps the tth feature map from the input tensor to the sth feature map of the output tensor
An illustration of the spectral transformation layer  implementation in a CNN is given in Fig
N
 N.N
Complexity Analysis  Computational complexity of a convolution operation  per kernel is O(nNkN) for an n × n input and a k × k ker- nel (normally n >> k)
Previous works apply Fast Fourier Transform (FFT) for spectral transformation which leads to  efficient computations due to equivalent Hadamard products in the spectral domain [NN, NN]
However, the FFT  computation introduces an additional transformation cost of  O(nN log N nN)
In comparison, our approach only requires  a matrix multiplication in the Fully-Connected (FC) layers  and a N × N convolution operation in the intermediate con- volution layers leading to a cost of O(nN) in both cases
We notice that a direct FFT transform has a lower computational cost of O(n log n), however a standard convo- lutional layer based implementation allows the adaptation  of preceding network layers via error propagation
Such an  implementation is also efficient since it uses fast BLAS rouNNNNN    Figure N: Qualitative results on the Places-N0N dataset shows more informative regions in an image using a heat map
 Approach Accuracy (%)  OOM-semClusters [NN] NN.N CNN-MOP [NN] NN.N  CNNaug-SVM [N0] NN.0 Hybrid-CNN [NN] N0.N SSICA-CNN [N0] NN.N Places-CNDS [N0] NN.N  Deep Filter Banks [NN] NN.0  Baseline CNN (Places-VGGnet [NN]) N0.N Places VGGnet + Spectral Features NN.N  Table N: Average accuracy on the MIT-NN Indoor Scene  dataset
For fairness, we only report comparisons with  methods based on deep networks
 tines for matrix multiplication
Furthermore, since we are  harnessing spectral representations, we do not require the  transformation back to the spectral domain as in [NN]
 N
Scene Classification  N.N
Implementation Details  We used a VGGnet-NN model trained on the Places-N0N  dataset [NN]
The VGGnet has demonstrated excellent performances on the object detection and scene classification  tasks [NN, NN]
The spectral transformation layer is deployed  before the first FC layer, and the network is fine-tuned with  relatively high learning rates in the subsequent FC layers  but very small learning rates in the earlier convolution layers
An equidimensional spectral transformation has been  applied; thus the input to the first FC layer still remains a  N0NN-dimensional feature vector
For training the network,  we augment each image with its flipped, cropped, and rotated versions [N0]
Specifically, from the original image,  we first crop five images (four from the corners and one  from the center)
We then rotate the original image by π N  and −π N  radians
Finally, we horizontally flip all these eight  images (one original, five cropped and two rotated)
The  augmented set of an image, therefore, has NN images
 N.N
Datasets  MIT-NN Dataset [NN] contains a total of NN,NN0 images belonging to NN indoor scene classes
For our experiments, we  follow the standard evaluation protocol, which uses a train  and test split of N0%− N0% for each class
Places-N0N Dataset [NN] is a large-scale scene-centric  Approach Accuracy (%)  Places-AlexNet [NN] N0.0 Places-GoogLeNet [NN] NN.N  Places-CNDS [N0] NN.N Places-VGGnet-NN [NN] NN.0  Baseline CNN (Places-VGGnet [NN]) N0.N Places VGGnet + Spectral Features NN.N  Table N: Average accuracy on the Places-N0N Scene Dataset
 dataset containing nearly N.N million labeled images
Each  scene category contains N,000-NN,000 images for training,  N00 images for validation and N00 images for testing
 SUN-NNN Dataset [NN] consists of N0N,NNN images belonging to NNN categories
Each scene category contains at least  N00 images
The dataset is divided into N0 train/test splits,  each split comprising of N0 train images and N0 test images  per category
 It is important to note that the Places and SUN datasets  use several same scene categories based on the WordNet hierarchy, however both datasets do not contain any overlapping images, and therefore, have complementary strengths
 N.N
Results  Our experimental results on the MIT-NN, Places-N0N and  SUN-NNN datasets are presented in Table N, N and N respectively
As a baseline, we use features extracted from  VGGnet-NN pre-trained on the Places-N0N dataset [NN] and  fine-tuned on the respective dataset
For comparison with  existing methods, we only report performances of methods  that employ learned feature representations from deep neural networks
The experimental results in Tables N, N and N  indicate the effectiveness of the proposed spectral features
 Specifically, we noticed a consistent relative improvement  of N.N%, N.N% and N.0% on the MIT-NN, Places-N0N and SUN-NNN datasets respectively
Class-wise improvements  in classification accuracy for spectral features on MIT-NN  dataset are shown in Fig
N
We also give examples of failure cases in Fig
N to illustrate the highly challenging nature  of the confused classes
It is noteworthy to mention that although we run our experiments with a VGGnet model, our  proposed spectral features can be used in conjunction with  any network configuration
 NNNNN    N0  N0  N0  N0  N0  N0  N00  ai rp  or t i  ns id  e ar  ts tu  di o  au di  to riu  m ba  ke ry ba r  ba th  ro om  be dr  oo m  bo ok  st or  e bo  w lin  g bu  ffe t  ca sin  o ch  ild re  n  ro  om ch  ur ch   in sid  e cl  as sr  oo m  cl oi  st er  cl os  et cl  ot hi  ng st  or e  co m  pu te  rr oo  m co  nc er  t h al  l co  rr id  or de li  de nt  al of  fic e  di ni  ng  ro  om el  ev at  or fa  st fo  od  re  st au  ra nt  flo ris  t ga  m er  oo m  ga ra  ge gr  ee nh  ou se  gr oc  er ys  to re  gy m  ha irs  al on  ho sp  ita lro  om in  sid e   bu s  in sid  e  su  bw ay  je w  el le  ry  sh  op ki  nd er  ga rd  en ki  tc he  n la  bo ra  to ry  w et  la un  dr om  at lib  ra ry  liv in  gr oo  m lo  bb y  lo ck  er  ro  om m al  l m  ee tin  g  ro  om m  ov ie   th ea  te r  m us  eu m  nu rs  er y  of fic  e op  er at  in g   ro om  pa nt  ry po  ol in  sid e  pr iso  nc el  l re  st au  ra nt  re st  au ra  nt  k  itc he  n sh  oe sh  op st  ai rs  ca se  st ud  io m  us ic  su bw  ay to  ys to  re tr  ai ns  ta tio  n tv   st ud  io vi  de os  to re  w ai  tin g   ro om  w ar  eh ou  se w  in ec  el la  r  Cl as  s A cc  ur ac  y  (M  IT -N  N)  B aseline C NN Spectral Features  Figure N: Comparison of class-wise accuracies on the MIT-NN dataset obtained using the baseline and the proposed spectral  features based approach
(Best seen when enlarged)  Approach Accuracy (%)  ImageNet-VGGnet-NN [NN] NN.N Hybrid-CNN [NN] NN.N  DeepNN-DAG CNN [NN] NN.N MetaObject-CNN [NN] NN.N  Places-CNDS [N0] N0.N  Baseline CNN (Places-VGGnet [NN]) NN.N Places-VGGnet + Spectral Features NN.N  Table N: Average accuracy on the SUNNNN Scene Dataset
 N.N
Analysis and Discussion  Dimensionality Analysis: We study the relationship between the number of DCT coefficients and the corresponding performance on the MIT-NN dataset (see Fig
N)
An increase in spectral coefficients generally yields an improvement in the classification performance, however beyond the  N0NN feature dimension, the trend reaches a plateau and no significant improvement is observed
We notice a slight  drop in performance beyond ∼ N0k spectral coefficients
Due to this trend and for the sake of a fair comparison with  baseline and VGGnet based approaches that use a N0NN- dimensional feature dimension, we use an equidimensional  spectral transform in the CNN model
 Fourier Transform and Phase Information: We also test  the closely related DFT features
For classification purposes, only real-valued feature vectors can be used
Therefore, we analyze the performance independently using the  magnitude and phase information as well as the combination of both
For this experiment, we use features before  the first FC layer of the VGGnet (pretrained on the Places  dataset) and a two-layer MLP classifier for classification after spectral transformation, and feature normalization on the  MIT-NN dataset
The results are reported in Table N
We  note that the DFT features perform slightly lower compared  to the DCT features, while the phase information performs  considerably lower than the magnitude features on the scene  classification task
When we concatenate both the normalBedroom Hotel room Beach Sandbar  Chemistry lab Biology lab Dining Room Dinette  Kitchen Kitchenette Bar Pub  Figure N: Pairs from the SUN dataset which were confused  with each other by the classification algorithm
We also  show one example from each class that was mistakenly categorized as the second class
 DCT DFT Mag
DFT Phase DFT Mag
+ Phase  N0.N NN.N NN.N NN.N  Table N: Performance comparison between the magnitude  and phase components of spectral transform
 ized phase and magnitude feature representations, the resulting accuracy is lower than the performance due to only  magnitude features
This indicates that the phase information does not help in scene classification
 Domain Transfer: Here, we evaluate the performance of  spectral features on the domain transfer task
To this end,  we obtained off-the-shelf feature representations before the  first fully connected layer of the VGGnet model which is  pretrained on the ImageNet objects dataset
Given these features, we apply DCT transform to generate spectral features  (N0k dimension), which are then used to test the scene clasNNNNN    N0  NN  NN  NN  NN  N0  NN  NN  NN  C la  ss if  ic a  ti o  n  A  cc u  ra cy   ( %  )  Number of DCT Coefficients  The relationship between Spectral Feature   Dimension and Accuracy   Figure N: The  relationship between the spectral  feature dimension  and classification  accuracy on the  MIT-NN dataset
 sification accuracy on a scene-centric dataset (MIT-NN)
The  spectral features were normalized and a linear SVM classifier was used for classification
We notice a significant boost  in classification performance compared to normal VGGnet  features both with and without data augmentation which depicts the superior discriminative ability of spectral features  (see Table N)
 Accuracy (%) with aug
w/o aug
 Off-the-shelf feat
+ SVM NN.N NN.N Spectral feat
+ SVM NN.N N0.N  Table N: Improvement due to spectral features when the  deep network is trained and tested on different tasks
 Kernel Transform: Fourier basis have been used in the  previous works to approximate popular kernel transforms  [NN, NN]
Therefore, one interesting aspect is to compare  the classification performance with a Radial Basis Function  (RBF) kernel and a DCT transformation
Using features  from a VGGnet trained on the Places-N0N dataset, we get  NN.N% accuracy with a linear SVM and N0.N% with a non- linear SVM using RBF kernel
We noted a similar trend  with the features extracted using a VGGnet pretrained on  the ImageNet dataset
On the MIT-NN dataset, NN.N% and NN.N% accuracy was achieved using a linear SVM and a nonlinear SVM with RBF kernel, respectively
 Data Augmentation: The results reported in § N.N use data augmentation for parameter learning and inference
We perform an ablation study to investigate the performance gain  with spectral features without any data augmentation on the  MIT-NN indoor scene dataset
The results are summarized in  Table N
For the domain adaptation task, where the features  from a VGGnet trained on another task are used to classify scenes, we report comparisons with and without data  augmentation (see Table N)
Although data augmentation  helps in achieving considerably higher performance levels,  the additional performance boost due to spectral features is  persistent and remarkable even without data augmentation
 Is Performance Gain due to Increased Depth? The spectral transformation is implemented as a FC layer in the deep  network
This results in an increase in overall depth of the  network
One may wonder whether the performance gain  is due to the additional depth or the spectral features? To  Accuracy (%) with aug
w/o aug
 Places-VGGnet N0.N NN.N Places-VGGnet (spectral feat) NN.N NN.N  Table N: The effect of data augmentation on the performance due to spectral features (MIT-NN dataset)
 investigate this question, we trained the same network as  used for spectral features based classification on the MITNN dataset, except that the spectral transform layer is replaced by a normal FC layer with the number of neurons  equal to the number of spectral coefficients
This model is  considered as the baseline model
Moreover, we also experimented by replacing the DCT transform matrix with a  random projection matrix whose columns are mutually independent
The results are summarized in Table N
We  note that an additional FC layer provides an improvement  on the MIT-NN dataset, however this improvement is less  pronounced compared to the spectral features
 Method Baseline Spectral Feat
Random Proj
 Acc.(%) NN.N NN.N NN.N  Table N: The comparison between the performance gain due  to spectral transformation, learned FC layer and the random  projection layer in a deep CNN
 Feature Decorrelation: We notice a decrease in feature coadaptation with the use of spectral transformation
Figure N  illustrates portions from the data covariance matrix corresponding to the baseline CNN features and the spectral features
The covariance matrix derived from spectral features  has a stronger diagonal with much weaker off-diagonal entries compared to the covariance matrix of baseline CNN  activations
To quantitatively verify this behavior, we define a diagonalization index (η) for a matrix X ∈ Rn×n as the ratio : η =  ∑n  i=NXi,i/ ∑n  i=N  ∑n  j=NXi,j 
In Table N, we notice a significant increase in η both for the baseline and spectral features covariance matrices projected onto the  DCT basis function (defined in Eq
N)
The η index of N is the ideal case, where the unitary transformation Qn is com- posed of eigenvectors of the data covariance matrix
 Matrix Cb−cnn Ab−cnn Cspec−cnn Aopt  η index 0.00N 0.NNN 0.NN0 N.0  Table N: The diagonalization measure for different covariance and projected matrices
 Generalization: To study the generalization of spectral transformation to other architectures, we tested with  GoogleNet and ResNet models pre-trained on the PlacesN0N dataset
While evaluated on MITNN dataset, the proposed method achieves a classification accuracy of NN.N%  NNNNN    Tree  Black Bison  Green Field  Overcast   Sky  Dense City  Dust-haze Sky  Tree  Pink Flower  Blue Sky  Green Field  Snowy Mountain  Swimming  Clouds  Trees  Biking  Vegetation  Dry, Sand  Concrete  Climbing  Vegetation  Glowers  Brick  Shrubbery  Outdoor Scene Attribute Dataset SUN Attribute Dataset  Figure N: Left box shows sample attribute predictions for the Outdoor Scene Attribute dataset
Right box shows sample  results on the SUN Attribute dataset, which contains more fine-grained and localized attributes
 N0 N00 NN0 N00 NN0 N00 NN0 N00 NN0  N0  N00  NN0  N00  NN0  N00  NN0  N00  NN0  N0 N00 NN0 N00 NN0 N00 NN0 N00 NN0  N0  N00  NN0  N00  NN0  N00  NN0  N00  NN0  N000 N0N0 NN00 NNN0 NN00 NNN0 NN00 NNN0 NN00 NNN0  N000  N0N0  NN00  NNN0  NN00  NNN0  NN00  NNN0  NN00  NNN0  (a) Baseline CNN Features  N000 N0N0 NN00 NNN0 NN00 NNN0 NN00 NNN0 NN00 NNN0  N000  N0N0  NN00  NNN0  NN00  NNN0  NN00  NNN0  NN00  NNN0  (b) Spectral Features  Figure N: Portions of covariance matrices corresponding to  baseline CNN features and spectral features
Feature detectors after spectral transformation are significantly decorrelated
See Table N for the diagonalization index (η) values
 compared with NN.N% using GoogleNet and NN.N% com- pared with NN.N% using ResNet model
 We also tested on the related task of object classification using Caltech-N0N dataset
Using the baseline CNN  with initial layers pre-trained on the ImageNet dataset and  final layers on the object dataset, we obtained an accuracy  of N0.N%
With the proposed spectral transformation layer and keeping other hyper-parameters same, the accuracy increased to NN.N%
 N
Scene Attribute Detection  In this section, we are interested in investigating the relationship between spectral features and visual attributes
The  scene attributes are semantically meaningful mid-level representations which are not only used by humans for scene  description, but have also been found very useful for tasks  such as caption generation, image retrieval and zero-shot  learning [NN]
Specifically, we experiment on two publicly  available scene attribute datasets:  SUN Attribute Dataset [NN] consists of NN,000 scene imMethod SUN-Att SceneAtt  cKernel+SVM [NN] NN.N NN.N HST-att [NN] - NN.N  Spec Feat + SVM N0.N NN.N  Table N: Classification performance in terms of mean Average Precision (mAP)
 ages labeled with N0N distinct attributes
The attributes relate to functional characteristics, materials, surface properties and the global spatial envelope
 Outdoor Scene Attribute Dataset [NN] contains NNNN  scene images with NN attributes as noun-adjective pairs
 Results: For both the datasets, we follow standard protocols as described in [NN, NN]
The proposed spectral features are obtained from the pretrained Paces-VGG network  and binary SVM classifiers are trained for each attribute
 Qualitative attribute predictions are shown in Fig
N
Note  that the SUN attribute dataset contain attribute labeling for  actions and scenarios which may not be present in the scene  but may occur depending on the scene type
Quantitative  results are summarized in Table N
There is a relatively less  pronounced increase in attribute recognition performance  on the SUN Attribute dataset
One possible reason can be  that most of the attributes relate to more fine-grained and  region specific local information, while the proposed descriptor works on global level in our experiments
 N
Conclusion  This paper presents a spectral domain feature representation on top of the convolution activations from a deep network
The spectral transformation enhances the discriminative ability of deep network by decorrelating the individual  feature detectors, thus introducing a regularization effect
 Our implementation does not introduce any significant computational cost
We tested our approach on three large-scale  scene-centric datasets and reported encouraging improvements on the baseline CNN features
We also performed  a detailed ablative analysis to validate the performance improvement
Finally, our experiments on attribute detection  using spectral features demonstrated their superior ability to  encode semantic cues relating to indoor and outdoor scenes
 NNNNN    References  [N] N
Ahmed and K
R
Rao
Orthogonal transforms for digital signal processing
Springer Science & Business Media,  N0NN
 [N] S
An, M
Hayat, S
H
Khan, M
Bennamoun, F
Boussaid,  and F
Sohel
Contractive rectifier networks for nonlinear  maximum margin classification
In Proceedings of the IEEE  international conference on computer vision, pages NNNN–  NNNN, N0NN
 [N] A
Babenko, A
Slesarev, A
Chigorin, and V
Lempitsky
 Neural codes for image retrieval
In European Conference  on Computer Vision, pages NNN–NNN
Springer, N0NN
 [N] S
Ben-Yacoub, B
Fasel, and J
Luettin
Fast face detection  using mlp and fft
In Proc
Second International Conference on Audio and Video-based Biometric Person Authentication (AVBPA” NN), number EPFL-CONF-NNNNN, pages  NN–NN, NNNN
 [N] I
Biederman
Recognition-by-components: a theory of human image understanding
Psychological review, NN(N):NNN,  NNNN
 [N] A
Bosch, A
Zisserman, and X
Muoz
Scene classification using a hybrid generative/discriminative approach
 Transactions on Pattern Analysis and Machine Intelligence,  N0(N):NNN–NNN, N00N
 [N] A
Böttcher and S
M
Grudsky
Toeplitz matrices, asymptotic linear algebra, and functional analysis
Birkhäuser,  N0NN
 [N] L
Breiman
Bagging predictors
Machine learning,  NN(N):NNN–NN0, NNNN
 [N] J
Bruna and S
Mallat
Invariant scattering convolution networks
IEEE transactions on pattern analysis and machine  intelligence, NN(N):NNNN–NNNN, N0NN
 [N0] W
Chen, J
T
Wilson, S
Tyree, K
Q
Weinberger, and  Y
Chen
Compressing convolutional neural networks
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
 [NN] Y
Cheng, F
X
Yu, R
S
Feris, S
Kumar, A
Choudhary,  and S.-F
Chang
An exploration of parameter redundancy  in deep networks with circulant projections
In Proceedings  of the IEEE International Conference on Computer Vision,  pages NNNN–NNNN, N0NN
 [NN] B
Cheung, J
A
Livezey, A
K
Bansal, and B
A
Olshausen
 Discovering hidden factors of variation in deep networks
 arXiv preprint arXiv:NNNN.NNNN, N0NN
 [NN] M
Cimpoi, S
Maji, I
Kokkinos, and A
Vedaldi
Deep filter banks for texture recognition, description, and segmentation
International Journal of Computer Vision, NNN(N):NN–  NN, N0NN
 [NN] M
Cogswell, F
Ahmed, R
Girshick, L
Zitnick, and D
Batra
Reducing overfitting in deep networks by decorrelating  representations
arXiv preprint arXiv:NNNN.0N0NN, N0NN
 [NN] C
Doersch, A
Gupta, and A
A
Efros
Mid-level visual element discovery as discriminative mode seeking
In Advances  in Neural Information Processing Systems, pages NNN–N0N,  N0NN
 [NN] M
George, M
Dixit, G
Zogg, and N
Vasconcelos
Semantic clustering for robust fine-grained scene recognition
In  European Conference on Computer Vision, pages NNN–NNN
 Springer, N0NN
 [NN] Y
Gong, L
Wang, R
Guo, and S
Lazebnik
Multi-scale  orderless pooling of deep convolutional activation features
 In European Conference on Computer Vision, pages NNN–  N0N
Springer, N0NN
 [NN] R
M
Gray et al
Toeplitz and circulant matrices: A review
 Foundations and Trends R© in Communications and Informa- tion Theory, N(N):NNN–NNN, N00N
 [NN] U
Grenander and G
Szegö
Toeplitz forms and their applications, volume NNN
Univ of California Press, N00N
 [N0] M
Hayat, S
H
Khan, M
Bennamoun, and S
An
A spatial  layout and scale invariant feature representation for indoor  scene classification
IEEE Transactions on Image Processing, NN(N0):NNNN–NNNN, Oct N0NN
 [NN] M
Hayat, S
H
Khan, N
Werghi, and R
Goecke
Joint registration and representation learning for unconstrained face  identification
In Proceedings of the IEEE Conference on  Computer Vision and Pattern Recognition, pages –, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Spatial pyramid pooling  in deep convolutional networks for visual recognition
In  European Conference on Computer Vision, pages NNN–NNN
 Springer, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
arXiv preprint arXiv:NNNN.0NNNN,  N0NN
 [NN] A
J
Hoffman, H
W
Wielandt, et al
The variation of the  spectrum of a normal matrix
 [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 In Proceedings of the NNnd International Conference on Machine Learning (ICML-NN), pages NNN–NNN, N0NN
 [NN] K
Jarrett, K
Kavukcuoglu, Y
Lecun, et al
What is the  best multi-stage architecture for object recognition? In N00N  IEEE NNth International Conference on Computer Vision,  pages NNNN–NNNN
IEEE, N00N
 [NN] M
Juneja, A
Vedaldi, C
Jawahar, and A
Zisserman
Blocks  that shout: Distinctive parts for scene classification
In International Conference on Computer Vision and Pattern Recognition, pages NNN–NN0
IEEE, N0NN
 [NN] S
H
Khan, M
Bennamoun, F
Sohel, and R
Togneri
Cost  sensitive learning of deep feature representations from imbalanced data
IEEE transactions on neural networks and  learning systems, N0NN
 [NN] S
H
Khan, M
Hayat, M
Bennamoun, R
Togneri, and F
A
 Sohel
A discriminative representation of convolutional features for indoor scene recognition
IEEE Transactions on  Image Processing, NN(N):NNNN–NNNN, N0NN
 [N0] S
H
Khan, X
He, F
Porikli, M
Bennamoun, F
Sohel, and  R
Togneri
Learning deep structured network for weakly supervised change detection
Proceedings of the International  Joint Conference on Artificial Intelligence, N0NN
 [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in Neural Information Processing Systems, pages  N0NN–NN0N, N0NN
 [NN] A
Lavin
Fast algorithms for convolutional neural networks
 arXiv preprint arXiv:NN0N.0NN0N, N0NN
 NNNNN    [NN] S
Lazebnik, C
Schmid, and J
Ponce
Beyond bags of  features: Spatial pyramid matching for recognizing natural  scene categories
In International Conference on Computer  Vision and Pattern Recognition, volume N, pages NNNN–NNNN
 IEEE, N00N
 [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNN0, N0NN
 [NN] M
Mathieu, M
Henaff, and Y
LeCun
Fast training  of convolutional networks through ffts
arXiv preprint  arXiv:NNNN.NNNN, N0NN
 [NN] A
Oliva and A
Torralba
Modeling the shape of the scene: A  holistic representation of the spatial envelope
International  Journal of Computer Vision, NN(N):NNN–NNN, N00N
 [NN] A
Oliva, A
Torralba, A
Guérin-Dugué, and J
Hérault
 Global semantic classification of scenes using power spectrum templates
In Proceedings of the NNNN international  conference on Challenge of Image Retrieval, pages N–N
 British Computer Society, NNNN
 [NN] A
V
Oppenheim and R
W
Schafer
Discrete-time signal  processing
Pearson Higher Education, N0N0
 [NN] G
Patterson, C
Xu, H
Su, and J
Hays
The sun attribute  database: Beyond categories for deeper scene understanding
 International Journal of Computer Vision, N0N(N-N):NN–NN,  N0NN
 [N0] N
Pinto, D
Doukhan, J
J
DiCarlo, and D
D
Cox
A highthroughput screening approach to discovering good forms of  biologically inspired visual representation
PLoS Comput  Biol, N(NN):eN000NNN, N00N
 [NN] F
Porikli and H
Ozkan
Data driven frequency mapping  for computationally scalable object detection
In IEEE Advanced Video and Signal based Surveillance (AVSS), N0NN
 [NN] A
Quattoni and A
Torralba
Recognizing indoor scenes
 In International Conference on Computer Vision and Pattern  Recognition
IEEE, N00N
 [NN] A
Rahimi and B
Recht
Random features for large-scale  kernel machines
In Advances in neural information processing systems, pages NNNN–NNNN, N00N
 [NN] S
Rahman, S
H
Khan, and F
Porikli
A unified approach  for conventional zero-shot, generalized zero-shot and fewshot learning
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] K
R
Rao and P
Yip
Discrete cosine transform: algorithms,  advantages, applications
Academic press, N0NN
 [NN] A
S
Razavian, H
Azizpour, J
Sullivan, and S
Carlsson
 Cnn features off-the-shelf: an astounding baseline for recognition
arXiv preprint arXiv:NN0N.NNNN, N0NN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  Advances in neural information processing systems, pages  NN–NN, N0NN
 [NN] O
Rippel, J
Snoek, and R
P
Adams
Spectral representations for convolutional neural networks
In C
Cortes, N
D
 Lawrence, D
D
Lee, M
Sugiyama, and R
Garnett, editors, Advances in Neural Information Processing Systems NN,  pages NNN0–NNNN
Curran Associates, Inc., N0NN
 [NN] A
Saxe, P
W
Koh, Z
Chen, M
Bhand, B
Suresh, and A
Y
 Ng
On random weights and unsupervised feature learning
 In Proceedings of the NNth international conference on machine learning (ICML-NN), pages N0NN–N0NN, N0NN
 [N0] A
Sharif Razavian, H
Azizpour, J
Sullivan, and S
Carlsson
Cnn features off-the-shelf: an astounding baseline for  recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages N0N–  NNN, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
CoRR,  abs/NN0N.NNNN, N0NN
 [NN] N
Srivastava, G
E
Hinton, A
Krizhevsky, I
Sutskever, and  R
Salakhutdinov
Dropout: a simple way to prevent neural networks from overfitting
Journal of Machine Learning  Research, NN(N):NNNN–NNNN, N0NN
 [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In Proceedings of the IEEE  Conference on Computer Vision and Pattern Recognition,  pages N–N, N0NN
 [NN] A
Torralba and A
Oliva
Statistics of natural image categories
Network: computation in neural systems, NN(N):NNN–  NNN, N00N
 [NN] S
K
Ungerleider and L
G
Mechanisms of visual attention in the human cortex
Annual review of neuroscience,  NN(N):NNN–NNN, N000
 [NN] N
Vasilache, J
Johnson, M
Mathieu, S
Chintala, S
Piantino, and Y
LeCun
Fast convolutional nets with  fbfft: A gpu performance evaluation
arXiv preprint  arXiv:NNNN.NNN0, N0NN
 [NN] W
E
Vinje and J
L
Gallant
Sparse coding and decorrelation in primary visual cortex during natural vision
Science,  NNN(NNNN):NNNN–NNNN, N000
 [NN] L
Wan, M
Zeiler, S
Zhang, Y
L
Cun, and R
Fergus
Regularization of neural networks using dropconnect
In Proceedings of the N0th International Conference on Machine  Learning (ICML-NN), pages N0NN–N0NN, N0NN
 [NN] L
Wang, S
Guo, W
Huang, and Y
Qiao
PlacesN0Nvggnet models for scene recognition
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [N0] L
Wang, C.-Y
Lee, Z
Tu, and S
Lazebnik
Training  deeper convolutional networks with deep supervision
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
 [NN] S
Wang, J
Joo, Y
Wang, and S.-C
Zhu
Weakly supervised  learning for attribute localization in outdoor scenes
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNNN–NNNN, N0NN
 [NN] J
Wu and J
M
Rehg
Centrist: A visual descriptor for scene  categorization
Transactions on Pattern Analysis and Machine Intelligence, NN(N):NNNN–NN0N, N0NN
 [NN] R
Wu, B
Wang, W
Wang, and Y
Yu
Harvesting discriminative meta objects with deep cnn features for scene classification
In Proceedings of the IEEE International Conference  on Computer Vision, pages NNNN–NNNN, N0NN
 [NN] J
Xiao, J
Hays, K
A
Ehinger, A
Oliva, and A
Torralba
 Sun database: Large-scale scene recognition from abbey to  N0NNNN    zoo
In International Conference on Computer Vision and  Pattern Recognition, pages NNNN–NNNN
IEEE, N0N0
 [NN] S
Yang and D
Ramanan
Multi-scale recognition with dagcnns
In Proceedings of the IEEE International Conference  on Computer Vision, pages NNNN–NNNN, N0NN
 [NN] B
Zhou, A
Khosla, A
Lapedriza, A
Oliva, and A
Torralba
 Object detectors emerge in deep scene cnns
arXiv preprint  arXiv:NNNN.NNNN, N0NN
 [NN] B
Zhou, A
Lapedriza, J
Xiao, A
Torralba, and A
Oliva
 Learning deep features for scene recognition using places  database
In Advances in neural information processing systems, pages NNN–NNN, N0NN
 [NN] H
Zhou, J
Alverez, and F
Porikli
Less is more: Towards  compact cnns,
In Proceedings of the European Conference  on Computer Vision, N0NN
 NNNNNN