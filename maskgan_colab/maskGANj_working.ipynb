{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "maskGANj_working.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python2",
      "display_name": "Python 2"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZyejo0JiKGZ",
        "colab_type": "code",
        "outputId": "c6ff03cb-c9c0-45d2-ab8b-0cb97e57c76d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'maskgan'...\n",
            "remote: Enumerating objects: 73, done.\u001b[K\n",
            "remote: Total 73 (delta 0), reused 0 (delta 0), pack-reused 73\u001b[K\n",
            "Unpacking objects: 100% (73/73), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qOguCX-SiNFt",
        "colab_type": "code",
        "outputId": "88b37d89-12bb-427a-98e3-149b22f1d8e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 910
        }
      },
      "source": [
        "!pip install tensorflow-gpu==1.6\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0f/a2/38929ec9677cb0009837b77674388ab4a35ad81573f3289b21963eda0f9a/tensorflow_gpu-1.6.0-cp27-cp27mu-manylinux1_x86_64.whl (209.2MB)\n",
            "\u001b[K     |████████████████████████████████| 209.2MB 69kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (1.15.0)\n",
            "Requirement already satisfied: mock>=2.0.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (2.0.0)\n",
            "Collecting tensorboard<1.7.0,>=1.6.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/b8/7f64efd6aea9e21b836dc9acac60634ce9c41fe153ffd4df2acedc9a86e6/tensorboard-1.6.0-py2-none-any.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 47.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.4.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (3.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (0.2.2)\n",
            "Requirement already satisfied: wheel in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (0.34.2)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (0.7.1)\n",
            "Requirement already satisfied: backports.weakref>=1.0rc1 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (1.0.post1)\n",
            "Requirement already satisfied: enum34>=1.1.6 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (1.1.6)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (1.16.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python2.7/dist-packages (from tensorflow-gpu==1.6) (0.8.0)\n",
            "Requirement already satisfied: futures>=2.2.0 in /usr/local/lib/python2.7/dist-packages (from grpcio>=1.8.6->tensorflow-gpu==1.6) (3.2.0)\n",
            "Requirement already satisfied: funcsigs>=1; python_version < \"3.3\" in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.6) (1.0.2)\n",
            "Requirement already satisfied: pbr>=0.11 in /usr/local/lib/python2.7/dist-packages (from mock>=2.0.0->tensorflow-gpu==1.6) (5.4.0)\n",
            "Collecting bleach==1.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/33/70/86c5fec937ea4964184d4d6c4f0b9551564f821e1c3575907639036d9b90/bleach-1.5.0-py2.py3-none-any.whl\n",
            "Collecting html5lib==0.9999999\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/ae/bcb60402c60932b32dfaf19bb53870b29eda2cd17551ba5639219fb5ebf9/html5lib-0.9999999.tar.gz (889kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 29.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python2.7/dist-packages (from tensorboard<1.7.0,>=1.6.0->tensorflow-gpu==1.6) (3.1.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python2.7/dist-packages (from protobuf>=3.4.0->tensorflow-gpu==1.6) (44.0.0)\n",
            "Building wheels for collected packages: html5lib\n",
            "  Building wheel for html5lib (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html5lib: filename=html5lib-0.9999999-cp27-none-any.whl size=107221 sha256=937c6f957ae070cf011de32574220c6db98c19ed6e0145d1706c76dbe33a00e6\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ae/f9/d2b189788efcf61d1ee0e36045476735c838898eef1cad6e29\n",
            "Successfully built html5lib\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorboard<1.16.0,>=1.15.0, but you'll have tensorboard 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 1.15.0 has requirement tensorflow-estimator==1.15.1, but you'll have tensorflow-estimator 1.15.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: fastai 0.7.0 has requirement torch<0.4, but you'll have torch 1.4.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: html5lib, bleach, tensorboard, tensorflow-gpu\n",
            "  Found existing installation: html5lib 1.0.1\n",
            "    Uninstalling html5lib-1.0.1:\n",
            "      Successfully uninstalled html5lib-1.0.1\n",
            "  Found existing installation: bleach 3.1.0\n",
            "    Uninstalling bleach-3.1.0:\n",
            "      Successfully uninstalled bleach-3.1.0\n",
            "  Found existing installation: tensorboard 1.15.0\n",
            "    Uninstalling tensorboard-1.15.0:\n",
            "      Successfully uninstalled tensorboard-1.15.0\n",
            "Successfully installed bleach-1.5.0 html5lib-0.9999999 tensorboard-1.6.0 tensorflow-gpu-1.6.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iXpGW05AVRr4",
        "colab_type": "code",
        "outputId": "9f950bd8-46aa-4ed9-c5aa-6c570cd19e2f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!wget https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!dpkg -i cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
        "!apt-key add /var/cuda-repo-9-0-local/7fa2af80.pub\n",
        "!apt-get update\n",
        "!apt-get install cuda=9.0.176-1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-02-12 15:14:25--  https://developer.nvidia.com/compute/cuda/9.0/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb\n",
            "Resolving developer.nvidia.com (developer.nvidia.com)... 192.229.189.146\n",
            "Connecting to developer.nvidia.com (developer.nvidia.com)|192.229.189.146|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?ZKxSdpE6LRSmKcO3BP5tf0IBUgwUSvZLeWwyqMAyBZanehmiWM4n1NC-LksPCpy13fcsQN_OiBiH8CYld8ULeg1pb1fF4EZDPba1WOYs0x0t_TREezt5ih3y3yyQlt_6ErwUlC3JiNjJhDT1YSymHqGKjd6EL68EB9RPJ4hQgLI2DBKzUHn6pjnKoXDfF9Wy0Lzo7lI2dndz0e4Gys5U [following]\n",
            "--2020-02-12 15:14:26--  https://developer.download.nvidia.com/compute/cuda/9.0/secure/Prod/local_installers/cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64.deb?ZKxSdpE6LRSmKcO3BP5tf0IBUgwUSvZLeWwyqMAyBZanehmiWM4n1NC-LksPCpy13fcsQN_OiBiH8CYld8ULeg1pb1fF4EZDPba1WOYs0x0t_TREezt5ih3y3yyQlt_6ErwUlC3JiNjJhDT1YSymHqGKjd6EL68EB9RPJ4hQgLI2DBKzUHn6pjnKoXDfF9Wy0Lzo7lI2dndz0e4Gys5U\n",
            "Resolving developer.download.nvidia.com (developer.download.nvidia.com)... 192.229.232.112, 2606:2800:21f:3aa:dcf:37b:1ed6:1fb\n",
            "Connecting to developer.download.nvidia.com (developer.download.nvidia.com)|192.229.232.112|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1212738714 (1.1G) [application/x-deb]\n",
            "Saving to: ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’\n",
            "\n",
            "cuda-repo-ubuntu160 100%[===================>]   1.13G  14.9MB/s    in 24s     \n",
            "\n",
            "2020-02-12 15:14:50 (48.5 MB/s) - ‘cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb’ saved [1212738714/1212738714]\n",
            "\n",
            "Selecting previously unselected package cuda-repo-ubuntu1604-9-0-local.\n",
            "(Reading database ... 145113 files and directories currently installed.)\n",
            "Preparing to unpack cuda-repo-ubuntu1604-9-0-local_9.0.176-1_amd64-deb ...\n",
            "Unpacking cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "Setting up cuda-repo-ubuntu1604-9-0-local (9.0.176-1) ...\n",
            "OK\n",
            "Get:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Ign:1 file:/var/cuda-repo-9-0-local  InRelease\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  Release [574 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  Release.gpg [819 B]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ InRelease [3,626 B]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  Packages [15.4 kB]\n",
            "Ign:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran35/ Packages [83.1 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Hit:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "Hit:10 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:14 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Hit:15 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [817 kB]\n",
            "Get:18 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic InRelease [15.4 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:20 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [36.9 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [817 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [1,104 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [7,064 B]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [27.5 kB]\n",
            "Get:25 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main Sources [1,765 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [1,344 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [41.2 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [11.1 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu bionic-backports/universe amd64 Packages [4,252 B]\n",
            "Get:30 http://ppa.launchpad.net/marutter/c2d4u3.5/ubuntu bionic/main amd64 Packages [852 kB]\n",
            "Fetched 7,202 kB in 8s (900 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-430\n",
            "Use 'apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "The following NEW packages will be installed:\n",
            "  cuda cuda-9-0 cuda-command-line-tools-9-0 cuda-core-9-0 cuda-cublas-9-0\n",
            "  cuda-cublas-dev-9-0 cuda-cudart-9-0 cuda-cudart-dev-9-0 cuda-cufft-9-0\n",
            "  cuda-cufft-dev-9-0 cuda-curand-9-0 cuda-curand-dev-9-0 cuda-cusolver-9-0\n",
            "  cuda-cusolver-dev-9-0 cuda-cusparse-9-0 cuda-cusparse-dev-9-0\n",
            "  cuda-demo-suite-9-0 cuda-documentation-9-0 cuda-driver-dev-9-0\n",
            "  cuda-libraries-9-0 cuda-libraries-dev-9-0 cuda-license-9-0\n",
            "  cuda-misc-headers-9-0 cuda-npp-9-0 cuda-npp-dev-9-0 cuda-nvgraph-9-0\n",
            "  cuda-nvgraph-dev-9-0 cuda-nvml-dev-9-0 cuda-nvrtc-9-0 cuda-nvrtc-dev-9-0\n",
            "  cuda-runtime-9-0 cuda-samples-9-0 cuda-toolkit-9-0 cuda-visual-tools-9-0\n",
            "0 upgraded, 34 newly installed, 0 to remove and 91 not upgraded.\n",
            "Need to get 0 B/1,097 MB of archives.\n",
            "After this operation, 2,315 MB of additional disk space will be used.\n",
            "Get:1 file:/var/cuda-repo-9-0-local  cuda-license-9-0 9.0.176-1 [22.0 kB]\n",
            "Get:2 file:/var/cuda-repo-9-0-local  cuda-misc-headers-9-0 9.0.176-1 [684 kB]\n",
            "Get:3 file:/var/cuda-repo-9-0-local  cuda-core-9-0 9.0.176-1 [16.9 MB]\n",
            "Get:4 file:/var/cuda-repo-9-0-local  cuda-cudart-9-0 9.0.176-1 [106 kB]\n",
            "Get:5 file:/var/cuda-repo-9-0-local  cuda-driver-dev-9-0 9.0.176-1 [10.9 kB]\n",
            "Get:6 file:/var/cuda-repo-9-0-local  cuda-cudart-dev-9-0 9.0.176-1 [767 kB]\n",
            "Get:7 file:/var/cuda-repo-9-0-local  cuda-command-line-tools-9-0 9.0.176-1 [25.4 MB]\n",
            "Get:8 file:/var/cuda-repo-9-0-local  cuda-nvrtc-9-0 9.0.176-1 [6,348 kB]\n",
            "Get:9 file:/var/cuda-repo-9-0-local  cuda-nvrtc-dev-9-0 9.0.176-1 [9,334 B]\n",
            "Get:10 file:/var/cuda-repo-9-0-local  cuda-cusolver-9-0 9.0.176-1 [26.2 MB]\n",
            "Get:11 file:/var/cuda-repo-9-0-local  cuda-cusolver-dev-9-0 9.0.176-1 [5,317 kB]\n",
            "Get:12 file:/var/cuda-repo-9-0-local  cuda-cublas-9-0 9.0.176-1 [25.0 MB]\n",
            "Get:13 file:/var/cuda-repo-9-0-local  cuda-cublas-dev-9-0 9.0.176-1 [49.4 MB]\n",
            "Get:14 file:/var/cuda-repo-9-0-local  cuda-cufft-9-0 9.0.176-1 [84.1 MB]\n",
            "Get:15 file:/var/cuda-repo-9-0-local  cuda-cufft-dev-9-0 9.0.176-1 [73.7 MB]\n",
            "Get:16 file:/var/cuda-repo-9-0-local  cuda-curand-9-0 9.0.176-1 [38.8 MB]\n",
            "Get:17 file:/var/cuda-repo-9-0-local  cuda-curand-dev-9-0 9.0.176-1 [57.9 MB]\n",
            "Get:18 file:/var/cuda-repo-9-0-local  cuda-cusparse-9-0 9.0.176-1 [25.2 MB]\n",
            "Get:19 file:/var/cuda-repo-9-0-local  cuda-cusparse-dev-9-0 9.0.176-1 [25.3 MB]\n",
            "Get:20 file:/var/cuda-repo-9-0-local  cuda-npp-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:21 file:/var/cuda-repo-9-0-local  cuda-npp-dev-9-0 9.0.176-1 [46.6 MB]\n",
            "Get:22 file:/var/cuda-repo-9-0-local  cuda-nvgraph-9-0 9.0.176-1 [6,081 kB]\n",
            "Get:23 file:/var/cuda-repo-9-0-local  cuda-nvgraph-dev-9-0 9.0.176-1 [5,658 kB]\n",
            "Get:24 file:/var/cuda-repo-9-0-local  cuda-samples-9-0 9.0.176-1 [75.9 MB]\n",
            "Get:25 file:/var/cuda-repo-9-0-local  cuda-documentation-9-0 9.0.176-1 [53.1 MB]\n",
            "Get:26 file:/var/cuda-repo-9-0-local  cuda-libraries-dev-9-0 9.0.176-1 [2,596 B]\n",
            "Get:27 file:/var/cuda-repo-9-0-local  cuda-nvml-dev-9-0 9.0.176-1 [47.6 kB]\n",
            "Get:28 file:/var/cuda-repo-9-0-local  cuda-visual-tools-9-0 9.0.176-1 [398 MB]\n",
            "Get:29 file:/var/cuda-repo-9-0-local  cuda-toolkit-9-0 9.0.176-1 [2,836 B]\n",
            "Get:30 file:/var/cuda-repo-9-0-local  cuda-libraries-9-0 9.0.176-1 [2,566 B]\n",
            "Get:31 file:/var/cuda-repo-9-0-local  cuda-runtime-9-0 9.0.176-1 [2,526 B]\n",
            "Get:32 file:/var/cuda-repo-9-0-local  cuda-demo-suite-9-0 9.0.176-1 [3,880 kB]\n",
            "Get:33 file:/var/cuda-repo-9-0-local  cuda-9-0 9.0.176-1 [2,552 B]\n",
            "Get:34 file:/var/cuda-repo-9-0-local  cuda 9.0.176-1 [2,504 B]\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package cuda-license-9-0.\n",
            "(Reading database ... 145172 files and directories currently installed.)\n",
            "Preparing to unpack .../00-cuda-license-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-license-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-misc-headers-9-0.\n",
            "Preparing to unpack .../01-cuda-misc-headers-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-core-9-0.\n",
            "Preparing to unpack .../02-cuda-core-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-core-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-9-0.\n",
            "Preparing to unpack .../03-cuda-cudart-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-driver-dev-9-0.\n",
            "Preparing to unpack .../04-cuda-driver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cudart-dev-9-0.\n",
            "Preparing to unpack .../05-cuda-cudart-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-command-line-tools-9-0.\n",
            "Preparing to unpack .../06-cuda-command-line-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-9-0.\n",
            "Preparing to unpack .../07-cuda-nvrtc-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvrtc-dev-9-0.\n",
            "Preparing to unpack .../08-cuda-nvrtc-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-9-0.\n",
            "Preparing to unpack .../09-cuda-cusolver-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusolver-dev-9-0.\n",
            "Preparing to unpack .../10-cuda-cusolver-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-9-0.\n",
            "Preparing to unpack .../11-cuda-cublas-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cublas-dev-9-0.\n",
            "Preparing to unpack .../12-cuda-cublas-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-9-0.\n",
            "Preparing to unpack .../13-cuda-cufft-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cufft-dev-9-0.\n",
            "Preparing to unpack .../14-cuda-cufft-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-9-0.\n",
            "Preparing to unpack .../15-cuda-curand-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-curand-dev-9-0.\n",
            "Preparing to unpack .../16-cuda-curand-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-9-0.\n",
            "Preparing to unpack .../17-cuda-cusparse-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-cusparse-dev-9-0.\n",
            "Preparing to unpack .../18-cuda-cusparse-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-9-0.\n",
            "Preparing to unpack .../19-cuda-npp-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-npp-dev-9-0.\n",
            "Preparing to unpack .../20-cuda-npp-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-9-0.\n",
            "Preparing to unpack .../21-cuda-nvgraph-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvgraph-dev-9-0.\n",
            "Preparing to unpack .../22-cuda-nvgraph-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-samples-9-0.\n",
            "Preparing to unpack .../23-cuda-samples-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-samples-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-documentation-9-0.\n",
            "Preparing to unpack .../24-cuda-documentation-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-dev-9-0.\n",
            "Preparing to unpack .../25-cuda-libraries-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-nvml-dev-9-0.\n",
            "Preparing to unpack .../26-cuda-nvml-dev-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-visual-tools-9-0.\n",
            "Preparing to unpack .../27-cuda-visual-tools-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-toolkit-9-0.\n",
            "Preparing to unpack .../28-cuda-toolkit-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-libraries-9-0.\n",
            "Preparing to unpack .../29-cuda-libraries-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-runtime-9-0.\n",
            "Preparing to unpack .../30-cuda-runtime-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-demo-suite-9-0.\n",
            "Preparing to unpack .../31-cuda-demo-suite-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda-9-0.\n",
            "Preparing to unpack .../32-cuda-9-0_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda-9-0 (9.0.176-1) ...\n",
            "Selecting previously unselected package cuda.\n",
            "Preparing to unpack .../33-cuda_9.0.176-1_amd64.deb ...\n",
            "Unpacking cuda (9.0.176-1) ...\n",
            "Setting up cuda-license-9-0 (9.0.176-1) ...\n",
            "*** LICENSE AGREEMENT ***\n",
            "By using this software you agree to fully comply with the terms and \n",
            "conditions of the EULA (End User License Agreement). The EULA is located\n",
            "at /usr/local/cuda-9.0/doc/EULA.txt. The EULA can also be found at\n",
            "http://docs.nvidia.com/cuda/eula/index.html. If you do not agree to the\n",
            "terms and conditions of the EULA, do not use the software.\n",
            "\n",
            "Setting up cuda-cusparse-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusparse-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvml-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cusolver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-misc-headers-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvrtc-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-driver-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-core-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-runtime-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cudart-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cufft-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-npp-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-curand-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-cublas-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-nvgraph-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-command-line-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-demo-suite-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-visual-tools-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-samples-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-libraries-dev-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-documentation-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-toolkit-9-0 (9.0.176-1) ...\n",
            "Setting up cuda-9-0 (9.0.176-1) ...\n",
            "Setting up cuda (9.0.176-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6_G3THLoUoi",
        "colab_type": "text"
      },
      "source": [
        "# Must be using python 2 runtime w GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMz5ZNbql-EW",
        "colab_type": "text"
      },
      "source": [
        "## Run MaskGAN in MLE pretraining mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "dd2cb950-e25f-4265-e6f6-5026bcdf99bd",
        "id": "3mV31eblvixR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/maskgan\n",
        "! python train_mask_gan.py \\\n",
        " --data_dir='dataset/iccv2017' \\\n",
        " --batch_size=20 \\\n",
        " --sequence_length=20 \\\n",
        " --base_directory='maskGAN' \\\n",
        " --hparams=\"gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,dis_num_layers=2,gen_learning_rate=0.00074876,dis_learning_rate=5e-4,baseline_decay=0.99,dis_train_iterations=1,gen_learning_rate_decay=0.95\" \\\n",
        " --mode='TRAIN' \\\n",
        " --max_steps=100 \\\n",
        " --generator_model='seq2seq_vd' \\\n",
        " --discriminator_model='seq2seq_vd' \\\n",
        " --is_present_rate=0.5 \\\n",
        " --summaries_every=10 \\\n",
        " --print_every=250 \\\n",
        " --max_num_to_print=3 \\\n",
        " --gen_training_strategy=cross_entropy \\\n",
        " --seq2seq_share_embedding=true \\\n",
        " --baseline_method=critic \\\n",
        " --attention_option=luong"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "   [0]  of                    0.450   2.658  \n",
            "   [0]  millions              0.447   10.468 \n",
            "   [0]  of                    0.446   2.868  \n",
            "   [0]  dollars               0.449   7.550  \n",
            "   [0]  of                    0.456   3.794  \n",
            "   [0]  stock                 0.463   7.076  \n",
            "   [0]  occur                 0.469   10.645 \n",
            "   [1]  in                    0.476   0.000  \n",
            "   [0]  a                     0.480   2.825  \n",
            "   [0]  matter                0.477   8.006  \n",
            "   [1]  of                    0.474   0.000  \n",
            "   [0]  seconds               0.471   12.137 \n",
            "   [1]  <eos>                 0.467   0.000  \n",
            " Sample 2.\n",
            "   [1]  past                  0.525   0.000  \n",
            "   [1]  year                  0.520   0.000  \n",
            "   [1]  or                    0.515   0.000  \n",
            "   [1]  two                   0.512   0.000  \n",
            "   [1]  the                   0.511   0.000  \n",
            "   [1]  carpet                0.512   0.000  \n",
            "   [0]  division              0.513   8.259  \n",
            "   [0]  's                    0.515   3.766  \n",
            "   [1]  operating             0.516   0.000  \n",
            "   [0]  profit                0.517   7.149  \n",
            "   [1]  margins               0.517   0.000  \n",
            "   [1]  have                  0.517   0.000  \n",
            "   [1]  <unk>                 0.517   0.000  \n",
            "   [1]  around                0.516   0.000  \n",
            "   [0]  N                     0.517   3.900  \n",
            "   [0]  N                     0.519   1.408  \n",
            "   [0]  high                  0.525   8.770  \n",
            "   [0]  by                    0.527   5.212  \n",
            "   [0]  industry              0.527   7.842  \n",
            "   [0]  standards             0.525   9.307  \n",
            "Samples\n",
            "Sample 0 .  increased demand for dairy products at a time of exceptionally high u.s. exports of dry milk coupled with very low\n",
            "Sample 1 .  which can involve the purchase or sale of millions of dollars of stock occur in a matter of seconds <eos>\n",
            "Sample 2 .  past year or two the carpet division 's operating profit margins have <unk> around N N high by industry standards\n",
            "\n",
            "\n",
            "targets[[11 606 508 7 2584 4 603 18 3320 1697 367 172 2 347 26 0 1203 234 9 467][2875 602 1623 7 0 3476 4 444 856 491 280 403 4560 538 1477 51 563 677 2 0][481 189 2 164 6934 0 954 566 19 213...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[652 11 606 508 7 2584 4 603 18 3320...]...][[0 0 0 1 1 1 1 0 0 1...]...][[652 10000 10000 10000 7 2584 4 603 10000 10000...]...][[11 606 508 7 2584 4 603 18 3320 1697...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[652 11 606 508 7 2584 4 603 18 3320...]...][[0 0 0 1 1 1 1 0 0 1...]...][[652 10000 10000 10000 7 2584 4 603 10000 10000...]...][[11 606 508 7 2584 4 603 18 3320 1697...]...]\n",
            "I0212 04:14:08.975794 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[2175 5207 2 1 1 7 35 92 1264 5 39 1048 2 347 26 0 1203 234 9 652][6 156 218 4 12 3 21 4 60 1461 6012 6 1 177 4 12 3 2 5 585][29 2434 370 22 0 3 3 5 3 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[615 2175 5207 2 1 1 7 35 92 1264...]...][[1 1 1 1 1 0 1 0 1 0...]...][[615 2175 5207 2 1 1 10000 35 10000 1264...]...][[2175 5207 2 1 1 7 35 92 1264 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[615 2175 5207 2 1 1 7 35 92 1264...]...][[1 1 1 1 1 0 1 0 1 0...]...][[615 2175 5207 2 1 1 10000 35 10000 1264...]...][[2175 5207 2 1 1 7 35 92 1264 5...]...]\n",
            "targets[[2175 5207 2 1 1 7 35 92 1264 5 39 1048 2 347 26 0 1203 234 9 652][6 156 218 4 12 3 21 4 60 1461 6012 6 1 177 4 12 3 2 5 585][29 2434 370 22 0 3 3 5 3 3...]...]\n",
            "targets[[2175 5207 2 1 1 7 35 92 1264 5 39 1048 2 347 26 0 1203 234 9 652][6 156 218 4 12 3 21 4 60 1461 6012 6 1 177 4 12 3 2 5 585][29 2434 370 22 0 3 3 5 3 3...]...]\n",
            "targets[[2175 5207 2 1 1 7 35 92 1264 5 39 1048 2 347 26 0 1203 234 9 652][6 156 218 4 12 3 21 4 60 1461 6012 6 1 177 4 12 3 2 5 585][29 2434 370 22 0 3 3 5 3 3...]...]\n",
            "global_step: 1410\n",
            " perplexity: 693.548\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[2175 5207 2 1 1 7 35 92 1264 5 39 1048 2 347 26 0 1203 234 9 652][6 156 218 4 12 3 21 4 60 1461 6012 6 1 177 4 12 3 2 5 585][29 2434 370 22 0 3 3 5 3 3...]...]\n",
            " percent of 3-grams captured: 0.200.\n",
            "targets[[2175 5207 2 1 1 7 35 92 1264 5 39 1048 2 347 26 0 1203 234 9 652][6 156 218 4 12 3 21 4 60 1461 6012 6 1 177 4 12 3 2 5 585][29 2434 370 22 0 3 3 5 3 3...]...]\n",
            " percent of 2-grams captured: 0.476.\n",
            "targets[[2175 5207 2 1 1 7 35 92 1264 5 39 1048 2 347 26 0 1203 234 9 652][6 156 218 4 12 3 21 4 60 1461 6012 6 1 177 4 12 3 2 5 585][29 2434 370 22 0 3 3 5 3 3...]...]\n",
            " percent of 4-grams captured: 0.091.\n",
            " geometric_avg: 0.206.\n",
            " arithmetic_avg: 0.256.\n",
            "global_step: 1410\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69630\n",
            " G train loss: 3.29995\n",
            "targets[[2175 5207 2 1 1 7 35 92 1264 5 39 1048 2 347 26 0 1203 234 9 652][6 156 218 4 12 3 21 4 60 1461 6012 6 1 177 4 12 3 2 5 585][29 2434 370 22 0 3 3 5 3 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[615 2175 5207 2 1 1 7 35 92 1264...]...][[1 1 1 1 1 0 1 0 1 0...]...][[615 2175 5207 2 1 1 10000 35 10000 1264...]...][[2175 5207 2 1 1 7 35 92 1264 5...]...]\n",
            " Sample 0.\n",
            "   [1]  import                0.536   0.000  \n",
            "   [1]  quotas                0.527   0.000  \n",
            "   [1]  <eos>                 0.518   0.000  \n",
            "   [1]  <unk>                 0.512   0.000  \n",
            "   [1]  <unk>                 0.509   0.000  \n",
            "   [0]  in                    0.513   3.671  \n",
            "   [1]  new                   0.517   0.000  \n",
            "   [0]  york                  0.525   5.089  \n",
            "   [1]  contributed           0.530   0.000  \n",
            "   [0]  to                    0.535   2.476  \n",
            "   [1]  this                  0.535   0.000  \n",
            "   [0]  article               0.535   8.161  \n",
            "   [0]  <eos>                 0.530   3.612  \n",
            "   [0]  here                  0.528   7.349  \n",
            "   [0]  are                   0.522   5.366  \n",
            "   [0]  the                   0.518   4.456  \n",
            "   [1]  commerce              0.519   0.000  \n",
            "   [1]  department            0.520   0.000  \n",
            "   [1]  's                    0.519   0.000  \n",
            "   [0]  figures               0.517   8.265  \n",
            " Sample 1.\n",
            "   [1]  a                     0.449   0.000  \n",
            "   [0]  program               0.441   7.089  \n",
            "   [1]  trade                 0.432   0.000  \n",
            "   [0]  of                    0.422   3.979  \n",
            "   [1]  $                     0.408   0.000  \n",
            "   [1]  N                     0.399   0.000  \n",
            "   [1]  million               0.391   0.000  \n",
            "   [1]  of                    0.387   0.000  \n",
            "   [1]  stock                 0.388   0.000  \n",
            "   [0]  typically             0.389   9.554  \n",
            "   [0]  earns                 0.391   12.233 \n",
            "   [1]  a                     0.397   0.000  \n",
            "   [1]  <unk>                 0.398   0.000  \n",
            "   [1]  profit                0.400   0.000  \n",
            "   [0]  of                    0.401   2.755  \n",
            "   [0]  $                     0.397   3.670  \n",
            "   [0]  N                     0.395   0.007  \n",
            "   [0]  <eos>                 0.390   1.222  \n",
            "   [0]  to                    0.389   3.256  \n",
            "   [0]  keep                  0.386   7.429  \n",
            " Sample 2.\n",
            "   [0]  but                   0.531   5.628  \n",
            "   [1]  disappointing         0.521   0.000  \n",
            "   [0]  compared              0.516   7.944  \n",
            "   [0]  with                  0.516   4.036  \n",
            "   [0]  the                   0.518   2.346  \n",
            "   [1]  N                     0.521   0.000  \n",
            "   [1]  N                     0.521   0.000  \n",
            "   [0]  to                    0.519   3.082  \n",
            "   [1]  N                     0.514   0.000  \n",
            "   [0]  N                     0.507   2.651  \n",
            "   [1]  margins               0.502   0.000  \n",
            "   [0]  for                   0.498   4.325  \n",
            "   [0]  two                   0.498   6.454  \n",
            "   [1]  of                    0.499   0.000  \n",
            "   [1]  armstrong             0.499   0.000  \n",
            "   [1]  's                    0.496   0.000  \n",
            "   [1]  chief                 0.493   0.000  \n",
            "   [1]  businesses            0.490   0.000  \n",
            "   [0]  <unk>                 0.487   4.069  \n",
            "   [0]  and                   0.488   3.867  \n",
            "Samples\n",
            "Sample 0 .  import quotas <eos> <unk> <unk> in new york contributed to this article <eos> here are the commerce department 's figures\n",
            "Sample 1 .  a program trade of $ N million of stock typically earns a <unk> profit of $ N <eos> to keep\n",
            "Sample 2 .  but disappointing compared with the N N to N N margins for two of armstrong 's chief businesses <unk> and\n",
            "\n",
            "\n",
            "targets[[652 11 930 7 2584 4 603 3320 1697 2 8182 20 0 1 7 1 1 9 6 2978][1803 22 41 88 156 1247 209 665 8 0 2045 115 4518 38 89 1202 26 114 156 77][53 6340 153 22 31 452 3 3 5 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[467 652 11 930 7 2584 4 603 3320 1697...]...][[0 0 0 0 1 1 1 1 0 1...]...][[467 10000 10000 10000 10000 2584 4 603 3320 10000...]...][[652 11 930 7 2584 4 603 3320 1697 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[467 652 11 930 7 2584 4 603 3320 1697...]...][[0 0 0 0 1 1 1 1 0 1...]...][[467 10000 10000 10000 10000 2584 4 603 3320 10000...]...][[652 11 930 7 2584 4 603 3320 1697 2...]...]\n",
            "I0212 04:14:12.288758 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[11 606 508 7 2584 4 603 18 3320 1697 367 172 2 347 26 0 1203 234 9 467][2875 602 1623 7 0 3476 4 444 856 491 280 403 4560 538 1477 51 563 677 2 0][481 189 2 164 6934 0 954 566 19 213...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[652 11 606 508 7 2584 4 603 18 3320...]...][[1 1 0 0 0 0 0 1 0 0...]...][[652 11 606 10000 10000 10000 10000 10000 18 10000...]...][[11 606 508 7 2584 4 603 18 3320 1697...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[652 11 606 508 7 2584 4 603 18 3320...]...][[1 1 0 0 0 0 0 1 0 0...]...][[652 11 606 10000 10000 10000 10000 10000 18 10000...]...][[11 606 508 7 2584 4 603 18 3320 1697...]...]\n",
            "targets[[11 606 508 7 2584 4 603 18 3320 1697 367 172 2 347 26 0 1203 234 9 467][2875 602 1623 7 0 3476 4 444 856 491 280 403 4560 538 1477 51 563 677 2 0][481 189 2 164 6934 0 954 566 19 213...]...]\n",
            "targets[[11 606 508 7 2584 4 603 18 3320 1697 367 172 2 347 26 0 1203 234 9 467][2875 602 1623 7 0 3476 4 444 856 491 280 403 4560 538 1477 51 563 677 2 0][481 189 2 164 6934 0 954 566 19 213...]...]\n",
            "targets[[11 606 508 7 2584 4 603 18 3320 1697 367 172 2 347 26 0 1203 234 9 467][2875 602 1623 7 0 3476 4 444 856 491 280 403 4560 538 1477 51 563 677 2 0][481 189 2 164 6934 0 954 566 19 213...]...]\n",
            "global_step: 1413\n",
            " perplexity: 693.348\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[11 606 508 7 2584 4 603 18 3320 1697 367 172 2 347 26 0 1203 234 9 467][2875 602 1623 7 0 3476 4 444 856 491 280 403 4560 538 1477 51 563 677 2 0][481 189 2 164 6934 0 954 566 19 213...]...]\n",
            " percent of 3-grams captured: 0.158.\n",
            "targets[[11 606 508 7 2584 4 603 18 3320 1697 367 172 2 347 26 0 1203 234 9 467][2875 602 1623 7 0 3476 4 444 856 491 280 403 4560 538 1477 51 563 677 2 0][481 189 2 164 6934 0 954 566 19 213...]...]\n",
            " percent of 2-grams captured: 0.458.\n",
            "targets[[11 606 508 7 2584 4 603 18 3320 1697 367 172 2 347 26 0 1203 234 9 467][2875 602 1623 7 0 3476 4 444 856 491 280 403 4560 538 1477 51 563 677 2 0][481 189 2 164 6934 0 954 566 19 213...]...]\n",
            " percent of 4-grams captured: 0.056.\n",
            " geometric_avg: 0.159.\n",
            " arithmetic_avg: 0.224.\n",
            "global_step: 1413\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69626\n",
            " G train loss: 3.29964\n",
            "targets[[11 606 508 7 2584 4 603 18 3320 1697 367 172 2 347 26 0 1203 234 9 467][2875 602 1623 7 0 3476 4 444 856 491 280 403 4560 538 1477 51 563 677 2 0][481 189 2 164 6934 0 954 566 19 213...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[652 11 606 508 7 2584 4 603 18 3320...]...][[1 1 0 0 0 0 0 1 0 0...]...][[652 11 606 10000 10000 10000 10000 10000 18 10000...]...][[11 606 508 7 2584 4 603 18 3320 1697...]...]\n",
            " Sample 0.\n",
            "   [1]  for                   0.574   0.000  \n",
            "   [1]  construction          0.566   0.000  \n",
            "   [0]  spending              0.562   7.113  \n",
            "   [0]  in                    0.564   3.830  \n",
            "   [0]  billions              0.567   9.681  \n",
            "   [0]  of                    0.574   2.731  \n",
            "   [0]  dollars               0.579   6.731  \n",
            "   [1]  at                    0.581   0.000  \n",
            "   [0]  seasonally            0.581   10.027 \n",
            "   [0]  adjusted              0.578   7.475  \n",
            "   [1]  annual                0.573   0.000  \n",
            "   [0]  rates                 0.572   7.287  \n",
            "   [0]  <eos>                 0.573   3.198  \n",
            "   [0]  here                  0.574   7.541  \n",
            "   [1]  are                   0.574   0.000  \n",
            "   [1]  the                   0.577   0.000  \n",
            "   [0]  commerce              0.579   6.593  \n",
            "   [1]  department            0.579   0.000  \n",
            "   [0]  's                    0.577   4.305  \n",
            "   [0]  latest                0.572   7.413  \n",
            " Sample 1.\n",
            "   [0]  program-trading       0.505   9.607  \n",
            "   [1]  units                 0.504   0.000  \n",
            "   [1]  profitable            0.510   0.000  \n",
            "   [0]  in                    0.515   3.461  \n",
            "   [0]  the                   0.521   3.139  \n",
            "   [1]  eyes                  0.522   0.000  \n",
            "   [1]  of                    0.520   0.000  \n",
            "   [1]  senior                0.517   0.000  \n",
            "   [1]  brokerage             0.513   0.000  \n",
            "   [1]  executives            0.511   0.000  \n",
            "   [0]  traders               0.511   7.259  \n",
            "   [1]  must                  0.512   0.000  \n",
            "   [0]  seize                 0.512   11.183 \n",
            "   [0]  every                 0.510   8.079  \n",
            "   [0]  opportunity           0.504   8.306  \n",
            "   [1]  their                 0.498   0.000  \n",
            "   [1]  computers             0.491   0.000  \n",
            "   [1]  find                  0.488   0.000  \n",
            "   [0]  <eos>                 0.487   4.303  \n",
            "   [1]  the                   0.489   0.000  \n",
            " Sample 2.\n",
            "   [0]  building              0.507   7.491  \n",
            "   [1]  products              0.499   0.000  \n",
            "   [1]  <eos>                 0.491   0.000  \n",
            "   [1]  analysts              0.486   0.000  \n",
            "   [0]  hailed                0.485   11.354 \n",
            "   [0]  the                   0.485   2.891  \n",
            "   [1]  planned               0.486   0.000  \n",
            "   [1]  transaction           0.487   0.000  \n",
            "   [1]  as                    0.489   0.000  \n",
            "   [0]  being                 0.488   7.150  \n",
            "   [1]  beneficial            0.487   0.000  \n",
            "   [0]  to                    0.487   2.653  \n",
            "   [1]  armstrong             0.488   0.000  \n",
            "   [0]  and                   0.489   3.820  \n",
            "   [0]  shaw                  0.491   10.819 \n",
            "   [1]  the                   0.491   0.000  \n",
            "   [1]  market                0.488   0.000  \n",
            "   [0]  leader                0.483   8.244  \n",
            "   [1]  in                    0.478   0.000  \n",
            "   [0]  the                   0.475   1.821  \n",
            "Samples\n",
            "Sample 0 .  for construction spending in billions of dollars at seasonally adjusted annual rates <eos> here are the commerce department 's latest\n",
            "Sample 1 .  program-trading units profitable in the eyes of senior brokerage executives traders must seize every opportunity their computers find <eos> the\n",
            "Sample 2 .  building products <eos> analysts hailed the planned transaction as being beneficial to armstrong and shaw the market leader in the\n",
            "\n",
            "\n",
            "targets[[9401 2082 1 3 1975 12 3 3452 6324 16 179 1802 4 0 691 34 6 581 7 246][1815 1 5 1 2 66 110 385 5 87 39 0 504 2899 8975 106 504 0 4527 1][81 4 43 12 3 48 8 30 3371 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2978 9401 2082 1 3 1975 12 3 3452 6324...]...][[0 0 1 1 1 1 0 0 1 0...]...][[2978 10000 10000 1 3 1975 12 10000 10000 6324...]...][[9401 2082 1 3 1975 12 3 3452 6324 16...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2978 9401 2082 1 3 1975 12 3 3452 6324...]...][[0 0 1 1 1 1 0 0 1 0...]...][[2978 10000 10000 1 3 1975 12 10000 10000 6324...]...][[9401 2082 1 3 1975 12 3 3452 6324 16...]...]\n",
            "I0212 04:14:15.594417 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[652 11 930 7 2584 4 603 3320 1697 2 8182 20 0 1 7 1 1 9 6 2978][1803 22 41 88 156 1247 209 665 8 0 2045 115 4518 38 89 1202 26 114 156 77][53 6340 153 22 31 452 3 3 5 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[467 652 11 930 7 2584 4 603 3320 1697...]...][[0 1 1 0 1 0 1 0 0 0...]...][[467 10000 11 930 10000 2584 10000 603 10000 10000...]...][[652 11 930 7 2584 4 603 3320 1697 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[467 652 11 930 7 2584 4 603 3320 1697...]...][[0 1 1 0 1 0 1 0 0 0...]...][[467 10000 11 930 10000 2584 10000 603 10000 10000...]...][[652 11 930 7 2584 4 603 3320 1697 2...]...]\n",
            "targets[[652 11 930 7 2584 4 603 3320 1697 2 8182 20 0 1 7 1 1 9 6 2978][1803 22 41 88 156 1247 209 665 8 0 2045 115 4518 38 89 1202 26 114 156 77][53 6340 153 22 31 452 3 3 5 3...]...]\n",
            "targets[[652 11 930 7 2584 4 603 3320 1697 2 8182 20 0 1 7 1 1 9 6 2978][1803 22 41 88 156 1247 209 665 8 0 2045 115 4518 38 89 1202 26 114 156 77][53 6340 153 22 31 452 3 3 5 3...]...]\n",
            "targets[[652 11 930 7 2584 4 603 3320 1697 2 8182 20 0 1 7 1 1 9 6 2978][1803 22 41 88 156 1247 209 665 8 0 2045 115 4518 38 89 1202 26 114 156 77][53 6340 153 22 31 452 3 3 5 3...]...]\n",
            "global_step: 1416\n",
            " perplexity: 693.426\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[652 11 930 7 2584 4 603 3320 1697 2 8182 20 0 1 7 1 1 9 6 2978][1803 22 41 88 156 1247 209 665 8 0 2045 115 4518 38 89 1202 26 114 156 77][53 6340 153 22 31 452 3 3 5 3...]...]\n",
            " percent of 3-grams captured: 0.158.\n",
            "targets[[652 11 930 7 2584 4 603 3320 1697 2 8182 20 0 1 7 1 1 9 6 2978][1803 22 41 88 156 1247 209 665 8 0 2045 115 4518 38 89 1202 26 114 156 77][53 6340 153 22 31 452 3 3 5 3...]...]\n",
            " percent of 2-grams captured: 0.424.\n",
            "targets[[652 11 930 7 2584 4 603 3320 1697 2 8182 20 0 1 7 1 1 9 6 2978][1803 22 41 88 156 1247 209 665 8 0 2045 115 4518 38 89 1202 26 114 156 77][53 6340 153 22 31 452 3 3 5 3...]...]\n",
            " percent of 4-grams captured: 0.038.\n",
            " geometric_avg: 0.137.\n",
            " arithmetic_avg: 0.207.\n",
            "global_step: 1416\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69623\n",
            " G train loss: 3.30000\n",
            "targets[[652 11 930 7 2584 4 603 3320 1697 2 8182 20 0 1 7 1 1 9 6 2978][1803 22 41 88 156 1247 209 665 8 0 2045 115 4518 38 89 1202 26 114 156 77][53 6340 153 22 31 452 3 3 5 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[467 652 11 930 7 2584 4 603 3320 1697...]...][[0 1 1 0 1 0 1 0 0 0...]...][[467 10000 11 930 10000 2584 10000 603 10000 10000...]...][[652 11 930 7 2584 4 603 3320 1697 2...]...]\n",
            " Sample 0.\n",
            "   [0]  figures               0.570   8.579  \n",
            "   [1]  for                   0.568   0.000  \n",
            "   [1]  manufacturers         0.563   0.000  \n",
            "   [0]  in                    0.558   4.882  \n",
            "   [1]  billions              0.552   0.000  \n",
            "   [0]  of                    0.550   2.980  \n",
            "   [1]  dollars               0.549   0.000  \n",
            "   [0]  seasonally            0.547   9.757  \n",
            "   [0]  adjusted              0.545   7.899  \n",
            "   [0]  <eos>                 0.544   2.229  \n",
            "   [0]  judging               0.541   10.997 \n",
            "   [0]  from                  0.535   5.258  \n",
            "   [1]  the                   0.532   0.000  \n",
            "   [1]  <unk>                 0.530   0.000  \n",
            "   [0]  in                    0.529   6.005  \n",
            "   [0]  <unk>                 0.528   4.071  \n",
            "   [0]  <unk>                 0.528   5.086  \n",
            "   [1]  's                    0.524   0.000  \n",
            "   [1]  a                     0.518   0.000  \n",
            "   [0]  wild                  0.509   10.019 \n",
            " Sample 1.\n",
            "   [0]  speed                 0.474   8.182  \n",
            "   [1]  with                  0.464   0.000  \n",
            "   [0]  which                 0.458   6.439  \n",
            "   [1]  such                  0.452   0.000  \n",
            "   [1]  program               0.450   0.000  \n",
            "   [0]  trades                0.454   8.424  \n",
            "   [0]  take                  0.456   7.998  \n",
            "   [0]  place                 0.456   8.061  \n",
            "   [0]  and                   0.457   3.717  \n",
            "   [0]  the                   0.460   3.827  \n",
            "   [1]  volatile              0.460   0.000  \n",
            "   [1]  price                 0.459   0.000  \n",
            "   [1]  movements             0.458   0.000  \n",
            "   [1]  they                  0.457   0.000  \n",
            "   [0]  can                   0.458   5.958  \n",
            "   [1]  cause                 0.460   0.000  \n",
            "   [1]  are                   0.461   0.000  \n",
            "   [0]  what                  0.463   6.688  \n",
            "   [0]  program               0.462   7.163  \n",
            "   [0]  trading               0.466   6.305  \n",
            " Sample 2.\n",
            "   [1]  u.s.                  0.553   0.000  \n",
            "   [1]  carpet                0.549   0.000  \n",
            "   [1]  industry              0.543   0.000  \n",
            "   [1]  with                  0.538   0.000  \n",
            "   [0]  an                    0.532   5.071  \n",
            "   [0]  estimated             0.527   8.494  \n",
            "   [1]  N                     0.524   0.000  \n",
            "   [0]  N                     0.522   3.696  \n",
            "   [1]  to                    0.519   0.000  \n",
            "   [0]  N                     0.516   1.932  \n",
            "   [0]  N                     0.512   2.820  \n",
            "   [0]  share                 0.507   6.007  \n",
            "   [1]  <eos>                 0.503   0.000  \n",
            "   [1]  shaw                  0.498   0.000  \n",
            "   [1]  based                 0.498   0.000  \n",
            "   [0]  in                    0.502   2.944  \n",
            "   [0]  <unk>                 0.505   4.095  \n",
            "   [0]  ga.                   0.510   11.669 \n",
            "   [1]  has                   0.514   0.000  \n",
            "   [1]  annual                0.515   0.000  \n",
            "Samples\n",
            "Sample 0 .  figures for manufacturers in billions of dollars seasonally adjusted <eos> judging from the <unk> in <unk> <unk> 's a wild\n",
            "Sample 1 .  speed with which such program trades take place and the volatile price movements they can cause are what program trading\n",
            "Sample 2 .  u.s. carpet industry with an estimated N N to N N share <eos> shaw based in <unk> ga. has annual\n",
            "\n",
            "\n",
            "targets[[2 366 388 7 203 0 3502 9 1 13 511 3172 650 859 140 2 3358 4465 1 5106][1359 193 8 1184 193 44 4734 1 1275 1061 18 9205 82 1 2 29 97 1395 8 280][0 2562 4 4423 9 1323 238 124 0 4423...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[246 2 366 388 7 203 0 3502 9 1...]...][[0 1 1 1 0 1 0 0 1 1...]...][[246 10000 366 388 7 10000 0 10000 10000 1...]...][[2 366 388 7 203 0 3502 9 1 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[246 2 366 388 7 203 0 3502 9 1...]...][[0 1 1 1 0 1 0 0 1 1...]...][[246 10000 366 388 7 10000 0 10000 10000 1...]...][[2 366 388 7 203 0 3502 9 1 13...]...]\n",
            "I0212 04:14:18.850733 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[9401 2082 1 3 1975 12 3 3452 6324 16 179 1802 4 0 691 34 6 581 7 246][1815 1 5 1 2 66 110 385 5 87 39 0 504 2899 8975 106 504 0 4527 1][81 4 43 12 3 48 8 30 3371 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2978 9401 2082 1 3 1975 12 3 3452 6324...]...][[0 1 0 1 0 0 0 0 0 1...]...][[2978 10000 2082 10000 3 10000 10000 10000 10000 10000...]...][[9401 2082 1 3 1975 12 3 3452 6324 16...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2978 9401 2082 1 3 1975 12 3 3452 6324...]...][[0 1 0 1 0 0 0 0 0 1...]...][[2978 10000 2082 10000 3 10000 10000 10000 10000 10000...]...][[9401 2082 1 3 1975 12 3 3452 6324 16...]...]\n",
            "targets[[9401 2082 1 3 1975 12 3 3452 6324 16 179 1802 4 0 691 34 6 581 7 246][1815 1 5 1 2 66 110 385 5 87 39 0 504 2899 8975 106 504 0 4527 1][81 4 43 12 3 48 8 30 3371 4...]...]\n",
            "targets[[9401 2082 1 3 1975 12 3 3452 6324 16 179 1802 4 0 691 34 6 581 7 246][1815 1 5 1 2 66 110 385 5 87 39 0 504 2899 8975 106 504 0 4527 1][81 4 43 12 3 48 8 30 3371 4...]...]\n",
            "targets[[9401 2082 1 3 1975 12 3 3452 6324 16 179 1802 4 0 691 34 6 581 7 246][1815 1 5 1 2 66 110 385 5 87 39 0 504 2899 8975 106 504 0 4527 1][81 4 43 12 3 48 8 30 3371 4...]...]\n",
            "global_step: 1419\n",
            " perplexity: 693.213\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[9401 2082 1 3 1975 12 3 3452 6324 16 179 1802 4 0 691 34 6 581 7 246][1815 1 5 1 2 66 110 385 5 87 39 0 504 2899 8975 106 504 0 4527 1][81 4 43 12 3 48 8 30 3371 4...]...]\n",
            " percent of 3-grams captured: 0.164.\n",
            "targets[[9401 2082 1 3 1975 12 3 3452 6324 16 179 1802 4 0 691 34 6 581 7 246][1815 1 5 1 2 66 110 385 5 87 39 0 504 2899 8975 106 504 0 4527 1][81 4 43 12 3 48 8 30 3371 4...]...]\n",
            " percent of 2-grams captured: 0.468.\n",
            "targets[[9401 2082 1 3 1975 12 3 3452 6324 16 179 1802 4 0 691 34 6 581 7 246][1815 1 5 1 2 66 110 385 5 87 39 0 504 2899 8975 106 504 0 4527 1][81 4 43 12 3 48 8 30 3371 4...]...]\n",
            " percent of 4-grams captured: 0.044.\n",
            " geometric_avg: 0.150.\n",
            " arithmetic_avg: 0.225.\n",
            "global_step: 1419\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69624\n",
            " G train loss: 3.30028\n",
            "targets[[9401 2082 1 3 1975 12 3 3452 6324 16 179 1802 4 0 691 34 6 581 7 246][1815 1 5 1 2 66 110 385 5 87 39 0 504 2899 8975 106 504 0 4527 1][81 4 43 12 3 48 8 30 3371 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2978 9401 2082 1 3 1975 12 3 3452 6324...]...][[0 1 0 1 0 0 0 0 0 1...]...][[2978 10000 2082 10000 3 10000 10000 10000 10000 10000...]...][[9401 2082 1 3 1975 12 3 3452 6324 16...]...]\n",
            " Sample 0.\n",
            "   [0]  sheep                 0.538   12.178 \n",
            "   [1]  chase                 0.537   0.000  \n",
            "   [0]  <unk>                 0.538   3.558  \n",
            "   [1]  N                     0.539   0.000  \n",
            "   [0]  pages                 0.538   11.095 \n",
            "   [0]  $                     0.532   5.789  \n",
            "   [0]  N                     0.529   0.038  \n",
            "   [0]  baby                  0.527   13.085 \n",
            "   [0]  boomers               0.526   13.733 \n",
            "   [1]  on                    0.528   0.000  \n",
            "   [1]  both                  0.531   0.000  \n",
            "   [0]  sides                 0.527   9.573  \n",
            "   [0]  of                    0.528   2.706  \n",
            "   [1]  the                   0.533   0.000  \n",
            "   [0]  pacific               0.534   8.667  \n",
            "   [1]  have                  0.533   0.000  \n",
            "   [1]  a                     0.531   0.000  \n",
            "   [1]  lot                   0.529   0.000  \n",
            "   [0]  in                    0.526   3.657  \n",
            "   [0]  common                0.523   7.433  \n",
            " Sample 1.\n",
            "   [0]  critics               0.460   9.464  \n",
            "   [0]  <unk>                 0.454   4.440  \n",
            "   [1]  to                    0.452   0.000  \n",
            "   [0]  <unk>                 0.448   4.326  \n",
            "   [0]  <eos>                 0.444   3.259  \n",
            "   [1]  if                    0.444   0.000  \n",
            "   [0]  you                   0.444   6.796  \n",
            "   [0]  continue              0.441   7.862  \n",
            "   [1]  to                    0.441   0.000  \n",
            "   [1]  do                    0.438   0.000  \n",
            "   [1]  this                  0.438   0.000  \n",
            "   [0]  the                   0.442   5.919  \n",
            "   [0]  investor              0.445   7.291  \n",
            "   [1]  becomes               0.447   0.000  \n",
            "   [0]  frightened            0.448   10.943 \n",
            "   [0]  any                   0.449   7.067  \n",
            "   [1]  investor              0.449   0.000  \n",
            "   [0]  the                   0.455   5.877  \n",
            "   [0]  odd                   0.456   9.457  \n",
            "   [1]  <unk>                 0.454   0.000  \n",
            " Sample 2.\n",
            "   [0]  sales                 0.553   6.966  \n",
            "   [1]  of                    0.543   0.000  \n",
            "   [1]  about                 0.536   0.000  \n",
            "   [1]  $                     0.525   0.000  \n",
            "   [0]  N                     0.513   0.235  \n",
            "   [0]  billion               0.504   1.510  \n",
            "   [0]  and                   0.500   2.853  \n",
            "   [0]  has                   0.500   6.894  \n",
            "   [0]  economies             0.498   11.482 \n",
            "   [1]  of                    0.499   0.000  \n",
            "   [0]  scale                 0.501   10.028 \n",
            "   [1]  and                   0.504   0.000  \n",
            "   [1]  lower                 0.506   0.000  \n",
            "   [0]  <unk>                 0.510   4.061  \n",
            "   [0]  costs                 0.510   7.431  \n",
            "   [0]  that                  0.507   4.021  \n",
            "   [0]  are                   0.504   5.893  \n",
            "   [0]  expected              0.502   6.902  \n",
            "   [1]  to                    0.502   0.000  \n",
            "   [0]  boost                 0.497   7.252  \n",
            "Samples\n",
            "Sample 0 .  sheep chase <unk> N pages $ N baby boomers on both sides of the pacific have a lot in common\n",
            "Sample 1 .  critics <unk> to <unk> <eos> if you continue to do this the investor becomes frightened any investor the odd <unk>\n",
            "Sample 2 .  sales of about $ N billion and has economies of scale and lower <unk> costs that are expected to boost\n",
            "\n",
            "\n",
            "targets[[1 1 1809 1 8 2245 4435 8705 9343 2 38 1336 1 1 8 1200 43 1 8 1][117 10 156 77 13 32 0 1108 881 11 2415 3187 2 68 34 63 919 54 1 4][35 92 60 111 6630 9 71 248 8922 18...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5106 1 1 1809 1 8 2245 4435 8705 9343...]...][[1 1 0 1 0 1 1 1 0 0...]...][[5106 1 1 10000 1 10000 2245 4435 8705 10000...]...][[1 1 1809 1 8 2245 4435 8705 9343 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5106 1 1 1809 1 8 2245 4435 8705 9343...]...][[1 1 0 1 0 1 1 1 0 0...]...][[5106 1 1 10000 1 10000 2245 4435 8705 10000...]...][[1 1 1809 1 8 2245 4435 8705 9343 2...]...]\n",
            "I0212 04:14:22.146301 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[2 366 388 7 203 0 3502 9 1 13 511 3172 650 859 140 2 3358 4465 1 5106][1359 193 8 1184 193 44 4734 1 1275 1061 18 9205 82 1 2 29 97 1395 8 280][0 2562 4 4423 9 1323 238 124 0 4423...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[246 2 366 388 7 203 0 3502 9 1...]...][[1 1 0 1 1 0 1 1 1 0...]...][[246 2 366 10000 7 203 10000 3502 9 1...]...][[2 366 388 7 203 0 3502 9 1 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[246 2 366 388 7 203 0 3502 9 1...]...][[1 1 0 1 1 0 1 1 1 0...]...][[246 2 366 10000 7 203 10000 3502 9 1...]...][[2 366 388 7 203 0 3502 9 1 13...]...]\n",
            "targets[[2 366 388 7 203 0 3502 9 1 13 511 3172 650 859 140 2 3358 4465 1 5106][1359 193 8 1184 193 44 4734 1 1275 1061 18 9205 82 1 2 29 97 1395 8 280][0 2562 4 4423 9 1323 238 124 0 4423...]...]\n",
            "targets[[2 366 388 7 203 0 3502 9 1 13 511 3172 650 859 140 2 3358 4465 1 5106][1359 193 8 1184 193 44 4734 1 1275 1061 18 9205 82 1 2 29 97 1395 8 280][0 2562 4 4423 9 1323 238 124 0 4423...]...]\n",
            "targets[[2 366 388 7 203 0 3502 9 1 13 511 3172 650 859 140 2 3358 4465 1 5106][1359 193 8 1184 193 44 4734 1 1275 1061 18 9205 82 1 2 29 97 1395 8 280][0 2562 4 4423 9 1323 238 124 0 4423...]...]\n",
            "global_step: 1422\n",
            " perplexity: 693.215\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[2 366 388 7 203 0 3502 9 1 13 511 3172 650 859 140 2 3358 4465 1 5106][1359 193 8 1184 193 44 4734 1 1275 1061 18 9205 82 1 2 29 97 1395 8 280][0 2562 4 4423 9 1323 238 124 0 4423...]...]\n",
            " percent of 3-grams captured: 0.150.\n",
            "targets[[2 366 388 7 203 0 3502 9 1 13 511 3172 650 859 140 2 3358 4465 1 5106][1359 193 8 1184 193 44 4734 1 1275 1061 18 9205 82 1 2 29 97 1395 8 280][0 2562 4 4423 9 1323 238 124 0 4423...]...]\n",
            " percent of 2-grams captured: 0.471.\n",
            "targets[[2 366 388 7 203 0 3502 9 1 13 511 3172 650 859 140 2 3358 4465 1 5106][1359 193 8 1184 193 44 4734 1 1275 1061 18 9205 82 1 2 29 97 1395 8 280][0 2562 4 4423 9 1323 238 124 0 4423...]...]\n",
            " percent of 4-grams captured: 0.041.\n",
            " geometric_avg: 0.143.\n",
            " arithmetic_avg: 0.221.\n",
            "global_step: 1422\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69625\n",
            " G train loss: 3.30035\n",
            "targets[[2 366 388 7 203 0 3502 9 1 13 511 3172 650 859 140 2 3358 4465 1 5106][1359 193 8 1184 193 44 4734 1 1275 1061 18 9205 82 1 2 29 97 1395 8 280][0 2562 4 4423 9 1323 238 124 0 4423...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[246 2 366 388 7 203 0 3502 9 1...]...][[1 1 0 1 1 0 1 1 1 0...]...][[246 2 366 10000 7 203 10000 3502 9 1...]...][[2 366 388 7 203 0 3502 9 1 13...]...]\n",
            " Sample 0.\n",
            "   [1]  <eos>                 0.497   0.000  \n",
            "   [1]  although              0.485   0.000  \n",
            "   [0]  set                   0.477   7.490  \n",
            "   [1]  in                    0.472   0.000  \n",
            "   [1]  japan                 0.469   0.000  \n",
            "   [0]  the                   0.466   4.222  \n",
            "   [1]  novel                 0.461   0.000  \n",
            "   [1]  's                    0.455   0.000  \n",
            "   [1]  <unk>                 0.453   0.000  \n",
            "   [0]  is                    0.452   4.221  \n",
            "   [1]  almost                0.451   0.000  \n",
            "   [1]  entirely              0.451   0.000  \n",
            "   [0]  western               0.450   8.347  \n",
            "   [0]  especially            0.448   8.369  \n",
            "   [0]  american              0.443   8.086  \n",
            "   [0]  <eos>                 0.439   4.438  \n",
            "   [1]  characters            0.437   0.000  \n",
            "   [1]  drink                 0.437   0.000  \n",
            "   [0]  <unk>                 0.439   3.753  \n",
            "   [0]  dogs                  0.444   12.336 \n",
            " Sample 1.\n",
            "   [1]  mutual                0.508   0.000  \n",
            "   [1]  funds                 0.502   0.000  \n",
            "   [0]  and                   0.497   3.376  \n",
            "   [1]  pension               0.493   0.000  \n",
            "   [0]  funds                 0.492   7.297  \n",
            "   [1]  says                  0.491   0.000  \n",
            "   [1]  larry                 0.491   0.000  \n",
            "   [1]  <unk>                 0.492   0.000  \n",
            "   [0]  managing              0.494   8.736  \n",
            "   [1]  partner               0.495   0.000  \n",
            "   [1]  at                    0.494   0.000  \n",
            "   [0]  neuberger             0.492   12.934 \n",
            "   [1]  &                     0.491   0.000  \n",
            "   [1]  <unk>                 0.492   0.000  \n",
            "   [0]  <eos>                 0.494   3.336  \n",
            "   [1]  but                   0.495   0.000  \n",
            "   [1]  many                  0.495   0.000  \n",
            "   [1]  experts               0.496   0.000  \n",
            "   [0]  and                   0.497   3.850  \n",
            "   [0]  traders               0.494   7.531  \n",
            " Sample 2.\n",
            "   [1]  the                   0.540   0.000  \n",
            "   [0]  profitability         0.532   8.702  \n",
            "   [0]  of                    0.528   2.999  \n",
            "   [1]  armstrong             0.526   0.000  \n",
            "   [0]  's                    0.524   4.642  \n",
            "   [1]  brands                0.520   0.000  \n",
            "   [0]  sold                  0.519   8.303  \n",
            "   [0]  under                 0.518   6.415  \n",
            "   [1]  the                   0.518   0.000  \n",
            "   [1]  armstrong             0.523   0.000  \n",
            "   [1]  and                   0.531   0.000  \n",
            "   [0]  <unk>                 0.540   3.684  \n",
            "   [0]  names                 0.546   11.226 \n",
            "   [0]  <eos>                 0.548   2.492  \n",
            "   [1]  yesterday             0.547   0.000  \n",
            "   [0]  in                    0.541   3.565  \n",
            "   [0]  composite             0.534   7.109  \n",
            "   [1]  trading               0.528   0.000  \n",
            "   [1]  on                    0.527   0.000  \n",
            "   [0]  the                   0.530   2.590  \n",
            "Samples\n",
            "Sample 0 .  <eos> although set in japan the novel 's <unk> is almost entirely western especially american <eos> characters drink <unk> dogs\n",
            "Sample 1 .  mutual funds and pension funds says larry <unk> managing partner at neuberger & <unk> <eos> but many experts and traders\n",
            "Sample 2 .  the profitability of armstrong 's brands sold under the armstrong and <unk> names <eos> yesterday in composite trading on the\n",
            "\n",
            "\n",
            "targets[[2 38 2020 43 51 6798 4465 306 121 8 2964 148 2774 1 8 1 1699 2 39 13][1042 5 414 1534 16 156 77 44 6 1 596 571 2035 1 5244 31 1283 16 0 967][1615 16 0 129 146 248 18 12 3 52...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 2 38 2020 43 51 6798 4465 306 121...]...][[0 1 1 0 1 1 1 0 1 1...]...][[1 10000 38 2020 10000 51 6798 4465 10000 121...]...][[2 38 2020 43 51 6798 4465 306 121 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 2 38 2020 43 51 6798 4465 306 121...]...][[0 1 1 0 1 1 1 0 1 1...]...][[1 10000 38 2020 10000 51 6798 4465 10000 121...]...][[2 38 2020 43 51 6798 4465 306 121 8...]...]\n",
            "I0212 04:14:25.427073 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1 1 1809 1 8 2245 4435 8705 9343 2 38 1336 1 1 8 1200 43 1 8 1][117 10 156 77 13 32 0 1108 881 11 2415 3187 2 68 34 63 919 54 1 4][35 92 60 111 6630 9 71 248 8922 18...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5106 1 1 1809 1 8 2245 4435 8705 9343...]...][[0 1 0 1 1 0 0 0 1 0...]...][[5106 10000 1 10000 1 8 10000 10000 10000 9343...]...][[1 1 1809 1 8 2245 4435 8705 9343 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5106 1 1 1809 1 8 2245 4435 8705 9343...]...][[0 1 0 1 1 0 0 0 1 0...]...][[5106 10000 1 10000 1 8 10000 10000 10000 9343...]...][[1 1 1809 1 8 2245 4435 8705 9343 2...]...]\n",
            "targets[[1 1 1809 1 8 2245 4435 8705 9343 2 38 1336 1 1 8 1200 43 1 8 1][117 10 156 77 13 32 0 1108 881 11 2415 3187 2 68 34 63 919 54 1 4][35 92 60 111 6630 9 71 248 8922 18...]...]\n",
            "targets[[1 1 1809 1 8 2245 4435 8705 9343 2 38 1336 1 1 8 1200 43 1 8 1][117 10 156 77 13 32 0 1108 881 11 2415 3187 2 68 34 63 919 54 1 4][35 92 60 111 6630 9 71 248 8922 18...]...]\n",
            "targets[[1 1 1809 1 8 2245 4435 8705 9343 2 38 1336 1 1 8 1200 43 1 8 1][117 10 156 77 13 32 0 1108 881 11 2415 3187 2 68 34 63 919 54 1 4][35 92 60 111 6630 9 71 248 8922 18...]...]\n",
            "global_step: 1425\n",
            " perplexity: 693.138\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1 1 1809 1 8 2245 4435 8705 9343 2 38 1336 1 1 8 1200 43 1 8 1][117 10 156 77 13 32 0 1108 881 11 2415 3187 2 68 34 63 919 54 1 4][35 92 60 111 6630 9 71 248 8922 18...]...]\n",
            " percent of 3-grams captured: 0.167.\n",
            "targets[[1 1 1809 1 8 2245 4435 8705 9343 2 38 1336 1 1 8 1200 43 1 8 1][117 10 156 77 13 32 0 1108 881 11 2415 3187 2 68 34 63 919 54 1 4][35 92 60 111 6630 9 71 248 8922 18...]...]\n",
            " percent of 2-grams captured: 0.476.\n",
            "targets[[1 1 1809 1 8 2245 4435 8705 9343 2 38 1336 1 1 8 1200 43 1 8 1][117 10 156 77 13 32 0 1108 881 11 2415 3187 2 68 34 63 919 54 1 4][35 92 60 111 6630 9 71 248 8922 18...]...]\n",
            " percent of 4-grams captured: 0.056.\n",
            " geometric_avg: 0.164.\n",
            " arithmetic_avg: 0.233.\n",
            "global_step: 1425\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69624\n",
            " G train loss: 3.30062\n",
            "targets[[1 1 1809 1 8 2245 4435 8705 9343 2 38 1336 1 1 8 1200 43 1 8 1][117 10 156 77 13 32 0 1108 881 11 2415 3187 2 68 34 63 919 54 1 4][35 92 60 111 6630 9 71 248 8922 18...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5106 1 1 1809 1 8 2245 4435 8705 9343...]...][[0 1 0 1 1 0 0 0 1 0...]...][[5106 10000 1 10000 1 8 10000 10000 10000 9343...]...][[1 1 1809 1 8 2245 4435 8705 9343 2...]...]\n",
            " Sample 0.\n",
            "   [0]  <unk>                 0.534   3.247  \n",
            "   [1]  <unk>                 0.515   0.000  \n",
            "   [0]  b.                    0.495   10.021 \n",
            "   [1]  <unk>                 0.481   0.000  \n",
            "   [1]  and                   0.477   0.000  \n",
            "   [0]  watch                 0.479   9.496  \n",
            "   [0]  bugs                  0.486   11.879 \n",
            "   [0]  bunny                 0.498   11.941 \n",
            "   [1]  reruns                0.514   0.000  \n",
            "   [0]  <eos>                 0.533   3.149  \n",
            "   [0]  they                  0.550   4.716  \n",
            "   [0]  read                  0.561   8.645  \n",
            "   [1]  <unk>                 0.566   0.000  \n",
            "   [1]  <unk>                 0.566   0.000  \n",
            "   [1]  and                   0.564   0.000  \n",
            "   [0]  talk                  0.557   8.117  \n",
            "   [0]  about                 0.549   5.934  \n",
            "   [0]  <unk>                 0.541   3.630  \n",
            "   [0]  and                   0.537   3.649  \n",
            "   [1]  <unk>                 0.534   0.000  \n",
            " Sample 1.\n",
            "   [1]  say                   0.577   0.000  \n",
            "   [0]  that                  0.576   3.960  \n",
            "   [0]  program               0.578   6.684  \n",
            "   [0]  trading               0.576   6.315  \n",
            "   [1]  is                    0.574   0.000  \n",
            "   [0]  n't                   0.569   4.591  \n",
            "   [1]  the                   0.568   0.000  \n",
            "   [0]  main                  0.567   6.823  \n",
            "   [0]  reason                0.563   7.461  \n",
            "   [1]  for                   0.559   0.000  \n",
            "   [1]  stock-market          0.556   0.000  \n",
            "   [1]  gyrations             0.555   0.000  \n",
            "   [1]  <eos>                 0.556   0.000  \n",
            "   [0]  i                     0.558   4.380  \n",
            "   [1]  have                  0.559   0.000  \n",
            "   [0]  not                   0.557   6.302  \n",
            "   [0]  seen                  0.554   8.293  \n",
            "   [1]  one                   0.552   0.000  \n",
            "   [0]  <unk>                 0.554   3.477  \n",
            "   [0]  of                    0.560   3.352  \n",
            " Sample 2.\n",
            "   [1]  new                   0.502   0.000  \n",
            "   [0]  york                  0.508   3.704  \n",
            "   [1]  stock                 0.518   0.000  \n",
            "   [1]  exchange              0.525   0.000  \n",
            "   [1]  shaw                  0.534   0.000  \n",
            "   [0]  's                    0.536   3.793  \n",
            "   [1]  shares                0.531   0.000  \n",
            "   [0]  closed                0.525   7.902  \n",
            "   [0]  ex-dividend           0.517   13.136 \n",
            "   [0]  at                    0.510   4.563  \n",
            "   [1]  $                     0.502   0.000  \n",
            "   [0]  N                     0.498   0.160  \n",
            "   [1]  up                    0.501   0.000  \n",
            "   [1]  $                     0.499   0.000  \n",
            "   [0]  N                     0.497   0.219  \n",
            "   [1]  <eos>                 0.497   0.000  \n",
            "   [1]  armstrong             0.500   0.000  \n",
            "   [0]  's                    0.500   4.058  \n",
            "   [1]  shares                0.496   0.000  \n",
            "   [1]  also                  0.498   0.000  \n",
            "Samples\n",
            "Sample 0 .  <unk> <unk> b. <unk> and watch bugs bunny reruns <eos> they read <unk> <unk> and talk about <unk> and <unk>\n",
            "Sample 1 .  say that program trading is n't the main reason for stock-market gyrations <eos> i have not seen one <unk> of\n",
            "Sample 2 .  new york stock exchange shaw 's shares closed ex-dividend at $ N up $ N <eos> armstrong 's shares also\n",
            "\n",
            "\n",
            "targets[[203 2 11 31 140 4982 201 4 0 1 4 39 6884 3502 205 433 7 9320 10 203][2 44 0 129 146 9 23 2189 932 13 1221 55 156 77 2 0 313 3 1227 24][277 109 8 510 126 1 17 0 60 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[13 203 2 11 31 140 4982 201 4 0...]...][[0 1 1 0 0 0 1 0 1 1...]...][[13 10000 2 11 10000 10000 10000 201 10000 0...]...][[203 2 11 31 140 4982 201 4 0 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[13 203 2 11 31 140 4982 201 4 0...]...][[0 1 1 0 0 0 1 0 1 1...]...][[13 10000 2 11 10000 10000 10000 201 10000 0...]...][[203 2 11 31 140 4982 201 4 0 1...]...]\n",
            "I0212 04:14:28.724376 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[2 38 2020 43 51 6798 4465 306 121 8 2964 148 2774 1 8 1 1699 2 39 13][1042 5 414 1534 16 156 77 44 6 1 596 571 2035 1 5244 31 1283 16 0 967][1615 16 0 129 146 248 18 12 3 52...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 2 38 2020 43 51 6798 4465 306 121...]...][[0 0 1 1 0 1 0 0 1 0...]...][[1 10000 10000 2020 43 10000 6798 10000 10000 121...]...][[2 38 2020 43 51 6798 4465 306 121 8...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 2 38 2020 43 51 6798 4465 306 121...]...][[0 0 1 1 0 1 0 0 1 0...]...][[1 10000 10000 2020 43 10000 6798 10000 10000 121...]...][[2 38 2020 43 51 6798 4465 306 121 8...]...]\n",
            "targets[[2 38 2020 43 51 6798 4465 306 121 8 2964 148 2774 1 8 1 1699 2 39 13][1042 5 414 1534 16 156 77 44 6 1 596 571 2035 1 5244 31 1283 16 0 967][1615 16 0 129 146 248 18 12 3 52...]...]\n",
            "targets[[2 38 2020 43 51 6798 4465 306 121 8 2964 148 2774 1 8 1 1699 2 39 13][1042 5 414 1534 16 156 77 44 6 1 596 571 2035 1 5244 31 1283 16 0 967][1615 16 0 129 146 248 18 12 3 52...]...]\n",
            "targets[[2 38 2020 43 51 6798 4465 306 121 8 2964 148 2774 1 8 1 1699 2 39 13][1042 5 414 1534 16 156 77 44 6 1 596 571 2035 1 5244 31 1283 16 0 967][1615 16 0 129 146 248 18 12 3 52...]...]\n",
            "global_step: 1428\n",
            " perplexity: 692.676\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[2 38 2020 43 51 6798 4465 306 121 8 2964 148 2774 1 8 1 1699 2 39 13][1042 5 414 1534 16 156 77 44 6 1 596 571 2035 1 5244 31 1283 16 0 967][1615 16 0 129 146 248 18 12 3 52...]...]\n",
            " percent of 3-grams captured: 0.142.\n",
            "targets[[2 38 2020 43 51 6798 4465 306 121 8 2964 148 2774 1 8 1 1699 2 39 13][1042 5 414 1534 16 156 77 44 6 1 596 571 2035 1 5244 31 1283 16 0 967][1615 16 0 129 146 248 18 12 3 52...]...]\n",
            " percent of 2-grams captured: 0.432.\n",
            "targets[[2 38 2020 43 51 6798 4465 306 121 8 2964 148 2774 1 8 1 1699 2 39 13][1042 5 414 1534 16 156 77 44 6 1 596 571 2035 1 5244 31 1283 16 0 967][1615 16 0 129 146 248 18 12 3 52...]...]\n",
            " percent of 4-grams captured: 0.047.\n",
            " geometric_avg: 0.142.\n",
            " arithmetic_avg: 0.207.\n",
            "global_step: 1428\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69621\n",
            " G train loss: 3.30051\n",
            "targets[[2 38 2020 43 51 6798 4465 306 121 8 2964 148 2774 1 8 1 1699 2 39 13][1042 5 414 1534 16 156 77 44 6 1 596 571 2035 1 5244 31 1283 16 0 967][1615 16 0 129 146 248 18 12 3 52...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 2 38 2020 43 51 6798 4465 306 121...]...][[0 0 1 1 0 1 0 0 1 0...]...][[1 10000 10000 2020 43 10000 6798 10000 10000 121...]...][[2 38 2020 43 51 6798 4465 306 121 8...]...]\n",
            " Sample 0.\n",
            "   [0]  <eos>                 0.508   4.109  \n",
            "   [0]  they                  0.506   5.283  \n",
            "   [1]  worry                 0.503   0.000  \n",
            "   [1]  about                 0.502   0.000  \n",
            "   [0]  their                 0.506   4.700  \n",
            "   [1]  careers               0.512   0.000  \n",
            "   [0]  drink                 0.518   12.160 \n",
            "   [0]  too                   0.523   8.897  \n",
            "   [1]  much                  0.526   0.000  \n",
            "   [0]  and                   0.535   3.221  \n",
            "   [1]  suffer                0.543   0.000  \n",
            "   [1]  through               0.541   0.000  \n",
            "   [1]  broken                0.539   0.000  \n",
            "   [0]  <unk>                 0.538   2.849  \n",
            "   [0]  and                   0.542   3.330  \n",
            "   [0]  <unk>                 0.548   2.235  \n",
            "   [1]  affairs               0.551   0.000  \n",
            "   [1]  <eos>                 0.553   0.000  \n",
            "   [1]  this                  0.549   0.000  \n",
            "   [0]  is                    0.542   5.791  \n",
            " Sample 1.\n",
            "   [1]  evidence              0.503   0.000  \n",
            "   [0]  to                    0.473   2.967  \n",
            "   [0]  support               0.446   7.935  \n",
            "   [1]  restrictions          0.421   0.000  \n",
            "   [0]  on                    0.401   4.496  \n",
            "   [0]  program               0.389   6.528  \n",
            "   [1]  trading               0.382   0.000  \n",
            "   [0]  says                  0.378   5.870  \n",
            "   [1]  a                     0.374   0.000  \n",
            "   [0]  <unk>                 0.371   2.742  \n",
            "   [0]  university            0.364   8.781  \n",
            "   [1]  finance               0.355   0.000  \n",
            "   [1]  professor             0.346   0.000  \n",
            "   [1]  <unk>                 0.343   0.000  \n",
            "   [1]  stoll                 0.343   0.000  \n",
            "   [0]  an                    0.343   5.823  \n",
            "   [0]  authority             0.345   9.316  \n",
            "   [0]  on                    0.347   4.763  \n",
            "   [0]  the                   0.348   2.306  \n",
            "   [1]  subject               0.347   0.000  \n",
            " Sample 2.\n",
            "   [1]  listed                0.509   0.000  \n",
            "   [0]  on                    0.478   4.135  \n",
            "   [0]  the                   0.458   1.393  \n",
            "   [0]  big                   0.449   6.206  \n",
            "   [1]  board                 0.446   0.000  \n",
            "   [1]  closed                0.451   0.000  \n",
            "   [1]  at                    0.458   0.000  \n",
            "   [0]  $                     0.463   2.918  \n",
            "   [1]  N                     0.470   0.000  \n",
            "   [0]  up                    0.478   7.271  \n",
            "   [0]  N                     0.484   2.523  \n",
            "   [1]  cents                 0.485   0.000  \n",
            "   [1]  <eos>                 0.481   0.000  \n",
            "   [0]  yesterday             0.475   7.861  \n",
            "   [0]  armstrong             0.470   12.324 \n",
            "   [0]  reported              0.465   7.266  \n",
            "   [1]  flat                  0.457   0.000  \n",
            "   [0]  earnings              0.449   7.728  \n",
            "   [0]  for                   0.444   3.873  \n",
            "   [1]  the                   0.442   0.000  \n",
            "Samples\n",
            "Sample 0 .  <eos> they worry about their careers drink too much and suffer through broken <unk> and <unk> affairs <eos> this is\n",
            "Sample 1 .  evidence to support restrictions on program trading says a <unk> university finance professor <unk> stoll an authority on the subject\n",
            "Sample 2 .  listed on the big board closed at $ N up N cents <eos> yesterday armstrong reported flat earnings for the\n",
            "\n",
            "\n",
            "targets[[13 32 0 1 1437 4 5350 140 1 2 14 9 59 1 5 1336 6 173 3061 56][2363 63 17 156 280 29 17 309 4 0 1 4 0 12 3 48 588 4 408 584][2 11 0 109 136 46 12 3 21 36...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[203 13 32 0 1 1437 4 5350 140 1...]...][[1 0 0 1 0 0 1 0 1 1...]...][[203 13 10000 10000 1 10000 10000 5350 10000 1...]...][[13 32 0 1 1437 4 5350 140 1 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[203 13 32 0 1 1437 4 5350 140 1...]...][[1 0 0 1 0 0 1 0 1 1...]...][[203 13 10000 10000 1 10000 10000 5350 10000 1...]...][[13 32 0 1 1437 4 5350 140 1 2...]...]\n",
            "I0212 04:14:32.028673 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[203 2 11 31 140 4982 201 4 0 1 4 39 6884 3502 205 433 7 9320 10 203][2 44 0 129 146 9 23 2189 932 13 1221 55 156 77 2 0 313 3 1227 24][277 109 8 510 126 1 17 0 60 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[13 203 2 11 31 140 4982 201 4 0...]...][[1 0 1 1 0 0 1 1 1 1...]...][[13 203 10000 11 31 10000 10000 201 4 0...]...][[203 2 11 31 140 4982 201 4 0 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[13 203 2 11 31 140 4982 201 4 0...]...][[1 0 1 1 0 0 1 1 1 1...]...][[13 203 10000 11 31 10000 10000 201 4 0...]...][[203 2 11 31 140 4982 201 4 0 1...]...]\n",
            "targets[[203 2 11 31 140 4982 201 4 0 1 4 39 6884 3502 205 433 7 9320 10 203][2 44 0 129 146 9 23 2189 932 13 1221 55 156 77 2 0 313 3 1227 24][277 109 8 510 126 1 17 0 60 1...]...]\n",
            "targets[[203 2 11 31 140 4982 201 4 0 1 4 39 6884 3502 205 433 7 9320 10 203][2 44 0 129 146 9 23 2189 932 13 1221 55 156 77 2 0 313 3 1227 24][277 109 8 510 126 1 17 0 60 1...]...]\n",
            "targets[[203 2 11 31 140 4982 201 4 0 1 4 39 6884 3502 205 433 7 9320 10 203][2 44 0 129 146 9 23 2189 932 13 1221 55 156 77 2 0 313 3 1227 24][277 109 8 510 126 1 17 0 60 1...]...]\n",
            "global_step: 1431\n",
            " perplexity: 692.763\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[203 2 11 31 140 4982 201 4 0 1 4 39 6884 3502 205 433 7 9320 10 203][2 44 0 129 146 9 23 2189 932 13 1221 55 156 77 2 0 313 3 1227 24][277 109 8 510 126 1 17 0 60 1...]...]\n",
            " percent of 3-grams captured: 0.172.\n",
            "targets[[203 2 11 31 140 4982 201 4 0 1 4 39 6884 3502 205 433 7 9320 10 203][2 44 0 129 146 9 23 2189 932 13 1221 55 156 77 2 0 313 3 1227 24][277 109 8 510 126 1 17 0 60 1...]...]\n",
            " percent of 2-grams captured: 0.492.\n",
            "targets[[203 2 11 31 140 4982 201 4 0 1 4 39 6884 3502 205 433 7 9320 10 203][2 44 0 129 146 9 23 2189 932 13 1221 55 156 77 2 0 313 3 1227 24][277 109 8 510 126 1 17 0 60 1...]...]\n",
            " percent of 4-grams captured: 0.041.\n",
            " geometric_avg: 0.152.\n",
            " arithmetic_avg: 0.235.\n",
            "global_step: 1431\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69619\n",
            " G train loss: 3.30070\n",
            "targets[[203 2 11 31 140 4982 201 4 0 1 4 39 6884 3502 205 433 7 9320 10 203][2 44 0 129 146 9 23 2189 932 13 1221 55 156 77 2 0 313 3 1227 24][277 109 8 510 126 1 17 0 60 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[13 203 2 11 31 140 4982 201 4 0...]...][[1 0 1 1 0 0 1 1 1 1...]...][[13 203 10000 11 31 10000 10000 201 4 0...]...][[203 2 11 31 140 4982 201 4 0 1...]...]\n",
            " Sample 0.\n",
            "   [1]  japan                 0.562   0.000  \n",
            "   [0]  <eos>                 0.529   2.739  \n",
            "   [1]  for                   0.507   0.000  \n",
            "   [1]  an                    0.491   0.000  \n",
            "   [0]  american              0.478   7.173  \n",
            "   [0]  reader                0.465   9.620  \n",
            "   [1]  part                  0.460   0.000  \n",
            "   [1]  of                    0.460   0.000  \n",
            "   [1]  the                   0.466   0.000  \n",
            "   [1]  <unk>                 0.475   0.000  \n",
            "   [0]  of                    0.483   4.004  \n",
            "   [0]  this                  0.489   5.134  \n",
            "   [1]  engaging              0.495   0.000  \n",
            "   [0]  novel                 0.498   11.871 \n",
            "   [0]  should                0.498   7.892  \n",
            "   [0]  come                  0.504   8.179  \n",
            "   [0]  in                    0.512   3.428  \n",
            "   [0]  recognizing           0.521   11.168 \n",
            "   [0]  that                  0.529   4.636  \n",
            "   [1]  japan                 0.535   0.000  \n",
            " Sample 1.\n",
            "   [1]  <eos>                 0.560   0.000  \n",
            "   [1]  says                  0.543   0.000  \n",
            "   [0]  the                   0.528   1.806  \n",
            "   [0]  big                   0.518   5.948  \n",
            "   [1]  board                 0.513   0.000  \n",
            "   [1]  's                    0.507   0.000  \n",
            "   [1]  mr.                   0.508   0.000  \n",
            "   [0]  phelan                0.512   10.072 \n",
            "   [0]  volatility            0.515   11.687 \n",
            "   [1]  is                    0.516   0.000  \n",
            "   [1]  greater               0.519   0.000  \n",
            "   [1]  than                  0.520   0.000  \n",
            "   [0]  program               0.525   6.446  \n",
            "   [1]  trading               0.532   0.000  \n",
            "   [0]  <eos>                 0.536   2.031  \n",
            "   [0]  the                   0.535   1.356  \n",
            "   [1]  oct.                  0.533   0.000  \n",
            "   [0]  N                     0.532   0.422  \n",
            "   [1]  plunge                0.530   0.000  \n",
            "   [1]  was                   0.527   0.000  \n",
            " Sample 2.\n",
            "   [1]  third                 0.496   0.000  \n",
            "   [1]  quarter               0.489   0.000  \n",
            "   [1]  and                   0.485   0.000  \n",
            "   [1]  nine                  0.483   0.000  \n",
            "   [0]  months                0.482   6.288  \n",
            "   [0]  <unk>                 0.480   4.704  \n",
            "   [0]  by                    0.475   4.597  \n",
            "   [1]  the                   0.467   0.000  \n",
            "   [0]  stock                 0.460   5.010  \n",
            "   [0]  <unk>                 0.450   3.747  \n",
            "   [0]  of                    0.438   3.270  \n",
            "   [1]  an                    0.423   0.000  \n",
            "   [1]  employee              0.410   0.000  \n",
            "   [0]  stock                 0.401   5.768  \n",
            "   [1]  ownership             0.390   0.000  \n",
            "   [0]  plan                  0.382   7.149  \n",
            "   [1]  adopted               0.378   0.000  \n",
            "   [1]  earlier               0.375   0.000  \n",
            "   [1]  this                  0.375   0.000  \n",
            "   [0]  year                  0.375   5.455  \n",
            "Samples\n",
            "Sample 0 .  japan <eos> for an american reader part of the <unk> of this engaging novel should come in recognizing that japan\n",
            "Sample 1 .  <eos> says the big board 's mr. phelan volatility is greater than program trading <eos> the oct. N plunge was\n",
            "Sample 2 .  third quarter and nine months <unk> by the stock <unk> of an employee stock ownership plan adopted earlier this year\n",
            "\n",
            "\n",
            "targets[[1506 175 32 5913 5 0 1 1 722 4 4604 56 1 0 3862 4 0 6697 173 1][2 2283 5 9539 408 8 61 763 71 7757 3228 36 645 5903 3820 538 2719 60 38 50][3 21 2 7 0 1522 109 136 46 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1506 175 32 5913 5 0 1 1 722...]...][[0 0 0 0 1 1 0 1 1 0...]...][[56 10000 10000 10000 10000 5 0 10000 1 722...]...][[1506 175 32 5913 5 0 1 1 722 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1506 175 32 5913 5 0 1 1 722...]...][[0 0 0 0 1 1 0 1 1 0...]...][[56 10000 10000 10000 10000 5 0 10000 1 722...]...][[1506 175 32 5913 5 0 1 1 722 4...]...]\n",
            "I0212 04:14:35.331404 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[13 32 0 1 1437 4 5350 140 1 2 14 9 59 1 5 1336 6 173 3061 56][2363 63 17 156 280 29 17 309 4 0 1 4 0 12 3 48 588 4 408 584][2 11 0 109 136 46 12 3 21 36...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[203 13 32 0 1 1437 4 5350 140 1...]...][[0 0 0 1 0 0 1 0 0 0...]...][[203 10000 10000 10000 1 10000 10000 5350 10000 10000...]...][[13 32 0 1 1437 4 5350 140 1 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[203 13 32 0 1 1437 4 5350 140 1...]...][[0 0 0 1 0 0 1 0 0 0...]...][[203 10000 10000 10000 1 10000 10000 5350 10000 10000...]...][[13 32 0 1 1437 4 5350 140 1 2...]...]\n",
            "targets[[13 32 0 1 1437 4 5350 140 1 2 14 9 59 1 5 1336 6 173 3061 56][2363 63 17 156 280 29 17 309 4 0 1 4 0 12 3 48 588 4 408 584][2 11 0 109 136 46 12 3 21 36...]...]\n",
            "targets[[13 32 0 1 1437 4 5350 140 1 2 14 9 59 1 5 1336 6 173 3061 56][2363 63 17 156 280 29 17 309 4 0 1 4 0 12 3 48 588 4 408 584][2 11 0 109 136 46 12 3 21 36...]...]\n",
            "targets[[13 32 0 1 1437 4 5350 140 1 2 14 9 59 1 5 1336 6 173 3061 56][2363 63 17 156 280 29 17 309 4 0 1 4 0 12 3 48 588 4 408 584][2 11 0 109 136 46 12 3 21 36...]...]\n",
            "global_step: 1434\n",
            " perplexity: 692.548\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[13 32 0 1 1437 4 5350 140 1 2 14 9 59 1 5 1336 6 173 3061 56][2363 63 17 156 280 29 17 309 4 0 1 4 0 12 3 48 588 4 408 584][2 11 0 109 136 46 12 3 21 36...]...]\n",
            " percent of 3-grams captured: 0.192.\n",
            "targets[[13 32 0 1 1437 4 5350 140 1 2 14 9 59 1 5 1336 6 173 3061 56][2363 63 17 156 280 29 17 309 4 0 1 4 0 12 3 48 588 4 408 584][2 11 0 109 136 46 12 3 21 36...]...]\n",
            " percent of 2-grams captured: 0.453.\n",
            "targets[[13 32 0 1 1437 4 5350 140 1 2 14 9 59 1 5 1336 6 173 3061 56][2363 63 17 156 280 29 17 309 4 0 1 4 0 12 3 48 588 4 408 584][2 11 0 109 136 46 12 3 21 36...]...]\n",
            " percent of 4-grams captured: 0.074.\n",
            " geometric_avg: 0.185.\n",
            " arithmetic_avg: 0.239.\n",
            "global_step: 1434\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69624\n",
            " G train loss: 3.30073\n",
            "targets[[13 32 0 1 1437 4 5350 140 1 2 14 9 59 1 5 1336 6 173 3061 56][2363 63 17 156 280 29 17 309 4 0 1 4 0 12 3 48 588 4 408 584][2 11 0 109 136 46 12 3 21 36...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[203 13 32 0 1 1437 4 5350 140 1...]...][[0 0 0 1 0 0 1 0 0 0...]...][[203 10000 10000 10000 1 10000 10000 5350 10000 10000...]...][[13 32 0 1 1437 4 5350 140 1 2...]...]\n",
            " Sample 0.\n",
            "   [0]  is                    0.604   4.346  \n",
            "   [0]  n't                   0.600   4.948  \n",
            "   [0]  the                   0.603   2.958  \n",
            "   [1]  <unk>                 0.605   0.000  \n",
            "   [0]  society               0.610   10.484 \n",
            "   [0]  of                    0.614   2.426  \n",
            "   [1]  contemporary          0.612   0.000  \n",
            "   [0]  american              0.605   7.178  \n",
            "   [0]  <unk>                 0.600   2.651  \n",
            "   [0]  <eos>                 0.593   2.243  \n",
            "   [1]  it                    0.591   0.000  \n",
            "   [1]  's                    0.589   0.000  \n",
            "   [1]  also                  0.585   0.000  \n",
            "   [1]  <unk>                 0.583   0.000  \n",
            "   [1]  to                    0.584   0.000  \n",
            "   [1]  read                  0.584   0.000  \n",
            "   [1]  a                     0.584   0.000  \n",
            "   [0]  japanese              0.577   6.905  \n",
            "   [1]  author                0.572   0.000  \n",
            "   [1]  who                   0.572   0.000  \n",
            " Sample 1.\n",
            "   [1]  triggered             0.560   0.000  \n",
            "   [0]  not                   0.536   6.460  \n",
            "   [0]  by                    0.515   4.658  \n",
            "   [0]  program               0.501   6.318  \n",
            "   [0]  traders               0.490   7.484  \n",
            "   [1]  but                   0.481   0.000  \n",
            "   [1]  by                    0.476   0.000  \n",
            "   [1]  news                  0.474   0.000  \n",
            "   [1]  of                    0.474   0.000  \n",
            "   [1]  the                   0.473   0.000  \n",
            "   [1]  <unk>                 0.473   0.000  \n",
            "   [1]  of                    0.470   0.000  \n",
            "   [0]  the                   0.466   1.919  \n",
            "   [1]  $                     0.458   0.000  \n",
            "   [0]  N                     0.451   0.201  \n",
            "   [0]  billion               0.441   2.354  \n",
            "   [1]  buy-out               0.434   0.000  \n",
            "   [0]  of                    0.429   3.098  \n",
            "   [0]  ual                   0.427   7.212  \n",
            "   [1]  corp                  0.429   0.000  \n",
            " Sample 2.\n",
            "   [0]  <eos>                 0.503   2.435  \n",
            "   [1]  for                   0.487   0.000  \n",
            "   [1]  the                   0.475   0.000  \n",
            "   [1]  quarter               0.466   0.000  \n",
            "   [0]  earnings              0.454   7.190  \n",
            "   [1]  were                  0.448   0.000  \n",
            "   [0]  $                     0.444   4.439  \n",
            "   [1]  N                     0.441   0.000  \n",
            "   [0]  million               0.444   0.639  \n",
            "   [0]  or                    0.449   4.443  \n",
            "   [0]  N                     0.450   1.343  \n",
            "   [0]  cents                 0.450   4.613  \n",
            "   [1]  a                     0.447   0.000  \n",
            "   [0]  share                 0.439   3.377  \n",
            "   [1]  including             0.430   0.000  \n",
            "   [1]  a                     0.422   0.000  \n",
            "   [1]  one-time              0.417   0.000  \n",
            "   [0]  gain                  0.415   7.864  \n",
            "   [0]  of                    0.415   1.711  \n",
            "   [1]  $                     0.415   0.000  \n",
            "Samples\n",
            "Sample 0 .  is n't the <unk> society of contemporary american <unk> <eos> it 's also <unk> to read a japanese author who\n",
            "Sample 1 .  triggered not by program traders but by news of the <unk> of the $ N billion buy-out of ual corp\n",
            "Sample 2 .  <eos> for the quarter earnings were $ N million or N cents a share including a one-time gain of $\n",
            "\n",
            "\n",
            "targets[[17 5758 2 66 6 2978 9401 2082 2430 31 8148 2620 11 197 1559 14 9 10 0 173][2 120 156 1247 5848 6493 7 6 2162 6785 10 4124 77 7 60 288 7 482 159 57][411 31 331 5 184 27 300 2469 2654 417...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 17 5758 2 66 6 2978 9401 2082 2430...]...][[0 1 0 1 0 0 1 0 0 1...]...][[1 10000 5758 10000 66 10000 10000 9401 10000 10000...]...][[17 5758 2 66 6 2978 9401 2082 2430 31...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 17 5758 2 66 6 2978 9401 2082 2430...]...][[0 1 0 1 0 0 1 0 0 1...]...][[1 10000 5758 10000 66 10000 10000 9401 10000 10000...]...][[17 5758 2 66 6 2978 9401 2082 2430 31...]...]\n",
            "I0212 04:14:38.644015 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1506 175 32 5913 5 0 1 1 722 4 4604 56 1 0 3862 4 0 6697 173 1][2 2283 5 9539 408 8 61 763 71 7757 3228 36 645 5903 3820 538 2719 60 38 50][3 21 2 7 0 1522 109 136 46 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1506 175 32 5913 5 0 1 1 722...]...][[0 1 1 0 1 1 0 0 0 0...]...][[56 10000 175 32 10000 5 0 10000 10000 10000...]...][[1506 175 32 5913 5 0 1 1 722 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1506 175 32 5913 5 0 1 1 722...]...][[0 1 1 0 1 1 0 0 0 0...]...][[56 10000 175 32 10000 5 0 10000 10000 10000...]...][[1506 175 32 5913 5 0 1 1 722 4...]...]\n",
            "targets[[1506 175 32 5913 5 0 1 1 722 4 4604 56 1 0 3862 4 0 6697 173 1][2 2283 5 9539 408 8 61 763 71 7757 3228 36 645 5903 3820 538 2719 60 38 50][3 21 2 7 0 1522 109 136 46 12...]...]\n",
            "targets[[1506 175 32 5913 5 0 1 1 722 4 4604 56 1 0 3862 4 0 6697 173 1][2 2283 5 9539 408 8 61 763 71 7757 3228 36 645 5903 3820 538 2719 60 38 50][3 21 2 7 0 1522 109 136 46 12...]...]\n",
            "targets[[1506 175 32 5913 5 0 1 1 722 4 4604 56 1 0 3862 4 0 6697 173 1][2 2283 5 9539 408 8 61 763 71 7757 3228 36 645 5903 3820 538 2719 60 38 50][3 21 2 7 0 1522 109 136 46 12...]...]\n",
            "global_step: 1437\n",
            " perplexity: 691.915\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1506 175 32 5913 5 0 1 1 722 4 4604 56 1 0 3862 4 0 6697 173 1][2 2283 5 9539 408 8 61 763 71 7757 3228 36 645 5903 3820 538 2719 60 38 50][3 21 2 7 0 1522 109 136 46 12...]...]\n",
            " percent of 3-grams captured: 0.175.\n",
            "targets[[1506 175 32 5913 5 0 1 1 722 4 4604 56 1 0 3862 4 0 6697 173 1][2 2283 5 9539 408 8 61 763 71 7757 3228 36 645 5903 3820 538 2719 60 38 50][3 21 2 7 0 1522 109 136 46 12...]...]\n",
            " percent of 2-grams captured: 0.453.\n",
            "targets[[1506 175 32 5913 5 0 1 1 722 4 4604 56 1 0 3862 4 0 6697 173 1][2 2283 5 9539 408 8 61 763 71 7757 3228 36 645 5903 3820 538 2719 60 38 50][3 21 2 7 0 1522 109 136 46 12...]...]\n",
            " percent of 4-grams captured: 0.059.\n",
            " geometric_avg: 0.167.\n",
            " arithmetic_avg: 0.229.\n",
            "global_step: 1437\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69624\n",
            " G train loss: 3.29996\n",
            "targets[[1506 175 32 5913 5 0 1 1 722 4 4604 56 1 0 3862 4 0 6697 173 1][2 2283 5 9539 408 8 61 763 71 7757 3228 36 645 5903 3820 538 2719 60 38 50][3 21 2 7 0 1522 109 136 46 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1506 175 32 5913 5 0 1 1 722...]...][[0 1 1 0 1 1 0 0 0 0...]...][[56 10000 175 32 10000 5 0 10000 10000 10000...]...][[1506 175 32 5913 5 0 1 1 722 4...]...]\n",
            " Sample 0.\n",
            "   [0]  clearly               0.542   9.380  \n",
            "   [1]  does                  0.545   0.000  \n",
            "   [1]  n't                   0.550   0.000  \n",
            "   [0]  belong                0.554   11.782 \n",
            "   [1]  to                    0.556   0.000  \n",
            "   [1]  the                   0.554   0.000  \n",
            "   [0]  <unk>                 0.549   2.641  \n",
            "   [0]  <unk>                 0.544   3.980  \n",
            "   [0]  school                0.540   10.014 \n",
            "   [0]  of                    0.534   2.696  \n",
            "   [0]  writers               0.528   11.878 \n",
            "   [1]  who                   0.527   0.000  \n",
            "   [1]  <unk>                 0.526   0.000  \n",
            "   [1]  the                   0.526   0.000  \n",
            "   [0]  notion                0.524   10.628 \n",
            "   [1]  of                    0.523   0.000  \n",
            "   [1]  the                   0.520   0.000  \n",
            "   [1]  unique                0.515   0.000  \n",
            "   [1]  japanese              0.510   0.000  \n",
            "   [1]  <unk>                 0.507   0.000  \n",
            " Sample 1.\n",
            "   [0]  <eos>                 0.570   2.132  \n",
            "   [0]  unable                0.568   10.342 \n",
            "   [1]  to                    0.570   0.000  \n",
            "   [1]  unload                0.569   0.000  \n",
            "   [1]  ual                   0.566   0.000  \n",
            "   [1]  and                   0.563   0.000  \n",
            "   [0]  other                 0.558   6.431  \n",
            "   [1]  airline               0.550   0.000  \n",
            "   [0]  shares                0.544   7.019  \n",
            "   [1]  takeover-stock        0.539   0.000  \n",
            "   [1]  speculators           0.534   0.000  \n",
            "   [0]  or                    0.533   5.687  \n",
            "   [1]  risk                  0.534   0.000  \n",
            "   [1]  arbitragers           0.535   0.000  \n",
            "   [0]  dumped                0.538   10.924 \n",
            "   [1]  every                 0.540   0.000  \n",
            "   [1]  blue-chip             0.538   0.000  \n",
            "   [0]  stock                 0.538   6.719  \n",
            "   [1]  they                  0.539   0.000  \n",
            "   [1]  had                   0.539   0.000  \n",
            " Sample 2.\n",
            "   [1]  N                     0.479   0.000  \n",
            "   [1]  million               0.472   0.000  \n",
            "   [1]  <eos>                 0.461   0.000  \n",
            "   [0]  in                    0.448   4.586  \n",
            "   [0]  the                   0.435   2.811  \n",
            "   [0]  year-ago              0.424   8.020  \n",
            "   [1]  quarter               0.411   0.000  \n",
            "   [0]  earnings              0.400   7.150  \n",
            "   [0]  were                  0.400   6.382  \n",
            "   [0]  $                     0.400   4.394  \n",
            "   [0]  N                     0.403   0.048  \n",
            "   [0]  million               0.406   1.408  \n",
            "   [0]  or                    0.408   5.157  \n",
            "   [0]  N                     0.409   2.594  \n",
            "   [1]  cents                 0.411   0.000  \n",
            "   [1]  a                     0.411   0.000  \n",
            "   [0]  share                 0.406   2.951  \n",
            "   [0]  <eos>                 0.401   3.651  \n",
            "   [1]  yesterday             0.396   0.000  \n",
            "   [0]  armstrong             0.392   10.381 \n",
            "Samples\n",
            "Sample 0 .  clearly does n't belong to the <unk> <unk> school of writers who <unk> the notion of the unique japanese <unk>\n",
            "Sample 1 .  <eos> unable to unload ual and other airline shares takeover-stock speculators or risk arbitragers dumped every blue-chip stock they had\n",
            "Sample 2 .  N million <eos> in the year-ago quarter earnings were $ N million or N cents a share <eos> yesterday armstrong\n",
            "\n",
            "\n",
            "targets[[26 45 166 505 55 90 4 505 316 2 10 9 63 5 117 10 0 1 5775 4][156 77 2488 2 6662 5363 1 638 1281 18 3429 145 265 95 44 1815 26 4132 0 799][1475 2 0 115 24 32 870 2 4423 405...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[173 26 45 166 505 55 90 4 505 316...]...][[0 0 1 1 1 0 1 0 0 0...]...][[173 10000 10000 166 505 55 10000 4 10000 10000...]...][[26 45 166 505 55 90 4 505 316 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[173 26 45 166 505 55 90 4 505 316...]...][[0 0 1 1 1 0 1 0 0 0...]...][[173 10000 10000 166 505 55 10000 4 10000 10000...]...][[26 45 166 505 55 90 4 505 316 2...]...]\n",
            "I0212 04:14:41.932320 139783917786880 supervisor.py:1117] Saving checkpoint to path maskGAN/train/model.ckpt\n",
            "I0212 04:14:42.141968 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[17 5758 2 66 6 2978 9401 2082 2430 31 8148 2620 11 197 1559 14 9 10 0 173][2 120 156 1247 5848 6493 7 6 2162 6785 10 4124 77 7 60 288 7 482 159 57][411 31 331 5 184 27 300 2469 2654 417...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 17 5758 2 66 6 2978 9401 2082 2430...]...][[1 1 1 1 1 0 1 1 0 0...]...][[1 17 5758 2 66 6 10000 9401 2082 10000...]...][[17 5758 2 66 6 2978 9401 2082 2430 31...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 17 5758 2 66 6 2978 9401 2082 2430...]...][[1 1 1 1 1 0 1 1 0 0...]...][[1 17 5758 2 66 6 10000 9401 2082 10000...]...][[17 5758 2 66 6 2978 9401 2082 2430 31...]...]\n",
            "targets[[17 5758 2 66 6 2978 9401 2082 2430 31 8148 2620 11 197 1559 14 9 10 0 173][2 120 156 1247 5848 6493 7 6 2162 6785 10 4124 77 7 60 288 7 482 159 57][411 31 331 5 184 27 300 2469 2654 417...]...]\n",
            "targets[[17 5758 2 66 6 2978 9401 2082 2430 31 8148 2620 11 197 1559 14 9 10 0 173][2 120 156 1247 5848 6493 7 6 2162 6785 10 4124 77 7 60 288 7 482 159 57][411 31 331 5 184 27 300 2469 2654 417...]...]\n",
            "targets[[17 5758 2 66 6 2978 9401 2082 2430 31 8148 2620 11 197 1559 14 9 10 0 173][2 120 156 1247 5848 6493 7 6 2162 6785 10 4124 77 7 60 288 7 482 159 57][411 31 331 5 184 27 300 2469 2654 417...]...]\n",
            "global_step: 1440\n",
            " perplexity: 691.350\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[17 5758 2 66 6 2978 9401 2082 2430 31 8148 2620 11 197 1559 14 9 10 0 173][2 120 156 1247 5848 6493 7 6 2162 6785 10 4124 77 7 60 288 7 482 159 57][411 31 331 5 184 27 300 2469 2654 417...]...]\n",
            " percent of 3-grams captured: 0.133.\n",
            "targets[[17 5758 2 66 6 2978 9401 2082 2430 31 8148 2620 11 197 1559 14 9 10 0 173][2 120 156 1247 5848 6493 7 6 2162 6785 10 4124 77 7 60 288 7 482 159 57][411 31 331 5 184 27 300 2469 2654 417...]...]\n",
            " percent of 2-grams captured: 0.437.\n",
            "targets[[17 5758 2 66 6 2978 9401 2082 2430 31 8148 2620 11 197 1559 14 9 10 0 173][2 120 156 1247 5848 6493 7 6 2162 6785 10 4124 77 7 60 288 7 482 159 57][411 31 331 5 184 27 300 2469 2654 417...]...]\n",
            " percent of 4-grams captured: 0.032.\n",
            " geometric_avg: 0.124.\n",
            " arithmetic_avg: 0.201.\n",
            "global_step: 1440\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69624\n",
            " G train loss: 3.29957\n",
            "targets[[17 5758 2 66 6 2978 9401 2082 2430 31 8148 2620 11 197 1559 14 9 10 0 173][2 120 156 1247 5848 6493 7 6 2162 6785 10 4124 77 7 60 288 7 482 159 57][411 31 331 5 184 27 300 2469 2654 417...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 17 5758 2 66 6 2978 9401 2082 2430...]...][[1 1 1 1 1 0 1 1 0 0...]...][[1 17 5758 2 66 6 10000 9401 2082 10000...]...][[17 5758 2 66 6 2978 9401 2082 2430 31...]...]\n",
            " Sample 0.\n",
            "   [1]  by                    0.544   0.000  \n",
            "   [1]  outsiders             0.530   0.000  \n",
            "   [1]  <eos>                 0.512   0.000  \n",
            "   [1]  if                    0.496   0.000  \n",
            "   [1]  a                     0.482   0.000  \n",
            "   [0]  wild                  0.466   9.191  \n",
            "   [1]  sheep                 0.452   0.000  \n",
            "   [1]  chase                 0.447   0.000  \n",
            "   [0]  carries               0.445   9.780  \n",
            "   [0]  an                    0.446   6.741  \n",
            "   [1]  implicit              0.449   0.000  \n",
            "   [0]  message               0.451   10.239 \n",
            "   [0]  for                   0.451   4.560  \n",
            "   [0]  international         0.454   7.354  \n",
            "   [1]  relations             0.456   0.000  \n",
            "   [0]  it                    0.463   5.852  \n",
            "   [0]  's                    0.467   3.722  \n",
            "   [0]  that                  0.467   5.291  \n",
            "   [1]  the                   0.466   0.000  \n",
            "   [0]  japanese              0.459   7.000  \n",
            " Sample 1.\n",
            "   [0]  <eos>                 0.510   4.437  \n",
            "   [1]  while                 0.501   0.000  \n",
            "   [0]  program               0.503   6.383  \n",
            "   [1]  trades                0.510   0.000  \n",
            "   [1]  swiftly               0.520   0.000  \n",
            "   [1]  kicked                0.528   0.000  \n",
            "   [1]  in                    0.536   0.000  \n",
            "   [1]  a                     0.541   0.000  \n",
            "   [1]  circuit               0.540   0.000  \n",
            "   [1]  breaker               0.536   0.000  \n",
            "   [0]  that                  0.530   3.796  \n",
            "   [1]  halted                0.523   0.000  \n",
            "   [1]  trading               0.516   0.000  \n",
            "   [1]  in                    0.512   0.000  \n",
            "   [0]  stock                 0.512   7.147  \n",
            "   [1]  futures               0.518   0.000  \n",
            "   [1]  in                    0.525   0.000  \n",
            "   [1]  chicago               0.527   0.000  \n",
            "   [0]  made                  0.529   6.833  \n",
            "   [0]  some                  0.529   6.236  \n",
            " Sample 2.\n",
            "   [1]  announced             0.519   0.000  \n",
            "   [0]  an                    0.506   4.892  \n",
            "   [1]  agreement             0.494   0.000  \n",
            "   [0]  to                    0.486   3.612  \n",
            "   [0]  sell                  0.483   6.512  \n",
            "   [1]  its                   0.482   0.000  \n",
            "   [1]  small                 0.484   0.000  \n",
            "   [1]  applied               0.485   0.000  \n",
            "   [1]  color                 0.482   0.000  \n",
            "   [0]  systems               0.484   7.284  \n",
            "   [0]  unit                  0.488   8.035  \n",
            "   [0]  to                    0.492   2.110  \n",
            "   [1]  a                     0.495   0.000  \n",
            "   [0]  subsidiary            0.495   7.630  \n",
            "   [0]  of                    0.492   2.590  \n",
            "   [1]  the                   0.491   0.000  \n",
            "   [0]  swiss                 0.491   9.631  \n",
            "   [0]  company               0.495   5.271  \n",
            "   [0]  <unk>                 0.496   4.554  \n",
            "   [0]  <unk>                 0.498   3.446  \n",
            "Samples\n",
            "Sample 0 .  by outsiders <eos> if a wild sheep chase carries an implicit message for international relations it 's that the japanese\n",
            "Sample 1 .  <eos> while program trades swiftly kicked in a circuit breaker that halted trading in stock futures in chicago made some\n",
            "Sample 2 .  announced an agreement to sell its small applied color systems unit to a subsidiary of the swiss company <unk> <unk>\n",
            "\n",
            "\n",
            "targets[[6 2978 9401 2082 13 9812 7 3747 2 14 9 1 8 516 5129 2 6 1 1 1][0 420 60 1 13 835 7 0 47 19 6 1072 4 932 2 97 1 26 1 163][1453 8 0 6340 226 7 923 22 0 552...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 6 2978 9401 2082 13 9812 7 3747 2...]...][[0 0 0 1 1 1 1 1 1 1...]...][[4 10000 10000 10000 2082 13 9812 7 3747 2...]...][[6 2978 9401 2082 13 9812 7 3747 2 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 6 2978 9401 2082 13 9812 7 3747 2...]...][[0 0 0 1 1 1 1 1 1 1...]...][[4 10000 10000 10000 2082 13 9812 7 3747 2...]...][[6 2978 9401 2082 13 9812 7 3747 2 14...]...]\n",
            "I0212 04:14:47.124077 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[26 45 166 505 55 90 4 505 316 2 10 9 63 5 117 10 0 1 5775 4][156 77 2488 2 6662 5363 1 638 1281 18 3429 145 265 95 44 1815 26 4132 0 799][1475 2 0 115 24 32 870 2 4423 405...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[173 26 45 166 505 55 90 4 505 316...]...][[0 0 1 0 0 0 1 1 1 0...]...][[173 10000 10000 166 10000 10000 10000 4 505 316...]...][[26 45 166 505 55 90 4 505 316 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[173 26 45 166 505 55 90 4 505 316...]...][[0 0 1 0 0 0 1 1 1 0...]...][[173 10000 10000 166 10000 10000 10000 4 505 316...]...][[26 45 166 505 55 90 4 505 316 2...]...]\n",
            "targets[[26 45 166 505 55 90 4 505 316 2 10 9 63 5 117 10 0 1 5775 4][156 77 2488 2 6662 5363 1 638 1281 18 3429 145 265 95 44 1815 26 4132 0 799][1475 2 0 115 24 32 870 2 4423 405...]...]\n",
            "targets[[26 45 166 505 55 90 4 505 316 2 10 9 63 5 117 10 0 1 5775 4][156 77 2488 2 6662 5363 1 638 1281 18 3429 145 265 95 44 1815 26 4132 0 799][1475 2 0 115 24 32 870 2 4423 405...]...]\n",
            "targets[[26 45 166 505 55 90 4 505 316 2 10 9 63 5 117 10 0 1 5775 4][156 77 2488 2 6662 5363 1 638 1281 18 3429 145 265 95 44 1815 26 4132 0 799][1475 2 0 115 24 32 870 2 4423 405...]...]\n",
            "global_step: 1443\n",
            " perplexity: 691.029\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[26 45 166 505 55 90 4 505 316 2 10 9 63 5 117 10 0 1 5775 4][156 77 2488 2 6662 5363 1 638 1281 18 3429 145 265 95 44 1815 26 4132 0 799][1475 2 0 115 24 32 870 2 4423 405...]...]\n",
            " percent of 3-grams captured: 0.186.\n",
            "targets[[26 45 166 505 55 90 4 505 316 2 10 9 63 5 117 10 0 1 5775 4][156 77 2488 2 6662 5363 1 638 1281 18 3429 145 265 95 44 1815 26 4132 0 799][1475 2 0 115 24 32 870 2 4423 405...]...]\n",
            " percent of 2-grams captured: 0.474.\n",
            "targets[[26 45 166 505 55 90 4 505 316 2 10 9 63 5 117 10 0 1 5775 4][156 77 2488 2 6662 5363 1 638 1281 18 3429 145 265 95 44 1815 26 4132 0 799][1475 2 0 115 24 32 870 2 4423 405...]...]\n",
            " percent of 4-grams captured: 0.059.\n",
            " geometric_avg: 0.173.\n",
            " arithmetic_avg: 0.240.\n",
            "global_step: 1443\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69621\n",
            " G train loss: 3.29962\n",
            "targets[[26 45 166 505 55 90 4 505 316 2 10 9 63 5 117 10 0 1 5775 4][156 77 2488 2 6662 5363 1 638 1281 18 3429 145 265 95 44 1815 26 4132 0 799][1475 2 0 115 24 32 870 2 4423 405...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[173 26 45 166 505 55 90 4 505 316...]...][[0 0 1 0 0 0 1 1 1 0...]...][[173 10000 10000 166 10000 10000 10000 4 505 316...]...][[26 45 166 505 55 90 4 505 316 2...]...]\n",
            " Sample 0.\n",
            "   [0]  are                   0.494   6.009  \n",
            "   [0]  more                  0.443   6.212  \n",
            "   [1]  like                  0.402   0.000  \n",
            "   [0]  us                    0.374   7.705  \n",
            "   [0]  than                  0.361   5.462  \n",
            "   [0]  most                  0.356   7.264  \n",
            "   [1]  of                    0.357   0.000  \n",
            "   [1]  us                    0.362   0.000  \n",
            "   [1]  think                 0.371   0.000  \n",
            "   [0]  <eos>                 0.375   3.047  \n",
            "   [1]  that                  0.378   0.000  \n",
            "   [0]  's                    0.377   5.150  \n",
            "   [1]  not                   0.371   0.000  \n",
            "   [0]  to                    0.365   4.620  \n",
            "   [0]  say                   0.361   7.628  \n",
            "   [0]  that                  0.358   4.110  \n",
            "   [0]  the                   0.356   2.500  \n",
            "   [0]  <unk>                 0.353   2.686  \n",
            "   [0]  plot                  0.351   12.337 \n",
            "   [0]  of                    0.351   3.169  \n",
            " Sample 1.\n",
            "   [0]  program               0.531   6.992  \n",
            "   [1]  trading               0.527   0.000  \n",
            "   [0]  impossible            0.524   10.557 \n",
            "   [0]  <eos>                 0.527   3.234  \n",
            "   [1]  susan                 0.533   0.000  \n",
            "   [1]  del                   0.541   0.000  \n",
            "   [0]  <unk>                 0.548   3.959  \n",
            "   [1]  head                  0.553   0.000  \n",
            "   [0]  trader                0.555   8.740  \n",
            "   [1]  at                    0.553   0.000  \n",
            "   [0]  travelers             0.547   10.778 \n",
            "   [1]  investment            0.543   0.000  \n",
            "   [1]  management            0.538   0.000  \n",
            "   [0]  co.                   0.534   6.209  \n",
            "   [1]  says                  0.532   0.000  \n",
            "   [1]  critics               0.530   0.000  \n",
            "   [1]  are                   0.524   0.000  \n",
            "   [0]  ignoring              0.519   10.726 \n",
            "   [1]  the                   0.517   0.000  \n",
            "   [1]  role                  0.514   0.000  \n",
            " Sample 2.\n",
            "   [0]  ltd                   0.558   9.641  \n",
            "   [1]  <eos>                 0.525   0.000  \n",
            "   [1]  the                   0.495   0.000  \n",
            "   [0]  price                 0.468   6.885  \n",
            "   [1]  was                   0.445   0.000  \n",
            "   [1]  n't                   0.428   0.000  \n",
            "   [1]  disclosed             0.422   0.000  \n",
            "   [0]  <eos>                 0.421   2.944  \n",
            "   [1]  armstrong             0.423   0.000  \n",
            "   [1]  expects               0.426   0.000  \n",
            "   [1]  to                    0.435   0.000  \n",
            "   [0]  close                 0.442   6.686  \n",
            "   [0]  the                   0.448   3.499  \n",
            "   [0]  sale                  0.451   5.663  \n",
            "   [0]  of                    0.451   2.158  \n",
            "   [1]  the                   0.452   0.000  \n",
            "   [0]  color                 0.446   12.359 \n",
            "   [1]  unit                  0.441   0.000  \n",
            "   [0]  in                    0.438   3.207  \n",
            "   [1]  late                  0.438   0.000  \n",
            "Samples\n",
            "Sample 0 .  are more like us than most of us think <eos> that 's not to say that the <unk> plot of\n",
            "Sample 1 .  program trading impossible <eos> susan del <unk> head trader at travelers investment management co. says critics are ignoring the role\n",
            "Sample 2 .  ltd <eos> the price was n't disclosed <eos> armstrong expects to close the sale of the color unit in late\n",
            "\n",
            "\n",
            "targets[[6941 2234 170 11 8422 357 7 3417 4 31 1 9401 22 6 1832 16 27 192 18 0][44 8 38 34 5 184 67 607 712 166 38 616 3148 2 166 1775 1492 16 330 321][0 98 9 1717 1 5132 119 6 3419 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 6941 2234 170 11 8422 357 7 3417 4...]...][[0 1 1 0 0 1 0 0 1 1...]...][[1 10000 2234 170 10000 10000 357 10000 10000 4...]...][[6941 2234 170 11 8422 357 7 3417 4 31...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 6941 2234 170 11 8422 357 7 3417 4...]...][[0 1 1 0 0 1 0 0 1 1...]...][[1 10000 2234 170 10000 10000 357 10000 10000 4...]...][[6941 2234 170 11 8422 357 7 3417 4 31...]...]\n",
            "I0212 04:14:50.439398 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[6 2978 9401 2082 13 9812 7 3747 2 14 9 1 8 516 5129 2 6 1 1 1][0 420 60 1 13 835 7 0 47 19 6 1072 4 932 2 97 1 26 1 163][1453 8 0 6340 226 7 923 22 0 552...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 6 2978 9401 2082 13 9812 7 3747 2...]...][[0 1 1 1 1 1 0 0 1 1...]...][[4 10000 2978 9401 2082 13 9812 10000 10000 2...]...][[6 2978 9401 2082 13 9812 7 3747 2 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 6 2978 9401 2082 13 9812 7 3747 2...]...][[0 1 1 1 1 1 0 0 1 1...]...][[4 10000 2978 9401 2082 13 9812 10000 10000 2...]...][[6 2978 9401 2082 13 9812 7 3747 2 14...]...]\n",
            "targets[[6 2978 9401 2082 13 9812 7 3747 2 14 9 1 8 516 5129 2 6 1 1 1][0 420 60 1 13 835 7 0 47 19 6 1072 4 932 2 97 1 26 1 163][1453 8 0 6340 226 7 923 22 0 552...]...]\n",
            "targets[[6 2978 9401 2082 13 9812 7 3747 2 14 9 1 8 516 5129 2 6 1 1 1][0 420 60 1 13 835 7 0 47 19 6 1072 4 932 2 97 1 26 1 163][1453 8 0 6340 226 7 923 22 0 552...]...]\n",
            "targets[[6 2978 9401 2082 13 9812 7 3747 2 14 9 1 8 516 5129 2 6 1 1 1][0 420 60 1 13 835 7 0 47 19 6 1072 4 932 2 97 1 26 1 163][1453 8 0 6340 226 7 923 22 0 552...]...]\n",
            "global_step: 1446\n",
            " perplexity: 690.314\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[6 2978 9401 2082 13 9812 7 3747 2 14 9 1 8 516 5129 2 6 1 1 1][0 420 60 1 13 835 7 0 47 19 6 1072 4 932 2 97 1 26 1 163][1453 8 0 6340 226 7 923 22 0 552...]...]\n",
            " percent of 3-grams captured: 0.175.\n",
            "targets[[6 2978 9401 2082 13 9812 7 3747 2 14 9 1 8 516 5129 2 6 1 1 1][0 420 60 1 13 835 7 0 47 19 6 1072 4 932 2 97 1 26 1 163][1453 8 0 6340 226 7 923 22 0 552...]...]\n",
            " percent of 2-grams captured: 0.450.\n",
            "targets[[6 2978 9401 2082 13 9812 7 3747 2 14 9 1 8 516 5129 2 6 1 1 1][0 420 60 1 13 835 7 0 47 19 6 1072 4 932 2 97 1 26 1 163][1453 8 0 6340 226 7 923 22 0 552...]...]\n",
            " percent of 4-grams captured: 0.047.\n",
            " geometric_avg: 0.155.\n",
            " arithmetic_avg: 0.224.\n",
            "global_step: 1446\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69631\n",
            " G train loss: 3.29962\n",
            "targets[[6 2978 9401 2082 13 9812 7 3747 2 14 9 1 8 516 5129 2 6 1 1 1][0 420 60 1 13 835 7 0 47 19 6 1072 4 932 2 97 1 26 1 163][1453 8 0 6340 226 7 923 22 0 552...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 6 2978 9401 2082 13 9812 7 3747 2...]...][[0 1 1 1 1 1 0 0 1 1...]...][[4 10000 2978 9401 2082 13 9812 10000 10000 2...]...][[6 2978 9401 2082 13 9812 7 3747 2 14...]...]\n",
            " Sample 0.\n",
            "   [0]  a                     0.493   3.335  \n",
            "   [1]  wild                  0.482   0.000  \n",
            "   [1]  sheep                 0.476   0.000  \n",
            "   [1]  chase                 0.476   0.000  \n",
            "   [1]  is                    0.484   0.000  \n",
            "   [1]  rooted                0.494   0.000  \n",
            "   [0]  in                    0.504   4.109  \n",
            "   [0]  reality               0.511   9.703  \n",
            "   [1]  <eos>                 0.514   0.000  \n",
            "   [1]  it                    0.520   0.000  \n",
            "   [0]  's                    0.519   3.867  \n",
            "   [0]  <unk>                 0.522   3.241  \n",
            "   [0]  and                   0.523   3.959  \n",
            "   [1]  often                 0.520   0.000  \n",
            "   [1]  funny                 0.518   0.000  \n",
            "   [0]  <eos>                 0.515   3.872  \n",
            "   [0]  a                     0.513   4.086  \n",
            "   [0]  <unk>                 0.518   3.139  \n",
            "   [1]  <unk>                 0.527   0.000  \n",
            "   [1]  <unk>                 0.537   0.000  \n",
            " Sample 1.\n",
            "   [1]  the                   0.541   0.000  \n",
            "   [0]  takeover              0.543   6.831  \n",
            "   [1]  stock                 0.545   0.000  \n",
            "   [0]  <unk>                 0.547   4.195  \n",
            "   [1]  is                    0.547   0.000  \n",
            "   [0]  taking                0.547   8.826  \n",
            "   [1]  in                    0.547   0.000  \n",
            "   [1]  the                   0.549   0.000  \n",
            "   [0]  market                0.545   5.552  \n",
            "   [1]  as                    0.543   0.000  \n",
            "   [1]  a                     0.541   0.000  \n",
            "   [1]  source                0.538   0.000  \n",
            "   [0]  of                    0.535   2.728  \n",
            "   [0]  volatility            0.533   10.677 \n",
            "   [0]  <eos>                 0.533   3.019  \n",
            "   [1]  many                  0.535   0.000  \n",
            "   [0]  <unk>                 0.541   3.498  \n",
            "   [0]  are                   0.546   5.097  \n",
            "   [0]  <unk>                 0.552   3.066  \n",
            "   [1]  she                   0.554   0.000  \n",
            " Sample 2.\n",
            "   [1]  november              0.485   0.000  \n",
            "   [0]  and                   0.467   4.190  \n",
            "   [1]  the                   0.454   0.000  \n",
            "   [0]  carpet                0.448   10.804 \n",
            "   [1]  sale                  0.447   0.000  \n",
            "   [1]  in                    0.443   0.000  \n",
            "   [0]  december              0.438   7.116  \n",
            "   [1]  with                  0.433   0.000  \n",
            "   [0]  the                   0.429   2.519  \n",
            "   [0]  gains                 0.424   8.606  \n",
            "   [1]  to                    0.421   0.000  \n",
            "   [0]  be                    0.418   4.702  \n",
            "   [0]  applied               0.413   10.409 \n",
            "   [0]  to                    0.407   3.238  \n",
            "   [0]  fourth                0.399   7.988  \n",
            "   [0]  quarter               0.393   7.062  \n",
            "   [1]  or                    0.391   0.000  \n",
            "   [1]  first-quarter         0.388   0.000  \n",
            "   [0]  results               0.387   8.069  \n",
            "   [0]  <eos>                 0.387   2.945  \n",
            "Samples\n",
            "Sample 0 .  a wild sheep chase is rooted in reality <eos> it 's <unk> and often funny <eos> a <unk> <unk> <unk>\n",
            "Sample 1 .  the takeover stock <unk> is taking in the market as a source of volatility <eos> many <unk> are <unk> she\n",
            "Sample 2 .  november and the carpet sale in december with the gains to be applied to fourth quarter or first-quarter results <eos>\n",
            "\n",
            "\n",
            "targets[[1 4 6 1 1 1 22 6 7159 3465 2 28 30 7 1 49 1 8109 624 5822][0 2875 1115 13 94 161 8 0 1 34 58 1823 84 16 1 4 14 5 0 35][35 529 16 0 3465 5 41 0 53 354...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 6 1 1 1 22 6 7159...]...][[0 0 0 1 0 1 1 1 0 1...]...][[0 10000 10000 10000 1 10000 1 22 6 10000...]...][[1 4 6 1 1 1 22 6 7159 3465...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 6 1 1 1 22 6 7159...]...][[0 0 0 1 0 1 1 1 0 1...]...][[0 10000 10000 10000 1 10000 1 22 6 10000...]...][[1 4 6 1 1 1 22 6 7159 3465...]...]\n",
            "I0212 04:14:53.816993 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[6941 2234 170 11 8422 357 7 3417 4 31 1 9401 22 6 1832 16 27 192 18 0][44 8 38 34 5 184 67 607 712 166 38 616 3148 2 166 1775 1492 16 330 321][0 98 9 1717 1 5132 119 6 3419 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 6941 2234 170 11 8422 357 7 3417 4...]...][[0 1 0 1 0 1 1 0 1 1...]...][[1 10000 2234 10000 11 10000 357 7 10000 4...]...][[6941 2234 170 11 8422 357 7 3417 4 31...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 6941 2234 170 11 8422 357 7 3417 4...]...][[0 1 0 1 0 1 1 0 1 1...]...][[1 10000 2234 10000 11 10000 357 7 10000 4...]...][[6941 2234 170 11 8422 357 7 3417 4 31...]...]\n",
            "targets[[6941 2234 170 11 8422 357 7 3417 4 31 1 9401 22 6 1832 16 27 192 18 0][44 8 38 34 5 184 67 607 712 166 38 616 3148 2 166 1775 1492 16 330 321][0 98 9 1717 1 5132 119 6 3419 3...]...]\n",
            "targets[[6941 2234 170 11 8422 357 7 3417 4 31 1 9401 22 6 1832 16 27 192 18 0][44 8 38 34 5 184 67 607 712 166 38 616 3148 2 166 1775 1492 16 330 321][0 98 9 1717 1 5132 119 6 3419 3...]...]\n",
            "targets[[6941 2234 170 11 8422 357 7 3417 4 31 1 9401 22 6 1832 16 27 192 18 0][44 8 38 34 5 184 67 607 712 166 38 616 3148 2 166 1775 1492 16 330 321][0 98 9 1717 1 5132 119 6 3419 3...]...]\n",
            "global_step: 1449\n",
            " perplexity: 689.939\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[6941 2234 170 11 8422 357 7 3417 4 31 1 9401 22 6 1832 16 27 192 18 0][44 8 38 34 5 184 67 607 712 166 38 616 3148 2 166 1775 1492 16 330 321][0 98 9 1717 1 5132 119 6 3419 3...]...]\n",
            " percent of 3-grams captured: 0.169.\n",
            "targets[[6941 2234 170 11 8422 357 7 3417 4 31 1 9401 22 6 1832 16 27 192 18 0][44 8 38 34 5 184 67 607 712 166 38 616 3148 2 166 1775 1492 16 330 321][0 98 9 1717 1 5132 119 6 3419 3...]...]\n",
            " percent of 2-grams captured: 0.474.\n",
            "targets[[6941 2234 170 11 8422 357 7 3417 4 31 1 9401 22 6 1832 16 27 192 18 0][44 8 38 34 5 184 67 607 712 166 38 616 3148 2 166 1775 1492 16 330 321][0 98 9 1717 1 5132 119 6 3419 3...]...]\n",
            " percent of 4-grams captured: 0.053.\n",
            " geometric_avg: 0.162.\n",
            " arithmetic_avg: 0.232.\n",
            "global_step: 1449\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69631\n",
            " G train loss: 3.30004\n",
            "targets[[6941 2234 170 11 8422 357 7 3417 4 31 1 9401 22 6 1832 16 27 192 18 0][44 8 38 34 5 184 67 607 712 166 38 616 3148 2 166 1775 1492 16 330 321][0 98 9 1717 1 5132 119 6 3419 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 6941 2234 170 11 8422 357 7 3417 4...]...][[0 1 0 1 0 1 1 0 1 1...]...][[1 10000 2234 10000 11 10000 357 7 10000 4...]...][[6941 2234 170 11 8422 357 7 3417 4 31...]...]\n",
            " Sample 0.\n",
            "   [0]  hero                  0.531   8.677  \n",
            "   [1]  sets                  0.518   0.000  \n",
            "   [0]  off                   0.509   6.725  \n",
            "   [1]  for                   0.506   0.000  \n",
            "   [0]  snow                  0.511   10.943 \n",
            "   [1]  country               0.517   0.000  \n",
            "   [1]  in                    0.524   0.000  \n",
            "   [0]  search                0.531   11.588 \n",
            "   [1]  of                    0.537   0.000  \n",
            "   [1]  an                    0.542   0.000  \n",
            "   [0]  <unk>                 0.544   3.666  \n",
            "   [1]  sheep                 0.547   0.000  \n",
            "   [0]  with                  0.552   5.479  \n",
            "   [1]  a                     0.555   0.000  \n",
            "   [1]  star                  0.556   0.000  \n",
            "   [1]  on                    0.554   0.000  \n",
            "   [0]  its                   0.553   4.672  \n",
            "   [1]  back                  0.551   0.000  \n",
            "   [0]  at                    0.547   5.774  \n",
            "   [0]  the                   0.545   2.534  \n",
            " Sample 1.\n",
            "   [1]  says                  0.510   0.000  \n",
            "   [1]  and                   0.508   0.000  \n",
            "   [1]  they                  0.510   0.000  \n",
            "   [1]  have                  0.511   0.000  \n",
            "   [1]  to                    0.505   0.000  \n",
            "   [1]  sell                  0.496   0.000  \n",
            "   [1]  when                  0.486   0.000  \n",
            "   [1]  things                0.477   0.000  \n",
            "   [0]  look                  0.470   8.463  \n",
            "   [0]  like                  0.465   6.739  \n",
            "   [1]  they                  0.463   0.000  \n",
            "   [0]  fall                  0.465   8.154  \n",
            "   [1]  apart                 0.466   0.000  \n",
            "   [1]  <eos>                 0.465   0.000  \n",
            "   [1]  like                  0.463   0.000  \n",
            "   [1]  virtually             0.461   0.000  \n",
            "   [0]  everything            0.460   8.867  \n",
            "   [0]  on                    0.460   4.894  \n",
            "   [0]  wall                  0.459   7.066  \n",
            "   [0]  street                0.461   7.576  \n",
            " Sample 2.\n",
            "   [0]  the                   0.453   1.247  \n",
            "   [1]  government            0.448   0.000  \n",
            "   [0]  's                    0.442   4.563  \n",
            "   [0]  primary               0.436   7.925  \n",
            "   [1]  <unk>                 0.433   0.000  \n",
            "   [0]  gauge                 0.434   10.754 \n",
            "   [1]  rose                  0.434   0.000  \n",
            "   [0]  a                     0.437   3.697  \n",
            "   [1]  slight                0.436   0.000  \n",
            "   [0]  N                     0.438   3.713  \n",
            "   [0]  N                     0.442   1.156  \n",
            "   [1]  in                    0.445   0.000  \n",
            "   [0]  september             0.446   4.208  \n",
            "   [0]  but                   0.447   6.888  \n",
            "   [1]  economists            0.446   0.000  \n",
            "   [1]  said                  0.445   0.000  \n",
            "   [1]  the                   0.444   0.000  \n",
            "   [1]  report                0.446   0.000  \n",
            "   [0]  offered               0.449   7.860  \n",
            "   [0]  little                0.453   8.727  \n",
            "Samples\n",
            "Sample 0 .  hero sets off for snow country in search of an <unk> sheep with a star on its back at the\n",
            "Sample 1 .  says and they have to sell when things look like they fall apart <eos> like virtually everything on wall street\n",
            "Sample 2 .  the government 's primary <unk> gauge rose a slight N N in september but economists said the report offered little\n",
            "\n",
            "\n",
            "targets[[1 1059 199 19 996 29 6 1 1 2 702 0 229 28 7548 6 1 4648 1 56][2859 7 181 72 2 209 0 1519 161 586 36 60 7063 19 38 26 1 708 211 0][810 3092 41 50 1324 3 3 7 397 29...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5822 1 1059 199 19 996 29 6 1 1...]...][[1 0 0 0 0 1 1 0 1 1...]...][[5822 1 10000 10000 10000 10000 29 6 10000 1...]...][[1 1059 199 19 996 29 6 1 1 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5822 1 1059 199 19 996 29 6 1 1...]...][[1 0 0 0 0 1 1 0 1 1...]...][[5822 1 10000 10000 10000 10000 29 6 10000 1...]...][[1 1059 199 19 996 29 6 1 1 2...]...]\n",
            "I0212 04:14:57.259360 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1 4 6 1 1 1 22 6 7159 3465 2 28 30 7 1 49 1 8109 624 5822][0 2875 1115 13 94 161 8 0 1 34 58 1823 84 16 1 4 14 5 0 35][35 529 16 0 3465 5 41 0 53 354...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 6 1 1 1 22 6 7159...]...][[0 1 0 0 1 1 0 0 0 0...]...][[0 10000 4 10000 10000 1 1 10000 10000 10000...]...][[1 4 6 1 1 1 22 6 7159 3465...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 6 1 1 1 22 6 7159...]...][[0 1 0 0 1 1 0 0 0 0...]...][[0 10000 4 10000 10000 1 1 10000 10000 10000...]...][[1 4 6 1 1 1 22 6 7159 3465...]...]\n",
            "targets[[1 4 6 1 1 1 22 6 7159 3465 2 28 30 7 1 49 1 8109 624 5822][0 2875 1115 13 94 161 8 0 1 34 58 1823 84 16 1 4 14 5 0 35][35 529 16 0 3465 5 41 0 53 354...]...]\n",
            "targets[[1 4 6 1 1 1 22 6 7159 3465 2 28 30 7 1 49 1 8109 624 5822][0 2875 1115 13 94 161 8 0 1 34 58 1823 84 16 1 4 14 5 0 35][35 529 16 0 3465 5 41 0 53 354...]...]\n",
            "targets[[1 4 6 1 1 1 22 6 7159 3465 2 28 30 7 1 49 1 8109 624 5822][0 2875 1115 13 94 161 8 0 1 34 58 1823 84 16 1 4 14 5 0 35][35 529 16 0 3465 5 41 0 53 354...]...]\n",
            "global_step: 1452\n",
            " perplexity: 689.662\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1 4 6 1 1 1 22 6 7159 3465 2 28 30 7 1 49 1 8109 624 5822][0 2875 1115 13 94 161 8 0 1 34 58 1823 84 16 1 4 14 5 0 35][35 529 16 0 3465 5 41 0 53 354...]...]\n",
            " percent of 3-grams captured: 0.144.\n",
            "targets[[1 4 6 1 1 1 22 6 7159 3465 2 28 30 7 1 49 1 8109 624 5822][0 2875 1115 13 94 161 8 0 1 34 58 1823 84 16 1 4 14 5 0 35][35 529 16 0 3465 5 41 0 53 354...]...]\n",
            " percent of 2-grams captured: 0.482.\n",
            "targets[[1 4 6 1 1 1 22 6 7159 3465 2 28 30 7 1 49 1 8109 624 5822][0 2875 1115 13 94 161 8 0 1 34 58 1823 84 16 1 4 14 5 0 35][35 529 16 0 3465 5 41 0 53 354...]...]\n",
            " percent of 4-grams captured: 0.050.\n",
            " geometric_avg: 0.152.\n",
            " arithmetic_avg: 0.225.\n",
            "global_step: 1452\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69635\n",
            " G train loss: 3.29973\n",
            "targets[[1 4 6 1 1 1 22 6 7159 3465 2 28 30 7 1 49 1 8109 624 5822][0 2875 1115 13 94 161 8 0 1 34 58 1823 84 16 1 4 14 5 0 35][35 529 16 0 3465 5 41 0 53 354...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 6 1 1 1 22 6 7159...]...][[0 1 0 0 1 1 0 0 0 0...]...][[0 10000 4 10000 10000 1 1 10000 10000 10000...]...][[1 4 6 1 1 1 22 6 7159 3465...]...]\n",
            " Sample 0.\n",
            "   [0]  <unk>                 0.465   4.030  \n",
            "   [1]  of                    0.462   0.000  \n",
            "   [0]  a                     0.461   3.277  \n",
            "   [0]  <unk>                 0.460   3.612  \n",
            "   [1]  <unk>                 0.462   0.000  \n",
            "   [1]  <unk>                 0.466   0.000  \n",
            "   [0]  with                  0.473   5.782  \n",
            "   [0]  a                     0.481   3.329  \n",
            "   [0]  stanford              0.485   11.012 \n",
            "   [0]  degree                0.487   11.259 \n",
            "   [1]  <eos>                 0.489   0.000  \n",
            "   [1]  he                    0.491   0.000  \n",
            "   [0]  has                   0.495   5.055  \n",
            "   [0]  in                    0.498   5.280  \n",
            "   [0]  <unk>                 0.499   3.869  \n",
            "   [1]  his                   0.500   0.000  \n",
            "   [1]  <unk>                 0.500   0.000  \n",
            "   [1]  girlfriend            0.498   0.000  \n",
            "   [0]  whose                 0.496   8.011  \n",
            "   [0]  sassy                 0.491   9.730  \n",
            " Sample 1.\n",
            "   [0]  the                   0.539   3.920  \n",
            "   [1]  program-trading       0.542   0.000  \n",
            "   [0]  battle                0.545   7.694  \n",
            "   [1]  is                    0.545   0.000  \n",
            "   [1]  over                  0.548   0.000  \n",
            "   [0]  money                 0.551   8.213  \n",
            "   [1]  and                   0.553   0.000  \n",
            "   [0]  the                   0.554   2.686  \n",
            "   [1]  <unk>                 0.556   0.000  \n",
            "   [1]  have                  0.559   0.000  \n",
            "   [1]  been                  0.562   0.000  \n",
            "   [0]  losing                0.562   8.973  \n",
            "   [0]  out                   0.561   6.681  \n",
            "   [1]  on                    0.560   0.000  \n",
            "   [1]  <unk>                 0.561   0.000  \n",
            "   [1]  of                    0.561   0.000  \n",
            "   [1]  it                    0.565   0.000  \n",
            "   [0]  to                    0.567   3.952  \n",
            "   [1]  the                   0.567   0.000  \n",
            "   [0]  new                   0.569   3.983  \n",
            " Sample 2.\n",
            "   [0]  new                   0.480   6.663  \n",
            "   [1]  information           0.468   0.000  \n",
            "   [1]  on                    0.459   0.000  \n",
            "   [1]  the                   0.454   0.000  \n",
            "   [1]  degree                0.448   0.000  \n",
            "   [0]  to                    0.445   4.331  \n",
            "   [1]  which                 0.442   0.000  \n",
            "   [0]  the                   0.437   4.128  \n",
            "   [1]  u.s.                  0.431   0.000  \n",
            "   [0]  economy               0.425   7.790  \n",
            "   [0]  is                    0.420   4.542  \n",
            "   [0]  slowing               0.415   9.691  \n",
            "   [1]  <eos>                 0.411   0.000  \n",
            "   [1]  the                   0.408   0.000  \n",
            "   [1]  small                 0.406   0.000  \n",
            "   [1]  increase              0.406   0.000  \n",
            "   [0]  in                    0.403   4.134  \n",
            "   [1]  the                   0.402   0.000  \n",
            "   [0]  index                 0.399   8.106  \n",
            "   [1]  of                    0.397   0.000  \n",
            "Samples\n",
            "Sample 0 .  <unk> of a <unk> <unk> <unk> with a stanford degree <eos> he has in <unk> his <unk> girlfriend whose sassy\n",
            "Sample 1 .  the program-trading battle is over money and the <unk> have been losing out on <unk> of it to the new\n",
            "Sample 2 .  new information on the degree to which the u.s. economy is slowing <eos> the small increase in the index of\n",
            "\n",
            "\n",
            "targets[[1333 0 6941 4711 9 1685 297 8 0 9401 896 6 5847 1 1181 56 8536 114 1394 6][224 1 2 1519 60 586 166 5 525 3 107 5 3 107 11 538 12 3 38 2930][10 0 354 30 3530 1 2 227 14 175...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1333 0 6941 4711 9 1685 297 8 0...]...][[0 0 0 1 1 1 1 1 0 1...]...][[56 10000 10000 10000 4711 9 1685 297 8 10000...]...][[1333 0 6941 4711 9 1685 297 8 0 9401...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1333 0 6941 4711 9 1685 297 8 0...]...][[0 0 0 1 1 1 1 1 0 1...]...][[56 10000 10000 10000 4711 9 1685 297 8 10000...]...][[1333 0 6941 4711 9 1685 297 8 0 9401...]...]\n",
            "I0212 04:15:00.687866 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1 1059 199 19 996 29 6 1 1 2 702 0 229 28 7548 6 1 4648 1 56][2859 7 181 72 2 209 0 1519 161 586 36 60 7063 19 38 26 1 708 211 0][810 3092 41 50 1324 3 3 7 397 29...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5822 1 1059 199 19 996 29 6 1 1...]...][[0 0 0 0 1 0 1 0 1 1...]...][[5822 10000 10000 10000 10000 996 10000 6 10000 1...]...][[1 1059 199 19 996 29 6 1 1 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5822 1 1059 199 19 996 29 6 1 1...]...][[0 0 0 0 1 0 1 0 1 1...]...][[5822 10000 10000 10000 10000 996 10000 6 10000 1...]...][[1 1059 199 19 996 29 6 1 1 2...]...]\n",
            "targets[[1 1059 199 19 996 29 6 1 1 2 702 0 229 28 7548 6 1 4648 1 56][2859 7 181 72 2 209 0 1519 161 586 36 60 7063 19 38 26 1 708 211 0][810 3092 41 50 1324 3 3 7 397 29...]...]\n",
            "targets[[1 1059 199 19 996 29 6 1 1 2 702 0 229 28 7548 6 1 4648 1 56][2859 7 181 72 2 209 0 1519 161 586 36 60 7063 19 38 26 1 708 211 0][810 3092 41 50 1324 3 3 7 397 29...]...]\n",
            "targets[[1 1059 199 19 996 29 6 1 1 2 702 0 229 28 7548 6 1 4648 1 56][2859 7 181 72 2 209 0 1519 161 586 36 60 7063 19 38 26 1 708 211 0][810 3092 41 50 1324 3 3 7 397 29...]...]\n",
            "global_step: 1455\n",
            " perplexity: 688.995\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1 1059 199 19 996 29 6 1 1 2 702 0 229 28 7548 6 1 4648 1 56][2859 7 181 72 2 209 0 1519 161 586 36 60 7063 19 38 26 1 708 211 0][810 3092 41 50 1324 3 3 7 397 29...]...]\n",
            " percent of 3-grams captured: 0.181.\n",
            "targets[[1 1059 199 19 996 29 6 1 1 2 702 0 229 28 7548 6 1 4648 1 56][2859 7 181 72 2 209 0 1519 161 586 36 60 7063 19 38 26 1 708 211 0][810 3092 41 50 1324 3 3 7 397 29...]...]\n",
            " percent of 2-grams captured: 0.474.\n",
            "targets[[1 1059 199 19 996 29 6 1 1 2 702 0 229 28 7548 6 1 4648 1 56][2859 7 181 72 2 209 0 1519 161 586 36 60 7063 19 38 26 1 708 211 0][810 3092 41 50 1324 3 3 7 397 29...]...]\n",
            " percent of 4-grams captured: 0.076.\n",
            " geometric_avg: 0.187.\n",
            " arithmetic_avg: 0.244.\n",
            "global_step: 1455\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69635\n",
            " G train loss: 3.29932\n",
            "targets[[1 1059 199 19 996 29 6 1 1 2 702 0 229 28 7548 6 1 4648 1 56][2859 7 181 72 2 209 0 1519 161 586 36 60 7063 19 38 26 1 708 211 0][810 3092 41 50 1324 3 3 7 397 29...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5822 1 1059 199 19 996 29 6 1 1...]...][[0 0 0 0 1 0 1 0 1 1...]...][[5822 10000 10000 10000 10000 996 10000 6 10000 1...]...][[1 1059 199 19 996 29 6 1 1 2...]...]\n",
            " Sample 0.\n",
            "   [0]  <unk>                 0.495   3.788  \n",
            "   [0]  mark                  0.486   9.640  \n",
            "   [0]  her                   0.482   8.065  \n",
            "   [0]  as                    0.476   5.248  \n",
            "   [1]  anything              0.468   0.000  \n",
            "   [0]  but                   0.459   4.893  \n",
            "   [1]  a                     0.451   0.000  \n",
            "   [0]  <unk>                 0.447   3.024  \n",
            "   [1]  <unk>                 0.447   0.000  \n",
            "   [1]  <eos>                 0.447   0.000  \n",
            "   [1]  along                 0.447   0.000  \n",
            "   [1]  the                   0.448   0.000  \n",
            "   [1]  way                   0.445   0.000  \n",
            "   [1]  he                    0.448   0.000  \n",
            "   [0]  meets                 0.452   13.047 \n",
            "   [0]  a                     0.457   4.059  \n",
            "   [1]  <unk>                 0.462   0.000  \n",
            "   [0]  christian             0.467   12.193 \n",
            "   [1]  <unk>                 0.470   0.000  \n",
            "   [0]  who                   0.473   5.919  \n",
            " Sample 1.\n",
            "   [0]  guard                 0.575   8.977  \n",
            "   [1]  in                    0.569   0.000  \n",
            "   [0]  recent                0.565   6.156  \n",
            "   [0]  years                 0.558   7.174  \n",
            "   [0]  <eos>                 0.552   2.162  \n",
            "   [1]  take                  0.547   0.000  \n",
            "   [0]  the                   0.551   3.441  \n",
            "   [0]  traditional           0.557   8.321  \n",
            "   [1]  money                 0.566   0.000  \n",
            "   [0]  managers              0.572   7.511  \n",
            "   [0]  or                    0.575   6.106  \n",
            "   [0]  stock                 0.580   7.028  \n",
            "   [0]  pickers               0.586   10.948 \n",
            "   [0]  as                    0.593   4.884  \n",
            "   [0]  they                  0.601   5.464  \n",
            "   [1]  are                   0.602   0.000  \n",
            "   [0]  <unk>                 0.604   3.253  \n",
            "   [0]  known                 0.605   8.067  \n",
            "   [1]  among                 0.604   0.000  \n",
            "   [0]  the                   0.602   4.122  \n",
            " Sample 2.\n",
            "   [0]  leading               0.450   8.803  \n",
            "   [1]  indicators            0.448   0.000  \n",
            "   [0]  which                 0.451   5.884  \n",
            "   [1]  had                   0.455   0.000  \n",
            "   [0]  climbed               0.458   9.279  \n",
            "   [1]  N                     0.465   0.000  \n",
            "   [0]  N                     0.475   2.791  \n",
            "   [1]  in                    0.485   0.000  \n",
            "   [0]  august                0.491   5.883  \n",
            "   [1]  but                   0.495   0.000  \n",
            "   [1]  was                   0.495   0.000  \n",
            "   [0]  unchanged             0.495   8.976  \n",
            "   [0]  in                    0.495   3.021  \n",
            "   [1]  july                  0.497   0.000  \n",
            "   [0]  does                  0.496   7.425  \n",
            "   [0]  lend                  0.498   10.233 \n",
            "   [0]  support               0.497   8.262  \n",
            "   [1]  to                    0.499   0.000  \n",
            "   [1]  the                   0.497   0.000  \n",
            "   [0]  view                  0.496   8.107  \n",
            "Samples\n",
            "Sample 0 .  <unk> mark her as anything but a <unk> <unk> <eos> along the way he meets a <unk> christian <unk> who\n",
            "Sample 1 .  guard in recent years <eos> take the traditional money managers or stock pickers as they are <unk> known among the\n",
            "Sample 2 .  leading indicators which had climbed N N in august but was unchanged in july does lend support to the view\n",
            "\n",
            "\n",
            "targets[[1 2 0 7827 23 1 13 6 1596 1 7 203 2 6 45 181 3502 8268 3141 538][11 129 1255 116 8 206 788 11 1046 116 2 396 97 88 586 3578 3377 5 113 585][965 13 16 0 7471 2 68 87 32 316...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6 1 2 0 7827 23 1 13 6 1596...]...][[1 1 0 0 0 1 1 1 0 0...]...][[6 1 2 10000 10000 10000 1 13 6 10000...]...][[1 2 0 7827 23 1 13 6 1596 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6 1 2 0 7827 23 1 13 6 1596...]...][[1 1 0 0 0 1 1 1 0 0...]...][[6 1 2 10000 10000 10000 1 13 6 10000...]...][[1 2 0 7827 23 1 13 6 1596 1...]...]\n",
            "I0212 04:15:04.033879 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1333 0 6941 4711 9 1685 297 8 0 9401 896 6 5847 1 1181 56 8536 114 1394 6][224 1 2 1519 60 586 166 5 525 3 107 5 3 107 11 538 12 3 38 2930][10 0 354 30 3530 1 2 227 14 175...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1333 0 6941 4711 9 1685 297 8 0...]...][[1 0 0 1 0 0 1 0 0 1...]...][[56 1333 10000 10000 4711 10000 10000 297 10000 10000...]...][[1333 0 6941 4711 9 1685 297 8 0 9401...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1333 0 6941 4711 9 1685 297 8 0...]...][[1 0 0 1 0 0 1 0 0 1...]...][[56 1333 10000 10000 4711 10000 10000 297 10000 10000...]...][[1333 0 6941 4711 9 1685 297 8 0 9401...]...]\n",
            "targets[[1333 0 6941 4711 9 1685 297 8 0 9401 896 6 5847 1 1181 56 8536 114 1394 6][224 1 2 1519 60 586 166 5 525 3 107 5 3 107 11 538 12 3 38 2930][10 0 354 30 3530 1 2 227 14 175...]...]\n",
            "targets[[1333 0 6941 4711 9 1685 297 8 0 9401 896 6 5847 1 1181 56 8536 114 1394 6][224 1 2 1519 60 586 166 5 525 3 107 5 3 107 11 538 12 3 38 2930][10 0 354 30 3530 1 2 227 14 175...]...]\n",
            "targets[[1333 0 6941 4711 9 1685 297 8 0 9401 896 6 5847 1 1181 56 8536 114 1394 6][224 1 2 1519 60 586 166 5 525 3 107 5 3 107 11 538 12 3 38 2930][10 0 354 30 3530 1 2 227 14 175...]...]\n",
            "global_step: 1458\n",
            " perplexity: 687.996\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1333 0 6941 4711 9 1685 297 8 0 9401 896 6 5847 1 1181 56 8536 114 1394 6][224 1 2 1519 60 586 166 5 525 3 107 5 3 107 11 538 12 3 38 2930][10 0 354 30 3530 1 2 227 14 175...]...]\n",
            " percent of 3-grams captured: 0.217.\n",
            "targets[[1333 0 6941 4711 9 1685 297 8 0 9401 896 6 5847 1 1181 56 8536 114 1394 6][224 1 2 1519 60 586 166 5 525 3 107 5 3 107 11 538 12 3 38 2930][10 0 354 30 3530 1 2 227 14 175...]...]\n",
            " percent of 2-grams captured: 0.474.\n",
            "targets[[1333 0 6941 4711 9 1685 297 8 0 9401 896 6 5847 1 1181 56 8536 114 1394 6][224 1 2 1519 60 586 166 5 525 3 107 5 3 107 11 538 12 3 38 2930][10 0 354 30 3530 1 2 227 14 175...]...]\n",
            " percent of 4-grams captured: 0.121.\n",
            " geometric_avg: 0.231.\n",
            " arithmetic_avg: 0.270.\n",
            "global_step: 1458\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69632\n",
            " G train loss: 3.29897\n",
            "targets[[1333 0 6941 4711 9 1685 297 8 0 9401 896 6 5847 1 1181 56 8536 114 1394 6][224 1 2 1519 60 586 166 5 525 3 107 5 3 107 11 538 12 3 38 2930][10 0 354 30 3530 1 2 227 14 175...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[56 1333 0 6941 4711 9 1685 297 8 0...]...][[1 0 0 1 0 0 1 0 0 1...]...][[56 1333 10000 10000 4711 10000 10000 297 10000 10000...]...][[1333 0 6941 4711 9 1685 297 8 0 9401...]...]\n",
            " Sample 0.\n",
            "   [1]  offers                0.556   0.000  \n",
            "   [0]  the                   0.550   3.141  \n",
            "   [0]  hero                  0.547   8.882  \n",
            "   [1]  god                   0.545   0.000  \n",
            "   [0]  's                    0.545   4.319  \n",
            "   [0]  phone                 0.543   8.466  \n",
            "   [1]  number                0.545   0.000  \n",
            "   [0]  and                   0.550   3.230  \n",
            "   [0]  the                   0.553   2.408  \n",
            "   [1]  sheep                 0.557   0.000  \n",
            "   [1]  man                   0.557   0.000  \n",
            "   [1]  a                     0.559   0.000  \n",
            "   [0]  sweet                 0.560   11.430 \n",
            "   [0]  <unk>                 0.561   4.173  \n",
            "   [1]  figure                0.564   0.000  \n",
            "   [0]  who                   0.567   5.948  \n",
            "   [1]  wears                 0.569   0.000  \n",
            "   [0]  what                  0.571   6.809  \n",
            "   [1]  else                  0.572   0.000  \n",
            "   [1]  a                     0.574   0.000  \n",
            " Sample 1.\n",
            "   [1]  computer              0.576   0.000  \n",
            "   [1]  <unk>                 0.586   0.000  \n",
            "   [1]  <eos>                 0.592   0.000  \n",
            "   [1]  traditional           0.596   0.000  \n",
            "   [0]  stock                 0.599   6.567  \n",
            "   [0]  managers              0.593   7.684  \n",
            "   [1]  like                  0.581   0.000  \n",
            "   [0]  to                    0.573   4.539  \n",
            "   [0]  charge                0.561   8.254  \n",
            "   [1]  N                     0.550   0.000  \n",
            "   [1]  cents                 0.535   0.000  \n",
            "   [1]  to                    0.525   0.000  \n",
            "   [1]  N                     0.517   0.000  \n",
            "   [0]  cents                 0.507   4.566  \n",
            "   [1]  for                   0.498   0.000  \n",
            "   [1]  every                 0.493   0.000  \n",
            "   [0]  $                     0.491   4.181  \n",
            "   [1]  N                     0.495   0.000  \n",
            "   [0]  they                  0.501   8.671  \n",
            "   [1]  manage                0.508   0.000  \n",
            " Sample 2.\n",
            "   [1]  that                  0.564   0.000  \n",
            "   [0]  the                   0.562   2.210  \n",
            "   [1]  economy               0.561   0.000  \n",
            "   [1]  has                   0.565   0.000  \n",
            "   [1]  slowed                0.568   0.000  \n",
            "   [0]  <unk>                 0.572   4.363  \n",
            "   [0]  <eos>                 0.578   3.650  \n",
            "   [0]  however               0.583   7.595  \n",
            "   [0]  it                    0.588   4.354  \n",
            "   [1]  does                  0.591   0.000  \n",
            "   [1]  n't                   0.591   0.000  \n",
            "   [1]  give                  0.590   0.000  \n",
            "   [0]  much                  0.585   6.766  \n",
            "   [0]  of                    0.582   2.914  \n",
            "   [0]  a                     0.580   2.774  \n",
            "   [1]  clue                  0.579   0.000  \n",
            "   [0]  as                    0.577   5.294  \n",
            "   [1]  to                    0.575   0.000  \n",
            "   [0]  whether               0.574   8.779  \n",
            "   [1]  a                     0.575   0.000  \n",
            "Samples\n",
            "Sample 0 .  offers the hero god 's phone number and the sheep man a sweet <unk> figure who wears what else a\n",
            "Sample 1 .  computer <unk> <eos> traditional stock managers like to charge N cents to N cents for every $ N they manage\n",
            "Sample 2 .  that the economy has slowed <unk> <eos> however it does n't give much of a clue as to whether a\n",
            "\n",
            "\n",
            "targets[[173 124 3 899 5 25 1 7 1 1 30 238 45 55 346 21 3259 155 1 2406][52 22 121 245 2770 0 981 4 739 1 166 0 833 2 63 3540 1 161 586 34][1 1188 18 2785 3659 4359 443 2 105 289...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[538 173 124 3 899 5 25 1 7 1...]...][[1 1 1 0 0 0 1 1 1 0...]...][[538 173 124 3 10000 10000 10000 1 7 1...]...][[173 124 3 899 5 25 1 7 1 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[538 173 124 3 899 5 25 1 7 1...]...][[1 1 1 0 0 0 1 1 1 0...]...][[538 173 124 3 10000 10000 10000 1 7 1...]...][[173 124 3 899 5 25 1 7 1 1...]...]\n",
            "I0212 04:15:07.402932 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1 2 0 7827 23 1 13 6 1596 1 7 203 2 6 45 181 3502 8268 3141 538][11 129 1255 116 8 206 788 11 1046 116 2 396 97 88 586 3578 3377 5 113 585][965 13 16 0 7471 2 68 87 32 316...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6 1 2 0 7827 23 1 13 6 1596...]...][[0 0 0 1 1 0 1 0 1 1...]...][[6 10000 10000 10000 7827 23 10000 13 10000 1596...]...][[1 2 0 7827 23 1 13 6 1596 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6 1 2 0 7827 23 1 13 6 1596...]...][[0 0 0 1 1 0 1 0 1 1...]...][[6 10000 10000 10000 7827 23 10000 13 10000 1596...]...][[1 2 0 7827 23 1 13 6 1596 1...]...]\n",
            "targets[[1 2 0 7827 23 1 13 6 1596 1 7 203 2 6 45 181 3502 8268 3141 538][11 129 1255 116 8 206 788 11 1046 116 2 396 97 88 586 3578 3377 5 113 585][965 13 16 0 7471 2 68 87 32 316...]...]\n",
            "targets[[1 2 0 7827 23 1 13 6 1596 1 7 203 2 6 45 181 3502 8268 3141 538][11 129 1255 116 8 206 788 11 1046 116 2 396 97 88 586 3578 3377 5 113 585][965 13 16 0 7471 2 68 87 32 316...]...]\n",
            "targets[[1 2 0 7827 23 1 13 6 1596 1 7 203 2 6 45 181 3502 8268 3141 538][11 129 1255 116 8 206 788 11 1046 116 2 396 97 88 586 3578 3377 5 113 585][965 13 16 0 7471 2 68 87 32 316...]...]\n",
            "global_step: 1461\n",
            " perplexity: 687.554\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1 2 0 7827 23 1 13 6 1596 1 7 203 2 6 45 181 3502 8268 3141 538][11 129 1255 116 8 206 788 11 1046 116 2 396 97 88 586 3578 3377 5 113 585][965 13 16 0 7471 2 68 87 32 316...]...]\n",
            " percent of 3-grams captured: 0.178.\n",
            "targets[[1 2 0 7827 23 1 13 6 1596 1 7 203 2 6 45 181 3502 8268 3141 538][11 129 1255 116 8 206 788 11 1046 116 2 396 97 88 586 3578 3377 5 113 585][965 13 16 0 7471 2 68 87 32 316...]...]\n",
            " percent of 2-grams captured: 0.458.\n",
            "targets[[1 2 0 7827 23 1 13 6 1596 1 7 203 2 6 45 181 3502 8268 3141 538][11 129 1255 116 8 206 788 11 1046 116 2 396 97 88 586 3578 3377 5 113 585][965 13 16 0 7471 2 68 87 32 316...]...]\n",
            " percent of 4-grams captured: 0.059.\n",
            " geometric_avg: 0.169.\n",
            " arithmetic_avg: 0.231.\n",
            "global_step: 1461\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69638\n",
            " G train loss: 3.29816\n",
            "targets[[1 2 0 7827 23 1 13 6 1596 1 7 203 2 6 45 181 3502 8268 3141 538][11 129 1255 116 8 206 788 11 1046 116 2 396 97 88 586 3578 3377 5 113 585][965 13 16 0 7471 2 68 87 32 316...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6 1 2 0 7827 23 1 13 6 1596...]...][[0 0 0 1 1 0 1 0 1 1...]...][[6 10000 10000 10000 7827 23 10000 13 10000 1596...]...][[1 2 0 7827 23 1 13 6 1596 1...]...]\n",
            " Sample 0.\n",
            "   [0]  <unk>                 0.471   2.725  \n",
            "   [0]  <eos>                 0.469   3.239  \n",
            "   [0]  the                   0.474   1.155  \n",
            "   [1]  40-year-old           0.485   0.000  \n",
            "   [1]  mr.                   0.495   0.000  \n",
            "   [0]  <unk>                 0.504   2.930  \n",
            "   [1]  is                    0.509   0.000  \n",
            "   [0]  a                     0.517   3.812  \n",
            "   [1]  publishing            0.519   0.000  \n",
            "   [1]  <unk>                 0.517   0.000  \n",
            "   [1]  in                    0.515   0.000  \n",
            "   [0]  japan                 0.514   6.259  \n",
            "   [1]  <eos>                 0.511   0.000  \n",
            "   [1]  a                     0.516   0.000  \n",
            "   [1]  more                  0.516   0.000  \n",
            "   [1]  recent                0.515   0.000  \n",
            "   [0]  novel                 0.512   11.056 \n",
            "   [1]  norwegian             0.510   0.000  \n",
            "   [0]  wood                  0.505   9.749  \n",
            "   [1]  every                 0.502   0.000  \n",
            " Sample 1.\n",
            "   [0]  for                   0.436   4.433  \n",
            "   [1]  big                   0.431   0.000  \n",
            "   [0]  institutional         0.427   8.749  \n",
            "   [1]  investors             0.430   0.000  \n",
            "   [1]  and                   0.436   0.000  \n",
            "   [1]  higher                0.444   0.000  \n",
            "   [1]  fees                  0.452   0.000  \n",
            "   [0]  for                   0.461   3.937  \n",
            "   [0]  smaller               0.466   9.194  \n",
            "   [0]  investors             0.472   7.064  \n",
            "   [1]  <eos>                 0.473   0.000  \n",
            "   [0]  yet                   0.474   7.839  \n",
            "   [1]  many                  0.472   0.000  \n",
            "   [0]  such                  0.470   6.398  \n",
            "   [1]  managers              0.468   0.000  \n",
            "   [1]  consistently          0.465   0.000  \n",
            "   [0]  fail                  0.462   11.946 \n",
            "   [0]  to                    0.459   2.166  \n",
            "   [0]  even                  0.456   7.157  \n",
            "   [1]  keep                  0.455   0.000  \n",
            " Sample 2.\n",
            "   [1]  recession             0.490   0.000  \n",
            "   [1]  is                    0.477   0.000  \n",
            "   [1]  on                    0.467   0.000  \n",
            "   [1]  the                   0.466   0.000  \n",
            "   [1]  horizon               0.469   0.000  \n",
            "   [1]  <eos>                 0.474   0.000  \n",
            "   [1]  i                     0.484   0.000  \n",
            "   [1]  do                    0.490   0.000  \n",
            "   [1]  n't                   0.491   0.000  \n",
            "   [0]  think                 0.490   6.369  \n",
            "   [1]  it                    0.492   0.000  \n",
            "   [1]  provides              0.495   0.000  \n",
            "   [0]  much                  0.495   6.601  \n",
            "   [0]  new                   0.495   7.283  \n",
            "   [0]  information           0.494   8.224  \n",
            "   [1]  on                    0.494   0.000  \n",
            "   [0]  the                   0.498   1.502  \n",
            "   [1]  economy               0.502   0.000  \n",
            "   [1]  said                  0.501   0.000  \n",
            "   [1]  richard               0.500   0.000  \n",
            "Samples\n",
            "Sample 0 .  <unk> <eos> the 40-year-old mr. <unk> is a publishing <unk> in japan <eos> a more recent novel norwegian wood every\n",
            "Sample 1 .  for big institutional investors and higher fees for smaller investors <eos> yet many such managers consistently fail to even keep\n",
            "Sample 2 .  recession is on the horizon <eos> i do n't think it provides much new information on the economy said richard\n",
            "\n",
            "\n",
            "targets[[14 7 3 2 29 28 13 152 54 4 249 1 4604 605 9 1 5761 56 26 1][58 1823 764 5 1098 974 193 10 269 563 5 1 2560 105 38 5735 0 833 3 2][346 126 2129 7 346 126 8 1867 1075 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2406 14 7 3 2 29 28 13 152 54...]...][[0 0 1 1 1 1 1 1 1 1...]...][[2406 10000 10000 3 2 29 28 13 152 54...]...][[14 7 3 2 29 28 13 152 54 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2406 14 7 3 2 29 28 13 152 54...]...][[0 0 1 1 1 1 1 1 1 1...]...][[2406 10000 10000 3 2 29 28 13 152 54...]...][[14 7 3 2 29 28 13 152 54 4...]...]\n",
            "I0212 04:15:10.684067 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[173 124 3 899 5 25 1 7 1 1 30 238 45 55 346 21 3259 155 1 2406][52 22 121 245 2770 0 981 4 739 1 166 0 833 2 63 3540 1 161 586 34][1 1188 18 2785 3659 4359 443 2 105 289...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[538 173 124 3 899 5 25 1 7 1...]...][[0 0 1 0 1 0 1 1 1 1...]...][[538 10000 10000 3 10000 5 10000 1 7 1...]...][[173 124 3 899 5 25 1 7 1 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[538 173 124 3 899 5 25 1 7 1...]...][[0 0 1 0 1 0 1 1 1 1...]...][[538 10000 10000 3 10000 5 10000 1 7 1...]...][[173 124 3 899 5 25 1 7 1 1...]...]\n",
            "targets[[173 124 3 899 5 25 1 7 1 1 30 238 45 55 346 21 3259 155 1 2406][52 22 121 245 2770 0 981 4 739 1 166 0 833 2 63 3540 1 161 586 34][1 1188 18 2785 3659 4359 443 2 105 289...]...]\n",
            "targets[[173 124 3 899 5 25 1 7 1 1 30 238 45 55 346 21 3259 155 1 2406][52 22 121 245 2770 0 981 4 739 1 166 0 833 2 63 3540 1 161 586 34][1 1188 18 2785 3659 4359 443 2 105 289...]...]\n",
            "targets[[173 124 3 899 5 25 1 7 1 1 30 238 45 55 346 21 3259 155 1 2406][52 22 121 245 2770 0 981 4 739 1 166 0 833 2 63 3540 1 161 586 34][1 1188 18 2785 3659 4359 443 2 105 289...]...]\n",
            "global_step: 1464\n",
            " perplexity: 687.177\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[173 124 3 899 5 25 1 7 1 1 30 238 45 55 346 21 3259 155 1 2406][52 22 121 245 2770 0 981 4 739 1 166 0 833 2 63 3540 1 161 586 34][1 1188 18 2785 3659 4359 443 2 105 289...]...]\n",
            " percent of 3-grams captured: 0.181.\n",
            "targets[[173 124 3 899 5 25 1 7 1 1 30 238 45 55 346 21 3259 155 1 2406][52 22 121 245 2770 0 981 4 739 1 166 0 833 2 63 3540 1 161 586 34][1 1188 18 2785 3659 4359 443 2 105 289...]...]\n",
            " percent of 2-grams captured: 0.505.\n",
            "targets[[173 124 3 899 5 25 1 7 1 1 30 238 45 55 346 21 3259 155 1 2406][52 22 121 245 2770 0 981 4 739 1 166 0 833 2 63 3540 1 161 586 34][1 1188 18 2785 3659 4359 443 2 105 289...]...]\n",
            " percent of 4-grams captured: 0.053.\n",
            " geometric_avg: 0.169.\n",
            " arithmetic_avg: 0.246.\n",
            "global_step: 1464\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69639\n",
            " G train loss: 3.29806\n",
            "targets[[173 124 3 899 5 25 1 7 1 1 30 238 45 55 346 21 3259 155 1 2406][52 22 121 245 2770 0 981 4 739 1 166 0 833 2 63 3540 1 161 586 34][1 1188 18 2785 3659 4359 443 2 105 289...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[538 173 124 3 899 5 25 1 7 1...]...][[0 0 1 0 1 0 1 1 1 1...]...][[538 10000 10000 3 10000 5 10000 1 7 1...]...][[173 124 3 899 5 25 1 7 1 1...]...]\n",
            " Sample 0.\n",
            "   [0]  japanese              0.473   6.844  \n",
            "   [0]  under                 0.475   7.243  \n",
            "   [1]  N                     0.476   0.000  \n",
            "   [0]  seems                 0.477   9.696  \n",
            "   [1]  to                    0.481   0.000  \n",
            "   [0]  be                    0.484   3.753  \n",
            "   [1]  <unk>                 0.488   0.000  \n",
            "   [1]  in                    0.489   0.000  \n",
            "   [1]  <unk>                 0.489   0.000  \n",
            "   [1]  <unk>                 0.490   0.000  \n",
            "   [1]  has                   0.492   0.000  \n",
            "   [1]  sold                  0.495   0.000  \n",
            "   [1]  more                  0.496   0.000  \n",
            "   [1]  than                  0.494   0.000  \n",
            "   [1]  four                  0.496   0.000  \n",
            "   [1]  million               0.496   0.000  \n",
            "   [1]  copies                0.498   0.000  \n",
            "   [0]  since                 0.499   6.314  \n",
            "   [1]  <unk>                 0.500   0.000  \n",
            "   [1]  published             0.497   0.000  \n",
            " Sample 1.\n",
            "   [0]  up                    0.521   6.375  \n",
            "   [1]  with                  0.516   0.000  \n",
            "   [1]  much                  0.517   0.000  \n",
            "   [0]  less                  0.523   8.705  \n",
            "   [0]  beat                  0.526   11.540 \n",
            "   [0]  the                   0.529   3.440  \n",
            "   [0]  returns               0.530   10.020 \n",
            "   [1]  of                    0.529   0.000  \n",
            "   [0]  standard              0.529   10.183 \n",
            "   [1]  <unk>                 0.531   0.000  \n",
            "   [0]  like                  0.532   6.941  \n",
            "   [1]  the                   0.532   0.000  \n",
            "   [1]  s&p                   0.529   0.000  \n",
            "   [0]  <eos>                 0.527   1.669  \n",
            "   [0]  not                   0.524   6.175  \n",
            "   [1]  surprisingly          0.522   0.000  \n",
            "   [1]  <unk>                 0.525   0.000  \n",
            "   [1]  money                 0.526   0.000  \n",
            "   [0]  managers              0.523   7.916  \n",
            "   [0]  have                  0.518   5.310  \n",
            " Sample 2.\n",
            "   [0]  <unk>                 0.454   2.215  \n",
            "   [0]  economist             0.455   8.986  \n",
            "   [0]  at                    0.460   4.515  \n",
            "   [1]  dean                  0.467   0.000  \n",
            "   [1]  witter                0.476   0.000  \n",
            "   [0]  reynolds              0.484   13.325 \n",
            "   [0]  inc                   0.491   8.105  \n",
            "   [1]  <eos>                 0.497   0.000  \n",
            "   [0]  so                    0.501   5.660  \n",
            "   [1]  far                   0.505   0.000  \n",
            "   [0]  this                  0.509   5.582  \n",
            "   [0]  year                  0.511   6.704  \n",
            "   [1]  the                   0.513   0.000  \n",
            "   [1]  index                 0.514   0.000  \n",
            "   [1]  of                    0.515   0.000  \n",
            "   [1]  leading               0.512   0.000  \n",
            "   [0]  indicators            0.511   12.992 \n",
            "   [0]  has                   0.513   4.965  \n",
            "   [0]  risen                 0.515   11.044 \n",
            "   [0]  in                    0.517   3.045  \n",
            "Samples\n",
            "Sample 0 .  japanese under N seems to be <unk> in <unk> <unk> has sold more than four million copies since <unk> published\n",
            "Sample 1 .  up with much less beat the returns of standard <unk> like the s&p <eos> not surprisingly <unk> money managers have\n",
            "Sample 2 .  <unk> economist at dean witter reynolds inc <eos> so far this year the index of leading indicators has risen in\n",
            "\n",
            "\n",
            "targets[[0 1 7923 7 203 2 51 1779 26 2047 7 1 5350 2135 8 1031 1526 3601 1 4][0 1 525 86 6 261 9246 1040 12 3 1858 2 325 43 12 3 48 36 3 3][234 15 81 4 35 7708 1328 1621 3 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 0 1 7923 7 203 2 51 1779 26...]...][[0 1 1 1 0 1 1 1 1 0...]...][[1 10000 1 7923 7 10000 2 51 1779 26...]...][[0 1 7923 7 203 2 51 1779 26 2047...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 0 1 7923 7 203 2 51 1779 26...]...][[0 1 1 1 0 1 1 1 1 0...]...][[1 10000 1 7923 7 10000 2 51 1779 26...]...][[0 1 7923 7 203 2 51 1779 26 2047...]...]\n",
            "I0212 04:15:13.994915 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[14 7 3 2 29 28 13 152 54 4 249 1 4604 605 9 1 5761 56 26 1][58 1823 764 5 1098 974 193 10 269 563 5 1 2560 105 38 5735 0 833 3 2][346 126 2129 7 346 126 8 1867 1075 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2406 14 7 3 2 29 28 13 152 54...]...][[0 1 0 0 1 1 0 0 0 1...]...][[2406 10000 7 10000 10000 29 28 10000 10000 10000...]...][[14 7 3 2 29 28 13 152 54 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2406 14 7 3 2 29 28 13 152 54...]...][[0 1 0 0 1 1 0 0 0 1...]...][[2406 10000 7 10000 10000 29 28 10000 10000 10000...]...][[14 7 3 2 29 28 13 152 54 4...]...]\n",
            "targets[[14 7 3 2 29 28 13 152 54 4 249 1 4604 605 9 1 5761 56 26 1][58 1823 764 5 1098 974 193 10 269 563 5 1 2560 105 38 5735 0 833 3 2][346 126 2129 7 346 126 8 1867 1075 7...]...]\n",
            "targets[[14 7 3 2 29 28 13 152 54 4 249 1 4604 605 9 1 5761 56 26 1][58 1823 764 5 1098 974 193 10 269 563 5 1 2560 105 38 5735 0 833 3 2][346 126 2129 7 346 126 8 1867 1075 7...]...]\n",
            "targets[[14 7 3 2 29 28 13 152 54 4 249 1 4604 605 9 1 5761 56 26 1][58 1823 764 5 1098 974 193 10 269 563 5 1 2560 105 38 5735 0 833 3 2][346 126 2129 7 346 126 8 1867 1075 7...]...]\n",
            "global_step: 1467\n",
            " perplexity: 686.582\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[14 7 3 2 29 28 13 152 54 4 249 1 4604 605 9 1 5761 56 26 1][58 1823 764 5 1098 974 193 10 269 563 5 1 2560 105 38 5735 0 833 3 2][346 126 2129 7 346 126 8 1867 1075 7...]...]\n",
            " percent of 3-grams captured: 0.156.\n",
            "targets[[14 7 3 2 29 28 13 152 54 4 249 1 4604 605 9 1 5761 56 26 1][58 1823 764 5 1098 974 193 10 269 563 5 1 2560 105 38 5735 0 833 3 2][346 126 2129 7 346 126 8 1867 1075 7...]...]\n",
            " percent of 2-grams captured: 0.474.\n",
            "targets[[14 7 3 2 29 28 13 152 54 4 249 1 4604 605 9 1 5761 56 26 1][58 1823 764 5 1098 974 193 10 269 563 5 1 2560 105 38 5735 0 833 3 2][346 126 2129 7 346 126 8 1867 1075 7...]...]\n",
            " percent of 4-grams captured: 0.044.\n",
            " geometric_avg: 0.148.\n",
            " arithmetic_avg: 0.224.\n",
            "global_step: 1467\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69638\n",
            " G train loss: 3.29760\n",
            "targets[[14 7 3 2 29 28 13 152 54 4 249 1 4604 605 9 1 5761 56 26 1][58 1823 764 5 1098 974 193 10 269 563 5 1 2560 105 38 5735 0 833 3 2][346 126 2129 7 346 126 8 1867 1075 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2406 14 7 3 2 29 28 13 152 54...]...][[0 1 0 0 1 1 0 0 0 1...]...][[2406 10000 7 10000 10000 29 28 10000 10000 10000...]...][[14 7 3 2 29 28 13 152 54 4...]...]\n",
            " Sample 0.\n",
            "   [0]  it                    0.522   4.773  \n",
            "   [1]  in                    0.522   0.000  \n",
            "   [0]  N                     0.526   3.687  \n",
            "   [0]  <eos>                 0.533   3.614  \n",
            "   [1]  but                   0.537   0.000  \n",
            "   [1]  he                    0.539   0.000  \n",
            "   [0]  is                    0.538   4.259  \n",
            "   [0]  just                  0.536   6.061  \n",
            "   [0]  one                   0.534   5.544  \n",
            "   [1]  of                    0.532   0.000  \n",
            "   [1]  several               0.531   0.000  \n",
            "   [1]  <unk>                 0.530   0.000  \n",
            "   [0]  writers               0.529   10.682 \n",
            "   [1]  tokyo                 0.527   0.000  \n",
            "   [1]  's                    0.525   0.000  \n",
            "   [0]  <unk>                 0.524   3.000  \n",
            "   [1]  pack                  0.521   0.000  \n",
            "   [1]  who                   0.521   0.000  \n",
            "   [0]  are                   0.521   5.566  \n",
            "   [1]  <unk>                 0.521   0.000  \n",
            " Sample 1.\n",
            "   [0]  been                  0.502   6.109  \n",
            "   [1]  losing                0.495   0.000  \n",
            "   [1]  clients               0.495   0.000  \n",
            "   [0]  to                    0.498   3.380  \n",
            "   [0]  giant                 0.503   8.288  \n",
            "   [0]  stock-index           0.506   9.840  \n",
            "   [1]  funds                 0.512   0.000  \n",
            "   [0]  that                  0.516   3.846  \n",
            "   [1]  use                   0.523   0.000  \n",
            "   [1]  computers             0.526   0.000  \n",
            "   [1]  to                    0.527   0.000  \n",
            "   [0]  <unk>                 0.528   3.292  \n",
            "   [0]  portfolios            0.529   10.133 \n",
            "   [0]  so                    0.526   6.732  \n",
            "   [0]  they                  0.522   5.126  \n",
            "   [0]  mirror                0.520   12.219 \n",
            "   [1]  the                   0.518   0.000  \n",
            "   [0]  s&p                   0.517   11.794 \n",
            "   [1]  N                     0.515   0.000  \n",
            "   [0]  <eos>                 0.515   2.928  \n",
            " Sample 2.\n",
            "   [1]  four                  0.463   0.000  \n",
            "   [0]  months                0.459   6.212  \n",
            "   [0]  fallen                0.456   11.492 \n",
            "   [1]  in                    0.458   0.000  \n",
            "   [0]  four                  0.465   7.271  \n",
            "   [0]  months                0.473   6.301  \n",
            "   [0]  and                   0.486   2.916  \n",
            "   [0]  remained              0.495   8.817  \n",
            "   [1]  unchanged             0.499   0.000  \n",
            "   [1]  in                    0.499   0.000  \n",
            "   [1]  the                   0.497   0.000  \n",
            "   [1]  other                 0.495   0.000  \n",
            "   [1]  month                 0.491   0.000  \n",
            "   [1]  <eos>                 0.485   0.000  \n",
            "   [0]  in                    0.482   4.274  \n",
            "   [0]  another               0.486   7.176  \n",
            "   [1]  report                0.492   0.000  \n",
            "   [0]  yesterday             0.495   7.793  \n",
            "   [1]  the                   0.494   0.000  \n",
            "   [0]  commerce              0.493   6.863  \n",
            "Samples\n",
            "Sample 0 .  it in N <eos> but he is just one of several <unk> writers tokyo 's <unk> pack who are <unk>\n",
            "Sample 1 .  been losing clients to giant stock-index funds that use computers to <unk> portfolios so they mirror the s&p N <eos>\n",
            "Sample 2 .  four months fallen in four months and remained unchanged in the other month <eos> in another report yesterday the commerce\n",
            "\n",
            "\n",
            "targets[[1 2 7 513 1 9 110 1 34 5449 3 1975 12 3 0 1 437 229 5 1778][4 73 1 60 744 13 416 17 216 193 2 0 35 330 321 4 563 8 5591 77][7 397 2 0 1249 46 782 1 7 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 1 2 7 513 1 9 110 1 34...]...][[0 0 1 0 0 1 0 1 1 1...]...][[4 10000 10000 7 10000 10000 9 10000 1 34...]...][[1 2 7 513 1 9 110 1 34 5449...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 1 2 7 513 1 9 110 1 34...]...][[0 0 1 0 0 1 0 1 1 1...]...][[4 10000 10000 7 10000 10000 9 10000 1 34...]...][[1 2 7 513 1 9 110 1 34 5449...]...]\n",
            "I0212 04:15:17.313179 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[0 1 7923 7 203 2 51 1779 26 2047 7 1 5350 2135 8 1031 1526 3601 1 4][0 1 525 86 6 261 9246 1040 12 3 1858 2 325 43 12 3 48 36 3 3][234 15 81 4 35 7708 1328 1621 3 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 0 1 7923 7 203 2 51 1779 26...]...][[1 0 1 1 1 1 1 0 0 1...]...][[1 0 10000 7923 7 203 2 51 10000 10000...]...][[0 1 7923 7 203 2 51 1779 26 2047...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 0 1 7923 7 203 2 51 1779 26...]...][[1 0 1 1 1 1 1 0 0 1...]...][[1 0 10000 7923 7 203 2 51 10000 10000...]...][[0 1 7923 7 203 2 51 1779 26 2047...]...]\n",
            "targets[[0 1 7923 7 203 2 51 1779 26 2047 7 1 5350 2135 8 1031 1526 3601 1 4][0 1 525 86 6 261 9246 1040 12 3 1858 2 325 43 12 3 48 36 3 3][234 15 81 4 35 7708 1328 1621 3 3...]...]\n",
            "targets[[0 1 7923 7 203 2 51 1779 26 2047 7 1 5350 2135 8 1031 1526 3601 1 4][0 1 525 86 6 261 9246 1040 12 3 1858 2 325 43 12 3 48 36 3 3][234 15 81 4 35 7708 1328 1621 3 3...]...]\n",
            "targets[[0 1 7923 7 203 2 51 1779 26 2047 7 1 5350 2135 8 1031 1526 3601 1 4][0 1 525 86 6 261 9246 1040 12 3 1858 2 325 43 12 3 48 36 3 3][234 15 81 4 35 7708 1328 1621 3 3...]...]\n",
            "global_step: 1470\n",
            " perplexity: 685.476\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[0 1 7923 7 203 2 51 1779 26 2047 7 1 5350 2135 8 1031 1526 3601 1 4][0 1 525 86 6 261 9246 1040 12 3 1858 2 325 43 12 3 48 36 3 3][234 15 81 4 35 7708 1328 1621 3 3...]...]\n",
            " percent of 3-grams captured: 0.194.\n",
            "targets[[0 1 7923 7 203 2 51 1779 26 2047 7 1 5350 2135 8 1031 1526 3601 1 4][0 1 525 86 6 261 9246 1040 12 3 1858 2 325 43 12 3 48 36 3 3][234 15 81 4 35 7708 1328 1621 3 3...]...]\n",
            " percent of 2-grams captured: 0.453.\n",
            "targets[[0 1 7923 7 203 2 51 1779 26 2047 7 1 5350 2135 8 1031 1526 3601 1 4][0 1 525 86 6 261 9246 1040 12 3 1858 2 325 43 12 3 48 36 3 3][234 15 81 4 35 7708 1328 1621 3 3...]...]\n",
            " percent of 4-grams captured: 0.091.\n",
            " geometric_avg: 0.200.\n",
            " arithmetic_avg: 0.246.\n",
            "global_step: 1470\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69637\n",
            " G train loss: 3.29716\n",
            "targets[[0 1 7923 7 203 2 51 1779 26 2047 7 1 5350 2135 8 1031 1526 3601 1 4][0 1 525 86 6 261 9246 1040 12 3 1858 2 325 43 12 3 48 36 3 3][234 15 81 4 35 7708 1328 1621 3 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 0 1 7923 7 203 2 51 1779 26...]...][[1 0 1 1 1 1 1 0 0 1...]...][[1 0 10000 7923 7 203 2 51 10000 10000...]...][[0 1 7923 7 203 2 51 1779 26 2047...]...]\n",
            " Sample 0.\n",
            "   [1]  the                   0.537   0.000  \n",
            "   [0]  <unk>                 0.536   2.822  \n",
            "   [1]  charts                0.535   0.000  \n",
            "   [1]  in                    0.539   0.000  \n",
            "   [1]  japan                 0.543   0.000  \n",
            "   [1]  <eos>                 0.549   0.000  \n",
            "   [1]  their                 0.553   0.000  \n",
            "   [0]  books                 0.555   8.907  \n",
            "   [0]  are                   0.554   5.285  \n",
            "   [1]  written               0.553   0.000  \n",
            "   [1]  in                    0.555   0.000  \n",
            "   [1]  <unk>                 0.558   0.000  \n",
            "   [0]  contemporary          0.560   14.292 \n",
            "   [1]  language              0.561   0.000  \n",
            "   [0]  and                   0.565   2.918  \n",
            "   [1]  usually               0.568   0.000  \n",
            "   [0]  carry                 0.567   10.876 \n",
            "   [1]  hefty                 0.565   0.000  \n",
            "   [1]  <unk>                 0.563   0.000  \n",
            "   [0]  of                    0.564   2.782  \n",
            " Sample 1.\n",
            "   [0]  the                   0.464   2.325  \n",
            "   [0]  <unk>                 0.465   4.032  \n",
            "   [1]  charge                0.467   0.000  \n",
            "   [1]  only                  0.469   0.000  \n",
            "   [0]  a                     0.473   3.462  \n",
            "   [1]  few                   0.477   0.000  \n",
            "   [0]  pennies               0.478   11.077 \n",
            "   [0]  per                   0.479   8.794  \n",
            "   [0]  $                     0.477   5.745  \n",
            "   [0]  N                     0.476   0.059  \n",
            "   [1]  managed               0.476   0.000  \n",
            "   [1]  <eos>                 0.479   0.000  \n",
            "   [0]  today                 0.480   7.664  \n",
            "   [1]  about                 0.481   0.000  \n",
            "   [1]  $                     0.481   0.000  \n",
            "   [0]  N                     0.482   0.034  \n",
            "   [0]  billion               0.485   1.142  \n",
            "   [0]  or                    0.486   3.722  \n",
            "   [0]  N                     0.485   1.635  \n",
            "   [0]  N                     0.483   1.386  \n",
            " Sample 2.\n",
            "   [0]  department            0.516   7.321  \n",
            "   [0]  said                  0.509   5.118  \n",
            "   [0]  sales                 0.513   7.280  \n",
            "   [0]  of                    0.526   3.893  \n",
            "   [0]  new                   0.540   5.243  \n",
            "   [1]  single-family         0.552   0.000  \n",
            "   [1]  houses                0.559   0.000  \n",
            "   [1]  plunged               0.563   0.000  \n",
            "   [1]  N                     0.563   0.000  \n",
            "   [0]  N                     0.562   2.175  \n",
            "   [1]  in                    0.559   0.000  \n",
            "   [1]  september             0.555   0.000  \n",
            "   [0]  to                    0.551   3.418  \n",
            "   [0]  an                    0.547   5.243  \n",
            "   [0]  annual                0.543   6.148  \n",
            "   [0]  rate                  0.544   6.530  \n",
            "   [1]  of                    0.549   0.000  \n",
            "   [0]  N                     0.552   1.771  \n",
            "   [0]  from                  0.555   4.122  \n",
            "   [0]  N                     0.556   1.215  \n",
            "Samples\n",
            "Sample 0 .  the <unk> charts in japan <eos> their books are written in <unk> contemporary language and usually carry hefty <unk> of\n",
            "Sample 1 .  the <unk> charge only a few pennies per $ N managed <eos> today about $ N billion or N N\n",
            "Sample 2 .  department said sales of new single-family houses plunged N N in september to an annual rate of N from N\n",
            "\n",
            "\n",
            "targets[[7 0 1 1350 64 42 25 710 335 5 786 6 1085 2 19 23 1 4672 14 1][7184 5 138 1 4 0 3 129 146 1 365 2 144 300 29 4722 1022 949 351 34][2216 2 366 311 9 1836 1159 75 404 126...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1778 7 0 1 1350 64 42 25 710 335...]...][[0 0 0 1 1 0 0 1 1 0...]...][[1778 10000 10000 10000 1350 64 10000 10000 710 335...]...][[7 0 1 1350 64 42 25 710 335 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1778 7 0 1 1350 64 42 25 710 335...]...][[0 0 0 1 1 0 0 1 1 0...]...][[1778 10000 10000 10000 1350 64 10000 10000 710 335...]...][[7 0 1 1350 64 42 25 710 335 5...]...]\n",
            "I0212 04:15:20.647186 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1 2 7 513 1 9 110 1 34 5449 3 1975 12 3 0 1 437 229 5 1778][4 73 1 60 744 13 416 17 216 193 2 0 35 330 321 4 563 8 5591 77][7 397 2 0 1249 46 782 1 7 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 1 2 7 513 1 9 110 1 34...]...][[0 1 1 0 0 1 0 1 0 0...]...][[4 10000 2 7 10000 10000 9 10000 1 10000...]...][[1 2 7 513 1 9 110 1 34 5449...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 1 2 7 513 1 9 110 1 34...]...][[0 1 1 0 0 1 0 1 0 0...]...][[4 10000 2 7 10000 10000 9 10000 1 10000...]...][[1 2 7 513 1 9 110 1 34 5449...]...]\n",
            "targets[[1 2 7 513 1 9 110 1 34 5449 3 1975 12 3 0 1 437 229 5 1778][4 73 1 60 744 13 416 17 216 193 2 0 35 330 321 4 563 8 5591 77][7 397 2 0 1249 46 782 1 7 0...]...]\n",
            "targets[[1 2 7 513 1 9 110 1 34 5449 3 1975 12 3 0 1 437 229 5 1778][4 73 1 60 744 13 416 17 216 193 2 0 35 330 321 4 563 8 5591 77][7 397 2 0 1249 46 782 1 7 0...]...]\n",
            "targets[[1 2 7 513 1 9 110 1 34 5449 3 1975 12 3 0 1 437 229 5 1778][4 73 1 60 744 13 416 17 216 193 2 0 35 330 321 4 563 8 5591 77][7 397 2 0 1249 46 782 1 7 0...]...]\n",
            "global_step: 1473\n",
            " perplexity: 684.283\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1 2 7 513 1 9 110 1 34 5449 3 1975 12 3 0 1 437 229 5 1778][4 73 1 60 744 13 416 17 216 193 2 0 35 330 321 4 563 8 5591 77][7 397 2 0 1249 46 782 1 7 0...]...]\n",
            " percent of 3-grams captured: 0.197.\n",
            "targets[[1 2 7 513 1 9 110 1 34 5449 3 1975 12 3 0 1 437 229 5 1778][4 73 1 60 744 13 416 17 216 193 2 0 35 330 321 4 563 8 5591 77][7 397 2 0 1249 46 782 1 7 0...]...]\n",
            " percent of 2-grams captured: 0.479.\n",
            "targets[[1 2 7 513 1 9 110 1 34 5449 3 1975 12 3 0 1 437 229 5 1778][4 73 1 60 744 13 416 17 216 193 2 0 35 330 321 4 563 8 5591 77][7 397 2 0 1249 46 782 1 7 0...]...]\n",
            " percent of 4-grams captured: 0.088.\n",
            " geometric_avg: 0.203.\n",
            " arithmetic_avg: 0.255.\n",
            "global_step: 1473\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69637\n",
            " G train loss: 3.29576\n",
            "targets[[1 2 7 513 1 9 110 1 34 5449 3 1975 12 3 0 1 437 229 5 1778][4 73 1 60 744 13 416 17 216 193 2 0 35 330 321 4 563 8 5591 77][7 397 2 0 1249 46 782 1 7 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 1 2 7 513 1 9 110 1 34...]...][[0 1 1 0 0 1 0 1 0 0...]...][[4 10000 2 7 10000 10000 9 10000 1 10000...]...][[1 2 7 513 1 9 110 1 34 5449...]...]\n",
            " Sample 0.\n",
            "   [0]  <unk>                 0.489   4.234  \n",
            "   [1]  <eos>                 0.500   0.000  \n",
            "   [1]  in                    0.513   0.000  \n",
            "   [0]  robert                0.526   8.331  \n",
            "   [0]  <unk>                 0.537   3.076  \n",
            "   [1]  's                    0.545   0.000  \n",
            "   [0]  you                   0.550   6.783  \n",
            "   [1]  <unk>                 0.549   0.000  \n",
            "   [0]  have                  0.549   5.304  \n",
            "   [0]  macmillan             0.545   10.122 \n",
            "   [1]  N                     0.543   0.000  \n",
            "   [1]  pages                 0.541   0.000  \n",
            "   [0]  $                     0.539   5.924  \n",
            "   [0]  N                     0.541   0.222  \n",
            "   [0]  the                   0.544   4.634  \n",
            "   [0]  <unk>                 0.547   4.140  \n",
            "   [0]  give                  0.546   9.540  \n",
            "   [1]  way                   0.543   0.000  \n",
            "   [0]  to                    0.541   3.553  \n",
            "   [1]  baseball              0.542   0.000  \n",
            " Sample 1.\n",
            "   [1]  of                    0.471   0.000  \n",
            "   [0]  all                   0.463   6.504  \n",
            "   [1]  <unk>                 0.461   0.000  \n",
            "   [0]  stock                 0.467   5.755  \n",
            "   [0]  investments           0.476   9.662  \n",
            "   [1]  is                    0.486   0.000  \n",
            "   [1]  held                  0.495   0.000  \n",
            "   [1]  by                    0.502   0.000  \n",
            "   [1]  index                 0.504   0.000  \n",
            "   [1]  funds                 0.505   0.000  \n",
            "   [0]  <eos>                 0.504   1.890  \n",
            "   [0]  the                   0.503   1.294  \n",
            "   [0]  new                   0.501   4.582  \n",
            "   [0]  wall                  0.499   9.324  \n",
            "   [1]  street                0.496   0.000  \n",
            "   [0]  of                    0.497   1.791  \n",
            "   [0]  computers             0.495   8.598  \n",
            "   [1]  and                   0.497   0.000  \n",
            "   [1]  automated             0.499   0.000  \n",
            "   [1]  trading               0.500   0.000  \n",
            " Sample 2.\n",
            "   [0]  in                    0.453   4.817  \n",
            "   [0]  august                0.456   5.842  \n",
            "   [0]  <eos>                 0.462   2.731  \n",
            "   [1]  the                   0.468   0.000  \n",
            "   [0]  declines              0.475   7.887  \n",
            "   [0]  were                  0.482   5.993  \n",
            "   [0]  particularly          0.486   9.853  \n",
            "   [0]  <unk>                 0.487   2.724  \n",
            "   [1]  in                    0.487   0.000  \n",
            "   [1]  the                   0.487   0.000  \n",
            "   [1]  northeast             0.486   0.000  \n",
            "   [0]  and                   0.486   3.339  \n",
            "   [0]  in                    0.484   4.524  \n",
            "   [0]  the                   0.483   1.381  \n",
            "   [0]  south                 0.480   8.637  \n",
            "   [0]  where                 0.478   7.104  \n",
            "   [0]  hurricane             0.477   8.658  \n",
            "   [1]  hugo                  0.475   0.000  \n",
            "   [0]  was                   0.474   5.109  \n",
            "   [1]  a                     0.474   0.000  \n",
            "Samples\n",
            "Sample 0 .  <unk> <eos> in robert <unk> 's you <unk> have macmillan N pages $ N the <unk> give way to baseball\n",
            "Sample 1 .  of all <unk> stock investments is held by index funds <eos> the new wall street of computers and automated trading\n",
            "Sample 2 .  in august <eos> the declines were particularly <unk> in the northeast and in the south where hurricane hugo was a\n",
            "\n",
            "\n",
            "targets[[1778 13 6 5735 4 203 9 1 1 4 710 221 8 9008 2 1 13 173 11 773][1019 3699 981 4 3 3 5 3 3 6 40 16 51 180 17 8525 4 51 7561 7][470 7 582 172 133 39 40 30 50 86...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1778 13 6 5735 4 203 9 1 1...]...][[0 0 0 1 1 1 1 0 1 1...]...][[1 10000 10000 10000 5735 4 203 9 10000 1...]...][[1778 13 6 5735 4 203 9 1 1 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1778 13 6 5735 4 203 9 1 1...]...][[0 0 0 1 1 1 1 0 1 1...]...][[1 10000 10000 10000 5735 4 203 9 10000 1...]...][[1778 13 6 5735 4 203 9 1 1 4...]...]\n",
            "I0212 04:15:23.956815 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[7 0 1 1350 64 42 25 710 335 5 786 6 1085 2 19 23 1 4672 14 1][7184 5 138 1 4 0 3 129 146 1 365 2 144 300 29 4722 1022 949 351 34][2216 2 366 311 9 1836 1159 75 404 126...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1778 7 0 1 1350 64 42 25 710 335...]...][[1 0 0 1 0 1 0 0 0 0...]...][[1778 7 10000 10000 1350 10000 42 10000 10000 10000...]...][[7 0 1 1350 64 42 25 710 335 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1778 7 0 1 1350 64 42 25 710 335...]...][[1 0 0 1 0 1 0 0 0 0...]...][[1778 7 10000 10000 1350 10000 42 10000 10000 10000...]...][[7 0 1 1350 64 42 25 710 335 5...]...]\n",
            "targets[[7 0 1 1350 64 42 25 710 335 5 786 6 1085 2 19 23 1 4672 14 1][7184 5 138 1 4 0 3 129 146 1 365 2 144 300 29 4722 1022 949 351 34][2216 2 366 311 9 1836 1159 75 404 126...]...]\n",
            "targets[[7 0 1 1350 64 42 25 710 335 5 786 6 1085 2 19 23 1 4672 14 1][7184 5 138 1 4 0 3 129 146 1 365 2 144 300 29 4722 1022 949 351 34][2216 2 366 311 9 1836 1159 75 404 126...]...]\n",
            "targets[[7 0 1 1350 64 42 25 710 335 5 786 6 1085 2 19 23 1 4672 14 1][7184 5 138 1 4 0 3 129 146 1 365 2 144 300 29 4722 1022 949 351 34][2216 2 366 311 9 1836 1159 75 404 126...]...]\n",
            "global_step: 1476\n",
            " perplexity: 684.055\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[7 0 1 1350 64 42 25 710 335 5 786 6 1085 2 19 23 1 4672 14 1][7184 5 138 1 4 0 3 129 146 1 365 2 144 300 29 4722 1022 949 351 34][2216 2 366 311 9 1836 1159 75 404 126...]...]\n",
            " percent of 3-grams captured: 0.186.\n",
            "targets[[7 0 1 1350 64 42 25 710 335 5 786 6 1085 2 19 23 1 4672 14 1][7184 5 138 1 4 0 3 129 146 1 365 2 144 300 29 4722 1022 949 351 34][2216 2 366 311 9 1836 1159 75 404 126...]...]\n",
            " percent of 2-grams captured: 0.468.\n",
            "targets[[7 0 1 1350 64 42 25 710 335 5 786 6 1085 2 19 23 1 4672 14 1][7184 5 138 1 4 0 3 129 146 1 365 2 144 300 29 4722 1022 949 351 34][2216 2 366 311 9 1836 1159 75 404 126...]...]\n",
            " percent of 4-grams captured: 0.074.\n",
            " geometric_avg: 0.186.\n",
            " arithmetic_avg: 0.243.\n",
            "global_step: 1476\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69636\n",
            " G train loss: 3.29609\n",
            "targets[[7 0 1 1350 64 42 25 710 335 5 786 6 1085 2 19 23 1 4672 14 1][7184 5 138 1 4 0 3 129 146 1 365 2 144 300 29 4722 1022 949 351 34][2216 2 366 311 9 1836 1159 75 404 126...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1778 7 0 1 1350 64 42 25 710 335...]...][[1 0 0 1 0 1 0 0 0 0...]...][[1778 7 10000 10000 1350 10000 42 10000 10000 10000...]...][[7 0 1 1350 64 42 25 710 335 5...]...]\n",
            " Sample 0.\n",
            "   [1]  in                    0.416   0.000  \n",
            "   [0]  the                   0.420   3.429  \n",
            "   [0]  <unk>                 0.430   3.050  \n",
            "   [1]  version               0.440   0.000  \n",
            "   [0]  we                    0.445   6.829  \n",
            "   [1]  would                 0.448   0.000  \n",
            "   [0]  be                    0.450   5.576  \n",
            "   [0]  hard                  0.448   7.820  \n",
            "   [0]  put                   0.445   7.538  \n",
            "   [0]  to                    0.444   4.210  \n",
            "   [1]  call                  0.444   0.000  \n",
            "   [0]  a                     0.448   3.863  \n",
            "   [0]  game                  0.450   9.724  \n",
            "   [1]  <eos>                 0.453   0.000  \n",
            "   [0]  as                    0.453   6.260  \n",
            "   [1]  mr.                   0.451   0.000  \n",
            "   [1]  <unk>                 0.451   0.000  \n",
            "   [0]  describes             0.450   11.849 \n",
            "   [0]  it                    0.450   5.185  \n",
            "   [0]  <unk>                 0.452   4.161  \n",
            " Sample 1.\n",
            "   [1]  threatens             0.463   0.000  \n",
            "   [1]  to                    0.464   0.000  \n",
            "   [1]  make                  0.468   0.000  \n",
            "   [0]  <unk>                 0.475   4.347  \n",
            "   [1]  of                    0.480   0.000  \n",
            "   [1]  the                   0.482   0.000  \n",
            "   [0]  N                     0.482   6.043  \n",
            "   [1]  big                   0.483   0.000  \n",
            "   [0]  board                 0.485   5.706  \n",
            "   [0]  <unk>                 0.487   4.149  \n",
            "   [0]  firms                 0.488   8.206  \n",
            "   [1]  <eos>                 0.488   0.000  \n",
            "   [1]  these                 0.487   0.000  \n",
            "   [0]  small                 0.487   7.421  \n",
            "   [1]  but                   0.487   0.000  \n",
            "   [1]  influential           0.487   0.000  \n",
            "   [0]  floor                 0.486   8.271  \n",
            "   [0]  brokers               0.486   9.772  \n",
            "   [0]  long                  0.484   7.768  \n",
            "   [1]  have                  0.484   0.000  \n",
            " Sample 2.\n",
            "   [1]  factor                0.482   0.000  \n",
            "   [1]  <eos>                 0.476   0.000  \n",
            "   [1]  although              0.474   0.000  \n",
            "   [0]  september             0.479   7.224  \n",
            "   [0]  's                    0.481   5.203  \n",
            "   [0]  weakness              0.486   10.129 \n",
            "   [1]  followed              0.492   0.000  \n",
            "   [0]  two                   0.498   7.252  \n",
            "   [0]  strong                0.501   7.697  \n",
            "   [0]  months                0.504   6.913  \n",
            "   [0]  for                   0.505   3.798  \n",
            "   [1]  home                  0.506   0.000  \n",
            "   [0]  sales                 0.504   7.364  \n",
            "   [1]  the                   0.499   0.000  \n",
            "   [1]  decline               0.495   0.000  \n",
            "   [0]  supports              0.496   13.380 \n",
            "   [1]  other                 0.497   0.000  \n",
            "   [0]  indications           0.496   9.737  \n",
            "   [0]  that                  0.497   4.368  \n",
            "   [1]  the                   0.495   0.000  \n",
            "Samples\n",
            "Sample 0 .  in the <unk> version we would be hard put to call a game <eos> as mr. <unk> describes it <unk>\n",
            "Sample 1 .  threatens to make <unk> of the N big board <unk> firms <eos> these small but influential floor brokers long have\n",
            "Sample 2 .  factor <eos> although september 's weakness followed two strong months for home sales the decline supports other indications that the\n",
            "\n",
            "\n",
            "targets[[4576 8 173 1 34 1358 8 1358 4 14 2 6 2559 9 2341 5 1361 8 773 1341][423 190 7 734 149 2 0 2198 378 106 1213 5 1289 77 19 6 1233 1 2 8][311 470 24 0 413 155 6 3 3 470...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[773 4576 8 173 1 34 1358 8 1358 4...]...][[1 1 0 1 0 1 0 1 1 0...]...][[773 4576 8 10000 1 10000 1358 10000 1358 4...]...][[4576 8 173 1 34 1358 8 1358 4 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[773 4576 8 173 1 34 1358 8 1358 4...]...][[1 1 0 1 0 1 0 1 1 0...]...][[773 4576 8 10000 1 10000 1358 10000 1358 4...]...][[4576 8 173 1 34 1358 8 1358 4 14...]...]\n",
            "I0212 04:15:27.399241 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1778 13 6 5735 4 203 9 1 1 4 710 221 8 9008 2 1 13 173 11 773][1019 3699 981 4 3 3 5 3 3 6 40 16 51 180 17 8525 4 51 7561 7][470 7 582 172 133 39 40 30 50 86...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1778 13 6 5735 4 203 9 1 1...]...][[1 0 1 0 1 1 1 0 0 1...]...][[1 1778 10000 6 10000 4 203 9 10000 10000...]...][[1778 13 6 5735 4 203 9 1 1 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1778 13 6 5735 4 203 9 1 1...]...][[1 0 1 0 1 1 1 0 0 1...]...][[1 1778 10000 6 10000 4 203 9 10000 10000...]...][[1778 13 6 5735 4 203 9 1 1 4...]...]\n",
            "targets[[1778 13 6 5735 4 203 9 1 1 4 710 221 8 9008 2 1 13 173 11 773][1019 3699 981 4 3 3 5 3 3 6 40 16 51 180 17 8525 4 51 7561 7][470 7 582 172 133 39 40 30 50 86...]...]\n",
            "targets[[1778 13 6 5735 4 203 9 1 1 4 710 221 8 9008 2 1 13 173 11 773][1019 3699 981 4 3 3 5 3 3 6 40 16 51 180 17 8525 4 51 7561 7][470 7 582 172 133 39 40 30 50 86...]...]\n",
            "targets[[1778 13 6 5735 4 203 9 1 1 4 710 221 8 9008 2 1 13 173 11 773][1019 3699 981 4 3 3 5 3 3 6 40 16 51 180 17 8525 4 51 7561 7][470 7 582 172 133 39 40 30 50 86...]...]\n",
            "global_step: 1479\n",
            " perplexity: 683.493\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1778 13 6 5735 4 203 9 1 1 4 710 221 8 9008 2 1 13 173 11 773][1019 3699 981 4 3 3 5 3 3 6 40 16 51 180 17 8525 4 51 7561 7][470 7 582 172 133 39 40 30 50 86...]...]\n",
            " percent of 3-grams captured: 0.164.\n",
            "targets[[1778 13 6 5735 4 203 9 1 1 4 710 221 8 9008 2 1 13 173 11 773][1019 3699 981 4 3 3 5 3 3 6 40 16 51 180 17 8525 4 51 7561 7][470 7 582 172 133 39 40 30 50 86...]...]\n",
            " percent of 2-grams captured: 0.455.\n",
            "targets[[1778 13 6 5735 4 203 9 1 1 4 710 221 8 9008 2 1 13 173 11 773][1019 3699 981 4 3 3 5 3 3 6 40 16 51 180 17 8525 4 51 7561 7][470 7 582 172 133 39 40 30 50 86...]...]\n",
            " percent of 4-grams captured: 0.065.\n",
            " geometric_avg: 0.169.\n",
            " arithmetic_avg: 0.228.\n",
            "global_step: 1479\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69633\n",
            " G train loss: 3.29586\n",
            "targets[[1778 13 6 5735 4 203 9 1 1 4 710 221 8 9008 2 1 13 173 11 773][1019 3699 981 4 3 3 5 3 3 6 40 16 51 180 17 8525 4 51 7561 7][470 7 582 172 133 39 40 30 50 86...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1778 13 6 5735 4 203 9 1 1...]...][[1 0 1 0 1 1 1 0 0 1...]...][[1 1778 10000 6 10000 4 203 9 10000 10000...]...][[1778 13 6 5735 4 203 9 1 1 4...]...]\n",
            " Sample 0.\n",
            "   [1]  baseball              0.459   0.000  \n",
            "   [0]  is                    0.451   4.691  \n",
            "   [1]  a                     0.446   0.000  \n",
            "   [0]  mirror                0.444   11.141 \n",
            "   [1]  of                    0.444   0.000  \n",
            "   [1]  japan                 0.444   0.000  \n",
            "   [1]  's                    0.444   0.000  \n",
            "   [0]  <unk>                 0.447   2.430  \n",
            "   [0]  <unk>                 0.451   3.307  \n",
            "   [1]  of                    0.454   0.000  \n",
            "   [1]  hard                  0.451   0.000  \n",
            "   [0]  work                  0.450   8.095  \n",
            "   [0]  and                   0.454   3.278  \n",
            "   [1]  harmony               0.457   0.000  \n",
            "   [0]  <eos>                 0.456   3.259  \n",
            "   [1]  <unk>                 0.457   0.000  \n",
            "   [0]  is                    0.459   4.482  \n",
            "   [1]  japanese              0.458   0.000  \n",
            "   [1]  for                   0.457   0.000  \n",
            "   [1]  team                  0.457   0.000  \n",
            " Sample 1.\n",
            "   [1]  earned                0.555   0.000  \n",
            "   [0]  fat                   0.554   12.217 \n",
            "   [0]  returns               0.557   9.878  \n",
            "   [0]  of                    0.561   3.576  \n",
            "   [0]  N                     0.563   3.616  \n",
            "   [0]  N                     0.567   0.603  \n",
            "   [0]  to                    0.570   2.828  \n",
            "   [1]  N                     0.574   0.000  \n",
            "   [0]  N                     0.577   0.491  \n",
            "   [1]  a                     0.582   0.000  \n",
            "   [1]  year                  0.585   0.000  \n",
            "   [1]  on                    0.582   0.000  \n",
            "   [1]  their                 0.577   0.000  \n",
            "   [1]  capital               0.571   0.000  \n",
            "   [1]  by                    0.568   0.000  \n",
            "   [1]  virtue                0.565   0.000  \n",
            "   [0]  of                    0.563   3.553  \n",
            "   [1]  their                 0.559   0.000  \n",
            "   [1]  monopoly              0.556   0.000  \n",
            "   [1]  in                    0.557   0.000  \n",
            " Sample 2.\n",
            "   [0]  drop                  0.525   6.775  \n",
            "   [1]  in                    0.527   0.000  \n",
            "   [1]  mortgage              0.531   0.000  \n",
            "   [0]  rates                 0.540   7.261  \n",
            "   [1]  earlier               0.548   0.000  \n",
            "   [0]  this                  0.551   5.732  \n",
            "   [1]  year                  0.552   0.000  \n",
            "   [0]  has                   0.550   5.415  \n",
            "   [1]  had                   0.549   0.000  \n",
            "   [1]  only                  0.546   0.000  \n",
            "   [1]  a                     0.545   0.000  \n",
            "   [0]  limited               0.545   7.617  \n",
            "   [1]  beneficial            0.544   0.000  \n",
            "   [0]  effect                0.544   8.523  \n",
            "   [0]  on                    0.541   5.211  \n",
            "   [1]  the                   0.539   0.000  \n",
            "   [0]  housing               0.540   9.012  \n",
            "   [1]  market                0.541   0.000  \n",
            "   [1]  <eos>                 0.541   0.000  \n",
            "   [0]  the                   0.539   2.219  \n",
            "Samples\n",
            "Sample 0 .  baseball is a mirror of japan 's <unk> <unk> of hard work and harmony <eos> <unk> is japanese for team\n",
            "Sample 1 .  earned fat returns of N N to N N a year on their capital by virtue of their monopoly in\n",
            "Sample 2 .  drop in mortgage rates earlier this year has had only a limited beneficial effect on the housing market <eos> the\n",
            "\n",
            "\n",
            "targets[[13 19 589 19 49 1 196 2 3306 496 521 605 2172 1832 1 1 6 1 1 1][38 620 0 129 146 124 23 2189 30 2831 51 131 2 0 2459 4 6 2569 8 496][113 245 1 55 150 7 61 233 3092 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1341 13 19 589 19 49 1 196 2 3306...]...][[1 0 1 0 1 0 0 0 0 1...]...][[1341 13 10000 589 10000 49 10000 10000 10000 10000...]...][[13 19 589 19 49 1 196 2 3306 496...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1341 13 19 589 19 49 1 196 2 3306...]...][[1 0 1 0 1 0 0 0 0 1...]...][[1341 13 10000 589 10000 49 10000 10000 10000 10000...]...][[13 19 589 19 49 1 196 2 3306 496...]...]\n",
            "I0212 04:15:30.692029 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[4576 8 173 1 34 1358 8 1358 4 14 2 6 2559 9 2341 5 1361 8 773 1341][423 190 7 734 149 2 0 2198 378 106 1213 5 1289 77 19 6 1233 1 2 8][311 470 24 0 413 155 6 3 3 470...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[773 4576 8 173 1 34 1358 8 1358 4...]...][[1 0 0 1 0 0 0 0 1 0...]...][[773 4576 10000 10000 1 10000 10000 10000 10000 4...]...][[4576 8 173 1 34 1358 8 1358 4 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[773 4576 8 173 1 34 1358 8 1358 4...]...][[1 0 0 1 0 0 0 0 1 0...]...][[773 4576 10000 10000 1 10000 10000 10000 10000 4...]...][[4576 8 173 1 34 1358 8 1358 4 14...]...]\n",
            "targets[[4576 8 173 1 34 1358 8 1358 4 14 2 6 2559 9 2341 5 1361 8 773 1341][423 190 7 734 149 2 0 2198 378 106 1213 5 1289 77 19 6 1233 1 2 8][311 470 24 0 413 155 6 3 3 470...]...]\n",
            "targets[[4576 8 173 1 34 1358 8 1358 4 14 2 6 2559 9 2341 5 1361 8 773 1341][423 190 7 734 149 2 0 2198 378 106 1213 5 1289 77 19 6 1233 1 2 8][311 470 24 0 413 155 6 3 3 470...]...]\n",
            "targets[[4576 8 173 1 34 1358 8 1358 4 14 2 6 2559 9 2341 5 1361 8 773 1341][423 190 7 734 149 2 0 2198 378 106 1213 5 1289 77 19 6 1233 1 2 8][311 470 24 0 413 155 6 3 3 470...]...]\n",
            "global_step: 1482\n",
            " perplexity: 683.002\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[4576 8 173 1 34 1358 8 1358 4 14 2 6 2559 9 2341 5 1361 8 773 1341][423 190 7 734 149 2 0 2198 378 106 1213 5 1289 77 19 6 1233 1 2 8][311 470 24 0 413 155 6 3 3 470...]...]\n",
            " percent of 3-grams captured: 0.161.\n",
            "targets[[4576 8 173 1 34 1358 8 1358 4 14 2 6 2559 9 2341 5 1361 8 773 1341][423 190 7 734 149 2 0 2198 378 106 1213 5 1289 77 19 6 1233 1 2 8][311 470 24 0 413 155 6 3 3 470...]...]\n",
            " percent of 2-grams captured: 0.495.\n",
            "targets[[4576 8 173 1 34 1358 8 1358 4 14 2 6 2559 9 2341 5 1361 8 773 1341][423 190 7 734 149 2 0 2198 378 106 1213 5 1289 77 19 6 1233 1 2 8][311 470 24 0 413 155 6 3 3 470...]...]\n",
            " percent of 4-grams captured: 0.038.\n",
            " geometric_avg: 0.145.\n",
            " arithmetic_avg: 0.231.\n",
            "global_step: 1482\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69633\n",
            " G train loss: 3.29542\n",
            "targets[[4576 8 173 1 34 1358 8 1358 4 14 2 6 2559 9 2341 5 1361 8 773 1341][423 190 7 734 149 2 0 2198 378 106 1213 5 1289 77 19 6 1233 1 2 8][311 470 24 0 413 155 6 3 3 470...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[773 4576 8 173 1 34 1358 8 1358 4...]...][[1 0 0 1 0 0 0 0 1 0...]...][[773 4576 10000 10000 1 10000 10000 10000 10000 4...]...][[4576 8 173 1 34 1358 8 1358 4 14...]...]\n",
            " Sample 0.\n",
            "   [1]  spirit                0.505   0.000  \n",
            "   [0]  and                   0.501   3.140  \n",
            "   [0]  japanese              0.501   6.617  \n",
            "   [1]  <unk>                 0.504   0.000  \n",
            "   [0]  have                  0.512   5.433  \n",
            "   [0]  miles                 0.516   8.859  \n",
            "   [0]  and                   0.518   3.080  \n",
            "   [0]  miles                 0.519   9.372  \n",
            "   [1]  of                    0.518   0.000  \n",
            "   [0]  it                    0.518   5.089  \n",
            "   [1]  <eos>                 0.514   0.000  \n",
            "   [1]  a                     0.511   0.000  \n",
            "   [1]  player                0.510   0.000  \n",
            "   [0]  's                    0.510   3.799  \n",
            "   [0]  commitment            0.511   9.693  \n",
            "   [0]  to                    0.512   4.233  \n",
            "   [0]  practice              0.515   8.402  \n",
            "   [1]  and                   0.518   0.000  \n",
            "   [0]  team                  0.520   8.356  \n",
            "   [1]  image                 0.521   0.000  \n",
            " Sample 1.\n",
            "   [1]  making                0.431   0.000  \n",
            "   [0]  markets               0.436   7.257  \n",
            "   [0]  in                    0.444   3.375  \n",
            "   [0]  individual            0.452   8.751  \n",
            "   [1]  stocks                0.459   0.000  \n",
            "   [1]  <eos>                 0.465   0.000  \n",
            "   [1]  the                   0.466   0.000  \n",
            "   [1]  specialists           0.462   0.000  \n",
            "   [1]  see                   0.456   0.000  \n",
            "   [1]  any                   0.451   0.000  \n",
            "   [1]  step                  0.448   0.000  \n",
            "   [0]  to                    0.446   3.450  \n",
            "   [0]  electronic            0.443   8.535  \n",
            "   [1]  trading               0.442   0.000  \n",
            "   [0]  as                    0.440   4.944  \n",
            "   [0]  a                     0.442   2.907  \n",
            "   [1]  death                 0.445   0.000  \n",
            "   [0]  <unk>                 0.447   3.781  \n",
            "   [0]  <eos>                 0.450   2.632  \n",
            "   [0]  and                   0.452   4.995  \n",
            " Sample 2.\n",
            "   [0]  september             0.545   6.095  \n",
            "   [0]  drop                  0.537   8.211  \n",
            "   [1]  was                   0.535   0.000  \n",
            "   [1]  the                   0.538   0.000  \n",
            "   [0]  largest               0.543   6.061  \n",
            "   [1]  since                 0.548   0.000  \n",
            "   [1]  a                     0.553   0.000  \n",
            "   [1]  N                     0.557   0.000  \n",
            "   [0]  N                     0.561   1.863  \n",
            "   [0]  drop                  0.561   7.782  \n",
            "   [0]  in                    0.561   2.616  \n",
            "   [0]  january               0.560   7.530  \n",
            "   [0]  N                     0.560   3.504  \n",
            "   [1]  but                   0.560   0.000  \n",
            "   [1]  monthly               0.557   0.000  \n",
            "   [0]  changes               0.554   8.463  \n",
            "   [1]  in                    0.554   0.000  \n",
            "   [1]  this                  0.556   0.000  \n",
            "   [0]  measure               0.557   8.139  \n",
            "   [1]  are                   0.556   0.000  \n",
            "Samples\n",
            "Sample 0 .  spirit and japanese <unk> have miles and miles of it <eos> a player 's commitment to practice and team image\n",
            "Sample 1 .  making markets in individual stocks <eos> the specialists see any step to electronic trading as a death <unk> <eos> and\n",
            "Sample 2 .  september drop was the largest since a N N drop in january N but monthly changes in this measure are\n",
            "\n",
            "\n",
            "targets[[9825 19 0 3724 3769 4 203 2 29 61 55 0 668 10 1 13 1863 22 6 4627][54 1052 23 2189 30 3294 58 1 22 189 166 0 35 60 2986 10 49 337 2163 8012][234 15 14 13 3 3 2844 86 10 8258...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 9825 19 0 3724 3769 4 203 2 29...]...][[0 0 0 0 1 1 1 1 0 0...]...][[1 10000 10000 10000 10000 3769 4 203 2 10000...]...][[9825 19 0 3724 3769 4 203 2 29 61...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 9825 19 0 3724 3769 4 203 2 29...]...][[0 0 0 0 1 1 1 1 0 0...]...][[1 10000 10000 10000 10000 3769 4 203 2 10000...]...][[9825 19 0 3724 3769 4 203 2 29 61...]...]\n",
            "I0212 04:15:34.037740 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[13 19 589 19 49 1 196 2 3306 496 521 605 2172 1832 1 1 6 1 1 1][38 620 0 129 146 124 23 2189 30 2831 51 131 2 0 2459 4 6 2569 8 496][113 245 1 55 150 7 61 233 3092 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1341 13 19 589 19 49 1 196 2 3306...]...][[0 0 1 1 1 0 0 1 0 1...]...][[1341 10000 10000 589 19 49 10000 10000 2 10000...]...][[13 19 589 19 49 1 196 2 3306 496...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1341 13 19 589 19 49 1 196 2 3306...]...][[0 0 1 1 1 0 0 1 0 1...]...][[1341 10000 10000 589 19 49 10000 10000 2 10000...]...][[13 19 589 19 49 1 196 2 3306 496...]...]\n",
            "targets[[13 19 589 19 49 1 196 2 3306 496 521 605 2172 1832 1 1 6 1 1 1][38 620 0 129 146 124 23 2189 30 2831 51 131 2 0 2459 4 6 2569 8 496][113 245 1 55 150 7 61 233 3092 2...]...]\n",
            "targets[[13 19 589 19 49 1 196 2 3306 496 521 605 2172 1832 1 1 6 1 1 1][38 620 0 129 146 124 23 2189 30 2831 51 131 2 0 2459 4 6 2569 8 496][113 245 1 55 150 7 61 233 3092 2...]...]\n",
            "targets[[13 19 589 19 49 1 196 2 3306 496 521 605 2172 1832 1 1 6 1 1 1][38 620 0 129 146 124 23 2189 30 2831 51 131 2 0 2459 4 6 2569 8 496][113 245 1 55 150 7 61 233 3092 2...]...]\n",
            "global_step: 1485\n",
            " perplexity: 682.731\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[13 19 589 19 49 1 196 2 3306 496 521 605 2172 1832 1 1 6 1 1 1][38 620 0 129 146 124 23 2189 30 2831 51 131 2 0 2459 4 6 2569 8 496][113 245 1 55 150 7 61 233 3092 2...]...]\n",
            " percent of 3-grams captured: 0.128.\n",
            "targets[[13 19 589 19 49 1 196 2 3306 496 521 605 2172 1832 1 1 6 1 1 1][38 620 0 129 146 124 23 2189 30 2831 51 131 2 0 2459 4 6 2569 8 496][113 245 1 55 150 7 61 233 3092 2...]...]\n",
            " percent of 2-grams captured: 0.468.\n",
            "targets[[13 19 589 19 49 1 196 2 3306 496 521 605 2172 1832 1 1 6 1 1 1][38 620 0 129 146 124 23 2189 30 2831 51 131 2 0 2459 4 6 2569 8 496][113 245 1 55 150 7 61 233 3092 2...]...]\n",
            " percent of 4-grams captured: 0.041.\n",
            " geometric_avg: 0.135.\n",
            " arithmetic_avg: 0.212.\n",
            "global_step: 1485\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69634\n",
            " G train loss: 3.29553\n",
            "targets[[13 19 589 19 49 1 196 2 3306 496 521 605 2172 1832 1 1 6 1 1 1][38 620 0 129 146 124 23 2189 30 2831 51 131 2 0 2459 4 6 2569 8 496][113 245 1 55 150 7 61 233 3092 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1341 13 19 589 19 49 1 196 2 3306...]...][[0 0 1 1 1 0 0 1 0 1...]...][[1341 10000 10000 589 19 49 10000 10000 2 10000...]...][[13 19 589 19 49 1 196 2 3306 496...]...]\n",
            " Sample 0.\n",
            "   [0]  is                    0.463   4.243  \n",
            "   [0]  as                    0.470   6.101  \n",
            "   [1]  important             0.477   0.000  \n",
            "   [1]  as                    0.485   0.000  \n",
            "   [1]  his                   0.493   0.000  \n",
            "   [0]  <unk>                 0.499   2.079  \n",
            "   [0]  average               0.504   8.171  \n",
            "   [1]  <eos>                 0.506   0.000  \n",
            "   [0]  polls                 0.505   9.979  \n",
            "   [1]  once                  0.503   0.000  \n",
            "   [1]  named                 0.498   0.000  \n",
            "   [0]  tokyo                 0.492   9.199  \n",
            "   [1]  giants                0.485   0.000  \n",
            "   [0]  star                  0.481   10.085 \n",
            "   [1]  <unk>                 0.479   0.000  \n",
            "   [0]  <unk>                 0.480   3.853  \n",
            "   [0]  a                     0.482   4.195  \n",
            "   [0]  <unk>                 0.483   2.148  \n",
            "   [0]  <unk>                 0.485   3.173  \n",
            "   [1]  <unk>                 0.487   0.000  \n",
            " Sample 1.\n",
            "   [0]  they                  0.481   6.201  \n",
            "   [1]  believe               0.471   0.000  \n",
            "   [0]  the                   0.468   4.759  \n",
            "   [0]  big                   0.466   5.881  \n",
            "   [1]  board                 0.468   0.000  \n",
            "   [0]  under                 0.470   6.779  \n",
            "   [1]  mr.                   0.471   0.000  \n",
            "   [1]  phelan                0.472   0.000  \n",
            "   [1]  has                   0.474   0.000  \n",
            "   [0]  abandoned             0.475   8.910  \n",
            "   [0]  their                 0.476   6.610  \n",
            "   [0]  interest              0.478   7.617  \n",
            "   [0]  <eos>                 0.479   2.433  \n",
            "   [1]  the                   0.480   0.000  \n",
            "   [0]  son                   0.479   9.405  \n",
            "   [0]  of                    0.477   2.963  \n",
            "   [1]  a                     0.476   0.000  \n",
            "   [0]  specialist            0.476   9.724  \n",
            "   [1]  and                   0.474   0.000  \n",
            "   [0]  once                  0.475   7.850  \n",
            " Sample 2.\n",
            "   [0]  even                  0.525   7.105  \n",
            "   [0]  less                  0.527   7.562  \n",
            "   [1]  <unk>                 0.529   0.000  \n",
            "   [1]  than                  0.532   0.000  \n",
            "   [0]  those                 0.534   6.813  \n",
            "   [1]  in                    0.533   0.000  \n",
            "   [1]  other                 0.531   0.000  \n",
            "   [1]  economic              0.528   0.000  \n",
            "   [1]  indicators            0.526   0.000  \n",
            "   [0]  <eos>                 0.522   2.908  \n",
            "   [0]  because               0.519   7.115  \n",
            "   [0]  the                   0.518   2.251  \n",
            "   [1]  figures               0.518   0.000  \n",
            "   [0]  are                   0.517   5.771  \n",
            "   [0]  based                 0.516   7.760  \n",
            "   [0]  on                    0.516   4.271  \n",
            "   [0]  a                     0.519   3.071  \n",
            "   [1]  small                 0.522   0.000  \n",
            "   [1]  sample                0.525   0.000  \n",
            "   [0]  the                   0.526   3.587  \n",
            "Samples\n",
            "Sample 0 .  is as important as his <unk> average <eos> polls once named tokyo giants star <unk> <unk> a <unk> <unk> <unk>\n",
            "Sample 1 .  they believe the big board under mr. phelan has abandoned their interest <eos> the son of a specialist and once\n",
            "Sample 2 .  even less <unk> than those in other economic indicators <eos> because the figures are based on a small sample the\n",
            "\n",
            "\n",
            "targets[[8 6 9867 14 9 1 3013 1 502 1 8643 5 2961 1 0 1111 8568 1 3263 16][105 121 5 585 216 193 8 61 156 280 20 835 51 85 5 1060 190 2 669 2198][0 168 2 0 234 59 15 14 1260 346...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4627 8 6 9867 14 9 1 3013 1 502...]...][[0 1 0 0 1 0 0 0 0 0...]...][[4627 10000 6 10000 10000 9 10000 10000 10000 10000...]...][[8 6 9867 14 9 1 3013 1 502 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4627 8 6 9867 14 9 1 3013 1 502...]...][[0 1 0 0 1 0 0 0 0 0...]...][[4627 10000 6 10000 10000 9 10000 10000 10000 10000...]...][[8 6 9867 14 9 1 3013 1 502 1...]...]\n",
            "I0212 04:15:37.307409 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[9825 19 0 3724 3769 4 203 2 29 61 55 0 668 10 1 13 1863 22 6 4627][54 1052 23 2189 30 3294 58 1 22 189 166 0 35 60 2986 10 49 337 2163 8012][234 15 14 13 3 3 2844 86 10 8258...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 9825 19 0 3724 3769 4 203 2 29...]...][[1 0 1 0 1 1 0 0 0 1...]...][[1 9825 10000 0 10000 3769 4 10000 10000 10000...]...][[9825 19 0 3724 3769 4 203 2 29 61...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 9825 19 0 3724 3769 4 203 2 29...]...][[1 0 1 0 1 1 0 0 0 1...]...][[1 9825 10000 0 10000 3769 4 10000 10000 10000...]...][[9825 19 0 3724 3769 4 203 2 29 61...]...]\n",
            "targets[[9825 19 0 3724 3769 4 203 2 29 61 55 0 668 10 1 13 1863 22 6 4627][54 1052 23 2189 30 3294 58 1 22 189 166 0 35 60 2986 10 49 337 2163 8012][234 15 14 13 3 3 2844 86 10 8258...]...]\n",
            "targets[[9825 19 0 3724 3769 4 203 2 29 61 55 0 668 10 1 13 1863 22 6 4627][54 1052 23 2189 30 3294 58 1 22 189 166 0 35 60 2986 10 49 337 2163 8012][234 15 14 13 3 3 2844 86 10 8258...]...]\n",
            "targets[[9825 19 0 3724 3769 4 203 2 29 61 55 0 668 10 1 13 1863 22 6 4627][54 1052 23 2189 30 3294 58 1 22 189 166 0 35 60 2986 10 49 337 2163 8012][234 15 14 13 3 3 2844 86 10 8258...]...]\n",
            "global_step: 1488\n",
            " perplexity: 682.199\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[9825 19 0 3724 3769 4 203 2 29 61 55 0 668 10 1 13 1863 22 6 4627][54 1052 23 2189 30 3294 58 1 22 189 166 0 35 60 2986 10 49 337 2163 8012][234 15 14 13 3 3 2844 86 10 8258...]...]\n",
            " percent of 3-grams captured: 0.178.\n",
            "targets[[9825 19 0 3724 3769 4 203 2 29 61 55 0 668 10 1 13 1863 22 6 4627][54 1052 23 2189 30 3294 58 1 22 189 166 0 35 60 2986 10 49 337 2163 8012][234 15 14 13 3 3 2844 86 10 8258...]...]\n",
            " percent of 2-grams captured: 0.476.\n",
            "targets[[9825 19 0 3724 3769 4 203 2 29 61 55 0 668 10 1 13 1863 22 6 4627][54 1052 23 2189 30 3294 58 1 22 189 166 0 35 60 2986 10 49 337 2163 8012][234 15 14 13 3 3 2844 86 10 8258...]...]\n",
            " percent of 4-grams captured: 0.074.\n",
            " geometric_avg: 0.184.\n",
            " arithmetic_avg: 0.243.\n",
            "global_step: 1488\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69631\n",
            " G train loss: 3.29535\n",
            "targets[[9825 19 0 3724 3769 4 203 2 29 61 55 0 668 10 1 13 1863 22 6 4627][54 1052 23 2189 30 3294 58 1 22 189 166 0 35 60 2986 10 49 337 2163 8012][234 15 14 13 3 3 2844 86 10 8258...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 9825 19 0 3724 3769 4 203 2 29...]...][[1 0 1 0 1 1 0 0 0 1...]...][[1 9825 10000 0 10000 3769 4 10000 10000 10000...]...][[9825 19 0 3724 3769 4 203 2 29 61...]...]\n",
            " Sample 0.\n",
            "   [1]  soul                  0.537   0.000  \n",
            "   [0]  as                    0.541   4.839  \n",
            "   [1]  the                   0.543   0.000  \n",
            "   [0]  male                  0.544   11.306 \n",
            "   [1]  symbol                0.547   0.000  \n",
            "   [1]  of                    0.548   0.000  \n",
            "   [0]  japan                 0.550   6.529  \n",
            "   [0]  <eos>                 0.550   3.354  \n",
            "   [0]  but                   0.550   4.293  \n",
            "   [1]  other                 0.550   0.000  \n",
            "   [0]  than                  0.550   7.137  \n",
            "   [0]  the                   0.547   1.737  \n",
            "   [1]  fact                  0.546   0.000  \n",
            "   [1]  that                  0.548   0.000  \n",
            "   [0]  <unk>                 0.550   2.949  \n",
            "   [1]  is                    0.550   0.000  \n",
            "   [1]  played                0.548   0.000  \n",
            "   [1]  with                  0.551   0.000  \n",
            "   [0]  a                     0.554   2.483  \n",
            "   [1]  ball                  0.555   0.000  \n",
            " Sample 1.\n",
            "   [1]  one                   0.476   0.000  \n",
            "   [0]  himself               0.466   8.795  \n",
            "   [1]  mr.                   0.461   0.000  \n",
            "   [1]  phelan                0.460   0.000  \n",
            "   [0]  has                   0.463   4.626  \n",
            "   [0]  nonetheless           0.468   10.663 \n",
            "   [0]  been                  0.475   5.316  \n",
            "   [0]  <unk>                 0.479   2.650  \n",
            "   [0]  with                  0.481   4.959  \n",
            "   [1]  products              0.480   0.000  \n",
            "   [1]  like                  0.479   0.000  \n",
            "   [1]  the                   0.476   0.000  \n",
            "   [0]  new                   0.474   4.102  \n",
            "   [0]  stock                 0.473   4.800  \n",
            "   [1]  basket                0.473   0.000  \n",
            "   [0]  that                  0.475   4.932  \n",
            "   [0]  his                   0.476   5.794  \n",
            "   [1]  former                0.476   0.000  \n",
            "   [0]  colleagues            0.477   8.407  \n",
            "   [0]  dislike               0.476   12.369 \n",
            " Sample 2.\n",
            "   [1]  department            0.503   0.000  \n",
            "   [0]  said                  0.495   4.421  \n",
            "   [0]  it                    0.495   4.240  \n",
            "   [0]  is                    0.494   4.136  \n",
            "   [1]  N                     0.490   0.000  \n",
            "   [1]  N                     0.487   0.000  \n",
            "   [0]  confident             0.490   11.803 \n",
            "   [0]  only                  0.492   7.240  \n",
            "   [1]  that                  0.497   0.000  \n",
            "   [1]  new-home              0.499   0.000  \n",
            "   [1]  sales                 0.499   0.000  \n",
            "   [1]  fell                  0.499   0.000  \n",
            "   [1]  somewhere             0.498   0.000  \n",
            "   [1]  between               0.498   0.000  \n",
            "   [0]  N                     0.495   2.820  \n",
            "   [0]  N                     0.491   1.801  \n",
            "   [1]  and                   0.490   0.000  \n",
            "   [1]  N                     0.490   0.000  \n",
            "   [1]  N                     0.490   0.000  \n",
            "   [0]  during                0.495   7.204  \n",
            "Samples\n",
            "Sample 0 .  soul as the male symbol of japan <eos> but other than the fact that <unk> is played with a ball\n",
            "Sample 1 .  one himself mr. phelan has nonetheless been <unk> with products like the new stock basket that his former colleagues dislike\n",
            "Sample 2 .  department said it is N N confident only that new-home sales fell somewhere between N N and N N during\n",
            "\n",
            "\n",
            "targets[[0 1071 4 0 1 2241 26 3207 113 6270 155 38 1 1 0 8401 4 4879 1573 403][134 77 1481 34 9422 19 6 410 4 2415 932 2 67 0 184 560 775 110 89 2541][3 2185 1095 1512 34 58 238 118 3 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 0 1071 4 0 1 2241 26 3207 113...]...][[1 1 0 0 0 1 0 1 0 0...]...][[16 0 1071 10000 10000 10000 2241 10000 3207 10000...]...][[0 1071 4 0 1 2241 26 3207 113 6270...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 0 1071 4 0 1 2241 26 3207 113...]...][[1 1 0 0 0 1 0 1 0 0...]...][[16 0 1071 10000 10000 10000 2241 10000 3207 10000...]...][[0 1071 4 0 1 2241 26 3207 113 6270...]...]\n",
            "I0212 04:15:40.832987 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[8 6 9867 14 9 1 3013 1 502 1 8643 5 2961 1 0 1111 8568 1 3263 16][105 121 5 585 216 193 8 61 156 280 20 835 51 85 5 1060 190 2 669 2198][0 168 2 0 234 59 15 14 1260 346...]...]\n",
            "I0212 04:15:41.932327 139783917786880 supervisor.py:1117] Saving checkpoint to path maskGAN/train/model.ckpt\n",
            "inputs, targets_present, masked_inputs, sequence[[4627 8 6 9867 14 9 1 3013 1 502...]...][[0 0 1 1 1 0 1 0 1 1...]...][[4627 10000 10000 9867 14 9 10000 3013 10000 502...]...][[8 6 9867 14 9 1 3013 1 502 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4627 8 6 9867 14 9 1 3013 1 502...]...][[0 0 1 1 1 0 1 0 1 1...]...][[4627 10000 10000 9867 14 9 10000 3013 10000 502...]...][[8 6 9867 14 9 1 3013 1 502 1...]...]\n",
            "targets[[8 6 9867 14 9 1 3013 1 502 1 8643 5 2961 1 0 1111 8568 1 3263 16][105 121 5 585 216 193 8 61 156 280 20 835 51 85 5 1060 190 2 669 2198][0 168 2 0 234 59 15 14 1260 346...]...]\n",
            "targets[[8 6 9867 14 9 1 3013 1 502 1 8643 5 2961 1 0 1111 8568 1 3263 16][105 121 5 585 216 193 8 61 156 280 20 835 51 85 5 1060 190 2 669 2198][0 168 2 0 234 59 15 14 1260 346...]...]\n",
            "targets[[8 6 9867 14 9 1 3013 1 502 1 8643 5 2961 1 0 1111 8568 1 3263 16][105 121 5 585 216 193 8 61 156 280 20 835 51 85 5 1060 190 2 669 2198][0 168 2 0 234 59 15 14 1260 346...]...]\n",
            "global_step: 1491\n",
            " perplexity: 681.290\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[8 6 9867 14 9 1 3013 1 502 1 8643 5 2961 1 0 1111 8568 1 3263 16][105 121 5 585 216 193 8 61 156 280 20 835 51 85 5 1060 190 2 669 2198][0 168 2 0 234 59 15 14 1260 346...]...]\n",
            " percent of 3-grams captured: 0.214.\n",
            "targets[[8 6 9867 14 9 1 3013 1 502 1 8643 5 2961 1 0 1111 8568 1 3263 16][105 121 5 585 216 193 8 61 156 280 20 835 51 85 5 1060 190 2 669 2198][0 168 2 0 234 59 15 14 1260 346...]...]\n",
            " percent of 2-grams captured: 0.518.\n",
            "targets[[8 6 9867 14 9 1 3013 1 502 1 8643 5 2961 1 0 1111 8568 1 3263 16][105 121 5 585 216 193 8 61 156 280 20 835 51 85 5 1060 190 2 669 2198][0 168 2 0 234 59 15 14 1260 346...]...]\n",
            " percent of 4-grams captured: 0.103.\n",
            " geometric_avg: 0.225.\n",
            " arithmetic_avg: 0.278.\n",
            "global_step: 1491\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69630\n",
            " G train loss: 3.29494\n",
            "targets[[8 6 9867 14 9 1 3013 1 502 1 8643 5 2961 1 0 1111 8568 1 3263 16][105 121 5 585 216 193 8 61 156 280 20 835 51 85 5 1060 190 2 669 2198][0 168 2 0 234 59 15 14 1260 346...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4627 8 6 9867 14 9 1 3013 1 502...]...][[0 0 1 1 1 0 1 0 1 1...]...][[4627 10000 10000 9867 14 9 10000 3013 10000 502...]...][[8 6 9867 14 9 1 3013 1 502 1...]...]\n",
            " Sample 0.\n",
            "   [0]  and                   0.503   3.821  \n",
            "   [0]  a                     0.505   3.791  \n",
            "   [1]  bat                   0.507   0.000  \n",
            "   [1]  it                    0.511   0.000  \n",
            "   [1]  's                    0.508   0.000  \n",
            "   [0]  <unk>                 0.505   3.097  \n",
            "   [1]  fans                  0.500   0.000  \n",
            "   [0]  <unk>                 0.496   3.326  \n",
            "   [1]  return                0.495   0.000  \n",
            "   [1]  <unk>                 0.493   0.000  \n",
            "   [0]  balls                 0.492   11.848 \n",
            "   [0]  to                    0.492   3.426  \n",
            "   [1]  stadium               0.493   0.000  \n",
            "   [0]  <unk>                 0.492   3.490  \n",
            "   [1]  the                   0.489   0.000  \n",
            "   [1]  strike                0.486   0.000  \n",
            "   [1]  zone                  0.486   0.000  \n",
            "   [0]  <unk>                 0.486   3.532  \n",
            "   [0]  depending             0.486   11.282 \n",
            "   [1]  on                    0.484   0.000  \n",
            " Sample 1.\n",
            "   [1]  so                    0.496   0.000  \n",
            "   [1]  much                  0.500   0.000  \n",
            "   [1]  to                    0.505   0.000  \n",
            "   [1]  keep                  0.511   0.000  \n",
            "   [1]  index                 0.515   0.000  \n",
            "   [0]  funds                 0.517   7.051  \n",
            "   [1]  and                   0.515   0.000  \n",
            "   [0]  other                 0.516   5.557  \n",
            "   [1]  program               0.513   0.000  \n",
            "   [1]  traders               0.510   0.000  \n",
            "   [1]  from                  0.510   0.000  \n",
            "   [1]  taking                0.512   0.000  \n",
            "   [1]  their                 0.511   0.000  \n",
            "   [1]  business              0.511   0.000  \n",
            "   [1]  to                    0.512   0.000  \n",
            "   [1]  overseas              0.512   0.000  \n",
            "   [0]  markets               0.513   7.572  \n",
            "   [0]  <eos>                 0.514   2.954  \n",
            "   [1]  meanwhile             0.514   0.000  \n",
            "   [0]  specialists           0.513   8.515  \n",
            " Sample 2.\n",
            "   [0]  the                   0.496   1.575  \n",
            "   [0]  month                 0.500   5.624  \n",
            "   [0]  <eos>                 0.506   2.283  \n",
            "   [1]  the                   0.510   0.000  \n",
            "   [1]  department            0.514   0.000  \n",
            "   [0]  also                  0.513   6.437  \n",
            "   [0]  said                  0.509   4.863  \n",
            "   [1]  it                    0.506   0.000  \n",
            "   [0]  takes                 0.504   8.889  \n",
            "   [1]  four                  0.505   0.000  \n",
            "   [0]  months                0.505   6.735  \n",
            "   [0]  to                    0.507   2.563  \n",
            "   [1]  establish             0.510   0.000  \n",
            "   [0]  a                     0.517   4.067  \n",
            "   [1]  trend                 0.522   0.000  \n",
            "   [0]  <eos>                 0.526   2.066  \n",
            "   [1]  so                    0.526   0.000  \n",
            "   [0]  far                   0.523   7.760  \n",
            "   [1]  this                  0.521   0.000  \n",
            "   [0]  year                  0.522   5.753  \n",
            "Samples\n",
            "Sample 0 .  and a bat it 's <unk> fans <unk> return <unk> balls to stadium <unk> the strike zone <unk> depending on\n",
            "Sample 1 .  so much to keep index funds and other program traders from taking their business to overseas markets <eos> meanwhile specialists\n",
            "Sample 2 .  the month <eos> the department also said it takes four months to establish a trend <eos> so far this year\n",
            "\n",
            "\n",
            "targets[[1 17 4581 908 4 2780 113 7 51 629 2136 1573 11 0 605 2172 11 471 403 849][0 530 8328 900 5 386 16 0 129 146 77 1022 44 54 2569 83 2 0 838 3656][810 3092 594 6 137 961 7 311 20 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[403 1 17 4581 908 4 2780 113 7 51...]...][[0 1 1 1 1 0 1 0 0 1...]...][[403 10000 17 4581 908 4 10000 113 10000 10000...]...][[1 17 4581 908 4 2780 113 7 51 629...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[403 1 17 4581 908 4 2780 113 7 51...]...][[0 1 1 1 1 0 1 0 0 1...]...][[403 10000 17 4581 908 4 10000 113 10000 10000...]...][[1 17 4581 908 4 2780 113 7 51 629...]...]\n",
            "I0212 04:15:46.035146 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[0 1071 4 0 1 2241 26 3207 113 6270 155 38 1 1 0 8401 4 4879 1573 403][134 77 1481 34 9422 19 6 410 4 2415 932 2 67 0 184 560 775 110 89 2541][3 2185 1095 1512 34 58 238 118 3 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 0 1071 4 0 1 2241 26 3207 113...]...][[1 0 1 0 1 0 1 0 1 1...]...][[16 0 10000 4 10000 1 10000 26 10000 113...]...][[0 1071 4 0 1 2241 26 3207 113 6270...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 0 1071 4 0 1 2241 26 3207 113...]...][[1 0 1 0 1 0 1 0 1 1...]...][[16 0 10000 4 10000 1 10000 26 10000 113...]...][[0 1071 4 0 1 2241 26 3207 113 6270...]...]\n",
            "targets[[0 1071 4 0 1 2241 26 3207 113 6270 155 38 1 1 0 8401 4 4879 1573 403][134 77 1481 34 9422 19 6 410 4 2415 932 2 67 0 184 560 775 110 89 2541][3 2185 1095 1512 34 58 238 118 3 3...]...]\n",
            "targets[[0 1071 4 0 1 2241 26 3207 113 6270 155 38 1 1 0 8401 4 4879 1573 403][134 77 1481 34 9422 19 6 410 4 2415 932 2 67 0 184 560 775 110 89 2541][3 2185 1095 1512 34 58 238 118 3 3...]...]\n",
            "targets[[0 1071 4 0 1 2241 26 3207 113 6270 155 38 1 1 0 8401 4 4879 1573 403][134 77 1481 34 9422 19 6 410 4 2415 932 2 67 0 184 560 775 110 89 2541][3 2185 1095 1512 34 58 238 118 3 3...]...]\n",
            "global_step: 1494\n",
            " perplexity: 681.274\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[0 1071 4 0 1 2241 26 3207 113 6270 155 38 1 1 0 8401 4 4879 1573 403][134 77 1481 34 9422 19 6 410 4 2415 932 2 67 0 184 560 775 110 89 2541][3 2185 1095 1512 34 58 238 118 3 3...]...]\n",
            " percent of 3-grams captured: 0.231.\n",
            "targets[[0 1071 4 0 1 2241 26 3207 113 6270 155 38 1 1 0 8401 4 4879 1573 403][134 77 1481 34 9422 19 6 410 4 2415 932 2 67 0 184 560 775 110 89 2541][3 2185 1095 1512 34 58 238 118 3 3...]...]\n",
            " percent of 2-grams captured: 0.489.\n",
            "targets[[0 1071 4 0 1 2241 26 3207 113 6270 155 38 1 1 0 8401 4 4879 1573 403][134 77 1481 34 9422 19 6 410 4 2415 932 2 67 0 184 560 775 110 89 2541][3 2185 1095 1512 34 58 238 118 3 3...]...]\n",
            " percent of 4-grams captured: 0.106.\n",
            " geometric_avg: 0.229.\n",
            " arithmetic_avg: 0.275.\n",
            "global_step: 1494\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69634\n",
            " G train loss: 3.29560\n",
            "targets[[0 1071 4 0 1 2241 26 3207 113 6270 155 38 1 1 0 8401 4 4879 1573 403][134 77 1481 34 9422 19 6 410 4 2415 932 2 67 0 184 560 775 110 89 2541][3 2185 1095 1512 34 58 238 118 3 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 0 1071 4 0 1 2241 26 3207 113...]...][[1 0 1 0 1 0 1 0 1 1...]...][[16 0 10000 4 10000 1 10000 26 10000 113...]...][[0 1071 4 0 1 2241 26 3207 113 6270...]...]\n",
            " Sample 0.\n",
            "   [1]  the                   0.510   0.000  \n",
            "   [0]  size                  0.519   7.254  \n",
            "   [1]  of                    0.525   0.000  \n",
            "   [0]  the                   0.527   0.636  \n",
            "   [1]  <unk>                 0.528   0.000  \n",
            "   [0]  ties                  0.526   10.383 \n",
            "   [1]  are                   0.524   0.000  \n",
            "   [0]  permitted             0.524   9.752  \n",
            "   [1]  even                  0.522   0.000  \n",
            "   [1]  welcomed              0.524   0.000  \n",
            "   [0]  since                 0.528   6.599  \n",
            "   [1]  they                  0.530   0.000  \n",
            "   [0]  <unk>                 0.532   2.926  \n",
            "   [0]  <unk>                 0.534   3.081  \n",
            "   [1]  the                   0.532   0.000  \n",
            "   [1]  shame                 0.527   0.000  \n",
            "   [1]  of                    0.523   0.000  \n",
            "   [0]  defeat                0.517   12.263 \n",
            "   [0]  players               0.513   10.226 \n",
            "   [1]  must                  0.510   0.000  \n",
            " Sample 1.\n",
            "   [0]  '                     0.477   6.361  \n",
            "   [0]  trading               0.473   7.030  \n",
            "   [0]  risks                 0.471   11.527 \n",
            "   [1]  have                  0.473   0.000  \n",
            "   [0]  skyrocketed           0.477   11.369 \n",
            "   [0]  as                    0.481   4.823  \n",
            "   [0]  a                     0.484   2.804  \n",
            "   [1]  result                0.484   0.000  \n",
            "   [0]  of                    0.483   1.695  \n",
            "   [0]  stock-market          0.481   10.883 \n",
            "   [0]  volatility            0.480   9.575  \n",
            "   [1]  <eos>                 0.478   0.000  \n",
            "   [1]  when                  0.476   0.000  \n",
            "   [0]  the                   0.474   2.144  \n",
            "   [1]  sell                  0.473   0.000  \n",
            "   [0]  programs              0.470   7.980  \n",
            "   [0]  hit                   0.468   9.593  \n",
            "   [1]  you                   0.469   0.000  \n",
            "   [1]  can                   0.470   0.000  \n",
            "   [0]  hear                  0.472   9.353  \n",
            " Sample 2.\n",
            "   [0]  N                     0.484   3.485  \n",
            "   [0]  newly                 0.492   11.261 \n",
            "   [1]  built                 0.501   0.000  \n",
            "   [1]  homes                 0.510   0.000  \n",
            "   [1]  have                  0.514   0.000  \n",
            "   [1]  been                  0.518   0.000  \n",
            "   [0]  sold                  0.519   7.827  \n",
            "   [0]  down                  0.513   6.798  \n",
            "   [1]  N                     0.509   0.000  \n",
            "   [0]  N                     0.508   2.088  \n",
            "   [0]  from                  0.507   3.992  \n",
            "   [1]  the                   0.504   0.000  \n",
            "   [0]  like                  0.500   10.096 \n",
            "   [0]  months                0.498   5.669  \n",
            "   [0]  of                    0.498   2.930  \n",
            "   [1]  N                     0.500   0.000  \n",
            "   [0]  <eos>                 0.502   3.438  \n",
            "   [0]  the                   0.502   1.682  \n",
            "   [0]  index                 0.499   7.504  \n",
            "   [0]  of                    0.497   4.348  \n",
            "Samples\n",
            "Sample 0 .  the size of the <unk> ties are permitted even welcomed since they <unk> <unk> the shame of defeat players must\n",
            "Sample 1 .  ' trading risks have skyrocketed as a result of stock-market volatility <eos> when the sell programs hit you can hear\n",
            "Sample 2 .  N newly built homes have been sold down N N from the like months of N <eos> the index of\n",
            "\n",
            "\n",
            "targets[[4599 2241 67 16 0 2104 2 110 1 34 1 13 0 516 7846 1 4 242 140 1][597 8 0 2569 13 696 1322 19 0 1988 4 69 3222 11 49 2759 4 149 28 2297][3031 2 39 854 50 556 1056 7 397 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[849 4599 2241 67 16 0 2104 2 110 1...]...][[0 0 0 0 1 0 1 1 0 0...]...][[849 10000 10000 10000 10000 0 10000 2 110 10000...]...][[4599 2241 67 16 0 2104 2 110 1 34...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[849 4599 2241 67 16 0 2104 2 110 1...]...][[0 0 0 0 1 0 1 1 0 0...]...][[849 10000 10000 10000 10000 0 10000 2 110 10000...]...][[4599 2241 67 16 0 2104 2 110 1 34...]...]\n",
            "I0212 04:15:49.372982 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1 17 4581 908 4 2780 113 7 51 629 2136 1573 11 0 605 2172 11 471 403 849][0 530 8328 900 5 386 16 0 129 146 77 1022 44 54 2569 83 2 0 838 3656][810 3092 594 6 137 961 7 311 20 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[403 1 17 4581 908 4 2780 113 7 51...]...][[0 0 0 1 0 0 1 0 1 1...]...][[403 10000 10000 10000 908 10000 10000 113 10000 51...]...][[1 17 4581 908 4 2780 113 7 51 629...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[403 1 17 4581 908 4 2780 113 7 51...]...][[0 0 0 1 0 0 1 0 1 1...]...][[403 10000 10000 10000 908 10000 10000 113 10000 51...]...][[1 17 4581 908 4 2780 113 7 51 629...]...]\n",
            "targets[[1 17 4581 908 4 2780 113 7 51 629 2136 1573 11 0 605 2172 11 471 403 849][0 530 8328 900 5 386 16 0 129 146 77 1022 44 54 2569 83 2 0 838 3656][810 3092 594 6 137 961 7 311 20 6...]...]\n",
            "targets[[1 17 4581 908 4 2780 113 7 51 629 2136 1573 11 0 605 2172 11 471 403 849][0 530 8328 900 5 386 16 0 129 146 77 1022 44 54 2569 83 2 0 838 3656][810 3092 594 6 137 961 7 311 20 6...]...]\n",
            "targets[[1 17 4581 908 4 2780 113 7 51 629 2136 1573 11 0 605 2172 11 471 403 849][0 530 8328 900 5 386 16 0 129 146 77 1022 44 54 2569 83 2 0 838 3656][810 3092 594 6 137 961 7 311 20 6...]...]\n",
            "global_step: 1497\n",
            " perplexity: 680.551\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1 17 4581 908 4 2780 113 7 51 629 2136 1573 11 0 605 2172 11 471 403 849][0 530 8328 900 5 386 16 0 129 146 77 1022 44 54 2569 83 2 0 838 3656][810 3092 594 6 137 961 7 311 20 6...]...]\n",
            " percent of 3-grams captured: 0.108.\n",
            "targets[[1 17 4581 908 4 2780 113 7 51 629 2136 1573 11 0 605 2172 11 471 403 849][0 530 8328 900 5 386 16 0 129 146 77 1022 44 54 2569 83 2 0 838 3656][810 3092 594 6 137 961 7 311 20 6...]...]\n",
            " percent of 2-grams captured: 0.403.\n",
            "targets[[1 17 4581 908 4 2780 113 7 51 629 2136 1573 11 0 605 2172 11 471 403 849][0 530 8328 900 5 386 16 0 129 146 77 1022 44 54 2569 83 2 0 838 3656][810 3092 594 6 137 961 7 311 20 6...]...]\n",
            " percent of 4-grams captured: 0.044.\n",
            " geometric_avg: 0.124.\n",
            " arithmetic_avg: 0.185.\n",
            "global_step: 1497\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69633\n",
            " G train loss: 3.29470\n",
            "targets[[1 17 4581 908 4 2780 113 7 51 629 2136 1573 11 0 605 2172 11 471 403 849][0 530 8328 900 5 386 16 0 129 146 77 1022 44 54 2569 83 2 0 838 3656][810 3092 594 6 137 961 7 311 20 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[403 1 17 4581 908 4 2780 113 7 51...]...][[0 0 0 1 0 0 1 0 1 1...]...][[403 10000 10000 10000 908 10000 10000 113 10000 51...]...][[1 17 4581 908 4 2780 113 7 51 629...]...]\n",
            " Sample 0.\n",
            "   [0]  <unk>                 0.483   2.550  \n",
            "   [0]  by                    0.484   4.576  \n",
            "   [0]  strict                0.485   11.579 \n",
            "   [1]  rules                 0.486   0.000  \n",
            "   [0]  of                    0.484   3.575  \n",
            "   [0]  conduct               0.482   10.171 \n",
            "   [1]  even                  0.479   0.000  \n",
            "   [0]  in                    0.478   5.006  \n",
            "   [1]  their                 0.475   0.000  \n",
            "   [1]  personal              0.476   0.000  \n",
            "   [1]  lives                 0.478   0.000  \n",
            "   [1]  players               0.481   0.000  \n",
            "   [0]  for                   0.483   4.950  \n",
            "   [1]  the                   0.485   0.000  \n",
            "   [1]  tokyo                 0.486   0.000  \n",
            "   [0]  giants                0.487   10.706 \n",
            "   [0]  for                   0.487   4.536  \n",
            "   [0]  example               0.485   7.641  \n",
            "   [1]  must                  0.485   0.000  \n",
            "   [1]  always                0.487   0.000  \n",
            " Sample 1.\n",
            "   [1]  the                   0.511   0.000  \n",
            "   [1]  order                 0.515   0.000  \n",
            "   [0]  printers              0.519   12.623 \n",
            "   [1]  start                 0.523   0.000  \n",
            "   [1]  to                    0.526   0.000  \n",
            "   [1]  go                    0.529   0.000  \n",
            "   [0]  on                    0.529   4.754  \n",
            "   [1]  the                   0.528   0.000  \n",
            "   [1]  big                   0.527   0.000  \n",
            "   [0]  board                 0.526   5.206  \n",
            "   [1]  trading               0.525   0.000  \n",
            "   [0]  floor                 0.526   8.909  \n",
            "   [0]  says                  0.528   5.564  \n",
            "   [1]  one                   0.529   0.000  \n",
            "   [1]  specialist            0.531   0.000  \n",
            "   [1]  there                 0.529   0.000  \n",
            "   [1]  <eos>                 0.528   0.000  \n",
            "   [1]  the                   0.526   0.000  \n",
            "   [0]  buyers                0.525   9.044  \n",
            "   [1]  walk                  0.526   0.000  \n",
            " Sample 2.\n",
            "   [0]  leading               0.516   8.585  \n",
            "   [1]  indicators            0.517   0.000  \n",
            "   [0]  got                   0.519   8.596  \n",
            "   [0]  a                     0.524   4.060  \n",
            "   [0]  major                 0.524   6.206  \n",
            "   [1]  boost                 0.522   0.000  \n",
            "   [0]  in                    0.520   4.381  \n",
            "   [1]  september             0.518   0.000  \n",
            "   [1]  from                  0.517   0.000  \n",
            "   [1]  a                     0.521   0.000  \n",
            "   [1]  surge                 0.523   0.000  \n",
            "   [1]  in                    0.520   0.000  \n",
            "   [1]  consumer              0.517   0.000  \n",
            "   [1]  expectations          0.517   0.000  \n",
            "   [0]  as                    0.515   4.873  \n",
            "   [1]  measured              0.513   0.000  \n",
            "   [1]  by                    0.513   0.000  \n",
            "   [1]  the                   0.514   0.000  \n",
            "   [0]  university            0.516   8.378  \n",
            "   [0]  of                    0.517   2.079  \n",
            "Samples\n",
            "Sample 0 .  <unk> by strict rules of conduct even in their personal lives players for the tokyo giants for example must always\n",
            "Sample 1 .  the order printers start to go on the big board trading floor says one specialist there <eos> the buyers walk\n",
            "Sample 2 .  leading indicators got a major boost in september from a surge in consumer expectations as measured by the university of\n",
            "\n",
            "\n",
            "targets[[1 5 75 1040 773 3828 7 203 2 485 0 2301 7169 4 161 38 275 489 5 1419][2 102 54 13 45 7210 22 156 77 55 0 507 9 7732 2 38 26 147 547 5][35 3661 5 0 3969 263 5 9870 0 216...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1 5 75 1040 773 3828 7 203 2...]...][[0 1 1 1 0 0 0 0 0 0...]...][[1 10000 5 75 1040 10000 10000 10000 10000 10000...]...][[1 5 75 1040 773 3828 7 203 2 485...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1 5 75 1040 773 3828 7 203 2...]...][[0 1 1 1 0 0 0 0 0 0...]...][[1 10000 5 75 1040 10000 10000 10000 10000 10000...]...][[1 5 75 1040 773 3828 7 203 2 485...]...]\n",
            "I0212 04:15:52.743887 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[4599 2241 67 16 0 2104 2 110 1 34 1 13 0 516 7846 1 4 242 140 1][597 8 0 2569 13 696 1322 19 0 1988 4 69 3222 11 49 2759 4 149 28 2297][3031 2 39 854 50 556 1056 7 397 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[849 4599 2241 67 16 0 2104 2 110 1...]...][[1 0 0 0 0 0 1 1 1 0...]...][[849 4599 10000 10000 10000 10000 10000 2 110 1...]...][[4599 2241 67 16 0 2104 2 110 1 34...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[849 4599 2241 67 16 0 2104 2 110 1...]...][[1 0 0 0 0 0 1 1 1 0...]...][[849 4599 10000 10000 10000 10000 10000 2 110 1...]...][[4599 2241 67 16 0 2104 2 110 1 34...]...]\n",
            "targets[[4599 2241 67 16 0 2104 2 110 1 34 1 13 0 516 7846 1 4 242 140 1][597 8 0 2569 13 696 1322 19 0 1988 4 69 3222 11 49 2759 4 149 28 2297][3031 2 39 854 50 556 1056 7 397 2...]...]\n",
            "targets[[4599 2241 67 16 0 2104 2 110 1 34 1 13 0 516 7846 1 4 242 140 1][597 8 0 2569 13 696 1322 19 0 1988 4 69 3222 11 49 2759 4 149 28 2297][3031 2 39 854 50 556 1056 7 397 2...]...]\n",
            "targets[[4599 2241 67 16 0 2104 2 110 1 34 1 13 0 516 7846 1 4 242 140 1][597 8 0 2569 13 696 1322 19 0 1988 4 69 3222 11 49 2759 4 149 28 2297][3031 2 39 854 50 556 1056 7 397 2...]...]\n",
            "global_step: 1500\n",
            " perplexity: 680.133\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[4599 2241 67 16 0 2104 2 110 1 34 1 13 0 516 7846 1 4 242 140 1][597 8 0 2569 13 696 1322 19 0 1988 4 69 3222 11 49 2759 4 149 28 2297][3031 2 39 854 50 556 1056 7 397 2...]...]\n",
            " percent of 3-grams captured: 0.172.\n",
            "targets[[4599 2241 67 16 0 2104 2 110 1 34 1 13 0 516 7846 1 4 242 140 1][597 8 0 2569 13 696 1322 19 0 1988 4 69 3222 11 49 2759 4 149 28 2297][3031 2 39 854 50 556 1056 7 397 2...]...]\n",
            " percent of 2-grams captured: 0.518.\n",
            "targets[[4599 2241 67 16 0 2104 2 110 1 34 1 13 0 516 7846 1 4 242 140 1][597 8 0 2569 13 696 1322 19 0 1988 4 69 3222 11 49 2759 4 149 28 2297][3031 2 39 854 50 556 1056 7 397 2...]...]\n",
            " percent of 4-grams captured: 0.053.\n",
            " geometric_avg: 0.168.\n",
            " arithmetic_avg: 0.248.\n",
            "global_step: 1500\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69630\n",
            " G train loss: 3.29435\n",
            "targets[[4599 2241 67 16 0 2104 2 110 1 34 1 13 0 516 7846 1 4 242 140 1][597 8 0 2569 13 696 1322 19 0 1988 4 69 3222 11 49 2759 4 149 28 2297][3031 2 39 854 50 556 1056 7 397 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[849 4599 2241 67 16 0 2104 2 110 1...]...][[1 0 0 0 0 0 1 1 1 0...]...][[849 4599 10000 10000 10000 10000 10000 2 110 1...]...][[4599 2241 67 16 0 2104 2 110 1 34...]...]\n",
            " Sample 0.\n",
            "   [1]  wear                  0.519   0.000  \n",
            "   [0]  ties                  0.512   11.035 \n",
            "   [0]  when                  0.510   6.228  \n",
            "   [0]  on                    0.507   5.650  \n",
            "   [0]  the                   0.506   1.709  \n",
            "   [0]  road                  0.505   10.590 \n",
            "   [1]  <eos>                 0.505   0.000  \n",
            "   [1]  you                   0.506   0.000  \n",
            "   [1]  <unk>                 0.508   0.000  \n",
            "   [0]  have                  0.510   4.724  \n",
            "   [0]  <unk>                 0.513   2.748  \n",
            "   [1]  is                    0.514   0.000  \n",
            "   [1]  the                   0.515   0.000  \n",
            "   [1]  often                 0.515   0.000  \n",
            "   [0]  amusing               0.514   11.951 \n",
            "   [0]  <unk>                 0.515   3.532  \n",
            "   [1]  of                    0.513   0.000  \n",
            "   [0]  how                   0.509   7.096  \n",
            "   [0]  american              0.505   7.139  \n",
            "   [1]  <unk>                 0.505   0.000  \n",
            " Sample 1.\n",
            "   [1]  away                  0.478   0.000  \n",
            "   [1]  and                   0.476   0.000  \n",
            "   [0]  the                   0.476   2.277  \n",
            "   [1]  specialist            0.477   0.000  \n",
            "   [1]  is                    0.477   0.000  \n",
            "   [0]  left                  0.479   8.522  \n",
            "   [0]  alone                 0.478   8.958  \n",
            "   [0]  as                    0.479   4.837  \n",
            "   [1]  the                   0.480   0.000  \n",
            "   [1]  buyer                 0.482   0.000  \n",
            "   [1]  of                    0.484   0.000  \n",
            "   [0]  last                  0.484   6.511  \n",
            "   [0]  resort                0.483   10.187 \n",
            "   [1]  for                   0.482   0.000  \n",
            "   [1]  his                   0.481   0.000  \n",
            "   [0]  stable                0.479   10.023 \n",
            "   [1]  of                    0.480   0.000  \n",
            "   [1]  stocks                0.479   0.000  \n",
            "   [1]  he                    0.478   0.000  \n",
            "   [1]  contends              0.477   0.000  \n",
            " Sample 2.\n",
            "   [1]  michigan              0.588   0.000  \n",
            "   [1]  <eos>                 0.580   0.000  \n",
            "   [0]  this                  0.579   4.806  \n",
            "   [0]  measure               0.582   7.968  \n",
            "   [1]  had                   0.585   0.000  \n",
            "   [0]  dropped               0.585   8.831  \n",
            "   [1]  sharply               0.584   0.000  \n",
            "   [0]  in                    0.583   3.769  \n",
            "   [0]  august                0.583   5.783  \n",
            "   [0]  <eos>                 0.581   3.346  \n",
            "   [0]  the                   0.578   1.564  \n",
            "   [1]  commerce              0.578   0.000  \n",
            "   [0]  department            0.577   6.329  \n",
            "   [1]  said                  0.576   0.000  \n",
            "   [0]  that                  0.578   3.769  \n",
            "   [1]  as                    0.581   0.000  \n",
            "   [0]  a                     0.586   2.673  \n",
            "   [1]  result                0.587   0.000  \n",
            "   [0]  of                    0.585   2.136  \n",
            "   [1]  a                     0.587   0.000  \n",
            "Samples\n",
            "Sample 0 .  wear ties when on the road <eos> you <unk> have <unk> is the often amusing <unk> of how american <unk>\n",
            "Sample 1 .  away and the specialist is left alone as the buyer of last resort for his stable of stocks he contends\n",
            "Sample 2 .  michigan <eos> this measure had dropped sharply in august <eos> the commerce department said that as a result of a\n",
            "\n",
            "\n",
            "targets[[52 18 6 173 5491 6 262 297 1880 14 9 63 985 14 8 611 11 318 2 5129][5173 192 300 116 9440 17 0 3 2415 1037 8 0 47 9 2042 155 223 2 300 116][0 3 3809 5 0 216 86 132 493 119...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1419 52 18 6 173 5491 6 262 297 1880...]...][[0 1 0 1 1 1 0 1 1 0...]...][[1419 10000 18 10000 173 5491 6 10000 297 1880...]...][[52 18 6 173 5491 6 262 297 1880 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1419 52 18 6 173 5491 6 262 297 1880...]...][[0 1 0 1 1 1 0 1 1 0...]...][[1419 10000 18 10000 173 5491 6 10000 297 1880...]...][[52 18 6 173 5491 6 262 297 1880 14...]...]\n",
            "I0212 04:15:56.264544 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1 5 75 1040 773 3828 7 203 2 485 0 2301 7169 4 161 38 275 489 5 1419][2 102 54 13 45 7210 22 156 77 55 0 507 9 7732 2 38 26 147 547 5][35 3661 5 0 3969 263 5 9870 0 216...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1 5 75 1040 773 3828 7 203 2...]...][[1 0 0 1 1 1 0 1 0 1...]...][[1 1 10000 10000 1040 773 3828 10000 203 10000...]...][[1 5 75 1040 773 3828 7 203 2 485...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1 5 75 1040 773 3828 7 203 2...]...][[1 0 0 1 1 1 0 1 0 1...]...][[1 1 10000 10000 1040 773 3828 10000 203 10000...]...][[1 5 75 1040 773 3828 7 203 2 485...]...]\n",
            "targets[[1 5 75 1040 773 3828 7 203 2 485 0 2301 7169 4 161 38 275 489 5 1419][2 102 54 13 45 7210 22 156 77 55 0 507 9 7732 2 38 26 147 547 5][35 3661 5 0 3969 263 5 9870 0 216...]...]\n",
            "targets[[1 5 75 1040 773 3828 7 203 2 485 0 2301 7169 4 161 38 275 489 5 1419][2 102 54 13 45 7210 22 156 77 55 0 507 9 7732 2 38 26 147 547 5][35 3661 5 0 3969 263 5 9870 0 216...]...]\n",
            "targets[[1 5 75 1040 773 3828 7 203 2 485 0 2301 7169 4 161 38 275 489 5 1419][2 102 54 13 45 7210 22 156 77 55 0 507 9 7732 2 38 26 147 547 5][35 3661 5 0 3969 263 5 9870 0 216...]...]\n",
            "global_step: 1503\n",
            " perplexity: 679.589\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1 5 75 1040 773 3828 7 203 2 485 0 2301 7169 4 161 38 275 489 5 1419][2 102 54 13 45 7210 22 156 77 55 0 507 9 7732 2 38 26 147 547 5][35 3661 5 0 3969 263 5 9870 0 216...]...]\n",
            " percent of 3-grams captured: 0.169.\n",
            "targets[[1 5 75 1040 773 3828 7 203 2 485 0 2301 7169 4 161 38 275 489 5 1419][2 102 54 13 45 7210 22 156 77 55 0 507 9 7732 2 38 26 147 547 5][35 3661 5 0 3969 263 5 9870 0 216...]...]\n",
            " percent of 2-grams captured: 0.532.\n",
            "targets[[1 5 75 1040 773 3828 7 203 2 485 0 2301 7169 4 161 38 275 489 5 1419][2 102 54 13 45 7210 22 156 77 55 0 507 9 7732 2 38 26 147 547 5][35 3661 5 0 3969 263 5 9870 0 216...]...]\n",
            " percent of 4-grams captured: 0.065.\n",
            " geometric_avg: 0.180.\n",
            " arithmetic_avg: 0.255.\n",
            "global_step: 1503\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69630\n",
            " G train loss: 3.29385\n",
            "targets[[1 5 75 1040 773 3828 7 203 2 485 0 2301 7169 4 161 38 275 489 5 1419][2 102 54 13 45 7210 22 156 77 55 0 507 9 7732 2 38 26 147 547 5][35 3661 5 0 3969 263 5 9870 0 216...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1 5 75 1040 773 3828 7 203 2...]...][[1 0 0 1 1 1 0 1 0 1...]...][[1 1 10000 10000 1040 773 3828 10000 203 10000...]...][[1 5 75 1040 773 3828 7 203 2 485...]...]\n",
            " Sample 0.\n",
            "   [1]  <unk>                 0.519   0.000  \n",
            "   [0]  to                    0.523   2.758  \n",
            "   [0]  two                   0.526   6.960  \n",
            "   [1]  per                   0.527   0.000  \n",
            "   [1]  team                  0.525   0.000  \n",
            "   [1]  fare                  0.520   0.000  \n",
            "   [0]  in                    0.517   3.453  \n",
            "   [1]  japan                 0.514   0.000  \n",
            "   [0]  <eos>                 0.509   2.337  \n",
            "   [1]  despite               0.505   0.000  \n",
            "   [0]  the                   0.504   2.720  \n",
            "   [0]  enormous              0.507   9.023  \n",
            "   [0]  sums                  0.510   11.530 \n",
            "   [1]  of                    0.511   0.000  \n",
            "   [0]  money                 0.514   7.716  \n",
            "   [0]  they                  0.515   6.268  \n",
            "   [1]  're                   0.514   0.000  \n",
            "   [0]  paid                  0.515   8.856  \n",
            "   [1]  to                    0.516   0.000  \n",
            "   [0]  stand                 0.514   9.592  \n",
            " Sample 1.\n",
            "   [0]  <eos>                 0.518   1.690  \n",
            "   [1]  no                    0.507   0.000  \n",
            "   [0]  one                   0.504   5.902  \n",
            "   [1]  is                    0.506   0.000  \n",
            "   [0]  more                  0.506   5.546  \n",
            "   [1]  unhappy               0.511   0.000  \n",
            "   [1]  with                  0.511   0.000  \n",
            "   [1]  program               0.507   0.000  \n",
            "   [0]  trading               0.507   6.649  \n",
            "   [0]  than                  0.504   4.938  \n",
            "   [1]  the                   0.502   0.000  \n",
            "   [1]  nation                0.503   0.000  \n",
            "   [1]  's                    0.502   0.000  \n",
            "   [1]  stockbrokers          0.501   0.000  \n",
            "   [1]  <eos>                 0.498   0.000  \n",
            "   [1]  they                  0.499   0.000  \n",
            "   [1]  are                   0.499   0.000  \n",
            "   [0]  still                 0.503   7.092  \n",
            "   [0]  trying                0.510   8.143  \n",
            "   [1]  to                    0.514   0.000  \n",
            " Sample 2.\n",
            "   [1]  new                   0.504   0.000  \n",
            "   [1]  adjustment            0.510   0.000  \n",
            "   [1]  to                    0.516   0.000  \n",
            "   [1]  the                   0.515   0.000  \n",
            "   [0]  formula               0.512   12.029 \n",
            "   [0]  used                  0.511   6.963  \n",
            "   [0]  to                    0.509   2.758  \n",
            "   [0]  calculate             0.506   12.298 \n",
            "   [0]  the                   0.500   3.868  \n",
            "   [1]  index                 0.495   0.000  \n",
            "   [1]  the                   0.489   0.000  \n",
            "   [1]  influence             0.487   0.000  \n",
            "   [1]  of                    0.488   0.000  \n",
            "   [0]  this                  0.490   5.260  \n",
            "   [1]  component             0.494   0.000  \n",
            "   [0]  has                   0.496   5.087  \n",
            "   [0]  been                  0.499   5.629  \n",
            "   [1]  reduced               0.499   0.000  \n",
            "   [0]  <eos>                 0.500   2.398  \n",
            "   [1]  of                    0.501   0.000  \n",
            "Samples\n",
            "Sample 0 .  <unk> to two per team fare in japan <eos> despite the enormous sums of money they 're paid to stand\n",
            "Sample 1 .  <eos> no one is more unhappy with program trading than the nation 's stockbrokers <eos> they are still trying to\n",
            "Sample 2 .  new adjustment to the formula used to calculate the index the influence of this component has been reduced <eos> of\n",
            "\n",
            "\n",
            "targets[[85 1 3 1975 12 3 17 3185 1 13 996 29 2 14 9 0 1 3459 4 31][26 3921 1 10 330 321 13 1 0 7359 174 127 8 144 1837 2042 26 9384 127 5][221 123 8 60 112 2 249 3809 10 2282...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5129 85 1 3 1975 12 3 17 3185 1...]...][[0 1 1 0 0 1 1 0 1 1...]...][[5129 10000 1 3 10000 10000 3 17 10000 1...]...][[85 1 3 1975 12 3 17 3185 1 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5129 85 1 3 1975 12 3 17 3185 1...]...][[0 1 1 0 0 1 1 0 1 1...]...][[5129 10000 1 3 10000 10000 3 17 10000 1...]...][[85 1 3 1975 12 3 17 3185 1 13...]...]\n",
            "I0212 04:15:59.699130 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[52 18 6 173 5491 6 262 297 1880 14 9 63 985 14 8 611 11 318 2 5129][5173 192 300 116 9440 17 0 3 2415 1037 8 0 47 9 2042 155 223 2 300 116][0 3 3809 5 0 216 86 132 493 119...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1419 52 18 6 173 5491 6 262 297 1880...]...][[1 1 1 1 0 1 0 0 1 1...]...][[1419 52 18 6 173 10000 6 10000 10000 1880...]...][[52 18 6 173 5491 6 262 297 1880 14...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1419 52 18 6 173 5491 6 262 297 1880...]...][[1 1 1 1 0 1 0 0 1 1...]...][[1419 52 18 6 173 10000 6 10000 10000 1880...]...][[52 18 6 173 5491 6 262 297 1880 14...]...]\n",
            "targets[[52 18 6 173 5491 6 262 297 1880 14 9 63 985 14 8 611 11 318 2 5129][5173 192 300 116 9440 17 0 3 2415 1037 8 0 47 9 2042 155 223 2 300 116][0 3 3809 5 0 216 86 132 493 119...]...]\n",
            "targets[[52 18 6 173 5491 6 262 297 1880 14 9 63 985 14 8 611 11 318 2 5129][5173 192 300 116 9440 17 0 3 2415 1037 8 0 47 9 2042 155 223 2 300 116][0 3 3809 5 0 216 86 132 493 119...]...]\n",
            "targets[[52 18 6 173 5491 6 262 297 1880 14 9 63 985 14 8 611 11 318 2 5129][5173 192 300 116 9440 17 0 3 2415 1037 8 0 47 9 2042 155 223 2 300 116][0 3 3809 5 0 216 86 132 493 119...]...]\n",
            "global_step: 1506\n",
            " perplexity: 679.083\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[52 18 6 173 5491 6 262 297 1880 14 9 63 985 14 8 611 11 318 2 5129][5173 192 300 116 9440 17 0 3 2415 1037 8 0 47 9 2042 155 223 2 300 116][0 3 3809 5 0 216 86 132 493 119...]...]\n",
            " percent of 3-grams captured: 0.183.\n",
            "targets[[52 18 6 173 5491 6 262 297 1880 14 9 63 985 14 8 611 11 318 2 5129][5173 192 300 116 9440 17 0 3 2415 1037 8 0 47 9 2042 155 223 2 300 116][0 3 3809 5 0 216 86 132 493 119...]...]\n",
            " percent of 2-grams captured: 0.476.\n",
            "targets[[52 18 6 173 5491 6 262 297 1880 14 9 63 985 14 8 611 11 318 2 5129][5173 192 300 116 9440 17 0 3 2415 1037 8 0 47 9 2042 155 223 2 300 116][0 3 3809 5 0 216 86 132 493 119...]...]\n",
            " percent of 4-grams captured: 0.076.\n",
            " geometric_avg: 0.188.\n",
            " arithmetic_avg: 0.245.\n",
            "global_step: 1506\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69629\n",
            " G train loss: 3.29342\n",
            "targets[[52 18 6 173 5491 6 262 297 1880 14 9 63 985 14 8 611 11 318 2 5129][5173 192 300 116 9440 17 0 3 2415 1037 8 0 47 9 2042 155 223 2 300 116][0 3 3809 5 0 216 86 132 493 119...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1419 52 18 6 173 5491 6 262 297 1880...]...][[1 1 1 1 0 1 0 0 1 1...]...][[1419 52 18 6 173 10000 6 10000 10000 1880...]...][[52 18 6 173 5491 6 262 297 1880 14...]...]\n",
            " Sample 0.\n",
            "   [1]  up                    0.507   0.000  \n",
            "   [1]  at                    0.506   0.000  \n",
            "   [1]  a                     0.507   0.000  \n",
            "   [1]  japanese              0.504   0.000  \n",
            "   [0]  plate                 0.500   10.916 \n",
            "   [1]  a                     0.502   0.000  \n",
            "   [0]  good                  0.502   6.872  \n",
            "   [0]  number                0.503   7.104  \n",
            "   [1]  decide                0.502   0.000  \n",
            "   [1]  it                    0.503   0.000  \n",
            "   [0]  's                    0.502   3.727  \n",
            "   [0]  not                   0.500   6.314  \n",
            "   [1]  worth                 0.499   0.000  \n",
            "   [0]  it                    0.499   4.964  \n",
            "   [1]  and                   0.499   0.000  \n",
            "   [1]  run                   0.496   0.000  \n",
            "   [0]  for                   0.492   4.928  \n",
            "   [1]  home                  0.490   0.000  \n",
            "   [1]  <eos>                 0.489   0.000  \n",
            "   [0]  funny                 0.490   11.556 \n",
            " Sample 1.\n",
            "   [1]  lure                  0.516   0.000  \n",
            "   [1]  back                  0.517   0.000  \n",
            "   [1]  small                 0.513   0.000  \n",
            "   [1]  investors             0.506   0.000  \n",
            "   [1]  spooked               0.502   0.000  \n",
            "   [1]  by                    0.498   0.000  \n",
            "   [0]  the                   0.494   1.706  \n",
            "   [0]  N                     0.491   5.987  \n",
            "   [0]  stock-market          0.491   10.556 \n",
            "   [1]  crash                 0.491   0.000  \n",
            "   [0]  and                   0.491   2.744  \n",
            "   [0]  the                   0.492   2.188  \n",
            "   [0]  market                0.494   6.006  \n",
            "   [1]  's                    0.494   0.000  \n",
            "   [1]  swings                0.495   0.000  \n",
            "   [0]  since                 0.495   6.560  \n",
            "   [0]  then                  0.494   7.444  \n",
            "   [0]  <eos>                 0.495   4.224  \n",
            "   [1]  small                 0.498   0.000  \n",
            "   [0]  investors             0.499   6.803  \n",
            " Sample 2.\n",
            "   [0]  the                   0.519   1.932  \n",
            "   [0]  N                     0.510   4.701  \n",
            "   [0]  components            0.506   10.644 \n",
            "   [1]  to                    0.504   0.000  \n",
            "   [0]  the                   0.501   2.633  \n",
            "   [0]  index                 0.501   7.426  \n",
            "   [0]  only                  0.500   7.588  \n",
            "   [1]  three                 0.497   0.000  \n",
            "   [0]  others                0.495   7.732  \n",
            "   [0]  rose                  0.494   6.473  \n",
            "   [1]  in                    0.493   0.000  \n",
            "   [0]  september             0.493   3.525  \n",
            "   [0]  the                   0.493   4.560  \n",
            "   [1]  money                 0.496   0.000  \n",
            "   [0]  supply                0.498   8.652  \n",
            "   [1]  the                   0.497   0.000  \n",
            "   [0]  length                0.498   11.126 \n",
            "   [1]  of                    0.497   0.000  \n",
            "   [0]  the                   0.494   1.826  \n",
            "   [1]  average               0.498   0.000  \n",
            "Samples\n",
            "Sample 0 .  up at a japanese plate a good number decide it 's not worth it and run for home <eos> funny\n",
            "Sample 1 .  lure back small investors spooked by the N stock-market crash and the market 's swings since then <eos> small investors\n",
            "Sample 2 .  the N components to the index only three others rose in september the money supply the length of the average\n",
            "\n",
            "\n",
            "targets[[1 140 1728 1152 1 11 6 40 120 28 24 16 6 1 1 7 605 5 0 7662][1233 44 3874 941 4146 141 4 1501 1843 9105 4146 80 7 2714 2 7732 134 85 8 254][118 7 311 2 144 535 35 455 11 5175...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 1 140 1728 1152 1 11 6 40 120...]...][[1 0 1 0 0 0 0 1 1 1...]...][[31 1 10000 1728 10000 10000 10000 10000 40 120...]...][[1 140 1728 1152 1 11 6 40 120 28...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 1 140 1728 1152 1 11 6 40 120...]...][[1 0 1 0 0 0 0 1 1 1...]...][[31 1 10000 1728 10000 10000 10000 10000 40 120...]...][[1 140 1728 1152 1 11 6 40 120 28...]...]\n",
            "I0212 04:16:02.966460 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[85 1 3 1975 12 3 17 3185 1 13 996 29 2 14 9 0 1 3459 4 31][26 3921 1 10 330 321 13 1 0 7359 174 127 8 144 1837 2042 26 9384 127 5][221 123 8 60 112 2 249 3809 10 2282...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5129 85 1 3 1975 12 3 17 3185 1...]...][[0 0 0 1 0 0 0 0 0 0...]...][[5129 10000 10000 10000 1975 10000 10000 10000 10000 10000...]...][[85 1 3 1975 12 3 17 3185 1 13...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5129 85 1 3 1975 12 3 17 3185 1...]...][[0 0 0 1 0 0 0 0 0 0...]...][[5129 10000 10000 10000 1975 10000 10000 10000 10000 10000...]...][[85 1 3 1975 12 3 17 3185 1 13...]...]\n",
            "targets[[85 1 3 1975 12 3 17 3185 1 13 996 29 2 14 9 0 1 3459 4 31][26 3921 1 10 330 321 13 1 0 7359 174 127 8 144 1837 2042 26 9384 127 5][221 123 8 60 112 2 249 3809 10 2282...]...]\n",
            "targets[[85 1 3 1975 12 3 17 3185 1 13 996 29 2 14 9 0 1 3459 4 31][26 3921 1 10 330 321 13 1 0 7359 174 127 8 144 1837 2042 26 9384 127 5][221 123 8 60 112 2 249 3809 10 2282...]...]\n",
            "targets[[85 1 3 1975 12 3 17 3185 1 13 996 29 2 14 9 0 1 3459 4 31][26 3921 1 10 330 321 13 1 0 7359 174 127 8 144 1837 2042 26 9384 127 5][221 123 8 60 112 2 249 3809 10 2282...]...]\n",
            "global_step: 1509\n",
            " perplexity: 678.184\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[85 1 3 1975 12 3 17 3185 1 13 996 29 2 14 9 0 1 3459 4 31][26 3921 1 10 330 321 13 1 0 7359 174 127 8 144 1837 2042 26 9384 127 5][221 123 8 60 112 2 249 3809 10 2282...]...]\n",
            " percent of 3-grams captured: 0.211.\n",
            "targets[[85 1 3 1975 12 3 17 3185 1 13 996 29 2 14 9 0 1 3459 4 31][26 3921 1 10 330 321 13 1 0 7359 174 127 8 144 1837 2042 26 9384 127 5][221 123 8 60 112 2 249 3809 10 2282...]...]\n",
            " percent of 2-grams captured: 0.468.\n",
            "targets[[85 1 3 1975 12 3 17 3185 1 13 996 29 2 14 9 0 1 3459 4 31][26 3921 1 10 330 321 13 1 0 7359 174 127 8 144 1837 2042 26 9384 127 5][221 123 8 60 112 2 249 3809 10 2282...]...]\n",
            " percent of 4-grams captured: 0.074.\n",
            " geometric_avg: 0.194.\n",
            " arithmetic_avg: 0.251.\n",
            "global_step: 1509\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69629\n",
            " G train loss: 3.29343\n",
            "targets[[85 1 3 1975 12 3 17 3185 1 13 996 29 2 14 9 0 1 3459 4 31][26 3921 1 10 330 321 13 1 0 7359 174 127 8 144 1837 2042 26 9384 127 5][221 123 8 60 112 2 249 3809 10 2282...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5129 85 1 3 1975 12 3 17 3185 1...]...][[0 0 0 1 0 0 0 0 0 0...]...][[5129 10000 10000 10000 1975 10000 10000 10000 10000 10000...]...][[85 1 3 1975 12 3 17 3185 1 13...]...]\n",
            " Sample 0.\n",
            "   [0]  business              0.462   7.405  \n",
            "   [0]  <unk>                 0.457   3.647  \n",
            "   [0]  N                     0.450   6.097  \n",
            "   [1]  pages                 0.445   0.000  \n",
            "   [0]  $                     0.443   4.248  \n",
            "   [0]  N                     0.440   0.033  \n",
            "   [0]  by                    0.438   6.513  \n",
            "   [0]  gary                  0.440   10.882 \n",
            "   [0]  <unk>                 0.442   4.361  \n",
            "   [0]  is                    0.441   4.398  \n",
            "   [0]  anything              0.441   8.199  \n",
            "   [0]  but                   0.442   4.623  \n",
            "   [1]  <eos>                 0.441   0.000  \n",
            "   [0]  it                    0.442   4.176  \n",
            "   [1]  's                    0.441   0.000  \n",
            "   [0]  the                   0.438   3.111  \n",
            "   [1]  <unk>                 0.439   0.000  \n",
            "   [0]  complaint             0.438   10.899 \n",
            "   [1]  of                    0.439   0.000  \n",
            "   [0]  an                    0.438   4.667  \n",
            " Sample 1.\n",
            "   [1]  are                   0.465   0.000  \n",
            "   [1]  absolutely            0.466   0.000  \n",
            "   [0]  <unk>                 0.473   3.009  \n",
            "   [0]  that                  0.480   4.131  \n",
            "   [1]  wall                  0.486   0.000  \n",
            "   [0]  street                0.489   6.997  \n",
            "   [1]  is                    0.490   0.000  \n",
            "   [1]  <unk>                 0.491   0.000  \n",
            "   [1]  the                   0.489   0.000  \n",
            "   [0]  deck                  0.488   10.784 \n",
            "   [0]  against               0.487   7.061  \n",
            "   [1]  them                  0.486   0.000  \n",
            "   [1]  and                   0.485   0.000  \n",
            "   [0]  these                 0.486   7.138  \n",
            "   [0]  wide                  0.487   10.186 \n",
            "   [0]  swings                0.487   9.888  \n",
            "   [1]  are                   0.487   0.000  \n",
            "   [0]  scaring               0.489   10.567 \n",
            "   [1]  them                  0.489   0.000  \n",
            "   [0]  to                    0.490   2.314  \n",
            " Sample 2.\n",
            "   [0]  work                  0.521   8.002  \n",
            "   [1]  week                  0.522   0.000  \n",
            "   [0]  and                   0.522   2.591  \n",
            "   [1]  stock                 0.527   0.000  \n",
            "   [0]  prices                0.528   6.533  \n",
            "   [0]  <eos>                 0.530   2.699  \n",
            "   [1]  several               0.531   0.000  \n",
            "   [1]  components            0.529   0.000  \n",
            "   [0]  that                  0.530   4.500  \n",
            "   [0]  track                 0.530   8.768  \n",
            "   [0]  the                   0.527   4.687  \n",
            "   [1]  health                0.525   0.000  \n",
            "   [0]  of                    0.523   4.032  \n",
            "   [0]  the                   0.521   1.685  \n",
            "   [0]  manufacturing         0.519   7.309  \n",
            "   [0]  sector                0.519   6.963  \n",
            "   [1]  of                    0.519   0.000  \n",
            "   [0]  the                   0.518   1.409  \n",
            "   [0]  economy               0.520   7.452  \n",
            "   [1]  turned                0.522   0.000  \n",
            "Samples\n",
            "Sample 0 .  business <unk> N pages $ N by gary <unk> is anything but <eos> it 's the <unk> complaint of an\n",
            "Sample 1 .  are absolutely <unk> that wall street is <unk> the deck against them and these wide swings are scaring them to\n",
            "Sample 2 .  work week and stock prices <eos> several components that track the health of the manufacturing sector of the economy turned\n",
            "\n",
            "\n",
            "targets[[4 179 1556 2 7 1279 7846 45 516 1 113 8518 1 23 1 4672 242 1152 1 113][30 58 1760 2 69 40 0 196 1843 1019 12 3 3 3 230 55 7 3 2 352][402 8 548 8 7872 4 455 11 3369 803...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7662 4 179 1556 2 7 1279 7846 45 516...]...][[0 1 1 1 1 1 0 0 0 1...]...][[7662 10000 179 1556 2 7 1279 10000 10000 10000...]...][[4 179 1556 2 7 1279 7846 45 516 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7662 4 179 1556 2 7 1279 7846 45 516...]...][[0 1 1 1 1 1 0 0 0 1...]...][[7662 10000 179 1556 2 7 1279 10000 10000 10000...]...][[4 179 1556 2 7 1279 7846 45 516 1...]...]\n",
            "I0212 04:16:06.315638 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1 140 1728 1152 1 11 6 40 120 28 24 16 6 1 1 7 605 5 0 7662][1233 44 3874 941 4146 141 4 1501 1843 9105 4146 80 7 2714 2 7732 134 85 8 254][118 7 311 2 144 535 35 455 11 5175...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 1 140 1728 1152 1 11 6 40 120...]...][[1 1 1 0 0 0 0 1 0 0...]...][[31 1 140 1728 10000 10000 10000 10000 40 10000...]...][[1 140 1728 1152 1 11 6 40 120 28...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 1 140 1728 1152 1 11 6 40 120...]...][[1 1 1 0 0 0 0 1 0 0...]...][[31 1 140 1728 10000 10000 10000 10000 40 10000...]...][[1 140 1728 1152 1 11 6 40 120 28...]...]\n",
            "targets[[1 140 1728 1152 1 11 6 40 120 28 24 16 6 1 1 7 605 5 0 7662][1233 44 3874 941 4146 141 4 1501 1843 9105 4146 80 7 2714 2 7732 134 85 8 254][118 7 311 2 144 535 35 455 11 5175...]...]\n",
            "targets[[1 140 1728 1152 1 11 6 40 120 28 24 16 6 1 1 7 605 5 0 7662][1233 44 3874 941 4146 141 4 1501 1843 9105 4146 80 7 2714 2 7732 134 85 8 254][118 7 311 2 144 535 35 455 11 5175...]...]\n",
            "targets[[1 140 1728 1152 1 11 6 40 120 28 24 16 6 1 1 7 605 5 0 7662][1233 44 3874 941 4146 141 4 1501 1843 9105 4146 80 7 2714 2 7732 134 85 8 254][118 7 311 2 144 535 35 455 11 5175...]...]\n",
            "global_step: 1512\n",
            " perplexity: 677.629\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1 140 1728 1152 1 11 6 40 120 28 24 16 6 1 1 7 605 5 0 7662][1233 44 3874 941 4146 141 4 1501 1843 9105 4146 80 7 2714 2 7732 134 85 8 254][118 7 311 2 144 535 35 455 11 5175...]...]\n",
            " percent of 3-grams captured: 0.164.\n",
            "targets[[1 140 1728 1152 1 11 6 40 120 28 24 16 6 1 1 7 605 5 0 7662][1233 44 3874 941 4146 141 4 1501 1843 9105 4146 80 7 2714 2 7732 134 85 8 254][118 7 311 2 144 535 35 455 11 5175...]...]\n",
            " percent of 2-grams captured: 0.437.\n",
            "targets[[1 140 1728 1152 1 11 6 40 120 28 24 16 6 1 1 7 605 5 0 7662][1233 44 3874 941 4146 141 4 1501 1843 9105 4146 80 7 2714 2 7732 134 85 8 254][118 7 311 2 144 535 35 455 11 5175...]...]\n",
            " percent of 4-grams captured: 0.047.\n",
            " geometric_avg: 0.150.\n",
            " arithmetic_avg: 0.216.\n",
            "global_step: 1512\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69636\n",
            " G train loss: 3.29306\n",
            "targets[[1 140 1728 1152 1 11 6 40 120 28 24 16 6 1 1 7 605 5 0 7662][1233 44 3874 941 4146 141 4 1501 1843 9105 4146 80 7 2714 2 7732 134 85 8 254][118 7 311 2 144 535 35 455 11 5175...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[31 1 140 1728 1152 1 11 6 40 120...]...][[1 1 1 0 0 0 0 1 0 0...]...][[31 1 140 1728 10000 10000 10000 10000 40 10000...]...][[1 140 1728 1152 1 11 6 40 120 28...]...]\n",
            " Sample 0.\n",
            "   [1]  <unk>                 0.505   0.000  \n",
            "   [1]  american              0.500   0.000  \n",
            "   [1]  whom                  0.499   0.000  \n",
            "   [0]  sony                  0.498   9.845  \n",
            "   [0]  <unk>                 0.498   3.206  \n",
            "   [0]  for                   0.497   4.187  \n",
            "   [0]  a                     0.500   3.172  \n",
            "   [1]  year                  0.502   0.000  \n",
            "   [0]  while                 0.500   7.352  \n",
            "   [0]  he                    0.497   6.129  \n",
            "   [1]  was                   0.493   0.000  \n",
            "   [1]  on                    0.489   0.000  \n",
            "   [0]  a                     0.490   3.372  \n",
            "   [1]  <unk>                 0.491   0.000  \n",
            "   [1]  <unk>                 0.492   0.000  \n",
            "   [0]  in                    0.493   3.862  \n",
            "   [1]  tokyo                 0.494   0.000  \n",
            "   [0]  to                    0.497   4.376  \n",
            "   [1]  the                   0.499   0.000  \n",
            "   [0]  regret                0.499   11.661 \n",
            " Sample 1.\n",
            "   [0]  death                 0.472   7.941  \n",
            "   [0]  says                  0.472   5.759  \n",
            "   [0]  raymond               0.474   10.331 \n",
            "   [1]  a.                    0.475   0.000  \n",
            "   [1]  mason                 0.476   0.000  \n",
            "   [1]  chairman              0.479   0.000  \n",
            "   [1]  of                    0.480   0.000  \n",
            "   [0]  regional              0.480   8.195  \n",
            "   [0]  broker                0.479   10.392 \n",
            "   [1]  legg                  0.478   0.000  \n",
            "   [1]  mason                 0.478   0.000  \n",
            "   [1]  inc.                  0.479   0.000  \n",
            "   [0]  in                    0.481   3.409  \n",
            "   [1]  baltimore             0.482   0.000  \n",
            "   [0]  <eos>                 0.483   3.541  \n",
            "   [1]  stockbrokers          0.484   0.000  \n",
            "   [0]  '                     0.482   6.206  \n",
            "   [1]  business              0.482   0.000  \n",
            "   [1]  and                   0.478   0.000  \n",
            "   [0]  pay                   0.477   7.714  \n",
            " Sample 2.\n",
            "   [1]  down                  0.515   0.000  \n",
            "   [1]  in                    0.511   0.000  \n",
            "   [1]  september             0.511   0.000  \n",
            "   [0]  <eos>                 0.508   2.209  \n",
            "   [0]  these                 0.506   6.827  \n",
            "   [0]  include               0.502   7.690  \n",
            "   [0]  new                   0.500   5.620  \n",
            "   [0]  orders                0.498   9.579  \n",
            "   [0]  for                   0.496   4.809  \n",
            "   [0]  manufactured          0.496   10.590 \n",
            "   [0]  consumer              0.498   7.854  \n",
            "   [1]  goods                 0.503   0.000  \n",
            "   [1]  lead                  0.504   0.000  \n",
            "   [1]  times                 0.505   0.000  \n",
            "   [1]  on                    0.505   0.000  \n",
            "   [1]  vendor                0.506   0.000  \n",
            "   [0]  deliveries            0.509   12.568 \n",
            "   [0]  orders                0.510   8.692  \n",
            "   [0]  for                   0.510   4.192  \n",
            "   [0]  new                   0.509   5.441  \n",
            "Samples\n",
            "Sample 0 .  <unk> american whom sony <unk> for a year while he was on a <unk> <unk> in tokyo to the regret\n",
            "Sample 1 .  death says raymond a. mason chairman of regional broker legg mason inc. in baltimore <eos> stockbrokers ' business and pay\n",
            "Sample 2 .  down in september <eos> these include new orders for manufactured consumer goods lead times on vendor deliveries orders for new\n",
            "\n",
            "\n",
            "targets[[0 90 9748 3345 4 27 431 134 2136 18 0 1 284 251 456 26 4425 4144 918 8][491 1 10 51 37 9 60 30 58 9513 91 6 1 2190 4 6 974 2986 2 216][181 2818 4 3 491 16 27 146 551 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[113 0 90 9748 3345 4 27 431 134 2136...]...][[0 1 1 0 0 1 0 0 0 0...]...][[113 10000 90 9748 10000 10000 27 10000 10000 10000...]...][[0 90 9748 3345 4 27 431 134 2136 18...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[113 0 90 9748 3345 4 27 431 134 2136...]...][[0 1 1 0 0 1 0 0 0 0...]...][[113 10000 90 9748 10000 10000 27 10000 10000 10000...]...][[0 90 9748 3345 4 27 431 134 2136 18...]...]\n",
            "I0212 04:16:09.669378 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[4 179 1556 2 7 1279 7846 45 516 1 113 8518 1 23 1 4672 242 1152 1 113][30 58 1760 2 69 40 0 196 1843 1019 12 3 3 3 230 55 7 3 2 352][402 8 548 8 7872 4 455 11 3369 803...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7662 4 179 1556 2 7 1279 7846 45 516...]...][[0 0 1 1 1 0 1 1 0 1...]...][[7662 10000 10000 1556 2 7 10000 7846 45 10000...]...][[4 179 1556 2 7 1279 7846 45 516 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7662 4 179 1556 2 7 1279 7846 45 516...]...][[0 0 1 1 1 0 1 1 0 1...]...][[7662 10000 10000 1556 2 7 10000 7846 45 10000...]...][[4 179 1556 2 7 1279 7846 45 516 1...]...]\n",
            "targets[[4 179 1556 2 7 1279 7846 45 516 1 113 8518 1 23 1 4672 242 1152 1 113][30 58 1760 2 69 40 0 196 1843 1019 12 3 3 3 230 55 7 3 2 352][402 8 548 8 7872 4 455 11 3369 803...]...]\n",
            "targets[[4 179 1556 2 7 1279 7846 45 516 1 113 8518 1 23 1 4672 242 1152 1 113][30 58 1760 2 69 40 0 196 1843 1019 12 3 3 3 230 55 7 3 2 352][402 8 548 8 7872 4 455 11 3369 803...]...]\n",
            "targets[[4 179 1556 2 7 1279 7846 45 516 1 113 8518 1 23 1 4672 242 1152 1 113][30 58 1760 2 69 40 0 196 1843 1019 12 3 3 3 230 55 7 3 2 352][402 8 548 8 7872 4 455 11 3369 803...]...]\n",
            "global_step: 1515\n",
            " perplexity: 676.913\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[4 179 1556 2 7 1279 7846 45 516 1 113 8518 1 23 1 4672 242 1152 1 113][30 58 1760 2 69 40 0 196 1843 1019 12 3 3 3 230 55 7 3 2 352][402 8 548 8 7872 4 455 11 3369 803...]...]\n",
            " percent of 3-grams captured: 0.172.\n",
            "targets[[4 179 1556 2 7 1279 7846 45 516 1 113 8518 1 23 1 4672 242 1152 1 113][30 58 1760 2 69 40 0 196 1843 1019 12 3 3 3 230 55 7 3 2 352][402 8 548 8 7872 4 455 11 3369 803...]...]\n",
            " percent of 2-grams captured: 0.447.\n",
            "targets[[4 179 1556 2 7 1279 7846 45 516 1 113 8518 1 23 1 4672 242 1152 1 113][30 58 1760 2 69 40 0 196 1843 1019 12 3 3 3 230 55 7 3 2 352][402 8 548 8 7872 4 455 11 3369 803...]...]\n",
            " percent of 4-grams captured: 0.071.\n",
            " geometric_avg: 0.176.\n",
            " arithmetic_avg: 0.230.\n",
            "global_step: 1515\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69639\n",
            " G train loss: 3.29229\n",
            "targets[[4 179 1556 2 7 1279 7846 45 516 1 113 8518 1 23 1 4672 242 1152 1 113][30 58 1760 2 69 40 0 196 1843 1019 12 3 3 3 230 55 7 3 2 352][402 8 548 8 7872 4 455 11 3369 803...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[7662 4 179 1556 2 7 1279 7846 45 516...]...][[0 0 1 1 1 0 1 1 0 1...]...][[7662 10000 10000 1556 2 7 10000 7846 45 10000...]...][[4 179 1556 2 7 1279 7846 45 516 1...]...]\n",
            " Sample 0.\n",
            "   [0]  of                    0.531   2.823  \n",
            "   [0]  both                  0.528   7.218  \n",
            "   [1]  parties               0.524   0.000  \n",
            "   [1]  <eos>                 0.517   0.000  \n",
            "   [1]  in                    0.511   0.000  \n",
            "   [0]  sometimes             0.508   9.384  \n",
            "   [1]  amusing               0.503   0.000  \n",
            "   [1]  more                  0.502   0.000  \n",
            "   [0]  often                 0.503   8.392  \n",
            "   [1]  <unk>                 0.505   0.000  \n",
            "   [1]  even                  0.506   0.000  \n",
            "   [0]  vicious               0.507   12.169 \n",
            "   [0]  <unk>                 0.507   3.310  \n",
            "   [1]  mr.                   0.502   0.000  \n",
            "   [0]  <unk>                 0.499   3.301  \n",
            "   [0]  describes             0.498   10.358 \n",
            "   [1]  how                   0.495   0.000  \n",
            "   [0]  sony                  0.494   10.876 \n",
            "   [1]  <unk>                 0.495   0.000  \n",
            "   [1]  even                  0.497   0.000  \n",
            " Sample 1.\n",
            "   [1]  has                   0.456   0.000  \n",
            "   [0]  been                  0.454   5.254  \n",
            "   [1]  falling               0.452   0.000  \n",
            "   [1]  <eos>                 0.451   0.000  \n",
            "   [0]  last                  0.451   6.088  \n",
            "   [1]  year                  0.451   0.000  \n",
            "   [1]  the                   0.448   0.000  \n",
            "   [0]  average               0.444   7.094  \n",
            "   [1]  broker                0.441   0.000  \n",
            "   [1]  earned                0.439   0.000  \n",
            "   [0]  $                     0.437   5.124  \n",
            "   [1]  N                     0.437   0.000  \n",
            "   [0]  N                     0.438   2.673  \n",
            "   [0]  N                     0.438   3.509  \n",
            "   [0]  lower                 0.439   6.915  \n",
            "   [1]  than                  0.440   0.000  \n",
            "   [0]  in                    0.439   5.606  \n",
            "   [1]  N                     0.438   0.000  \n",
            "   [1]  <eos>                 0.437   0.000  \n",
            "   [0]  corporate             0.434   7.936  \n",
            " Sample 2.\n",
            "   [1]  plant                 0.484   0.000  \n",
            "   [1]  and                   0.480   0.000  \n",
            "   [1]  equipment             0.477   0.000  \n",
            "   [1]  and                   0.475   0.000  \n",
            "   [0]  backlogs              0.473   12.166 \n",
            "   [1]  of                    0.470   0.000  \n",
            "   [0]  orders                0.466   7.585  \n",
            "   [1]  for                   0.464   0.000  \n",
            "   [1]  durable               0.464   0.000  \n",
            "   [0]  goods                 0.464   7.852  \n",
            "   [0]  <eos>                 0.463   2.490  \n",
            "   [1]  meanwhile             0.463   0.000  \n",
            "   [0]  the                   0.462   3.611  \n",
            "   [0]  national              0.461   6.265  \n",
            "   [0]  association           0.462   7.841  \n",
            "   [1]  of                    0.461   0.000  \n",
            "   [1]  manufacturers         0.459   0.000  \n",
            "   [1]  said                  0.457   0.000  \n",
            "   [1]  yesterday             0.457   0.000  \n",
            "   [1]  a                     0.464   0.000  \n",
            "Samples\n",
            "Sample 0 .  of both parties <eos> in sometimes amusing more often <unk> even vicious <unk> mr. <unk> describes how sony <unk> even\n",
            "Sample 1 .  has been falling <eos> last year the average broker earned $ N N N lower than in N <eos> corporate\n",
            "Sample 2 .  plant and equipment and backlogs of orders for durable goods <eos> meanwhile the national association of manufacturers said yesterday a\n",
            "\n",
            "\n",
            "targets[[18 318 7 0 1 37 1 611 17 6 1 1 2 57 4 49 1 43 173 265][280 56 182 73 3 149 7 0 833 3 516 87 32 113 479 114 0 101 38 194][3 36 466 2 0 4554 523 6 2528 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 18 318 7 0 1 37 1 611 17...]...][[0 0 0 1 1 0 0 1 0 0...]...][[8 10000 10000 10000 0 1 10000 10000 611 10000...]...][[18 318 7 0 1 37 1 611 17 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 18 318 7 0 1 37 1 611 17...]...][[0 0 0 1 1 0 0 1 0 0...]...][[8 10000 10000 10000 0 1 10000 10000 611 10000...]...][[18 318 7 0 1 37 1 611 17 6...]...]\n",
            "I0212 04:16:12.985327 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[0 90 9748 3345 4 27 431 134 2136 18 0 1 284 251 456 26 4425 4144 918 8][491 1 10 51 37 9 60 30 58 9513 91 6 1 2190 4 6 974 2986 2 216][181 2818 4 3 491 16 27 146 551 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[113 0 90 9748 3345 4 27 431 134 2136...]...][[1 0 1 0 0 0 0 1 1 0...]...][[113 0 10000 9748 10000 10000 10000 10000 134 2136...]...][[0 90 9748 3345 4 27 431 134 2136 18...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[113 0 90 9748 3345 4 27 431 134 2136...]...][[1 0 1 0 0 0 0 1 1 0...]...][[113 0 10000 9748 10000 10000 10000 10000 134 2136...]...][[0 90 9748 3345 4 27 431 134 2136 18...]...]\n",
            "targets[[0 90 9748 3345 4 27 431 134 2136 18 0 1 284 251 456 26 4425 4144 918 8][491 1 10 51 37 9 60 30 58 9513 91 6 1 2190 4 6 974 2986 2 216][181 2818 4 3 491 16 27 146 551 10...]...]\n",
            "targets[[0 90 9748 3345 4 27 431 134 2136 18 0 1 284 251 456 26 4425 4144 918 8][491 1 10 51 37 9 60 30 58 9513 91 6 1 2190 4 6 974 2986 2 216][181 2818 4 3 491 16 27 146 551 10...]...]\n",
            "targets[[0 90 9748 3345 4 27 431 134 2136 18 0 1 284 251 456 26 4425 4144 918 8][491 1 10 51 37 9 60 30 58 9513 91 6 1 2190 4 6 974 2986 2 216][181 2818 4 3 491 16 27 146 551 10...]...]\n",
            "global_step: 1518\n",
            " perplexity: 676.083\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[0 90 9748 3345 4 27 431 134 2136 18 0 1 284 251 456 26 4425 4144 918 8][491 1 10 51 37 9 60 30 58 9513 91 6 1 2190 4 6 974 2986 2 216][181 2818 4 3 491 16 27 146 551 10...]...]\n",
            " percent of 3-grams captured: 0.150.\n",
            "targets[[0 90 9748 3345 4 27 431 134 2136 18 0 1 284 251 456 26 4425 4144 918 8][491 1 10 51 37 9 60 30 58 9513 91 6 1 2190 4 6 974 2986 2 216][181 2818 4 3 491 16 27 146 551 10...]...]\n",
            " percent of 2-grams captured: 0.476.\n",
            "targets[[0 90 9748 3345 4 27 431 134 2136 18 0 1 284 251 456 26 4425 4144 918 8][491 1 10 51 37 9 60 30 58 9513 91 6 1 2190 4 6 974 2986 2 216][181 2818 4 3 491 16 27 146 551 10...]...]\n",
            " percent of 4-grams captured: 0.044.\n",
            " geometric_avg: 0.147.\n",
            " arithmetic_avg: 0.223.\n",
            "global_step: 1518\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69635\n",
            " G train loss: 3.29136\n",
            "targets[[0 90 9748 3345 4 27 431 134 2136 18 0 1 284 251 456 26 4425 4144 918 8][491 1 10 51 37 9 60 30 58 9513 91 6 1 2190 4 6 974 2986 2 216][181 2818 4 3 491 16 27 146 551 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[113 0 90 9748 3345 4 27 431 134 2136...]...][[1 0 1 0 0 0 0 1 1 0...]...][[113 0 10000 9748 10000 10000 10000 10000 134 2136...]...][[0 90 9748 3345 4 27 431 134 2136 18...]...]\n",
            " Sample 0.\n",
            "   [1]  the                   0.569   0.000  \n",
            "   [0]  most                  0.565   7.095  \n",
            "   [1]  mundane               0.565   0.000  \n",
            "   [0]  aspects               0.565   10.420 \n",
            "   [0]  of                    0.564   2.502  \n",
            "   [0]  its                   0.561   4.268  \n",
            "   [0]  workers               0.558   7.872  \n",
            "   [1]  '                     0.554   0.000  \n",
            "   [1]  lives                 0.551   0.000  \n",
            "   [0]  at                    0.550   4.335  \n",
            "   [1]  the                   0.549   0.000  \n",
            "   [0]  <unk>                 0.550   3.983  \n",
            "   [1]  office                0.552   0.000  \n",
            "   [1]  where                 0.552   0.000  \n",
            "   [0]  employees             0.554   7.579  \n",
            "   [0]  are                   0.553   5.260  \n",
            "   [0]  assigned              0.552   10.183 \n",
            "   [0]  lunch                 0.553   13.026 \n",
            "   [1]  partners              0.553   0.000  \n",
            "   [1]  and                   0.556   0.000  \n",
            " Sample 1.\n",
            "   [0]  executives            0.555   7.950  \n",
            "   [1]  <unk>                 0.558   0.000  \n",
            "   [0]  that                  0.561   3.720  \n",
            "   [1]  their                 0.559   0.000  \n",
            "   [0]  company               0.557   5.660  \n",
            "   [0]  's                    0.556   2.969  \n",
            "   [1]  stock                 0.555   0.000  \n",
            "   [0]  has                   0.555   5.306  \n",
            "   [0]  been                  0.555   5.451  \n",
            "   [0]  transformed           0.556   10.716 \n",
            "   [1]  into                  0.557   0.000  \n",
            "   [0]  a                     0.559   2.886  \n",
            "   [1]  <unk>                 0.561   0.000  \n",
            "   [1]  piece                 0.561   0.000  \n",
            "   [0]  of                    0.560   2.495  \n",
            "   [1]  a                     0.561   0.000  \n",
            "   [0]  stock-index           0.560   9.014  \n",
            "   [1]  basket                0.560   0.000  \n",
            "   [1]  <eos>                 0.560   0.000  \n",
            "   [1]  index                 0.560   0.000  \n",
            " Sample 2.\n",
            "   [1]  recent                0.448   0.000  \n",
            "   [0]  poll                  0.443   11.682 \n",
            "   [1]  of                    0.440   0.000  \n",
            "   [1]  N                     0.437   0.000  \n",
            "   [1]  executives            0.435   0.000  \n",
            "   [0]  on                    0.432   4.616  \n",
            "   [1]  its                   0.432   0.000  \n",
            "   [0]  board                 0.435   5.583  \n",
            "   [1]  found                 0.437   0.000  \n",
            "   [0]  that                  0.439   3.913  \n",
            "   [0]  N                     0.437   5.193  \n",
            "   [1]  N                     0.434   0.000  \n",
            "   [0]  do                    0.433   10.163 \n",
            "   [1]  n't                   0.432   0.000  \n",
            "   [1]  expect                0.430   0.000  \n",
            "   [1]  a                     0.433   0.000  \n",
            "   [1]  recession             0.433   0.000  \n",
            "   [0]  to                    0.436   3.246  \n",
            "   [1]  occur                 0.438   0.000  \n",
            "   [0]  until                 0.440   7.251  \n",
            "Samples\n",
            "Sample 0 .  the most mundane aspects of its workers ' lives at the <unk> office where employees are assigned lunch partners and\n",
            "Sample 1 .  executives <unk> that their company 's stock has been transformed into a <unk> piece of a stock-index basket <eos> index\n",
            "Sample 2 .  recent poll of N executives on its board found that N N do n't expect a recession to occur until\n",
            "\n",
            "\n",
            "targets[[2074 26 16 0 1059 2 14 9 684 1565 10 97 1 335 7 1 5484 152 11 0][1064 87 5341 5899 1 141 4 4438 197 584 2 87 110 138 1 36 1 2 7041 110][786 11 230 131 172 3 3 4 0 491...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[265 2074 26 16 0 1059 2 14 9 684...]...][[0 0 1 1 1 0 1 0 1 1...]...][[265 10000 10000 16 0 1059 10000 14 10000 684...]...][[2074 26 16 0 1059 2 14 9 684 1565...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[265 2074 26 16 0 1059 2 14 9 684...]...][[0 0 1 1 1 0 1 0 1 1...]...][[265 10000 10000 16 0 1059 10000 14 10000 684...]...][[2074 26 16 0 1059 2 14 9 684 1565...]...]\n",
            "I0212 04:16:16.337843 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[18 318 7 0 1 37 1 611 17 6 1 1 2 57 4 49 1 43 173 265][280 56 182 73 3 149 7 0 833 3 516 87 32 113 479 114 0 101 38 194][3 36 466 2 0 4554 523 6 2528 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 18 318 7 0 1 37 1 611 17...]...][[0 0 1 0 1 1 1 1 0 1...]...][[8 10000 10000 7 10000 1 37 1 611 10000...]...][[18 318 7 0 1 37 1 611 17 6...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 18 318 7 0 1 37 1 611 17...]...][[0 0 1 0 1 1 1 1 0 1...]...][[8 10000 10000 7 10000 1 37 1 611 10000...]...][[18 318 7 0 1 37 1 611 17 6...]...]\n",
            "targets[[18 318 7 0 1 37 1 611 17 6 1 1 2 57 4 49 1 43 173 265][280 56 182 73 3 149 7 0 833 3 516 87 32 113 479 114 0 101 38 194][3 36 466 2 0 4554 523 6 2528 5...]...]\n",
            "targets[[18 318 7 0 1 37 1 611 17 6 1 1 2 57 4 49 1 43 173 265][280 56 182 73 3 149 7 0 833 3 516 87 32 113 479 114 0 101 38 194][3 36 466 2 0 4554 523 6 2528 5...]...]\n",
            "targets[[18 318 7 0 1 37 1 611 17 6 1 1 2 57 4 49 1 43 173 265][280 56 182 73 3 149 7 0 833 3 516 87 32 113 479 114 0 101 38 194][3 36 466 2 0 4554 523 6 2528 5...]...]\n",
            "global_step: 1521\n",
            " perplexity: 675.610\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[18 318 7 0 1 37 1 611 17 6 1 1 2 57 4 49 1 43 173 265][280 56 182 73 3 149 7 0 833 3 516 87 32 113 479 114 0 101 38 194][3 36 466 2 0 4554 523 6 2528 5...]...]\n",
            " percent of 3-grams captured: 0.153.\n",
            "targets[[18 318 7 0 1 37 1 611 17 6 1 1 2 57 4 49 1 43 173 265][280 56 182 73 3 149 7 0 833 3 516 87 32 113 479 114 0 101 38 194][3 36 466 2 0 4554 523 6 2528 5...]...]\n",
            " percent of 2-grams captured: 0.497.\n",
            "targets[[18 318 7 0 1 37 1 611 17 6 1 1 2 57 4 49 1 43 173 265][280 56 182 73 3 149 7 0 833 3 516 87 32 113 479 114 0 101 38 194][3 36 466 2 0 4554 523 6 2528 5...]...]\n",
            " percent of 4-grams captured: 0.038.\n",
            " geometric_avg: 0.143.\n",
            " arithmetic_avg: 0.229.\n",
            "global_step: 1521\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69639\n",
            " G train loss: 3.29101\n",
            "targets[[18 318 7 0 1 37 1 611 17 6 1 1 2 57 4 49 1 43 173 265][280 56 182 73 3 149 7 0 833 3 516 87 32 113 479 114 0 101 38 194][3 36 466 2 0 4554 523 6 2528 5...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 18 318 7 0 1 37 1 611 17...]...][[0 0 1 0 1 1 1 1 0 1...]...][[8 10000 10000 7 10000 1 37 1 611 10000...]...][[18 318 7 0 1 37 1 611 17 6...]...]\n",
            " Sample 0.\n",
            "   [0]  at                    0.525   5.706  \n",
            "   [0]  home                  0.521   7.884  \n",
            "   [1]  in                    0.512   0.000  \n",
            "   [0]  the                   0.504   2.068  \n",
            "   [1]  <unk>                 0.499   0.000  \n",
            "   [1]  company               0.496   0.000  \n",
            "   [1]  <unk>                 0.493   0.000  \n",
            "   [1]  run                   0.494   0.000  \n",
            "   [0]  by                    0.496   4.829  \n",
            "   [1]  a                     0.501   0.000  \n",
            "   [0]  <unk>                 0.503   3.687  \n",
            "   [0]  <unk>                 0.503   4.262  \n",
            "   [1]  <eos>                 0.503   0.000  \n",
            "   [0]  some                  0.504   5.358  \n",
            "   [0]  of                    0.502   6.752  \n",
            "   [1]  his                   0.500   0.000  \n",
            "   [0]  <unk>                 0.500   3.353  \n",
            "   [0]  about                 0.502   6.778  \n",
            "   [1]  japanese              0.505   0.000  \n",
            "   [0]  management            0.507   7.875  \n",
            " Sample 1.\n",
            "   [1]  traders               0.486   0.000  \n",
            "   [1]  who                   0.474   0.000  \n",
            "   [0]  buy                   0.466   8.027  \n",
            "   [1]  all                   0.462   0.000  \n",
            "   [1]  N                     0.459   0.000  \n",
            "   [0]  stocks                0.458   8.600  \n",
            "   [1]  in                    0.458   0.000  \n",
            "   [0]  the                   0.457   0.942  \n",
            "   [0]  s&p                   0.460   9.595  \n",
            "   [1]  N                     0.461   0.000  \n",
            "   [0]  often                 0.462   9.177  \n",
            "   [0]  do                    0.464   6.987  \n",
            "   [1]  n't                   0.466   0.000  \n",
            "   [0]  even                  0.465   6.667  \n",
            "   [1]  know                  0.465   0.000  \n",
            "   [1]  what                  0.463   0.000  \n",
            "   [1]  the                   0.461   0.000  \n",
            "   [0]  companies             0.462   6.764  \n",
            "   [0]  they                  0.465   6.876  \n",
            "   [0]  own                   0.465   6.860  \n",
            " Sample 2.\n",
            "   [0]  N                     0.557   4.534  \n",
            "   [1]  or                    0.559   0.000  \n",
            "   [0]  later                 0.563   8.050  \n",
            "   [0]  <eos>                 0.561   3.822  \n",
            "   [1]  the                   0.556   0.000  \n",
            "   [1]  remainder             0.549   0.000  \n",
            "   [0]  expect                0.543   8.237  \n",
            "   [0]  a                     0.543   3.768  \n",
            "   [1]  downturn              0.541   0.000  \n",
            "   [0]  to                    0.539   4.633  \n",
            "   [1]  begin                 0.539   0.000  \n",
            "   [0]  sometime              0.540   10.239 \n",
            "   [1]  in                    0.541   0.000  \n",
            "   [1]  <eos>                 0.541   0.000  \n",
            "   [0]  although              0.544   6.545  \n",
            "   [0]  manufacturers         0.547   8.313  \n",
            "   [0]  often                 0.550   8.017  \n",
            "   [0]  are                   0.550   5.460  \n",
            "   [0]  quick                 0.552   8.683  \n",
            "   [0]  to                    0.551   3.563  \n",
            "Samples\n",
            "Sample 0 .  at home in the <unk> company <unk> run by a <unk> <unk> <eos> some of his <unk> about japanese management\n",
            "Sample 1 .  traders who buy all N stocks in the s&p N often do n't even know what the companies they own\n",
            "Sample 2 .  N or later <eos> the remainder expect a downturn to begin sometime in <eos> although manufacturers often are quick to\n",
            "\n",
            "\n",
            "targets[[1 4 4189 10 0 274 13 105 1 10 86 0 1634 609 89 1200 5 0 609 8][275 7 0 398 85 13 54 2102 23 1 44 28 9 3382 20 49 129 1255 590 2][27 524 2561 113 66 10 863 206 172 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 4189 10 0 274 13 105 1...]...][[1 1 1 0 0 1 0 0 1 0...]...][[0 1 4 4189 10000 10000 274 10000 10000 1...]...][[1 4 4189 10 0 274 13 105 1 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 4189 10 0 274 13 105 1...]...][[1 1 1 0 0 1 0 0 1 0...]...][[0 1 4 4189 10000 10000 274 10000 10000 1...]...][[1 4 4189 10 0 274 13 105 1 10...]...]\n",
            "I0212 04:16:19.635503 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[2074 26 16 0 1059 2 14 9 684 1565 10 97 1 335 7 1 5484 152 11 0][1064 87 5341 5899 1 141 4 4438 197 584 2 87 110 138 1 36 1 2 7041 110][786 11 230 131 172 3 3 4 0 491...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[265 2074 26 16 0 1059 2 14 9 684...]...][[1 0 1 1 1 0 0 1 1 1...]...][[265 2074 10000 16 0 1059 10000 10000 9 684...]...][[2074 26 16 0 1059 2 14 9 684 1565...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[265 2074 26 16 0 1059 2 14 9 684...]...][[1 0 1 1 1 0 0 1 1 1...]...][[265 2074 10000 16 0 1059 10000 10000 9 684...]...][[2074 26 16 0 1059 2 14 9 684 1565...]...]\n",
            "targets[[2074 26 16 0 1059 2 14 9 684 1565 10 97 1 335 7 1 5484 152 11 0][1064 87 5341 5899 1 141 4 4438 197 584 2 87 110 138 1 36 1 2 7041 110][786 11 230 131 172 3 3 4 0 491...]...]\n",
            "targets[[2074 26 16 0 1059 2 14 9 684 1565 10 97 1 335 7 1 5484 152 11 0][1064 87 5341 5899 1 141 4 4438 197 584 2 87 110 138 1 36 1 2 7041 110][786 11 230 131 172 3 3 4 0 491...]...]\n",
            "targets[[2074 26 16 0 1059 2 14 9 684 1565 10 97 1 335 7 1 5484 152 11 0][1064 87 5341 5899 1 141 4 4438 197 584 2 87 110 138 1 36 1 2 7041 110][786 11 230 131 172 3 3 4 0 491...]...]\n",
            "global_step: 1524\n",
            " perplexity: 675.004\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[2074 26 16 0 1059 2 14 9 684 1565 10 97 1 335 7 1 5484 152 11 0][1064 87 5341 5899 1 141 4 4438 197 584 2 87 110 138 1 36 1 2 7041 110][786 11 230 131 172 3 3 4 0 491...]...]\n",
            " percent of 3-grams captured: 0.136.\n",
            "targets[[2074 26 16 0 1059 2 14 9 684 1565 10 97 1 335 7 1 5484 152 11 0][1064 87 5341 5899 1 141 4 4438 197 584 2 87 110 138 1 36 1 2 7041 110][786 11 230 131 172 3 3 4 0 491...]...]\n",
            " percent of 2-grams captured: 0.413.\n",
            "targets[[2074 26 16 0 1059 2 14 9 684 1565 10 97 1 335 7 1 5484 152 11 0][1064 87 5341 5899 1 141 4 4438 197 584 2 87 110 138 1 36 1 2 7041 110][786 11 230 131 172 3 3 4 0 491...]...]\n",
            " percent of 4-grams captured: 0.053.\n",
            " geometric_avg: 0.144.\n",
            " arithmetic_avg: 0.201.\n",
            "global_step: 1524\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69640\n",
            " G train loss: 3.29039\n",
            "targets[[2074 26 16 0 1059 2 14 9 684 1565 10 97 1 335 7 1 5484 152 11 0][1064 87 5341 5899 1 141 4 4438 197 584 2 87 110 138 1 36 1 2 7041 110][786 11 230 131 172 3 3 4 0 491...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[265 2074 26 16 0 1059 2 14 9 684...]...][[1 0 1 1 1 0 0 1 1 1...]...][[265 2074 10000 16 0 1059 10000 10000 9 684...]...][[2074 26 16 0 1059 2 14 9 684 1565...]...]\n",
            " Sample 0.\n",
            "   [1]  style                 0.512   0.000  \n",
            "   [0]  are                   0.517   4.764  \n",
            "   [1]  on                    0.520   0.000  \n",
            "   [1]  the                   0.521   0.000  \n",
            "   [1]  mark                  0.522   0.000  \n",
            "   [0]  <eos>                 0.521   2.617  \n",
            "   [0]  it                    0.520   3.806  \n",
            "   [1]  's                    0.517   0.000  \n",
            "   [1]  probably              0.512   0.000  \n",
            "   [1]  true                  0.510   0.000  \n",
            "   [0]  that                  0.508   3.752  \n",
            "   [0]  many                  0.507   6.375  \n",
            "   [1]  <unk>                 0.507   0.000  \n",
            "   [1]  put                   0.509   0.000  \n",
            "   [1]  in                    0.510   0.000  \n",
            "   [0]  <unk>                 0.510   3.413  \n",
            "   [1]  overtime              0.512   0.000  \n",
            "   [1]  just                  0.515   0.000  \n",
            "   [0]  for                   0.515   4.359  \n",
            "   [1]  the                   0.514   0.000  \n",
            " Sample 1.\n",
            "   [1]  actually              0.468   0.000  \n",
            "   [0]  do                    0.471   6.631  \n",
            "   [1]  complains             0.474   0.000  \n",
            "   [1]  andrew                0.478   0.000  \n",
            "   [0]  <unk>                 0.483   2.974  \n",
            "   [0]  chairman              0.486   7.329  \n",
            "   [0]  of                    0.486   2.553  \n",
            "   [1]  champion              0.487   0.000  \n",
            "   [1]  international         0.486   0.000  \n",
            "   [1]  corp                  0.482   0.000  \n",
            "   [1]  <eos>                 0.478   0.000  \n",
            "   [1]  do                    0.474   0.000  \n",
            "   [0]  you                   0.473   6.178  \n",
            "   [1]  make                  0.471   0.000  \n",
            "   [0]  <unk>                 0.474   2.751  \n",
            "   [1]  or                    0.475   0.000  \n",
            "   [0]  <unk>                 0.477   3.586  \n",
            "   [1]  <eos>                 0.477   0.000  \n",
            "   [1]  oh                    0.477   0.000  \n",
            "   [1]  you                   0.476   0.000  \n",
            " Sample 2.\n",
            "   [1]  call                  0.508   0.000  \n",
            "   [1]  for                   0.495   0.000  \n",
            "   [1]  lower                 0.487   0.000  \n",
            "   [1]  interest              0.487   0.000  \n",
            "   [1]  rates                 0.485   0.000  \n",
            "   [1]  N                     0.480   0.000  \n",
            "   [0]  N                     0.476   2.067  \n",
            "   [0]  of                    0.478   2.577  \n",
            "   [1]  the                   0.482   0.000  \n",
            "   [1]  executives            0.484   0.000  \n",
            "   [0]  said                  0.484   4.070  \n",
            "   [1]  they                  0.488   0.000  \n",
            "   [0]  would                 0.489   5.852  \n",
            "   [0]  prefer                0.490   10.698 \n",
            "   [1]  that                  0.489   0.000  \n",
            "   [0]  the                   0.489   2.411  \n",
            "   [1]  fed                   0.490   0.000  \n",
            "   [1]  keep                  0.496   0.000  \n",
            "   [1]  <unk>                 0.497   0.000  \n",
            "   [0]  as                    0.497   4.899  \n",
            "Samples\n",
            "Sample 0 .  style are on the mark <eos> it 's probably true that many <unk> put in <unk> overtime just for the\n",
            "Sample 1 .  actually do complains andrew <unk> chairman of champion international corp <eos> do you make <unk> or <unk> <eos> oh you\n",
            "Sample 2 .  call for lower interest rates N N of the executives said they would prefer that the fed keep <unk> as\n",
            "\n",
            "\n",
            "targets[[0 609 5 0 187 609 8 10 1152 24 1 4 4506 6 848 966 140 1608 209 16][17 39 311 156 280 46 839 6 439 3 3 4 0 129 146 9 196 858 77 395][245 43 618 8 1141 131 172 118 2 73...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 0 609 5 0 187 609 8 10 1152...]...][[1 1 1 0 0 0 1 1 1 1...]...][[8 0 609 5 10000 10000 10000 8 10 1152...]...][[0 609 5 0 187 609 8 10 1152 24...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 0 609 5 0 187 609 8 10 1152...]...][[1 1 1 0 0 0 1 1 1 1...]...][[8 0 609 5 10000 10000 10000 8 10 1152...]...][[0 609 5 0 187 609 8 10 1152 24...]...]\n",
            "I0212 04:16:22.941508 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1 4 4189 10 0 274 13 105 1 10 86 0 1634 609 89 1200 5 0 609 8][275 7 0 398 85 13 54 2102 23 1 44 28 9 3382 20 49 129 1255 590 2][27 524 2561 113 66 10 863 206 172 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 4189 10 0 274 13 105 1...]...][[1 1 0 1 1 1 0 0 0 0...]...][[0 1 4 10000 10 0 274 10000 10000 10000...]...][[1 4 4189 10 0 274 13 105 1 10...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 4189 10 0 274 13 105 1...]...][[1 1 0 1 1 1 0 0 0 0...]...][[0 1 4 10000 10 0 274 10000 10000 10000...]...][[1 4 4189 10 0 274 13 105 1 10...]...]\n",
            "targets[[1 4 4189 10 0 274 13 105 1 10 86 0 1634 609 89 1200 5 0 609 8][275 7 0 398 85 13 54 2102 23 1 44 28 9 3382 20 49 129 1255 590 2][27 524 2561 113 66 10 863 206 172 2...]...]\n",
            "targets[[1 4 4189 10 0 274 13 105 1 10 86 0 1634 609 89 1200 5 0 609 8][275 7 0 398 85 13 54 2102 23 1 44 28 9 3382 20 49 129 1255 590 2][27 524 2561 113 66 10 863 206 172 2...]...]\n",
            "targets[[1 4 4189 10 0 274 13 105 1 10 86 0 1634 609 89 1200 5 0 609 8][275 7 0 398 85 13 54 2102 23 1 44 28 9 3382 20 49 129 1255 590 2][27 524 2561 113 66 10 863 206 172 2...]...]\n",
            "global_step: 1527\n",
            " perplexity: 674.792\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1 4 4189 10 0 274 13 105 1 10 86 0 1634 609 89 1200 5 0 609 8][275 7 0 398 85 13 54 2102 23 1 44 28 9 3382 20 49 129 1255 590 2][27 524 2561 113 66 10 863 206 172 2...]...]\n",
            " percent of 3-grams captured: 0.125.\n",
            "targets[[1 4 4189 10 0 274 13 105 1 10 86 0 1634 609 89 1200 5 0 609 8][275 7 0 398 85 13 54 2102 23 1 44 28 9 3382 20 49 129 1255 590 2][27 524 2561 113 66 10 863 206 172 2...]...]\n",
            " percent of 2-grams captured: 0.447.\n",
            "targets[[1 4 4189 10 0 274 13 105 1 10 86 0 1634 609 89 1200 5 0 609 8][275 7 0 398 85 13 54 2102 23 1 44 28 9 3382 20 49 129 1255 590 2][27 524 2561 113 66 10 863 206 172 2...]...]\n",
            " percent of 4-grams captured: 0.032.\n",
            " geometric_avg: 0.122.\n",
            " arithmetic_avg: 0.202.\n",
            "global_step: 1527\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69638\n",
            " G train loss: 3.29052\n",
            "targets[[1 4 4189 10 0 274 13 105 1 10 86 0 1634 609 89 1200 5 0 609 8][275 7 0 398 85 13 54 2102 23 1 44 28 9 3382 20 49 129 1255 590 2][27 524 2561 113 66 10 863 206 172 2...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1 4 4189 10 0 274 13 105 1...]...][[1 1 0 1 1 1 0 0 0 0...]...][[0 1 4 10000 10 0 274 10000 10000 10000...]...][[1 4 4189 10 0 274 13 105 1 10...]...]\n",
            " Sample 0.\n",
            "   [1]  <unk>                 0.480   0.000  \n",
            "   [1]  of                    0.480   0.000  \n",
            "   [0]  solidarity            0.481   7.796  \n",
            "   [1]  that                  0.478   0.000  \n",
            "   [1]  the                   0.475   0.000  \n",
            "   [1]  system                0.472   0.000  \n",
            "   [0]  is                    0.469   3.874  \n",
            "   [0]  so                    0.465   5.780  \n",
            "   [0]  <unk>                 0.465   3.383  \n",
            "   [0]  that                  0.467   4.040  \n",
            "   [1]  only                  0.470   0.000  \n",
            "   [1]  the                   0.473   0.000  \n",
            "   [1]  assistant             0.474   0.000  \n",
            "   [1]  manager               0.475   0.000  \n",
            "   [1]  can                   0.473   0.000  \n",
            "   [1]  talk                  0.468   0.000  \n",
            "   [1]  to                    0.464   0.000  \n",
            "   [1]  the                   0.464   0.000  \n",
            "   [1]  manager               0.466   0.000  \n",
            "   [1]  and                   0.467   0.000  \n",
            " Sample 1.\n",
            "   [0]  're                   0.417   7.648  \n",
            "   [0]  in                    0.417   5.588  \n",
            "   [0]  the                   0.418   1.504  \n",
            "   [0]  paper                 0.420   6.614  \n",
            "   [0]  business              0.423   6.032  \n",
            "   [0]  is                    0.423   3.521  \n",
            "   [0]  one                   0.423   5.666  \n",
            "   [1]  reaction              0.423   0.000  \n",
            "   [0]  mr.                   0.421   5.777  \n",
            "   [0]  <unk>                 0.419   2.589  \n",
            "   [1]  says                  0.418   0.000  \n",
            "   [1]  he                    0.416   0.000  \n",
            "   [0]  's                    0.415   4.080  \n",
            "   [0]  gotten                0.415   10.908 \n",
            "   [0]  from                  0.410   6.438  \n",
            "   [1]  his                   0.404   0.000  \n",
            "   [0]  big                   0.404   5.954  \n",
            "   [1]  institutional         0.402   0.000  \n",
            "   [0]  shareholders          0.402   8.658  \n",
            "   [1]  <eos>                 0.400   0.000  \n",
            " Sample 2.\n",
            "   [1]  its                   0.466   0.000  \n",
            "   [0]  top                   0.455   7.228  \n",
            "   [1]  priority              0.446   0.000  \n",
            "   [0]  even                  0.439   7.253  \n",
            "   [1]  if                    0.434   0.000  \n",
            "   [0]  that                  0.432   4.208  \n",
            "   [1]  means                 0.435   0.000  \n",
            "   [1]  higher                0.437   0.000  \n",
            "   [1]  rates                 0.438   0.000  \n",
            "   [1]  <eos>                 0.436   0.000  \n",
            "   [0]  the                   0.436   1.889  \n",
            "   [0]  other                 0.439   7.060  \n",
            "   [0]  N                     0.439   7.455  \n",
            "   [1]  N                     0.439   0.000  \n",
            "   [0]  said                  0.439   5.889  \n",
            "   [0]  the                   0.440   2.768  \n",
            "   [0]  fed                   0.443   6.715  \n",
            "   [1]  ought                 0.445   0.000  \n",
            "   [0]  to                    0.446   3.341  \n",
            "   [1]  worry                 0.448   0.000  \n",
            "Samples\n",
            "Sample 0 .  <unk> of solidarity that the system is so <unk> that only the assistant manager can talk to the manager and\n",
            "Sample 1 .  're in the paper business is one reaction mr. <unk> says he 's gotten from his big institutional shareholders <eos>\n",
            "Sample 2 .  its top priority even if that means higher rates <eos> the other N N said the fed ought to worry\n",
            "\n",
            "\n",
            "targets[[106 2069 2 73 4 39 403 34 58 1 9698 5 23 1 56 792 5 1152 22 6381][2 211 0 524 8321 46 330 321 2718 1 906 1348 82 95 1086 2452 812 831 1278 997][347 26 0 135 2433 4 0 3809 4 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 106 2069 2 73 4 39 403 34 58...]...][[1 1 0 0 1 0 1 1 0 0...]...][[16 106 2069 10000 10000 4 10000 403 34 10000...]...][[106 2069 2 73 4 39 403 34 58 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 106 2069 2 73 4 39 403 34 58...]...][[1 1 0 0 1 0 1 1 0 0...]...][[16 106 2069 10000 10000 4 10000 403 34 10000...]...][[106 2069 2 73 4 39 403 34 58 1...]...]\n",
            "I0212 04:16:26.401108 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[0 609 5 0 187 609 8 10 1152 24 1 4 4506 6 848 966 140 1608 209 16][17 39 311 156 280 46 839 6 439 3 3 4 0 129 146 9 196 858 77 395][245 43 618 8 1141 131 172 118 2 73...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 0 609 5 0 187 609 8 10 1152...]...][[0 0 1 0 1 0 1 1 1 0...]...][[8 10000 10000 5 10000 187 10000 8 10 1152...]...][[0 609 5 0 187 609 8 10 1152 24...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 0 609 5 0 187 609 8 10 1152...]...][[0 0 1 0 1 0 1 1 1 0...]...][[8 10000 10000 5 10000 187 10000 8 10 1152...]...][[0 609 5 0 187 609 8 10 1152 24...]...]\n",
            "targets[[0 609 5 0 187 609 8 10 1152 24 1 4 4506 6 848 966 140 1608 209 16][17 39 311 156 280 46 839 6 439 3 3 4 0 129 146 9 196 858 77 395][245 43 618 8 1141 131 172 118 2 73...]...]\n",
            "targets[[0 609 5 0 187 609 8 10 1152 24 1 4 4506 6 848 966 140 1608 209 16][17 39 311 156 280 46 839 6 439 3 3 4 0 129 146 9 196 858 77 395][245 43 618 8 1141 131 172 118 2 73...]...]\n",
            "targets[[0 609 5 0 187 609 8 10 1152 24 1 4 4506 6 848 966 140 1608 209 16][17 39 311 156 280 46 839 6 439 3 3 4 0 129 146 9 196 858 77 395][245 43 618 8 1141 131 172 118 2 73...]...]\n",
            "global_step: 1530\n",
            " perplexity: 673.915\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[0 609 5 0 187 609 8 10 1152 24 1 4 4506 6 848 966 140 1608 209 16][17 39 311 156 280 46 839 6 439 3 3 4 0 129 146 9 196 858 77 395][245 43 618 8 1141 131 172 118 2 73...]...]\n",
            " percent of 3-grams captured: 0.194.\n",
            "targets[[0 609 5 0 187 609 8 10 1152 24 1 4 4506 6 848 966 140 1608 209 16][17 39 311 156 280 46 839 6 439 3 3 4 0 129 146 9 196 858 77 395][245 43 618 8 1141 131 172 118 2 73...]...]\n",
            " percent of 2-grams captured: 0.492.\n",
            "targets[[0 609 5 0 187 609 8 10 1152 24 1 4 4506 6 848 966 140 1608 209 16][17 39 311 156 280 46 839 6 439 3 3 4 0 129 146 9 196 858 77 395][245 43 618 8 1141 131 172 118 2 73...]...]\n",
            " percent of 4-grams captured: 0.074.\n",
            " geometric_avg: 0.192.\n",
            " arithmetic_avg: 0.253.\n",
            "global_step: 1530\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69640\n",
            " G train loss: 3.29010\n",
            "targets[[0 609 5 0 187 609 8 10 1152 24 1 4 4506 6 848 966 140 1608 209 16][17 39 311 156 280 46 839 6 439 3 3 4 0 129 146 9 196 858 77 395][245 43 618 8 1141 131 172 118 2 73...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 0 609 5 0 187 609 8 10 1152...]...][[0 0 1 0 1 0 1 1 1 0...]...][[8 10000 10000 5 10000 187 10000 8 10 1152...]...][[0 609 5 0 187 609 8 10 1152 24...]...]\n",
            " Sample 0.\n",
            "   [0]  the                   0.505   2.613  \n",
            "   [0]  manager               0.509   9.368  \n",
            "   [1]  to                    0.513   0.000  \n",
            "   [0]  the                   0.514   2.890  \n",
            "   [1]  general               0.514   0.000  \n",
            "   [0]  manager               0.512   7.778  \n",
            "   [1]  and                   0.510   0.000  \n",
            "   [1]  that                  0.509   0.000  \n",
            "   [1]  sony                  0.506   0.000  \n",
            "   [0]  was                   0.504   4.958  \n",
            "   [0]  <unk>                 0.505   3.084  \n",
            "   [1]  of                    0.504   0.000  \n",
            "   [0]  letting               0.503   12.204 \n",
            "   [1]  a                     0.507   0.000  \n",
            "   [1]  young                 0.509   0.000  \n",
            "   [0]  short-term            0.510   9.167  \n",
            "   [0]  american              0.508   7.796  \n",
            "   [0]  employee              0.506   9.796  \n",
            "   [1]  take                  0.504   0.000  \n",
            "   [1]  on                    0.503   0.000  \n",
            " Sample 1.\n",
            "   [0]  by                    0.486   6.910  \n",
            "   [0]  this                  0.487   4.980  \n",
            "   [0]  september             0.486   6.003  \n",
            "   [0]  program               0.483   7.234  \n",
            "   [1]  traders               0.482   0.000  \n",
            "   [1]  were                  0.482   0.000  \n",
            "   [0]  doing                 0.480   9.035  \n",
            "   [1]  a                     0.481   0.000  \n",
            "   [0]  record                0.482   6.717  \n",
            "   [0]  N                     0.482   3.828  \n",
            "   [0]  N                     0.482   2.471  \n",
            "   [0]  of                    0.482   2.726  \n",
            "   [0]  the                   0.482   1.708  \n",
            "   [0]  big                   0.481   5.879  \n",
            "   [1]  board                 0.480   0.000  \n",
            "   [1]  's                    0.481   0.000  \n",
            "   [1]  average               0.481   0.000  \n",
            "   [0]  daily                 0.481   8.483  \n",
            "   [0]  trading               0.480   6.504  \n",
            "   [0]  volume                0.479   8.062  \n",
            " Sample 2.\n",
            "   [1]  less                  0.523   0.000  \n",
            "   [1]  about                 0.521   0.000  \n",
            "   [0]  inflation             0.521   9.355  \n",
            "   [1]  and                   0.521   0.000  \n",
            "   [1]  bring                 0.522   0.000  \n",
            "   [1]  interest              0.523   0.000  \n",
            "   [0]  rates                 0.523   6.867  \n",
            "   [0]  down                  0.520   7.467  \n",
            "   [1]  <eos>                 0.519   0.000  \n",
            "   [1]  all                   0.521   0.000  \n",
            "   [0]  the                   0.522   2.231  \n",
            "   [1]  figures               0.522   0.000  \n",
            "   [1]  are                   0.523   0.000  \n",
            "   [1]  adjusted              0.524   0.000  \n",
            "   [0]  to                    0.526   1.876  \n",
            "   [1]  remove                0.527   0.000  \n",
            "   [0]  usual                 0.527   12.066 \n",
            "   [1]  seasonal              0.526   0.000  \n",
            "   [0]  patterns              0.526   13.091 \n",
            "   [0]  <eos>                 0.525   1.878  \n",
            "Samples\n",
            "Sample 0 .  the manager to the general manager and that sony was <unk> of letting a young short-term american employee take on\n",
            "Sample 1 .  by this september program traders were doing a record N N of the big board 's average daily trading volume\n",
            "Sample 2 .  less about inflation and bring interest rates down <eos> all the figures are adjusted to remove usual seasonal patterns <eos>\n",
            "\n",
            "\n",
            "targets[[7 85 8 224 2953 8 24 1 5 1 204 1 2 29 1152 2156 453 6 5442 20][80 8 1620 96 443 2 29 223 544 313 3 8 0 1332 4542 1 17 0 393 2859][3442 38 1110 6 3 3 499 7 0 216...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6381 7 85 8 224 2953 8 24 1 5...]...][[0 1 1 0 0 0 0 0 1 0...]...][[6381 10000 85 8 10000 10000 10000 10000 10000 5...]...][[7 85 8 224 2953 8 24 1 5 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6381 7 85 8 224 2953 8 24 1 5...]...][[0 1 1 0 0 0 0 0 1 0...]...][[6381 10000 85 8 10000 10000 10000 10000 10000 5...]...][[7 85 8 224 2953 8 24 1 5 1...]...]\n",
            "I0212 04:16:29.685431 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[106 2069 2 73 4 39 403 34 58 1 9698 5 23 1 56 792 5 1152 22 6381][2 211 0 524 8321 46 330 321 2718 1 906 1348 82 95 1086 2452 812 831 1278 997][347 26 0 135 2433 4 0 3809 4 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 106 2069 2 73 4 39 403 34 58...]...][[1 1 0 1 0 0 0 0 0 0...]...][[16 106 2069 10000 73 10000 10000 10000 10000 10000...]...][[106 2069 2 73 4 39 403 34 58 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 106 2069 2 73 4 39 403 34 58...]...][[1 1 0 1 0 0 0 0 0 0...]...][[16 106 2069 10000 73 10000 10000 10000 10000 10000...]...][[106 2069 2 73 4 39 403 34 58 1...]...]\n",
            "targets[[106 2069 2 73 4 39 403 34 58 1 9698 5 23 1 56 792 5 1152 22 6381][2 211 0 524 8321 46 330 321 2718 1 906 1348 82 95 1086 2452 812 831 1278 997][347 26 0 135 2433 4 0 3809 4 0...]...]\n",
            "targets[[106 2069 2 73 4 39 403 34 58 1 9698 5 23 1 56 792 5 1152 22 6381][2 211 0 524 8321 46 330 321 2718 1 906 1348 82 95 1086 2452 812 831 1278 997][347 26 0 135 2433 4 0 3809 4 0...]...]\n",
            "targets[[106 2069 2 73 4 39 403 34 58 1 9698 5 23 1 56 792 5 1152 22 6381][2 211 0 524 8321 46 330 321 2718 1 906 1348 82 95 1086 2452 812 831 1278 997][347 26 0 135 2433 4 0 3809 4 0...]...]\n",
            "global_step: 1533\n",
            " perplexity: 672.976\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[106 2069 2 73 4 39 403 34 58 1 9698 5 23 1 56 792 5 1152 22 6381][2 211 0 524 8321 46 330 321 2718 1 906 1348 82 95 1086 2452 812 831 1278 997][347 26 0 135 2433 4 0 3809 4 0...]...]\n",
            " percent of 3-grams captured: 0.242.\n",
            "targets[[106 2069 2 73 4 39 403 34 58 1 9698 5 23 1 56 792 5 1152 22 6381][2 211 0 524 8321 46 330 321 2718 1 906 1348 82 95 1086 2452 812 831 1278 997][347 26 0 135 2433 4 0 3809 4 0...]...]\n",
            " percent of 2-grams captured: 0.513.\n",
            "targets[[106 2069 2 73 4 39 403 34 58 1 9698 5 23 1 56 792 5 1152 22 6381][2 211 0 524 8321 46 330 321 2718 1 906 1348 82 95 1086 2452 812 831 1278 997][347 26 0 135 2433 4 0 3809 4 0...]...]\n",
            " percent of 4-grams captured: 0.097.\n",
            " geometric_avg: 0.229.\n",
            " arithmetic_avg: 0.284.\n",
            "global_step: 1533\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69639\n",
            " G train loss: 3.28920\n",
            "targets[[106 2069 2 73 4 39 403 34 58 1 9698 5 23 1 56 792 5 1152 22 6381][2 211 0 524 8321 46 330 321 2718 1 906 1348 82 95 1086 2452 812 831 1278 997][347 26 0 135 2433 4 0 3809 4 0...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[16 106 2069 2 73 4 39 403 34 58...]...][[1 1 0 1 0 0 0 0 0 0...]...][[16 106 2069 10000 73 10000 10000 10000 10000 10000...]...][[106 2069 2 73 4 39 403 34 58 1...]...]\n",
            " Sample 0.\n",
            "   [1]  any                   0.494   0.000  \n",
            "   [1]  responsibility        0.481   0.000  \n",
            "   [0]  <eos>                 0.471   2.255  \n",
            "   [1]  all                   0.466   0.000  \n",
            "   [0]  of                    0.461   4.707  \n",
            "   [0]  this                  0.460   4.548  \n",
            "   [0]  must                  0.459   8.552  \n",
            "   [0]  have                  0.459   5.615  \n",
            "   [0]  been                  0.461   5.123  \n",
            "   [0]  <unk>                 0.462   2.457  \n",
            "   [1]  frustrating           0.461   0.000  \n",
            "   [0]  to                    0.458   2.467  \n",
            "   [1]  mr.                   0.455   0.000  \n",
            "   [0]  <unk>                 0.454   2.670  \n",
            "   [0]  who                   0.451   5.769  \n",
            "   [0]  went                  0.451   7.846  \n",
            "   [0]  to                    0.450   2.696  \n",
            "   [0]  sony                  0.451   9.943  \n",
            "   [0]  with                  0.450   4.986  \n",
            "   [1]  degrees               0.451   0.000  \n",
            " Sample 1.\n",
            "   [1]  <eos>                 0.534   0.000  \n",
            "   [1]  among                 0.541   0.000  \n",
            "   [0]  the                   0.545   2.436  \n",
            "   [1]  top                   0.548   0.000  \n",
            "   [0]  practitioners         0.548   11.141 \n",
            "   [1]  were                  0.547   0.000  \n",
            "   [1]  wall                  0.545   0.000  \n",
            "   [0]  street                0.545   6.650  \n",
            "   [0]  blue                  0.548   13.501 \n",
            "   [1]  <unk>                 0.551   0.000  \n",
            "   [1]  morgan                0.551   0.000  \n",
            "   [0]  stanley               0.551   9.640  \n",
            "   [0]  &                     0.548   6.990  \n",
            "   [0]  co.                   0.545   6.697  \n",
            "   [1]  kidder                0.542   0.000  \n",
            "   [1]  peabody               0.541   0.000  \n",
            "   [0]  merrill               0.541   11.412 \n",
            "   [1]  lynch                 0.541   0.000  \n",
            "   [1]  salomon               0.544   0.000  \n",
            "   [0]  brothers              0.544   10.619 \n",
            " Sample 2.\n",
            "   [1]  here                  0.515   0.000  \n",
            "   [0]  are                   0.515   4.771  \n",
            "   [0]  the                   0.514   3.855  \n",
            "   [0]  net                   0.516   7.929  \n",
            "   [1]  contributions         0.519   0.000  \n",
            "   [0]  of                    0.524   2.401  \n",
            "   [1]  the                   0.525   0.000  \n",
            "   [1]  components            0.528   0.000  \n",
            "   [1]  of                    0.529   0.000  \n",
            "   [1]  the                   0.528   0.000  \n",
            "   [1]  commerce              0.530   0.000  \n",
            "   [1]  department            0.531   0.000  \n",
            "   [1]  's                    0.534   0.000  \n",
            "   [0]  index                 0.534   7.390  \n",
            "   [1]  of                    0.532   0.000  \n",
            "   [1]  leading               0.531   0.000  \n",
            "   [1]  indicators            0.530   0.000  \n",
            "   [1]  <eos>                 0.529   0.000  \n",
            "   [0]  after                 0.530   6.446  \n",
            "   [0]  various               0.534   8.876  \n",
            "Samples\n",
            "Sample 0 .  any responsibility <eos> all of this must have been <unk> frustrating to mr. <unk> who went to sony with degrees\n",
            "Sample 1 .  <eos> among the top practitioners were wall street blue <unk> morgan stanley & co. kidder peabody merrill lynch salomon brothers\n",
            "Sample 2 .  here are the net contributions of the components of the commerce department 's index of leading indicators <eos> after various\n",
            "\n",
            "\n",
            "targets[[0 140 265 1779 8 3179 23 1 78 28 2252 0 1347 1545 4 423 31 3663 5 378][782 174 216 975 2 0 1 134 750 11 0 1859 13 5 1 118 8 1028 0 6047][311 8 0 428 20 397 26 20 3 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 140 265 1779 8 3179 23 1 78...]...][[0 0 0 0 1 1 0 0 1 1...]...][[20 10000 10000 10000 10000 8 3179 10000 10000 78...]...][[0 140 265 1779 8 3179 23 1 78 28...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 140 265 1779 8 3179 23 1 78...]...][[0 0 0 0 1 1 0 0 1 1...]...][[20 10000 10000 10000 10000 8 3179 10000 10000 78...]...][[0 140 265 1779 8 3179 23 1 78 28...]...]\n",
            "I0212 04:16:33.141738 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[7 85 8 224 2953 8 24 1 5 1 204 1 2 29 1152 2156 453 6 5442 20][80 8 1620 96 443 2 29 223 544 313 3 8 0 1332 4542 1 17 0 393 2859][3442 38 1110 6 3 3 499 7 0 216...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6381 7 85 8 224 2953 8 24 1 5...]...][[1 0 0 0 1 0 1 1 0 1...]...][[6381 7 10000 10000 10000 2953 10000 24 1 10000...]...][[7 85 8 224 2953 8 24 1 5 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6381 7 85 8 224 2953 8 24 1 5...]...][[1 0 0 0 1 0 1 1 0 1...]...][[6381 7 10000 10000 10000 2953 10000 24 1 10000...]...][[7 85 8 224 2953 8 24 1 5 1...]...]\n",
            "targets[[7 85 8 224 2953 8 24 1 5 1 204 1 2 29 1152 2156 453 6 5442 20][80 8 1620 96 443 2 29 223 544 313 3 8 0 1332 4542 1 17 0 393 2859][3442 38 1110 6 3 3 499 7 0 216...]...]\n",
            "targets[[7 85 8 224 2953 8 24 1 5 1 204 1 2 29 1152 2156 453 6 5442 20][80 8 1620 96 443 2 29 223 544 313 3 8 0 1332 4542 1 17 0 393 2859][3442 38 1110 6 3 3 499 7 0 216...]...]\n",
            "targets[[7 85 8 224 2953 8 24 1 5 1 204 1 2 29 1152 2156 453 6 5442 20][80 8 1620 96 443 2 29 223 544 313 3 8 0 1332 4542 1 17 0 393 2859][3442 38 1110 6 3 3 499 7 0 216...]...]\n",
            "global_step: 1536\n",
            " perplexity: 672.784\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[7 85 8 224 2953 8 24 1 5 1 204 1 2 29 1152 2156 453 6 5442 20][80 8 1620 96 443 2 29 223 544 313 3 8 0 1332 4542 1 17 0 393 2859][3442 38 1110 6 3 3 499 7 0 216...]...]\n",
            " percent of 3-grams captured: 0.178.\n",
            "targets[[7 85 8 224 2953 8 24 1 5 1 204 1 2 29 1152 2156 453 6 5442 20][80 8 1620 96 443 2 29 223 544 313 3 8 0 1332 4542 1 17 0 393 2859][3442 38 1110 6 3 3 499 7 0 216...]...]\n",
            " percent of 2-grams captured: 0.484.\n",
            "targets[[7 85 8 224 2953 8 24 1 5 1 204 1 2 29 1152 2156 453 6 5442 20][80 8 1620 96 443 2 29 223 544 313 3 8 0 1332 4542 1 17 0 393 2859][3442 38 1110 6 3 3 499 7 0 216...]...]\n",
            " percent of 4-grams captured: 0.094.\n",
            " geometric_avg: 0.201.\n",
            " arithmetic_avg: 0.252.\n",
            "global_step: 1536\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69642\n",
            " G train loss: 3.28913\n",
            "targets[[7 85 8 224 2953 8 24 1 5 1 204 1 2 29 1152 2156 453 6 5442 20][80 8 1620 96 443 2 29 223 544 313 3 8 0 1332 4542 1 17 0 393 2859][3442 38 1110 6 3 3 499 7 0 216...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6381 7 85 8 224 2953 8 24 1 5...]...][[1 0 0 0 1 0 1 1 0 1...]...][[6381 7 10000 10000 10000 2953 10000 24 1 10000...]...][[7 85 8 224 2953 8 24 1 5 1...]...]\n",
            " Sample 0.\n",
            "   [1]  in                    0.518   0.000  \n",
            "   [0]  business              0.519   8.346  \n",
            "   [0]  and                   0.518   3.438  \n",
            "   [0]  computer              0.516   7.482  \n",
            "   [1]  science               0.515   0.000  \n",
            "   [0]  and                   0.513   4.096  \n",
            "   [1]  was                   0.510   0.000  \n",
            "   [1]  <unk>                 0.510   0.000  \n",
            "   [0]  to                    0.510   2.504  \n",
            "   [1]  <unk>                 0.513   0.000  \n",
            "   [0]  another               0.513   7.366  \n",
            "   [1]  <unk>                 0.514   0.000  \n",
            "   [0]  <eos>                 0.514   2.559  \n",
            "   [1]  but                   0.515   0.000  \n",
            "   [0]  sony                  0.515   9.753  \n",
            "   [1]  ultimately            0.516   0.000  \n",
            "   [1]  took                  0.515   0.000  \n",
            "   [0]  a                     0.516   2.871  \n",
            "   [1]  lesson                0.514   0.000  \n",
            "   [1]  from                  0.513   0.000  \n",
            " Sample 1.\n",
            "   [1]  inc.                  0.486   0.000  \n",
            "   [1]  and                   0.481   0.000  \n",
            "   [0]  painewebber           0.479   7.617  \n",
            "   [0]  group                 0.477   7.671  \n",
            "   [0]  inc                   0.473   7.804  \n",
            "   [0]  <eos>                 0.469   2.162  \n",
            "   [1]  but                   0.467   0.000  \n",
            "   [1]  then                  0.467   0.000  \n",
            "   [0]  came                  0.464   8.185  \n",
            "   [0]  oct.                  0.465   7.618  \n",
            "   [0]  N                     0.465   0.336  \n",
            "   [0]  and                   0.469   4.323  \n",
            "   [1]  the                   0.472   0.000  \n",
            "   [0]  negative              0.477   9.114  \n",
            "   [1]  publicity             0.480   0.000  \n",
            "   [1]  <unk>                 0.479   0.000  \n",
            "   [0]  by                    0.477   4.487  \n",
            "   [1]  the                   0.475   0.000  \n",
            "   [1]  old                   0.475   0.000  \n",
            "   [1]  guard                 0.473   0.000  \n",
            " Sample 2.\n",
            "   [1]  adjustments           0.475   0.000  \n",
            "   [0]  they                  0.478   5.487  \n",
            "   [1]  produced              0.479   0.000  \n",
            "   [1]  a                     0.479   0.000  \n",
            "   [0]  N                     0.478   4.424  \n",
            "   [1]  N                     0.478   0.000  \n",
            "   [0]  rise                  0.481   8.792  \n",
            "   [0]  in                    0.482   2.461  \n",
            "   [1]  the                   0.481   0.000  \n",
            "   [0]  index                 0.481   6.631  \n",
            "   [1]  for                   0.480   0.000  \n",
            "   [1]  august                0.478   0.000  \n",
            "   [1]  and                   0.478   0.000  \n",
            "   [1]  a                     0.480   0.000  \n",
            "   [1]  N                     0.481   0.000  \n",
            "   [1]  N                     0.484   0.000  \n",
            "   [1]  rise                  0.488   0.000  \n",
            "   [1]  for                   0.492   0.000  \n",
            "   [1]  september             0.492   0.000  \n",
            "   [1]  <eos>                 0.492   0.000  \n",
            "Samples\n",
            "Sample 0 .  in business and computer science and was <unk> to <unk> another <unk> <eos> but sony ultimately took a lesson from\n",
            "Sample 1 .  inc. and painewebber group inc <eos> but then came oct. N and the negative publicity <unk> by the old guard\n",
            "Sample 2 .  adjustments they produced a N N rise in the index for august and a N N rise for september <eos>\n",
            "\n",
            "\n",
            "targets[[0 8516 1 1 2217 4 1152 2 14 9 6 8401 51 474 576 453 665 2 23 1][3951 2 83 9 6 1 4170 382 99 44 0 524 2875 454 18 6 330 321 191 2][209 6 719 220 12 3 21 525 174 136...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[378 0 8516 1 1 2217 4 1152 2 14...]...][[1 1 0 0 0 0 0 0 0 0...]...][[378 0 8516 10000 10000 10000 10000 10000 10000 10000...]...][[0 8516 1 1 2217 4 1152 2 14 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[378 0 8516 1 1 2217 4 1152 2 14...]...][[1 1 0 0 0 0 0 0 0 0...]...][[378 0 8516 10000 10000 10000 10000 10000 10000 10000...]...][[0 8516 1 1 2217 4 1152 2 14 9...]...]\n",
            "I0212 04:16:36.511142 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[0 140 265 1779 8 3179 23 1 78 28 2252 0 1347 1545 4 423 31 3663 5 378][782 174 216 975 2 0 1 134 750 11 0 1859 13 5 1 118 8 1028 0 6047][311 8 0 428 20 397 26 20 3 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 140 265 1779 8 3179 23 1 78...]...][[0 1 0 0 0 0 0 1 0 1...]...][[20 10000 140 10000 10000 10000 10000 10000 1 10000...]...][[0 140 265 1779 8 3179 23 1 78 28...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 140 265 1779 8 3179 23 1 78...]...][[0 1 0 0 0 0 0 1 0 1...]...][[20 10000 140 10000 10000 10000 10000 10000 1 10000...]...][[0 140 265 1779 8 3179 23 1 78 28...]...]\n",
            "targets[[0 140 265 1779 8 3179 23 1 78 28 2252 0 1347 1545 4 423 31 3663 5 378][782 174 216 975 2 0 1 134 750 11 0 1859 13 5 1 118 8 1028 0 6047][311 8 0 428 20 397 26 20 3 7...]...]\n",
            "targets[[0 140 265 1779 8 3179 23 1 78 28 2252 0 1347 1545 4 423 31 3663 5 378][782 174 216 975 2 0 1 134 750 11 0 1859 13 5 1 118 8 1028 0 6047][311 8 0 428 20 397 26 20 3 7...]...]\n",
            "targets[[0 140 265 1779 8 3179 23 1 78 28 2252 0 1347 1545 4 423 31 3663 5 378][782 174 216 975 2 0 1 134 750 11 0 1859 13 5 1 118 8 1028 0 6047][311 8 0 428 20 397 26 20 3 7...]...]\n",
            "global_step: 1539\n",
            " perplexity: 672.080\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[0 140 265 1779 8 3179 23 1 78 28 2252 0 1347 1545 4 423 31 3663 5 378][782 174 216 975 2 0 1 134 750 11 0 1859 13 5 1 118 8 1028 0 6047][311 8 0 428 20 397 26 20 3 7...]...]\n",
            " percent of 3-grams captured: 0.250.\n",
            "targets[[0 140 265 1779 8 3179 23 1 78 28 2252 0 1347 1545 4 423 31 3663 5 378][782 174 216 975 2 0 1 134 750 11 0 1859 13 5 1 118 8 1028 0 6047][311 8 0 428 20 397 26 20 3 7...]...]\n",
            " percent of 2-grams captured: 0.503.\n",
            "targets[[0 140 265 1779 8 3179 23 1 78 28 2252 0 1347 1545 4 423 31 3663 5 378][782 174 216 975 2 0 1 134 750 11 0 1859 13 5 1 118 8 1028 0 6047][311 8 0 428 20 397 26 20 3 7...]...]\n",
            " percent of 4-grams captured: 0.109.\n",
            " geometric_avg: 0.239.\n",
            " arithmetic_avg: 0.287.\n",
            "global_step: 1539\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69643\n",
            " G train loss: 3.28938\n",
            "targets[[0 140 265 1779 8 3179 23 1 78 28 2252 0 1347 1545 4 423 31 3663 5 378][782 174 216 975 2 0 1 134 750 11 0 1859 13 5 1 118 8 1028 0 6047][311 8 0 428 20 397 26 20 3 7...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 140 265 1779 8 3179 23 1 78...]...][[0 1 0 0 0 0 0 1 0 1...]...][[20 10000 140 10000 10000 10000 10000 10000 1 10000...]...][[0 140 265 1779 8 3179 23 1 78 28...]...]\n",
            " Sample 0.\n",
            "   [0]  the                   0.452   1.076  \n",
            "   [1]  american              0.446   0.000  \n",
            "   [0]  management            0.443   7.727  \n",
            "   [0]  books                 0.441   8.929  \n",
            "   [0]  and                   0.438   3.285  \n",
            "   [0]  fired                 0.435   9.992  \n",
            "   [0]  mr.                   0.434   5.465  \n",
            "   [1]  <unk>                 0.433   0.000  \n",
            "   [0]  after                 0.433   6.175  \n",
            "   [1]  he                    0.434   0.000  \n",
            "   [0]  committed             0.435   9.737  \n",
            "   [0]  the                   0.434   4.312  \n",
            "   [0]  social                0.436   9.964  \n",
            "   [0]  crime                 0.441   11.247 \n",
            "   [1]  of                    0.442   0.000  \n",
            "   [1]  making                0.442   0.000  \n",
            "   [1]  an                    0.440   0.000  \n",
            "   [0]  appointment           0.436   11.455 \n",
            "   [0]  to                    0.433   2.989  \n",
            "   [0]  see                   0.430   7.751  \n",
            " Sample 1.\n",
            "   [1]  particularly          0.578   0.000  \n",
            "   [1]  against               0.580   0.000  \n",
            "   [0]  index                 0.582   7.453  \n",
            "   [0]  arbitrage             0.582   8.801  \n",
            "   [1]  <eos>                 0.580   0.000  \n",
            "   [0]  the                   0.578   1.552  \n",
            "   [0]  <unk>                 0.578   2.181  \n",
            "   [1]  '                     0.578   0.000  \n",
            "   [1]  strategy              0.578   0.000  \n",
            "   [1]  for                   0.577   0.000  \n",
            "   [0]  the                   0.574   1.364  \n",
            "   [1]  moment                0.574   0.000  \n",
            "   [1]  is                    0.573   0.000  \n",
            "   [0]  to                    0.574   4.332  \n",
            "   [0]  <unk>                 0.577   3.335  \n",
            "   [0]  down                  0.579   6.647  \n",
            "   [1]  and                   0.581   0.000  \n",
            "   [0]  let                   0.585   8.113  \n",
            "   [0]  the                   0.586   3.505  \n",
            "   [0]  furor                 0.587   11.343 \n",
            " Sample 2.\n",
            "   [1]  september             0.445   0.000  \n",
            "   [0]  and                   0.442   4.011  \n",
            "   [0]  the                   0.442   2.664  \n",
            "   [1]  change                0.443   0.000  \n",
            "   [0]  from                  0.446   4.723  \n",
            "   [1]  august                0.449   0.000  \n",
            "   [0]  are                   0.451   5.772  \n",
            "   [1]  from                  0.455   0.000  \n",
            "   [1]  N                     0.457   0.000  \n",
            "   [1]  in                    0.460   0.000  \n",
            "   [0]  the                   0.464   1.354  \n",
            "   [0]  previous              0.468   7.274  \n",
            "   [1]  month                 0.470   0.000  \n",
            "   [0]  <eos>                 0.469   1.802  \n",
            "   [0]  boston                0.470   8.854  \n",
            "   [0]  edison                0.471   9.019  \n",
            "   [0]  co.                   0.469   6.354  \n",
            "   [1]  said                  0.465   0.000  \n",
            "   [0]  it                    0.461   3.718  \n",
            "   [0]  will                  0.458   4.609  \n",
            "Samples\n",
            "Sample 0 .  the american management books and fired mr. <unk> after he committed the social crime of making an appointment to see\n",
            "Sample 1 .  particularly against index arbitrage <eos> the <unk> ' strategy for the moment is to <unk> down and let the furor\n",
            "Sample 2 .  september and the change from august are from N in the previous month <eos> boston edison co. said it will\n",
            "\n",
            "\n",
            "targets[[1527 42 34 2619 784 8 14 9 113 494 23 1 42 34 306 2 486 1 0 916][330 321 9 267 1 30 58 1 29 68 87 32 316 1442 30 5790 10 216 975 13][920 678 122 17 0 2547 234 4 273 1726...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1527 42 34 2619 784 8 14 9 113...]...][[1 1 0 0 0 1 0 1 0 1...]...][[1 1527 42 10000 10000 10000 8 10000 9 10000...]...][[1527 42 34 2619 784 8 14 9 113 494...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1527 42 34 2619 784 8 14 9 113...]...][[1 1 0 0 0 1 0 1 0 1...]...][[1 1527 42 10000 10000 10000 8 10000 9 10000...]...][[1527 42 34 2619 784 8 14 9 113 494...]...]\n",
            "I0212 04:16:39.850275 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[0 8516 1 1 2217 4 1152 2 14 9 6 8401 51 474 576 453 665 2 23 1][3951 2 83 9 6 1 4170 382 99 44 0 524 2875 454 18 6 330 321 191 2][209 6 719 220 12 3 21 525 174 136...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[378 0 8516 1 1 2217 4 1152 2 14...]...][[1 0 0 1 1 0 1 0 1 0...]...][[378 0 10000 10000 1 2217 10000 1152 10000 14...]...][[0 8516 1 1 2217 4 1152 2 14 9...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[378 0 8516 1 1 2217 4 1152 2 14...]...][[1 0 0 1 1 0 1 0 1 0...]...][[378 0 10000 10000 1 2217 10000 1152 10000 14...]...][[0 8516 1 1 2217 4 1152 2 14 9...]...]\n",
            "I0212 04:16:41.932138 139783917786880 supervisor.py:1117] Saving checkpoint to path maskGAN/train/model.ckpt\n",
            "targets[[0 8516 1 1 2217 4 1152 2 14 9 6 8401 51 474 576 453 665 2 23 1][3951 2 83 9 6 1 4170 382 99 44 0 524 2875 454 18 6 330 321 191 2][209 6 719 220 12 3 21 525 174 136...]...]\n",
            "targets[[0 8516 1 1 2217 4 1152 2 14 9 6 8401 51 474 576 453 665 2 23 1][3951 2 83 9 6 1 4170 382 99 44 0 524 2875 454 18 6 330 321 191 2][209 6 719 220 12 3 21 525 174 136...]...]\n",
            "targets[[0 8516 1 1 2217 4 1152 2 14 9 6 8401 51 474 576 453 665 2 23 1][3951 2 83 9 6 1 4170 382 99 44 0 524 2875 454 18 6 330 321 191 2][209 6 719 220 12 3 21 525 174 136...]...]\n",
            "global_step: 1542\n",
            " perplexity: 671.350\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[0 8516 1 1 2217 4 1152 2 14 9 6 8401 51 474 576 453 665 2 23 1][3951 2 83 9 6 1 4170 382 99 44 0 524 2875 454 18 6 330 321 191 2][209 6 719 220 12 3 21 525 174 136...]...]\n",
            " percent of 3-grams captured: 0.256.\n",
            "targets[[0 8516 1 1 2217 4 1152 2 14 9 6 8401 51 474 576 453 665 2 23 1][3951 2 83 9 6 1 4170 382 99 44 0 524 2875 454 18 6 330 321 191 2][209 6 719 220 12 3 21 525 174 136...]...]\n",
            " percent of 2-grams captured: 0.489.\n",
            "targets[[0 8516 1 1 2217 4 1152 2 14 9 6 8401 51 474 576 453 665 2 23 1][3951 2 83 9 6 1 4170 382 99 44 0 524 2875 454 18 6 330 321 191 2][209 6 719 220 12 3 21 525 174 136...]...]\n",
            " percent of 4-grams captured: 0.138.\n",
            " geometric_avg: 0.259.\n",
            " arithmetic_avg: 0.294.\n",
            "global_step: 1542\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69642\n",
            " G train loss: 3.28875\n",
            "targets[[0 8516 1 1 2217 4 1152 2 14 9 6 8401 51 474 576 453 665 2 23 1][3951 2 83 9 6 1 4170 382 99 44 0 524 2875 454 18 6 330 321 191 2][209 6 719 220 12 3 21 525 174 136...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[378 0 8516 1 1 2217 4 1152 2 14...]...][[1 0 0 1 1 0 1 0 1 0...]...][[378 0 10000 10000 1 2217 10000 1152 10000 14...]...][[0 8516 1 1 2217 4 1152 2 14 9...]...]\n",
            " Sample 0.\n",
            "   [1]  the                   0.552   0.000  \n",
            "   [0]  venerable             0.558   11.108 \n",
            "   [0]  <unk>                 0.561   3.731  \n",
            "   [1]  <unk>                 0.560   0.000  \n",
            "   [1]  founder               0.556   0.000  \n",
            "   [0]  of                    0.550   2.815  \n",
            "   [1]  sony                  0.548   0.000  \n",
            "   [0]  <eos>                 0.543   2.648  \n",
            "   [1]  it                    0.543   0.000  \n",
            "   [0]  's                    0.546   4.149  \n",
            "   [0]  a                     0.555   4.197  \n",
            "   [0]  shame                 0.561   10.563 \n",
            "   [1]  their                 0.564   0.000  \n",
            "   [1]  meeting               0.560   0.000  \n",
            "   [0]  never                 0.554   9.617  \n",
            "   [0]  took                  0.548   7.646  \n",
            "   [1]  place                 0.546   0.000  \n",
            "   [1]  <eos>                 0.544   0.000  \n",
            "   [1]  mr.                   0.541   0.000  \n",
            "   [0]  <unk>                 0.541   2.770  \n",
            " Sample 1.\n",
            "   [0]  die                   0.489   14.461 \n",
            "   [1]  <eos>                 0.499   0.000  \n",
            "   [0]  there                 0.508   7.063  \n",
            "   [0]  's                    0.516   4.069  \n",
            "   [0]  a                     0.523   3.866  \n",
            "   [1]  <unk>                 0.527   0.000  \n",
            "   [0]  psychology            0.526   13.042 \n",
            "   [1]  right                 0.521   0.000  \n",
            "   [1]  now                   0.516   0.000  \n",
            "   [1]  says                  0.512   0.000  \n",
            "   [0]  the                   0.509   1.871  \n",
            "   [1]  top                   0.510   0.000  \n",
            "   [1]  program-trading       0.511   0.000  \n",
            "   [0]  official              0.509   9.010  \n",
            "   [1]  at                    0.506   0.000  \n",
            "   [0]  a                     0.506   2.187  \n",
            "   [0]  wall                  0.505   6.355  \n",
            "   [0]  street                0.506   6.628  \n",
            "   [1]  firm                  0.507   0.000  \n",
            "   [1]  <eos>                 0.508   0.000  \n",
            " Sample 2.\n",
            "   [1]  take                  0.532   0.000  \n",
            "   [1]  a                     0.545   0.000  \n",
            "   [0]  previously            0.554   7.531  \n",
            "   [0]  reported              0.554   7.451  \n",
            "   [1]  $                     0.550   0.000  \n",
            "   [0]  N                     0.544   0.018  \n",
            "   [0]  million               0.541   0.885  \n",
            "   [0]  charge                0.538   8.921  \n",
            "   [1]  against               0.537   0.000  \n",
            "   [0]  earnings              0.536   6.530  \n",
            "   [0]  in                    0.535   3.176  \n",
            "   [1]  the                   0.536   0.000  \n",
            "   [1]  fourth                0.534   0.000  \n",
            "   [1]  quarter               0.530   0.000  \n",
            "   [1]  <eos>                 0.525   0.000  \n",
            "   [0]  the                   0.524   1.577  \n",
            "   [0]  charge                0.524   7.588  \n",
            "   [1]  resulted              0.524   0.000  \n",
            "   [0]  from                  0.524   4.414  \n",
            "   [0]  a                     0.527   2.484  \n",
            "Samples\n",
            "Sample 0 .  the venerable <unk> <unk> founder of sony <eos> it 's a shame their meeting never took place <eos> mr. <unk>\n",
            "Sample 1 .  die <eos> there 's a <unk> psychology right now says the top program-trading official at a wall street firm <eos>\n",
            "Sample 2 .  take a previously reported $ N million charge against earnings in the fourth quarter <eos> the charge resulted from a\n",
            "\n",
            "\n",
            "targets[[9 1783 2479 3177 1704 1389 7 605 11 132 72 2 45 8 45 7967 4 0 3973 26][0 434 2 306 121 161 13 18 320 11 156 280 5 437 52 2 11 471 974 288][72 8 2241 506 592 5 526 11 1151 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[916 9 1783 2479 3177 1704 1389 7 605 11...]...][[1 0 1 0 0 0 1 0 0 0...]...][[916 9 10000 2479 10000 10000 10000 7 10000 10000...]...][[9 1783 2479 3177 1704 1389 7 605 11 132...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[916 9 1783 2479 3177 1704 1389 7 605 11...]...][[1 0 1 0 0 0 1 0 0 0...]...][[916 9 10000 2479 10000 10000 10000 7 10000 10000...]...][[9 1783 2479 3177 1704 1389 7 605 11 132...]...]\n",
            "I0212 04:16:44.327815 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[1527 42 34 2619 784 8 14 9 113 494 23 1 42 34 306 2 486 1 0 916][330 321 9 267 1 30 58 1 29 68 87 32 316 1442 30 5790 10 216 975 13][920 678 122 17 0 2547 234 4 273 1726...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1527 42 34 2619 784 8 14 9 113...]...][[0 1 1 0 1 1 0 0 0 0...]...][[1 10000 42 34 10000 784 8 10000 10000 10000...]...][[1527 42 34 2619 784 8 14 9 113 494...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1527 42 34 2619 784 8 14 9 113...]...][[0 1 1 0 1 1 0 0 0 0...]...][[1 10000 42 34 10000 784 8 10000 10000 10000...]...][[1527 42 34 2619 784 8 14 9 113 494...]...]\n",
            "targets[[1527 42 34 2619 784 8 14 9 113 494 23 1 42 34 306 2 486 1 0 916][330 321 9 267 1 30 58 1 29 68 87 32 316 1442 30 5790 10 216 975 13][920 678 122 17 0 2547 234 4 273 1726...]...]\n",
            "targets[[1527 42 34 2619 784 8 14 9 113 494 23 1 42 34 306 2 486 1 0 916][330 321 9 267 1 30 58 1 29 68 87 32 316 1442 30 5790 10 216 975 13][920 678 122 17 0 2547 234 4 273 1726...]...]\n",
            "targets[[1527 42 34 2619 784 8 14 9 113 494 23 1 42 34 306 2 486 1 0 916][330 321 9 267 1 30 58 1 29 68 87 32 316 1442 30 5790 10 216 975 13][920 678 122 17 0 2547 234 4 273 1726...]...]\n",
            "global_step: 1545\n",
            " perplexity: 670.885\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[1527 42 34 2619 784 8 14 9 113 494 23 1 42 34 306 2 486 1 0 916][330 321 9 267 1 30 58 1 29 68 87 32 316 1442 30 5790 10 216 975 13][920 678 122 17 0 2547 234 4 273 1726...]...]\n",
            " percent of 3-grams captured: 0.222.\n",
            "targets[[1527 42 34 2619 784 8 14 9 113 494 23 1 42 34 306 2 486 1 0 916][330 321 9 267 1 30 58 1 29 68 87 32 316 1442 30 5790 10 216 975 13][920 678 122 17 0 2547 234 4 273 1726...]...]\n",
            " percent of 2-grams captured: 0.479.\n",
            "targets[[1527 42 34 2619 784 8 14 9 113 494 23 1 42 34 306 2 486 1 0 916][330 321 9 267 1 30 58 1 29 68 87 32 316 1442 30 5790 10 216 975 13][920 678 122 17 0 2547 234 4 273 1726...]...]\n",
            " percent of 4-grams captured: 0.103.\n",
            " geometric_avg: 0.222.\n",
            " arithmetic_avg: 0.268.\n",
            "global_step: 1545\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69645\n",
            " G train loss: 3.28821\n",
            "targets[[1527 42 34 2619 784 8 14 9 113 494 23 1 42 34 306 2 486 1 0 916][330 321 9 267 1 30 58 1 29 68 87 32 316 1442 30 5790 10 216 975 13][920 678 122 17 0 2547 234 4 273 1726...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1527 42 34 2619 784 8 14 9 113...]...][[0 1 1 0 1 1 0 0 0 0...]...][[1 10000 42 34 10000 784 8 10000 10000 10000...]...][[1527 42 34 2619 784 8 14 9 113 494...]...]\n",
            " Sample 0.\n",
            "   [0]  certainly             0.511   9.750  \n",
            "   [1]  would                 0.511   0.000  \n",
            "   [1]  have                  0.510   0.000  \n",
            "   [0]  learned               0.509   10.188 \n",
            "   [1]  something             0.508   0.000  \n",
            "   [1]  and                   0.506   0.000  \n",
            "   [0]  it                    0.508   4.434  \n",
            "   [0]  's                    0.511   3.897  \n",
            "   [0]  even                  0.514   6.465  \n",
            "   [0]  possible              0.517   7.534  \n",
            "   [0]  mr.                   0.517   5.549  \n",
            "   [1]  <unk>                 0.516   0.000  \n",
            "   [0]  would                 0.513   5.961  \n",
            "   [1]  have                  0.511   0.000  \n",
            "   [0]  too                   0.510   6.539  \n",
            "   [1]  <eos>                 0.510   0.000  \n",
            "   [0]  ms.                   0.512   6.698  \n",
            "   [1]  <unk>                 0.515   0.000  \n",
            "   [1]  the                   0.517   0.000  \n",
            "   [0]  journal               0.519   8.918  \n",
            " Sample 1.\n",
            "   [1]  wall                  0.540   0.000  \n",
            "   [1]  street                0.538   0.000  \n",
            "   [1]  's                    0.537   0.000  \n",
            "   [1]  cash                  0.536   0.000  \n",
            "   [1]  <unk>                 0.534   0.000  \n",
            "   [0]  has                   0.533   5.688  \n",
            "   [0]  been                  0.533   5.489  \n",
            "   [1]  <unk>                 0.533   0.000  \n",
            "   [0]  but                   0.534   6.088  \n",
            "   [1]  i                     0.533   0.000  \n",
            "   [1]  do                    0.534   0.000  \n",
            "   [1]  n't                   0.534   0.000  \n",
            "   [1]  think                 0.535   0.000  \n",
            "   [0]  anyone                0.534   8.351  \n",
            "   [0]  has                   0.534   5.311  \n",
            "   [1]  proven                0.534   0.000  \n",
            "   [0]  that                  0.536   4.147  \n",
            "   [1]  index                 0.537   0.000  \n",
            "   [1]  arbitrage             0.534   0.000  \n",
            "   [0]  is                    0.530   4.871  \n",
            " Sample 2.\n",
            "   [1]  settlement            0.453   0.000  \n",
            "   [1]  approved              0.450   0.000  \n",
            "   [0]  yesterday             0.451   6.656  \n",
            "   [0]  by                    0.453   4.323  \n",
            "   [0]  the                   0.455   1.423  \n",
            "   [0]  massachusetts         0.457   11.032 \n",
            "   [0]  department            0.460   6.297  \n",
            "   [0]  of                    0.458   3.147  \n",
            "   [1]  public                0.453   0.000  \n",
            "   [0]  utilities             0.449   9.651  \n",
            "   [1]  <eos>                 0.445   0.000  \n",
            "   [1]  as                    0.442   0.000  \n",
            "   [0]  expected              0.442   6.556  \n",
            "   [0]  the                   0.443   2.487  \n",
            "   [0]  settlement            0.444   9.600  \n",
            "   [1]  limits                0.446   0.000  \n",
            "   [1]  rate                  0.446   0.000  \n",
            "   [1]  increases             0.447   0.000  \n",
            "   [0]  for                   0.448   3.736  \n",
            "   [1]  three                 0.449   0.000  \n",
            "Samples\n",
            "Sample 0 .  certainly would have learned something and it 's even possible mr. <unk> would have too <eos> ms. <unk> the journal\n",
            "Sample 1 .  wall street 's cash <unk> has been <unk> but i do n't think anyone has proven that index arbitrage is\n",
            "Sample 2 .  settlement approved yesterday by the massachusetts department of public utilities <eos> as expected the settlement limits rate increases for three\n",
            "\n",
            "\n",
            "targets[[2332 809 4 2514 4569 2 7 2107 6 35 278 2408 8421 5 335 84 51 3574 157 4689][542 77 7 482 7 3 8 531 75 72 38 46 0 6901 288 315 821 1613 2 60][767 2 7 27 530 0 139 1388 302 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[26 2332 809 4 2514 4569 2 7 2107 6...]...][[1 0 0 0 0 1 1 0 1 0...]...][[26 2332 10000 10000 10000 10000 2 7 10000 6...]...][[2332 809 4 2514 4569 2 7 2107 6 35...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[26 2332 809 4 2514 4569 2 7 2107 6...]...][[1 0 0 0 0 1 1 0 1 0...]...][[26 2332 10000 10000 10000 10000 2 7 10000 6...]...][[2332 809 4 2514 4569 2 7 2107 6 35...]...]\n",
            "I0212 04:16:48.326273 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[9 1783 2479 3177 1704 1389 7 605 11 132 72 2 45 8 45 7967 4 0 3973 26][0 434 2 306 121 161 13 18 320 11 156 280 5 437 52 2 11 471 974 288][72 8 2241 506 592 5 526 11 1151 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[916 9 1783 2479 3177 1704 1389 7 605 11...]...][[1 0 1 0 0 1 1 1 0 0...]...][[916 9 10000 2479 10000 10000 1389 7 605 10000...]...][[9 1783 2479 3177 1704 1389 7 605 11 132...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[916 9 1783 2479 3177 1704 1389 7 605 11...]...][[1 0 1 0 0 1 1 1 0 0...]...][[916 9 10000 2479 10000 10000 1389 7 605 10000...]...][[9 1783 2479 3177 1704 1389 7 605 11 132...]...]\n",
            "targets[[9 1783 2479 3177 1704 1389 7 605 11 132 72 2 45 8 45 7967 4 0 3973 26][0 434 2 306 121 161 13 18 320 11 156 280 5 437 52 2 11 471 974 288][72 8 2241 506 592 5 526 11 1151 4...]...]\n",
            "targets[[9 1783 2479 3177 1704 1389 7 605 11 132 72 2 45 8 45 7967 4 0 3973 26][0 434 2 306 121 161 13 18 320 11 156 280 5 437 52 2 11 471 974 288][72 8 2241 506 592 5 526 11 1151 4...]...]\n",
            "targets[[9 1783 2479 3177 1704 1389 7 605 11 132 72 2 45 8 45 7967 4 0 3973 26][0 434 2 306 121 161 13 18 320 11 156 280 5 437 52 2 11 471 974 288][72 8 2241 506 592 5 526 11 1151 4...]...]\n",
            "global_step: 1548\n",
            " perplexity: 670.454\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[9 1783 2479 3177 1704 1389 7 605 11 132 72 2 45 8 45 7967 4 0 3973 26][0 434 2 306 121 161 13 18 320 11 156 280 5 437 52 2 11 471 974 288][72 8 2241 506 592 5 526 11 1151 4...]...]\n",
            " percent of 3-grams captured: 0.225.\n",
            "targets[[9 1783 2479 3177 1704 1389 7 605 11 132 72 2 45 8 45 7967 4 0 3973 26][0 434 2 306 121 161 13 18 320 11 156 280 5 437 52 2 11 471 974 288][72 8 2241 506 592 5 526 11 1151 4...]...]\n",
            " percent of 2-grams captured: 0.468.\n",
            "targets[[9 1783 2479 3177 1704 1389 7 605 11 132 72 2 45 8 45 7967 4 0 3973 26][0 434 2 306 121 161 13 18 320 11 156 280 5 437 52 2 11 471 974 288][72 8 2241 506 592 5 526 11 1151 4...]...]\n",
            " percent of 4-grams captured: 0.141.\n",
            " geometric_avg: 0.246.\n",
            " arithmetic_avg: 0.278.\n",
            "global_step: 1548\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69650\n",
            " G train loss: 3.28799\n",
            "targets[[9 1783 2479 3177 1704 1389 7 605 11 132 72 2 45 8 45 7967 4 0 3973 26][0 434 2 306 121 161 13 18 320 11 156 280 5 437 52 2 11 471 974 288][72 8 2241 506 592 5 526 11 1151 4...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[916 9 1783 2479 3177 1704 1389 7 605 11...]...][[1 0 1 0 0 1 1 1 0 0...]...][[916 9 10000 2479 10000 10000 1389 7 605 10000...]...][[9 1783 2479 3177 1704 1389 7 605 11 132...]...]\n",
            " Sample 0.\n",
            "   [1]  's                    0.460   0.000  \n",
            "   [0]  deputy                0.455   8.227  \n",
            "   [1]  editorial             0.456   0.000  \n",
            "   [0]  features              0.459   10.919 \n",
            "   [0]  editor                0.463   9.818  \n",
            "   [1]  worked                0.464   0.000  \n",
            "   [1]  in                    0.463   0.000  \n",
            "   [1]  tokyo                 0.464   0.000  \n",
            "   [0]  for                   0.464   3.889  \n",
            "   [0]  three                 0.466   6.798  \n",
            "   [0]  years                 0.467   5.685  \n",
            "   [0]  <eos>                 0.462   2.552  \n",
            "   [0]  more                  0.459   5.896  \n",
            "   [0]  and                   0.461   4.962  \n",
            "   [1]  more                  0.462   0.000  \n",
            "   [0]  corners               0.462   11.113 \n",
            "   [0]  of                    0.463   3.472  \n",
            "   [0]  the                   0.464   2.797  \n",
            "   [1]  globe                 0.464   0.000  \n",
            "   [0]  are                   0.464   4.848  \n",
            " Sample 1.\n",
            "   [0]  the                   0.414   2.552  \n",
            "   [1]  problem               0.416   0.000  \n",
            "   [0]  <eos>                 0.420   2.831  \n",
            "   [1]  too                   0.425   0.000  \n",
            "   [0]  much                  0.432   6.683  \n",
            "   [1]  money                 0.440   0.000  \n",
            "   [0]  is                    0.444   4.857  \n",
            "   [0]  at                    0.443   5.236  \n",
            "   [0]  stake                 0.442   8.112  \n",
            "   [1]  for                   0.438   0.000  \n",
            "   [0]  program               0.435   6.433  \n",
            "   [1]  traders               0.434   0.000  \n",
            "   [0]  to                    0.432   3.185  \n",
            "   [0]  give                  0.427   7.383  \n",
            "   [0]  up                    0.423   6.618  \n",
            "   [0]  <eos>                 0.419   3.488  \n",
            "   [1]  for                   0.417   0.000  \n",
            "   [1]  example               0.419   0.000  \n",
            "   [1]  stock-index           0.422   0.000  \n",
            "   [0]  futures               0.426   8.113  \n",
            " Sample 2.\n",
            "   [0]  years                 0.532   6.032  \n",
            "   [1]  and                   0.534   0.000  \n",
            "   [0]  ties                  0.537   10.100 \n",
            "   [0]  future                0.536   9.293  \n",
            "   [1]  charges               0.533   0.000  \n",
            "   [1]  to                    0.532   0.000  \n",
            "   [0]  customers             0.532   9.365  \n",
            "   [1]  for                   0.532   0.000  \n",
            "   [0]  operation             0.532   10.608 \n",
            "   [1]  of                    0.532   0.000  \n",
            "   [0]  the                   0.532   0.988  \n",
            "   [1]  troubled              0.531   0.000  \n",
            "   [1]  <unk>                 0.530   0.000  \n",
            "   [1]  nuclear               0.530   0.000  \n",
            "   [0]  power                 0.529   7.345  \n",
            "   [0]  station               0.528   9.641  \n",
            "   [0]  to                    0.528   3.396  \n",
            "   [0]  that                  0.528   5.801  \n",
            "   [1]  plant                 0.530   0.000  \n",
            "   [0]  's                    0.532   4.736  \n",
            "Samples\n",
            "Sample 0 .  's deputy editorial features editor worked in tokyo for three years <eos> more and more corners of the globe are\n",
            "Sample 1 .  the problem <eos> too much money is at stake for program traders to give up <eos> for example stock-index futures\n",
            "Sample 2 .  years and ties future charges to customers for operation of the troubled <unk> nuclear power station to that plant 's\n",
            "\n",
            "\n",
            "targets[[2750 234 643 8 1259 1584 36 663 6 12 3 1818 2 1 8 549 5951 26 5664 20][288 77 30 1 3818 4 1 7 51 1 8 1 2 99 16 6 262 272 482 9][7 0 265 4 0 402 9 1151 2 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4689 2750 234 643 8 1259 1584 36 663 6...]...][[1 0 0 0 1 1 0 0 1 1...]...][[4689 2750 10000 10000 10000 1259 1584 10000 10000 6...]...][[2750 234 643 8 1259 1584 36 663 6 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4689 2750 234 643 8 1259 1584 36 663 6...]...][[1 0 0 0 1 1 0 0 1 1...]...][[4689 2750 10000 10000 10000 1259 1584 10000 10000 6...]...][[2750 234 643 8 1259 1584 36 663 6 12...]...]\n",
            "I0212 04:16:51.593808 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[2332 809 4 2514 4569 2 7 2107 6 35 278 2408 8421 5 335 84 51 3574 157 4689][542 77 7 482 7 3 8 531 75 72 38 46 0 6901 288 315 821 1613 2 60][767 2 7 27 530 0 139 1388 302 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[26 2332 809 4 2514 4569 2 7 2107 6...]...][[1 1 0 0 0 0 1 1 0 1...]...][[26 2332 809 10000 10000 10000 10000 7 2107 10000...]...][[2332 809 4 2514 4569 2 7 2107 6 35...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[26 2332 809 4 2514 4569 2 7 2107 6...]...][[1 1 0 0 0 0 1 1 0 1...]...][[26 2332 809 10000 10000 10000 10000 7 2107 10000...]...][[2332 809 4 2514 4569 2 7 2107 6 35...]...]\n",
            "targets[[2332 809 4 2514 4569 2 7 2107 6 35 278 2408 8421 5 335 84 51 3574 157 4689][542 77 7 482 7 3 8 531 75 72 38 46 0 6901 288 315 821 1613 2 60][767 2 7 27 530 0 139 1388 302 15...]...]\n",
            "targets[[2332 809 4 2514 4569 2 7 2107 6 35 278 2408 8421 5 335 84 51 3574 157 4689][542 77 7 482 7 3 8 531 75 72 38 46 0 6901 288 315 821 1613 2 60][767 2 7 27 530 0 139 1388 302 15...]...]\n",
            "targets[[2332 809 4 2514 4569 2 7 2107 6 35 278 2408 8421 5 335 84 51 3574 157 4689][542 77 7 482 7 3 8 531 75 72 38 46 0 6901 288 315 821 1613 2 60][767 2 7 27 530 0 139 1388 302 15...]...]\n",
            "global_step: 1551\n",
            " perplexity: 669.539\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[2332 809 4 2514 4569 2 7 2107 6 35 278 2408 8421 5 335 84 51 3574 157 4689][542 77 7 482 7 3 8 531 75 72 38 46 0 6901 288 315 821 1613 2 60][767 2 7 27 530 0 139 1388 302 15...]...]\n",
            " percent of 3-grams captured: 0.225.\n",
            "targets[[2332 809 4 2514 4569 2 7 2107 6 35 278 2408 8421 5 335 84 51 3574 157 4689][542 77 7 482 7 3 8 531 75 72 38 46 0 6901 288 315 821 1613 2 60][767 2 7 27 530 0 139 1388 302 15...]...]\n",
            " percent of 2-grams captured: 0.492.\n",
            "targets[[2332 809 4 2514 4569 2 7 2107 6 35 278 2408 8421 5 335 84 51 3574 157 4689][542 77 7 482 7 3 8 531 75 72 38 46 0 6901 288 315 821 1613 2 60][767 2 7 27 530 0 139 1388 302 15...]...]\n",
            " percent of 4-grams captured: 0.109.\n",
            " geometric_avg: 0.229.\n",
            " arithmetic_avg: 0.275.\n",
            "global_step: 1551\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69648\n",
            " G train loss: 3.28739\n",
            "targets[[2332 809 4 2514 4569 2 7 2107 6 35 278 2408 8421 5 335 84 51 3574 157 4689][542 77 7 482 7 3 8 531 75 72 38 46 0 6901 288 315 821 1613 2 60][767 2 7 27 530 0 139 1388 302 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[26 2332 809 4 2514 4569 2 7 2107 6...]...][[1 1 0 0 0 0 1 1 0 1...]...][[26 2332 809 10000 10000 10000 10000 7 2107 10000...]...][[2332 809 4 2514 4569 2 7 2107 6 35...]...]\n",
            " Sample 0.\n",
            "   [1]  becoming              0.489   0.000  \n",
            "   [1]  free                  0.492   0.000  \n",
            "   [0]  of                    0.494   4.261  \n",
            "   [0]  tobacco               0.494   7.967  \n",
            "   [0]  smoke                 0.490   10.732 \n",
            "   [0]  <eos>                 0.491   3.964  \n",
            "   [1]  in                    0.491   0.000  \n",
            "   [1]  singapore             0.492   0.000  \n",
            "   [0]  a                     0.495   4.612  \n",
            "   [1]  new                   0.496   0.000  \n",
            "   [0]  law                   0.495   7.709  \n",
            "   [0]  requires              0.491   10.173 \n",
            "   [1]  smokers               0.490   0.000  \n",
            "   [1]  to                    0.490   0.000  \n",
            "   [1]  put                   0.491   0.000  \n",
            "   [1]  out                   0.493   0.000  \n",
            "   [1]  their                 0.495   0.000  \n",
            "   [1]  cigarettes            0.495   0.000  \n",
            "   [0]  before                0.496   6.958  \n",
            "   [1]  entering              0.495   0.000  \n",
            " Sample 1.\n",
            "   [0]  began                 0.488   8.133  \n",
            "   [0]  trading               0.490   6.462  \n",
            "   [1]  in                    0.491   0.000  \n",
            "   [0]  chicago               0.493   9.091  \n",
            "   [1]  in                    0.494   0.000  \n",
            "   [0]  N                     0.497   2.692  \n",
            "   [0]  and                   0.500   4.411  \n",
            "   [1]  within                0.502   0.000  \n",
            "   [1]  two                   0.502   0.000  \n",
            "   [1]  years                 0.500   0.000  \n",
            "   [0]  they                  0.498   6.112  \n",
            "   [1]  were                  0.496   0.000  \n",
            "   [1]  the                   0.494   0.000  \n",
            "   [0]  fastest-growing       0.493   11.228 \n",
            "   [1]  futures               0.494   0.000  \n",
            "   [0]  contract              0.495   7.585  \n",
            "   [1]  ever                  0.497   0.000  \n",
            "   [1]  launched              0.499   0.000  \n",
            "   [1]  <eos>                 0.499   0.000  \n",
            "   [0]  stock                 0.500   8.294  \n",
            " Sample 2.\n",
            "   [1]  performance           0.513   0.000  \n",
            "   [0]  <eos>                 0.517   1.814  \n",
            "   [0]  in                    0.525   5.068  \n",
            "   [1]  its                   0.529   0.000  \n",
            "   [1]  order                 0.530   0.000  \n",
            "   [0]  the                   0.527   5.465  \n",
            "   [0]  state                 0.521   5.678  \n",
            "   [1]  regulatory            0.513   0.000  \n",
            "   [0]  agency                0.506   8.515  \n",
            "   [1]  said                  0.503   0.000  \n",
            "   [0]  the                   0.502   1.935  \n",
            "   [1]  company               0.503   0.000  \n",
            "   [1]  must                  0.504   0.000  \n",
            "   [0]  be                    0.506   5.034  \n",
            "   [1]  held                  0.506   0.000  \n",
            "   [0]  <unk>                 0.505   4.073  \n",
            "   [0]  for                   0.507   4.181  \n",
            "   [0]  the                   0.508   1.717  \n",
            "   [1]  mistakes              0.511   0.000  \n",
            "   [1]  made                  0.511   0.000  \n",
            "Samples\n",
            "Sample 0 .  becoming free of tobacco smoke <eos> in singapore a new law requires smokers to put out their cigarettes before entering\n",
            "Sample 1 .  began trading in chicago in N and within two years they were the fastest-growing futures contract ever launched <eos> stock\n",
            "Sample 2 .  performance <eos> in its order the state regulatory agency said the company must be held <unk> for the mistakes made\n",
            "\n",
            "\n",
            "targets[[0 2050 8 3760 33 25 3207 7 7278 1609 198 1 887 31 454 15 2 2107 292 6765][974 280 218 45 603 985 4 60 288 55 0 129 146 1247 7 60 2 99 0 2278][158 119 5 3 3 18 1989 9 12 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 2050 8 3760 33 25 3207 7 7278...]...][[1 1 0 1 1 1 0 1 1 1...]...][[20 0 2050 10000 3760 33 25 10000 7 7278...]...][[0 2050 8 3760 33 25 3207 7 7278 1609...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 2050 8 3760 33 25 3207 7 7278...]...][[1 1 0 1 1 1 0 1 1 1...]...][[20 0 2050 10000 3760 33 25 10000 7 7278...]...][[0 2050 8 3760 33 25 3207 7 7278 1609...]...]\n",
            "I0212 04:16:55.069463 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[2750 234 643 8 1259 1584 36 663 6 12 3 1818 2 1 8 549 5951 26 5664 20][288 77 30 1 3818 4 1 7 51 1 8 1 2 99 16 6 262 272 482 9][7 0 265 4 0 402 9 1151 2 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4689 2750 234 643 8 1259 1584 36 663 6...]...][[0 0 0 0 0 0 0 1 1 1...]...][[4689 10000 10000 10000 10000 10000 10000 10000 663 6...]...][[2750 234 643 8 1259 1584 36 663 6 12...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4689 2750 234 643 8 1259 1584 36 663 6...]...][[0 0 0 0 0 0 0 1 1 1...]...][[4689 10000 10000 10000 10000 10000 10000 10000 663 6...]...][[2750 234 643 8 1259 1584 36 663 6 12...]...]\n",
            "targets[[2750 234 643 8 1259 1584 36 663 6 12 3 1818 2 1 8 549 5951 26 5664 20][288 77 30 1 3818 4 1 7 51 1 8 1 2 99 16 6 262 272 482 9][7 0 265 4 0 402 9 1151 2 1...]...]\n",
            "targets[[2750 234 643 8 1259 1584 36 663 6 12 3 1818 2 1 8 549 5951 26 5664 20][288 77 30 1 3818 4 1 7 51 1 8 1 2 99 16 6 262 272 482 9][7 0 265 4 0 402 9 1151 2 1...]...]\n",
            "targets[[2750 234 643 8 1259 1584 36 663 6 12 3 1818 2 1 8 549 5951 26 5664 20][288 77 30 1 3818 4 1 7 51 1 8 1 2 99 16 6 262 272 482 9][7 0 265 4 0 402 9 1151 2 1...]...]\n",
            "global_step: 1554\n",
            " perplexity: 669.068\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[2750 234 643 8 1259 1584 36 663 6 12 3 1818 2 1 8 549 5951 26 5664 20][288 77 30 1 3818 4 1 7 51 1 8 1 2 99 16 6 262 272 482 9][7 0 265 4 0 402 9 1151 2 1...]...]\n",
            " percent of 3-grams captured: 0.203.\n",
            "targets[[2750 234 643 8 1259 1584 36 663 6 12 3 1818 2 1 8 549 5951 26 5664 20][288 77 30 1 3818 4 1 7 51 1 8 1 2 99 16 6 262 272 482 9][7 0 265 4 0 402 9 1151 2 1...]...]\n",
            " percent of 2-grams captured: 0.508.\n",
            "targets[[2750 234 643 8 1259 1584 36 663 6 12 3 1818 2 1 8 549 5951 26 5664 20][288 77 30 1 3818 4 1 7 51 1 8 1 2 99 16 6 262 272 482 9][7 0 265 4 0 402 9 1151 2 1...]...]\n",
            " percent of 4-grams captured: 0.106.\n",
            " geometric_avg: 0.222.\n",
            " arithmetic_avg: 0.272.\n",
            "global_step: 1554\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69650\n",
            " G train loss: 3.28675\n",
            "targets[[2750 234 643 8 1259 1584 36 663 6 12 3 1818 2 1 8 549 5951 26 5664 20][288 77 30 1 3818 4 1 7 51 1 8 1 2 99 16 6 262 272 482 9][7 0 265 4 0 402 9 1151 2 1...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4689 2750 234 643 8 1259 1584 36 663 6...]...][[0 0 0 0 0 0 0 1 1 1...]...][[4689 10000 10000 10000 10000 10000 10000 10000 663 6...]...][[2750 234 643 8 1259 1584 36 663 6 12...]...]\n",
            " Sample 0.\n",
            "   [0]  restaurants           0.496   11.548 \n",
            "   [0]  department            0.491   8.567  \n",
            "   [0]  stores                0.491   10.310 \n",
            "   [0]  and                   0.493   3.652  \n",
            "   [0]  sports                0.495   9.226  \n",
            "   [0]  centers               0.495   9.180  \n",
            "   [0]  or                    0.494   6.023  \n",
            "   [1]  face                  0.495   0.000  \n",
            "   [1]  a                     0.497   0.000  \n",
            "   [1]  $                     0.499   0.000  \n",
            "   [1]  N                     0.500   0.000  \n",
            "   [1]  fine                  0.502   0.000  \n",
            "   [1]  <eos>                 0.502   0.000  \n",
            "   [0]  <unk>                 0.505   3.298  \n",
            "   [1]  and                   0.506   0.000  \n",
            "   [0]  private               0.506   7.885  \n",
            "   [1]  clubs                 0.505   0.000  \n",
            "   [0]  are                   0.504   4.595  \n",
            "   [1]  exempt                0.507   0.000  \n",
            "   [1]  from                  0.510   0.000  \n",
            " Sample 1.\n",
            "   [1]  futures               0.476   0.000  \n",
            "   [1]  trading               0.477   0.000  \n",
            "   [1]  has                   0.480   0.000  \n",
            "   [0]  <unk>                 0.485   2.832  \n",
            "   [0]  dozens                0.488   10.559 \n",
            "   [1]  of                    0.489   0.000  \n",
            "   [1]  <unk>                 0.489   0.000  \n",
            "   [1]  in                    0.486   0.000  \n",
            "   [1]  their                 0.480   0.000  \n",
            "   [1]  <unk>                 0.476   0.000  \n",
            "   [0]  and                   0.474   3.469  \n",
            "   [0]  <unk>                 0.474   3.192  \n",
            "   [1]  <eos>                 0.473   0.000  \n",
            "   [1]  now                   0.473   0.000  \n",
            "   [1]  on                    0.472   0.000  \n",
            "   [1]  a                     0.472   0.000  \n",
            "   [1]  good                  0.472   0.000  \n",
            "   [0]  day                   0.472   8.752  \n",
            "   [0]  chicago               0.471   9.946  \n",
            "   [0]  's                    0.472   4.298  \n",
            " Sample 2.\n",
            "   [0]  in                    0.471   4.301  \n",
            "   [0]  the                   0.467   0.490  \n",
            "   [0]  management            0.467   7.465  \n",
            "   [0]  of                    0.463   1.995  \n",
            "   [1]  the                   0.460   0.000  \n",
            "   [0]  plant                 0.459   6.848  \n",
            "   [1]  's                    0.460   0.000  \n",
            "   [1]  operation             0.460   0.000  \n",
            "   [1]  <eos>                 0.458   0.000  \n",
            "   [0]  <unk>                 0.460   4.102  \n",
            "   [0]  had                   0.464   5.559  \n",
            "   [0]  been                  0.470   5.205  \n",
            "   [0]  closed                0.470   8.250  \n",
            "   [0]  for                   0.469   3.429  \n",
            "   [0]  N                     0.464   2.796  \n",
            "   [0]  months                0.460   6.223  \n",
            "   [0]  <eos>                 0.454   1.839  \n",
            "   [1]  the                   0.455   0.000  \n",
            "   [1]  average               0.464   0.000  \n",
            "   [0]  interest              0.474   7.408  \n",
            "Samples\n",
            "Sample 0 .  restaurants department stores and sports centers or face a $ N fine <eos> <unk> and private clubs are exempt from\n",
            "Sample 1 .  futures trading has <unk> dozens of <unk> in their <unk> and <unk> <eos> now on a good day chicago 's\n",
            "Sample 2 .  in the management of the plant 's operation <eos> <unk> had been closed for N months <eos> the average interest\n",
            "\n",
            "\n",
            "targets[[3760 7 73 7763 5927 273 6879 3385 8 2534 2750 2 7 3394 1 1 1 6 1783 1044][13 388 11 0 1115 5 1119 84 2 0 1 26 633 57 6940 1 20 286 2 156][20 3 3 18 69 123 9 226 2 1392...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6765 3760 7 73 7763 5927 273 6879 3385 8...]...][[1 0 0 1 0 0 0 0 0 1...]...][[6765 3760 10000 10000 7763 10000 10000 10000 10000 10000...]...][[3760 7 73 7763 5927 273 6879 3385 8 2534...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6765 3760 7 73 7763 5927 273 6879 3385 8...]...][[1 0 0 1 0 0 0 0 0 1...]...][[6765 3760 10000 10000 7763 10000 10000 10000 10000 10000...]...][[3760 7 73 7763 5927 273 6879 3385 8 2534...]...]\n",
            "I0212 04:16:58.393505 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[0 2050 8 3760 33 25 3207 7 7278 1609 198 1 887 31 454 15 2 2107 292 6765][974 280 218 45 603 985 4 60 288 55 0 129 146 1247 7 60 2 99 0 2278][158 119 5 3 3 18 1989 9 12 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 2050 8 3760 33 25 3207 7 7278...]...][[1 0 0 0 1 0 0 0 1 0...]...][[20 0 10000 10000 10000 33 10000 10000 10000 7278...]...][[0 2050 8 3760 33 25 3207 7 7278 1609...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 2050 8 3760 33 25 3207 7 7278...]...][[1 0 0 0 1 0 0 0 1 0...]...][[20 0 10000 10000 10000 33 10000 10000 10000 7278...]...][[0 2050 8 3760 33 25 3207 7 7278 1609...]...]\n",
            "targets[[0 2050 8 3760 33 25 3207 7 7278 1609 198 1 887 31 454 15 2 2107 292 6765][974 280 218 45 603 985 4 60 288 55 0 129 146 1247 7 60 2 99 0 2278][158 119 5 3 3 18 1989 9 12 3...]...]\n",
            "targets[[0 2050 8 3760 33 25 3207 7 7278 1609 198 1 887 31 454 15 2 2107 292 6765][974 280 218 45 603 985 4 60 288 55 0 129 146 1247 7 60 2 99 0 2278][158 119 5 3 3 18 1989 9 12 3...]...]\n",
            "targets[[0 2050 8 3760 33 25 3207 7 7278 1609 198 1 887 31 454 15 2 2107 292 6765][974 280 218 45 603 985 4 60 288 55 0 129 146 1247 7 60 2 99 0 2278][158 119 5 3 3 18 1989 9 12 3...]...]\n",
            "global_step: 1557\n",
            " perplexity: 668.399\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[0 2050 8 3760 33 25 3207 7 7278 1609 198 1 887 31 454 15 2 2107 292 6765][974 280 218 45 603 985 4 60 288 55 0 129 146 1247 7 60 2 99 0 2278][158 119 5 3 3 18 1989 9 12 3...]...]\n",
            " percent of 3-grams captured: 0.206.\n",
            "targets[[0 2050 8 3760 33 25 3207 7 7278 1609 198 1 887 31 454 15 2 2107 292 6765][974 280 218 45 603 985 4 60 288 55 0 129 146 1247 7 60 2 99 0 2278][158 119 5 3 3 18 1989 9 12 3...]...]\n",
            " percent of 2-grams captured: 0.455.\n",
            "targets[[0 2050 8 3760 33 25 3207 7 7278 1609 198 1 887 31 454 15 2 2107 292 6765][974 280 218 45 603 985 4 60 288 55 0 129 146 1247 7 60 2 99 0 2278][158 119 5 3 3 18 1989 9 12 3...]...]\n",
            " percent of 4-grams captured: 0.109.\n",
            " geometric_avg: 0.217.\n",
            " arithmetic_avg: 0.257.\n",
            "global_step: 1557\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69649\n",
            " G train loss: 3.28685\n",
            "targets[[0 2050 8 3760 33 25 3207 7 7278 1609 198 1 887 31 454 15 2 2107 292 6765][974 280 218 45 603 985 4 60 288 55 0 129 146 1247 7 60 2 99 0 2278][158 119 5 3 3 18 1989 9 12 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[20 0 2050 8 3760 33 25 3207 7 7278...]...][[1 0 0 0 1 0 0 0 1 0...]...][[20 0 10000 10000 10000 33 10000 10000 10000 7278...]...][[0 2050 8 3760 33 25 3207 7 7278 1609...]...]\n",
            " Sample 0.\n",
            "   [1]  the                   0.391   0.000  \n",
            "   [0]  ban                   0.388   8.644  \n",
            "   [0]  and                   0.389   3.511  \n",
            "   [0]  smoking               0.391   9.542  \n",
            "   [1]  will                  0.390   0.000  \n",
            "   [0]  be                    0.388   4.242  \n",
            "   [0]  permitted             0.385   9.550  \n",
            "   [0]  in                    0.382   3.730  \n",
            "   [1]  bars                  0.382   0.000  \n",
            "   [0]  except                0.382   8.739  \n",
            "   [1]  during                0.384   0.000  \n",
            "   [1]  <unk>                 0.386   0.000  \n",
            "   [0]  hours                 0.386   8.199  \n",
            "   [0]  an                    0.384   6.794  \n",
            "   [0]  official              0.383   7.591  \n",
            "   [0]  said                  0.382   4.589  \n",
            "   [0]  <eos>                 0.381   3.732  \n",
            "   [0]  singapore             0.383   9.148  \n",
            "   [1]  already               0.387   0.000  \n",
            "   [0]  bans                  0.389   11.015 \n",
            " Sample 1.\n",
            "   [0]  stock-index           0.486   8.138  \n",
            "   [0]  traders               0.485   7.372  \n",
            "   [0]  trade                 0.482   8.291  \n",
            "   [1]  more                  0.481   0.000  \n",
            "   [0]  dollars               0.480   8.111  \n",
            "   [0]  worth                 0.482   8.335  \n",
            "   [0]  of                    0.482   3.733  \n",
            "   [0]  stock                 0.483   7.424  \n",
            "   [1]  futures               0.484   0.000  \n",
            "   [1]  than                  0.484   0.000  \n",
            "   [1]  the                   0.483   0.000  \n",
            "   [1]  big                   0.482   0.000  \n",
            "   [0]  board                 0.483   4.885  \n",
            "   [1]  trades                0.484   0.000  \n",
            "   [1]  in                    0.485   0.000  \n",
            "   [0]  stock                 0.486   8.000  \n",
            "   [1]  <eos>                 0.486   0.000  \n",
            "   [1]  now                   0.488   0.000  \n",
            "   [1]  the                   0.488   0.000  \n",
            "   [0]  stage                 0.487   9.629  \n",
            " Sample 2.\n",
            "   [0]  rate                  0.453   7.104  \n",
            "   [1]  rose                  0.448   0.000  \n",
            "   [1]  to                    0.441   0.000  \n",
            "   [0]  N                     0.434   1.709  \n",
            "   [0]  N                     0.427   1.150  \n",
            "   [1]  at                    0.424   0.000  \n",
            "   [0]  citicorp              0.426   13.652 \n",
            "   [0]  's                    0.429   4.217  \n",
            "   [0]  $                     0.430   2.590  \n",
            "   [0]  N                     0.429   0.011  \n",
            "   [0]  million               0.428   0.703  \n",
            "   [1]  weekly                0.429   0.000  \n",
            "   [1]  auction               0.430   0.000  \n",
            "   [0]  of                    0.431   3.265  \n",
            "   [1]  <unk>                 0.434   0.000  \n",
            "   [1]  commercial            0.437   0.000  \n",
            "   [1]  paper                 0.438   0.000  \n",
            "   [0]  or                    0.438   6.179  \n",
            "   [0]  corporate             0.436   8.091  \n",
            "   [0]  <unk>                 0.435   3.812  \n",
            "Samples\n",
            "Sample 0 .  the ban and smoking will be permitted in bars except during <unk> hours an official said <eos> singapore already bans\n",
            "Sample 1 .  stock-index traders trade more dollars worth of stock futures than the big board trades in stock <eos> now the stage\n",
            "Sample 2 .  rate rose to N N at citicorp 's $ N million weekly auction of <unk> commercial paper or corporate <unk>\n",
            "\n",
            "\n",
            "targets[[7 0 956 1044 9 284 1613 6 1 123 18 0 1 758 4 503 952 8193 8214 8][280 134 338 5 1286 345 3866 13 105 591 10 38 562 32 25 1140 5 34 51 229][20 3 3 5 3 3 2 1989 59 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1044 7 0 956 1044 9 284 1613 6 1...]...][[0 1 1 0 0 1 1 1 0 0...]...][[1044 10000 0 956 10000 10000 284 1613 6 10000...]...][[7 0 956 1044 9 284 1613 6 1 123...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1044 7 0 956 1044 9 284 1613 6 1...]...][[0 1 1 0 0 1 1 1 0 0...]...][[1044 10000 0 956 10000 10000 284 1613 6 10000...]...][[7 0 956 1044 9 284 1613 6 1 123...]...]\n",
            "I0212 04:17:01.822948 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[3760 7 73 7763 5927 273 6879 3385 8 2534 2750 2 7 3394 1 1 1 6 1783 1044][13 388 11 0 1115 5 1119 84 2 0 1 26 633 57 6940 1 20 286 2 156][20 3 3 18 69 123 9 226 2 1392...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6765 3760 7 73 7763 5927 273 6879 3385 8...]...][[1 0 0 0 0 1 0 1 1 0...]...][[6765 3760 10000 10000 10000 10000 273 10000 3385 8...]...][[3760 7 73 7763 5927 273 6879 3385 8 2534...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6765 3760 7 73 7763 5927 273 6879 3385 8...]...][[1 0 0 0 0 1 0 1 1 0...]...][[6765 3760 10000 10000 10000 10000 273 10000 3385 8...]...][[3760 7 73 7763 5927 273 6879 3385 8 2534...]...]\n",
            "targets[[3760 7 73 7763 5927 273 6879 3385 8 2534 2750 2 7 3394 1 1 1 6 1783 1044][13 388 11 0 1115 5 1119 84 2 0 1 26 633 57 6940 1 20 286 2 156][20 3 3 18 69 123 9 226 2 1392...]...]\n",
            "targets[[3760 7 73 7763 5927 273 6879 3385 8 2534 2750 2 7 3394 1 1 1 6 1783 1044][13 388 11 0 1115 5 1119 84 2 0 1 26 633 57 6940 1 20 286 2 156][20 3 3 18 69 123 9 226 2 1392...]...]\n",
            "targets[[3760 7 73 7763 5927 273 6879 3385 8 2534 2750 2 7 3394 1 1 1 6 1783 1044][13 388 11 0 1115 5 1119 84 2 0 1 26 633 57 6940 1 20 286 2 156][20 3 3 18 69 123 9 226 2 1392...]...]\n",
            "global_step: 1560\n",
            " perplexity: 667.818\n",
            " gen_learning_rate: 0.000749\n",
            "targets[[3760 7 73 7763 5927 273 6879 3385 8 2534 2750 2 7 3394 1 1 1 6 1783 1044][13 388 11 0 1115 5 1119 84 2 0 1 26 633 57 6940 1 20 286 2 156][20 3 3 18 69 123 9 226 2 1392...]...]\n",
            " percent of 3-grams captured: 0.169.\n",
            "targets[[3760 7 73 7763 5927 273 6879 3385 8 2534 2750 2 7 3394 1 1 1 6 1783 1044][13 388 11 0 1115 5 1119 84 2 0 1 26 633 57 6940 1 20 286 2 156][20 3 3 18 69 123 9 226 2 1392...]...]\n",
            " percent of 2-grams captured: 0.450.\n",
            "targets[[3760 7 73 7763 5927 273 6879 3385 8 2534 2750 2 7 3394 1 1 1 6 1783 1044][13 388 11 0 1115 5 1119 84 2 0 1 26 633 57 6940 1 20 286 2 156][20 3 3 18 69 123 9 226 2 1392...]...]\n",
            " percent of 4-grams captured: 0.091.\n",
            " geometric_avg: 0.191.\n",
            " arithmetic_avg: 0.237.\n",
            "global_step: 1560\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69649\n",
            " G train loss: 3.28676\n",
            "targets[[3760 7 73 7763 5927 273 6879 3385 8 2534 2750 2 7 3394 1 1 1 6 1783 1044][13 388 11 0 1115 5 1119 84 2 0 1 26 633 57 6940 1 20 286 2 156][20 3 3 18 69 123 9 226 2 1392...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6765 3760 7 73 7763 5927 273 6879 3385 8...]...][[1 0 0 0 0 1 0 1 1 0...]...][[6765 3760 10000 10000 10000 10000 273 10000 3385 8...]...][[3760 7 73 7763 5927 273 6879 3385 8 2534...]...]\n",
            " Sample 0.\n",
            "   [1]  smoking               0.489   0.000  \n",
            "   [0]  in                    0.488   3.452  \n",
            "   [0]  all                   0.490   5.940  \n",
            "   [0]  theaters              0.492   9.169  \n",
            "   [0]  buses                 0.492   11.917 \n",
            "   [1]  public                0.490   0.000  \n",
            "   [0]  elevators             0.489   12.272 \n",
            "   [1]  hospitals             0.489   0.000  \n",
            "   [1]  and                   0.488   0.000  \n",
            "   [0]  fast-food             0.486   10.404 \n",
            "   [1]  restaurants           0.483   0.000  \n",
            "   [1]  <eos>                 0.478   0.000  \n",
            "   [1]  in                    0.472   0.000  \n",
            "   [1]  malaysia              0.470   0.000  \n",
            "   [0]  <unk>                 0.471   3.310  \n",
            "   [0]  <unk>                 0.472   2.902  \n",
            "   [0]  <unk>                 0.473   2.853  \n",
            "   [1]  a                     0.472   0.000  \n",
            "   [0]  deputy                0.472   8.762  \n",
            "   [1]  minister              0.470   0.000  \n",
            " Sample 1.\n",
            "   [1]  is                    0.513   0.000  \n",
            "   [1]  set                   0.513   0.000  \n",
            "   [1]  for                   0.514   0.000  \n",
            "   [0]  the                   0.516   1.776  \n",
            "   [0]  battle                0.517   8.507  \n",
            "   [1]  to                    0.517   0.000  \n",
            "   [1]  play                  0.518   0.000  \n",
            "   [1]  out                   0.518   0.000  \n",
            "   [0]  <eos>                 0.517   2.803  \n",
            "   [0]  the                   0.518   1.587  \n",
            "   [0]  <unk>                 0.522   2.311  \n",
            "   [0]  are                   0.526   4.608  \n",
            "   [1]  getting               0.531   0.000  \n",
            "   [1]  some                  0.536   0.000  \n",
            "   [0]  helpful               0.537   12.582 \n",
            "   [0]  <unk>                 0.538   2.698  \n",
            "   [1]  from                  0.536   0.000  \n",
            "   [0]  congress              0.535   6.832  \n",
            "   [1]  <eos>                 0.530   0.000  \n",
            "   [1]  program               0.525   0.000  \n",
            " Sample 2.\n",
            "   [1]  from                  0.470   0.000  \n",
            "   [0]  N                     0.471   1.925  \n",
            "   [0]  N                     0.471   1.401  \n",
            "   [0]  at                    0.473   5.241  \n",
            "   [0]  last                  0.477   5.350  \n",
            "   [1]  week                  0.478   0.000  \n",
            "   [0]  's                    0.481   4.217  \n",
            "   [1]  sale                  0.485   0.000  \n",
            "   [1]  <eos>                 0.488   0.000  \n",
            "   [0]  bids                  0.491   9.949  \n",
            "   [1]  totaling              0.492   0.000  \n",
            "   [0]  $                     0.489   4.670  \n",
            "   [0]  N                     0.485   0.004  \n",
            "   [0]  million               0.482   0.559  \n",
            "   [1]  were                  0.481   0.000  \n",
            "   [1]  submitted             0.483   0.000  \n",
            "   [1]  <eos>                 0.484   0.000  \n",
            "   [1]  accepted              0.487   0.000  \n",
            "   [1]  bids                  0.488   0.000  \n",
            "   [1]  ranged                0.489   0.000  \n",
            "Samples\n",
            "Sample 0 .  smoking in all theaters buses public elevators hospitals and fast-food restaurants <eos> in malaysia <unk> <unk> <unk> a deputy minister\n",
            "Sample 1 .  is set for the battle to play out <eos> the <unk> are getting some helpful <unk> from congress <eos> program\n",
            "Sample 2 .  from N N at last week 's sale <eos> bids totaling $ N million were submitted <eos> accepted bids ranged\n",
            "\n",
            "\n",
            "targets[[2244 61 1647 5 2050 1 3760 2 435 1591 30 769 827 2 7 3757 160 542 7220 43][44 823 2085 7010 6 2547 2845 2 64 34 5 34 6 274 10 44 5 150 413 116][12 3 21 960 4 1 476 398 20 3...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 2244 61 1647 5 2050 1 3760 2 435...]...][[0 0 1 1 1 1 0 0 1 0...]...][[8 10000 10000 1647 5 2050 1 10000 10000 435...]...][[2244 61 1647 5 2050 1 3760 2 435 1591...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[8 2244 61 1647 5 2050 1 3760 2 435...]...][[0 0 1 1 1 1 0 0 1 0...]...][[8 10000 10000 1647 5 2050 1 10000 10000 435...]...][[2244 61 1647 5 2050 1 3760 2 435 1591...]...]\n",
            "I0212 04:17:05.101608 139787361208192 train_mask_gan.py:690] Generator is stateful.\n",
            "Generator is stateful.\n",
            "targets[[7 0 956 1044 9 284 1613 6 1 123 18 0 1 758 4 503 952 8193 8214 8][280 134 338 5 1286 345 3866 13 105 591 10 38 562 32 25 1140 5 34 51 229][20 3 3 5 3 3 2 1989 59 15...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1044 7 0 956 1044 9 284 1613 6 1...]...][[0 1 1 1 1 0 1 0 1 0...]...][[1044 10000 0 956 1044 9 10000 1613 10000 1...]...][[7 0 956 1044 9 284 1613 6 1 123...]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1044 7 0 956 1044 9 284 1613 6 1...]...][[0 1 1 1 1 0 1 0 1 0...]...][[1044 10000 0 956 1044 9 10000 1613 10000 1...]...][[7 0 956 1044 9 284 1613 6 1 123...]...]\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dFD9rWo42ezv",
        "colab_type": "code",
        "outputId": "780479a1-06f9-4007-cbfc-5ee8e37a1e7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "%cp /content/maskgan/maskGAN/train /content/maskgan/maskGAN_mle/train # make copy of mle training checkpoints"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "'/content/maskgan/maskGAN/maskGAN_mle/train' -> '/content/maskgan/maskGAN/train'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/.ipynb_checkpoints' -> '/content/maskgan/maskGAN/train/.ipynb_checkpoints'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/graph.pbtxt' -> '/content/maskgan/maskGAN/train/graph.pbtxt'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/train' -> '/content/maskgan/maskGAN/train/train'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/train/.ipynb_checkpoints' -> '/content/maskgan/maskGAN/train/train/.ipynb_checkpoints'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/train/graph.pbtxt' -> '/content/maskgan/maskGAN/train/train/graph.pbtxt'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-759.meta' -> '/content/maskgan/maskGAN/train/model.ckpt-759.meta'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-810.meta' -> '/content/maskgan/maskGAN/train/model.ckpt-810.meta'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-858.meta' -> '/content/maskgan/maskGAN/train/model.ckpt-858.meta'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.data-00000-of-00001' -> '/content/maskgan/maskGAN/train/model.ckpt-906.data-00000-of-00001'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.index' -> '/content/maskgan/maskGAN/train/model.ckpt-906.index'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.meta' -> '/content/maskgan/maskGAN/train/model.ckpt-906.meta'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-711.data-00000-of-00001' -> '/content/maskgan/maskGAN/train/model.ckpt-711.data-00000-of-00001'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-711.index' -> '/content/maskgan/maskGAN/train/model.ckpt-711.index'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-711.meta' -> '/content/maskgan/maskGAN/train/model.ckpt-711.meta'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-759.data-00000-of-00001' -> '/content/maskgan/maskGAN/train/model.ckpt-759.data-00000-of-00001'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-759.index' -> '/content/maskgan/maskGAN/train/model.ckpt-759.index'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-810.data-00000-of-00001' -> '/content/maskgan/maskGAN/train/model.ckpt-810.data-00000-of-00001'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-810.index' -> '/content/maskgan/maskGAN/train/model.ckpt-810.index'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/checkpoint' -> '/content/maskgan/maskGAN/train/checkpoint'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-858.data-00000-of-00001' -> '/content/maskgan/maskGAN/train/model.ckpt-858.data-00000-of-00001'\n",
            "'/content/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-858.index' -> '/content/maskgan/maskGAN/train/model.ckpt-858.index'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3pcQ4Gpl2wp",
        "colab_type": "text"
      },
      "source": [
        "## Run MaskGAN in GAN mode"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "33b9de14-fdd5-411f-edbb-f147cc070c14",
        "id": "dLyPlm0cGEZu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/maskgan\n",
        "!python train_mask_gan.py \\\n",
        " --data_dir='dataset/iccv2017' \\\n",
        " --batch_size=128 \\\n",
        " --sequence_length=20 \\\n",
        " --base_directory='/content/maskgan/maskGAN' \\\n",
        " --mask_strategy=contiguous \\\n",
        " --maskgan_ckpt='/content/maskgan/maskGAN/train/model.ckpt-1542'\\\n",
        " --hparams=\"gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,dis_num_layers=2,gen_learning_rate=0.000038877,gen_learning_rate_decay=1.0,gen_full_learning_rate_steps=2000000,gen_vd_keep_prob=0.33971,rl_discount_rate=0.89072,dis_learning_rate=5e-4,baseline_decay=0.99,dis_train_iterations=2,dis_pretrain_learning_rate=0.005,critic_learning_rate=5.1761e-7,dis_vd_keep_prob=0.71940\" \\\n",
        " --mode='TRAIN' \\\n",
        " --max_steps=100 \\\n",
        " --generator_model='seq2seq_vd' \\\n",
        " --discriminator_model='seq2seq_vd' \\\n",
        " --is_present_rate=0.5 \\\n",
        " --summaries_every=250 \\\n",
        " --print_every=250 \\\n",
        " --max_num_to_print=3 \\\n",
        " --gen_training_strategy='reinforce' \\\n",
        " --seq2seq_share_embedding=true \\\n",
        " --baseline_method=critic \\\n",
        " --attention_option=luong"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/maskgan\n",
            "<eos>: 2\n",
            "<eos>: 2\n",
            "Unique 2-grams: 38514\n",
            "Unique 3-grams: 61239\n",
            "Unique 4-grams: 69064\n",
            "Vocab size: 10000\n",
            "Training model.\n",
            "\n",
            "Optimizing Generator vars:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(10000, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(10000,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "\n",
            "Optimizing Discriminator vars:\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(10000, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Optimizing Critic vars:\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Trainable Variables in Graph:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(10000, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(10000,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/embedding:0' shape=(10000, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "WARNING:tensorflow:From train_mask_gan.py:546: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2020-02-12 16:05:52.159194: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-02-12 16:05:52.276623: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-12 16:05:52.277009: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \n",
            "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 7.43GiB freeMemory: 7.32GiB\n",
            "2020-02-12 16:05:52.277040: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n",
            "2020-02-12 16:05:52.655618: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7072 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9996 9997 9998 9999 2 9256 1 3 72 393]...][[1 1 1 1 1 1 1 0 0 0]...][[9996 9997 9998 9999 2 9256 1 3 10000 10000]...][[9997 9998 9999 2 9256 1 3 72 393 33]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9996 9997 9998 9999 2 9256 1 3 72 393]...][[1 1 1 1 1 1 1 0 0 0]...][[9996 9997 9998 9999 2 9256 1 3 10000 10000]...][[9997 9998 9999 2 9256 1 3 5 3 11]...]\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 23 1 13 141 4 1 5465 0 3081]...][[1 1 1 1 1 1 1 1 1 1]...][[2 23 1 13 141 4 1 5465 0 3081]...][[23 1 13 141 4 1 5465 0 3081 1596]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 23 1 13 141 4 1 5465 0 3081]...][[1 1 1 1 1 1 1 1 1 1]...][[2 23 1 13 141 4 1 5465 0 3081]...][[23 1 13 141 4 1 5465 0 3081 1596]...]\n",
            "Generator is stateful.\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][996 9 494 242 43 0 35 6932 293 1 1027 3181 6 1275 1061 18 1 265 1114 4][13 835 3324 23 1284 15 2 0 53 22]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9970 9971 9972 9974 9975 9976 9980 9981 9982 9983]...][[1 1 1 0 0 0 0 0 0 0]...][[9970 9971 9972 9974 10000 10000 10000 10000 10000 10000]...][[9971 9972 9974 26 0 686 3 3 3 1176]...]\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][996 9 494 242 43 0 35 6932 293 1 1027 3181 6 1275 1061 18 1 265 1114 4][13 835 3324 23 1284 15 2 0 53 22]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9970 9971 9972 9974 9975 9976 9980 9981 9982 9983]...][[1 1 1 0 0 0 0 0 0 0]...][[9970 9971 9972 9974 10000 10000 10000 10000 10000 10000]...][[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9970 9971 9972 9974 9975 9976 9980 9981 9982 9983]...][[1 1 1 0 0 0 0 0 0 0]...][[9970 9971 9972 9974 10000 10000 10000 10000 10000 10000]...][[9971 9972 9974 5 105 121 55 174 46 1497]...]\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][996 9 494 242 43 0 35 6932 293 1 1027 3181 6 1275 1061 18 1 265 1114 4][13 835 3324 23 1284 15 2 0 53 22]...]\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][996 9 494 242 43 0 35 6932 293 1 1027 3181 6 1275 1061 18 1 265 1114 4][13 835 3324 23 1284 15 2 0 53 22]...]\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][996 9 494 242 43 0 35 6932 293 1 1027 3181 6 1275 1061 18 1 265 1114 4][13 835 3324 23 1284 15 2 0 53 22]...]\n",
            "global_step: 1547\n",
            " perplexity: 539.850\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][996 9 494 242 43 0 35 6932 293 1 1027 3181 6 1275 1061 18 1 265 1114 4][13 835 3324 23 1284 15 2 0 53 22]...]\n",
            " percent of 3-grams captured: 0.092.\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][996 9 494 242 43 0 35 6932 293 1 1027 3181 6 1275 1061 18 1 265 1114 4][13 835 3324 23 1284 15 2 0 53 22]...]\n",
            " percent of 2-grams captured: 0.271.\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][996 9 494 242 43 0 35 6932 293 1 1027 3181 6 1275 1061 18 1 265 1114 4][13 835 3324 23 1284 15 2 0 53 22]...]\n",
            " percent of 4-grams captured: 0.033.\n",
            " geometric_avg: 0.093.\n",
            " arithmetic_avg: 0.132.\n",
            "global_step: 1547\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.68863\n",
            " G train loss: 13.79052\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][996 9 494 242 43 0 35 6932 293 1 1027 3181 6 1275 1061 18 1 265 1114 4][13 835 3324 23 1284 15 2 0 53 22]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9970 9971 9972 9974 9975 9976 9980 9981 9982 9983]...][[1 1 1 0 0 0 0 0 0 0]...][[9970 9971 9972 9974 10000 10000 10000 10000 10000 10000]...][[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9970 9971 9972 9974 9975 9976 9980 9981 9982 9983]...][[1 1 1 0 0 0 0 0 0 0]...][[9970 9971 9972 9974 10000 10000 10000 10000 10000 10000]...][[9971 9972 9974 15 82 158 3 3 3 7]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  banknote            0.521        0.000        0.000        0.000        -2.744       -1.745       -0.000       \n",
            "   [1]  berlitz             0.519        0.000        0.000        0.000        -3.081       -2.187       -0.000       \n",
            "   [1]  calloway            0.519        0.000        0.000        0.000        -3.459       -2.448       -0.000       \n",
            "   [0]  said                0.519        13.496       -4.688       -0.656       -3.883       -2.527       -1.356       \n",
            "   [0]  &                   0.518        13.054       -6.771       -0.658       -3.623       -2.462       -1.161       \n",
            "   [0]  rate                0.519        11.677       -7.736       -0.655       -3.329       -2.355       -0.974       \n",
            "   [0]  N                   0.518        13.180       -2.227       -0.657       -3.001       -2.229       -0.772       \n",
            "   [0]  N                   0.518        12.587       -1.489       -0.659       -2.632       -2.093       -0.539       \n",
            "   [0]  N                   0.518        12.760       -2.095       -0.658       -2.215       -1.904       -0.312       \n",
            "   [0]  in                  0.519        12.716       -3.379       -0.656       -1.748       -1.683       -0.065       \n",
            "   [0]  ual                 0.522        13.550       -6.892       -0.651       -1.226       -1.416       0.190        \n",
            "   [0]  government          0.524        11.745       -6.623       -0.646       -0.646       -1.309       0.663        \n",
            "   [1]  mlx                 0.525        0.000        0.000        0.000        0.000        -1.439       0.000        \n",
            "   [1]  nahb                0.526        0.000        0.000        0.000        0.000        -1.652       0.000        \n",
            "   [1]  punts               0.525        0.000        0.000        0.000        0.000        -1.906       0.000        \n",
            "   [1]  rake                0.524        0.000        0.000        0.000        0.000        -2.077       0.000        \n",
            "   [1]  regatta             0.523        0.000        0.000        0.000        0.000        -2.200       0.000        \n",
            "   [1]  rubens              0.522        0.000        0.000        0.000        0.000        -2.223       0.000        \n",
            "   [1]  sim                 0.520        0.000        0.000        0.000        0.000        -2.202       0.000        \n",
            "   [1]  snack-food          0.520        0.000        0.000        0.000        0.000        -2.132       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  anything            0.487        0.000        0.000        0.000        -1.521       -1.671       0.000        \n",
            "   [1]  's                  0.481        0.000        0.000        0.000        -1.708       -2.034       0.000        \n",
            "   [1]  possible            0.478        0.000        0.000        0.000        -1.918       -2.268       0.000        \n",
            "   [1]  how                 0.477        0.000        0.000        0.000        -2.153       -2.460       0.000        \n",
            "   [1]  about               0.476        0.000        0.000        0.000        -2.417       -2.551       0.000        \n",
            "   [1]  the                 0.475        0.000        0.000        0.000        -2.713       -2.554       -0.000       \n",
            "   [1]  new                 0.475        0.000        0.000        0.000        -3.046       -2.459       -0.000       \n",
            "   [1]  guinea              0.476        0.000        0.000        0.000        -3.420       -2.376       -0.000       \n",
            "   [1]  fund                0.476        0.000        0.000        0.000        -3.840       -2.342       -0.000       \n",
            "   [0]  average             0.477        4.324        -8.051       -0.741       -4.311       -2.325       -1.986       \n",
            "   [0]  a                   0.480        9.781        -4.115       -0.734       -4.008       -2.222       -1.786       \n",
            "   [0]  holders             0.485        8.767        -8.654       -0.724       -3.676       -2.225       -1.451       \n",
            "   [0]  dropped             0.488        5.041        -7.794       -0.718       -3.314       -2.364       -0.950       \n",
            "   [0]  him                 0.488        10.103       -7.404       -0.717       -2.915       -2.586       -0.329       \n",
            "   [0]  with                0.486        10.185       -4.566       -0.721       -2.468       -2.773       0.305        \n",
            "   [0]  its                 0.484        5.732        -4.214       -0.726       -1.961       -2.787       0.827        \n",
            "   [0]  hartford            0.481        3.883        -9.779       -0.731       -1.386       -2.599       1.213        \n",
            "   [0]  approval            0.480        7.513        -8.213       -0.735       -0.735       -2.291       1.556        \n",
            "   [1]  associates          0.476        0.000        0.000        0.000        0.000        -2.019       0.000        \n",
            "   [1]  of                  0.473        0.000        0.000        0.000        0.000        -1.907       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  is                  0.470        0.000        0.000        0.000        -2.682       -1.697       -0.000       \n",
            "   [1]  taking              0.470        0.000        0.000        0.000        -3.011       -2.147       -0.000       \n",
            "   [1]  shape               0.470        0.000        0.000        0.000        -3.380       -2.442       -0.000       \n",
            "   [1]  mr.                 0.470        0.000        0.000        0.000        -3.795       -2.428       -0.000       \n",
            "   [0]  scheduled           0.471        10.141       -9.117       -0.753       -4.261       -2.342       -1.918       \n",
            "   [0]  this                0.475        4.642        -6.728       -0.744       -3.938       -2.312       -1.626       \n",
            "   [0]  falconbridge        0.481        7.041        -9.672       -0.732       -3.586       -2.119       -1.467       \n",
            "   [0]  to                  0.488        4.079        -2.663       -0.718       -3.204       -2.006       -1.198       \n",
            "   [0]  next                0.494        6.126        -5.708       -0.706       -2.791       -1.978       -0.813       \n",
            "   [0]  last                0.499        6.203        -5.510       -0.695       -2.341       -1.981       -0.360       \n",
            "   [0]  then                0.503        5.199        -7.580       -0.687       -1.849       -2.115       0.267        \n",
            "   [0]  rates               0.504        8.967        -6.549       -0.685       -1.305       -2.258       0.954        \n",
            "   [0]  activities          0.499        11.614       -10.243      -0.696       -0.696       -2.312       1.616        \n",
            "   [1]  must                0.491        0.000        0.000        0.000        0.000        -2.298       0.000        \n",
            "   [1]  play                0.486        0.000        0.000        0.000        0.000        -2.301       0.000        \n",
            "   [1]  a                   0.486        0.000        0.000        0.000        0.000        -2.195       0.000        \n",
            "   [1]  crucial             0.489        0.000        0.000        0.000        0.000        -2.064       0.000        \n",
            "   [1]  role                0.492        0.000        0.000        0.000        0.000        -1.965       0.000        \n",
            "   [1]  in                  0.496        0.000        0.000        0.000        0.000        -1.833       0.000        \n",
            "   [1]  designing           0.499        0.000        0.000        0.000        0.000        -1.758       0.000        \n",
            "Samples\n",
            "Sample 0 .  banknote berlitz calloway said & rate N N N in ual government mlx nahb punts rake regatta rubens sim snack-food\n",
            "Sample 1 .  anything 's possible how about the new guinea fund average a holders dropped him with its hartford approval associates of\n",
            "Sample 2 .  is taking shape mr. scheduled this falconbridge to next last then rates activities must play a crucial role in designing\n",
            "\n",
            "\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[141 4 2477 657 2170 955 24 521 6 9207]...][[1 1 1 1 1 1 1 1 1 1]...][[141 4 2477 657 2170 955 24 521 6 9207]...][[4 2477 657 2170 955 24 521 6 9207 276]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[141 4 2477 657 2170 955 24 521 6 9207]...][[1 1 1 1 1 1 1 1 1 1]...][[141 4 2477 657 2170 955 24 521 6 9207]...][[4 2477 657 2170 955 24 521 6 9207 276]...]\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[3150 496 263 5 138 6092 4241 6036 30 988]...][[1 1 1 1 1 1 1 0 0 0]...][[3150 496 263 5 138 6092 4241 6036 10000 10000]...][[496 263 5 138 6092 4241 6036 30 988 6]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[3150 496 263 5 138 6092 4241 6036 30 988]...][[1 1 1 1 1 1 1 0 0 0]...][[3150 496 263 5 138 6092 4241 6036 10000 10000]...][[496 263 5 138 6092 4241 6036 34 268 1]...]\n",
            "Generator is stateful.\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9996 9997 9998 9999 2 9256 1 3 72 393]...][[1 1 1 1 1 1 1 1 1 1]...][[9996 9997 9998 9999 2 9256 1 3 72 393]...][[9997 9998 9999 2 9256 1 3 72 393 33]...]\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9996 9997 9998 9999 2 9256 1 3 72 393]...][[1 1 1 1 1 1 1 1 1 1]...][[9996 9997 9998 9999 2 9256 1 3 72 393]...][[9997 9998 9999 2 9256 1 3 72 393 33]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9996 9997 9998 9999 2 9256 1 3 72 393]...][[1 1 1 1 1 1 1 1 1 1]...][[9996 9997 9998 9999 2 9256 1 3 72 393]...][[9997 9998 9999 2 9256 1 3 72 393 33]...]\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            "global_step: 1552\n",
            " perplexity: 553.562\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            " percent of 3-grams captured: 0.097.\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            " percent of 2-grams captured: 0.288.\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            " percent of 4-grams captured: 0.034.\n",
            " geometric_avg: 0.098.\n",
            " arithmetic_avg: 0.140.\n",
            "global_step: 1552\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69132\n",
            " G train loss: 15.59337\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][1 2676 2 0 181 6027 4 357 193 1 0 5949 293 9135 4 0 7825 23 3181 44][27 4616 2 29 4317 53 1909 33 25 1018]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9996 9997 9998 9999 2 9256 1 3 72 393]...][[1 1 1 1 1 1 1 1 1 1]...][[9996 9997 9998 9999 2 9256 1 3 72 393]...][[9997 9998 9999 2 9256 1 3 72 393 33]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9996 9997 9998 9999 2 9256 1 3 72 393]...][[1 1 1 1 1 1 1 1 1 1]...][[9996 9997 9998 9999 2 9256 1 3 72 393]...][[9997 9998 9999 2 9256 1 3 72 393 33]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  ssangyong           0.527        0.000        0.000        0.000        -1.199       -1.653       0.000        \n",
            "   [1]  swapo               0.525        0.000        0.000        0.000        -1.346       -2.068       0.000        \n",
            "   [1]  wachter             0.526        0.000        0.000        0.000        -1.511       -2.308       0.000        \n",
            "   [1]  <eos>               0.525        0.000        0.000        0.000        -1.696       -2.382       0.000        \n",
            "   [1]  pierre              0.526        0.000        0.000        0.000        -1.904       -2.315       0.000        \n",
            "   [1]  <unk>               0.528        0.000        0.000        0.000        -2.138       -2.217       0.000        \n",
            "   [1]  N                   0.528        0.000        0.000        0.000        -2.400       -2.051       -0.000       \n",
            "   [1]  years               0.526        0.000        0.000        0.000        -2.694       -1.952       -0.000       \n",
            "   [1]  old                 0.527        0.000        0.000        0.000        -3.025       -1.903       -0.000       \n",
            "   [1]  will                0.528        0.000        0.000        0.000        -3.396       -1.963       -0.000       \n",
            "   [0]  large               0.528        11.005       -6.710       -0.638       -3.813       -2.069       -1.744       \n",
            "   [0]  but                 0.527        5.411        -6.558       -0.640       -3.564       -2.339       -1.225       \n",
            "   [0]  served              0.527        7.763        -9.416       -0.641       -3.283       -2.498       -0.786       \n",
            "   [0]  to                  0.525        4.518        -1.972       -0.644       -2.966       -2.471       -0.496       \n",
            "   [0]  marks               0.525        4.190        -8.643       -0.644       -2.607       -2.331       -0.276       \n",
            "   [0]  the                 0.525        13.441       -2.350       -0.645       -2.204       -2.207       0.003        \n",
            "   [0]  public              0.523        8.424        -6.906       -0.648       -1.751       -2.215       0.464        \n",
            "   [0]  off                 0.520        8.422        -6.691       -0.653       -1.238       -2.300       1.062        \n",
            "   [0]  the                 0.519        4.886        -3.223       -0.656       -0.656       -2.444       1.788        \n",
            "   [1]  <eos>               0.515        0.000        0.000        0.000        0.000        -2.527       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  <unk>               0.539        0.000        0.000        0.000        -3.265       -1.693       -0.000       \n",
            "   [0]  with                0.539        10.526       -5.348       -0.618       -3.666       -2.063       -1.603       \n",
            "   [0]  second              0.539        5.552        -8.186       -0.617       -3.422       -2.321       -1.101       \n",
            "   [0]  ready               0.538        4.205        -8.378       -0.620       -3.149       -2.415       -0.733       \n",
            "   [0]  of                  0.536        9.234        -2.687       -0.624       -2.839       -2.337       -0.501       \n",
            "   [0]  talk                0.536        9.808        -8.452       -0.625       -2.486       -2.295       -0.191       \n",
            "   [0]  less                0.539        3.716        -7.339       -0.619       -2.090       -2.269       0.179        \n",
            "   [0]  backed              0.540        8.485        -9.135       -0.616       -1.652       -2.260       0.608        \n",
            "   [0]  western             0.540        7.208        -8.969       -0.616       -1.163       -2.303       1.141        \n",
            "   [0]  unload              0.541        2.956        -11.230      -0.614       -0.614       -2.331       1.717        \n",
            "   [1]  the                 0.540        0.000        0.000        0.000        0.000        -2.303       0.000        \n",
            "   [1]  closed-end          0.540        0.000        0.000        0.000        0.000        -2.272       0.000        \n",
            "   [1]  fund                0.541        0.000        0.000        0.000        0.000        -2.257       0.000        \n",
            "   [1]  mania               0.542        0.000        0.000        0.000        0.000        -2.306       0.000        \n",
            "   [1]  of                  0.543        0.000        0.000        0.000        0.000        -2.396       0.000        \n",
            "   [1]  the                 0.542        0.000        0.000        0.000        0.000        -2.521       0.000        \n",
            "   [1]  1920s               0.546        0.000        0.000        0.000        0.000        -2.626       0.000        \n",
            "   [1]  mr.                 0.547        0.000        0.000        0.000        0.000        -2.692       0.000        \n",
            "   [1]  foot                0.549        0.000        0.000        0.000        0.000        -2.804       0.000        \n",
            "   [1]  says                0.549        0.000        0.000        0.000        0.000        -2.784       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  its                 0.501        0.000        0.000        0.000        -2.805       -1.687       -0.000       \n",
            "   [1]  architecture        0.500        0.000        0.000        0.000        -3.150       -2.072       -0.000       \n",
            "   [1]  <eos>               0.499        0.000        0.000        0.000        -3.536       -2.280       -0.000       \n",
            "   [0]  neutrons            0.501        4.777        -10.396      -0.690       -3.970       -2.325       -1.644       \n",
            "   [0]  detail              0.505        9.401        -10.050      -0.683       -3.682       -2.250       -1.432       \n",
            "   [0]  surgery             0.509        8.449        -9.835       -0.676       -3.367       -2.134       -1.232       \n",
            "   [0]  they                0.513        9.149        -5.499       -0.667       -3.021       -2.099       -0.922       \n",
            "   [0]  heels               0.517        6.321        -10.535      -0.660       -2.643       -2.119       -0.525       \n",
            "   [0]  obtained            0.518        8.078        -10.056      -0.657       -2.227       -2.179       -0.047       \n",
            "   [0]  kong                0.518        9.294        -8.126       -0.657       -1.762       -2.196       0.434        \n",
            "   [0]  regatta             0.519        4.595        -10.307      -0.657       -1.240       -2.322       1.082        \n",
            "   [0]  indicates           0.519        5.241        -9.756       -0.655       -0.655       -2.383       1.727        \n",
            "   [1]  face                0.519        0.000        0.000        0.000        0.000        -2.372       0.000        \n",
            "   [1]  of                  0.519        0.000        0.000        0.000        0.000        -2.383       0.000        \n",
            "   [1]  japanese            0.520        0.000        0.000        0.000        0.000        -2.432       0.000        \n",
            "   [1]  dominance           0.520        0.000        0.000        0.000        0.000        -2.443       0.000        \n",
            "   [1]  in                  0.518        0.000        0.000        0.000        0.000        -2.406       0.000        \n",
            "   [1]  the                 0.518        0.000        0.000        0.000        0.000        -2.201       0.000        \n",
            "   [1]  region              0.519        0.000        0.000        0.000        0.000        -2.142       0.000        \n",
            "   [1]  <eos>               0.520        0.000        0.000        0.000        0.000        -2.168       0.000        \n",
            "Samples\n",
            "Sample 0 .  ssangyong swapo wachter <eos> pierre <unk> N years old will large but served to marks the public off the <eos>\n",
            "Sample 1 .  <unk> with second ready of talk less backed western unload the closed-end fund mania of the 1920s mr. foot says\n",
            "Sample 2 .  its architecture <eos> neutrons detail surgery they heels obtained kong regatta indicates face of japanese dominance in the region <eos>\n",
            "\n",
            "\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 4115 5 14 45 55 3 72 195 1244]...][[1 1 1 1 1 1 0 0 0 0]...][[431 4115 5 14 45 55 3 10000 10000 10000]...][[4115 5 14 45 55 3 72 195 1244 220]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 4115 5 14 45 55 3 72 195 1244]...][[1 1 1 1 1 1 0 0 0 0]...][[431 4115 5 14 45 55 3 10000 10000 10000]...][[4115 5 14 45 55 3 36 3366 2 6]...]\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[14 6885 0 1 22 113 2652 8068 5 14]...][[1 1 1 1 1 1 1 1 1 1]...][[14 6885 0 1 22 113 2652 8068 5 14]...][[6885 0 1 22 113 2652 8068 5 14 2474]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[14 6885 0 1 22 113 2652 8068 5 14]...][[1 1 1 1 1 1 1 1 1 1]...][[14 6885 0 1 22 113 2652 8068 5 14]...][[6885 0 1 22 113 2652 8068 5 14 2474]...]\n",
            "Generator is stateful.\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 23 1 13 141 4 1 5465 0 3081]...][[1 1 1 1 1 1 1 1 1 1]...][[2 23 1 13 141 4 1 5465 0 3081]...][[23 1 13 141 4 1 5465 0 3081 1596]...]\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 23 1 13 141 4 1 5465 0 3081]...][[1 1 1 1 1 1 1 1 1 1]...][[2 23 1 13 141 4 1 5465 0 3081]...][[23 1 13 141 4 1 5465 0 3081 1596]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 23 1 13 141 4 1 5465 0 3081]...][[1 1 1 1 1 1 1 1 1 1]...][[2 23 1 13 141 4 1 5465 0 3081]...][[23 1 13 141 4 1 5465 0 3081 1596]...]\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            "global_step: 1557\n",
            " perplexity: 527.207\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            " percent of 3-grams captured: 0.084.\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            " percent of 2-grams captured: 0.292.\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            " percent of 4-grams captured: 0.027.\n",
            " geometric_avg: 0.087.\n",
            " arithmetic_avg: 0.134.\n",
            "global_step: 1557\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69173\n",
            " G train loss: 18.50768\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][67 5466 3598 193 1569 6713 1434 2 38 202 91 1 78 0 3 1037 2 1725 1519 1][203 63 86 1 0 53 7 145 4704 29]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 23 1 13 141 4 1 5465 0 3081]...][[1 1 1 1 1 1 1 1 1 1]...][[2 23 1 13 141 4 1 5465 0 3081]...][[23 1 13 141 4 1 5465 0 3081 1596]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2 23 1 13 141 4 1 5465 0 3081]...][[1 1 1 1 1 1 1 1 1 1]...][[2 23 1 13 141 4 1 5465 0 3081]...][[23 1 13 141 4 1 5465 0 3081 1596]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  mr.                 0.504        0.000        0.000        0.000        -1.123       -1.699       0.000        \n",
            "   [1]  <unk>               0.508        0.000        0.000        0.000        -1.261       -2.114       0.000        \n",
            "   [1]  is                  0.514        0.000        0.000        0.000        -1.415       -2.227       0.000        \n",
            "   [1]  chairman            0.520        0.000        0.000        0.000        -1.589       -2.289       0.000        \n",
            "   [1]  of                  0.521        0.000        0.000        0.000        -1.784       -2.079       0.000        \n",
            "   [1]  <unk>               0.520        0.000        0.000        0.000        -2.003       -2.037       0.000        \n",
            "   [1]  n.v.                0.517        0.000        0.000        0.000        -2.249       -2.018       -0.000       \n",
            "   [1]  the                 0.515        0.000        0.000        0.000        -2.525       -2.180       -0.000       \n",
            "   [1]  dutch               0.512        0.000        0.000        0.000        -2.834       -2.359       -0.000       \n",
            "   [1]  publishing          0.510        0.000        0.000        0.000        -3.182       -2.452       -0.000       \n",
            "   [1]  group               0.509        0.000        0.000        0.000        -3.573       -2.666       -0.000       \n",
            "   [0]  loss                0.510        3.158        -8.321       -0.674       -4.011       -2.854       -1.157       \n",
            "   [0]  increase            0.511        11.699       -7.847       -0.672       -3.747       -3.009       -0.738       \n",
            "   [0]  million             0.510        5.574        -4.700       -0.673       -3.452       -3.015       -0.436       \n",
            "   [0]  <unk>               0.510        3.456        -4.635       -0.674       -3.120       -2.916       -0.204       \n",
            "   [0]  markets             0.507        6.233        -6.990       -0.679       -2.746       -2.723       -0.023       \n",
            "   [0]  fell                0.506        8.502        -6.178       -0.682       -2.320       -2.615       0.294        \n",
            "   [0]  N                   0.504        4.201        -1.486       -0.685       -1.839       -2.417       0.578        \n",
            "   [0]  N                   0.504        10.925       -1.644       -0.685       -1.296       -2.169       0.873        \n",
            "   [0]  issue               0.504        8.598        -8.362       -0.685       -0.685       -1.886       1.200        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  when                0.507        0.000        0.000        0.000        -1.424       -1.696       0.000        \n",
            "   [1]  narrowly            0.504        0.000        0.000        0.000        -1.599       -2.081       0.000        \n",
            "   [1]  focused             0.504        0.000        0.000        0.000        -1.795       -2.420       0.000        \n",
            "   [1]  funds               0.503        0.000        0.000        0.000        -2.015       -2.569       0.000        \n",
            "   [1]  grew                0.503        0.000        0.000        0.000        -2.262       -2.517       0.000        \n",
            "   [1]  wildly              0.506        0.000        0.000        0.000        -2.540       -2.534       -0.000       \n",
            "   [1]  popular             0.507        0.000        0.000        0.000        -2.851       -2.514       -0.000       \n",
            "   [1]  <eos>               0.507        0.000        0.000        0.000        -3.201       -2.547       -0.000       \n",
            "   [1]  they                0.507        0.000        0.000        0.000        -3.594       -2.595       -0.000       \n",
            "   [0]  u.s.                0.507        8.221        -6.264       -0.679       -4.035       -2.551       -1.483       \n",
            "   [0]  early               0.508        6.647        -7.627       -0.676       -3.768       -2.454       -1.314       \n",
            "   [0]  to                  0.507        2.952        -4.004       -0.679       -3.471       -2.405       -1.065       \n",
            "   [0]  local               0.505        8.298        -5.446       -0.682       -3.135       -2.355       -0.780       \n",
            "   [0]  european            0.503        4.580        -8.553       -0.688       -2.753       -2.391       -0.362       \n",
            "   [0]  plus                0.503        5.894        -9.408       -0.688       -2.319       -2.343       0.024        \n",
            "   [0]  to                  0.503        9.238        -3.007       -0.688       -1.831       -2.208       0.377        \n",
            "   [0]  active              0.506        8.748        -8.731       -0.682       -1.283       -2.006       0.723        \n",
            "   [0]  <unk>               0.509        9.132        -3.330       -0.676       -0.676       -1.812       1.136        \n",
            "   [1]  traditional         0.509        0.000        0.000        0.000        0.000        -1.709       0.000        \n",
            "   [1]  <unk>               0.507        0.000        0.000        0.000        0.000        -1.814       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  japan               0.514        0.000        0.000        0.000        -2.413       -1.683       -0.000       \n",
            "   [1]  not                 0.513        0.000        0.000        0.000        -2.709       -2.034       -0.000       \n",
            "   [1]  only                0.514        0.000        0.000        0.000        -3.042       -2.268       -0.000       \n",
            "   [1]  <unk>               0.517        0.000        0.000        0.000        -3.415       -2.280       -0.000       \n",
            "   [0]  on                  0.520        3.469        -5.003       -0.654       -3.834       -2.211       -1.623       \n",
            "   [0]  richard             0.523        7.154        -9.535       -0.648       -3.570       -2.183       -1.387       \n",
            "   [0]  series              0.524        5.161        -7.549       -0.647       -3.281       -2.203       -1.077       \n",
            "   [0]  holders             0.524        8.049        -8.376       -0.645       -2.957       -2.250       -0.707       \n",
            "   [0]  and                 0.524        12.503       -3.636       -0.645       -2.595       -2.205       -0.390       \n",
            "   [0]  my                  0.524        4.617        -7.399       -0.646       -2.189       -2.111       -0.079       \n",
            "   [0]  contract            0.524        6.608        -6.411       -0.646       -1.733       -2.052       0.319        \n",
            "   [0]  has                 0.524        2.928        -4.629       -0.646       -1.220       -2.086       0.865        \n",
            "   [0]  in                  0.525        5.065        -4.495       -0.645       -0.645       -2.019       1.374        \n",
            "   [1]  in                  0.525        0.000        0.000        0.000        0.000        -1.925       0.000        \n",
            "   [1]  trade               0.526        0.000        0.000        0.000        0.000        -1.855       0.000        \n",
            "   [1]  with                0.524        0.000        0.000        0.000        0.000        -1.894       0.000        \n",
            "   [1]  most                0.522        0.000        0.000        0.000        0.000        -2.068       0.000        \n",
            "   [1]  southeast           0.520        0.000        0.000        0.000        0.000        -2.172       0.000        \n",
            "   [1]  asian               0.519        0.000        0.000        0.000        0.000        -2.235       0.000        \n",
            "   [1]  countries           0.518        0.000        0.000        0.000        0.000        -2.257       0.000        \n",
            "Samples\n",
            "Sample 0 .  mr. <unk> is chairman of <unk> n.v. the dutch publishing group loss increase million <unk> markets fell N N issue\n",
            "Sample 1 .  when narrowly focused funds grew wildly popular <eos> they u.s. early to local european plus to active <unk> traditional <unk>\n",
            "Sample 2 .  japan not only <unk> on richard series holders and my contract has in in trade with most southeast asian countries\n",
            "\n",
            "\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 80 0 167 4 35 2645 1 65 10]...][[1 1 1 1 1 1 1 1 1 1]...][[1 80 0 167 4 35 2645 1 65 10]...][[80 0 167 4 35 2645 1 65 10 558]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 80 0 167 4 35 2645 1 65 10]...][[1 1 1 1 1 1 1 1 1 1]...][[1 80 0 167 4 35 2645 1 65 10]...][[80 0 167 4 35 2645 1 65 10 558]...]\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6036 7 3 2 366 1976 3178 46 220 45]...][[1 1 1 1 1 1 0 0 0 0]...][[6036 7 3 2 366 1976 3178 10000 10000 10000]...][[7 3 2 366 1976 3178 46 220 45 55]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6036 7 3 2 366 1976 3178 46 220 45]...][[1 1 1 1 1 1 0 0 0 0]...][[6036 7 3 2 366 1976 3178 10000 10000 10000]...][[7 3 2 366 1976 3178 5 0 37 7]...]\n",
            "Generator is stateful.\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[141 4 2477 657 2170 955 24 521 6 9207]...][[1 1 1 1 1 1 1 1 0 0]...][[141 4 2477 657 2170 955 24 521 6 10000]...][[4 2477 657 2170 955 24 521 6 12 107]...]\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[141 4 2477 657 2170 955 24 521 6 9207]...][[1 1 1 1 1 1 1 1 0 0]...][[141 4 2477 657 2170 955 24 521 6 10000]...][[4 2477 657 2170 955 24 521 6 9207 276]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[141 4 2477 657 2170 955 24 521 6 9207]...][[1 1 1 1 1 1 1 1 0 0]...][[141 4 2477 657 2170 955 24 521 6 10000]...][[4 2477 657 2170 955 24 521 6 1 155]...]\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            "global_step: 1562\n",
            " perplexity: 519.322\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            " percent of 3-grams captured: 0.099.\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            " percent of 2-grams captured: 0.292.\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            " percent of 4-grams captured: 0.022.\n",
            " geometric_avg: 0.086.\n",
            " arithmetic_avg: 0.138.\n",
            "global_step: 1562\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69242\n",
            " G train loss: 19.20710\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1359 193 90 4 144 1 2560 26 0 5949 2155 5160 6 1425 297 4 71 10 218 1386][366 0 53 783 0 810 218 1061 11 73]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[141 4 2477 657 2170 955 24 521 6 9207]...][[1 1 1 1 1 1 1 1 0 0]...][[141 4 2477 657 2170 955 24 521 6 10000]...][[4 2477 657 2170 955 24 521 6 9207 276]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[141 4 2477 657 2170 955 24 521 6 9207]...][[1 1 1 1 1 1 1 1 0 0]...][[141 4 2477 657 2170 955 24 521 6 10000]...][[4 2477 657 2170 955 24 521 6 1551 3574]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  of                  0.513        0.000        0.000        0.000        -1.592       -1.707       0.000        \n",
            "   [1]  consolidated        0.515        0.000        0.000        0.000        -1.788       -2.075       0.000        \n",
            "   [1]  gold                0.515        0.000        0.000        0.000        -2.007       -2.118       0.000        \n",
            "   [1]  fields              0.512        0.000        0.000        0.000        -2.253       -2.163       -0.000       \n",
            "   [1]  plc                 0.508        0.000        0.000        0.000        -2.530       -2.205       -0.000       \n",
            "   [1]  was                 0.504        0.000        0.000        0.000        -2.840       -2.158       -0.000       \n",
            "   [1]  named               0.501        0.000        0.000        0.000        -3.189       -2.159       -0.000       \n",
            "   [1]  a                   0.503        0.000        0.000        0.000        -3.580       -2.171       -0.000       \n",
            "   [0]  global              0.505        10.208       -8.951       -0.683       -4.019       -2.112       -1.907       \n",
            "   [0]  cigarettes          0.506        7.509        -8.906       -0.681       -3.745       -2.120       -1.625       \n",
            "   [0]  to                  0.507        2.355        -2.473       -0.680       -3.440       -2.190       -1.249       \n",
            "   [0]  preferred           0.507        6.082        -7.278       -0.679       -3.098       -2.225       -0.873       \n",
            "   [0]  an                  0.507        7.469        -5.473       -0.678       -2.716       -2.254       -0.462       \n",
            "   [0]  efficiency          0.508        7.457        -10.460      -0.678       -2.288       -2.199       -0.089       \n",
            "   [0]  that                0.508        10.813       -4.437       -0.677       -1.807       -2.208       0.400        \n",
            "   [0]  the                 0.511        4.888        -2.130       -0.672       -1.269       -2.203       0.934        \n",
            "   [0]  late                0.512        7.554        -7.780       -0.670       -0.670       -2.284       1.614        \n",
            "   [1]  form                0.510        0.000        0.000        0.000        0.000        -2.349       0.000        \n",
            "   [1]  of                  0.509        0.000        0.000        0.000        0.000        -2.404       0.000        \n",
            "   [1]  asbestos            0.508        0.000        0.000        0.000        0.000        -2.349       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  mutual              0.538        0.000        0.000        0.000        -1.022       -1.671       0.000        \n",
            "   [1]  funds               0.544        0.000        0.000        0.000        -1.148       -1.926       0.000        \n",
            "   [1]  most                0.549        0.000        0.000        0.000        -1.289       -1.971       0.000        \n",
            "   [1]  of                  0.551        0.000        0.000        0.000        -1.447       -1.969       0.000        \n",
            "   [1]  these               0.550        0.000        0.000        0.000        -1.624       -2.019       0.000        \n",
            "   [1]  <unk>               0.549        0.000        0.000        0.000        -1.823       -2.002       0.000        \n",
            "   [1]  portfolios          0.545        0.000        0.000        0.000        -2.047       -1.864       -0.000       \n",
            "   [1]  are                 0.541        0.000        0.000        0.000        -2.298       -1.764       -0.000       \n",
            "   [1]  the                 0.541        0.000        0.000        0.000        -2.580       -1.700       -0.000       \n",
            "   [1]  closed-end          0.542        0.000        0.000        0.000        -2.897       -1.720       -0.000       \n",
            "   [1]  type                0.545        0.000        0.000        0.000        -3.252       -1.795       -0.000       \n",
            "   [0]  made                0.547        9.604        -7.074       -0.603       -3.651       -1.923       -1.728       \n",
            "   [0]  gets                0.547        3.191        -9.109       -0.603       -3.422       -2.005       -1.416       \n",
            "   [0]  when                0.543        9.933        -6.108       -0.610       -3.164       -2.151       -1.013       \n",
            "   [0]  no                  0.538        8.836        -7.482       -0.619       -2.868       -2.281       -0.587       \n",
            "   [0]  limits              0.535        5.155        -8.195       -0.626       -2.524       -2.411       -0.113       \n",
            "   [0]  <eos>               0.532        6.412        -2.541       -0.631       -2.132       -2.467       0.335        \n",
            "   [0]  the                 0.533        3.979        -1.963       -0.629       -1.685       -2.316       0.631        \n",
            "   [0]  happy               0.534        7.421        -9.778       -0.628       -1.186       -2.145       0.959        \n",
            "   [0]  can                 0.534        9.757        -6.368       -0.627       -0.627       -2.026       1.399        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  although            0.529        0.000        0.000        0.000        -1.543       -1.707       0.000        \n",
            "   [1]  the                 0.530        0.000        0.000        0.000        -1.732       -2.133       0.000        \n",
            "   [1]  u.s.                0.530        0.000        0.000        0.000        -1.945       -2.408       0.000        \n",
            "   [1]  remains             0.528        0.000        0.000        0.000        -2.183       -2.466       0.000        \n",
            "   [1]  the                 0.525        0.000        0.000        0.000        -2.451       -2.459       0.000        \n",
            "   [1]  leading             0.524        0.000        0.000        0.000        -2.752       -2.496       -0.000       \n",
            "   [1]  trade               0.523        0.000        0.000        0.000        -3.089       -2.599       -0.000       \n",
            "   [1]  partner             0.523        0.000        0.000        0.000        -3.468       -2.580       -0.000       \n",
            "   [0]  than                0.522        3.864        -5.648       -0.650       -3.894       -2.506       -1.388       \n",
            "   [0]  N                   0.518        5.893        -2.522       -0.659       -3.642       -2.536       -1.107       \n",
            "   [0]  <eos>               0.515        3.904        -3.047       -0.663       -3.350       -2.514       -0.836       \n",
            "   [0]  he                  0.514        11.089       -4.873       -0.665       -3.016       -2.431       -0.585       \n",
            "   [0]  takes               0.514        3.376        -8.198       -0.665       -2.639       -2.301       -0.338       \n",
            "   [0]  can                 0.516        7.576        -6.657       -0.662       -2.217       -2.198       -0.019       \n",
            "   [0]  bonds               0.517        2.935        -8.089       -0.659       -1.746       -2.151       0.405        \n",
            "   [0]  running             0.522        7.195        -8.456       -0.651       -1.220       -2.066       0.846        \n",
            "   [0]  soon                0.528        7.440        -8.340       -0.639       -0.639       -1.980       1.341        \n",
            "   [1]  now                 0.533        0.000        0.000        0.000        0.000        -2.000       0.000        \n",
            "   [1]  the                 0.535        0.000        0.000        0.000        0.000        -2.121       0.000        \n",
            "   [1]  world               0.532        0.000        0.000        0.000        0.000        -2.370       0.000        \n",
            "Samples\n",
            "Sample 0 .  of consolidated gold fields plc was named a global cigarettes to preferred an efficiency that the late form of asbestos\n",
            "Sample 1 .  mutual funds most of these <unk> portfolios are the closed-end type made gets when no limits <eos> the happy can\n",
            "Sample 2 .  although the u.s. remains the leading trade partner than N <eos> he takes can bonds running soon now the world\n",
            "\n",
            "\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 35 1491 916 4 3199 6 8967 371 5]...][[1 1 1 1 1 1 1 1 1 1]...][[9 35 1491 916 4 3199 6 8967 371 5]...][[35 1491 916 4 3199 6 8967 371 5 1141]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 35 1491 916 4 3199 6 8967 371 5]...][[1 1 1 1 1 1 1 1 1 1]...][[9 35 1491 916 4 3199 6 8967 371 5]...][[35 1491 916 4 3199 6 8967 371 5 1141]...]\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 39 13 31 393 1366 2 64 275 1921]...][[1 1 1 1 1 1 1 1 0 0]...][[15 39 13 31 393 1366 2 64 275 10000]...][[39 13 31 393 1366 2 64 275 1921 43]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 39 13 31 393 1366 2 64 275 1921]...][[1 1 1 1 1 1 1 1 0 0]...][[15 39 13 31 393 1366 2 64 275 10000]...][[39 13 31 393 1366 2 64 275 1601 489]...]\n",
            "Generator is stateful.\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[3150 496 263 5 138 6092 4241 6036 30 988]...][[1 1 1 1 1 1 1 1 1 0]...][[3150 496 263 5 138 6092 4241 6036 30 988]...][[496 263 5 138 6092 4241 6036 30 988 152]...]\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[3150 496 263 5 138 6092 4241 6036 30 988]...][[1 1 1 1 1 1 1 1 1 0]...][[3150 496 263 5 138 6092 4241 6036 30 988]...][[496 263 5 138 6092 4241 6036 30 988 6]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[3150 496 263 5 138 6092 4241 6036 30 988]...][[1 1 1 1 1 1 1 1 1 0]...][[3150 496 263 5 138 6092 4241 6036 30 988]...][[496 263 5 138 6092 4241 6036 30 988 2]...]\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            "global_step: 1567\n",
            " perplexity: 526.550\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            " percent of 3-grams captured: 0.107.\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            " percent of 2-grams captured: 0.301.\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            " percent of 4-grams captured: 0.025.\n",
            " geometric_avg: 0.093.\n",
            " arithmetic_avg: 0.144.\n",
            "global_step: 1567\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69228\n",
            " G train loss: 21.34885\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][2 0 2075 4859 5 440 3 0 297 4 357 193 10 26 36 640 33 25 1615 7][9 413 868 1 13 8339 289 45 1954 91]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[3150 496 263 5 138 6092 4241 6036 30 988]...][[1 1 1 1 1 1 1 1 1 0]...][[3150 496 263 5 138 6092 4241 6036 30 988]...][[496 263 5 138 6092 4241 6036 30 988 6]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[3150 496 263 5 138 6092 4241 6036 30 988]...][[1 1 1 1 1 1 1 1 1 0]...][[3150 496 263 5 138 6092 4241 6036 30 988]...][[496 263 5 138 6092 4241 6036 30 988 375]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  once                0.512        0.000        0.000        0.000        -1.456       -1.713       0.000        \n",
            "   [1]  used                0.517        0.000        0.000        0.000        -1.635       -2.061       0.000        \n",
            "   [1]  to                  0.520        0.000        0.000        0.000        -1.836       -2.255       0.000        \n",
            "   [1]  make                0.519        0.000        0.000        0.000        -2.061       -2.329       0.000        \n",
            "   [1]  kent                0.515        0.000        0.000        0.000        -2.314       -2.273       -0.000       \n",
            "   [1]  cigarette           0.509        0.000        0.000        0.000        -2.598       -2.209       -0.000       \n",
            "   [1]  filters             0.503        0.000        0.000        0.000        -2.916       -2.202       -0.000       \n",
            "   [1]  has                 0.500        0.000        0.000        0.000        -3.274       -2.115       -0.000       \n",
            "   [1]  caused              0.496        0.000        0.000        0.000        -3.676       -2.007       -0.000       \n",
            "   [0]  policy              0.492        4.689        -8.464       -0.709       -4.127       -2.061       -2.066       \n",
            "   [0]  if                  0.491        7.785        -6.716       -0.710       -3.837       -2.211       -1.626       \n",
            "   [0]  charges             0.491        8.541        -8.012       -0.711       -3.510       -2.319       -1.192       \n",
            "   [0]  to                  0.493        2.289        -2.330       -0.706       -3.142       -2.363       -0.779       \n",
            "   [0]  sensitive           0.500        7.643        -9.472       -0.692       -2.735       -2.284       -0.451       \n",
            "   [0]  mr.                 0.506        8.985        -5.955       -0.681       -2.293       -2.036       -0.258       \n",
            "   [0]  employees           0.509        10.333       -9.033       -0.676       -1.810       -1.961       0.151        \n",
            "   [0]  much                0.510        5.030        -6.576       -0.674       -1.274       -2.024       0.751        \n",
            "   [0]  want                0.510        6.720        -8.121       -0.673       -0.673       -1.930       1.257        \n",
            "   [1]  of                  0.510        0.000        0.000        0.000        0.000        -1.954       0.000        \n",
            "   [1]  workers             0.507        0.000        0.000        0.000        0.000        -2.084       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  <eos>               0.514        0.000        0.000        0.000        -1.440       -1.702       0.000        \n",
            "   [1]  the                 0.520        0.000        0.000        0.000        -1.617       -2.047       0.000        \n",
            "   [1]  surge               0.527        0.000        0.000        0.000        -1.815       -2.195       0.000        \n",
            "   [1]  brings              0.528        0.000        0.000        0.000        -2.038       -2.215       0.000        \n",
            "   [1]  to                  0.525        0.000        0.000        0.000        -2.288       -2.229       -0.000       \n",
            "   [1]  nearly              0.519        0.000        0.000        0.000        -2.568       -2.147       -0.000       \n",
            "   [1]  N                   0.510        0.000        0.000        0.000        -2.883       -2.067       -0.000       \n",
            "   [1]  the                 0.503        0.000        0.000        0.000        -3.237       -1.973       -0.000       \n",
            "   [1]  number              0.498        0.000        0.000        0.000        -3.634       -1.929       -0.000       \n",
            "   [0]  <eos>               0.496        1.935        -1.687       -0.700       -4.080       -1.943       -2.138       \n",
            "   [0]  his                 0.497        8.987        -6.224       -0.700       -3.794       -1.968       -1.827       \n",
            "   [0]  that                0.501        7.485        -6.857       -0.691       -3.474       -1.940       -1.534       \n",
            "   [0]  energy              0.505        5.542        -8.093       -0.683       -3.124       -1.802       -1.322       \n",
            "   [0]  book                0.506        4.955        -9.665       -0.681       -2.740       -1.708       -1.032       \n",
            "   [0]  signed              0.505        6.187        -8.684       -0.682       -2.312       -1.728       -0.585       \n",
            "   [0]  are                 0.505        8.034        -4.438       -0.683       -1.830       -1.691       -0.139       \n",
            "   [0]  aug.                0.505        7.195        -8.408       -0.682       -1.288       -1.785       0.497        \n",
            "   [0]  attached            0.507        7.320        -10.517      -0.680       -0.680       -1.946       1.266        \n",
            "   [1]  listed              0.505        0.000        0.000        0.000        0.000        -2.095       0.000        \n",
            "   [1]  in                  0.502        0.000        0.000        0.000        0.000        -2.081       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  's                  0.497        0.000        0.000        0.000        -3.230       -1.663       -0.000       \n",
            "   [1]  largest             0.500        0.000        0.000        0.000        -3.626       -2.061       -0.000       \n",
            "   [0]  staff               0.502        8.347        -7.038       -0.690       -4.071       -2.255       -1.816       \n",
            "   [0]  to                  0.503        3.705        -2.932       -0.688       -3.796       -2.332       -1.464       \n",
            "   [0]  merrill             0.503        9.672        -8.053       -0.688       -3.490       -2.255       -1.234       \n",
            "   [0]  committee           0.503        11.246       -7.745       -0.687       -3.146       -2.133       -1.013       \n",
            "   [0]  <eos>               0.503        8.364        -2.552       -0.687       -2.761       -2.070       -0.691       \n",
            "   [0]  supposed            0.504        5.924        -9.773       -0.686       -2.329       -1.998       -0.331       \n",
            "   [0]  utsumi              0.503        10.865       -9.659       -0.687       -1.844       -1.923       0.078        \n",
            "   [0]  <eos>               0.503        6.039        -2.943       -0.687       -1.299       -1.845       0.546        \n",
            "   [0]  amendments          0.503        3.256        -11.071      -0.687       -0.687       -1.811       1.124        \n",
            "   [1]  region              0.504        0.000        0.000        0.000        0.000        -1.839       0.000        \n",
            "   [1]  than                0.505        0.000        0.000        0.000        0.000        -1.838       0.000        \n",
            "   [1]  the                 0.505        0.000        0.000        0.000        0.000        -1.881       0.000        \n",
            "   [1]  u.s.                0.506        0.000        0.000        0.000        0.000        -2.018       0.000        \n",
            "   [1]  is                  0.506        0.000        0.000        0.000        0.000        -2.054       0.000        \n",
            "   [1]  <eos>               0.507        0.000        0.000        0.000        0.000        -2.200       0.000        \n",
            "   [1]  while               0.507        0.000        0.000        0.000        0.000        -2.281       0.000        \n",
            "   [1]  u.s.                0.507        0.000        0.000        0.000        0.000        -2.331       0.000        \n",
            "   [1]  officials           0.506        0.000        0.000        0.000        0.000        -2.163       0.000        \n",
            "Samples\n",
            "Sample 0 .  once used to make kent cigarette filters has caused policy if charges to sensitive mr. employees much want of workers\n",
            "Sample 1 .  <eos> the surge brings to nearly N the number <eos> his that energy book signed are aug. attached listed in\n",
            "Sample 2 .  's largest staff to merrill committee <eos> supposed utsumi <eos> amendments region than the u.s. is <eos> while u.s. officials\n",
            "\n",
            "\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5791 1304 2 83 13 102 3150 7 228 189]...][[1 1 1 1 1 1 1 0 0 0]...][[5791 1304 2 83 13 102 3150 7 10000 10000]...][[1304 2 83 13 102 3150 7 228 189 99]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5791 1304 2 83 13 102 3150 7 228 189]...][[1 1 1 1 1 1 1 0 0 0]...][[5791 1304 2 83 13 102 3150 7 10000 10000]...][[1304 2 83 13 102 3150 7 1 4 188]...]\n",
            "targets[[46 2647 4 106 319 16 8421 4 0 6092 3574 2 64 34 102 4213 529 16 369 2365][5030 4 243 149 349 0 710 319 4 817 84 734 101 2 29 14 175 32 209 121][53 5 2313 52 27 1 8 386 318 23]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 46 2647 4 106 319 16 8421 4 0]...][[1 1 1 1 1 1 1 0 0 0]...][[431 46 2647 4 106 319 16 8421 10000 10000]...][[46 2647 4 106 319 16 8421 4 0 6092]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 46 2647 4 106 319 16 8421 4 0]...][[1 1 1 1 1 1 1 0 0 0]...][[431 46 2647 4 106 319 16 8421 10000 10000]...][[46 2647 4 106 319 16 8421 836 258 6748]...]\n",
            "Generator is stateful.\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 4115 5 14 45 55 3 72 195 1244]...][[1 1 1 1 1 1 1 0 0 0]...][[431 4115 5 14 45 55 3 72 10000 10000]...][[4115 5 14 45 55 3 72 1992 153 52]...]\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 4115 5 14 45 55 3 72 195 1244]...][[1 1 1 1 1 1 1 0 0 0]...][[431 4115 5 14 45 55 3 72 10000 10000]...][[4115 5 14 45 55 3 72 195 1244 220]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 4115 5 14 45 55 3 72 195 1244]...][[1 1 1 1 1 1 1 0 0 0]...][[431 4115 5 14 45 55 3 72 10000 10000]...][[4115 5 14 45 55 3 72 4 411 69]...]\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            "global_step: 1572\n",
            " perplexity: 520.074\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            " percent of 3-grams captured: 0.102.\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            " percent of 2-grams captured: 0.290.\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            " percent of 4-grams captured: 0.035.\n",
            " geometric_avg: 0.102.\n",
            " arithmetic_avg: 0.142.\n",
            "global_step: 1572\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69255\n",
            " G train loss: 22.62619\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][35 92 36 409 2 144 193 99 631 11 249 2584 4 603 7 294 2 108 26 735][1982 7046 43 203 9 1 799 7 2287 38]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 4115 5 14 45 55 3 72 195 1244]...][[1 1 1 1 1 1 1 0 0 0]...][[431 4115 5 14 45 55 3 72 10000 10000]...][[4115 5 14 45 55 3 72 195 1244 220]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 4115 5 14 45 55 3 72 195 1244]...][[1 1 1 1 1 1 1 0 0 0]...][[431 4115 5 14 45 55 3 72 10000 10000]...][[4115 5 14 45 55 3 72 1 20 1361]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  exposed             0.491        0.000        0.000        0.000        -1.853       -1.673       -0.000       \n",
            "   [1]  to                  0.493        0.000        0.000        0.000        -2.080       -2.045       -0.000       \n",
            "   [1]  it                  0.492        0.000        0.000        0.000        -2.336       -2.214       -0.000       \n",
            "   [1]  more                0.489        0.000        0.000        0.000        -2.622       -2.282       -0.000       \n",
            "   [1]  than                0.486        0.000        0.000        0.000        -2.944       -2.210       -0.000       \n",
            "   [1]  N                   0.483        0.000        0.000        0.000        -3.305       -2.134       -0.000       \n",
            "   [1]  years               0.483        0.000        0.000        0.000        -3.710       -2.036       -0.000       \n",
            "   [0]  <unk>               0.487        6.866        -4.629       -0.719       -4.166       -1.950       -2.216       \n",
            "   [0]  from                0.491        10.011       -5.294       -0.712       -3.870       -1.890       -1.980       \n",
            "   [0]  practice            0.492        8.655        -8.558       -0.709       -3.545       -2.011       -1.534       \n",
            "   [0]  <eos>               0.495        3.481        -3.481       -0.703       -3.185       -2.150       -1.035       \n",
            "   [0]  he                  0.498        2.008        -4.343       -0.697       -2.786       -2.244       -0.541       \n",
            "   [0]  <unk>               0.502        9.475        -2.984       -0.689       -2.345       -2.263       -0.082       \n",
            "   [0]  industries          0.502        11.240       -8.409       -0.690       -1.859       -2.181       0.323        \n",
            "   [0]  prices              0.500        3.488        -6.774       -0.693       -1.313       -2.186       0.873        \n",
            "   [0]  him                 0.499        4.895        -6.779       -0.696       -0.696       -2.173       1.477        \n",
            "   [1]  unusually           0.496        0.000        0.000        0.000        0.000        -2.209       0.000        \n",
            "   [1]  <unk>               0.496        0.000        0.000        0.000        0.000        -2.219       0.000        \n",
            "   [1]  once                0.496        0.000        0.000        0.000        0.000        -2.160       0.000        \n",
            "   [1]  it                  0.497        0.000        0.000        0.000        0.000        -2.132       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  new                 0.481        0.000        0.000        0.000        -3.853       -1.653       -0.000       \n",
            "   [0]  spark               0.480        4.248        -11.259      -0.734       -4.326       -1.925       -2.400       \n",
            "   [0]  wage                0.479        6.237        -7.936       -0.737       -4.032       -2.019       -2.013       \n",
            "   [0]  that                0.479        12.084       -3.499       -0.735       -3.700       -2.043       -1.657       \n",
            "   [0]  him                 0.480        5.191        -7.921       -0.734       -3.328       -1.986       -1.343       \n",
            "   [0]  after               0.480        7.690        -5.772       -0.735       -2.913       -1.974       -0.939       \n",
            "   [0]  clearing            0.480        7.400        -10.113      -0.734       -2.445       -1.866       -0.579       \n",
            "   [0]  <eos>               0.484        6.260        -3.145       -0.726       -1.922       -1.760       -0.162       \n",
            "   [0]  that                0.490        10.334       -3.370       -0.714       -1.342       -1.627       0.285        \n",
            "   [0]  merchant            0.494        6.032        -9.741       -0.704       -0.704       -1.488       0.783        \n",
            "   [1]  several             0.495        0.000        0.000        0.000        0.000        -1.441       0.000        \n",
            "   [1]  billions            0.496        0.000        0.000        0.000        0.000        -1.515       0.000        \n",
            "   [1]  of                  0.495        0.000        0.000        0.000        0.000        -1.541       0.000        \n",
            "   [1]  dollars             0.493        0.000        0.000        0.000        0.000        -1.519       0.000        \n",
            "   [1]  in                  0.490        0.000        0.000        0.000        0.000        -1.497       0.000        \n",
            "   [1]  assets              0.489        0.000        0.000        0.000        0.000        -1.313       0.000        \n",
            "   [1]  <eos>               0.490        0.000        0.000        0.000        0.000        -1.224       0.000        \n",
            "   [1]  people              0.493        0.000        0.000        0.000        0.000        -1.136       0.000        \n",
            "   [1]  are                 0.495        0.000        0.000        0.000        0.000        -1.158       0.000        \n",
            "   [1]  looking             0.496        0.000        0.000        0.000        0.000        -1.189       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  voice               0.511        0.000        0.000        0.000        -1.440       -1.636       0.000        \n",
            "   [1]  optimism            0.513        0.000        0.000        0.000        -1.617       -2.009       0.000        \n",
            "   [1]  about               0.513        0.000        0.000        0.000        -1.815       -2.197       0.000        \n",
            "   [1]  japan               0.510        0.000        0.000        0.000        -2.038       -2.266       0.000        \n",
            "   [1]  's                  0.507        0.000        0.000        0.000        -2.288       -2.204       -0.000       \n",
            "   [1]  <unk>               0.506        0.000        0.000        0.000        -2.568       -2.117       -0.000       \n",
            "   [1]  role                0.504        0.000        0.000        0.000        -2.883       -1.922       -0.000       \n",
            "   [1]  in                  0.501        0.000        0.000        0.000        -3.237       -1.818       -0.000       \n",
            "   [1]  asia                0.501        0.000        0.000        0.000        -3.634       -1.692       -0.000       \n",
            "   [0]  fast                0.502        5.622        -9.334       -0.688       -4.080       -1.729       -2.351       \n",
            "   [0]  of                  0.504        6.015        -3.156       -0.686       -3.808       -1.780       -2.028       \n",
            "   [0]  source              0.506        12.080       -8.308       -0.682       -3.506       -1.866       -1.640       \n",
            "   [0]  rose                0.506        5.566        -6.373       -0.680       -3.170       -1.981       -1.189       \n",
            "   [0]  ual                 0.504        5.837        -8.973       -0.684       -2.795       -2.102       -0.693       \n",
            "   [0]  N                   0.501        4.045        -3.758       -0.691       -2.369       -2.154       -0.216       \n",
            "   [0]  to                  0.498        10.939       -2.162       -0.698       -1.884       -2.043       0.158        \n",
            "   [0]  gain                0.494        9.376        -7.454       -0.704       -1.332       -1.844       0.513        \n",
            "   [0]  of                  0.494        7.999        -2.839       -0.705       -0.705       -1.617       0.912        \n",
            "   [1]  's                  0.496        0.000        0.000        0.000        0.000        -1.541       0.000        \n",
            "   [1]  an                  0.500        0.000        0.000        0.000        0.000        -1.621       0.000        \n",
            "Samples\n",
            "Sample 0 .  exposed to it more than N years <unk> from practice <eos> he <unk> industries prices him unusually <unk> once it\n",
            "Sample 1 .  new spark wage that him after clearing <eos> that merchant several billions of dollars in assets <eos> people are looking\n",
            "Sample 2 .  voice optimism about japan 's <unk> role in asia fast of source rose ual N to gain of 's an\n",
            "\n",
            "\n",
            "targets[[26 18 645 15 557 941 1 4 651 9 1 1015 758 2 978 1 658 6 773 4][5 188 5926 2 389 8 819 3187 89 1 0 193 2 204 287 0 193 134 62 112][26 169 5 1213 52 51 194 744 7 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2365 26 18 645 15 557 941 1 4 651]...][[1 1 1 1 1 1 1 1 0 0]...][[2365 26 18 645 15 557 941 1 4 10000]...][[26 18 645 15 557 941 1 4 651 9]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[2365 26 18 645 15 557 941 1 4 651]...][[1 1 1 1 1 1 1 1 0 0]...][[2365 26 18 645 15 557 941 1 4 10000]...][[26 18 645 15 557 941 1 4 823 115]...]\n",
            "targets[[1244 20 0 162 1015 758 8 0 766 1647 4 3483 596 8 651 596 2 0 1 1047][2153 5 5550 45 55 0 2773 47 2 67 0 60 47 556 440 3 3 313 3 11][38 87 32 358 203 5 1 0 1646 8]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 1244 20 0 162 1015 758 8 0 766]...][[1 1 1 1 1 0 0 0 0 0]...][[4 1244 20 0 162 1015 10000 10000 10000 10000]...][[1244 20 0 162 1015 758 8 0 766 1647]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 1244 20 0 162 1015 758 8 0 766]...][[1 1 1 1 1 0 0 0 0 0]...][[4 1244 20 0 162 1015 10000 10000 10000 10000]...][[1244 20 0 162 1015 9 888 1089 815 198]...]\n",
            "Generator is stateful.\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[14 6885 0 1 22 113 2652 8068 5 14]...][[1 1 1 1 1 1 0 0 0 0]...][[14 6885 0 1 22 113 2652 10000 10000 10000]...][[6885 0 1 22 113 2652 29 0 84 5]...]\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[14 6885 0 1 22 113 2652 8068 5 14]...][[1 1 1 1 1 1 0 0 0 0]...][[14 6885 0 1 22 113 2652 10000 10000 10000]...][[6885 0 1 22 113 2652 8068 5 14 2474]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[14 6885 0 1 22 113 2652 8068 5 14]...][[1 1 1 1 1 1 0 0 0 0]...][[14 6885 0 1 22 113 2652 10000 10000 10000]...][[6885 0 1 22 113 2652 59 5 330 892]...]\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            "global_step: 1577\n",
            " perplexity: 527.689\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            " percent of 3-grams captured: 0.088.\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            " percent of 2-grams captured: 0.277.\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            " percent of 4-grams captured: 0.019.\n",
            " geometric_avg: 0.077.\n",
            " arithmetic_avg: 0.128.\n",
            "global_step: 1577\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69315\n",
            " G train loss: 23.88719\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][5 320 51 593 99 157 0 297 4 679 1618 1690 84 44 796 8312 31 364 18 1127][4594 16 0 201 4 0 53 10 203 30]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[14 6885 0 1 22 113 2652 8068 5 14]...][[1 1 1 1 1 1 0 0 0 0]...][[14 6885 0 1 22 113 2652 10000 10000 10000]...][[6885 0 1 22 113 2652 8068 5 14 2474]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[14 6885 0 1 22 113 2652 8068 5 14]...][[1 1 1 1 1 1 0 0 0 0]...][[14 6885 0 1 22 113 2652 10000 10000 10000]...][[6885 0 1 22 113 2652 4 9550 5 141]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  enters              0.501        0.000        0.000        0.000        -2.009       -1.703       -0.000       \n",
            "   [1]  the                 0.497        0.000        0.000        0.000        -2.256       -2.110       -0.000       \n",
            "   [1]  <unk>               0.497        0.000        0.000        0.000        -2.532       -2.331       -0.000       \n",
            "   [1]  with                0.495        0.000        0.000        0.000        -2.843       -2.297       -0.000       \n",
            "   [1]  even                0.497        0.000        0.000        0.000        -3.192       -2.287       -0.000       \n",
            "   [1]  brief               0.500        0.000        0.000        0.000        -3.583       -2.210       -0.000       \n",
            "   [0]  of                  0.503        12.055       -2.979       -0.687       -4.023       -2.212       -1.811       \n",
            "   [0]  vickers             0.504        6.467        -11.017      -0.685       -3.745       -2.290       -1.455       \n",
            "   [0]  to                  0.503        4.759        -2.495       -0.686       -3.436       -2.341       -1.095       \n",
            "   [0]  chairman            0.506        9.533        -6.980       -0.681       -3.087       -2.294       -0.792       \n",
            "   [0]  generally           0.509        12.232       -8.389       -0.675       -2.701       -2.093       -0.608       \n",
            "   [0]  expects             0.511        3.719        -7.461       -0.671       -2.274       -1.978       -0.297       \n",
            "   [0]  it                  0.513        10.728       -5.085       -0.668       -1.800       -1.899       0.098        \n",
            "   [0]  to                  0.511        5.837        -2.326       -0.672       -1.271       -1.875       0.604        \n",
            "   [0]  N                   0.510        10.132       -3.765       -0.673       -0.673       -1.754       1.081        \n",
            "   [1]  later               0.511        0.000        0.000        0.000        0.000        -1.592       0.000        \n",
            "   [1]  researchers         0.510        0.000        0.000        0.000        0.000        -1.459       0.000        \n",
            "   [1]  said                0.511        0.000        0.000        0.000        0.000        -1.426       0.000        \n",
            "   [1]  <eos>               0.512        0.000        0.000        0.000        0.000        -1.450       0.000        \n",
            "   [1]  <unk>               0.516        0.000        0.000        0.000        0.000        -1.522       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  to                  0.516        0.000        0.000        0.000        -1.318       -1.640       0.000        \n",
            "   [1]  stake               0.520        0.000        0.000        0.000        -1.480       -1.865       0.000        \n",
            "   [1]  their               0.524        0.000        0.000        0.000        -1.662       -1.950       0.000        \n",
            "   [1]  claims              0.524        0.000        0.000        0.000        -1.866       -2.048       0.000        \n",
            "   [1]  now                 0.519        0.000        0.000        0.000        -2.094       -2.108       0.000        \n",
            "   [1]  before              0.512        0.000        0.000        0.000        -2.351       -2.115       -0.000       \n",
            "   [1]  the                 0.505        0.000        0.000        0.000        -2.640       -1.987       -0.000       \n",
            "   [1]  number              0.499        0.000        0.000        0.000        -2.964       -1.907       -0.000       \n",
            "   [1]  of                  0.495        0.000        0.000        0.000        -3.327       -1.851       -0.000       \n",
            "   [1]  available           0.492        0.000        0.000        0.000        -3.736       -1.851       -0.000       \n",
            "   [0]  and                 0.492        11.155       -3.311       -0.710       -4.194       -1.873       -2.321       \n",
            "   [0]  they                0.492        8.188        -5.290       -0.708       -3.911       -1.766       -2.145       \n",
            "   [0]  news                0.493        6.334        -7.950       -0.708       -3.596       -1.705       -1.891       \n",
            "   [0]  at                  0.492        5.531        -5.045       -0.710       -3.242       -1.767       -1.475       \n",
            "   [0]  the                 0.493        7.702        -1.367       -0.708       -2.844       -1.832       -1.012       \n",
            "   [0]  company             0.494        10.195       -5.353       -0.706       -2.398       -1.893       -0.505       \n",
            "   [0]  agreement           0.493        7.856        -7.873       -0.707       -1.900       -2.010       0.111        \n",
            "   [0]  court               0.492        8.201        -7.666       -0.708       -1.339       -2.037       0.698        \n",
            "   [0]  debt                0.493        5.283        -7.407       -0.708       -0.708       -1.986       1.279        \n",
            "   [1]  smith               0.493        0.000        0.000        0.000        0.000        -2.030       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  understanding       0.499        0.000        0.000        0.000        -1.437       -1.717       0.000        \n",
            "   [1]  on                  0.504        0.000        0.000        0.000        -1.613       -2.149       0.000        \n",
            "   [1]  the                 0.509        0.000        0.000        0.000        -1.811       -2.391       0.000        \n",
            "   [1]  part                0.513        0.000        0.000        0.000        -2.034       -2.432       0.000        \n",
            "   [1]  of                  0.512        0.000        0.000        0.000        -2.283       -2.374       0.000        \n",
            "   [1]  the                 0.509        0.000        0.000        0.000        -2.563       -2.262       -0.000       \n",
            "   [1]  u.s.                0.506        0.000        0.000        0.000        -2.878       -2.168       -0.000       \n",
            "   [1]  that                0.506        0.000        0.000        0.000        -3.231       -2.013       -0.000       \n",
            "   [1]  japan               0.505        0.000        0.000        0.000        -3.627       -1.976       -0.000       \n",
            "   [0]  a                   0.505        4.863        -4.023       -0.682       -4.072       -2.042       -2.030       \n",
            "   [0]  proposal            0.506        10.214       -6.592       -0.680       -3.805       -2.073       -1.732       \n",
            "   [0]  $                   0.507        10.628       -6.242       -0.680       -3.508       -2.113       -1.396       \n",
            "   [0]  N                   0.506        7.346        -0.035       -0.681       -3.175       -2.089       -1.086       \n",
            "   [0]  million             0.504        17.771       -0.405       -0.685       -2.801       -2.010       -0.790       \n",
            "   [0]  on                  0.501        2.065        -3.722       -0.691       -2.375       -1.864       -0.511       \n",
            "   [0]  than                0.498        11.361       -6.450       -0.698       -1.891       -1.813       -0.078       \n",
            "   [0]  prosperity          0.493        9.621        -13.789      -0.706       -1.339       -1.764       0.425        \n",
            "   [0]  chairman            0.491        9.202        -7.366       -0.711       -0.711       -1.793       1.082        \n",
            "   [1]  michael             0.488        0.000        0.000        0.000        0.000        -1.739       0.000        \n",
            "   [1]  <unk>               0.492        0.000        0.000        0.000        0.000        -1.882       0.000        \n",
            "Samples\n",
            "Sample 0 .  enters the <unk> with even brief of vickers to chairman generally expects it to N later researchers said <eos> <unk>\n",
            "Sample 1 .  to stake their claims now before the number of available and they news at the company agreement court debt smith\n",
            "Sample 2 .  understanding on the part of the u.s. that japan a proposal $ N million on than prosperity chairman michael <unk>\n",
            "\n",
            "\n",
            "targets[[15 3150 24 263 7 253 1451 1351 7 423 398 11 0 6036 7 0 266 5278 8 3122][1223 0 1430 293 1621 43 3 3 8 0 2508 293 202 3 3 2 8 90 357 193][2101 18 0 596 4 2357 2 1 4028 79]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1047 15 3150 24 263 7 253 1451 1351 7]...][[1 1 1 1 1 0 0 0 0 0]...][[1047 15 3150 24 263 7 10000 10000 10000 10000]...][[15 3150 24 263 7 253 1451 1351 7 423]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1047 15 3150 24 263 7 253 1451 1351 7]...][[1 1 1 1 1 0 0 0 0 0]...][[1047 15 3150 24 263 7 10000 10000 10000 10000]...][[15 3150 24 263 7 3858 482 1556 5 25]...]\n",
            "targets[[22 6 769 2155 4 1 7 3 2 20 3 5 3 3 48 6092 3574 22 0 6036][46 8751 45 55 90 149 78 0 3 1037 2 114 9 105 2978 43 0 193 134 5677][4 2248 3894 5 9206 1 18 4713 241 722]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[3122 22 6 769 2155 4 1 7 3 2]...][[1 1 1 0 0 0 0 0 0 0]...][[3122 22 6 769 10000 10000 10000 10000 10000 10000]...][[22 6 769 2155 4 1 7 3 2 20]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[3122 22 6 769 2155 4 1 7 3 2]...][[1 1 1 0 0 0 0 0 0 0]...][[3122 22 6 769 10000 10000 10000 10000 10000 10000]...][[22 6 769 1 457 3246 18 329 22 7678]...]\n",
            "Generator is stateful.\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 80 0 167 4 35 2645 1 65 10]...][[1 1 1 1 1 1 1 1 0 0]...][[1 80 0 167 4 35 2645 1 65 10000]...][[80 0 167 4 35 2645 1 65 54 248]...]\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 80 0 167 4 35 2645 1 65 10]...][[1 1 1 1 1 1 1 1 0 0]...][[1 80 0 167 4 35 2645 1 65 10000]...][[80 0 167 4 35 2645 1 65 10 558]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 80 0 167 4 35 2645 1 65 10]...][[1 1 1 1 1 1 1 1 0 0]...][[1 80 0 167 4 35 2645 1 65 10000]...][[80 0 167 4 35 2645 1 65 2 586]...]\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            "global_step: 1582\n",
            " perplexity: 530.123\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            " percent of 3-grams captured: 0.099.\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            " percent of 2-grams captured: 0.289.\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            " percent of 4-grams captured: 0.029.\n",
            " geometric_avg: 0.095.\n",
            " arithmetic_avg: 0.139.\n",
            "global_step: 1582\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69303\n",
            " G train loss: 24.88457\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][3153 2738 4211 82 95 35 92 2 1116 73 0 1 13 57 1 869 2 19 734 116][9533 4 1203 11 218 2 66 38 1390 14]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 80 0 167 4 35 2645 1 65 10]...][[1 1 1 1 1 1 1 1 0 0]...][[1 80 0 167 4 35 2645 1 65 10000]...][[80 0 167 4 35 2645 1 65 10 558]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 80 0 167 4 35 2645 1 65 10]...][[1 1 1 1 1 1 1 1 0 0]...][[1 80 0 167 4 35 2645 1 65 10000]...][[80 0 167 4 35 2645 1 65 7 0]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  inc.                0.535        0.000        0.000        0.000        -1.422       -1.733       0.000        \n",
            "   [1]  the                 0.537        0.000        0.000        0.000        -1.596       -2.064       0.000        \n",
            "   [1]  unit                0.541        0.000        0.000        0.000        -1.792       -2.276       0.000        \n",
            "   [1]  of                  0.543        0.000        0.000        0.000        -2.012       -2.338       0.000        \n",
            "   [1]  new                 0.544        0.000        0.000        0.000        -2.259       -2.407       0.000        \n",
            "   [1]  york-based          0.540        0.000        0.000        0.000        -2.536       -2.389       -0.000       \n",
            "   [1]  <unk>               0.538        0.000        0.000        0.000        -2.847       -2.436       -0.000       \n",
            "   [1]  corp.               0.536        0.000        0.000        0.000        -3.197       -2.309       -0.000       \n",
            "   [0]  in                  0.536        3.699        -3.514       -0.624       -3.589       -2.120       -1.469       \n",
            "   [0]  the                 0.540        8.911        -1.366       -0.616       -3.329       -1.921       -1.408       \n",
            "   [0]  plummeted           0.544        9.688        -9.519       -0.608       -3.046       -1.878       -1.168       \n",
            "   [0]  the                 0.550        9.406        -4.156       -0.599       -2.737       -1.964       -0.773       \n",
            "   [0]  prove               0.554        7.877        -7.473       -0.591       -2.400       -2.105       -0.295       \n",
            "   [0]  been                0.555        9.400        -7.037       -0.588       -2.032       -2.238       0.207        \n",
            "   [0]  measures            0.553        2.938        -8.239       -0.592       -1.621       -2.350       0.729        \n",
            "   [0]  kenneth             0.546        3.911        -9.358       -0.605       -1.155       -2.434       1.279        \n",
            "   [0]  market              0.539        6.657        -6.180       -0.617       -0.617       -2.541       1.923        \n",
            "   [1]  <unk>               0.536        0.000        0.000        0.000        0.000        -2.446       0.000        \n",
            "   [1]  cigarette           0.532        0.000        0.000        0.000        0.000        -2.256       0.000        \n",
            "   [1]  filters             0.529        0.000        0.000        0.000        0.000        -2.148       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  barney              0.486        0.000        0.000        0.000        -2.319       -1.666       -0.000       \n",
            "   [1]  harris              0.481        0.000        0.000        0.000        -2.604       -1.973       -0.000       \n",
            "   [1]  upham               0.478        0.000        0.000        0.000        -2.923       -2.139       -0.000       \n",
            "   [1]  &                   0.475        0.000        0.000        0.000        -3.282       -2.222       -0.000       \n",
            "   [1]  co.                 0.476        0.000        0.000        0.000        -3.685       -2.196       -0.000       \n",
            "   [0]  a                   0.480        7.708        -3.602       -0.734       -4.137       -2.150       -1.987       \n",
            "   [0]  departments         0.485        7.107        -8.012       -0.723       -3.820       -2.069       -1.750       \n",
            "   [0]  on                  0.490        2.530        -4.515       -0.713       -3.476       -2.078       -1.398       \n",
            "   [0]  the                 0.498        9.403        -1.181       -0.698       -3.103       -2.140       -0.963       \n",
            "   [0]  fourth              0.504        7.805        -7.120       -0.685       -2.700       -2.183       -0.516       \n",
            "   [0]  <unk>               0.511        8.401        -2.969       -0.671       -2.263       -2.226       -0.037       \n",
            "   [0]  of                  0.514        4.113        -2.745       -0.665       -1.787       -2.166       0.380        \n",
            "   [0]  arts                0.514        8.434        -10.747      -0.666       -1.259       -2.228       0.969        \n",
            "   [0]  might               0.514        6.137        -6.900       -0.667       -0.667       -2.269       1.603        \n",
            "   [1]  <unk>               0.515        0.000        0.000        0.000        0.000        -2.314       0.000        \n",
            "   [1]  competition         0.514        0.000        0.000        0.000        0.000        -2.208       0.000        \n",
            "   [1]  <eos>               0.513        0.000        0.000        0.000        0.000        -2.129       0.000        \n",
            "   [1]  as                  0.515        0.000        0.000        0.000        0.000        -2.123       0.000        \n",
            "   [1]  individual          0.517        0.000        0.000        0.000        0.000        -2.053       0.000        \n",
            "   [1]  investors           0.520        0.000        0.000        0.000        0.000        -1.993       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  undersecretary      0.522        0.000        0.000        0.000        -1.552       -1.710       0.000        \n",
            "   [1]  of                  0.524        0.000        0.000        0.000        -1.742       -2.082       0.000        \n",
            "   [1]  commerce            0.523        0.000        0.000        0.000        -1.956       -2.318       0.000        \n",
            "   [1]  for                 0.519        0.000        0.000        0.000        -2.196       -2.267       0.000        \n",
            "   [1]  trade               0.515        0.000        0.000        0.000        -2.465       -2.225       -0.000       \n",
            "   [1]  <eos>               0.515        0.000        0.000        0.000        -2.767       -2.130       -0.000       \n",
            "   [1]  if                  0.516        0.000        0.000        0.000        -3.107       -2.120       -0.000       \n",
            "   [1]  they                0.519        0.000        0.000        0.000        -3.488       -2.110       -0.000       \n",
            "   [0]  tried               0.520        9.706        -9.159       -0.655       -3.916       -2.041       -1.875       \n",
            "   [0]  but                 0.518        4.228        -3.903       -0.657       -3.662       -2.066       -1.595       \n",
            "   [0]  i                   0.518        6.171        -5.623       -0.658       -3.373       -2.207       -1.165       \n",
            "   [0]  trelleborg          0.516        4.553        -9.632       -0.662       -3.047       -2.201       -0.847       \n",
            "   [0]  's                  0.515        3.320        -4.043       -0.664       -2.678       -2.237       -0.441       \n",
            "   [0]  been                0.515        3.135        -5.483       -0.663       -2.262       -2.317       0.055        \n",
            "   [0]  will                0.516        10.776       -5.378       -0.663       -1.795       -2.357       0.562        \n",
            "   [0]  further             0.513        7.708        -6.565       -0.668       -1.271       -2.372       1.100        \n",
            "   [0]  win                 0.508        6.572        -8.565       -0.677       -0.677       -2.429       1.752        \n",
            "   [1]  be                  0.504        0.000        0.000        0.000        0.000        -2.465       0.000        \n",
            "   [1]  a                   0.505        0.000        0.000        0.000        0.000        -2.458       0.000        \n",
            "   [1]  net                 0.508        0.000        0.000        0.000        0.000        -2.377       0.000        \n",
            "Samples\n",
            "Sample 0 .  inc. the unit of new york-based <unk> corp. in the plummeted the prove been measures kenneth market <unk> cigarette filters\n",
            "Sample 1 .  barney harris upham & co. a departments on the fourth <unk> of arts might <unk> competition <eos> as individual investors\n",
            "Sample 2 .  undersecretary of commerce for trade <eos> if they tried but i trelleborg 's been will further win be a net\n",
            "\n",
            "\n",
            "targets[[46 238 0 37 15 2 211 3 945 56 1389 1101 22 0 5545 3 34 2214 45 55][382 99 13 10 97 26 77 18 4718 3699 2946 5 0 310 4 51 2113 2560 2 78][5006 2 163 50 919 4645 157 29 144 373]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6036 46 238 0 37 15 2 211 3 945]...][[1 1 1 0 0 0 0 0 0 0]...][[6036 46 238 0 10000 10000 10000 10000 10000 10000]...][[46 238 0 37 15 2 211 3 945 56]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6036 46 238 0 37 15 2 211 3 945]...][[1 1 1 0 0 0 0 0 0 0]...][[6036 46 238 0 10000 10000 10000 10000 10000 10000]...][[46 238 0 40 11 27 629 242 33 415]...]\n",
            "targets[[132 421 0 169 297 2 346 4 0 258 6661 431 34 1 4464 210 132 22 376 1][77 18 31 196 1176 4 45 55 3 3 7 291 3 8 201 4 69 40 357 193][7079 7 218 8 571 2 18 0 235 4]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[55 132 421 0 169 297 2 346 4 0]...][[1 1 1 1 0 0 0 0 0 0]...][[55 132 421 0 169 10000 10000 10000 10000 10000]...][[132 421 0 169 297 2 346 4 0 258]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[55 132 421 0 169 297 2 346 4 0]...][[1 1 1 1 0 0 0 0 0 0]...][[55 132 421 0 169 10000 10000 10000 10000 10000]...][[132 421 0 169 2 0 1546 1 1521 396]...]\n",
            "Generator is stateful.\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6036 7 3 2 366 1976 3178 46 220 45]...][[1 1 1 1 1 0 0 0 0 0]...][[6036 7 3 2 366 1976 10000 10000 10000 10000]...][[7 3 2 366 1976 480 4 311 1305 3406]...]\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6036 7 3 2 366 1976 3178 46 220 45]...][[1 1 1 1 1 0 0 0 0 0]...][[6036 7 3 2 366 1976 10000 10000 10000 10000]...][[7 3 2 366 1976 3178 46 220 45 55]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6036 7 3 2 366 1976 3178 46 220 45]...][[1 1 1 1 1 0 0 0 0 0]...][[6036 7 3 2 366 1976 10000 10000 10000 10000]...][[7 3 2 366 1976 2580 7 24 19 143]...]\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            "global_step: 1587\n",
            " perplexity: 532.554\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            " percent of 3-grams captured: 0.101.\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            " percent of 2-grams captured: 0.300.\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            " percent of 4-grams captured: 0.031.\n",
            " geometric_avg: 0.098.\n",
            " arithmetic_avg: 0.144.\n",
            "global_step: 1587\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69311\n",
            " G train loss: 25.46520\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][34 959 597 20 0 60 47 94 0 72 125 365 34 6208 5 677 35 189 10 949][430 11 1376 2 57 2769 1618 26 1 43]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6036 7 3 2 366 1976 3178 46 220 45]...][[1 1 1 1 1 0 0 0 0 0]...][[6036 7 3 2 366 1976 10000 10000 10000 10000]...][[7 3 2 366 1976 3178 46 220 45 55]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[6036 7 3 2 366 1976 3178 46 220 45]...][[1 1 1 1 1 0 0 0 0 0]...][[6036 7 3 2 366 1976 10000 10000 10000 10000]...][[7 3 2 366 1976 60 1381 50 0 1]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  in                  0.445        0.000        0.000        0.000        -2.542       -1.738       -0.000       \n",
            "   [1]  N                   0.445        0.000        0.000        0.000        -2.854       -1.991       -0.000       \n",
            "   [1]  <eos>               0.450        0.000        0.000        0.000        -3.204       -2.044       -0.000       \n",
            "   [1]  although            0.454        0.000        0.000        0.000        -3.597       -1.980       -0.000       \n",
            "   [1]  preliminary         0.459        0.000        0.000        0.000        -4.039       -1.871       -0.000       \n",
            "   [0]  stock               0.462        12.924       -5.547       -0.772       -4.534       -1.769       -2.765       \n",
            "   [0]  lawson              0.464        6.196        -9.293       -0.767       -4.224       -1.741       -2.482       \n",
            "   [0]  had                 0.465        7.544        -5.211       -0.766       -3.881       -1.803       -2.077       \n",
            "   [0]  the                 0.464        5.534        -1.923       -0.767       -3.496       -1.869       -1.627       \n",
            "   [0]  <unk>               0.468        7.997        -1.026       -0.759       -3.064       -1.954       -1.110       \n",
            "   [0]  of                  0.469        6.124        -1.095       -0.757       -2.588       -1.914       -0.673       \n",
            "   [0]  who                 0.468        6.714        -6.790       -0.760       -2.055       -1.955       -0.099       \n",
            "   [0]  season              0.465        8.467        -11.473      -0.766       -1.454       -2.072       0.618        \n",
            "   [0]  for                 0.462        3.893        -3.715       -0.772       -0.772       -2.074       1.302        \n",
            "   [1]  latest              0.460        0.000        0.000        0.000        0.000        -1.987       0.000        \n",
            "   [1]  results             0.458        0.000        0.000        0.000        0.000        -1.983       0.000        \n",
            "   [1]  appear              0.455        0.000        0.000        0.000        0.000        -2.017       0.000        \n",
            "   [1]  in                  0.453        0.000        0.000        0.000        0.000        -1.993       0.000        \n",
            "   [1]  today               0.453        0.000        0.000        0.000        0.000        -1.836       0.000        \n",
            "   [1]  's                  0.455        0.000        0.000        0.000        0.000        -1.773       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  have                0.528        0.000        0.000        0.000        -1.116       -1.681       0.000        \n",
            "   [1]  turned              0.534        0.000        0.000        0.000        -1.253       -2.088       0.000        \n",
            "   [1]  away                0.538        0.000        0.000        0.000        -1.407       -2.381       0.000        \n",
            "   [1]  from                0.537        0.000        0.000        0.000        -1.579       -2.515       0.000        \n",
            "   [1]  the                 0.537        0.000        0.000        0.000        -1.773       -2.595       0.000        \n",
            "   [1]  stock               0.532        0.000        0.000        0.000        -1.991       -2.626       0.000        \n",
            "   [1]  market              0.528        0.000        0.000        0.000        -2.235       -2.573       0.000        \n",
            "   [1]  over                0.525        0.000        0.000        0.000        -2.509       -2.468       -0.000       \n",
            "   [1]  the                 0.525        0.000        0.000        0.000        -2.817       -2.495       -0.000       \n",
            "   [1]  years               0.525        0.000        0.000        0.000        -3.163       -2.562       -0.000       \n",
            "   [1]  securities          0.523        0.000        0.000        0.000        -3.551       -2.569       -0.000       \n",
            "   [0]  with                0.518        8.589        -5.677       -0.658       -3.986       -2.559       -1.428       \n",
            "   [0]  selling             0.514        7.636        -7.728       -0.665       -3.737       -2.561       -1.176       \n",
            "   [0]  lift                0.513        10.097       -9.493       -0.668       -3.449       -2.384       -1.065       \n",
            "   [0]  mr.                 0.511        2.555        -5.923       -0.672       -3.123       -2.209       -0.914       \n",
            "   [0]  privatization       0.506        8.580        -7.629       -0.680       -2.752       -2.096       -0.657       \n",
            "   [0]  japanese            0.504        8.077        -8.321       -0.685       -2.326       -2.012       -0.314       \n",
            "   [0]  cases               0.502        7.082        -8.252       -0.688       -1.842       -1.953       0.111        \n",
            "   [0]  numbers             0.503        4.239        -9.182       -0.687       -1.296       -1.938       0.643        \n",
            "   [0]  income              0.505        8.203        -8.547       -0.683       -0.683       -1.957       1.274        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  gain                0.478        0.000        0.000        0.000        -2.192       -1.735       -0.000       \n",
            "   [1]  for                 0.476        0.000        0.000        0.000        -2.461       -2.019       -0.000       \n",
            "   [1]  everyone            0.473        0.000        0.000        0.000        -2.763       -2.173       -0.000       \n",
            "   [1]  <eos>               0.475        0.000        0.000        0.000        -3.101       -2.225       -0.000       \n",
            "   [1]  some                0.475        0.000        0.000        0.000        -3.482       -2.190       -0.000       \n",
            "   [1]  asian               0.476        0.000        0.000        0.000        -3.909       -2.127       -0.000       \n",
            "   [0]  away                0.476        9.366        -7.677       -0.742       -4.389       -2.148       -2.240       \n",
            "   [0]  no                  0.476        4.907        -6.809       -0.742       -4.094       -2.221       -1.873       \n",
            "   [0]  is                  0.476        2.506        -5.197       -0.743       -3.763       -2.378       -1.385       \n",
            "   [0]  asked               0.475        6.213        -7.922       -0.745       -3.390       -2.529       -0.861       \n",
            "   [0]  much                0.473        8.241        -6.541       -0.748       -2.970       -2.600       -0.370       \n",
            "   [0]  others              0.473        5.038        -7.823       -0.748       -2.494       -2.356       -0.138       \n",
            "   [0]  we                  0.477        6.932        -5.833       -0.741       -1.960       -2.148       0.188        \n",
            "   [0]  's                  0.483        4.266        -5.145       -0.727       -1.369       -2.072       0.703        \n",
            "   [0]  be                  0.487        8.798        -4.673       -0.720       -0.720       -2.109       1.389        \n",
            "   [1]  step                0.487        0.000        0.000        0.000        0.000        -2.197       0.000        \n",
            "   [1]  up                  0.485        0.000        0.000        0.000        0.000        -2.224       0.000        \n",
            "   [1]  its                 0.482        0.000        0.000        0.000        0.000        -2.344       0.000        \n",
            "   [1]  military            0.480        0.000        0.000        0.000        0.000        -2.446       0.000        \n",
            "   [1]  spending            0.477        0.000        0.000        0.000        0.000        -2.399       0.000        \n",
            "Samples\n",
            "Sample 0 .  in N <eos> although preliminary stock lawson had the <unk> of who season for latest results appear in today 's\n",
            "Sample 1 .  have turned away from the stock market over the years securities with selling lift mr. privatization japanese cases numbers income\n",
            "Sample 2 .  gain for everyone <eos> some asian away no is asked much others we 's be step up its military spending\n",
            "\n",
            "\n",
            "targets[[1015 2 0 345 4 3 2786 20 7536 1 5172 1015 8 1 24 289 206 55 169 0][488 218 18 31 196 1256 4 3 3 2 0 881 62 112 4 97 4 144 193 39][877 372 13 263 67 6 260 13 1360 17]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1015 2 0 345 4 3 2786 20 7536]...][[1 1 1 1 1 1 1 1 1 0]...][[1 1015 2 0 345 4 3 2786 20 7536]...][[1015 2 0 345 4 3 2786 20 7536 1]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1 1015 2 0 345 4 3 2786 20 7536]...][[1 1 1 1 1 1 1 1 1 0]...][[1 1015 2 0 345 4 3 2786 20 7536]...][[1015 2 0 345 4 3 2786 20 7536 555]...]\n",
            "targets[[1244 15 2 0 1 158 13 6 4196 1883 211 150 4 505 56 938 1 4464 15 978][40 34 1324 121 45 1056 55 0 243 149 38 841 2 14 9 684 985 1118 6 1256][11 2079 0 373 4318 1089 8 2836 16 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1244 15 2 0 1 158 13 6 4196]...][[1 1 1 1 1 1 1 1 1 0]...][[0 1244 15 2 0 1 158 13 6 4196]...][[1244 15 2 0 1 158 13 6 4196 1883]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[0 1244 15 2 0 1 158 13 6 4196]...][[1 1 1 1 1 1 1 1 1 0]...][[0 1244 15 2 0 1 158 13 6 4196]...][[1244 15 2 0 1 158 13 6 4196 74]...]\n",
            "Generator is stateful.\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 35 1491 916 4 3199 6 8967 371 5]...][[1 1 1 0 0 0 0 0 0 0]...][[9 35 1491 916 10000 10000 10000 10000 10000 10000]...][[35 1491 916 2 1175 0 1085 0 1 26]...]\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 35 1491 916 4 3199 6 8967 371 5]...][[1 1 1 0 0 0 0 0 0 0]...][[9 35 1491 916 10000 10000 10000 10000 10000 10000]...][[35 1491 916 4 3199 6 8967 371 5 1141]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 35 1491 916 4 3199 6 8967 371 5]...][[1 1 1 0 0 0 0 0 0 0]...][[9 35 1491 916 10000 10000 10000 10000 10000 10000]...][[35 1491 916 2885 8 0 354 269 2380 11]...]\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            "global_step: 1592\n",
            " perplexity: 525.775\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            " percent of 3-grams captured: 0.096.\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            " percent of 2-grams captured: 0.275.\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            " percent of 4-grams captured: 0.035.\n",
            " geometric_avg: 0.098.\n",
            " arithmetic_avg: 0.136.\n",
            "global_step: 1592\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69282\n",
            " G train loss: 26.74460\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][677 1547 5 184 2 8 0 365 26 7736 51 1 289 8 1837 5 87 14 2 142][5 1757 0 53 649 3157 7 0 1646 2]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 35 1491 916 4 3199 6 8967 371 5]...][[1 1 1 0 0 0 0 0 0 0]...][[9 35 1491 916 10000 10000 10000 10000 10000 10000]...][[35 1491 916 4 3199 6 8967 371 5 1141]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[9 35 1491 916 4 3199 6 8967 371 5]...][[1 1 1 0 0 0 0 0 0 0]...][[9 35 1491 916 10000 10000 10000 10000 10000 10000]...][[35 1491 916 68 329 26 1283 99 38 1090]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  new                 0.490        0.000        0.000        0.000        -2.951       -1.660       -0.000       \n",
            "   [1]  england             0.489        0.000        0.000        0.000        -3.313       -1.986       -0.000       \n",
            "   [1]  journal             0.488        0.000        0.000        0.000        -3.720       -2.201       -0.000       \n",
            "   [0]  i                   0.488        4.752        -6.301       -0.718       -4.176       -2.277       -1.899       \n",
            "   [0]  move                0.487        10.061       -7.707       -0.719       -3.882       -2.144       -1.738       \n",
            "   [0]  are                 0.489        4.400        -4.555       -0.716       -3.552       -2.005       -1.547       \n",
            "   [0]  authority           0.492        9.986        -8.777       -0.710       -3.184       -1.911       -1.273       \n",
            "   [0]  now                 0.496        9.091        -6.726       -0.702       -2.778       -1.888       -0.890       \n",
            "   [0]  they                0.499        3.665        -5.729       -0.694       -2.331       -1.848       -0.482       \n",
            "   [0]  regulators          0.502        9.205        -8.620       -0.689       -1.837       -1.777       -0.060       \n",
            "   [0]  's                  0.504        6.710        -3.680       -0.684       -1.289       -1.815       0.526        \n",
            "   [0]  recorded            0.507        9.643        -9.474       -0.679       -0.679       -1.870       1.191        \n",
            "   [1]  to                  0.509        0.000        0.000        0.000        0.000        -1.896       0.000        \n",
            "   [1]  the                 0.513        0.000        0.000        0.000        0.000        -1.833       0.000        \n",
            "   [1]  problem             0.514        0.000        0.000        0.000        0.000        -1.845       0.000        \n",
            "   [1]  <eos>               0.519        0.000        0.000        0.000        0.000        -1.942       0.000        \n",
            "   [1]  a                   0.523        0.000        0.000        0.000        0.000        -1.872       0.000        \n",
            "   [1]  <unk>               0.529        0.000        0.000        0.000        0.000        -1.776       0.000        \n",
            "   [1]  <unk>               0.535        0.000        0.000        0.000        0.000        -1.706       0.000        \n",
            "   [1]  said                0.536        0.000        0.000        0.000        0.000        -1.658       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  find                0.477        0.000        0.000        0.000        -3.452       -1.682       -0.000       \n",
            "   [1]  easy                0.473        0.000        0.000        0.000        -3.875       -2.002       -0.000       \n",
            "   [0]  cooled              0.469        3.729        -10.649      -0.756       -4.351       -2.192       -2.159       \n",
            "   [0]  after               0.468        9.472        -5.344       -0.759       -4.035       -2.245       -1.790       \n",
            "   [0]  movie               0.471        4.773        -9.647       -0.753       -3.679       -2.081       -1.598       \n",
            "   [0]  rear                0.474        3.528        -11.051      -0.746       -3.285       -1.992       -1.293       \n",
            "   [0]  's                  0.481        4.521        -4.548       -0.732       -2.850       -1.980       -0.871       \n",
            "   [0]  gary                0.490        8.072        -9.980       -0.713       -2.378       -2.037       -0.341       \n",
            "   [0]  sunday              0.496        5.208        -9.992       -0.702       -1.870       -2.058       0.188        \n",
            "   [0]  plunge              0.498        11.544       -8.192       -0.697       -1.311       -2.148       0.837        \n",
            "   [0]  peter               0.501        7.390        -11.352      -0.690       -0.690       -2.183       1.493        \n",
            "   [1]  <unk>               0.507        0.000        0.000        0.000        0.000        -2.181       0.000        \n",
            "   [1]  far                 0.508        0.000        0.000        0.000        0.000        -2.135       0.000        \n",
            "   [1]  and                 0.514        0.000        0.000        0.000        0.000        -2.130       0.000        \n",
            "   [1]  wide                0.518        0.000        0.000        0.000        0.000        -2.001       0.000        \n",
            "   [1]  to                  0.519        0.000        0.000        0.000        0.000        -2.022       0.000        \n",
            "   [1]  do                  0.520        0.000        0.000        0.000        0.000        -2.051       0.000        \n",
            "   [1]  it                  0.522        0.000        0.000        0.000        0.000        -2.084       0.000        \n",
            "   [1]  <eos>               0.524        0.000        0.000        0.000        0.000        -2.137       0.000        \n",
            "   [1]  financial           0.526        0.000        0.000        0.000        0.000        -2.111       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  to                  0.532        0.000        0.000        0.000        -1.412       -1.738       0.000        \n",
            "   [1]  ease                0.537        0.000        0.000        0.000        -1.586       -2.125       0.000        \n",
            "   [1]  the                 0.540        0.000        0.000        0.000        -1.780       -2.447       0.000        \n",
            "   [1]  u.s.                0.539        0.000        0.000        0.000        -1.999       -2.529       0.000        \n",
            "   [1]  security            0.531        0.000        0.000        0.000        -2.244       -2.421       0.000        \n",
            "   [1]  burden              0.521        0.000        0.000        0.000        -2.519       -2.417       -0.000       \n",
            "   [1]  in                  0.508        0.000        0.000        0.000        -2.828       -2.350       -0.000       \n",
            "   [1]  the                 0.500        0.000        0.000        0.000        -3.175       -2.223       -0.000       \n",
            "   [1]  region              0.498        0.000        0.000        0.000        -3.565       -2.178       -0.000       \n",
            "   [0]  <eos>               0.502        2.472        -2.472       -0.690       -4.002       -2.150       -1.852       \n",
            "   [0]  have                0.506        2.846        -4.890       -0.682       -3.719       -2.092       -1.626       \n",
            "   [0]  working             0.507        8.769        -6.852       -0.679       -3.409       -2.145       -1.264       \n",
            "   [0]  <eos>               0.510        3.878        -2.835       -0.673       -3.065       -2.300       -0.765       \n",
            "   [0]  acquisition         0.512        6.560        -9.143       -0.668       -2.685       -2.290       -0.395       \n",
            "   [0]  to                  0.511        10.630       -2.406       -0.671       -2.264       -2.327       0.063        \n",
            "   [0]  N                   0.510        9.160        -4.878       -0.673       -1.789       -2.356       0.568        \n",
            "   [0]  and                 0.514        11.755       -4.685       -0.665       -1.253       -2.276       1.023        \n",
            "   [0]  up                  0.517        7.122        -5.987       -0.660       -0.660       -2.093       1.433        \n",
            "   [1]  the                 0.520        0.000        0.000        0.000        0.000        -2.083       0.000        \n",
            "   [1]  future              0.519        0.000        0.000        0.000        0.000        -2.169       0.000        \n",
            "Samples\n",
            "Sample 0 .  new england journal i move are authority now they regulators 's recorded to the problem <eos> a <unk> <unk> said\n",
            "Sample 1 .  find easy cooled after movie rear 's gary sunday plunge peter <unk> far and wide to do it <eos> financial\n",
            "Sample 2 .  to ease the u.s. security burden in the region <eos> have working <eos> acquisition to N and up the future\n",
            "\n",
            "\n",
            "targets[[1 2 0 760 4 5172 1015 2786 211 0 431 18 0 379 1 1713 398 2025 1310 5][11 193 10 2096 7 190 10 26 7595 248 5 243 116 88 19 435 1591 57 2198 117][7 668 0 2238 50 0 2836 5 511 73]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[978 1 2 0 760 4 5172 1015 2786 211]...][[1 1 1 1 1 0 0 0 0 0]...][[978 1 2 0 760 4 10000 10000 10000 10000]...][[1 2 0 760 4 5172 1015 2786 211 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[978 1 2 0 760 4 5172 1015 2786 211]...][[1 1 1 1 1 0 0 0 0 0]...][[978 1 2 0 760 4 10000 10000 10000 10000]...][[1 2 0 760 4 329 512 7 0 10]...]\n",
            "targets[[25 0 1590 11 106 3150 431 4375 7 650 6080 672 28 15 2 0 402 41 13 1023][2 29 57 569 193 376 34 9422 2508 293 30 1744 5 6 8429 3 3 1256 2 14][8448 0 373 29 63 349 6 4345 2 406]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5 25 0 1590 11 106 3150 431 4375 7]...][[1 1 0 0 0 0 0 0 0 0]...][[5 25 0 10000 10000 10000 10000 10000 10000 10000]...][[25 0 1590 11 106 3150 431 4375 7 650]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5 25 0 1590 11 106 3150 431 4375 7]...][[1 1 0 0 0 0 0 0 0 0]...][[5 25 0 10000 10000 10000 10000 10000 10000 10000]...][[25 0 1189 1861 248 41 67 288 8701 209]...]\n",
            "Generator is stateful.\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 39 13 31 393 1366 2 64 275 1921]...][[1 1 1 0 0 0 0 0 0 0]...][[15 39 13 31 10000 10000 10000 10000 10000 10000]...][[39 13 31 6 12 3 21 36 12 3]...]\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 39 13 31 393 1366 2 64 275 1921]...][[1 1 1 0 0 0 0 0 0 0]...][[15 39 13 31 10000 10000 10000 10000 10000 10000]...][[39 13 31 393 1366 2 64 275 1921 43]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 39 13 31 393 1366 2 64 275 1921]...][[1 1 1 0 0 0 0 0 0 0]...][[15 39 13 31 10000 10000 10000 10000 10000 10000]...][[39 13 31 49 218 453 6 272 9 1243]...]\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            "global_step: 1597\n",
            " perplexity: 517.593\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            " percent of 3-grams captured: 0.098.\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            " percent of 2-grams captured: 0.298.\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            " percent of 4-grams captured: 0.035.\n",
            " geometric_avg: 0.101.\n",
            " arithmetic_avg: 0.144.\n",
            "global_step: 1597\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69269\n",
            " G train loss: 27.75294\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][2686 516 4595 116 5 7377 8 5 841 6 1 4 197 125 2 8 97 3170 190 34][4 0 53 9 6500 16 759 3669 7 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 39 13 31 393 1366 2 64 275 1921]...][[1 1 1 0 0 0 0 0 0 0]...][[15 39 13 31 10000 10000 10000 10000 10000 10000]...][[39 13 31 393 1366 2 64 275 1921 43]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[15 39 13 31 393 1366 2 64 275 1921]...][[1 1 1 0 0 0 0 0 0 0]...][[15 39 13 31 10000 10000 10000 10000 10000 10000]...][[39 13 31 142 1657 99 198 6 470 19]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  this                0.469        0.000        0.000        0.000        -3.081       -1.675       -0.000       \n",
            "   [1]  is                  0.470        0.000        0.000        0.000        -3.459       -1.947       -0.000       \n",
            "   [1]  an                  0.472        0.000        0.000        0.000        -3.883       -2.164       -0.000       \n",
            "   [0]  financial           0.474        7.141        -6.562       -0.747       -4.359       -2.215       -2.144       \n",
            "   [0]  m.                  0.476        9.261        -9.363       -0.742       -4.055       -2.197       -1.859       \n",
            "   [0]  now                 0.480        3.349        -6.078       -0.734       -3.720       -2.144       -1.576       \n",
            "   [0]  during              0.483        5.737        -7.572       -0.729       -3.352       -2.131       -1.221       \n",
            "   [0]  a                   0.483        8.043        -2.749       -0.727       -2.945       -2.124       -0.821       \n",
            "   [0]  drop                0.481        9.775        -5.587       -0.732       -2.490       -2.066       -0.425       \n",
            "   [0]  as                  0.480        6.378        -4.906       -0.734       -1.974       -2.079       0.105        \n",
            "   [0]  a                   0.480        5.564        -2.605       -0.735       -1.392       -2.087       0.695        \n",
            "   [0]  domestic            0.478        7.758        -9.068       -0.738       -0.738       -2.061       1.322        \n",
            "   [1]  before              0.477        0.000        0.000        0.000        0.000        -2.081       0.000        \n",
            "   [1]  anyone              0.474        0.000        0.000        0.000        0.000        -2.051       0.000        \n",
            "   [1]  heard               0.474        0.000        0.000        0.000        0.000        -2.129       0.000        \n",
            "   [1]  of                  0.474        0.000        0.000        0.000        0.000        -2.232       0.000        \n",
            "   [1]  asbestos            0.475        0.000        0.000        0.000        0.000        -2.282       0.000        \n",
            "   [1]  having              0.476        0.000        0.000        0.000        0.000        -2.350       0.000        \n",
            "   [1]  any                 0.479        0.000        0.000        0.000        0.000        -2.299       0.000        \n",
            "   [1]  questionable        0.483        0.000        0.000        0.000        0.000        -2.220       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  planners            0.459        0.000        0.000        0.000        -3.961       -1.615       -0.000       \n",
            "   [0]  target              0.460        8.027        -9.606       -0.777       -4.447       -1.892       -2.555       \n",
            "   [0]  held                0.462        12.472       -8.100       -0.773       -4.120       -2.130       -1.990       \n",
            "   [0]  for                 0.465        7.610        -3.731       -0.766       -3.757       -2.173       -1.584       \n",
            "   [0]  costs               0.470        5.012        -8.017       -0.755       -3.358       -2.095       -1.264       \n",
            "   [0]  in                  0.473        12.700       -3.195       -0.748       -2.922       -1.967       -0.955       \n",
            "   [0]  million             0.478        6.811        -6.204       -0.739       -2.441       -1.821       -0.619       \n",
            "   [0]  <unk>               0.487        2.653        -3.687       -0.720       -1.911       -1.778       -0.133       \n",
            "   [0]  as                  0.492        8.985        -4.462       -0.709       -1.337       -1.674       0.337        \n",
            "   [0]  prove               0.494        2.583        -8.973       -0.705       -0.705       -1.652       0.948        \n",
            "   [1]  <unk>               0.500        0.000        0.000        0.000        0.000        -1.675       0.000        \n",
            "   [1]  of                  0.501        0.000        0.000        0.000        0.000        -1.687       0.000        \n",
            "   [1]  international       0.498        0.000        0.000        0.000        0.000        -1.830       0.000        \n",
            "   [1]  securities          0.491        0.000        0.000        0.000        0.000        -1.976       0.000        \n",
            "   [1]  <eos>               0.489        0.000        0.000        0.000        0.000        -2.105       0.000        \n",
            "   [1]  and                 0.491        0.000        0.000        0.000        0.000        -2.111       0.000        \n",
            "   [1]  many                0.493        0.000        0.000        0.000        0.000        -2.032       0.000        \n",
            "   [1]  emerging            0.496        0.000        0.000        0.000        0.000        -1.976       0.000        \n",
            "   [1]  markets             0.499        0.000        0.000        0.000        0.000        -1.895       0.000        \n",
            "   [1]  have                0.499        0.000        0.000        0.000        0.000        -1.857       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  of                  0.478        0.000        0.000        0.000        -2.099       -1.720       -0.000       \n",
            "   [1]  the                 0.483        0.000        0.000        0.000        -2.357       -2.132       -0.000       \n",
            "   [1]  u.s.                0.488        0.000        0.000        0.000        -2.646       -2.265       -0.000       \n",
            "   [1]  's                  0.493        0.000        0.000        0.000        -2.971       -2.171       -0.000       \n",
            "   [1]  leases              0.492        0.000        0.000        0.000        -3.335       -2.232       -0.000       \n",
            "   [1]  on                  0.489        0.000        0.000        0.000        -3.744       -2.391       -0.000       \n",
            "   [0]  the                 0.490        8.438        -1.438       -0.714       -4.204       -2.498       -1.706       \n",
            "   [0]  previously          0.492        10.320       -7.709       -0.709       -3.918       -2.425       -1.493       \n",
            "   [0]  off                 0.491        5.869        -7.609       -0.710       -3.603       -2.305       -1.298       \n",
            "   [0]  as                  0.490        3.132        -5.155       -0.713       -3.248       -2.275       -0.973       \n",
            "   [0]  own                 0.492        11.864       -7.819       -0.710       -2.846       -2.166       -0.680       \n",
            "   [0]  today               0.493        4.822        -7.844       -0.707       -2.398       -2.014       -0.384       \n",
            "   [0]  oct.                0.493        4.505        -8.931       -0.707       -1.899       -1.930       0.032        \n",
            "   [0]  as                  0.492        2.545        -5.470       -0.709       -1.338       -1.944       0.606        \n",
            "   [0]  the                 0.494        8.689        -1.302       -0.706       -0.706       -1.925       1.219        \n",
            "   [1]  u.s.                0.496        0.000        0.000        0.000        0.000        -1.878       0.000        \n",
            "   [1]  troop               0.501        0.000        0.000        0.000        0.000        -1.762       0.000        \n",
            "   [1]  reduction           0.501        0.000        0.000        0.000        0.000        -1.718       0.000        \n",
            "   [1]  in                  0.498        0.000        0.000        0.000        0.000        -1.867       0.000        \n",
            "   [1]  south               0.494        0.000        0.000        0.000        0.000        -1.889       0.000        \n",
            "Samples\n",
            "Sample 0 .  this is an financial m. now during a drop as a domestic before anyone heard of asbestos having any questionable\n",
            "Sample 1 .  planners target held for costs in million <unk> as prove <unk> of international securities <eos> and many emerging markets have\n",
            "Sample 2 .  of the u.s. 's leases on the previously off as own today oct. as the u.s. troop reduction in south\n",
            "\n",
            "\n",
            "targets[[17 1 82 1 95 24 124 315 22 1 5 138 0 4241 6036 2 0 1883 684 33][30 58 4383 17 173 116 19 6 262 690 1119 2281 5 3 9 569 233 4723 2 8][373 16 0 901 28 15 2 0 4045 7]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1023 17 1 82 1 95 24 124 315 22]...][[1 1 1 1 1 1 0 0 0 0]...][[1023 17 1 82 1 95 24 10000 10000 10000]...][[17 1 82 1 95 24 124 315 22 1]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[1023 17 1 82 1 95 24 124 315 22]...][[1 1 1 1 1 1 0 0 0 0]...][[1023 17 1 82 1 95 24 10000 10000 10000]...][[17 1 82 1 95 24 34 1932 28 372]...]\n",
            "targets[[414 150 56 1987 10 0 53 205 6190 0 1354 4 3150 210 1 45 1 55 0 246][249 35 193 10 26 32 113 1144 2306 396 34 1007 5 218 18 129 2946 2 68 826][5 25 54 4 0 598 18 0 722 2]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[33 414 150 56 1987 10 0 53 205 6190]...][[1 1 1 1 1 1 1 1 1 1]...][[33 414 150 56 1987 10 0 53 205 6190]...][[414 150 56 1987 10 0 53 205 6190 0]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[33 414 150 56 1987 10 0 53 205 6190]...][[1 1 1 1 1 1 1 1 1 1]...][[33 414 150 56 1987 10 0 53 205 6190]...][[414 150 56 1987 10 0 53 205 6190 0]...]\n",
            "Generator is stateful.\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5791 1304 2 83 13 102 3150 7 228 189]...][[1 1 1 1 1 0 0 0 0 0]...][[5791 1304 2 83 13 102 10000 10000 10000 10000]...][[1304 2 83 13 102 561 1 178 99 4]...]\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5791 1304 2 83 13 102 3150 7 228 189]...][[1 1 1 1 1 0 0 0 0 0]...][[5791 1304 2 83 13 102 10000 10000 10000 10000]...][[1304 2 83 13 102 3150 7 228 189 99]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5791 1304 2 83 13 102 3150 7 228 189]...][[1 1 1 1 1 0 0 0 0 0]...][[5791 1304 2 83 13 102 10000 10000 10000 10000]...][[1304 2 83 13 102 1657 169 19 3 3]...]\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            "global_step: 1602\n",
            " perplexity: 513.001\n",
            " gen_learning_rate: 0.000039\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            " percent of 3-grams captured: 0.114.\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            " percent of 2-grams captured: 0.308.\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            " percent of 4-grams captured: 0.047.\n",
            " geometric_avg: 0.118.\n",
            " arithmetic_avg: 0.156.\n",
            "global_step: 1602\n",
            " is_present_rate: 0.500\n",
            " D train loss: 0.69258\n",
            " G train loss: 28.57785\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][1 45 3198 190 88 19 0 53 8 203 2 357 193 208 31 1547 229 5 188 6][1591 2 97 1 5211 6 53 2229 19 6]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5791 1304 2 83 13 102 3150 7 228 189]...][[1 1 1 1 1 0 0 0 0 0]...][[5791 1304 2 83 13 102 10000 10000 10000 10000]...][[1304 2 83 13 102 3150 7 228 189 99]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[5791 1304 2 83 13 102 3150 7 228 189]...][[1 1 1 1 1 0 0 0 0 0]...][[5791 1304 2 83 13 102 10000 10000 10000 10000]...][[1304 2 83 13 102 124 121 513 70 13]...]\n",
            " Sample: 0\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  properties          0.513        0.000        0.000        0.000        -2.069       -1.679       -0.000       \n",
            "   [1]  <eos>               0.517        0.000        0.000        0.000        -2.323       -2.048       -0.000       \n",
            "   [1]  there               0.519        0.000        0.000        0.000        -2.608       -2.127       -0.000       \n",
            "   [1]  is                  0.521        0.000        0.000        0.000        -2.928       -2.168       -0.000       \n",
            "   [1]  no                  0.525        0.000        0.000        0.000        -3.287       -2.178       -0.000       \n",
            "   [0]  under               0.527        8.199        -7.664       -0.641       -3.691       -2.137       -1.554       \n",
            "   [0]  much                0.530        6.043        -6.207       -0.634       -3.423       -2.068       -1.355       \n",
            "   [0]  robert              0.535        7.363        -9.487       -0.626       -3.131       -1.870       -1.261       \n",
            "   [0]  president           0.539        6.986        -6.497       -0.617       -2.813       -1.735       -1.077       \n",
            "   [0]  is                  0.542        6.743        -2.993       -0.613       -2.465       -1.735       -0.730       \n",
            "   [0]  a                   0.543        5.446        -3.155       -0.611       -2.079       -1.790       -0.288       \n",
            "   [0]  its                 0.543        8.737        -6.905       -0.610       -1.648       -1.786       0.138        \n",
            "   [0]  stations            0.541        3.442        -10.753      -0.614       -1.165       -1.844       0.679        \n",
            "   [0]  in                  0.539        10.403       -3.588       -0.618       -0.618       -1.904       1.286        \n",
            "   [1]  the                 0.539        0.000        0.000        0.000        0.000        -1.861       0.000        \n",
            "   [1]  researchers         0.537        0.000        0.000        0.000        0.000        -1.866       0.000        \n",
            "   [1]  who                 0.535        0.000        0.000        0.000        0.000        -1.933       0.000        \n",
            "   [1]  studied             0.531        0.000        0.000        0.000        0.000        -2.111       0.000        \n",
            "   [1]  the                 0.528        0.000        0.000        0.000        0.000        -2.169       0.000        \n",
            "   [1]  workers             0.527        0.000        0.000        0.000        0.000        -2.134       0.000        \n",
            " Sample: 1\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  <unk>               0.501        0.000        0.000        0.000        -1.760       -1.650       -0.000       \n",
            "   [1]  more                0.508        0.000        0.000        0.000        -1.976       -1.928       -0.000       \n",
            "   [1]  mature              0.513        0.000        0.000        0.000        -2.218       -2.047       -0.000       \n",
            "   [1]  markets             0.515        0.000        0.000        0.000        -2.490       -2.095       -0.000       \n",
            "   [1]  such                0.514        0.000        0.000        0.000        -2.796       -2.117       -0.000       \n",
            "   [1]  as                  0.513        0.000        0.000        0.000        -3.138       -2.117       -0.000       \n",
            "   [1]  the                 0.513        0.000        0.000        0.000        -3.524       -2.029       -0.000       \n",
            "   [0]  favors              0.513        4.824        -12.041      -0.667       -3.956       -1.948       -2.008       \n",
            "   [0]  business            0.514        3.435        -6.422       -0.666       -3.693       -1.797       -1.896       \n",
            "   [0]  it                  0.514        8.472        -5.157       -0.665       -3.398       -1.669       -1.729       \n",
            "   [0]  none                0.513        3.291        -9.223       -0.668       -3.067       -1.621       -1.446       \n",
            "   [0]  canada              0.509        8.811        -9.082       -0.675       -2.694       -1.595       -1.098       \n",
            "   [0]  and                 0.511        6.981        -3.247       -0.671       -2.267       -1.552       -0.715       \n",
            "   [0]  a                   0.513        7.258        -3.388       -0.667       -1.792       -1.488       -0.304       \n",
            "   [0]  cost                0.514        6.334        -6.853       -0.666       -1.263       -1.473       0.211        \n",
            "   [0]  of                  0.512        9.042        -3.097       -0.670       -0.670       -1.598       0.928        \n",
            "   [1]  way                 0.508        0.000        0.000        0.000        0.000        -1.842       0.000        \n",
            "   [1]  to                  0.505        0.000        0.000        0.000        0.000        -1.989       0.000        \n",
            "   [1]  get                 0.502        0.000        0.000        0.000        0.000        -2.019       0.000        \n",
            "   [1]  a                   0.501        0.000        0.000        0.000        0.000        -2.057       0.000        \n",
            " Sample: 2\n",
            "   [p]  Word                p(real)      log-perp     log(p(a))    r            R=V*(s)      b=V(s)       A(a,s)       \n",
            "   [1]  korea               0.500        0.000        0.000        0.000        -1.397       -1.662       0.000        \n",
            "   [1]  <eos>               0.504        0.000        0.000        0.000        -1.568       -2.069       0.000        \n",
            "   [1]  many                0.503        0.000        0.000        0.000        -1.760       -2.140       0.000        \n",
            "   [1]  <unk>               0.508        0.000        0.000        0.000        -1.976       -2.163       0.000        \n",
            "   [1]  regard              0.510        0.000        0.000        0.000        -2.219       -1.973       -0.000       \n",
            "   [1]  a                   0.511        0.000        0.000        0.000        -2.491       -1.795       -0.000       \n",
            "   [1]  u.s.                0.509        0.000        0.000        0.000        -2.797       -1.715       -0.000       \n",
            "   [1]  presence            0.504        0.000        0.000        0.000        -3.140       -1.722       -0.000       \n",
            "   [1]  as                  0.495        0.000        0.000        0.000        -3.525       -1.876       -0.000       \n",
            "   [1]  a                   0.489        0.000        0.000        0.000        -3.958       -2.033       -0.000       \n",
            "   [0]  greater             0.483        12.176       -8.146       -0.727       -4.443       -2.107       -2.336       \n",
            "   [0]  exchange            0.480        3.606        -6.799       -0.734       -4.172       -2.166       -2.006       \n",
            "   [0]  publishes           0.475        3.405        -10.928      -0.744       -3.860       -2.188       -1.672       \n",
            "   [0]  credentials         0.470        7.592        -10.166      -0.755       -3.498       -2.139       -1.359       \n",
            "   [0]  strong              0.462        9.329        -8.095       -0.771       -3.080       -2.086       -0.993       \n",
            "   [0]  should              0.457        2.758        -7.074       -0.783       -2.592       -2.069       -0.523       \n",
            "   [0]  <unk>               0.463        7.322        -3.150       -0.770       -2.031       -2.046       0.015        \n",
            "   [0]  of                  0.469        6.456        -3.601       -0.757       -1.415       -1.878       0.462        \n",
            "   [0]  the                 0.477        9.916        -1.134       -0.740       -0.740       -1.857       1.117        \n",
            "   [1]  the                 0.485        0.000        0.000        0.000        0.000        -1.897       0.000        \n",
            "Samples\n",
            "Sample 0 .  properties <eos> there is no under much robert president is a its stations in the researchers who studied the workers\n",
            "Sample 1 .  <unk> more mature markets such as the favors business it none canada and a cost of way to get a\n",
            "Sample 2 .  korea <eos> many <unk> regard a u.s. presence as a greater exchange publishes credentials strong should <unk> of the the\n",
            "\n",
            "\n",
            "targets[[944 4 3150 1 551 7 90 1647 8 61 1636 978 1 15 2 0 53 13 54 4][253 7253 5 378 144 2010 1 44 1127 3153 9 23 8312 2 0 2185 1 2946 1893 0][2836 75 171 157 0 4112 5 75 1 8985]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[246 944 4 3150 1 551 7 90 1647 8]...][[1 1 1 1 1 1 1 1 0 0]...][[246 944 4 3150 1 551 7 90 1647 10000]...][[944 4 3150 1 551 7 90 1647 8 61]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[246 944 4 3150 1 551 7 90 1647 8]...][[1 1 1 1 1 1 1 1 0 0]...][[246 944 4 3150 1 551 7 90 1647 10000]...][[944 4 3150 1 551 7 90 1647 896 1]...]\n",
            "targets[[0 261 6080 1618 10 175 32 34 6 206 739 4 3119 11 0 5533 1 5394 88 19][1553 1551 610 4 57 357 193 23 8312 2041 2 1725 97 53 116 150 7 2287 36 545][0 1089 16 31 4756 1 8 1 0 2836]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 0 261 6080 1618 10 175 32 34 6]...][[1 1 1 1 1 1 1 1 1 0]...][[4 0 261 6080 1618 10 175 32 34 6]...][[0 261 6080 1618 10 175 32 34 6 206]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[4 0 261 6080 1618 10 175 32 34 6]...][[1 1 1 1 1 1 1 1 1 0]...][[4 0 261 6080 1618 10 175 32 34 6]...][[0 261 6080 1618 10 175 32 34 6 312]...]\n",
            "Generator is stateful.\n",
            "targets[[46 2647 4 106 319 16 8421 4 0 6092 3574 2 64 34 102 4213 529 16 369 2365][5030 4 243 149 349 0 710 319 4 817 84 734 101 2 29 14 175 32 209 121][53 5 2313 52 27 1 8 386 318 23]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 46 2647 4 106 319 16 8421 4 0]...][[1 1 1 1 0 0 0 0 0 0]...][[431 46 2647 4 106 10000 10000 10000 10000 10000]...][[46 2647 4 106 23 101 96 449 8 0]...]\n",
            "targets[[46 2647 4 106 319 16 8421 4 0 6092 3574 2 64 34 102 4213 529 16 369 2365][5030 4 243 149 349 0 710 319 4 817 84 734 101 2 29 14 175 32 209 121][53 5 2313 52 27 1 8 386 318 23]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 46 2647 4 106 319 16 8421 4 0]...][[1 1 1 1 0 0 0 0 0 0]...][[431 46 2647 4 106 10000 10000 10000 10000 10000]...][[46 2647 4 106 319 16 8421 4 0 6092]...]\n",
            "inputs, targets_present, masked_inputs, sequence[[431 46 2647 4 106 319 16 8421 4 0]...][[1 1 1 1 0 0 0 0 0 0]...][[431 46 2647 4 106 10000 10000 10000 10000 10000]...][[46 2647 4 106 1055 4 611 1501 1697 5]...]\n",
            "^C\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j8hYuMZmmkcm",
        "colab_type": "text"
      },
      "source": [
        "## Generate samples"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iw-SWqvTmpsi",
        "colab_type": "code",
        "outputId": "1571bff2-39db-4c2b-da34-f73bd64f401b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/maskgan\n",
        "!python generate_samples.py \\\n",
        "--data_dir='/content/maskgan/dataset/iccv2017' \\\n",
        "--data_set=ptb \\\n",
        "--batch_size=256 \\\n",
        "--sequence_length=20 \\\n",
        "--base_directory='/content/maskgan/maskGAN' \\\n",
        "--hparams=\"gen_rnn_size=650,dis_rnn_size=650,gen_num_layers=2,gen_vd_keep_prob=0.33971\" \\\n",
        "--generator_model='seq2seq_vd' \\\n",
        "--discriminator_model='seq2seq_vd' \\\n",
        "--is_present_rate=0.0 \\\n",
        "--maskgan_ckpt='/content/maskgan/maskGAN/train/model.ckpt-1587' \\\n",
        "--seq2seq_share_embedding=true \\\n",
        "--dis_share_embedding=true \\\n",
        "--attention_option=luong \\\n",
        "--mask_strategy=contiguous \\\n",
        "--output_path='/content/maskgan' \\\n",
        "--baseline_method=critic \\\n",
        "--number_epochs=1"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/maskgan\n",
            "<eos>: 2\n",
            "<eos>: 2\n",
            "Vocab size: 10000\n",
            "\n",
            "Optimizing Generator vars:\n",
            "<tf.Variable 'gen/decoder/rnn/embedding:0' shape=(10000, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/softmax_b:0' shape=(10000,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'gen/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "\n",
            "Optimizing Discriminator vars:\n",
            "<tf.Variable 'dis/encoder/rnn/missing_embedding:0' shape=(1, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/encoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/attention_keys/weights:0' shape=(650, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_0/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/kernel:0' shape=(1300, 2600) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/multi_rnn_cell/cell_1/basic_lstm_cell/bias:0' shape=(2600,) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/attention_construct/weights:0' shape=(1300, 650) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'dis/decoder/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "\n",
            "Optimizing Critic vars:\n",
            "<tf.Variable 'critic/rnn/weights:0' shape=(650, 1) dtype=float32_ref>\n",
            "<tf.Variable 'critic/rnn/biases:0' shape=(1,) dtype=float32_ref>\n",
            "WARNING:tensorflow:From generate_samples.py:181: __init__ (from tensorflow.python.training.supervisor) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please switch to tf.train.MonitoredTrainingSession\n",
            "2020-02-12 16:08:52.867346: I tensorflow/core/platform/cpu_feature_guard.cc:140] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 AVX512F FMA\n",
            "2020-02-12 16:08:52.982732: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:898] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2020-02-12 16:08:52.983066: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1212] Found device 0 with properties: \n",
            "name: Tesla P4 major: 6 minor: 1 memoryClockRate(GHz): 1.1135\n",
            "pciBusID: 0000:00:04.0\n",
            "totalMemory: 7.43GiB freeMemory: 7.32GiB\n",
            "2020-02-12 16:08:52.983101: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1312] Adding visible gpu devices: 0\n",
            "2020-02-12 16:08:53.379572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:993] Creating TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 7072 MB memory) -> physical GPU (device: 0, name: Tesla P4, pci bus id: 0000:00:04.0, compute capability: 6.1)\n",
            "Restoring Generator from /content/maskgan/maskGAN/train/model.ckpt-1587.\n",
            "Asserting Generator is a seq2seq-variant.\n",
            "Restoring Discriminator from /content/maskgan/maskGAN/train/model.ckpt-1587.\n",
            "Epoch number: 0\n",
            "targets[[9971 9972 9974 9975 9976 9980 9981 9982 9983 9984 9986 9987 9988 9989 9991 9992 9993 9994 9995 9996][2 81 4 5729 699 41 4079 20 115 3313 9600 20 3715 4 0 2594 217 45 55 2793][996 9 494 242 43 0 35 6932 293 1]...]\n",
            "targets[[9997 9998 9999 2 9256 1 3 72 393 33 2133 0 146 19 6 9207 276 407 3 2][5 3 602 20 3 7 522 3 2 483 1854 203 555 6 167 4 483 1854 80 15][1 2676 2 0 181 6027 4 357 193 1]...]\n",
            "targets[[23 1 13 141 4 1 5465 0 3081 1596 96 2 7682 1 3 72 393 8 337 141][14 1241 6 402 7 435 1591 5 5174 350 1673 2 0 35 402 3495 7 1 43 3][67 5466 3598 193 1569 6713 1434 2 38 202]...]\n",
            "targets[[4 2477 657 2170 955 24 521 6 9207 276 4 39 303 438 3684 2 6 942 4 3150][1358 20 3757 33 333 897 991 8 7378 436 11 350 189 7 435 1591 0 37 15 2][1359 193 90 4 144 1 2560 26 0 5949]...]\n",
            "targets[[496 263 5 138 6092 4241 6036 30 988 6 241 760 4 1015 2786 211 6 96 4 431][0 402 33 980 350 1673 263 7 993 1580 8 2263 5295 2 0 4806 4 3897 1544 224][2 0 2075 4859 5 440 3 0 297 4]...]\n",
            "targets[[4115 5 14 45 55 3 72 195 1244 220 2 0 3150 7426 1 13 4052 1 496 14][65 19 6 6427 7 0 4381 85 1310 5 5992 1427 16 0 5976 8 1 4 27 141][35 92 36 409 2 144 193 99 631 11]...]\n",
            "targets[[6885 0 1 22 113 2652 8068 5 14 2474 5250 10 464 52 3004 466 1244 15 2 1][8 151 4459 8400 1544 2 63 86 13 442 4 0 35 37 9 1067 1043 2281 1083 5][5 320 51 593 99 157 0 297 4 679]...]\n",
            "targets[[80 0 167 4 35 2645 1 65 10 558 6092 3574 1898 666 1 7 27 1 4241 6036][23 1544 105 13 27 1671 3043 2 1674 527 22 0 125 8 111 473 16 0 1499 3897][3153 2738 4211 82 95 35 92 2 1116 73]...]\n",
            "targets[[7 3 2 366 1976 3178 46 220 45 55 6 40 195 0 467 342 1292 7 325 9][870 10 1544 319 80 33 4403 0 511 12 3 21 7 604 14 13 2065 0 35 191][34 959 597 20 0 60 47 94 0 72]...]\n",
            "targets[[35 1491 916 4 3199 6 8967 371 5 1141 35 1411 5 0 434 2 6 1 1 15][66 23 1544 3286 36 66 0 1 806 28 2540 13 6619 2 0 1674 59 15 10 366][677 1547 5 184 2 8 0 365 26 7736]...]\n",
            "targets[[39 13 31 393 1366 2 64 275 1921 43 72 195 157 1442 2395 4 3150 718 106 5791][0 1 23 1544 30 58 671 16 0 806 11 45 55 363 72 0 6369 1043 13 18][2686 516 4595 116 5 7377 8 5 841 6]...]\n",
            "targets[[1304 2 83 13 102 3150 7 228 189 99 2 1400 1 1415 0 1244 56 4375 0 431][296 204 40 597 20 6 1144 3734 9300 2 1206 83 34 58 102 455 11 0 6369 105][1 45 3198 190 88 19 0 53 8 203]...]\n",
            "targets[[46 2647 4 106 319 16 8421 4 0 6092 3574 2 64 34 102 4213 529 16 369 2365][289 328 0 37 44 14 13 1921 22 249 2192 2 120 97 4 0 1481 46 2329 67][5030 4 243 149 349 0 710 319 4 817]...]\n",
            "targets[[26 18 645 15 557 941 1 4 651 9 1 1015 758 2 978 1 658 6 773 4][1 1544 319 74 411 0 3897 7 93 0 1 14 3347 5 0 604 50 32 58 159][5 188 5926 2 389 8 819 3187 89 1]...]\n",
            "targets[[1244 20 0 162 1015 758 8 0 766 1647 4 3483 596 8 651 596 2 0 1 1047][273 314 122 2 64 143 32 34 121 4 6 2161 1544 224 9 151 142 290 9705 1][2153 5 5550 45 55 0 2773 47 2 67]...]\n",
            "targets[[15 3150 24 263 7 253 1451 1351 7 423 398 11 0 6036 7 0 266 5278 8 3122][15 7 31 1224 2 0 2573 13 10 8400 13 0 151 4459 4 0 6369 8 349 299][1223 0 1430 293 1621 43 3 3 8 0]...]\n",
            "targets[[22 6 769 2155 4 1 7 3 2 20 3 5 3 3 48 6092 3574 22 0 6036][14 79 63 25 787 2 1544 319 143 63 358 5 293 6 806 10 143 63 535 8400][46 8751 45 55 90 149 78 0 3 1037]...]\n",
            "targets[[46 238 0 37 15 2 211 3 945 56 1389 1101 22 0 5545 3 34 2214 45 55][2 0 1674 59 15 10 1544 224 5065 1 979 204 12 3 21 7 604 933 130 311][382 99 13 10 97 26 77 18 4718 3699]...]\n",
            "targets[[132 421 0 169 297 2 346 4 0 258 6661 431 34 1 4464 210 132 22 376 1][2 29 23 1 340 10 6 1 2952 2 0 963 16 0 1509 4 0 3897 988 1544][77 18 31 196 1176 4 45 55 3 3]...]\n",
            "targets[[1015 2 0 345 4 3 2786 20 7536 1 5172 1015 8 1 24 289 206 55 169 0][319 60 5 2134 12 3 122 5 317 18 12 3 7 35 92 60 111 429 77 2][488 218 18 31 196 1256 4 3 3 2]...]\n",
            "targets[[1244 15 2 0 1 158 13 6 4196 1883 211 150 4 505 56 938 1 4464 15 978][164 664 122 10 1544 319 9 528 5 3288 27 12 3 21 1 953 5 23 1544 9][40 34 1324 121 45 1056 55 0 243 149]...]\n",
            "targets[[1 2 0 760 4 5172 1015 2786 211 0 431 18 0 379 1 1713 398 2025 1310 5][2229 33 8766 6 7215 4 0 35 37 2 14 30 5 25 989 19 31 579 645 11][11 193 10 2096 7 190 10 26 7595 248]...]\n",
            "targets[[25 0 1590 11 106 3150 431 4375 7 650 6080 672 28 15 2 0 402 41 13 1023][0 504 15 3185 3300 1 4 1 96 80 3293 2 1544 224 33 25 6 2779 60 28][2 29 57 569 193 376 34 9422 2508 293]...]\n",
            "targets[[17 1 82 1 95 24 124 315 22 1 5 138 0 4241 6036 2 0 1883 684 33][15 2 110 705 620 8400 89 87 14 554 36 110 87 32 2 2717 0 4459 9 1065][30 58 4383 17 173 116 19 6 262 690]...]\n",
            "targets[[414 150 56 1987 10 0 53 205 6190 0 1354 4 3150 210 1 45 1 55 0 246][61 645 1424 11 23 1544 9 35 37 535 0 6369 9 8483 1 2083 503 2 0 1134][249 35 193 10 26 32 113 1144 2306 396]...]\n",
            "targets[[944 4 3150 1 551 7 90 1647 8 61 1636 978 1 15 2 0 53 13 54 4][1674 5648 150 1586 41 26 159 4 1 1 19 213 105 6042 8 3999 38 33 1211 656][253 7253 5 378 144 2010 1 44 1127 3153]...]\n",
            "targets[[0 261 6080 1618 10 175 32 34 6 206 739 4 3119 11 0 5533 1 5394 88 19][1 2485 548 2 7 487 0 6369 33 3810 3 6581 1981 19 97 19 0 413 264 4381][1553 1551 610 4 57 357 193 23 8312 2041]...]\n",
            "targets[[1 10 26 6349 19 1 214 5 1 2239 1 6 2035 4 1 18 0 596 4 5042][2 1544 224 59 33 663 3024 869 63 86 20 1544 319 41 30 43 3 3 4 0][817 1 2303 93 25 245 1 5 1118 206]...]\n",
            "targets[[1488 4 3199 2 45 246 1 5394 26 1 8 26 45 2125 1402 17 0 2839 978 1][1408 4381 47 8 41 13 169 5 2360 84 0 1 1043 6 1187 3806 22 0 6369 7][9551 2989 16 0 193 1615 347 23 8312 44]...]\n",
            "targets[[3967 2 7 565 0 1084 957 302 3091 6 8113 2050 16 1775 73 1870 4 3150 2 17][3 2 0 3897 59 33 2843 22 197 85 753 65 8 203 9 129 132 4295 555 2550][1 18 0 1 77 790 4 57 357 193]...]\n",
            "targets[[3 511 73 1435 1870 4 1 3150 33 25 9231 2 43 3 431 18 6 2025 10 159][65 8 2668 1475 2 0 35 37 15 14 1094 83 26 1736 55 3 648 526 11 1][4458 3218 16 0 129 149 7 97 193 144]...]\n",
            "targets[[398 11 0 6092 6036 46 4115 5 3150 7 0 5278 2 874 4 0 2025 46 782 9673][424 178 12 3 21 8 12 3 21 9284 0 6369 115 880 2 124 520 4 0 3897][53 2 10 229 116 89 2602 182 0 193]...]\n",
            "targets[[251 0 1 24 263 2 431 3820 380 1 1 4 0 3021 1681 91 6 828 1 7070][1544 319 3641 26 5 936 54 1544 224 62 11 538 75 1544 319 71 38 194 7 6][7506 7 99 5 182 0 193 1 9 23]...]\n",
            "targets[[7 3260 8 1 5394 8 1 1765 0 4893 5394 7 6 797 263 5 138 6036 2 431][1784 169 5 3295 7 43 75 422 2 102 115 11 0 35 71 30 58 388 2 595][13 10 150 56 433 5 0 730 291 50]...]\n",
            "targets[[2166 7934 4 2718 7389 10 9029 94 935 4 0 2025 113 328 1 3013 1 0 448 2][0 101 33 1237 14 52 5 0 3029 5 1880 2 1544 224 30 2469 5 218 16 1344][383 322 26 478 1600 711 43 494 7669 17]...]\n",
            "targets[[83 9 102 798 10 57 4 150 431 8 586 5351 1 4464 15 1 1890 232 70 4][2 164 9870 1544 224 9 1067 1285 310 18 43 12 3 6 62 2 702 22 0 953][5 0 53 98 0 139 234 15 2 66]...]\n",
            "targets[[1222 1063 11 1 82 1 2 29 110 34 5 4173 10 144 1466 453 665 3 72 195][1544 319 13 1 43 12 3 21 7 294 1558 150 865 5 0 6369 442 41 30 58][383 128 5 25 238 7 0 53 2 227]...]\n",
            "targets[[2 14 30 102 5596 16 228 221 534 325 2 927 16 3730 1359 193 608 5 2276 1633][6 5109 16 1544 319 9 136 2 1 1671 5006 1506 464 762 1544 319 4279 0 3897 2][15 10 14 9 306 266 5 117 369 10]...]\n",
            "targets[[1692 10 683 586 523 391 1249 7 131 172 2 0 196 5227 4096 236 4 0 3 3542][349 0 6369 319 8 442 1158 0 37 42 34 58 583 5 237 6 177 4 12 3][15 14 30 3207 6 383 104 5 670 6]...]\n",
            "targets[[193 7192 17 1 9 161 293 237 2851 6 6913 4 6 760 374 5 3 3 20 3][21 11 0 74 387 4 3 642 55 0 12 3 21 14 635 2 16 0 61 1051][243 233 1699 24 678 69 1212 8 1241 7]...]\n",
            "targets[[3 11 0 123 327 475 2 4096 927 2288 6191 4 1422 8 10 0 264 236 1049 11][50 14 8065 223 1544 224 42 34 4299 6 12 3 21 252 2 23 1544 56 79 32][1156 1497 551 6 229 5 1945 0 12 3]...]\n",
            "targets[[6 40 2 196 1531 4 0 193 134 744 1 17 6 272 5 3 171 0 8210 155][25 697 11 469 33 221 11 0 35 3576 5239 5955 37 19 31 829 3362 0 2768 28][962 1 98 157 0 4649 5003 338 7 3]...]\n",
            "targets[[266 397 214 5 8024 9 2 917 4320 26 948 5 2176 1607 131 172 76 38 3304 683][50 22 1544 319 2 4021 19 0 2388 4 0 4381 23 1544 24 489 12 3 18 1544][19 4610 14 9 2132 11 1013 5 2384 353]...]\n",
            "targets[[586 5 2457 1308 206 172 11 6 917 341 2 8411 4320 26 989 6 1230 4 1122 172][319 69 40 2 18 1544 224 28 33 25 489 12 3 2 2717 2401 1544 8 1 61][26 449 4 0 219 104 8 197 1452 293]...]\n",
            "targets[[76 683 586 89 6797 206 172 4797 2 0 196 1531 11 193 670 86 5 822 989 17][444 265 18 0 37 794 7033 1 3 70 8 151 154 290 1971 1657 1 3 232 70][1627 1 3364 22 61 672 7 181 72 18]...]\n",
            "targets[[57 5 25 6 1947 4922 76 150 586 2245 0 47 1101 697 6 241 374 11 0 40][1299 2929 941 1 3 232 70 995 8 2383 1173 1 3 232 70 3600 2 73 544 20][1 3364 19 0 862 5 9383 0 1236 755]...]\n",
            "targets[[3 171 2 2937 15 1 1 1 1704 4 161 293 237 927 93 1 52 554 157 38][1544 319 2 1544 224 41 488 4109 3 108 15 14 405 6 221 534 4 3 17 0][1417 161 7 0 53 314 38 1945 1680 4]...]\n",
            "targets[[1 118 76 4 181 4783 7 966 131 172 2 0 236 16 2015 281 613 238 18 381][235 4 3 2 400 1173 5018 3 72 393 24 521 444 154 232 70 8 151 298 290][219 984 1908 1 156 2 7 204 7098 10]...]\n",
            "targets[[9 960 11 471 119 5 3 3 20 3 3 2 485 181 1249 7 927 116 385 5][179 35 1384 2 28 33 385 5 237 5 2435 1 70 8 151 154 290 2 23 5018][15 10 455 11 5175 803 8 508 16 606]...]\n",
            "targets[[9273 267 91 161 193 2 294 4 0 3 3542 193 1569 17 12 3 48 198 0 467][24 154 232 70 4 39 1 478 37 2 2838 941 9011 3 24 521 154 232 70 4][4 1943 265 15 27 467 1112 1301 10 0]...]\n",
            "targets[[123 5 12 3 48 2 1461 9175 927 2770 1991 966 744 76 683 586 89 3238 4320 8][0 37 2 28 24 719 70 4 0 37 9 733 2796 95 167 2 400 673 1 3][2 27 216 6473 52 5 3 3 7 522]...]\n",
            "targets[[386 78 0 1590 172 2 0 524 161 193 26 488 4406 200 94 3 3 2 6010 1408][24 521 5 2280 23 9011 19 70 4 733 2796 2 719 28 24 232 70 4 733 2796][3 2041 0 736 1070 13 925 1607 2 0]...]\n",
            "targets[[332 0 1 293 50 6 5227 4096 236 4 3 3 198 0 467 123 118 20 3 3][2 513 3300 1 3 24 521 444 232 70 4 733 1726 2 28 24 719 232 70 2][522 78 346 126 4 460 2 3376 9614 12]...]\n",
            "targets[[6 123 133 2 14 9721 1427 7 7381 125 1060 8 13 488 1 265 788 41 5074 27][0 53 3937 57 1406 7 27 218 1 4175 435 1591 2827 8 3632 5296 20 6 1396 4][0 12 3 48 7 397 0 1203 234 15]...]\n",
            "targets[[236 2 0 196 5227 2235 236 4 0 3 193 24 3 3 118 20 3 3 2 0][672 14 13 1101 2974 11 3145 3378 5 3978 53 4758 1 8 61 1 536 2 227 258][11 180 803 17 533 3687 2025 455 42 34]...]\n",
            "targets[[3782 2235 236 202 5 31 196 3 3 20 3 3 0 3782 4096 236 2824 5 31 196][61 672 637 9490 2612 2772 8 1430 33 707 16 10 1363 2561 2245 1396 19 6 410 4][15 606 508 2456 18 31 367 158 4 12]...]\n",
            "targets[[3 3 20 3 3 2 8171 1 232 141 4 1 3599 82 95 41 1191 6 3 3][31 4497 1277 53 218 3220 9625 2396 411 2 124 0 35 53 218 278 150 672 79 663][220 11 397 2 549 606 508 24 118 29]...]\n",
            "targets[[131 7 39 1 37 24 1234 6 276 2 28 2040 1 673 1 2390 6 1 3599 232][7240 1 4301 8 5543 218 3415 66 38 87 32 1146 51 957 4 5155 813 17 130 1212][660 46 1697 5 3219 0 1676 4 1774 3756]...]\n",
            "targets[[141 56 1245 2 1 3599 1191 132 4 3599 770 9 914 146 1829 2 691 74 142 65][2 659 2396 15 97 4 0 3 672 10 163 1686 124 1 6381 4 5001 34 159 5134][1188 11 1437 65 6 2776 104 15 436 11]...]\n",
            "targets[[15 590 678 27 468 17 2754 1 555 4 2112 11 12 3 6 62 36 12 3 21][2454 16 39 9503 185 2 163 15 83 13 774 1 384 0 219 10 1 4 1 536][18 0 312 103 0 470 7 131 172 155]...]\n",
            "targets[[2 0 1073 478 37 15 14 405 5 2142 1388 724 8 1489 0 566 17 4404 2 1][1 73 77 1618 8 782 0 5976 8 1 4 31 1 357 9 194 2377 2 53 218][2 114 1070 13 6223 3478 5 2313 52 0]...]\n",
            "targets[[197 80 15 27 1 82 1 167 787 0 226 4 27 1 1542 222 5 1 7120 941][4948 1987 10 672 22 5145 1 11 1 536 79 25 5690 873 17 5999 51 194 1483 8][90 1315 0 507 9 438 1070 13 99 774]...]\n",
            "targets[[11 12 3 21 2 1 13 31 3489 3422 478 37 22 776 7 0 8230 1299 153 2][4624 8 17 1 53 8135 365 20 2178 36 610 51 598 189 83 2 659 2396 1 435][311 2 105 143 0 100 720 146 9 1]...]\n",
            "targets[[1 1542 304 7 1 1572 558 5959 438 1542 417 2 14 4109 3 108 8 30 367 186][1591 11 2343 31 1 2017 534 8 656 2532 2969 4 1401 2679 8 1595 7194 5 2273 1593][354 13 43 5 5012 91 965 2 38 6806]...]\n",
            "targets[[4 43 12 3 21 2 0 100 98 1949 81 4 53 745 128 76 286 30 32 4141][8 1285 1 2 3757 59 30 6482 1143 1 2746 5 868 144 2969 163 15 2 2827 30][6 2528 2 2397 26 1101 3435 11 88 7324]...]\n",
            "targets[[0 2081 16 98 207 2 314 286 2832 0 98 30 32 106 1283 5 185 35 207 4524][1300 27 2760 22 0 53 17 1 6 1 4449 331 1 27 9508 278 8 6485 804 5][1 5979 7 359 10 89 780 5 6 965]...]\n",
            "targets[[4 106 944 0 281 15 2 0 98 9 2840 1283 556 18 5732 475 5 12 3 1746][1865 243 1593 855 20 7203 1 4 51 4282 2 10 854 79 1 5251 9 774 297 4][16 10 1907 14 15 2025 2397 202 3 3]...]\n",
            "targets[[20 12 3 1746 2 804 5 3494 0 207 2081 13 1 7 0 1447 94 1929 1369 958][300 1 1 5 254 1593 855 11 1666 51 4282 2 3632 5296 11 27 201 30 5570 5][1 5 0 2693 4139 2952 15 8895 1 31]...]\n",
            "targets[[2 0 165 30 1950 5 630 0 2081 5 12 3 1746 29 0 463 13 32 169 5][8044 6 4449 278 7336 22 197 1405 8 5 2424 0 278 5 224 995 19 200 19 5][87 32 378 106 1692 10 2397 26 4471 2]...]\n",
            "targets[[755 314 130 123 18 0 7390 2 0 281 15 0 53 33 2381 16 407 3 66 286][7000 1321 659 2396 15 2 144 132 672 26 32 3360 170 0 8140 328 2 38 33 707][349 810 5 6 965 2 0 234 15 455]...]\n",
            "targets[[175 32 755 17 223 2 5333 888 1 24 521 444 232 70 8 187 609 4 39 53][16 6 1 1396 10 794 3 61 672 2 150 672 210 203 1970 725 6056 8 2508 26][72 202 3 3 7 311 5 12 3 48]...]\n",
            "targets[[81 8 610 2837 4 173 614 339 5453 993 584 2 7 0 35 532 28 33 8285 5453][147 4 57 287 5 0 53 29 26 6833 5 8316 1 355 11 140 2064 8 4449 1276][3369 803 46 52 3 3 5 12 3 48]...]\n",
            "targets[[9 53 81 336 935 8 610 222 2 719 23 1 3 72 393 24 187 610 609 4][55 150 16 0 2561 1396 2 3185 4490 6 514 1357 7721 7 1 681 15 0 1722 4][719 452 10 1 455 202 3 3 7 311]...]\n",
            "targets[[1956 65 9 1956 537 2 28 50 58 6 81 8 610 154 22 1956 11 3 72 2][53 1 1541 22 6 774 3409 10 5204 5155 813 13 7 6 357 9 194 131 2499 0][78 1122 3 3 7 397 2 2323 34 58]...]\n",
            "targets[[67 14 9 103 11 51 1 1 0 507 9 736 1 1461 4730 170 5 0 1 1][3487 159 17 435 1591 2827 8 3632 5296 2 114 39 3329 505 13 10 53 218 278 13][134 7872 4 9535 455 119 3 3 7 311]...]\n",
            "targets[[4 3222 6682 166 1 1 8 2805 5239 2 63 39 40 2 0 162 441 4 930 1627][671 28 15 2 28 15 1430 79 25 54 4 0 130 672 5 25 4175 20 0 2561][180 803 1070 2 2302 144 455 7872 412 3]...]\n",
            "targets[[16 0 1 180 4 4921 11 27 616 146 474 2 8 0 285 1102 5 4049 27 5684][1396 76 4 27 700 5 7971 6 35 2064 278 2 659 2396 15 10 0 53 13 147][15 3410 606 41 903 11 440 387 4 73]...]\n",
            "targets[[45 166 8381 36 2885 6221 55 2025 1276 2 0 1162 4 904 5 2007 5 3 352 528][1490 43 6863 3078 7 3650 8 1017 1199 2454 7 3394 2 163 143 32 2530 366 133 53][367 158 4 12 3 48 2 692 1 1188]...]\n",
            "targets[[765 10 0 8700 16 0 1 7883 13 32 105 1 78 73 10 14 9 6 262 665][218 660 34 2525 4 8520 1 7 3394 8 1 11 53 1942 4758 7 3650 2 0 3][172 1413 33 961 508 16 7708 1512 29 684]...]\n",
            "targets[[11 6 37 5 1707 2 16 0 3627 235 4 0 2620 46 160 20 2172 166 2727 2871][218 755 2408 659 2396 5 185 204 1277 4 0 767 4 144 672 17 850 3 2 105][606 24 170 3 3 5 31 367 158 4]...]\n",
            "targets[[8 1 702 22 6996 1 166 1 601 8 0 1271 8342 1 2025 2 11 1 0 491][289 659 2396 30 32 6833 106 681 884 543 5 5731 31 7240 1342 124 0 1363 656 3][606 508 119 3 3 5 12 3 48 2]...]\n",
            "targets[[2028 1824 785 2174 1 3387 11 31 3697 4 0 4921 1 1 8 6 7458 1 5270 1][1210 4 0 755 2 7859 15 14 33 1903 4252 247 5 1 27 243 207 4 12 3][143 32 428 7 311 2 11 0 74 510]...]\n",
            "targets[[2 7922 8 1 1159 2 0 130 1207 22 6 1401 1 1 4 491 8 51 7226 1][48 0 1 7 0 1587 219 2 0 6379 17 354 1044 1 1 13 1539 5 25 0][3 625 69 40 9 461 2 0 98 9]...]\n",
            "targets[[5 0 4921 993 1 1 17 1723 36 1012 5167 2 0 2670 79 32 138 14 105 0][74 103 88 31 587 30 58 340 11 17 31 1 454 4 88 1 2 0 2811 140][0 123 17 5454 80 9 1 4890 96 2]...]\n",
            "targets[[1 2670 6270 0 656 5684 2 6 9620 6330 24 416 7 0 4002 251 622 8 6870 26][507 30 489 253 326 16 27 207 155 266 69 40 2 7859 1 5 1362 6 1500 4][7 311 2 0 1 3814 161 19 14 13]...]\n",
            "targets[[3668 5 6892 3239 2 223 7 0 5684 134 3978 0 1 1 84 346 4268 6823 8 113][3 3 7 0 310 4 27 6030 207 23 1 15 148 49 301 1 1 2 23 1][98 794 161 1153 16 3410 1 4890 175 32]...]\n",
            "targets[[0 454 4921 3 5901 11 6 1 6024 2502 2 78 0 2502 6431 3 491 1 166 1][1316 7 397 22 53 1634 281 723 692 6530 2 1 9753 4239 1 24 7 514 8 35][6 1628 354 14 13 32 1 31 4919 965]...]\n",
            "targets[[94 0 699 8 4268 2 102 1 0 4268 3870 84 38 147 50 973 16 51 753 11][92 39 123 5 897 22 247 2 23 1 376 30 15 0 98 4 70 4239 1 56][8 276 4 1314 265 18 8305 7897 80 4193]...]\n",
            "targets[[204 5538 9 816 36 75 2 192 3368 0 1 4370 7 6 261 1939 18 0 1709 157][453 284 565 3 4699 6 891 1500 4 1517 8 131 13 0 86 229 0 207 434 93][7 0 615 3 3 880 11 249 126 5]...]\n",
            "targets[[1 0 5927 554 2 39 103 14 24 11 4679 8 1 6 1035 597 2 124 0 6221][25 7717 2 29 28 30 63 15 157 10 0 357 825 387 0 207 1 2 198 27][35 1042 10 0 507 9 1446 240 328 147]...]\n",
            "targets[[8 1 4 0 1 5425 1 1 510 4 0 6467 8739 7 1745 564 127 5425 1 1][5329 40 0 330 321 916 33 237 1466 4 0 225 1732 10 1419 19 6124 4 140 85][1943 586 220 500 1446 455 7 522 118 20]...]\n",
            "targets[[1 1 1 1 8 1 1 22 6 1 1 2 5705 6 1 8 809 1 67 38][964 2 132 563 10 969 0 663 4 629 4098 46 1613 7 3 2 10 40 0 1524][455 46 118 69 168 370 22 3 3 0]...]\n",
            "targets[[4895 54 0 491 1220 0 8739 6 2760 1 2 45 55 6 261 1 117 0 1 1368][1908 4244 6156 8 3907 1 544 5 47 2 0 563 46 1879 17 325 9 1405 2 1524][1042 10 618 13 124 350 2 11 0 2734]...]\n",
            "targets[[1 127 5 502 5 6 1 285 11 506 1939 2 29 11 99 38 275 735 3478 5][1908 1276 11 471 50 5 269 51 761 2234 19 5000 8 1 451 16 1 2 29 1524][1660 202 2 0 460 24 113 1 55 7]...]\n",
            "targets[[51 2706 474 1 7 1785 2 435 1591 2821 6 218 820 4 12 3 21 7 522 1307][1908 24 6 137 1631 20 1524 68 41 24 1095 7 6 8982 17 1897 1 8 3231 1147][45 913 7 522 55 38 50 11 215 4]...]\n",
            "targets[[0 357 9 233 1 214 5 98 652 1625 915 2 1976 1 17 0 218 8 153 1240][11 1 88 19 0 1 224 2053 2 7 487 0 1524 1908 24 31 8592 12 3 2][10 1 2819 26 1 2 67 436 13 1947]...]\n",
            "targets[[994 204 218 820 7 522 0 2734 1496 4035 39 40 5326 6 7933 16 435 1591 9 1][1879 19 38 46 144 266 4759 2363 8931 519 442 7 5367 1911 11 0 318 8 284 2][5 499 2 0 1943 586 134 237 13 304]...]\n",
            "targets[[354 2 1000 7 522 3046 18 12 3 48 6 4512 3 3 244 20 6 40 133 120][129 2928 563 11 85 50 58 384 11 72 2 29 0 35 3 4759 1725 133 1 2242][215 4 0 1112 9 3092 1 0 1568 178]...]\n",
            "targets[[943 368 1056 5 12 3 48 52 3 3 20 69 522 2 435 1591 9 233 2521 41][88 19 0 1 1 8 1 50 1 8 79 1365 43 75 1975 4 451 7 51 3200][8 0 297 2195 6 8557 2 11 0 74]...]\n",
            "targets[[542 7 3 1898 39 40 76 4 4972 701 4887 218 5347 8 2637 1000 2 98 160 15][2 264 4759 26 45 55 3 421 2346 8 34 2492 1131 3 421 1221 55 51 3 4659][551 10 4 0 3 3 56 2175 3 3]...]\n",
            "targets[[1000 18 0 235 4 0 40 42 707 124 6 98 1320 4 12 3 48 2 485 0][2 83 46 97 4010 4339 1 2 785 4710 8 1557 2834 7 3 1250 31 266 1 274][38 3021 245 55 0 559 168 2 120 8582]...]\n",
            "targets[[8110 1469 435 1591 30 2503 6 218 2826 4 12 3 21 105 289 39 40 2 20 1133][11 4759 8 4710 977 31 153 6776 363 72 78 688 5888 54 4 144 3777 7 3 2][1 15 14 175 780 110 5 3905 943 26]...]\n",
            "targets[[5 522 0 507 9 8581 1000 368 3 3 20 0 312 341 69 40 5 12 3 48][1731 2089 1 488 141 4 7132 503 658 0 773 10 1250 0 2791 2060 11 4759 2 3466][1473 1615 19 213 7 693 931 9212 86 43]...]\n",
            "targets[[2 943 46 18 12 3 48 52 3 3 2 6540 547 5 585 1887 22 2147 103 781][1 8 6827 1 75 2426 2731 46 1 4 0 1330 1 10 893 4759 5 62 451 646][1 2 14 9 31 4527 902 5 335 16]...]\n",
            "targets[[411 35 626 172 11 3 8 15 14 33 2544 6 35 3710 176 11 1698 2 0 35][0 732 2 688 0 219 860 7 563 143 32 208 27 74 4339 314 397 3 19 97][0 334 168 7 6 3630 622 6581 220 6]...]\n",
            "targets[[694 176 20 6540 6 167 4 0 514 1100 95 13 0 334 3710 176 0 781 30 492][61 101 2127 0 47 2 325 4339 2323 1751 345 57 12 3 48 1408 2 1 1 82][11 7982 189 18 6 103 4 8925 241 53]...]\n",
            "targets[[1698 7 132 72 2 231 10 437 1698 4261 11 4317 36 991 694 508 34 427 2870 1][95 31 1752 3507 37 15 27 1 80 2328 742 1 80 11 12 3 21 2 1 13][2 1 1 7 35 92 1264 5 39 1048]...]\n",
            "targets[[18 0 309 1 8 9532 0 6421 869 178 6540 103 578 80 9 103 781 8 1 1809][6 35 1 3507 287 10 1626 189 124 0 1 5706 2 1 15 14 832 3 3 4][508 7 2584 4 603 18 3320 1697 367 172]...]\n",
            "targets[[1 9 53 309 82 219 237 2 1731 1 376 521 6540 70 15 6540 9 694 172 42][1 9 2285 60 8 30 31 331 5 866 31 579 3 3 2 10 320 1201 22 27][930 7 2584 4 603 3320 1697 2 8182 20]...]\n",
            "targets[[244 3 3 7 1133 2 6 632 1 3735 7 6540 33 361 12 3 2 7 4514 103][950 1011 60 886 1493 1 0 382 5 244 27 131 5 3 3 4 1 9 2285 60][1 3 1975 12 3 3452 6324 16 179 1802]...]\n",
            "targets[[781 2181 27 2260 2903 158 976 11 3 120 63 991 694 3735 172 22 6 230 2903 976][2 268 359 20 1248 9 3931 1 2170 33 25 749 17 3 1841 6 272 5 43 3][388 7 203 0 3502 9 1 13 511 3172]...]\n",
            "targets[[103 9 694 158 33 25 2853 3 3 206 1040 9459 6 632 3735 7 103 270 43 12][1841 22 0 2225 4 0 1 1103 0 74 4 258 300 2170 834 5 25 1130 91 359][1809 1 8 2245 4435 8705 9343 2 38 1336]...]\n",
            "targets[[3 2 53 309 30 396 5 2583 27 3 694 172 2 6540 15 14 33 2544 0 2903][157 0 235 4 3 2 8916 1248 555 6 167 4 35 2645 1678 65 8 2774 2062 1][2020 43 51 6798 4465 306 121 8 2964 148]...]\n",
            "targets[[353 176 41 1 973 2999 5 1698 16 9340 626 2 0 781 33 5217 22 3735 3932 1698][1886 0 2170 7 6 795 818 2 8916 15 0 1 1103 892 359 475 2 1456 33 25][11 31 140 4982 201 4 0 1 4 39]...]\n",
            "targets[[56 7 3 897 36 2604 51 3 508 19 351 19 38 1153 12 3 7 3 8 12][4909 368 314 14 4349 43 3 1841 6 272 2 0 1103 30 824 4 3 21 1841 2][0 1 1437 4 5350 140 1 2 14 9]...]\n",
            "targets[[3 7 3 2 23 1 15 0 176 13 63 31 1034 5 4366 52 6 460 7 694][824 11 0 258 35 2170 345 3 21 1841 2 0 1 8 1 2170 26 169 5 900][32 5913 5 0 1 1 722 4 4604 56]...]\n",
            "targets[[1975 7 0 74 510 126 4 3 6540 9 694 1975 1309 3 6 470 4 3 3 20][1892 266 130 40 8 0 1 8 1 2170 466 130 40 2 8916 15 0 2170 46 1250][2 66 6 2978 9401 2082 2430 31 8148 2620]...]\n",
            "targets[[69 40 214 5 3872 529 1463 2 114 2099 13 114 1698 26 1118 1040 3735 8 7 10][78 0 1752 98 1102 7 3 5 138 0 74 3 21 1841 20 35 2170 809 4 1][166 505 55 90 4 505 316 2 10 9]...]\n",
            "targets[[234 64 26 839 1818 39 616 15 23 1 2 179 6540 8 53 309 34 58 5130 2903][217 2 1 1 65 15 14 787 0 12 3 21 226 4 27 1280 4007 661 5 6][9401 2082 13 9812 7 3747 2 14 9 1]...]\n",
            "targets[[7 181 72 349 772 269 4 1289 1 5 3538 88 19 9836 36 5871 2 227 1913 4][96 658 17 0 167 9 70 1270 1173 6215 8 61 586 2 572 0 468 4 1 1][170 11 8422 357 7 3417 4 31 1 9401]...]\n",
            "targets[[0 129 132 1 2503 2903 552 376 2 214 5 3060 1463 4 1 103 0 413 1 50][17 6 588 96 658 17 937 1238 1340 133 39 40 0 339 4 1 1 1102 5 1][6 1 1 1 22 6 7159 3465 2 28]...]\n",
            "targets[[196 2903 4 3 6 4454 4 3 3 2 6540 9 2903 11 0 74 363 126 4 3][682 4 425 4 27 1 432 2 0 226 4 1280 4007 13 6 201 4 0 156 2][199 19 996 29 6 1 1 2 702 0]...]\n",
            "targets[[24 3 1219 20 0 312 341 69 40 2 53 309 134 2903 7 0 312 103 24 3][0 446 165 15 70 256 30 678 8035 1368 11 943 4 425 2242 4 5871 10 26 32][6941 4711 9 1685 297 8 0 9401 896 6]...]\n",
            "targets[[118 3 3 2 35 1491 934 274 9616 84 4 0 1487 11 273 336 95 4 35 2261][1110 7 891 4772 7 0 53 0 7802 4928 8 61 53 1 2 0 587 544 7 1120][0 7827 23 1 13 6 1596 1 7 203]...]\n",
            "targets[[721 10 0 1481 46 306 241 8 0 648 1 306 289 7 0 506 5 4931 6 206][5 6 5769 527 17 1 80 11 512 7 0 53 1 274 4 6577 11 943 20 1587][3 899 5 25 1 7 1 1 30 238]...]\n",
            "targets[[208 2 0 329 3286 561 5144 95 8 2552 1726 19 0 1435 843 3350 11 3406 4 35][1618 2 719 2245 943 46 2024 88 8035 1368 2 1 50 3123 8035 1368 11 97 2242 4][3 2 29 28 13 152 54 4 249 1]...]\n",
            "targets[[2261 41 59 30 497 31 1330 1418 176 7 1445 3 1025 2191 124 41 14 42 707 31][5871 2123 17 3 769 53 5553 1 2 0 446 165 15 23 256 1102 5 3276 8035 2362][7923 7 203 2 51 1779 26 2047 7 1]...]\n",
            "targets[[829 37 2 35 1491 934 304 7 1 1713 50 492 12 3 48 5 866 3406 4 35][11 3 4089 29 959 118 88 1368 11 61 2242 4 5871 76 4 0 648 11 1681 5694][7 513 1 9 110 1 34 5449 3 1975]...]\n",
            "targets[[2261 200 628 0 12 3 48 310 561 5144 2745 16 27 257 8 0 12 3 48 2552][5 2245 855 3495 7 0 53 8 0 7802 4928 2 1 13 6 137 53 1594 8 4032][1 1350 64 42 25 710 335 5 786 6]...]\n",
            "targets[[44 27 257 13 985 2 561 5144 13 304 7 35 4485 2054 8 2552 13 304 7 4913][4 5871 210 1 1 5871 5906 7 0 3402 8 61 1587 1618 2123 17 0 53 5553 6577][6 5735 4 203 9 1 1 4 710 221]...]\n",
            "targets[[4653 2 3406 4 35 2261 1 8252 1695 27 1330 1418 176 18 43 12 3 48 2 400][2 53 218 160 15 0 3402 8 9490 42 25 0 1108 4632 4 0 70 9 587 2][173 1 34 1358 8 1358 4 14 2 6]...]\n",
            "targets[[4030 70 8 151 154 290 4 35 1491 934 15 0 37 9 502 16 621 79 2964 66][943 4 0 2242 4 5871 10 99 33 25 3821 11 8035 1368 1309 43 12 3 21 7][589 19 49 1 196 2 3306 496 521 605]...]\n",
            "targets[[14 159 6 206 257 8 27 2305 865 5 3406 4 35 2261 88 19 240 7 3473 436][3 6 1308 300 62 4 0 12 3 48 7 53 2245 943 10 40 214 5 31 2712][0 3724 3769 4 203 2 29 61 55 0]...]\n",
            "targets[[8 1300 298 1 143 32 433 1565 2 67 64 1 1417 228 257 0 1481 1830 1319 8][5 53 218 3220 9625 2396 2 7534 197 80 9 151 142 290 557 7543 1245 8 27 141][9867 14 9 1 3013 1 502 1 8643 5]...]\n",
            "targets[[4962 94 0 130 258 72 8 0 5519 1830 6 351 229 84 2 10 594 710 5 209][1642 1 13 6223 7 5 333 836 0 1 2182 384 0 37 15 2 23 1 33 1187][4 0 1 2241 26 3207 113 6270 155 38]...]\n",
            "targets[[28 283 2 23 4030 59 664 10 389 827 59 2115 35 1491 934 2 102 1193 56 832][31 676 5 713 4756 8 3165 180 508 314 6 45 8388 461 4 177 13 2708 8 3196][4581 908 4 2780 113 7 51 629 2136 1573]...]\n",
            "targets[[3406 4 35 2261 78 14 7398 20 1025 2191 27 172 33 25 211 0 1590 7 0 507][7534 15 2 1897 1 488 232 70 571 33 2280 23 7543 2 31 4069 1588 30 696 7534][67 16 0 2104 2 110 1 34 1 13]...]\n",
            "targets[[28 15 2 10 7868 1411 2 14 24 152 204 54 4 0 645 1424 10 658 5 0][22 2086 1131 8 6 772 207 3195 19 0 1604 153 6885 6 2528 2 0 37 30 220][75 1040 773 3828 7 203 2 485 0 2301]...]\n",
            "targets[[37 9 528 5 4403 20 0 1487 28 283 2 7814 2197 1205 4 3524 80 0 142 1750][1249 7 298 177 7 215 4 0 225 132 72 485 2509 81 240 2 7534 376 377 27][6 173 5491 6 262 297 1880 14 9 63]...]\n",
            "targets[[5 0 1564 37 9 621 674 15 0 3780 4 35 1491 934 279 1803 52 0 1418 797][1088 703 7 387 8 0 37 9 1354 6 71 26 1 289 628 51 7828 241 4 3][3 1975 12 3 17 3185 1 13 996 29]...]\n",
            "targets[[2 0 668 10 35 1491 497 230 158 695 3 3 94 914 72 174 384 3 3 5074][741 603 1538 3 2 16 0 2112 60 111 122 7534 71 248 52 3 741 107 5 1505][1728 1152 1 11 6 40 120 28 24 16]...]\n",
            "targets[[497 17 0 61 75 843 3350 3681 1399 22 139 160 23 2197 4839 2 99 0 1103 13][3 2 23 1 2217 8 3163 1030 4 7534 1245 19 151 154 290 69 40 5 982 8504][1556 2 7 1279 7846 45 516 1 113 8518]...]\n",
            "targets[[245 1 28 283 2 1258 0 100 770 1388 473 959 118 11 99 6 1689 17 2552 817][6 2690 7 725 9 2557 2 164 15 23 1 825 5 2504 6 45 4722 799 7 1123][9748 3345 4 27 431 134 2136 18 0 1]...]\n",
            "targets[[724 4 27 494 498 4 3406 4 35 2261 2 2552 15 14 42 1 27 1689 8 147][0 37 2 38 523 299 5 377 270 1773 0 1177 2 0 37 15 23 1 33 4963][7 0 1 37 1 611 17 6 1 1]...]\n",
            "targets[[1099 11 31 1 1277 17 0 9978 105 10 14 79 1489 0 498 17 130 983 66 27][1187 0 655 1 17 1 1 70 8 151 154 2 1400 38 1415 23 7543 79 25 697][16 0 1059 2 14 9 684 1565 10 97]...]\n",
            "targets[[257 13 0 54 678 17 0 1025 212 2 3406 4 35 2261 71 248 122 18 12 3][11 469 2 7534 15 23 7543 1245 5 2273 6 1605 2337 22 7534 19 54 4 49 764][4189 10 0 274 13 105 1 10 86 0]...]\n",
            "targets[[170 3 107 7 35 92 60 111 429 77 2 4951 1 3 72 393 8 337 70 8][2 7530 1 1 141 4 2732 637 1 955 24 521 6 9207 276 4 39 303 716 37][5 0 187 609 8 10 1152 24 1 4]...]\n",
            "targets[[151 298 290 4 6684 9307 505 80 8 4121 1 1205 3 141 4 1 567 65 46 1234][2 173 116 440 1 680 52 75 35 582 1 1359 193 2641 12 3 21 0 53 100][2 73 4 39 403 34 58 1 9698 5]...]\n",
            "targets[[748 4 39 580 1298 8 5295 2409 1273 2 38 2280 2478 1657 1 2632 2162 285 154 232][162 582 441 15 2 0 1335 464 0 404 131 4 173 116 7 53 1 1854 1967 1592][8 224 2953 8 24 1 5 1 204 1]...]\n",
            "targets[[70 8 513 1173 1 53 281 9533 16 0 1 146 2 2995 2796 95 24 1826 5 4020][9 141 692 5476 2491 15 18 6 309 877 2 28 15 45 55 3 3 4 0 193][265 1779 8 3179 23 1 78 28 2252 0]...]\n",
            "targets[[43 12 3 21 5 27 264 8 337 1 11 2132 172 6353 11 361 8284 16 6 1683][46 1686 22 173 1255 116 2 0 1121 792 5 116 20 1001 8 793 842 2 133 39][1 1 2217 4 1152 2 14 9 6 8401]...]\n",
            "targets[[338 402 2 0 4020 24 43 12 3 21 45 55 719 1826 17 0 2611 1203 473 8][40 173 116 5232 52 6 709 12 3 21 3616 125 1359 293 2 10 293 24 335 1201][34 2619 784 8 14 9 113 494 23 1]...]\n",
            "targets[[218 717 15 14 93 25 0 413 821 971 4 6 139 36 575 1924 2 139 212 401][17 7285 96 6 35 92 145 104 2 0 467 75 193 46 5906 2029 17 1589 1691 82][2479 3177 1704 1389 7 605 11 132 72 2]...]\n",
            "targets[[872 6375 1826 2796 5 138 196 5800 4 43 12 3 5 12 3 215 5 2796 526 56][95 4 0 53 8 203 9 2725 125 570 2 0 35 6627 193 54 356 6 3180 502][4 2514 4569 2 7 2107 6 35 278 2408]...]\n",
            "targets[[34 517 934 336 155 850 3 210 43 75 21 526 56 34 1265 198 10 341 2 401][8 0 61 22 6 7433 502 2865 5 0 409 3093 492 158 208 75 862 5059 5 173][643 8 1259 1584 36 663 6 12 3 1818]...]\n",
            "targets[[6375 1826 0 5800 5 852 3829 3 8 15 10 28 42 32 1 106 1902 36 61 3451][116 2 74 38 26 1097 5 2531 0 645 4 9280 3616 125 89 25 2632 266 66 131][8 3760 33 25 3207 7 7278 1609 198 1]...]\n",
            "targets[[5 1035 49 530 17 2995 2796 2 0 4020 3510 93 63 25 416 1 148 204 2886 4][172 460 8 88 9280 1254 116 5 1 51 161 18 230 172 2 334 38 2524 1496 582][73 7763 5927 273 6879 3385 8 2534 2750 2]...]\n",
            "targets[[1902 401 6375 15 2 2995 2796 15 14 13 292 7261 0 2113 473 530 8 13 1421 7261][731 91 7135 731 2319 0 2205 3157 16 116 2 17 6729 150 355 23 2491 15 0 35][956 1044 9 284 1613 6 1 123 18 0]...]\n",
            "targets[[401 6375 9 530 2 0 4276 541 4 0 4020 33 25 1962 130 40 304 16 1169 1][193 34 427 1965 2427 5 173 8 61 116 843 0 53 2 88 1673 34 1811 173 145][1647 5 2050 1 3760 2 435 1591 30 769]...]\n",
            "targets[[159 314 929 3 4 39 40 2 2995 2796 15 0 1229 79 534 14 5 5531 27 3][7 3616 125 5 45 55 3 3 4 0 12 3 48 7 88 1854 546 8 51 1335][1 5 3219 2132 1 8 1 626 3021 3574]...]\n",
            "targets[[136 17 12 3 6 62 2 11 3 2995 2796 220 136 4 12 3 21 36 12 3][26 774 18 6 2629 158 2 38 59 34 427 380 6182 4 1967 1592 9 352 207 360][29 9351 626 5 5366 2745 2 6 610 938]...]\n",
            "targets[[6 62 2 6 2995 2796 301 15 10 5560 118 0 75 21 526 624 8586 34 969 198][12 3 48 7 1967 1592 128 198 0 74 510 126 4 0 40 36 511 6 1 4][0 3 137 190 251 0 1112 24 2375 84]...]\n",
            "targets[[0 225 3 3 72 42 25 31 2205 8263 2 7 35 92 60 111 429 77 122 2995][0 345 541 895 2 557 1571 1 1 154 232 70 24 521 6 276 4 39 268 287][59 551 10 0 4093 9 1132 1105 45 5494]...]\n",
            "targets[[2796 248 18 12 3 118 3 107 2 0 12 3 48 1 3 402 952 1 3281 24][2605 0 146 5 3 449 2 2675 65 15 6 100 1025 212 401 344 5 2384 314 619][535 0 53 8 203 2 0 1112 551 10]...]\n",
            "targets[[787 7 3 2 7 6 5103 3 1229 0 1203 473 15 2995 2796 79 630 27 3473 172][3 3 0 341 7 41 0 601 1925 8 770 189 37 30 0 2733 382 5 1996 6][19 1 1695 370 22 43 2680 7 203 8]...]\n",
            "targets[[17 12 3 21 5 254 11 0 402 2 29 139 1928 5867 6 1877 17 580 717 5][1418 176 2 0 37 13 298 124 1445 3 4 0 100 1025 2378 1448 14 212 957 20][26 124 6 591 495 4 3233 90 4 0]...]\n",
            "targets[[0 473 9 158 244 8 551 0 172 2132 2 0 2611 1155 212 1826 0 473 5 3060][998 134 2179 120 14 3451 5 221 84 6 176 5 254 27 3364 2 3489 716 1098 5740][8 54 7 346 7 203 2 0 1 4860]...]\n",
            "targets[[2995 2796 9 606 1158 8 4020 106 1 1158 2 0 1924 30 58 5953 11 0 402 9][1 148 27 5740 468 5465 6475 167 542 27 12 1 1772 208 11 73 0 246 71 546][12 3 21 877 715 11 6 795 474 4]...]\n",
            "targets[[606 361 20 27 3 21 526 967 5 6 4020 155 3 2 7 397 0 473 2411 10][4 6018 5465 6 339 4 3507 7485 7 0 4004 2 0 208 8590 7 325 9 8885 4][99 2 0 474 41 13 169 5 3080 3]...]\n",
            "targets[[178 12 3 21 8 12 3 21 4 0 402 9 606 361 24 1 8 205 25 1][0 330 321 916 13 834 5 2798 18 0 235 4 1453 2 5740 488 832 43 3 3][6569 1709 29 0 98 7874 18 0 1709 9]...]\n",
            "targets[[1716 131 2 7 49 1229 401 6375 283 31 579 12 3 21 5 0 473 9 3935 2][4 6018 9 246 71 546 2 0 208 13 213 1613 1 5 6 719 411 331 178 0][0 264 176 13 369 0 35 715 89 25]...]\n",
            "targets[[69 168 401 6375 388 0 131 158 16 0 4020 18 3 3 2 2995 2796 99 1759 31][101 2 203 9 824 4 657 950 243 2380 8 656 4682 536 202 17 6 3601 12 3][2047 5 0 141 4 0 197 9759 372 2022]...]\n",
            "targets[[579 1 4020 16 27 1 158 1 1 10 0 2611 4836 212 30 452 18 12 3 21][48 7 522 5 12 3 48 0 571 1240 15 2 0 345 754 0 4367 2210 1496 460][0 1 9111 1177 309 302 1 15 2 31]...]\n",
            "targets[[2 8 580 717 1161 10 401 6375 9 1 3 530 93 388 6 6575 11 6 334 1683][2 0 1 2528 1718 0 7494 4 104 4 203 1 4134 155 623 67 0 53 819 2152][2469 11 2139 7 3 8 2879 27 4073 7]...]\n",
            "targets[[158 307 1570 2995 2796 9 1 3 402 2 2995 2796 13 817 43 12 3 21 7 158][1744 625 0 3 255 461 2 0 1033 1850 6 8404 12 3 48 460 7 0 357 9][30 58 547 5 2133 197 2624 29 779 133]...]\n",
            "targets[[695 5 254 11 1 3 2 0 473 13 169 5 1165 16 0 1 3 307 17 40][243 824 7 311 5 12 3 48 2 2313 6 357 106 357 2 14 9 0 467 145][8 0 219 4390 1177 2 6 2051 1 1634]...]\n",
            "targets[[235 2 69 40 2995 2796 50 5 4020 12 3 21 11 890 767 4 27 1 68 1683][8802 3541 330 321 6 7087 4 35 5949 357 193 150 1386 926 2560 10 2096 7 149 4][3042 0 108 9 858 15 2 14 15 0]...]\n",
            "targets[[402 2 203 9 704 81 4 699 1693 8 5927 7 522 119 3 3 20 6 40 133][6 1136 243 357 2 102 1736 55 3 357 193 34 58 1613 36 2821 22 1090 39 40][34 0 3169 78 1820 1649 2 496 0 3169]...]\n",
            "targets[[5 3 602 6 439 11 0 168 0 203 3062 667 134 441 15 2 0 404 240 1159][7198 0 461 4 73 4 3 214 5 1371 1510 7704 82 95 6 9558 319 191 2 0][4811 29 1913 34 105 289 58 551 5 34]...]\n",
            "targets[[9577 695 4 3 3 7 397 8 3 3 7 311 2 0 1496 81 34 58 2150 2037][6689 376 30 5509 20 7926 5 1 5 8315 2 130 123 0 4964 293 9 2225 33 25][351 103 50 6 9874 3042 390 210 1559 22]...]\n",
            "targets[[538 168 155 619 2 522 81 370 22 0 559 168 6473 118 3 3 2 81 4 4163][3067 17 6 2284 17 4964 70 1 7264 0 74 103 6 638 4 139 30 6493 170 31][318 3473 592 17 3 3 8 2793 539 112]...]\n",
            "Closing output_file.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XHl8sFSttCHd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RiPsRppGw1CK",
        "colab_type": "code",
        "outputId": "dac7ff89-0451-4bd9-9a25-a7d43b48cd3f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/gdrive\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1FB7DYX0w17O",
        "colab_type": "code",
        "outputId": "7f24f459-61d6-4e1e-a74c-9d9e241829db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# #copy checkpoints to drive\n",
        "%cd /content\n",
        "%cp -av maskgan /content/gdrive/My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n",
            "'maskgan' -> '/content/gdrive/My Drive/maskgan'\n",
            "'maskgan/checkpoint_convert.py' -> '/content/gdrive/My Drive/maskgan/checkpoint_convert.py'\n",
            "'maskgan/sample_shuffler.py' -> '/content/gdrive/My Drive/maskgan/sample_shuffler.py'\n",
            "'maskgan/pretrain_mask_gan.py' -> '/content/gdrive/My Drive/maskgan/pretrain_mask_gan.py'\n",
            "'maskgan/README.md' -> '/content/gdrive/My Drive/maskgan/README.md'\n",
            "'maskgan/dataset' -> '/content/gdrive/My Drive/maskgan/dataset'\n",
            "'maskgan/dataset/iccv' -> '/content/gdrive/My Drive/maskgan/dataset/iccv'\n",
            "'maskgan/dataset/iccv/ptb.valid.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv/ptb.valid.txt'\n",
            "'maskgan/dataset/iccv/ptb.train.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv/ptb.train.txt'\n",
            "'maskgan/dataset/iccv/ptb.test.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv/ptb.test.txt'\n",
            "'maskgan/dataset/iccv2017' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017'\n",
            "'maskgan/dataset/iccv2017/ptb.train.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017/ptb.train.txt'\n",
            "'maskgan/dataset/iccv2017/ptb.valid.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017/ptb.valid.txt'\n",
            "'maskgan/dataset/iccv2017/ptb.test.txt' -> '/content/gdrive/My Drive/maskgan/dataset/iccv2017/ptb.test.txt'\n",
            "'maskgan/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/.ipynb_checkpoints'\n",
            "'maskgan/pretrain_mask_gan.pyc' -> '/content/gdrive/My Drive/maskgan/pretrain_mask_gan.pyc'\n",
            "'maskgan/data' -> '/content/gdrive/My Drive/maskgan/data'\n",
            "'maskgan/data/imdb_loader.py' -> '/content/gdrive/My Drive/maskgan/data/imdb_loader.py'\n",
            "'maskgan/data/ptb_loader.py' -> '/content/gdrive/My Drive/maskgan/data/ptb_loader.py'\n",
            "'maskgan/data/__init__.py' -> '/content/gdrive/My Drive/maskgan/data/__init__.py'\n",
            "'maskgan/data/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/data/__init__.pyc'\n",
            "'maskgan/data/ptb_loader.pyc' -> '/content/gdrive/My Drive/maskgan/data/ptb_loader.pyc'\n",
            "'maskgan/data/imdb_loader.pyc' -> '/content/gdrive/My Drive/maskgan/data/imdb_loader.pyc'\n",
            "'maskgan/nas_utils' -> '/content/gdrive/My Drive/maskgan/nas_utils'\n",
            "'maskgan/nas_utils/configs.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/configs.py'\n",
            "'maskgan/nas_utils/variational_dropout.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/variational_dropout.py'\n",
            "'maskgan/nas_utils/__init__.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/__init__.py'\n",
            "'maskgan/nas_utils/custom_cell.py' -> '/content/gdrive/My Drive/maskgan/nas_utils/custom_cell.py'\n",
            "'maskgan/nas_utils/custom_cell.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/custom_cell.pyc'\n",
            "'maskgan/nas_utils/variational_dropout.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/variational_dropout.pyc'\n",
            "'maskgan/nas_utils/configs.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/configs.pyc'\n",
            "'maskgan/nas_utils/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/nas_utils/__init__.pyc'\n",
            "'maskgan/models' -> '/content/gdrive/My Drive/maskgan/models'\n",
            "'maskgan/models/cnn.py' -> '/content/gdrive/My Drive/maskgan/models/cnn.py'\n",
            "'maskgan/models/attention_utils.py' -> '/content/gdrive/My Drive/maskgan/models/attention_utils.py'\n",
            "'maskgan/models/rollout.py' -> '/content/gdrive/My Drive/maskgan/models/rollout.py'\n",
            "'maskgan/models/bidirectional_zaremba.py' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_zaremba.py'\n",
            "'maskgan/models/rnn_nas.py' -> '/content/gdrive/My Drive/maskgan/models/rnn_nas.py'\n",
            "'maskgan/models/seq2seq_vd.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_vd.py'\n",
            "'maskgan/models/feedforward.py' -> '/content/gdrive/My Drive/maskgan/models/feedforward.py'\n",
            "'maskgan/models/critic_vd.py' -> '/content/gdrive/My Drive/maskgan/models/critic_vd.py'\n",
            "'maskgan/models/__init__.py' -> '/content/gdrive/My Drive/maskgan/models/__init__.py'\n",
            "'maskgan/models/bidirectional.py' -> '/content/gdrive/My Drive/maskgan/models/bidirectional.py'\n",
            "'maskgan/models/seq2seq_zaremba.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_zaremba.py'\n",
            "'maskgan/models/seq2seq.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq.py'\n",
            "'maskgan/models/rnn_vd.py' -> '/content/gdrive/My Drive/maskgan/models/rnn_vd.py'\n",
            "'maskgan/models/bidirectional_vd.py' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_vd.py'\n",
            "'maskgan/models/seq2seq_nas.py' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_nas.py'\n",
            "'maskgan/models/rnn.py' -> '/content/gdrive/My Drive/maskgan/models/rnn.py'\n",
            "'maskgan/models/rnn_zaremba.py' -> '/content/gdrive/My Drive/maskgan/models/rnn_zaremba.py'\n",
            "'maskgan/models/evaluation_utils.py' -> '/content/gdrive/My Drive/maskgan/models/evaluation_utils.py'\n",
            "'maskgan/models/bidirectional.pyc' -> '/content/gdrive/My Drive/maskgan/models/bidirectional.pyc'\n",
            "'maskgan/models/evaluation_utils.pyc' -> '/content/gdrive/My Drive/maskgan/models/evaluation_utils.pyc'\n",
            "'maskgan/models/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/models/__init__.pyc'\n",
            "'maskgan/models/rnn_zaremba.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn_zaremba.pyc'\n",
            "'maskgan/models/bidirectional_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_vd.pyc'\n",
            "'maskgan/models/rnn.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn.pyc'\n",
            "'maskgan/models/bidirectional_zaremba.pyc' -> '/content/gdrive/My Drive/maskgan/models/bidirectional_zaremba.pyc'\n",
            "'maskgan/models/seq2seq_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_vd.pyc'\n",
            "'maskgan/models/attention_utils.pyc' -> '/content/gdrive/My Drive/maskgan/models/attention_utils.pyc'\n",
            "'maskgan/models/cnn.pyc' -> '/content/gdrive/My Drive/maskgan/models/cnn.pyc'\n",
            "'maskgan/models/seq2seq.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq.pyc'\n",
            "'maskgan/models/seq2seq_nas.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_nas.pyc'\n",
            "'maskgan/models/rnn_nas.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn_nas.pyc'\n",
            "'maskgan/models/seq2seq_zaremba.pyc' -> '/content/gdrive/My Drive/maskgan/models/seq2seq_zaremba.pyc'\n",
            "'maskgan/models/rollout.pyc' -> '/content/gdrive/My Drive/maskgan/models/rollout.pyc'\n",
            "'maskgan/models/rnn_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/rnn_vd.pyc'\n",
            "'maskgan/models/critic_vd.pyc' -> '/content/gdrive/My Drive/maskgan/models/critic_vd.pyc'\n",
            "'maskgan/models/feedforward.pyc' -> '/content/gdrive/My Drive/maskgan/models/feedforward.pyc'\n",
            "'maskgan/regularization' -> '/content/gdrive/My Drive/maskgan/regularization'\n",
            "'maskgan/regularization/variational_dropout.py' -> '/content/gdrive/My Drive/maskgan/regularization/variational_dropout.py'\n",
            "'maskgan/regularization/zoneout.py' -> '/content/gdrive/My Drive/maskgan/regularization/zoneout.py'\n",
            "'maskgan/regularization/__init__.py' -> '/content/gdrive/My Drive/maskgan/regularization/__init__.py'\n",
            "'maskgan/regularization/zoneout.pyc' -> '/content/gdrive/My Drive/maskgan/regularization/zoneout.pyc'\n",
            "'maskgan/regularization/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/regularization/__init__.pyc'\n",
            "'maskgan/regularization/variational_dropout.pyc' -> '/content/gdrive/My Drive/maskgan/regularization/variational_dropout.pyc'\n",
            "'maskgan/losses' -> '/content/gdrive/My Drive/maskgan/losses'\n",
            "'maskgan/losses/losses.py' -> '/content/gdrive/My Drive/maskgan/losses/losses.py'\n",
            "'maskgan/losses/__init__.py' -> '/content/gdrive/My Drive/maskgan/losses/__init__.py'\n",
            "'maskgan/losses/losses.pyc' -> '/content/gdrive/My Drive/maskgan/losses/losses.pyc'\n",
            "'maskgan/losses/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/losses/__init__.pyc'\n",
            "'maskgan/train_mask_gan.py' -> '/content/gdrive/My Drive/maskgan/train_mask_gan.py'\n",
            "'maskgan/model_utils' -> '/content/gdrive/My Drive/maskgan/model_utils'\n",
            "'maskgan/model_utils/model_losses.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_losses.py'\n",
            "'maskgan/model_utils/variable_mapping.py' -> '/content/gdrive/My Drive/maskgan/model_utils/variable_mapping.py'\n",
            "'maskgan/model_utils/model_optimization.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_optimization.py'\n",
            "'maskgan/model_utils/model_utils.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_utils.py'\n",
            "'maskgan/model_utils/helper.py' -> '/content/gdrive/My Drive/maskgan/model_utils/helper.py'\n",
            "'maskgan/model_utils/__init__.py' -> '/content/gdrive/My Drive/maskgan/model_utils/__init__.py'\n",
            "'maskgan/model_utils/n_gram.py' -> '/content/gdrive/My Drive/maskgan/model_utils/n_gram.py'\n",
            "'maskgan/model_utils/variable_mapping.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/variable_mapping.pyc'\n",
            "'maskgan/model_utils/n_gram.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/n_gram.pyc'\n",
            "'maskgan/model_utils/model_utils.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_utils.pyc'\n",
            "'maskgan/model_utils/helper.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/helper.pyc'\n",
            "'maskgan/model_utils/__init__.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/__init__.pyc'\n",
            "'maskgan/model_utils/model_losses.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_losses.pyc'\n",
            "'maskgan/model_utils/model_optimization.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_optimization.pyc'\n",
            "'maskgan/model_utils/model_construction.py' -> '/content/gdrive/My Drive/maskgan/model_utils/model_construction.py'\n",
            "'maskgan/model_utils/model_construction.pyc' -> '/content/gdrive/My Drive/maskgan/model_utils/model_construction.pyc'\n",
            "'maskgan/train_mask_gan.pyc' -> '/content/gdrive/My Drive/maskgan/train_mask_gan.pyc'\n",
            "'maskgan/generate_samples.py' -> '/content/gdrive/My Drive/maskgan/generate_samples.py'\n",
            "'maskgan/maskGAN' -> '/content/gdrive/My Drive/maskgan/maskGAN'\n",
            "'maskgan/maskGAN/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/maskGAN/.ipynb_checkpoints'\n",
            "'maskgan/maskGAN/train-log.txt' -> '/content/gdrive/My Drive/maskgan/maskGAN/train-log.txt'\n",
            "'maskgan/maskGAN/maskGAN_mle' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle'\n",
            "'maskgan/maskGAN/maskGAN_mle/train-log.txt' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train-log.txt'\n",
            "'maskgan/maskGAN/maskGAN_mle/train' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/.ipynb_checkpoints'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/checkpoint' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/checkpoint'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.index' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.index'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.data-00000-of-00001' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.data-00000-of-00001'\n",
            "'maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.meta' -> '/content/gdrive/My Drive/maskgan/maskGAN/maskGAN_mle/train/model.ckpt-906.meta'\n",
            "'maskgan/maskGAN/train' -> '/content/gdrive/My Drive/maskgan/maskGAN/train'\n",
            "'maskgan/maskGAN/train/.ipynb_checkpoints' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/.ipynb_checkpoints'\n",
            "'maskgan/maskGAN/train/graph.pbtxt' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/graph.pbtxt'\n",
            "'maskgan/maskGAN/train/model.ckpt-1059.index' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/model.ckpt-1059.index'\n",
            "'maskgan/maskGAN/train/model.ckpt-1059.data-00000-of-00001' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/model.ckpt-1059.data-00000-of-00001'\n",
            "'maskgan/maskGAN/train/checkpoint' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/checkpoint'\n",
            "'maskgan/maskGAN/train/model.ckpt-1059.meta' -> '/content/gdrive/My Drive/maskgan/maskGAN/train/model.ckpt-1059.meta'\n",
            "'maskgan/maskGANj_working.ipynb' -> '/content/gdrive/My Drive/maskgan/maskGANj_working.ipynb'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fV7rJwxZ0Xa",
        "colab_type": "code",
        "outputId": "6b8d2052-fc97-4b59-b923-1b5f40e55c1a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# copy from drive into runtime\n",
        "%cd /content/gdrive/My\\ Drive\n",
        "%cp -r maskgan /content"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CXXGgoq0aZo8",
        "colab_type": "code",
        "outputId": "a89c677f-572e-46da-9978-e07b0e2b7edb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "%cd /content/maskgan/maskGAN/train\n",
        "from google.colab import files\n",
        "files.download('model.ckpt-1542.data-00000-of-00001') "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/maskgan/maskGAN/train\n",
            "----------------------------------------\n",
            "Exception happened during processing of request from ('::ffff:127.0.0.1', 35792, 0, 0)\n",
            "----------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 293, in _handle_request_noblock\n",
            "    self.process_request(request, client_address)\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 321, in process_request\n",
            "    self.finish_request(request, client_address)\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 334, in finish_request\n",
            "    self.RequestHandlerClass(request, client_address, self)\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 657, in __init__\n",
            "    self.finish()\n",
            "  File \"/usr/lib/python2.7/SocketServer.py\", line 716, in finish\n",
            "    self.wfile.close()\n",
            "  File \"/usr/lib/python2.7/socket.py\", line 283, in close\n",
            "    self.flush()\n",
            "  File \"/usr/lib/python2.7/socket.py\", line 307, in flush\n",
            "    self._sock.sendall(view[write_offset:write_offset+buffer_size])\n",
            "error: [Errno 32] Broken pipe\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2AcystW6oRMh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}