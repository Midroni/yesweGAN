{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Topic Modeling?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Topic modeling is a way to figure out the underlying structure of a piece of text. It tries to represent a text by a collection of few concepts (or topics) rather than a whole bunch of words. In other words, we assign tags to a document so that it can be compared with other documents or be put into categories. It can be used in recommendation systems by suggesting a similar article after one has read an article.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What should I know before TM?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Document-term matrix, term-document matrix, count matrix\n",
    "A matrix whose rows and columns correspond to documents and terms. The entries correspond to the frequency of the term in the document. Sometimes the rows correspond to terms and columns correspond to terms.\n",
    "\n",
    "Here's what it could look like,\n",
    "\n",
    "|...|term1|term2|term3|term4|\n",
    "|---|:---:|:---:|:---:|:---:|\n",
    "|d1 |1    |0    |4    |2    |\n",
    "|d2 |6    |2    |1    |0    |\n",
    "|d3 |4    |3    |0    |0    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Term frequency (tf)\n",
    "The number of times a term appears in a document. It gives all terms equal importance and so cannot be used by itself to understand the importance of a term in a document. Words that appear a lot, in many documents (high tf - low \"specificity\") can be words that do not impact the meaning of a document significantly. Therefore tf can be used to identify stop-words. \n",
    "\n",
    "tf can either be a raw-count of a term in a document or expressed as a fraction of raw-count to total number of terms in the document.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{tf}(t, d) = \\frac{\\mathrm{n}(t, d)}{\\sum _{i \\in d}{\\mathrm{n}(i, d)}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where 't' is a term, 'd' is a document, 'n' gives the frequency of the term in the document, and 'i' represents an arbitrary term. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Inverse document frequency (idf)\n",
    "A measure that tells us whether a term is \"specific\" to a document or found in a lot of documents. idf scales up rare words giving them more importance and scales down common words giving them less importance.\n",
    "\n",
    "It is measured as,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{idf}(t, D) = \\log{\\frac{|D|}{n_t}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where 't' is a term, 'D' is the set of all documents (corpus) and '|D|' is the total number of documents, and 'n sub t' is the number of documents where 't' appears (or tf(t, d) =/= 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### tf-idf score\n",
    "Measured as the product of tf and idf. Tells us how important a term is in a passage.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{tfidf}(t, d, D) = \\mathrm{tf}(t, d) \\cdot \\mathrm{idf}(t, D)\n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Singular-value decomposition\n",
    "A way of factorising a matrix into a product of three matrices. We use this later on to produce a low-rank approximation of a term-document matrix.\n",
    "\n",
    "*Formula ignoring cases involving imaginary numbers*\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\small{\n",
    "\\mathbf {M} = \\mathbf {U} \\boldsymbol{\\Sigma } \\mathbf {V} ^{T}\n",
    "\\\\\n",
    "\\textrm{where } \\mathbf {M} \\textrm{ is an m × n matrix,}\n",
    "\\\\\n",
    "\\mathbf{U} \\textrm{ is an m × m matrix whose columns are called the left-singular vectors of } \\mathbf{M}\n",
    "\\\\\n",
    "\\boldsymbol{\\Sigma} \\textrm{ is an m × n \"diagonal\" matrix whose diagonal entries are the singular values of } \\mathbf{M} \\textrm{ and}\n",
    "\\\\\n",
    "\\mathbf {V}^{T} \\textrm{ is an n × n matrix whose rows are called the right-singular vectors of } \\mathbf{M}\n",
    "}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In Python, we can easily perform SVD by using scipy's built-in `linalg.svd() method`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import numpy as np\n",
    "from scipy.linalg import svd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia example\n",
    "M = np.array([[1, 0, 0, 0, 2],\n",
    "              [0, 0, 3, 0, 0],\n",
    "              [0, 0, 0, 0, 0],\n",
    "              [0, 2, 0, 0, 0]])\n",
    "U, S, Vt = svd(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U # a 4x4 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S # an array of the singular values in descending order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vt # a 5x5 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we generate Sigma\n",
    "m = M.shape[0] # no. of rows in M\n",
    "n = M.shape[1] # no. of columns in M\n",
    "Sigma = np.zeros([m, n]) # for now, an mxn zero matrix\n",
    "Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Populate Sigma with singular values in correct positions\n",
    "for i, v in enumerate(S):\n",
    "    Sigma[i, i] = v # singular value\n",
    "Sigma # notice the diagonal entries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can truncate (ignore a few dimensions in) U, Sigma, and Vt to produce an approximation of M. This is used as a form of data-reduction and solves certain issues that we come across with topic models. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What are some examples of topic models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A technique to figure out the relationship between a document and the terms present in the document. It is based on the assumption that similar terms appear in similar pieces of text. Given a term, LSA can be used to generate other terms that are relevant to it. \n",
    "\n",
    "LSA is used in search engine optimisation where you can obtain key-words that are relevant to a query. These key-words can be used by search engines to understand the content of a page and its relevance to the original query.\n",
    "\n",
    "LSA attempts to compare the concepts represented by terms rather than the terms itself. The terms and documents are mapped into a 'concept' space where the comparisons are performed.\n",
    "\n",
    "We do not consider the order in which the terms appear in a document (bag of words model).\n",
    "\n",
    "We will use SVD in this method.\n",
    "\n",
    "Overview\n",
    "1. Prepare text for analysis by removing symbols, stop-words, etc.\n",
    "2. Obtain document-term matrix\n",
    "3. Change entries in the matrix from raw count to tf-idf scores\n",
    "4. Perform Singular-Value Decomposition on this matrix to get 3 different matrices, U, Sigma, and Vt\n",
    "5. Remove a few dimensions from the matrix\n",
    "6. \n",
    "\n",
    "(ref.\n",
    "http://www.scholarpedia.org/article/Latent_semantic_analysis\n",
    "\n",
    "https://technowiki.wordpress.com/2011/08/27/latent-semantic-analysis-lsa-tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How to perform LSA in Python?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a few sample articles to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'if you\\'re from Syria and you\\'re a Christian, you cannot come into this country as a refugee says Trump',\n",
    "    'video shows federal troops in armored vehicles patrolling the streets of Chicago on President Trump\\'s orders.',\n",
    "    'Donald Trump wrote in \\The Art of the Deal\\ that he punched out his second grade music teacher.',\n",
    "    'Actor Denzel Washington said electing President Trump saved the US from becoming an \\\"Orwellian police state.\\\"',\n",
    "    \"President Trump fired longtime White House butler Cecil Gaines for disobedience.\",\n",
    "    'Congress has approved the creation of a taxpayer-funded network called \\\"Trump TV.\\\"',\n",
    "    'The Islamic State \"just built a hotel in Syria\", according to President Donald Trump',\n",
    "    \"A Gucci ensemble worn by Trump counselor Kellyanne Conway to the inauguration closely resembled a 1970s 'Simplicity' pattern.,\"\n",
    "    'Says Melania Trump hired exorcist to \\\"cleanse White House of Obama demons.\\\"',\n",
    "    '\"\\\"Russia, Iran, Syria & many others are not happy\\\" about US troops leaving Syria according to the US President.'\n",
    "    \"In January 2019, President Donald Trump ordered FEMA to stop or cancel funding for its disaster assistance efforts in California.\",\n",
    "    \"Trump looking to open up E Coast & new areas for offshore oil drilling when Congress has passed no new safety standards since BP\",\n",
    "    '\"You were here long before any of us were here, although we have a representative in Congress who, they say, was here a long time ago. They call her Pocahontas.\", said Trump',\n",
    "    \"A photograph shows an elephant carrying a lion cub.\",\n",
    "    \"Elephant carrying thirsty lion cub\",\n",
    "    \"Nike makes their sneakers with elephant skins.\",\n",
    "    \"A photograph shows a jumping baby elephant.\",\n",
    "    \"Is this a video of an elephant trampling a man to death in India?\",\n",
    "    \"The lion used for the original MGM logo killed its trainer and his assistants.\",\n",
    "    \"A friend and I are arguing about the origin of the photo of Donald Trump, Ivanka, and Barron with Barron sitting on the stuffed lion. She swears it's a photo-shopped, fake photo. Do you have any idea who took it or published it first?\",\n",
    "    \"A photograph shows a real baby platypus.\",\n",
    "    \"Photograph shows a drop bear cub being fed human blood.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first step is to convert these articles into a term-document matrix. This involves a few preparatory steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import nltk, re\n",
    "from math import log\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preparing data before conversion to matrix\n",
    "\n",
    "# define stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words += ['comes', 'says', 'according', 'come', 'shows', 'show', 'cannot']\n",
    "\n",
    "for i in range(len(corpus)):\n",
    "    \n",
    "    # remove symbols and numbers\n",
    "    document = re.sub('[^a-zA-Z]', ' ', corpus[i])\n",
    "    \n",
    "    # change to lower case\n",
    "    document = document.lower()\n",
    "    \n",
    "    # convert string to a list of strings\n",
    "    document = document.split()\n",
    "    \n",
    "    # remove stopwords and perform lemmatisation\n",
    "    lem = WordNetLemmatizer()\n",
    "    \n",
    "    document = [lem.lemmatize(word) for word in document if (not word in  \n",
    "            stop_words) and (len(word) > 1)]\n",
    "    \n",
    "    corpus[i] = document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's create a dictionary that has keys as terms and values\n",
    "# as the list of documents it is found in\n",
    "\n",
    "# the documents here can be mapped to [0, 1, 2, 3, 4, ...]\n",
    "\n",
    "term_dict = {}\n",
    "for d in range(len(corpus)):\n",
    "    document = corpus[d]\n",
    "    for term in document:\n",
    "        if term not in term_dict:\n",
    "            term_dict[term] = [d]\n",
    "        else:\n",
    "            term_dict[term].append(d)\n",
    "            \n",
    "term_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only those words that appear in more than one document.\n",
    "new_term_dict = {}\n",
    "for term in term_dict:\n",
    "    if len(term_dict[term]) >1:\n",
    "        new_term_dict[term] = term_dict[term]\n",
    "new_term_dict\n",
    "new_term_dict = term_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the length of each list within the dict gives us the frequency of the term in the corpus\n",
    "\n",
    "# let us create a list of all the words in this dictionary that appear in two or more documents\n",
    "keys = []\n",
    "for term in term_dict:\n",
    "    if len(term_dict[term]) >1:\n",
    "        keys += [term]\n",
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we can create the term-document matrix\n",
    "m = len(new_term_dict) # the number of rows in the matrix is the number of unique terms\n",
    "n = len(corpus) # the number of columns is the number of documents\n",
    "A = np.zeros([m, n])\n",
    "A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# populate the matrix with the term frequencies\n",
    "for index, term in enumerate(list(new_term_dict)):\n",
    "    for d in new_term_dict[term]:\n",
    "        A[index, d] += 1\n",
    "A[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we convert term frequencies to tfidf scores\n",
    "rows = A.shape[0]\n",
    "cols = A.shape[1]\n",
    "\n",
    "# array of sums of all entries in each column\n",
    "col_sums = np.sum(A, axis=0)\n",
    "\n",
    "B = np.copy(A)\n",
    "\n",
    "for i in range(rows):\n",
    "\n",
    "    for j in range(cols):\n",
    "        \n",
    "        # total number of terms in the doc j\n",
    "        n = col_sums[j]\n",
    "        tf = B[i, j] / n\n",
    "        \n",
    "        # get ith row\n",
    "        row_i = list(B[i])\n",
    "        \n",
    "        # filter out documents that do not have the term\n",
    "        row_i = [d for d in row_i if d > 0]\n",
    "        \n",
    "        # number of documents that have the term\n",
    "        nt = len(row_i)\n",
    "\n",
    "        \n",
    "        idf = log(float(cols)) / (nt)\n",
    "        \n",
    "        A[i, j] = tf * idf\n",
    "        \n",
    "A[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time to use SVD to create a low-rank approximation of our matrix\n",
    "# and prepare for comparisons\n",
    "\n",
    "# svd makes the best possible reconstruction of the matrix with the least possible information.\n",
    "\n",
    "U, S, Vt = svd(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# U gives us coordinates of each term in our new 'concept' space\n",
    "\n",
    "print(U.shape)\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S tells us how many dimensions we can include when truncating\n",
    "\n",
    "print(S.shape)\n",
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vt gives us the coordinates of each document in the 'concept' space\n",
    "\n",
    "print(Vt.shape)\n",
    "Vt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to decide which dimensions to consider.\n",
    "\n",
    "For documents, the first dimension correlates with the length of the document. For words, it correlates with the number of times that word has been used in all documents.\n",
    "\n",
    "So we will remove the first dimension from our matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we truncate these three matrices\n",
    "k = 2 # the number of dimensions in our approximation\n",
    "# the optimal number is around 250\n",
    "# but we do not have that many columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uk = U[:, 1:k+1] # all rows and 1-k+1 columns\n",
    "print(Uk.shape)\n",
    "Uk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sk = S[1:k+1]\n",
    "print(Sk.shape)\n",
    "Sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Vtk = Vt[1:k+1, :] # k rows and all columns\n",
    "print(Vtk.shape)\n",
    "Vtk.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can plot the relationship between words and documents\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "# we have only used 2 dimensions and so we could plot this in an xy-plane\n",
    "\n",
    "term_x = [list(Uk)[i][0] for i in range(len(list(Uk)))]\n",
    "term_y = [list(Uk)[i][1] for i in range(len(list(Uk)))]\n",
    "\n",
    "doc_x = [list(Vtk.T)[i][0] for i in range(len(list(Vtk.T)))]\n",
    "doc_y = [list(Vtk.T)[i][1] for i in range(len(list(Vtk.T)))]\n",
    "\n",
    "for index, term in enumerate(list(new_term_dict)):\n",
    "    x = term_x[index]\n",
    "    y = term_y[index]\n",
    "    plt.scatter(x, y, color='red')\n",
    "    plt.text(x+0.001, y+0.001, term, fontsize=8)\n",
    "for i in range(len(corpus)):\n",
    "    x = doc_x[i]\n",
    "    y = doc_y[i]\n",
    "    plt.scatter(x, y, color='blue')\n",
    "    plt.text(x+0.0001, y+0.0001, f'doc{i}', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's use gensim to perform LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "from gensim.models import LsiModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a term dictionary of our corpus\n",
    "# the keys are terms and values are integer ids\n",
    "dictionary = corpora.Dictionary(corpus)\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create doc-term matrix\n",
    "doc_term = [dictionary.doc2bow(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create instance of LSA model and pass req. params\n",
    "n = 3 # number of topics\n",
    "lsa = LsiModel(doc_term, num_topics=n, id2word=dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa.print_topics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Latent Dirichlet Allocation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is it?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Pronounced Dirishlay or Diriklay)\n",
    "\n",
    "ref. http://blog.echen.me/2011/08/22/introduction-to-latent-dirichlet-allocation/\n",
    "\n",
    "\n",
    "Most commonly used topic model.\n",
    "\n",
    "LDA looks at a document as a mixture of topics that spit out words with certain probabilities. To generate a document, decide how many words it will have, choose a mixture of topics (the mixture can be fractional), and then generate each word by picking a topic from the mixture according to the fractional probability and use it to generate a topic-related word.\n",
    "\n",
    "In Python, we can use the gensim library to perform LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Say we have a document that said \"I like apples and oranges\", and another that said \"I like dogs and cats\". We need to specify how many topics we thing are in the corpus. Here, it is evident that there are two topics atleast - 'fruits' and 'animals' say.\n",
    "\n",
    "As we did in LSA, we create a document-term matrix. And then two other matrices, a topic-document matrix and a term-topic matrix.\n",
    "\n",
    "We randomly assign each term in the document to one of the topics. \n",
    "\n",
    "For each term, we look at two things.\n",
    "1. The number of times a topic appears in a document.\n",
    "2. The number of times a term appears in a topic.\n",
    "\n",
    "Say a term in our document was randomly assigned to a topic A initially, but we find that topic A does not appear a lot in the document and that the term does not appear a lot in the topic then we can change the topic of the term to something else.\n",
    "\n",
    "We make appropiate changes to the two matrices.\n",
    "\n",
    "After multiple iterations, we reach a point in which the topic assignment stops changing. At this point we can say that the correct topics have been assigned to each term and each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A # this is our document-term matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 3 # number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils, models\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A.T[:5] # this is our term-document matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert matrix to compressed sparse matrix format\n",
    "# i believe it helps in making computation easier\n",
    "A_sparse = sparse.csr_matrix(A.T)\n",
    "A_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to gensim suitable format\n",
    "corpus = matutils.Sparse2Corpus(A_sparse)\n",
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# another input needed for the model is a dict of all terms and \n",
    "# their positions in the matrix\n",
    "\n",
    "id2word = dict((index, list(term_dict)[index]) for index in range(len(term_dict))) # temp\n",
    "id2word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=2, passes=250) # 2 topics\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=3, passes=250) # 3 topics\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=4, passes=250) # 4 topics\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=5, passes=250) # 5 topics\n",
    "lda.print_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = models.LdaModel(corpus=corpus, id2word=id2word, num_topics=6, passes=250) # 6 topics\n",
    "lda.print_topics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
