Visual Forecasting by Imitating Dynamics in Natural Sequences   Visual Forecasting by Imitating Dynamics in Natural Sequences  Kuo-Hao Zeng†‡ William B
Shen† De-An Huang† Min Sun‡ Juan Carlos Niebles†  †Stanford University ‡National Tsing Hua University  {khzeng, bshenNN, dahuang, jniebles}@cs.stanford.edu sunmin@ee.nthu.edu.tw  Abstract  We introduce a general framework for visual forecasting, which directly imitates visual sequences without additional supervision
As a result, our model can be applied at several semantic levels and does not require any  domain knowledge or handcrafted features
We achieve this  by formulating visual forecasting as an inverse reinforcement learning (IRL) problem, and directly imitate the dynamics in natural sequences from their raw pixel values
 The key challenge is the high-dimensional and continuous  state-action space that prohibits the application of previous  IRL algorithms
We address this computational bottleneck  by extending recent progress in model-free imitation with  trainable deep feature representations, which (N) bypasses  the exhaustive state-action pair visits in dynamic programming by using a dual formulation and (N) avoids explicit  state sampling at gradient computation using a deep feature  reparametrization
This allows us to apply IRL at scale and  directly imitate the dynamics in high-dimensional continuous visual sequences from the raw pixel values
We evaluate  our approach at three different level-of-abstraction, from  low level pixels to higher level semantics: future frame generation, action anticipation, visual story forecasting
At all  levels, our approach outperforms existing methods
 N
Introduction  Our goal is to expand the boundaries of video analysis towards visual forecasting by building intelligent systems that are capable of imitating the dynamics in natural visual sequences
The idea of visual forecasting has  been studied in several contexts, such as trajectory forecasting [N, NN, NN], activity prediction [NN, N0], and future frame  synthesis [NN, N0]
However, most previous work either assumes short forecasting horizons [NN, NN], or focuses on a  specific application, such as walking trajectories [N, NN, NN]  or human actions [NN, NN], where handcrafted features play  an important role
 In this paper, we propose a general framework for visual  forecasting, where it can be applied to learn a model at any  Hug?  StateStateState State � Action�  Future Frames   Generation  � � ?  Storyline ?  Forecasting  � � Action  Anticipation  � �High-l e  v e  l  Lo  w -l  e v  e l  t Figure N
We propose a general framework for visual forecasting
 We formulate this as an imitation learning problem, where the natural visual sequences are served as expert behaviors, and visual  forecasting is reformulated as learning a policy to reproduce the  expert’s behavior
Our framework imitate dynamics in visual sequences directly from its raw pixel value and is applicable to tasks  with different levels of abstraction
 given semantic-level (see Figure N) and does not require any  domain knowledge or handcrafted features
Such framework for forecasting can be widely useful in real world scenarios: robot motion planning [NN, N0] or warning systems  for autonomous vehicles [N]
We achieve this by formulating visual forecasting as an inverse reinforcement learning (IRL) [NN] problem, where the natural sequences serve  as expert demonstrations, and the goal becomes recovering the underlying cost function that prioritizes the entire  expert trajectories over others
This eliminates the compounding error problem of single-timestep decisions [NN],  and has been proven effective in long-term prediction in  both robotics [N, NN, NN] and vision [NN, NN]
 Our key insight is to extend previous IRL work in vision to directly imitate the dynamics in visual sequences  from their raw pixel values, where no domain knowledge or  handcrafted feature would be required
This is in contrast  to previous work that applies IRL on handcrafted features,  such as the semantic scene labeling for trajectory forecasting [NN, NN] and interaction statistics for dual agent interaction [NN]
The main challenge is that both the state and  action space would become high-dimensional and continuNNNN    ous in such setting, which prevent scaling up existing IRL  algorithms used in previous work [NN, NN]
 We address this challenge by extending recent progress  in model-free imitation learning [NN, NN] with the capacity of jointly trainable deep feature representations
More  specifically, we extend the Generative Adversarial Imitation Learning framework of Ho and Ermon [NN] with deep  feature reparametrization to forecast the dynamics in natural visual sequences
The deep representation is fully differentiable and we optimize it jointly using policy gradient, which results in learning an intermediate representation that is best-suited for visual forecasting
The resulting  model (N) bypasses the dynamic programming based subroutine of [NN] that exhaustively visits every state-action  pair by using a dual formulation, and (N) avoids explicit  state sampling (image synthesis in this case) of gradient  computation in [NN] with reparametrization using deep feature representations
This allows us to operate beyond lowdimensional state-action spaces and directly forecast the  natural sequence from the pixel-level
 We demonstrate the effectiveness of our approach by applying our model to three tasks that require different levels of understanding of visual dynamics
First, we examine the ability of our method to perform generation of future frames on the moving MNIST data sequences [NN]
 Results from future frame generation demonstrate that our  framework can capture the low-level (pixel-level) visual dynamics between consecutive frames
Second, we examine the ability of our method to perform action anticipation by unsupervised training on the THUMOSNN dataset  [N], and evaluating the resulting model on the TV Human  Interactions dataset [NN]
Results from action anticipation  show that our framework is effective in modeling mid-level  (motion-level) visual dynamics in natural videos
Finally,  we apply our method on the Visual Storytelling dataset [NN]  and try to forecast the next photo in a story composed of images
Results on this task demonstrate that our framework  can generalize to visual dynamics on a highly abstract level  (semantic-level)
In each task, our method outperforms all  existing methods
 In summary, the main contributions of our work are: (N)  We propose a general IRL based formulation for visual sequence forecasting that requires no domain knowledge or  handcrafted features
(N) We address the computational bottlenecks of IRL when operating at high-dimensional continuous spaces by using a dual formulation and reparametrization with deep feature representations
(N) We evaluate our  method on three tasks at different levels of abstraction and  show that our framework outperforms all existing methods
 N
Related Work  Visual Prediction
There has been growing interest in  developing computational models of human activities that  can extrapolate unseen information and predict future unobserved activities [N, NN, NN, NN, NN, NN, N0, NN]
Some  of the existing approaches [N, NN, NN, NN, NN, N0] tried to  generate realistic future frames using generative adversarial  networks [N]
Unlike these methods, we emphasize longerterm sequential dynamics in videos using inverse reinforcement learning
Other line of work attempted to infer the action or human trajectories that will occur in the subsequent  time-step based on previous observation [N, NN, NN, NN, NN]
 Our model directly imitates the natural sequence from the  pixel-level and assumes no domain knowledge
 Reinforcement Learning in Computer Vision
Reinforcement learning (RL) achieves remarkable success in multiple domains ranging from robotics [N], computer vision  [N, N0, NN] and natural language processing [NN, NN]
In  the RL setting, the reward function that the agent aims to  maximize is given as signal for training, where the goal  is to learn a behavior that maximizes the expected reward
 On the other hand, we work on the inverse reinforcement  learning (IRL) problem, where the reward function must  be discovered from demonstrated behavior [NN, NN, NN]
 This is inspired by recent progress of IRL in computer vision [N, NN, NN, NN, NN, NN]
Nonetheless, these frameworks  require heavy use of domain knowledge to construct the  handcrafted features that are important to the task
Unlike  these approaches, we aim to generalize IRL to natural sequential data without annotations
 Unsupervised Representation Learning in Video
Our  combination of deep neural networks with IRL to directly  imitate natural videos is related to recent progress in unsupervised representation learning in videos
The temporal  information in the video has been utilized to learn useful  features based on tracking [NN], flow prediction [NN], future  frame synthesis [NN], frame order [NN], and object movement [NN]
Our work utilizes not only the frame-to-frame  dynamics, but tries to imitate the entire sequence directly to  utilize long-range temporal information
 Generative Adversarial Learning
Our extension of generative adversarial imitation learning [NN] is related to recent progress in generative adversarial networks (GAN) [N]
 While there has been multiple works on applying GAN to  image and video [NN, NN, NN, NN], we extend it to long-term  prediction of natural visual sequences and directly imitate  the high-dimensional continuous sequence
 N
Forecasting Natural Sequences  Our goal is to build a system that can forecast the dynamics of natural videos
We formulate this as an inverse reinforcement learning problem, where the goal is to recover  the underlying cost function that induces the natural visual  sequence
This is equivalent to learning a policy π(v′|v) that parameterizes the transition from a current frame v to  the next frame v′
As shown in Figure N, at training time,  N000    �E �  Training  Testing  input � prediction � ′ �S  Current Policy ( )  ≈E  Figure N
We formulate visual prediction as an imitation learning problem, where the goal becomes learning a policy π that can  generate trajectory τ following the behavior of natural visual sequence, or expert demonstration τE 
This is reformulated as finding a policy π that has a state-action pair visit distribution ρ that  is similar to that of the expert (ρE)
At test time, we thus have  a learned policy π that can mimic the behavior of natural visual  sequence and perform visual prediction
 our model observes and imitates a large amount of natural visual sequences to learn the policy
At testing time,  the learned policy π(v′|v) can then be directly applied to an image for long-term visual prediction
Despite its many  potential applications, learning π(v′|v) is challenging as v is an image that is high-dimensional and continuous
This  makes both the exhaustive state-action pairs visit and the  explicit state sampling in previous work [NN, NN] intractable  at scale
Our solution to this extends and reparametrizes the  model from Ho and Ermon [NN] with deep representations
 Our framework bypasses exhaustive state-action pair visit  of [NN] by the dual formulation of [NN], and avoids explicit  state sampling (which is equivalent to frame synthesis) during the gradient computation in [NN]
 In the following, we discuss how we formulate natural  visual sequences as Markov Decision Processes so that our  inverse reinforcement learning framework can be applied
 N.N
Visual Sequences as Markov Decision Process  In this work, we model natural visual sequences as  Markov Decision Processes (MDP) [NN]
At each time step,  the sequence is at some frame/image v, which is defined  as the state of our MDP
Given a chosen action a from the  state, the process responds by moving to a new state, or  frame v′ based on the transition model p(v′|v, a) and re- ceives a cost c(v, a)
In this work, the transition model is deterministic as we are directly imitating the natural visual  sequences
Therefore, p(v′|v, a) = N for the single visual state v′ = va that we can visit after taking action a, and is zero otherwise
In this case, the policy π(va|v) of moving to a state va is equivalent to choosing the corresponding action  π(a|v)
As result, we are able to use π(v′|v) and π(a|v) in- terchangeably depending on the context (same for c(v, v′) and c(v, a))
Our goal of inverse reinforcement learning (IRL) is in contrast to reinforcement learning (RL), where  the cost function is observed and the goal is to learn the  optimal policy π(a|v) that minimizes the expected cost
In IRL, the cost function is not observed, and the goal is to  imitate the behaviors of the expert by recovering the underlying cost function c(v, a) that induces the observed optimal behavior by the expert
 N.N
Imitating Natural Visual Sequences  We formulate the visual prediction as an inverse reinforcement learning problem, where the goal is to imitate  the behavior of natural sequences that are treated as expert  demonstrations
In this section, we discuss the model-free  imitation learning framework of Ho and Ermon [NN] that  allows us to bypass the exhaustive state-action pair visit  in dynamic programming of Maximum Entropy (MaxEnt)  IOC [NN] used in previous works [NN, NN]
In the next section, we discuss how we apply it efficiently to natural visual  sequences from the pixel level
 In contrast to reinforcement learning, the cost function  in IRL is not given and has to be recovered from expert  demonstrations τE 
We follow previous work and adopt the  maximum causal entropy framework [NN] for the optimization of our cost function:  max c∈C  (min π∈Π  −H(π) + Eπ[c(v, a)] )− EπE [c(v, a)], (N)  where Π is a policy space and H(π) , Eπ[−log π(a|v)] is the γ-discounted causal entropy [N] of the policy π
 The standard approach for this IRL problem used in previous work [NN, NN] involves a RL inner loop that utilizes  dynamic programming to exhaustively visit every state and  the available actions for value function computation
However, such procedure is intractable in our problem, as both  our state and action spaces are high-dimensional and continuous
The key observation to resolve this problem is  that IRL Eq
(N) is actually a dual of an occupancy measure matching problem [NN]
Here, the occupancy measure  ρπ(v, a) of a policy π is defined as the distribution of the visit of state-action pairs while following the policy π
This  reformulates Eq
(N) as:  min π dψ(ρπ, ρπE )−H(π), (N)  where dψ(·, ·) measures the difference between two occu- pancy measures depending on the regularizer ψ
Intuitively,  N00N    �  � �  �E � �� �  Fix �, Update � Expert � �  … … � ො� , ො� …� � , �  Fix �, Update �  Gen
� � , ො�� � , � � � �  ො� ො�  ෤�  Figure N
Overview of our imitation learning
The optimization iterates between policy update and cost function update
During the cost  (D) update, the goal is to find a cost that best differentiate expert’s trajectory τE from our generated trajectory τ 
During the policy (π)  update, we estimate the expected cost Q and use policy gradient with roll out to minimize it (see more detail in supplementary)
This  framework also has a generative adversarial learning interpretation, where the cost is operating as the discriminator and the policy is  functioning as the generator
 this aims to induce a policy that visits state-action pairs with  similar frequency to that of the expert policy
This allows us  to use the demonstrated state-action pairs of expert directly  as learning signal to encourage the policy toward expert-like  region in state-action space and bypass the exhaustive visit  of dynamic programming
 As presented in [NN], under certain selection regularizer  ψ, dψ(·, ·) becomes:  max D  Eπθ [log(D(v, a))] + EπE [log(N−D(v, a))], (N)  where D(v, a) can be interpreted as a discriminator that tries to distinguish the state-action pairs from the expert or  the induced policy
This interpretation connects imitation  learning with generative adversarial network (GAN) [N]
 Essentially, our imitation learning of natural visual sequences can be seen as jointly training (N) a discriminator  that tries to distinguish our generated sequence from expert’s and (N) a policy that forms as a generator that uses  the learning signal provided by the discriminator to move  toward expert-like regions
We refer the readers to [NN] for  more details of the algorithm
The important note is that no  specific restriction would be put on the form of approximation for both D and π and this allows us to directly imitate  high-dimensional continuous sequence
 Optimization
Let Dw be D parameterized by weights w,  πθ be π parameterized by θ, τ = {vt} be trajectory of states vt from current policy, and τE = {v  E t } be expert trajectory  given as demonstration, the optimization of Eq
(N) with Eq
 (N) alternates between the maximizing the discriminative  power of Dw through (the left panel in Figure N):  Eτ∼πθ [∇w log(Dw(vt, vt+N))] (N)  + EτE∼πE [∇w log(N−Dw(v E t , v  E t+N))], (N)  and a policy gradient step (the right panel in Figure N):  Eτ∼πθ [∇θ log(πθ(vt+N|vt))Q(vt, vt+N)] (N)  that aims to maximize the expected return Q(vt, vt+N), which is in turn defined as  Q(v, v′) = Eτ∼πθ [log(Dw(vt, vt+N))|v0 = v, vN = v ′], (N)  the expectation of confusing the discriminator that the generated sequence τ is a expert’s sequence
 N.N
Efficient Sampling with Deep Representation  Now we have formulated visual prediction as an IRL  problem and presented a generative adversarial imitation  learning framework [NN] that is applicable to our problem,  because it bypasses the dynamic programming based subroutine that iterates through all the states and actions
However, we would like to point out there is still a remaining  computational bottleneck that prevents the direct application of this framework to the image space
 Notice in the gradient computation for both the discriminator and the policy (Eq
(N) and Eq
(N)) that we are required to sample trajectories τ from the current policy πθ
 This is equivalent to sampling a sequence of images/frames  at each gradient computation
The problem of image synthesis itself is challenging and computational intensive [NN],  and prevents the direct application of their framework to  large scale visual sequence datasets
 Our key insight to address this is to parameterize both  the discriminator Dw and the policy πθ with a intermediate representation φ(·) that is feasible to sample at the op- timization, but still captures sufficient information from the  image:  π(vt+N|vt) = π̂(φ(vt+N)|φ(vt)), (N)  D(vt, vt+N) = D̂(φ(vt), φ(vt+N))
(N)  In this case, the sampling of vt+N from vt is a three step  process (all steps in Figure N):  vt+N = φ −N(ht+N), ht+N ∼ π̂θ(·|ht), ht = φ(vt)
 (N0)  N00N    � � ො� ො���, ��  �  �� ��+ ��+  ℎ� ℎ�+ ℎ�+  Figure N
We reparametrize the policy with deep feature representation to avoid explicit state sampling during the gradient computation
The steps of sampling from π is decomposed in to (N)  applying φ(vt) to get the hidden representation ht, (N) sampling from reparametrized policy π̂ to get ht+N, (N) applying φ  −N(ht+N) for the next state
However, the third step is not required as we subsequently apply φ(φ−N(ht+N)) for the sampling in the next time step and cancel out the inverse operation
 The main advantage of this parameterization is that the challenging φ−N (image synthesis from hidden representation)  is actually not required in the gradient computation, since:  π(vt+N|vt) = π̂(φ(vt+N)|φ(vt)) (NN)  = π̂(φ(φ−N(ht+N))|ht) = π̂(ht+N|ht), (NN)  and similar for the discriminator
In this case, only the intermediate representation is required to be sampled for the gradient computation (the top path in Figure N)
In this work,  we use the deep convolutional neural networks as our φ(·) because of its success in representing the visual information [NN]
It is important to note that this φ(·) is also fully differentiable and we approximate it by optimizing it jointly  in the policy gradient step of Eq
(N)
This reparametrization  allows us to directly imitate natural visual sequences from  its pixel value and goes beyond the computational limitation  in previous works
 N
Experiments  We evaluate our model on three challenging tasks at  three different levels of abstraction
The first is future frame  generation in the moving MNIST dataset [NN], where the  image synthesis (φ−N) is feasible to verify the effectiveness  of our imitation learning formulation
The second task is action anticipation on the TV Human Interactions dataset [NN]
 We verify the predictive capacity of our model directly imitating natural video [N] from the pixel-level with our deep  reparametrization
Finally, we apply our method on the Visual Storytelling dataset [NN] to predict the next photo in a  storyline that consists of five photos
This is extremely challenging as there can be no clue from direct visual appearance matching
We show that our method can still mimic  human behavior at the semantic-level from the pixel values
 N.N
Future Frame Generation  In this experiment, we examine our method’s ability to  perform future frame generation by training on the moving  MNIST data sequences [NN]
Unlike the two following experiments, frame generation (φ−N) is feasible at this scale  and we thus also optimize it in our framework and explicitly sample images at the gradient computation
 Dataset
The moving MNIST dataset generates moving  digits by randomly selecting N digits from MNIST dataset [NN] and randomly moving the digits according to the positions in the previous frame
The size of frames are NN× NN
Implementation Details
We note that the state of this  dataset is not fully specified by a single image, because  the lack of motion information
We thus define the state  as three consecutive images
We extend the architecture  of DCGAN [NN] to have multi-frame input for frame synthesis, which consists of a N-layer ND convolutional neu- ral network encoder capturing the information of the image,  and a N-layer ND deconvolutional neural network with up- sampling as decoder for image generation (See more detail  in supplementary)
The encoder is treated as our φ(·) and decoder is treated as our φ(·)−N
We train on sequences of length N0
We use curriculum learning to sequentially increase the prediction horizon
At test time, the goal is  to generate four consecutive frames given only the initial  frame
 Baselines
We compare to the following models:  - LN loss
We use the same network architecture as ours based on DCGAN [NN], but only use the LN reconstruction error of future frame as loss
 - LSTM
We compare to the LSTM prediction model of [NN]
 - Generative Adversarial Network (GAN)
We perform the  ablation study of our model by removing the sequence modeling of imitation learning to demonstrate its importance
In  this case, the policy directly aims to minimize the local cost  functionD from the discriminator without sampling it to be  its long term expectationQ
On the other hand, the discriminator only aims to differentiate a single step generated by  our policy from the expert’s
This reduces our model to be  equivalent to GAN [N]
 Metric
We follow previous works [NN] and evaluate all  methods by human evaluation (See more detail for the setting of human evaluation in supplementary)
We show on  average how many times the output sequence of a method  can fool the annotator that it is an actual sequence from  the moving MNIST dataset
In this case, the upper bound  would be N0% where the generated sequence is the same as real, and the differentiation of real and generated is just by  chance
 Results
The result is shown in Table N
It verifies that  N00N    Accuracy (%) Real sequence  Real sequence N0.0  Our method NN.N  GAN N0.N  LSTM [NN] N.N  LN 0.N  Table N
Human evaluation results on the moving MNIST dataset
 Our imitation learning framework is able to predict and synthesize more realistic consecutive future frames compared to the baselines because our IRL’s minimization of long-term expected cost  at training
 Ground   Truth  Our   Method  GAN  LSTM  LN  t  Figure N
Qualitative results on the moving MNIST dataset
Our  model trained with IRL aims to minimize long-term expected cost  over the entire sequence
This results in having consistent highquality frame prediction across time step, and avoids the compounding error problem of baselines
 our method can better generate realistic future frames compared to all baselines
The qualitative results are shown in  Figure N (See more qualitative results in supplementary)
 It is important to note the difference of our method compared to GAN
As shown in Figure N, the result of GAN  becomes much worse and drifts away from the original digits as the time step increases
This is due to the fact that  it only considers the local cost at training time in contrast  to our IRL framework that handles the compounding error  problem in long-term prediction
On the other hand, our  method is trained with IRL objective and effectively show  high-quality synthesis even for long-term prediction
 N.N
Action Prediction on Videos in the Wild  We examined our method’s ability to perform action prediction on the videos in the wild following the setup of [NN],  where the goal is to forecast basic human actions one second before they start
In this experiment, frames synthesis  (φ−N) is no longer computationally feasible at scale
Therefore we need to use our deep reparametrization of the framework in Section N.N
We demonstrate the model’s ability to  capture the dynamics of natural videos directly from raw  pixels
 Dataset
We evaluate our results on TV Human Interactions dataset [NN], which has N00 videos of four different  interactive actions performed by people
We follow previous work [NN] and use the THUMOSNN dataset [N] as  our unsupervised imitation learning dataset
THUMOSNN  consists of trimmed UCF-N0N dataset [NN] and untrimmed  background videos
It has N0N action categories covering a wide range of actions in real world
 Implementation Details
We follow the training procedure  and hyperparameters in [NN] and reimplemented their methods for a fair comparison
Our reported number is similar to  that reported in [NN]
We train our model with the same setting
We first do imitation learning on the THUMOS dataset  with the states/frames being sampled at the same rate as test  time (one second in this case)
The predictive methods use  the “adapted” setting in [NN] during training
As it is infeasible to perform frame synthesis in this task, we use our  reparametrization of policy
Our φ(·) uses the architecture in [N0]
This architecture is also used baselines
Our π̂ for  sampling the learned deep representation uses autoencoder  with N-layer encoder and decoder (See more detail in supplementary)
 Baselines
We compare to the following methods:  - SVM
Two setups on SVM training are considered [NN]
 The first setup is to train SVM one second before the action  starts in the training time
In this case, the distribution of  visual features are the same at both training and testing time
 The second is to train SVM during the action at the training  time
In this case, the input at testing (one second before the  action) has different distribution from training, but training  frames are given the correct feature for the action
We refer  to the first one as SVM and the second and SVM static
 - LSTM
We compare to the LSTM prediction model of [NN]  as sequential training baseline that addresses the compounding error problem
 - Deep Regression Network [NN]
We compare to the direct  deep feature regression approach of Vondrick et al
[NN]
 Their K = N model can be treated as a proxy of ablation study of our approach by replacing IRL with LN regres- sion loss
We also reimplemented their best performing approach with explicit mixture model (K = N) for predicting multiple possible futures as reference
 -Pixel Synthesis
As mentioned in Section N.N, we resolve  the challenge of computational bottleneck from pixel synthesis by introducing a reparametrization that jointly learns  a deep representation
It is important to point out that this  not only makes our framework efficient, but also makes it  effective for semantic prediction
We develop a baseline  to demonstrate that our reparametrization is more effective  than synthesizing pixels of the future frames
We follow our  approach in Section N.N and apply the model to downsampled version (NN x NN) of the videos
We anticipate the action with the same setting used by other baselines
The only  difference is that now the SVM is applied to deep features  N00N    Hand  shake  High   Five  Hug  Kiss  Kiss  Hug  Predicted FrameInput  In co  rr e  ct C  o rr  e ct  Ground  Truth  Figure N
Single step action prediction results
Given an input  frame one second before the the interaction takes place, our model  is able to robustly predict the future frame and improve significantly over the baselines for action prediction
The frame visualization is retrieved by nearest neighbor image of the predicted  deep representation
 Model Accuracy (%)  Random NN.0  SVM static NN.N  SVM NN.N  Deep Regression (K = N) [NN] NN.N LSTM [NN] N0.N  Pixel Synthesis NN.N  Ours NN.N  Deep Regression (K = N) [NN] NN.0  Table N
Accuracy of action prediction on the TV Human Interaction dataset
Our learned policy significantly outperforms reconstruction loss based baselines
Our single mode policy also outperforms deep mixture regression model which is designed specifically for this task
 extracted from the synthesized future frames rather than the  deep representation directly predicted by our model
 Results
The results for action anticipation are shown in  Table N
Our IRL formulation of action prediction significantly outperforms the corresponding version using just LN loss (Deep Regression K = N)
In addition, we are able to outperform the sequential baseline using just the reconstruction loss (LSTM)
It is also important to note that our  single mode policy without mixture model is already able  to outperform the best performing Deep Regression with  explicit mixture model for handling multiple possible futures
Note that our reported reimplementation of K = N is slightly worse than their reporting number because we  perform unsupervised learning for all methods only on the  THUMOS dataset for a fair comparison
This indicates  Input Predicted Frames  Figure N
Multi-step video prediction results
Given only a single  frame as input, our learned policy is able to accurately forecast  multiple future frames in natural video
This verifies the effectiveness of our IRL formulation and reparametrization that allows  direct imitation from the pixel level
Images are retrieved by nearest neighbor of predicted deep feature
 that single mode regression is more prone to be affected by  dataset bias and demonstrate the robustness of our method
 On the other hand, the accuracy of Pixel Synthesis is NN.N%, which is lower than our framework by a large margin
This  verifies the importance of bypassing pixel synthesis in our  framework for anticipating action in natural videos
Qualitative results of action prediction is shown in Figure N (See  more qualitative results in supplementary)
It can be seen  that our learned policy robustly predicts the action after one  second
We further demonstrate that our model is able to  predict more than one step into future frames
The qualitative results are shown in Figure N
This shows the effectiveness of our imitation learning framework for visual  prediction, and verifies that our model can imitate natural  video from the pixel value using our reparametrization of  deep representation
 N.N
Storyline Forecasting on Semantic Dynamics  In the final experiment, we evaluate our method on the  challenging semantic forecasting in visual storylines [NN,  NN]
Given a storyline consisting of images, our goal is to  predict the next image
This is challenging as the change of  semantic meaning might not be capture directly through visual feature matching
Our setup is similar to the prediction  task of [NN]
 Dataset
We use the Visual Storytelling Dataset  (VIST) [NN] for our storyline forecasting task
The dataset  consists of photos from more than N00,000 albums from  Flickr
We randomly select one photo sequence of storyline from each album
Using the original dataset split in the  dataset, this results in total N0NN training, N0NN testing, and  NNN validation storylines, where each consists of five photos  illustrating a story
Note that while we follow the evaluation setup of [NN], their dataset is not applicable to our task  as they only have N0 storylines per concept for evaluation  only and does not have the large scale human demonstrated  N00N    Figure N
Accuracy of storyline prediction on the Visual Storytelling Dataset
We follow previous work and evaluate short-term  and long-term prediction separately
It can be seen that nearest  neighbor on the deep feature representation is a strong baseline  is this case because the goal is similar to retrieve visually similar frames
On the other hand, nearest neighbor does not work  for long-term prediction
Our general visual prediction framework  performs the best for the long-term prediction
This verifies that  our model can also be applied to the challenging semantic prediction, while learning directly from the pixel
 storyline available for imitation learning
 Experimental Setup and Metrics
We follow [NN] and  split the storyline evaluation into two goals: the first is  short-term, where the image is visually similar consecutive  image in the album
The second goal is long-term prediction, where prediction is for the next representative event involving large visual appearance change
We follow [NN] and  posed this as a classification task, where the goal is to select  the true image along with other four other images selected  randomly from the same album
We use the training set  storylines as expert demonstrations for both our imitation  learning and unsupervised learning of baselines
We report  resulting accuracy for the test split on both the short-term  and long-term prediction (See more detail for experimental  setup in supplementary)
 Baselines
We again compare to LSTM [NN] and Deep Regression Network [NN] from the previous task
In addition,  we compare to nearest neighbor of the ResNet-N0N feature  representation [N0], which is proven to be a strong baseline  for short-term prediction [NN]
 Results
The results of short-term and long-term prediction are shown in Figure N
For short term prediction, we  observe the similar results as with in [NN], where nearest  neighbor is the strongest baseline as the goal is similar to  finding the photo with the most similar visual feature
It  is important to note that our method still outperforms reconstruction loss based baselines (LSTM and Deep Regression Network) in this case
For the long-term prediction,  it can be seen that the nearest neighbor of visual appearance matching no longer works and performs poorly
On  the other hand, our direct imitation of storyline photos from  the pixel values is proven effective compared to both LSTM  and Deep Regression Networks
This verifies that our apInput Our NN Deep Regression   (K=N) Ground   Truth  Figure N
Qualitative results of storyline forecasting
Green boxes  indicate correct prediction of the next image in the storyline, and  red boxes indicate incorrect predictions
Our imitation learning  based framework can best capture the semantics by imitating storylines generated by human
On the other hand, baselines focusing  on appearance matching or reconstruction loss is harder to successfully predict semantics in the storylines
 proach is general and can even applied to imitate human behavior at the challenging semantic-level
Qualitative results  are shown in Figure N (See more qualitative results in supplementary)
It can be seen from the figure that our method  can better predict semantically meaning next photo in the  storyline
First three rows show our correct prediction, and  the final row shows incorrect prediction
It can be seen that  our method is able to capture the semantic meaning (event  or location) of the input photo and continue to forecast photos with similar semantic meaning
 N
Conclusion  We have presented a general framework for long-term visual prediction
We formulate this as an IRL problem, and  extend previous works to directly imitate natural visual sequences from the pixel level
The key challenge is the highdimensional and continuous natural of images
We address  this by (N) introducing dual formulation of IRL to natural  visual sequences that bypasses exhaustive state-action pair  visit, and (N) reparametrization using deep representation to  avoid explicit state synthesis during gradient computation
 We demonstrated that our framework is general and effective using three tasks at different level of abstractions: (N)  future frame generation, (N) action prediction, (N) storyline  forecasting
We verified that our unified framework is able  to outperform existing methods on all three tasks
 Acknowledgement
We thank National Taiwan University  (NTU-N0NRN0N0NN), NOVATEK Fellowship, MediaTek,  Intel, Naval Research (N000NN-NN- N-NNNN), and Panasonic  for their support
We thank Danfei Xu, Linxi Fan, Animesh  Garg, Tseng-Hung Chen and Hou-Ning Hu for helpful comments and discussion
 N00N    References  [N] A
Alahi, K
Goel, V
Ramanathan, A
Robicquet,  L
Fei-Fei, and S
Savarese
Social lstm: Human trajectory prediction in crowded spaces
In CVPR, N0NN
 N, N  [N] M
Bloem and N
Bambos
Infinite time horizon maximum causal entropy inverse reinforcement learning
 In CDC, N0NN
N  [N] N
J
Butko and J
R
Movellan
Optimal scanning for  faster object detection
In CVPR, N00N
N  [N] F.-H
Chan, Y.-T
Chen, Y
Xiang, and M
Sun
Anticipating accidents in dashcam videos
In ACCV, N0NN
 N  [N] C
Finn, I
Goodfellow, and S
Levine
Unsupervised  learning for physical interaction through video prediction
In NIPS, N0NN
N  [N] C
Finn, S
Levine, and P
Abbeel
Guided cost learning: Deep inverse optimal control via policy optimization
In ICML, N0NN
N, N  [N] I
Goodfellow, J
Pouget-Abadie, M
Mirza, B
Xu,  D
Warde-Farley, S
Ozair, A
Courville, and Y
Bengio
Generative adversarial nets
In NIPS, N0NN
N, N,  N  [N] A
Gorban, H
Idrees, Y.-G
Jiang, A
Roshan Zamir,  I
Laptev, M
Shah, and R
Sukthankar
THUMOS  challenge: Action recognition with a large number of  classes, N0NN
N, N, N  [N] S
Gu, E
Holly, T
Lillicrap, and S
Levine
Deep  reinforcement learning for robotic manipulation with  asynchronous off-policy updates
In ICRA, N0NN
N  [N0] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual  learning for image recognition
In CVPR, N0NN
N, N  [NN] J
Ho and S
Ermon
Generative adversarial imitation  learning
In NIPS, N0NN
N, N, N, N  [NN] J
Ho, J
K
Gupta, and S
Ermon
Model-free imitation  learning with policy optimization
In ICML, N0NN
N,  N  [NN] S
Hochreiter and J
Schmidhuber
Long short-term  memory
Neural computation, NNNN
N  [NN] D.-A
Huang and K
M
Kitani
Action-reaction: Forecasting the dynamics of human interaction
In ECCV,  N0NN
N, N, N  [NN] T.-H
K
Huang, F
Ferraro, N
Mostafazadeh, I
Misra,  J
Devlin, A
Agrawal, R
Girshick, X
He, P
Kohli,  D
Batra, et al
Visual storytelling
In NAACL, N0NN
 N, N, N  [NN] P
Isola, J.-Y
Zhu, T
Zhou, and A
A
Efros
Imageto-image translation with conditional adversarial networks
In CVPR, N0NN
N  [NN] A
Jain, A
Singh, H
S
Koppula, S
Soh, and A
Saxena
Recurrent neural networks for driver activity anticipation via sensory-fusion architecture
In ICRA,  N0NN
N  [NN] N
Kalchbrenner, A
v
d
Oord, K
Simonyan,  I
Danihelka, O
Vinyals, A
Graves, and  K
Kavukcuoglu
Video pixel networks
arXiv  preprint arXiv:NNN0.00NNN, N0NN
N, N  [NN] K
M
Kitani, B
D
Ziebart, J
A
Bagnell, and  M
Hebert
Activity forecasting
In ECCV, N0NN
N, N,  N  [N0] H
Koppula and A
Saxena
Anticipating human activities using object affordances for reactive robotic response
In TPAMI, N0NN
N  [NN] H
Kretzschmar, M
Spies, C
Sprunk, and W
Burgard
Socially compliant mobile robot navigation via  inverse reinforcement learning
IJRR, N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet classification with deep convolutional neural  networks
In NIPS, N0NN
N  [NN] T
Lan, T.-C
Chen, and S
Savarese
A hierarchical  representation for future action prediction
In ECCV,  N0NN
N, N  [NN] Y
LeCun, L
Bottou, Y
Bengio, and P
Haffner
 Gradient-based learning applied to document recognition
IEEE, NNNN
N  [NN] J
Li, W
Monroe, T
Shi, A
Ritter, and D
Jurafsky
 Adversarial learning for neural dialogue generation
In  EMNLP, N0NN
N  [NN] W.-C
Ma, D.-A
Huang, N
Lee, and K
M
Kitani
 Forecasting interactive dynamics of pedestrians with  fictitious play
In CVPR, N0NN
N, N  [NN] S
Mathe and C
Sminchisescu
Action from still image dataset and inverse optimal control to learn task  specific visual scanpaths
In NIPS, N0NN
N, N  [NN] M
Mathieu, C
Couprie, and Y
LeCun
Deep multiscale video prediction beyond mean square error
In  ICLR, N0NN
N  [NN] I
Misra, C
L
Zitnick, and M
Hebert
Shuffle and  learn: unsupervised learning using temporal order verification
In ECCV, N0NN
N  [N0] V
Mnih, N
Heess, A
Graves, et al
Recurrent models  of visual attention
In NIPS, N0NN
N  [NN] A
Y
Ng, S
J
Russell, et al
Algorithms for inverse  reinforcement learning
In ICML, N000
N, N  [NN] L
Paletta, G
Fritz, and C
Seifert
Q-learning of sequential attention for visual object recognition from  informative local descriptors
In ICML, N00N
N  [NN] H
S
Park, J.-J
Hwang, Y
Niu, and J
Shi
Egocentric  future localization
In CVPR, N0NN
N  N00N    [NN] D
Pathak, R
Girshick, P
Dollár, T
Darrell, and  B
Hariharan
Learning features by watching objects  move
In CVPR, N0NN
N  [NN] A
Patron-Perez, M
Marszalek, A
Zisserman, and  I
D
Reid
High five: Recognising human interactions  in tv shows
In BMVC, N0N0
N, N, N  [NN] M
L
Puterman
Markov decision processes: discrete  stochastic dynamic programming
John Wiley & Sons,  N0NN
N  [NN] A
Radford, L
Metz, and S
Chintala
Unsupervised  representation learning with deep convolutional generative adversarial networks
In ICLR, N0NN
N, N  [NN] G
A
Sigurdsson, X
Chen, and A
Gupta
Learning  visual storylines with skipping recurrent neural networks
In ECCV, N0NN
N, N  [NN] K
Soomro, A
R
Zamir, and M
Shah
UcfN0N: A  dataset of N0N human actions classes from videos in  the wild
arXiv preprint arXiv:NNNN.0N0N, N0NN
N  [N0] B
Soran, A
Farhadi, and L
Shapiro
Generating notifications for missing actions: Don’t forget to turn the  lights off! In ICCV, N0NN
N  [NN] N
Srivastava, E
Mansimov, and R
Salakhutdinov
 Unsupervised learning of video representations using  lstms
In ICML, N0NN
N, N, N, N, N  [NN] C
Vondrick, H
Pirsiavash, and A
Torralba
Anticipating visual representations from unlabeled video
In  CVPR, N0NN
N, N, N, N, N  [NN] C
Vondrick, H
Pirsiavash, and A
Torralba
Generating videos with scene dynamics
In NIPS, N0NN
N,  N  [NN] C
Vondrick and A
Torralba
Generating the future  with adversarial transformers
In CVPR, N0NN
N  [NN] J
Walker, C
Doersch, A
Gupta, and M
Hebert
An  uncertain future: Forecasting from variational autoencoders
In ECCV, N0NN
N  [NN] J
Walker, A
Gupta, and M
Hebert
Patch to the future: Unsupervised visual prediction
In CVPR, N0NN
 N, N  [NN] J
Walker, A
Gupta, and M
Hebert
Dense optical  flow prediction from a static image
In ICCV, N0NN
N,  N  [NN] X
Wang and A
Gupta
Unsupervised learning of visual representations using videos
In ICCV, N0NN
N  [NN] D
Xie, S
Todorovic, and S.-C
Zhu
Inferring “dark  matter” and “dark energy” from videos
In ICCV,  N0NN
N  [N0] T
Xue, J
Wu, K
Bouman, and B
Freeman
Visual dynamics: Probabilistic future frame synthesis  via cross convolutional networks
In NIPS, N0NN
N, N  [NN] X
Yan, J
Yang, K
Sohn, and H
Lee
AttributeNimage: Conditional image generation from visual attributes
In ECCV, N0NN
N  [NN] L
Yu, W
Zhang, J
Wang, and Y
Yu
Seqgan: sequence generative adversarial nets with policy gradient
In AAAI, N0NN
N  [NN] K.-H
Zeng, S.-H
Chou, F.-H
Chan, J
C
Niebles,  and M
Sun
Agent-centric risk assessment: Accident  anticipation and risky region localization
In CVPR,  N0NN
N  [NN] B
D
Ziebart, A
L
Maas, J
A
Bagnell, and A
K
 Dey
Maximum entropy inverse reinforcement learning
In AAAI, N00N
N, N, N  N00NOrientation Invariant Feature Embedding and Spatial Temporal Regularization for Vehicle Re-Identification   Orientation Invariant Feature Embedding and Spatial Temporal Regularization  for Vehicle Re-identification  Zhongdao WangN,N, Luming TangN,N, Xihui LiuN,N, Zhuliang YaoN,N, Shuai YiN, Jing ShaoN, Junjie YanN,  Shengjin WangN, Hongsheng LiN, Xiaogang WangN  NSenseTime Group Limited NTsinghua University NThe Chinese University of Hong Kong  yishuai@sensetime.com hsli@ee.cuhk.edu.hk  Abstract  In this paper, we tackle the vehicle Re-identification  (ReID) problem which is of great importance in urban  surveillance and can be used for multiple applications
In  our vehicle ReID framework, an orientation invariant feature embedding module and a spatial-temporal regularization module are proposed
With orientation invariant feature embedding, local region features of different orientations can be extracted based on N0 key point locations and  can be well aligned and combined
With spatial-temporal  regularization, the log-normal distribution is adopted to  model the spatial-temporal constraints and the retrieval results can be refined
Experiments are conducted on public  vehicle ReID datasets and our proposed method achieves  state-of-the-art performance
Investigations of the proposed  framework is conducted, including the landmark regressor  and comparisons with attention mechanism
Both the orientation invariant feature embedding and the spatio-temporal  regularization achieve considerable improvements
N N N  N
Introduction  In this paper, we target on the problem of vehicle reidentification (ReID), which aims to identify all the images  of the same vehicle from a large gallery database
Such a  task is particularly useful when the car licence plate is occluded or cannot be seen clearly
Vehicle ReID methods can  be used in these scenarios to effectively locate vehicles of  interest from surveillance databases
They have extensive  NZ
Wang and L
Tang share equal contribution
NS
Yi and H
Li are the corresponding authors
NThis work is supported in part by SenseTime Group Limited, in part by  the General Research Fund through the Research Grants Council of Hong  Kong under Grants CUHKNNNNNNNN, CUHKNNN0NNNN, CUHKNNN0NNNN,  CUHKNNNNNN, CUHKNNN0N0NN, CUHKNNNNNNNN, CUHKNNN0NNNN, in  part by the Hong Kong Innovation and Technology Support Programme  Grant ITS/NNN/NNFX, and in part by the China Postdoctoral Science Foundation under Grant N0NNMNNNNNN
 (a) (b)  (c) (d)  A B CLocation  Snapshot  (e)  Figure N
Difficulties of the problem of vehicle ReID
(a-b) Vehicle  image pairs that share quite similar overall appearances
They can  only be distinguished from local regions like the wheels in (a) and  the logos in (b)
(c-d) Different faces of one vehicle may have different visibility, which results in difficulties in aligning the features  of different faces
In our proposed orientation invariant network,  visible faces such as the right faces in (c) and the back faces in (d)  are assigned with a larger weight in the final feature embedding  process
(e) Spatio-temporal constraints of a vehicle’s appearance  should be satisfied, e.g
around NN.NN seconds between A and B  and NN.NN seconds between B and C
 applications in intelligent surveillance and attract increasing attention in recent years
Compared with the problem  of person ReID, which has been studied for years, vehicle  ReID is a recently proposed research topic
There exist specific characteristics and challenges for this problem
 Firstly, some specific regions in vehicle images are important for vehicle ReID
Different from person images,  which contain rich textures, the vehicles are generally solidcolored and sometimes the color patterns between different  vehicles can be quite similar
For example, as shown in  Fig
N (a-b), the cars are all red and are difficult to distinguish based on their general appearances
The wheel regions in (a) and the logo regions in (b) are keys to determine  whether those vehicles are the same ones
Most existing  NNNN    vehicle ReID approaches [N, N, NN] focus on the whole image and such subtle differences cannot be well taken into  account
In our proposed method, region features are calculated based on N0 vehicle key point locations
In this way,  vehicles with subtle differences can be well distinguished
 Secondly, there are always some key points not visible  for vehicle images in one view
If we simply consider a vehicle as a cube with four sides, at most two of them could  be visible at each time
As shown in Fig
N (c), the same  car is captured in two views
The frontal face is invisible  in (cN) while the left face is invisible in (cN)
It is similar for the bus shown in Fig
N (d)
Therefore, comparing  whole-image features between vehicle images with different views is generally not optimal
In our framework, the  N0 key points are clustered into four sets based on their orientations (front, back, left, and right), so that the key points  in each set share the same visibility (all visible or all invisible)
In order to distinguish the visible key point sets from  the invisible ones, an orientation based feature calculation  module is proposed in our framework
Different learnable  weights are assigned to different key point sets according  to the orientation of the input vehicle image
For instance,  large weights are assigned to the right face in (c), as well as  the back face in (d)
In this way, the visible key points can  contribute more to the final decision while the influences of  invisible key points are weakened by lower weights
In addition, orientation invariant feature embedding is proposed  to transform the weighted region features into the final orientation invariant feature vector
 Lastly, spatio-temporal constraints are also helpful for  vehicle ReID
As shown in Fig
N (e), if a car is observed  in camera (A), it is more likely to be observed in camera  (B) with a time delay around NN.NN seconds
In the proposed approach, a conditional spatio-temporal distribution  is modeled to regularize the final ReID results
 The proposed framework is evaluated on two standard  vehicle ReID datasets, i.e
VeRi-NNN [N] and VehicleID [N]
 Our proposed framework outperforms state-of-the-art vehicle ReID methods
It achieves a mAP of 0.NNN on the Veri- NNN [N] dataset, NN% higher than the the best result (0.NNN) in literature [N]
For the VehicleID [N] dataset, a Top-N accuracy of NN.0% can be achieved, which is NN% higher than the the best result (NN.N%) in literature [N]
 The contribution of this work can be summarized as follows
N) A deep learning framework is proposed for vehicle ReID, which contains four main components
The  orientation-based region proposal module and feature extraction module are proposed to capture vehicles’ region appearance information thus different vehicles showing similar overall appearances can be better distinguished
The orientation invariant feature aggregation module is proposed  so that the region features of different views can be aligned  and combined
The spatio-temporal regularization module  is proposed to utilize spatio-temporal constraints to regularize the final ReID results
N) The proposed framework is  evaluated on two vehicle ReID datasets
Significant performance improvements over existing methods are achieved
 N) Ablation study of the proposed framework is conducted  to investigate the effectiveness of its individual compoents,  which includes investigations on the key point regressor and  the comparisons with attention mechanism
 N
Related Work  Re-identification (ReID) is widely studied in computer  vision which has various important applications
Most existing ReID methods focused on the person ReID problem,  which aims to find target persons in a large gallery set given  probe images
Many hand-crafted features are proposed to  capture visual features for pedestrians [N,N,N,NN,NN,NN]
Recently, CNN-based features [N,NN,NN,NN] have also achieved  great progress on person ReID
 Vehicle ReID is a newly proposed research topic and  has not received much attention
Recent works on vehicle ReID mainly concentrate on building retrieval pipelines  and benchmarks
Liu et al
[N] released a high-quality  multi-viewed vehicle ReID dataset (named VeRi-NNN) with  NNN vehicle identities, and proposed a progressive retrieval  pipeline by combining vehicle appearance features, license plates, and spatio-temporal information
Another  large surveillance-nature vehicle ReID dataset (VehicleID)  is proposed by Liu et al
[N], which contains more than  N0,000 identities
The Coupled Clusters Loss (CCL) is  proposed for performance evaluation on this benchmark  dataset
However, all these approaches on vehicle ReID utilize global appearance features of the input vehicle but do  not focus on specific local discriminative regions
 Fine-grained vehicle model classification is relevant  to vehicle ReID
Both tasks focus on learning discriminative feature representations for vehicle appearance
Yang et  al
[NN] published a large scale dataset (CompCars) for finegrained vehicle model classification, which is the largest  vehicle model dataset
Dominik et al
[NN] and Jakub et  al
[NN] proposed to using ND-boxes for aligning different  vehicle faces and three visible faces are used for accurate  feature extraction
However, this method may introduce  ambiguities when the three visible faces are different
In  our proposed method, the local feature extraction and aggregation modules is used to solve this issue
 Object key point localization has many important applications, e.g., face alignment [NN] and human pose estimation [NN, N0]
Key point-based face alignment is conducted  in most face recognition frameworks [NN, NN]
Locations of  key points are helpful as the learned features can be well  aligned by the key points
However, vehicle key points are  not well studied in existing literature
Our proposed method  shows that vehicle key points can guide the learning and  NN0    CNN  Landmark  regressor  Input image  CNN  CNN  CNN  CNN  CNN  M A X  A V E  M A X  M A X  M A X  FC Softmax FC  NNN-d orientation  invariant  feature  (a) Orientation-based Region Proposal  (b) Orientation-based Feature Extraction  (c) Orientation Invariant Feature Aggregation  Stage-N Stage-N  Figure N
Illustration of the overall feature embedding pipeline,  which consists of (a) the orientation-based region proposal module, (b) the orientation-based feature extraction module, and (c)  the orientation-invariant feature aggregation module
 alignment of local regions in input vehicle images and improve the overall vehicle ReID performance
 N
Methodology  Our framework consists of two main components, the  orientation invariant feature embedding component and the  spatial-temporal regularization component
The pipeline of  the orientation invariant feature embedding component is  presented in Fig
N, including three sub-modules, i.e
the  orientation-based region proposal module (Sec
N.N), the  orientation-based feature extraction module (Sec
N.N), and  the orientation invariant feature aggregation module (Sec
 N.N)
Firstly, vehicle images are fed into the region proposal module, which produces the response maps of N0 vehicle key points
The key points are then clustered into four  orientation-based region proposal masks
Afterwards, the  original image together with the four region proposal masks  are utilized by the feature learning module to obtain one  global feature vector and four region feature vectors
Finally, these features are fused by the aggregation module  that outputs an orientation invariant feature vector
Besides  learning the above mentioned appearance feature representations, a regularization strategy (Sec
N.N) is adopted by  modeling the spatio-temporal relations between the probe  and gallery images
Training details of the proposed framework are introduced in Sec
N.N
 N.N
Orientation-based Region Proposal  As shown in Fig
N(a), a region proposal network is introduced in this section, which contains two steps, i.e
vehicle key point prediction and orientation-based region mask  generation
The proposed region proposal network takes  the image as input and estimates the vehicle key point locations
Four orientation-based region proposal masks are  then generated based on the key points
 N N NN  NN NN  N N NN  NN NN  N0 N0NNN  NN  N  N  N  N  NNNN  NN  NN  NN  NN  NN NN  NN  landmarks on front face landmarks on back face landmarks on left face landmarks on right face  Figure N
Illustration of the N0 selected vehicle key points
The N0  points are clustered into four sets based on their orientations, i.e.,  the front face, the back face, the left face and the right face
 The first step of the region proposal network is to predict  one response map for each vehicle key point
As listed in  Table
N and shown in Fig
N, N0 key points are specified for  the vehicle ReID task
Instead of directly predicting boundary points or corner points, these key points are chosen as  some discriminative locations or some main vehicle components, e.g
the wheels, the lamps, the logos, the rear-view  mirrors, the license plates
 Inspired by the Stacked Hourglass Networks which generate response maps of human joints in a stacked coarse-tofine manner for human pose estimation [NN], an hourglasslike fully convolution network is adopted to generate vehicle key point response maps
The key point regressor  takes the image as input and outputs one response map  Fi ∈ RX×Y (i ∈ N, ..., N0) for each of the N0 key points, where X and Y are the horizontal and vertical dimensions of the feature maps
 The target response maps have Gaussian-like responses  around the ground truth locations of key points and used as  training supervisions
However, the Hourglass model [NN]  is computational expensive
Modifications to the network  are made to reduce model complexity and also preserve the  quality of output key point response maps
The input image size, the number of framework stages and the channel numbers of convolution layers are all reduced for fast  computation
The per-pixel cross entropy loss between estimated response maps and the ground truth maps is adopted  for training the network
 The second step of the region proposal network is  to generate four orientation-based region masks
As  introduced in Sec
N, there are always some invisible regions for vehicles in specific orientations
To address the issue of invisible key points and make full  use of the geometrical relationships among key points,  the N0 key points indexed in Table N are assigned to  four clusters, i.e., CN = [N, N, N, N, N, N0, NN, NN], CN = [NN, NN, NN, NN, NN, N0], CN = [N, N, N, N, NN, NN, NN, NN], and CN = [N, N, N, N, NN, NN, NN, NN], corresponding to the key points belonging to the vehicle’s front face, back face, left  NNN    N left-front wheel NN left rear-view mirror  N left-back wheel NN right rear-view mirror  N right-front wheel NN right-front corner of vehicle top  N right-back wheel NN left-front corner of vehicle top  N right fog lamp NN left-back corner of vehicle top  N left fog lamp NN right-back corner of vehicle top  N right headlight NN left rear lamp  N left headlight NN right rear lamp  N front auto logo NN rear auto logo  N0 front license plate N0 rear license plate  Table N
Definition of the N0 selected vehicle key points
 (a)  (b)  (c)  Figure N
Examples of the four output response masks of the orientation based region proposal module
The input image and the  corresponding four region masks, i.e
RN of front face, RN of the  rear face, RN of the left face and RN of the right face, are shown  in each row, from left to right respectively
Features of invisible  faces, e.g., RN of (a) and (c), and RN of (b), generally have low response masks
A feature aggregation method is adopted to reduce  their impact on the final feature vector
 face, and right face, respectively
The final output region  masks are computed as the summation of all the feature  maps belonging to each cluster, i.e
 Ri = ∑  l∈Ci  Fl, (i = N, N, N, N)
(N)  Examples of the output region masks Ri are shown in Fig
N
From the results, we can observe that visible region  masks generally have larger responses than the invisible  ones, which demonstrate that the learned key point localization model not only estimates the key point locations but  also discriminates the visible key points from the invisible  ones
As the invisible region masks may not be suitable for  feature extraction, the orientation invariant feature aggregation is proposed in Sec
N.N to handle such problem
 N.N
Orientation-based Feature Extraction  In the feature extraction module, deep convolutional neural network (CNN) is adopted to obtain one feature vector from the whole vehicle image and four orientationrelated region feature vectors from the four corresponding  regions
The network structure is shown in Fig
N (b), which  contains two convolution stages, i.e
Stage-N, and StageN
The global feature and local features are extracted in a  backbone-branch fashion
 In Stage-N, input images are resized to NNN × NNN and convolved by three convolution layers and two inception  modules [NN]
The output feature map is denoted as fCN0 with spatio size NN × NN
In Stage-N, fN0 is assigned to five branches, including one global branch and four local  region branches
For the global branch, the global feature  map fN0 is convolved by one more inception module, and results in a set of N × N feature maps
Then global aver- age pooling is applied on these feature maps to obtain a  NNNN-dimensional global feature vector fN0 
For each local branch, the corresponding orientation-related region masks  Ri(i = N, N, N, N) is resized to the same size as fN0 , and fN0 is element-wisely multiplied by the region masks to obtain  the local feature maps, i.e
fNi = f N 0 · Ri(i = N, N, N, N)
 The results fNi is further convolved by one more inception module
Global max pooling is adopted since the maximum  responses are more suitable for guiding feature extraction  from local regions
Every region branch outputs a NNNNdimensional feature vector fNi (i = N, N, N, N)
 N.N
Orientation Invariant Feature Aggregation  As shown in Fig
N(c), the feature aggregation module takes the five NNNN-dimensional feature vectors, including one global feature fN0 and four local features f N i , (i =  N, N, N, N), as input and computes one NNN-dimensional fea- ture vector as output
In the aggregation module, the four local feature vectors are first concatenated and passed through  a fully connected layer, yielding a set of scalars {ei}
Then {ei} pass through the Softmax operator, producing a set of weights {wi}, where  ∑  i wi = N, (i = N, N, N, N)
The four local feature vectors are weighted by {wi} and concatenated together with the global feature vector fN0 
The concatena- tion result [fN0 , wNf  N N , wNf  N N , wNf  N N , wNf  N N ]  T , is then fed into  a fully connected layer and the output dimension is reduced  to NNN
The NNN-dimensional feature vector is the final aggregated feature vector of the whole image, including the  four local region features and one global region feature
 Examples in Fig
N demonstrates the effectiveness of the  proposed orientation invariant feature aggregation module
 Features of selected vehicle images in the VeRi-NNN test set  are projected to N-dimensional space using t-SNE [N0] and  are visualized in Fig
N(b)
We can observe that features  of the same identity can be clustered together, no matter  which orientation the vehicle image is
Moreover, the input  vehicle images and the corresponding learned weights of  two clusters are shown in Figs
N(a) and (c)
For each image,  the weights are learned for the four side faces, i.e
front,  back, left, and right, and then the local features are fused  based on these weights
We can observe that visible face  are more likely to have higher weights than invisible ones
 NNN    weight of font face weight of back face weight of left face weight of right face  (b)  (a)  (c)  Figure N
Illustration of the orientation invariant features with  t-SNE [N0]
(a,c) The input images of two different vehicles  and their corresponding learned weights for different orientations,  where visible faces are more likely to have higher weights
(b) ND  feature projections of selected vehicle images in the VeRi-NNN test  set using t-SNE
 N.N
Regularization by spatio-temporal Modeling  In real-world scenarios, appearance features may not be  adequate enough to distinguish one vehicle from others, especially when the vehicles are of the same model without  personalized decorations
However, in surveillance applications, the location and time information of a vehicle is  easy to obtain
It is possible to refine vehicle search results  with the help of such spatio-temporal information
 In order to investigate whether the spatio-temporal constraints are effective for vehicle ReID, we analyze the vehicle transition interval between pairs of cameras
For each  camera pair, the transition interval can be modeled as a  random variable that follows some probability distribution
 Due to the Gaussian-like and long tail property of the transition interval, the logarithmic normal distribution is adopted  to model this random variable
Given l and e as the leav- ing and entering cameras, the conditional probability of the  transition interval τ between l and e can be estimated as the log-normal distribution p(τ |l, e),  p(τ |l, e;µl,e, σl,e) = lnN (τ ;µl,e, σl,e)  = N  τσl,e √ Nπ  exp  [  − (ln τ − µl,e) N  NσNl,e  ]  , (N)  where µl,eandσl,e are the parameters to be estimated for each camera pair (l, e)
The model parameters can be esti- mated by maximizing the following log-likelihood function,  L(τ |l, e;µl,e, σl,e) = N ∏  n=N  (  N  τn  )  N (ln τn;µl,e, σl,e), (N)  ID:N0N Cam:00N t = NNNN0  = 0.0NNN  ID:N0N Cam:00N t = NNNNN  = 0.0NNN  ID:N0N Cam:0NN t = NNNN = 0.0NNN  ID:N0N Cam:00N t = NNN00  = 0.0NNN  ID:N0N Cam:0NN t = NNNNN  = 0.0NNN  ID:N0N Cam:0NN t = NNNNN  0.0NN 0.NNN0 0.NNNN Regularization   by  ID:N0N Cam:0NN t = NNNNN  = 0.NNN0  ID:N0N Cam:00N t = NNNNN  = 0.NNNN  ID:N0N Cam:00N t = NNNN0  = 0.NNNN  ID:N0N Cam:00N t = NNN00  = 0.NNN0  ID:N0N Cam:0NN t = NNNN = 0.NNNN  Figure N
Illustration of the proposed spatio-temporal regularization step
Images in the first row are the query image and the  top-N retrieval results without spatio-temporal regularization
The  green box represents the correct hit while red ones denote noncorresponding vehicles
The spatio-temporal distance Ds between  probe and gallery images are computed using the estimated lognormal distribution
The gallery images are regularized, and the  results after regularization are shown in the bottom row
 where τn ∈ τ (n = N, N, N, ..., N) is transition interval be- tween camera pair (l, e) sampled from the training set, and τ contains all the time interval samples between the two  cameras in the training set
 During the retrieval process, the appearance distance Da is first computed via the proposed orientation-invariant feature aggregation framework
The spatio-temporal distance  Ds is then computed for regularization
As shown in Fig
N, the transition time interval between two cameras (l, e) can be computed as τ = |tl − te|, where tl, te are the ap- pearance time of this vehicle at these two cameras
The  spatio-temporal probability can be computed as  p(τ |l, e;µl,e, σl,e) = lnN (τ ;µl,e, σl,e)
(N)  High probabilities corresponds to small distances, thus  Ds = N/(N + e α(p(τ |l,e;µl,e,σl,e)−0.N))
(N)  Finally the overall similarity distance between the probe and  gallery images are calculated as the weighted summation,  D = Da + βDs, (N)  where α is set to N and β is set to 0.N in our experiments
 N.N
Training Scheme  An alternative training strategy is adopted to train the  proposed network, which include the following four steps
 NNN    (b)  (a)  (c)  (d)  Figure N
Datasets used for training the proposed model, including  (a) VeRi-NNN [N], (b) VehicleID [N], (c) BoxCarsNNk [NN],and (d)  CompCars [NN]
 (i) The backbone of Stage-N and the global branch of StageN are trained from random initialization, by applying supervision to the global feature of full image region
(ii) With  Stage-N fixed, the four orientation branches are trained with  parameters initialized as the global branch of Stage-N, since  the global branch and the orientation branches in Stage-N  share the same structure
The four branches are trained separately by giving the classification label as supervision
(iii)  With Stage-N and all branches of Stage-N fixed, the orientation invariant feature aggregation module is trained
(iv) Initializing all the modules with parameters learned from the  above steps, and all the parameters are jointly fine-tuned
 When training the model, existing vehicle datasets are used  and the cross-entropy classification loss is adopted
 N
Experiments  N.N
Datasets  Four existing vehicle datasets are used to train the proposed orientation invariant network, including VeRi-NNN  [N], VehicleID [N], BoxCarsNNk [NN], and CompCars [NN]
 VeRi-NNN [N] is a benchmark dateset for vehicle ReID that is  collected from real-world surveillance scenarios, with over  N0,000 images of NNN vehicles in total
VehicleID [N] is  a surveillance dataset, which contains NN,NNN vehicles and  NN,NNNN images in total
BoxCarsNNk [NN] is designed for  fine-grained vehicle make and model recognition
The images of BoxCarsNNk are ordered by identities thus can also  be used for vehicle ReID
This dataset contains NN,NN0 vehicle identities and NN,NN0 images
CompCars [NN] is also designed for fine-grained vehicle model classification, which  consists of both web images and surveillance images
However, we only utilize its surveillance data for training
Images in this dataset are sorted by vehicle model and color  annotations are also provided
We can roughly regard vehicles with specific model and specific color as a specific  Dataset #Trn ID/img #Prb ID/img #Gal ID/img  VeRi-NNN [N] NNN/N0NNN N00/NNNN N00/NNNNN  VehicleID [N] NNNNN/N00NNN NN00/NNNNN NN00/NN00  BoxCars [NN] NNNN0/NNNN0 - / - - / CompCars [NN] NNNN/NNNNN - / - - / Table N
Statistics of the four datasets used in our experiment
The  number of train identities and images, together with the number of  query and gallery identities and images are listed
 identity to train our ReID network
 We merge the training samples from VeRi-NNN [N] and  VehicleID [N], together with all the samples from BoxCarsNNk [NN] and CompCars [NN] into one large training set  to train our orientation invariant network
The training set  contains around NNN,NNN images of NN,N0N identities in total
Selected samples of these datasets are shown in Fig
N  and the statistical information are listed in Table N
 N.N
Evaluation results  The proposed framework is compared with two stateof-the-art vehicle ReID approaches, i.e
PROVID [N] and  DRDL [N], together with several conventional person ReID  methods, i.e
Bag of Words with Color Name Descriptor  (BOW-CN) [NN], the LOMO feature [N], and the KEPLER  method [NN], which learns salient regions for constructing discriminative features
Performance evaluation is conducted on VeRi-NNN [N] and VehicleID [N], and multiple  evaluation metrics are applied
 For the VeRi-NNN dataset, cumulative match curve  (CMC) metric [N] is adopted for evaluation
For each identity, one image is random selected from all the gallery images to generate the gallery set, while the probe set remains  unchanged
The random selection procedure was repeated  for N00 times to obtain an average CMC result
The imageto-track metric (HIT) introduced in [N] is also evaluated,  and there is no random gallery selection process for the HIT  evaluation
Mean average precision (mAP) is also adopt for  evaluation following [N]
 For the VehicleID dataset, standard CMC metric is  adopted with random gallery selection
Only the Large test  set of VehicleID is evaluated since it is the most challenging  set
During testing, one image is randomly selected from  one identity to obtain a gallery set with N,N00 images, then  the remaining NN,NNN images are all used as probe images
 Evaluation results on both datasets are listed in Table  N
The proposed approach achieves the best performance  on both datasets, which is much better than the compared  methods
Three experiments are conducted to demonstrate  the effectiveness of the proposed main modules
Firstly,  a baseline single-branch appearance model (“Baseline”) is  evaluated to investigate the performance of our proposed  network
Significant performance gain can be observed  NNN    VeRi-NNN mAP HIT@N CMC@N CMC@N  BOW-CN [NN] NN.N0 NN.N - LOMO [N] N.NN NN.N - KEPLER [NN] NN.NN NN.N NN.N NN.N  PROVID [N] NN.NN NN.N - Baseline NN.N0 NN.NN NN.N NN.N  Ours NN.00 NN.NN NN.N NN.N  Ours + ST NN.NN NN.NN NN.N NN.N  VehicleID CMC@N CMC@N  KEPLER [NN] NN.N NN.N  VGG + Triplet Loss [N] NN.N N0.N  VGG + CCL [N] NN.N NN.N  Mixed Diff + CCL [N] NN.N NN.N  Baseline NN.N N0.N  Ours NN.0 NN.N  Table N
Experiment results of the proposed method and other  compared methods on the VeRi-NNN dataset and VehicleID dataset
 Multiple evaluation criteria are adopted, including mAP, HIT accuracy, and CMC accuracy
“Baseline” refers to our single mainbranch model without region features, “Ours” denotes the proposed orientation invariant network, and “Ours+ST” indicates the  overall pipeline with the proposed spatio-temporal regularization  model
 compared with other methods
Secondly, the proposed orientation invariant network (“Ours”) is tested by using the  proposed orientation invariant appearance features
Region  features are introduced and the performance can be further improved
Finally, the overall framework (“Ours+ST”)  is evaluated by adding the spatio-temporal regularization  module
This experiment is only conducted on VeRi-NNN  since no spatio-temporal information is provided on VehicleID
Experimental result demonstrates that the regularization module results in significant improvements
 Note that the HIT@N results of the proposed method  in Table N outperform existing methods by large margins
 This is because the HIT@N metric is based on image-totrack search, and all gallery images (which contain multiple  ground truth results) are searched to identify the probe identity
If there exists one gallery image of this identity that  shares similar orientation as the probe image, such gallery  images can be easily found
In this case, CMC should be  a more proper metric for vehicle ReID evaluation, since the  image-to-track search may always obtain search results with  similar orientation as the probe image
 N
Ablation Study  N.N
Investigations on the Key point Regressor  In this section, the proposed key point regressor is thoroughly investigated, in terms of the regression accuracy and  relationship between landmarks and orientations
In order  to train and evaluate the key point regressor, locations of  Models r0 = N r0 = N  LN-Loss N0.N0 NN.N  Cross-Entropy Loss NN.0N NN.N  Table N
Evaluation results of the landmark regressor
 (a) (b)  Figure N
(a) Annotation examples
(b) Regression results
 N0 key points are annotated manually on the images of the  whole VeRi-NNN [N] dataset and some annotation results are  shown in Fig
N (a)
During the testing stage, response maps  of the testing images are extracted and the key points are  predicted as the locations with maximum response value
If  the distance between the regressed landmark location and  the ground truth location is smaller than a threshold r0, this key point is considered as correctly predicted, otherwise the  key point is wrongly predicted
Invisible key points are ignored in the evaluation step since they are expected to be  handled by the proposed orientation invariant module
The  prediction accuracy of visible key points are listed in Table  N, and two loss functions are adopted to train the landmark  model
We can observe that NN.N% key points can be cor- rectly predicted within r0 = N pixels to the ground truth (the final response map is of size NN× NN)
Some key point prediction results are shown in Fig
N (b)
 Investigations are also conducted on the relationship between key point locations and orientation classes
Since  no orientation information is provided for the VeRi-NNN  dataset, four orientations, i.e
front, back, left, right, are  manually annotated for the VeRi-NNN dataset
With the annotated training images, a vehicle orientation classifier is  trained by using the N0 landmark response maps as input
 The trained classifier yields a NN.N% accuracy on the testing images, which demonstrates our key point response maps  contain sufficient information to infer vehicles’ orientation
 It also validates that clustering landmarks by orientation  is reasonable in the proposed orientation-based region proposal module
 N.N
Comparison with Attention Mechanism  Besides the proposed orientation-based region proposal  module, attention mechanism is another possible way to select the salience regions and to obtain local region features
 Experiments are conducted to compare the soft attention  mechanism and the proposed orientation based region proposal framework
 We follow the standard strategy as [NN] to implement the  NNN    (a)  (b)  Figure N
Comparison between the learned salience masks by the  attention mechanism (in red boxes) and the orientation based region proposals (in green boxes)
 attention module
N × N convolution layers are employed to produce salience masks from the input feature maps
For  these salience masks, locations with larger value represent  salience regions that are useful for feature extraction
The  salience masks are then passed through a sigmoid nonlinear  unit, yielding values in the range (0, N)
Finally, element- wise multiplication is applied between the input feature  maps and the salience mask to output the local feature maps  of the salience regions
 The compared attention network is similar with the proposed pipeline in terms of network architecture
The only  difference is that the N orientation-based region proposals  are replaced with N attention masks
The attention module takes the feature maps fN0 as input and output N attention masks
Experiments are conducted by setting the number  of attention mask N = N, N, N for comparison and CMC results are reported in Table N
 The compared attention network and the proposed orientation invariant network is different
In attention network,  the salience regions are learned by the attention module  automatically, while in our proposed network the region  masks are defined based on orientation information (four  side faces of the vehicle with different landmark points) and  the landmarks are regressed by the trained landmark regressor
 The orientation based region proposals and the salience  masks learned by the attention modules are visualized in  Fig
N, which are also different
Attention masks are reasonable but not stable, i.e., different attention blocks may  provide quite similar salience masks (the first and the last  mask in Fig
N(b))
However, our orientation based region  proposals is much more stable, because they are designed  to focus on different faces
Experimental results in Table N  also demonstrate that the orientation based region proposals  outperform attention mechanism
 Models VeRi-CMC@N VehicleID-CMC@N  attention-Nbranch NN.N NN.N  attention-Nbranch NN.N NN.N  attention-Nbranch NN.N NN.N  Ours NN.N NN.N  Table N
Quantitative results by replacing orientation based region  proposal with attention masks
 Models VeRi-CMC@N VID-CMC@N  base NN.NN NN.N  global+KISSME [N] NN.0N NN.N  global+MLAPG [N] NN.NN NN.N  global+Zhang et al
[NN] NN.NN NN.N  final NN.NN NN.N  final+KISSME [N] NN.NN NN.N  final+MLAPG [N] NN.NN NN.N  final+Zhang et al
[NN] N0.0N NN.0  Table N
Experimental results with and without metric learning
 The first group of experiments are based on our global features  while the second group are based on our global + local features  N.N
Metric Learning Methods  In person Re-ID methods, metric learning is usually utilized to refine the final search results
In this section, we  tested some popular metric learning methods utilized in person Re-ID methods, including KISSME [N], MLAPG [N]  and Zhang et al
[NN]
We utilized the three metric learning algorithms on features extracted from both our baseline  model (global feature) and final model (global + local feature) and conducted two groups of experiments
The results  on both the VeRi and VehicleID (VID) datasets are listed in  Table N:  As show in the table, metric learning methods do lead  to additional performance gain based on our features
Note  that in the second group of experiments performance gains  by metric learning methods are smaller, which suggest that  our global+local features are more discriminative than the  global ones
 N
Conclusion  In this paper, a novel framework is proposed for vehicle ReID, which consists of two main components, i.e
 the orientation invariant feature embedding and the spatialtemporal regularization
Local region features are extracted  based on the locations of key points
Th features are aligned  and combined to form orientation invariant feature representations
Spatio temporal regularization is adopted for refining the retrieval results
The proposed framework is evaluated on two public vehicle ReID datasets and state-of-theart performance is achieved
Detailed investigations of the  proposed methods are conducted in terms of the landmark  regressor and the comparisons with attention mechanism
 NNN    References  [N] D
Chen, Z
Yuan, B
Chen, and N
Zheng
Similarity learning with spatial constraints for person re-identification
In  CVPR, N0NN
N  [N] D
Cheng, Y
Gong, S
Zhou, J
Wang, and N
Zheng
Person re-identification by multi-channel parts-based cnn with  improved triplet loss function
In CVPR, N0NN
N  [N] D
Gray, S
Brennan, and H
Tao
Evaluating appearance  models for recognition, reacquisition, and tracking
In Proc
 IEEE International Workshop on Performance Evaluation  for Tracking and Surveillance (PETS), volume N, N00N
N  [N] M
Koestinger, M
Hirzer, P
Wohlhart, P
M
Roth, and  H
Bischof
Large scale metric learning from equivalence  constraints
In CVPR, pages NNNN–NNNN
IEEE, N0NN
N  [N] W
Li and X
Wang
Locally aligned feature transforms  across views
In CVPR, N0NN
N  [N] S
Liao, Y
Hu, X
Zhu, and S
Z
Li
Person re-identification  by local maximal occurrence representation and metric  learning
In CVPR, June N0NN
N, N, N  [N] S
Liao and S
Z
Li
Efficient psd constrained asymmetric  metric learning for person re-identification
In ICCV, pages  NNNN–NNNN, N0NN
N  [N] H
Liu, Y
Tian, Y
Wang, L
Pang, and T
Huang
Deep  relative distance learning: Tell the difference between similar  vehicles
In CVPR, pages NNNN–NNNN, N0NN
N, N, N  [N] M
T
M
H
Liu X., Liu W
A deep learning-based approach  to progressive vehicle re-identification for urban surveillance
In ECCV, N0NN
N, N, N  [N0] L
v
d
Maaten and G
Hinton
Visualizing data using t-sne
 Journal of Machine Learning Research, N(Nov):NNNN–NN0N,  N00N
N, N  [NN] N
Martinel, C
Micheloni, and G
L
Foresti
Kernelized saliency-based person re-identification through multiple  metric learning
IEEE Transactions on Image Processing,  NN(NN):NNNN–NNNN, N0NN
N, N  [NN] T
Matsukawa, T
Okabe, E
Suzuki, and Y
Sato
Hierarchical gaussian descriptor for person re-identification
In CVPR,  N0NN
N  [NN] A
Newell, K
Yang, and J
Deng
Stacked hourglass networks for human pose estimation
In ECCV, pages NNN–NNN
 Springer, N0NN
N, N  [NN] S
Ren, X
Cao, Y
Wei, and J
Sun
Face alignment at N000  fps via regressing local binary features
In CVPR, pages  NNNN–NNNN, N0NN
N  [NN] F
Schroff, D
Kalenichenko, and J
Philbin
Facenet: A unified embedding for face recognition and clustering
In CVPR,  pages NNN–NNN, N0NN
N  [NN] Z
Shi, T
M
Hospedales, and T
Xiang
Transferring a semantic representation for person re-identification and search
 In CVPR, N0NN
N  [NN] J
Sochor, A
Herout, and J
Havel
BoxCars: ND boxes as  CNN input for improved fine-grained vehicle recognition
In  CVPR, June N0NN
N, N  [NN] Y
Sun, D
Liang, X
Wang, and X
Tang
DeepidN: Face  recognition with very deep neural networks
arXiv preprint  arXiv:NN0N.00NNN, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, June N0NN
N  [N0] S.-E
Wei, V
Ramakrishna, T
Kanade, and Y
Sheikh
Convolutional pose machines
In CVPR, pages NNNN–NNNN,  N0NN
N  [NN] L
Wu, C
Shen, and A
v
d
Hengel
Personnet: Person  re-identification with deep convolutional neural networks
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] T
Xiao, H
Li, W
Ouyang, and X
Wang
Learning deep feature representations with domain guided dropout for person  re-identification
arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] L
Yang, P
Luo, C
Change Loy, and X
Tang
A large-scale  car dataset for fine-grained categorization and verification
 In CVPR, June N0NN
N, N  [NN] Z
Yang, X
He, J
Gao, L
Deng, and A
Smola
Stacked  attention networks for image question answering
In CVPR,  June N0NN
N  [NN] D
Zapletal and A
Herout
Vehicle re-identification for automatic video traffic surveillance
In CVPR Workshops, pages  NN–NN, N0NN
N  [NN] L
Zhang, T
Xiang, and S
Gong
Learning a discriminative null space for person re-identification
In CVPR, pages  NNNN–NNNN, N0NN
N  [NN] H
Zhao, M
Tian, S
Sun, J
Shao, J
Yan, S
Yi, X
Wang,  and X
Tang
Spindle net: Person re-identification with human body region guided feature decomposition and fusion
 In CVPR, N0NN
N  [NN] L
Zheng, L
Shen, L
Tian, S
Wang, J
Wang, and Q
Tian
 Scalable person re-identification: A benchmark
In ICCV,  pages NNNN–NNNN, N0NN
N, N, N  NNNHuman Pose Estimation Using Global and Local Normalization   Human Pose Estimation using Global and Local Normalization  Ke Sun N, Cuiling Lan N, Junliang Xing N, Wenjun Zeng N, Dong Liu N, Jingdong Wang N  N University of Science and Technology of China, Anhui, China N Microsoft Research Asia, Beijing, China N National Laboratory of Pattern Recognition, Institute of Automation, Chinese Academy of Sciences, Beijing, China  sunk@mail.ustc.edu.cn, {culan,wezeng,jingdw}@microsoft.com, jlxing@nlpr.ia.ac.cn, dongeliu@ustc.edu.cn  Abstract  In this paper, we address the problem of estimating the  positions of human joints, i.e., articulated pose estimation
 Recent state-of-the-art solutions model two key issues, joint  detection and spatial configuration refinement, together using convolutional neural networks
Our work mainly focuses on spatial configuration refinement by reducing variations of human poses statistically, which is motivated by the  observation that the scattered distribution of the relative locations of joints (e.g., the left wrist is distributed nearly uniformly in a circular area around the left shoulder) makes the  learning of convolutional spatial models hard
We present  a two-stage normalization scheme, human body normalization and limb normalization, to make the distribution of the  relative joint locations compact, resulting in easier learning of convolutional spatial models and more accurate pose  estimation
In addition, our empirical results show that incorporating multi-scale supervision and multi-scale fusion  into the joint detection network is beneficial
Experiment results demonstrate that our method consistently outperforms  state-of-the-art methods on the benchmarks
 N
Introduction  Human pose estimation is one of the most challenging problems in computer vision and plays an essential  role in human body modeling
It has wide applications  such as human action recognition [NN], activity analyses  [N], and human-computer interaction [NN]
Despite many  years of research with significant progress made recently  [N, NN, N0, N, NN, NN], pose estimation still remains a very  challenging task, mainly due to the large variations in body  postures, shapes, complex inter-dependency of parts, clothing and so on
 This work was done when Ke Sun was an intern at Microsoft Research  Asia
Junliang Xing is partly supported by the Natural Science Foundation  of China (Grant No
NNNNNNNN)
 (a)  0.N 0.N 0.N 0.N 0.0 0.N 0.N 0.N 0.N x  0.N  0.N  0.N  0.N  0.0  0.N  0.N  0.N  0.N  y Head wrt center  (b)  0.N 0.N 0.N 0.N 0.0 0.N 0.N 0.N 0.N x  0.N  0.N  0.N  0.N  0.0  0.N  0.N  0.N  0.N  y  After body rotation  (c)  0.N 0.N 0.N 0.N 0.0 0.N 0.N 0.N 0.N x  0.N  0.N  0.N  0.N  0.0  0.N  0.N  0.N  0.N  y  Left wrist wrt shoulder  (d)  0.N 0.N 0.N 0.N 0.0 0.N 0.N 0.N 0.N x  0.N  0.N  0.N  0.N  0.0  0.N  0.N  0.N  0.N  y  After limb rotation  (e)  Figure N: Pose normalization can compact the relative position distribution
(a) Example images with various poses  from LSPET [NN]
(b) and (d) are originally relative positions of two joints
(b) shows the positions of heads with  respect to the body centers
(d) shows the positions of the  left wrists with respect to the left shoulders
(c) and (e) are  the relative positions after body and limb normalization corresponding to (b) and (d) respectively
The distributions of  the relative positions in (c) and (e) are much more compact
 NNNN    There are two key problems in pose estimation: joint detection, which estimates the confidence level of each pixel  being some joint, and spatial refinement, which usually refines the confidence for joints by exploiting the spatial configuration of the human body
 Our work follows this path and mainly focuses on spatial configuration refinement
It is observed that the distribution of relative positions of joints may be very diverse  with respect to their neighboring ones
Examples regarding  the distributions of the joints on the LSPET dataset [NN] are  shown in Figure N
The relative positions of the head with  respect to the center of the human body are shown in Figure N (b), which is distributed almost uniformly in a circular  region
After making the human body upright, the distribution becomes much more compact, as shown in Figure N (c)
 We have similar observations for other neighboring joints
 For some joints on the limbs (e.g., wrist, ankle), their distributions are still diverse even after positioning the torso  upright
We further rotate the human upper limb (e.g., the  left arm) to a vertical downward positions
The distribution  of the relative positions of the left wrist, shown in Figure N  (e), becomes much more compact
 The diversity of orientations (e.g., body and limb) is the  main factor in the variations of pose
Motivated by these  observations, we propose two normalization schemes, reducing diversity to generate compact distributions
The first  normalization scheme is human body normalization, rotating the human body to upright according to joint detection  results, which globally makes the relative positions between  joints compactly distributed
This scheme is followed by a  global spatial refinement module to refine all the estimations of the joints
The second one is limb normalization:  rotating the joints of each limb to make the relative positions more compact
There are four total limb normalization modules, and each is followed by a spatial limb refinement module to refine the estimations from the global  spatial refinement
Thanks to the normalization schemes,  a much more consistent spatial configuration of the human  body can be obtained, which facilitates the learning of spatial refinement models
 Besides the observations in [N0, NN, N0, NN] that the  multi-stage supervision, e.g., supervision on the joint detection stage and the spatial refinement stage, is helpful, we  observe that multi-scale supervision and multi-scale fusion  over the convolutional network within the joint detection  stage are also beneficial
 Our main contribution lies in effective normalization  schemes to facilitate the learning of convolutional spatial  models
Our scheme can be applied following different  joint detectors for refining the spatial configurations
Our  experiment results demonstrate the effectiveness on several  joint detectors, such as FCN [NN], ResNet [NN] and Hourglass [NN]
An additional minor contribution is that we empirically show the improvement by using an architecture  with multi-scale supervision and fusion for joint detection
 N
Related Work  Significant progress has been made recently in human  pose estimation by deep learning based methods [NN, NN,  N, NN, NN, NN, NN]
The joint detection and joint relation  models are widely recognized as two key components in  solving this problem
In the following, we briefly review  related developments on these two components respectively  and discuss some related works which motivate our design  of the normalization scheme
 Joint detection model
Many recent works use convolutional neural networks to learn feature representations for  obtaining the score maps of joints or the locations of joints  [NN, NN, N, NN, NN, NN, N]
Some methods directly employ  learned feature representations to regress joint positions,  e.g., the DeepPose method [NN]
A more typical way of joint  detection is to estimate a score map for each joint based on  the fully convolutional neural network (FCN) [NN]
The estimation procedure can be formulated as a multi-class classification problem [NN, NN] or regression problem [N, NN]
 For the multi-class formulation, either a single-label based  loss (e.g., softmax cross-entropy loss) [N] or a multi-label  based loss (e.g., sigmoid cross-entropy loss) [NN] can be  used
One main problem for the FCN-based joint detection model is that the positions of joints are estimated from  low resolution score maps
This reduces the location accuracy of the joints
In our work, we introduce multi-scale  supervision and fusion to further improve performance with  gradual up-sampling
 Joint relation model
The pictorial structures [NN, NN]  define the deformable configurations by spring-like connections between pairs of parts to model complex joint relations
Subsequent works [NN, N, NN] extend such an idea  to convolutional neuron networks
In those approaches, to  model the human poses with large variations, a mixture  model is usually learned for each joint
Tompson et al
 [NN] formulates the spatial relations as a Markov Random  Field (MRF) like model over the distribution of spatial locations for each body part
The location distribution of one  joint relative to another is modeled by convolutional prior  which is expected to give some spatial predictions and remove false positive outliers for each joint
Similarly, the  structured feature learning method in [N] adapts geometrical transform kernels to capture the spatial relationships  of joints from feature maps
To better estimate the human  pose, complex network architecture design with many more  parameters are expected on account of the articulated structure of the human body, such as [N] and [NN]
In our work,  we address this problem by compensating for the variations  of poses both globally and locally for facilitating the spatial  configuration exploration
 NN00    c  FCN Body Norm
 Score MapsScore Maps  Joint  Detection Global Refinement  Limb Norm
Refine   Net
 Inverse  ST  Inverse  ST Refine   Net
 Local Refinement  Refine   Net
 Figure N: Proposed framework with global and local normalization
Joint detection with fully convolutional network (FCN)  provides initial estimation of joints in terms of score maps
In the global refinement stage, a body (global) normalization  module rotates the score maps to have upright position for the body, followed by a refinement module
In the local refinement  stage, limb (local) normalization modules rotates the score maps to have vertical downward position for limbs, followed by  refinements
 Normalization
Normalization of the training samples  to reduce their variations has been proven to be a key step  before learning models using these samples
For example,  in the PCA Whitening and ZCA Whitening operations [NN],  the feature pre-processing step are adapted before training an SVM classifier, etc
The batch normalization [NN]  technique accelerates the deep network training by reducing the internal covariate shift across layers
In computer  vision applications, face normalization has been found to  be very helpful for improving the face recognition performance [N, NN, N]
It is beneficial for decreasing the intraperson variations and achieving pose-invariant recognition
 N
Our Approach  Human pose estimation is defined as the problem of localization of human joints, i.e., head, neck et al., from an  image
Given an image I , the goal is to estimate the posi- tions of the joints: {(xk, yk)}  K k=N, where K is the number  of joints
 N.N
Pipeline  Figure N shows the framework of our proposed approach
 It consists of joint detection and spatial configuration refinement, which are both realized with convolutional neural  networks
The output of joint detector consists of K + N score maps, including K joint score maps, providing spatial configuration information and one non-joint (background)  score map
The value in each score map indicates the degree  of confidence that the pixel is the corresponding joint
With  the score maps generated by the former stage (e.g., joint  detector or refinement stage) as the input, two normalization stages correct wrongly predicted joints based on spatial  configurations of the human body
Note that we focus on  the exploration of the spatial configurations of joints for refinement
Unlike many other works [N, NN, NN], we do not  incorporate low level features and our refinement is based  on the score maps which indicate the probabilities of being  each joint
 N.N
Spatial Configuration Refinement  There are two stages for spatial configuration refinement  as depicted in Figure N
The first stage is a global refinement, consisting of a global normalization module and a refinement module that refines all K joints
The second stage includes two parallel refinement modules: semi-global refinement and local refinement
The local refinement module consists of four branches
Each branch corresponds to a  limb and contains a local limb normalization module and a  local refinement module
Inverse normalizations by inverse  spatial transforms are used to rotate the joints/body back for  obtaining the final results
 Body normalization
The purpose of body normalization  is to make the orientation of the whole body the same, e.g.,  upright in our implementationN
Specifically, we rotate the  body as well as the K score maps around the center of the four joints (i.e., left shoulder, right shoulder, left hip, right  hip) so that the line from the center to the neck joint is upright, as shown in Figure N (b)
The positions of the joints  are estimated from the K Gaussian-smoothed score maps by finding the maximum responses in each map and returning the corresponding position as the position of the joint
 We implement the normalization through spatial transform, which is written as follows,  x̄ = R(x− c) + c, (N)  where c is defined as the center of the four joints on the  torso, c = NN (pl−shoulder+pr−shoulder+pl−hip+pr−hip), pl−shoulder denotes the estimated location of the left shoulder joint, and R is a rotation matrix,  R =  [  cos θ − sin θ  sin θ cos θ  ]  
(N)  Here θ = arccos (pneck−c)·e⊥‖pneck−c‖N , e⊥ denotes the unit vector  along the vertical upward direction, which is illustrated in  Figure N
 NEssentially, any orientation is fine in our approach
 NN0N    �⊥ ���  (a) (b) (c) (d)  Figure N: Illustration of the body normalization and limb  normalization
(a) and (b) show the rotation angle θ for body normalization
(c) is the image after body normalization and the rotation angle for a limb normalization
(d)  shows the image after limb normalization
Note that our  network actually performs the normalization on the score  maps rather than on the image
From (b) to (d), we show  the magnified view of the images for clarity
 0.N 0.N 0.N 0.N 0.0 0.N 0.N 0.N 0.N x  0.N  0.N  0.N  0.N  0.0  0.N  0.N  0.N  0.N  y  Left wrist wrt shoulder  (a)  0.N 0.N 0.N 0.N 0.0 0.N 0.N 0.N 0.N x  0.N  0.N  0.N  0.N  0.0  0.N  0.N  0.N  0.N  y  After body rotation  (b)  0.N 0.N 0.N 0.N 0.0 0.N 0.N 0.N 0.N x  0.N  0.N  0.N  0.N  0.0  0.N  0.N  0.N  0.N  y  After limb rotation  (c)  Figure N: Limb normalization can compact the relative position distribution for some joints on limbs, which is hard  to address via body normalization
(a), (b) and (c) are the  relative positions of left wrist with respect to the position of  left shoulder, and the relative positions after body normalization, and that after limb normalization
The distribution  in (c) is much more compact
 Local normalization
The end joints on the four limbs have  higher variations
As illustrated in Figure N (a) and (b),  through body normalization, the distribution of the wrist  with respect to the shoulder is still not compact
Limb  normalization is then adopted where we rotate the arm to  have upper arm vertical downwards, with the distribution,  as shown in Figure N (c), becoming much more compact
 There are four local normalization modules corresponding  to the four limbs respectively
Each limb contains three  joints: a root joint (shoulder, hip), a middle joint (elbow,  knee), and an end joint (wrist, ankle)
We perform the normalization by rotating the corresponding three score maps  around the root joint such that the line connecting the root  joint and the middle joint has a consistent orientation, e.g.,  vertical downwards in our implementation
The normalization process is illustrated in Figures N (c) and (d)
 Discussions
There are some alternative solutions for  handling the diverse distribution problem of relative locations of the joints, e.g., type supervision [N], and mixture  model [NN]
To check the effectiveness of our proposed norHead Shoulder Elbow Wrist Hip Knee Ankle Total #param
 FCN NN.N NN.N NN.N NN.0 NN.N NN.0 NN.N NN.N NNNM  Type-supervision NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N (NNN+NN)M  Multi-branch NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N (NNN+NN)M  Global normalization NN.N NN.N NN.N NN.N NN.N NN.0 NN.0 NN.N (NNN+NN)M  Table N: Comparing our global normalization-based solution with type supervision and the multi-branch solution on  LSP dataset with the OC annotation (@PCK 0.N) trained on  the LSP dataset
 malization scheme, we provide two alternative solutions by  considering the diversity of pose types
Here, we obtain  pose type information by clustering human poses into three  types from the LSP dataset
 In our first alternative solution (type-supervision), based  on our global refinement framework, we remove the normalization model but add type supervision in the refinement  network by learning three sets of score maps (i.e
N×K+N) rather than one set
In the second alternative solution (multibranch), based on our global refinement framework, we remove the normalization model but extend the refinement  network to multi-branches, with each branch handling one  type of pose
Note that the number of parameters for the  three-branch spatial configuration refinement is three times  of ours
Specifically, for the alternative two solutions, we  process the training data with extra data augmentation, to  make the number of training data for each type similar to  ours
The multi-branch approach is computationally more  expensive and requires more training time than ours
 We take the original FCN [NN] as the joint detector, and  make a comparison among the two alternative solutions and  our global normalization refinement scheme
Table N shows  the results
The two alternative solutions improve performance over FCN, but under-performs our approach
It is  possible to further improve the performance of the multibranch approach with more extensive data augmentation  and more branches, but this will increase the computational  complexity for both training and testing
In addition, our  approach can also benefit from the multi-branch and type  supervision solutions, where our normalization is applied  to each branch and the type supervision further constrains  the degrees of freedom of parts
 Figure N shows examples of estimated poses from different stages of our network (Figure N (a), (c), (d)), and that  from the similar network but without normalization modules (i.e., Figure N (b))
With global and local normalization, joint estimation accuracy is much improved (e.g., knee  on the top images, ankle on the bottom images)
Our normalization scheme can reduce the diversity of human poses  and facilitate the inference process
For example, the consistent orientation of the left hip and the left knee makes it  easier to infer the location of the left ankle on the bottom  example in Figure N
 NN0N    (a) (b) (c) (d)  Figure N: Estimated poses from (a) FCN, (b) the scheme  with spatial refinement but without body normalization and  limb normalization, (c) our scheme with body normalization, (d) our scheme with both body normalization and limb  normalization
 N.N
Multi-Scale Supervision and Fusion for Joint Detection  To efficiently train the FCN and exploit intermediatelevel representations, we introduce multi-scale supervision  and multi-scale fusion, which show performance gain in  many works [N0, NN, N0, NN]
The network structure is provided in Figure N
Multi-scale supervision makes the network concentrate on accurate localization on different resolutions, avoiding loss in accuracy due to down-sampling
 This is different from [NN, NN], adding multi-supervision to  each stage with the same resolution
Multi-scale fusion exploits the information at different scales
More details are  introduced in the Section N.N
 N.N
Implementation Details  Network architectures
The proposed network architecture contains three main parts: the base network for joint  detection, the normalization network, and the refinement  network
 Joint detection network: We use fully convolutional network as our joint detector
For fairness of comparison, we  use architectures similar to the compared methods as the  joint detectors
We demonstrate the effectiveness of our  normalization scheme on top of different joint detectors: the  improved FCN as showed in Figure N, ResNet-NNN similar  to [NN, N] and Hourglass [NN]
The FCN generates three  sets of score maps (FCN NNs, FCN NNs, and FCN Ns), at  different resolutions, corresponding to the last three deconvolution layers with strides NN, NN, and N respectively
The  cFCN_NNs cFCN_NNs cFCN_Ns  cFusion  Feature maps  (High resolution)  Feature maps  (Middle resolution)  Feature maps  (Low resolution) �� �� �� Final output  cFusion �� …  ��  Figure N: Architecture of the improved FCN
We utilize  multi-scale supervision and fusion
 Joint position   Determination  Output   score maps  Input   score maps  Normalization  Parameter   Calculation  Spatial   Transform  Figure N: Normalization module
Based on the score maps,  a module derives the joint positions
Then the spatial transform parameters can be calculated directly
 fusion is an ensemble of different scales with a N×N convo- lutional layer
We introduce multi-scale supervision (with  losses LDN, LDN, and LDN) and multi-scale fusion (with losses LFN, LFN) (see Figure N)
The architecture of Hour- glass is the same as [NN] and ResNet is similar to [NN]
 Normalization network: In Figure N, we show the  flowchart of the normalization module
The spatial transform is performed on the score maps with the calculated  transform parameters
End-to-end training is supported  with the error back propagation along the transform path (as  denoted by the green line)
For the joint position determination module, a Gaussian blur is performed on the mapped  score maps, with the mapping corresponding to the Sigmoid  like operation or no operation, depending the loss design of  the joint detection network
Then, the position corresponding to the maximal value in each processed score map is  estimated as the position of that joint
The network calculates the rotation center c and the rotation angle θ based on the estimated positions of joints
All the operations are  incorporated into the network as layers
 Refinement network: The refinement network consists of  four convolutional layers
The convolutional kernel sizes  and channel numbers for the four layers are N×N× (K+N) with NNN output channels, NN × NN × NNN with NNN output channels, NN× NN× NNN with NNN output channels, and N× N×NNN with J output channels, where J denotes the number of output joints
Large kernel sizes of N×N and NN×NN are beneficial for capturing spatial information of each joint
 NN0N    Loss functions
The groundtruth is generated to be K score maps
When FCN or ResNet detector is utilized, the pixels in a circled region with radius of r centered at a joint are labeled by N while other pixels are set by 0
We define the radius r as 0.NN times of the distance between left shoulder and right hip
When Hourglass detector is utilized, ND Gaussian centered on the joint location is used for  groundtruth labeling [NN]
For the spatial refinement stages,  both visible and occluded joints are labeled
 For the FCN joint detector, softmax function is utilized  to estimate the probability of being some joint for the visible  joints
For spatial refinement, after the several convolution  layers, sigmoid-like operation N/(N + e−(wx+b)) is used to map the score x to the estimation of the probability of being some joint (both visible and occluded joints)
Here, w and b are two learnable parameters which transform the scores to be in a suitable range of the domain of sigmoid function
 For other joint detectors, i.e
ResNet and Hourglass, we use  their designed loss
 Optimization
We pretrain the network by optimizing joint  detector, global and local refinement model, and then finetune the whole framework
 We initialize the parameters of the refinement model  randomly with a Gaussian distributed variable of variance  0.00N
When the network of joint detector converges, we fix  it and train the body refinement network with a base learning rate of 0.00N
Afterwards, we fix the former networks  and train the limb refinement network
Finally, we fine-tune  the entire network with learning rate 0.000N
 For FCN, we initialize it with the model weights from  PASCAL VOC [NN]
During training, we progressively  minimize the loss function: first minimize LDN then LDN + LDN + LFN, then LDN + LDN + LDN + LFN + LFN (see Figure N)
FCN detector is implemented based on Caffe  and SGD is taken as the optimization algorithm
The initial  learning rate is set to 0.00N
For other joint detectors, such  as ResNet-NNN and Hourglass, we adopt the same settings  as proposed by the authors in their papers
 N
Experiments  Datasets
We evaluate the proposed method on four  datasets: Leeds Sports Pose (LSP) [NN], extended LSP  (LSPET) [NN], Frames Labeled in Cinema (FLIC) [NN] and  MPII Human Pose [N]
The LSP dataset contains N000  training and N000 testing images from sports activities, with  NN full body joints annotated
The LSPET dataset adds  N0,000 more training samples to the LSP dataset
The  FLIC dataset contains NNNN training and N0NN testing images with N0 upper body joints annotated
The MPII Human  Pose dataset includes about NNk images with N0k annotated  poses
Existing works evaluate the performance on the LSP  dataset with different training data, which we follow for performance comparisons respectively
 Evaluation criteria
The metrics “Percentage of Correct  Keypoints (PCK)” and the “Area Under Curve (AUC)” are  utilized for evaluation [NN, NN]
A joint is correct if it falls  within α · lr pixels of the groundtruth position, with α de- noting a threshold and lr a reference length
lr is the torso length for the LSP, FLIC, and the head size for the MPII
 Data Augmentation
For the LSP dataset, we augment the  training data by performing random scaling with a scaling  factor between 0.N0 and N.NN, horizontal flipping, and rotating the data across NN0 degrees, in consideration of its unbalanced distribution of pose orientations
All input images  are resized to NN0× NN0 pixels
For the FLIC and the MPII dataset, we randomly rotate the data across +/- N0 degrees  and resize images into NNN× NNN pixels
 N.N
Results  We denote our final model as Ours(detector+Refine),  and the scheme with only detector as Ours(detector)
All  the experiments are conducted without any post-processing
 LSP OC
With the LSP dataset as training data, Table N  shows the comparisons with the OC annotation for perjoint PCK results, the overall results at threshould α = 0.N (@PCK0.N), and the AUC
Our method achieves the best  performance, where the AUC is N.N% higher than Chu et al
 [N], even though the layer number of their additional network (NN0 conv layers) is much larger than our refinement  network (NN conv layers)
Our refinement provides N% improvement in the overall accuracy
 Head Shoulder Elbow Wrist Hip Knee Ankle Total AUC  Kiefel et al
[NN] NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N  Ramakrishna et al
[NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N  Pishchulin et al
[NN] NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.0 NN.0  Ouyang et al
[NN] NN.N NN.N NN.N NN.N NN.N N0.0 NN.N N0.0 NN.N  Chen&Yuille [N] NN.N NN.N N0.N NN.N NN.N NN.N NN.0 NN.N NN.N  Yang et al
[NN] N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  Chu et al
[N] NN.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N N0.N  Ours(FCN) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours(FCN+Refine) NN.N NN.N NN.N N0.N NN.N NN.N N0.N NN.N NN.N  Table N: Performance comparison on the LSP testing set  with the OC annotation (@PCK0.N) trained on the LSP  training set
 Head Shoulder Elbow Wrist Hip Knee Ankle Total AUC  Pishchulin et al
[NN] NN.N NN.0 NN.N NN.0 NN.N NN.N NN.N NN.N NN.0  Ours(FCN) NN.N N0.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N  Ours(FCN+Refine) NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.0  Table N: Performance comparison on the LSP testing  set with the OC annotation (@PCK0.N) trained on the  MPII+LSPET+LSP training set
 NN0N    Head Shoulder Elbow Wrist Hip Knee Ankle Total AUC  Tompson et al
[NN] N0.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N  Fan et al
[NN] NN.N NN.N NN.N NN.0 NN.N NN.N N0.N NN.0 NN.N  Carreira et al
[N] N0.N NN.N NN.N NN.N NN.N N0.N NN.0 NN.N NN.N  Chen&Yuille [N] NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N N0.N  Yang et al
[NN] N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours(FCN) NN.N N0.N NN.N NN.N NN.0 NN.N NN.N NN.N N0.N  Ours(FCN+Refine) NN.0 N0.N N0.N NN.N NN.N NN.N NN.N NN.N N0.N  Table N: Performance comparison on the LSP testing set  with PC (@PCK0.N) trained on the LSP training set
 Head Shoulder Elbow Wrist Hip Knee Ankle Total AUC  Bulat et al
[N] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N –  Wei et al
[NN] – – – – – – – NN.NN –  Rafi et al
[NN] NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N  Yu et al
[N0] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours(FCN) NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N  Ours(FCN+Refine) NN.N NN.N N0.0 NN.N NN.N NN.N NN.N NN.0 NN.N  Table N: Performance comparison on the LSP testing set  with PC (@PCK0.N) trained on the LSP+LSPET training  set
 Another work [NN] incorporates the MPII and LSPET  dataset for training
The results with the same training set  are shown in Table N
Our refinement achieves N.N% improvement in overall accuracy and outperforms the startof-the-art even though our detector does not use location  refinement and an auxiliary task as used by [NN]
 LSP PC
Table N shows the comparisons with PC annotation
Compared with the result of Yang et al
[NN] on the  LSP dataset, our method significantly improves the performance by N.N% in overall accuracy and NN.N% in AUC
 We incorporate the LSPET dataset into the training data  and evaluate the performance with PC annotation
From Table N, we can see that our scheme achieves the best performance
Yu et al
[N0] extracts many pose bases to represent  various human poses
In contrast, our method normalizes  various poses
Our method outperforms theirs by N.N% in  AUC and 0.N% in the overall accuracy
 To verify the effectiveness of our normalization scheme,  we connect our refinement model at the end of those deeper  joint detectors, i.e., ResNet-NNN (NNN layers) [NN, N] and  Hourglass (about N00 layers) [NN]
The results are shown in  Table N
Without using the location refinement and auxiliary  task [NN], our baseline scheme Ours(ResNet-NNN) drops  about N% than [NN]
With the proposed refinement added,  our scheme Ours(ResNet+Refine) improves over the baseline by N% in the overall accuracy and is comparable to [NN]
 Bulat et al
[N] added a modified hourglass network [NN] (N0  layers, with parameters being three times larger than our refinement model) after ResNet-NNN
Ours(ResNet+Refine)  Head Shoulder Elbow Wrist Hip Knee Ankle Total AUC  Insafutdinov et al
[NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N  Wei et al
[NN] NN.N NN.N NN.0 NN.N NN.N N0.N NN.N N0.N NN.N  Bulat et al
[N] NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N  Ours(ResNet-NNN) NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours(ResNet+Refine) NN.N NN.N NN.N NN.N NN.N N0.N NN.N N0.N NN.N  Ours(Hourglass) NN.N NN.0 NN.N NN.N NN.N N0.N N0.0 N0.N NN  Ours(Hg+Refine) NN.N NN.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.N  Table N: Performance comparison on the LSP testing  set with the PC annotation (@PCK0.N) trained on the  MPII+LSPET+LSP training set
 Head Shoulder Elbow Wrist AUC  Toshev et al
[NN] – – NN.N NN –  Tompson et al
[NN] – – NN.N NN –  Chen&Yuille.[N] – – NN.N NN.N –  Wei et al
[NN] – – NN.N NN –  Newell et al
[NN] – – NN.0 NN.0 –  ResNet-NNN NN.N NN.N NN.N NN NN.N  Ours(Refine) NN.N NN.N NN.N NN.N NN.N  Table N: Performance comparison on the FLIC dataset with  OC annotation (@PCK0.N)
 Head Shoulder Elbow Wrist Hip Knee Ankle Total  Hourglass [NN] NN.N NN.N NN.N NN.N N0.N NN.N NN.N N0.N  Ours(Hourglass+Refine) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0  Table N: Performance comparison on the MPII test set  (@PCKh0.N) trained on the MPII training set
 is N.N% better than that of Bulat et al
[N] in AUC
When  we take Hourglass as our detector, the proposed refinement  brings 0.N% improvement in the overall accuracy
 FLIC dataset
We evaluate our method on the FLIC dataset  with the OC annotation
We take ResNet-NNN [NN] as our  joint detection network
Table N shows that our refinement  improves over the baseline model by 0.N% for elbow, 0.N%  for wrist, and N.N% in AUC
 MPII dataset
We take Hourglass [NN] as our joint detector and evaluate our method on the MPII dataset
Table N  shows that our refinement performs similarly on the test set  in overall accuracy
On the validation set, we obtains 0.N%  improvement
To check the reason for small gains, we analyze the relative position distribution on the MPII validation  dataset
We found that the original distribution without normalization is already compact, being similar to the distribution after the normalization on the LSP dataset
Unlike the  poses in the LSP dataset (sport poses), the majority of poses  are upright and normal, as shown in Figure N
Our normalization scheme presents its advantages on the datasets  including high diverse poses
In reality, these complicated  postures are inevitable
 NN0N    Figure N: Body pose clusters on the MPII test set
The  maker of MPII dataset clusters body poses into NN types  on the test set
Note the figure is from http://  human-pose.mpi-inf.mpg.de./#results
 Head Shoulder Elbow Wrist Hip Knee Ankle Total  FCN NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Stage-N w/o body norm
NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N  Stage-N w body norm
NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N  Stage-N w/o limb norm
NN NN.N NN.N NN.N NN NN.N N0.N NN  Stage-N w limb norm
NN.N NN.N NN.N N0.N NN.N NN.N N0.N NN.N  Table N: Evaluation of body normalization and limb normalization on the LSP test dataset with the OC annotation  (@PCK0.N) trained on the LSP training dataset
 Head Shoulder Elbow Wrist Hip Knee Ankle Total  FCN NN.N N0.N NN.N NN.N NN NN.N NN.N NN.N  Stage-N w/o body norm
NN.N N0.N NN.N NN.N N0 NN.N NN NN  Stage-N w body norm
NN.N N0.N NN.N NN.N NN.N NN.N NN NN.N  Stage-N w/o limb norm
NN NN.N N0.N NN.N NN.N NN.N NN.N NN.N  Stage-N w limb norm
NN.N NN.N NN NN.N N0.N NN NN.N NN.N  Table N0: Evaluation of body normalization and limb normalization on the LSP test dataset with the OC annotation  (@PCK0.N) trained on the LSP+LSPET training dataset
 N.N
Ablation Study  We analyze the effectiveness of the proposed components, including the two pose normalization and refinement  stages, and the multi-scale supervision and fusion
 Global and local normalization
To verify the effectiveness of body and limb normalization, we compare the results of the network with normalization versus that without  normalization on the two stages separately
 Table N shows the comparisons on the LSP dataset
With  body normalization, the shoulder is 0.N% higher than that of  FCN and the hip estimation is improved by N.N%
In contrast, the model without the body normalization introduces  much smaller improvement
With limb normalization, the  accuracy of wrist, knee, and ankle is improved by 0.N%,  Head Shoulder Elbow Wrist Hip Knee Ankle Total AUC  FCN NNs NN.N NN.N NN.N NN.N NN.N NN.N NN N0.N N0  FCN NNs NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  FCN Ns NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN NN.N  FCN NNs (Extra) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  FCN NNs (Fusion) NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N  FCN Ns (Extra) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  FCN Ns (Fusion) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table NN: Evaluation of multi-scale supervision and multiscale fusion on top of FCN on the LSP testing set with the  OC annotation (@PCK0.N) trained on the LSP training set
 0.N%, and 0.N% respectively
Without pose normalization,  the subnetwork tends to preserve the results of the former  stage
Similar phenomena are observed when we use the  LSP+LSPET dataset for training as shown in Table N0
We  notice that the performance of Stage-N without body normalization even provides interior performance than FCN
 In contrast, when body normalization is utilized, consistent  performance improvement can be achieved
 Multi-scale supervision and fusion
For FCN, we add  multi-scale supervision and multi-scale score map fusion to  improve accuracy
Here, we evaluate the efficiency of the  extra supervision and fusion respectively
Table NN shows  the experiment results
FCN NNs and FCN Ns denote the  results of the original FCN without extra loss and fusion at  the middle and high resolution respectively
FCN NNs (Extra) and FNC Ns (Extra) denote the results after adding supervision
FCN NNs (Fusion) and FCN Ns (Fusion) denote  the results after adding both supervision and fusion
From  Table NN, we have the following two observations
First,  with extra supervision, the accuracy of most joints improves  by more than N% and the AUC increases noticeably at the  same resolution level
Note that FCN Ns (Extra) achieves  similar accuracy as FCN NNs (Extra) but its AUC is much  higher
Second, we fuse the score maps together with different weights to exploit their respective advantages
We can  see the overall accuracy improves by a further 0.N%
 N
Conclusion  In this paper, considering that the distributions of the  relative locations of joints are very diverse, we propose a  two-stage normalization scheme: human body normalization and limb normalization, making the distributions compact and facilitating the learning of spatial refinement models
To validate the effectiveness of our method, we connect the refinement model to various state-of-the-art joint  detectors
Experiment results demonstrate that our method  consistently improves the performance on different benchmarks
 NN0N  http://human-pose.mpi-inf.mpg.de./#results http://human-pose.mpi-inf.mpg.de./#results   References  [N] J
K
Aggarwal and M
S
Ryoo
Human activity analysis: A  review
ACM Computing Surveys, NN(N):NN, N0NN
 [N] M
Andriluka, L
Pishchulin, P
Gehler, and B
Schiele
ND  human pose estimation: New benchmark and state of the art  analysis
In CVPR, N0NN
 [N] M
Andriluka, S
Roth, and B
Schiele
Pictorial structures  revisited: People detection and articulated pose estimation
 In CVPR, N00N
 [N] A
Asthana, T
K
Marks, M
J
Jones, K
H
Tieu, and M
Rohith
Fully automatic pose-invariant face recognition via ND  pose normalization
In ICCV, N0NN
 [N] A
Bulat and G
Tzimiropoulos
Human pose estimation via  convolutional part heatmap regression
In ECCV, N0NN
 [N] J
Carreira, P
Agrawal, K
Fragkiadaki, and J
Malik
Human pose estimation with iterative error feedback
In CVPR,  N0NN
 [N] D
Chen, G
Hua, F
Wen, and J
Sun
Supervised transformer  network for efficient face detection
In ECCV, N0NN
 [N] X
Chen and A
Yuille
Articulated pose estimation by a  graphical model with image dependent pairwise relations
In  NIPS, N0NN
 [N] X
Chu, W
Ouyang, H
Li, and X
Wang
Structured feature  learning for pose estimation
In CVPR, N0NN
 [N0] M
Dantone, J
Gall, C
Leistner, and L
Van Gool
Human  pose estimation using body parts dependent joint regressors
 In ICCV, N0NN
 [NN] M
Eichner, V
Ferrari, and S
Zurich
Better appearance  models for pictorial structures
In BMVC, volume N, page N,  N00N
 [NN] X
Fan, K
Zheng, Y
Lin, and S
Wang
Combining local  appearance and holistic view: Dual-source deep neural networks for human pose estimation
In CVPR, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
 [NN] E
Insafutdinov, L
Pishchulin, B
Andres, M
Andriluka, and  B
Schiele
Deepercut: A deeper, stronger, and faster multiperson pose estimation model
In ECCV, N0NN
 [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
In  ICML, pages NNN–NNN, N0NN
 [NN] S
Johnson and M
Everingham
Clustered pose and nonlinear appearance models for human pose estimation
In BMVC,  N0N0
 [NN] S
Johnson and M
Everingham
Learning effective human  pose estimation from inaccurate annotation
In CVPR, N0NN
 [NN] A
Kessy, A
Lewin, and K
Strimmer
Optimal whitening  and decorrelation
arXiv preprint arXiv:NNNN.00N0N, N0NN
 [NN] M
Kiefel and P
V
Gehler
Human pose estimation with  fields of parts
In ECCV, N0NN
 [N0] C.-Y
Lee, S
Xie, P
W
Gallagher, Z
Zhang, and Z
Tu
 Deeply-supervised nets
In AISTATS, N0NN
 [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In CVPR, N0NN
 [NN] A
Newell, K
Yang, and J
Deng
Stacked hourglass networks for human pose estimation
In ECCV, N0NN
 [NN] W
Ouyang, X
Chu, and X
Wang
Multi-source deep learning for human pose estimation
In CVPR, N0NN
 [NN] L
Pishchulin, M
Andriluka, P
Gehler, and B
Schiele
Poselet conditioned pictorial structures
In CVPR, N0NN
 [NN] L
Pishchulin, E
Insafutdinov, S
Tang, B
Andres, M
Andriluka, P
Gehler, and B
Schiele
Deepcut: Joint subset  partition and labeling for multi person pose estimation
In  CVPR, N0NN
 [NN] U
Rafi, J
Gall, and B
Leibe
An efficient convolutional  network for human pose estimation
In BMVC, N0NN
 [NN] V
Ramakrishna, D
Munoz, M
Hebert, A
Bagnell, and  Y
Sheikh
Pose machines: Articulated pose estimation via  inference machines
In ECCV, N0NN
 [NN] B
Sapp and B
Taskar
Modec: Multimodal decomposable  models for human pose estimation
In CVPR, N0NN
 [NN] J
Shotton, T
Sharp, A
Kipman, A
Fitzgibbon, M
Finocchio, A
Blake, M
Cook, and R
Moore
Real-time human  pose recognition in parts from single depth images
Communications of the ACM, NN(N):NNN–NNN, N0NN
 [N0] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
 [NN] J
J
Tompson, R
Goroshin, A
Jain, Y
LeCun, and C
Bregler
Efficient object localization using convolutional networks
In CVPR, N0NN
 [NN] J
J
Tompson, A
Jain, Y
LeCun, and C
Bregler
Joint training of a convolutional network and a graphical model for  human pose estimation
In NIPS, N0NN
 [NN] A
Toshev and C
Szegedy
DeepPose: Human pose estimation via deep neural networks
In CVPR, N0NN
 [NN] S.-E
Wei, V
Ramakrishna, T
Kanade, and Y
Sheikh
Convolutional pose machines
In CVPR, N0NN
 [NN] B
Xiaohan Nie, C
Xiong, and S.-C
Zhu
Joint action recognition and pose estimation from video
In CVPR, N0NN
 [NN] S
Xie and Z
Tu
Holistically-nested edge detection
In  ICCV, N0NN
 [NN] W
Yang, W
Ouyang, H
Li, and X
Wang
End-to-end learning of deformable mixture of parts and deep convolutional  neural networks for human pose estimation
In CVPR, N0NN
 [NN] Y
Yang and D
Ramanan
Articulated pose estimation with  flexible mixtures-of-parts
In CVPR, N0NN
 [NN] Y
Yang and D
Ramanan
Articulated human detection with  flexible mixtures of parts
IEEE Trans
Pattern Anal
and  Mach
Intell., NN(NN):NNNN–NNN0, N0NN
 [N0] X
Yu, F
Zhou, and M
Chandraker
Deep deformation network for object landmark localization
In ECCV, N0NN
 [NN] X
Zhu, Z
Lei, J
Yan, D
Yi, and S
Z
Li
High-fidelity  pose and expression normalization for face recognition in the  wild
In CVPR, N0NN
 NN0NFashion Forward: Forecasting Visual Style in Fashion   Fashion Forward: Forecasting Visual Style in Fashion  Ziad Al-HalahN* Rainer StiefelhagenN Kristen GraumanN NKarlsruhe Institute of Technology, NNNNN Karlsruhe, Germany  NThe University of Texas at Austin, NNN0N Austin, USA {ziad.al-halah, rainer.stiefelhagen}@kit.edu, grauman@cs.utexas.edu  Abstract  What is the future of fashion? Tackling this question from  a data-driven vision perspective, we propose to forecast visual style trends before they occur
We introduce the first  approach to predict the future popularity of styles discovered from fashion images in an unsupervised manner
Using these styles as a basis, we train a forecasting model to  represent their trends over time
The resulting model can  hypothesize new mixtures of styles that will become popular in the future, discover style dynamics (trendy vs
classic), and name the key visual attributes that will dominate  tomorrow’s fashion
We demonstrate our idea applied to  three datasets encapsulating N0,000 fashion products sold  across six years on Amazon
Results indicate that fashion  forecasting benefits greatly from visual analysis, much more  than textual or meta-data cues surrounding products
 N
Introduction  “The customer is the final filter
What survives the whole  process is what people wear.” – Marc Jacobs  Fashion is a fascinating domain for computer vision
 Not only does it offer a challenging testbed for fundamental vision problems—human body parsing [NN, NN], crossdomain image matching [NN, N0, NN, NN], and recognition [N, NN, N, NN]—but it also inspires new problems that  can drive a research agenda, such as modeling visual compatibility [NN, NN], interactive fine-grained retrieval [NN, NN],  or reading social cues from what people choose to wear [NN,  NN, N0, NN]
At the same time, the space has potential for  high impact: the global market for apparel is estimated at  $N Trillion USD [N]
It is increasingly entwined with online  shopping, social media, and mobile computing—all arenas  where automated visual analysis should be synergetic
 In this work, we consider the problem of visual fashion  forecasting
The goal is to predict the future popularity of  fine-grained fashion styles
For example, having observed  the purchase statistics for all women’s dresses sold on Ama* Work done while first author was a visiting researcher at UT Austin
 N0N0 N0NN N0NN N0NN N0NN N0N0  P o  p u  la ri  ty    S ty  le  B    S  ty le   A    Figure N: We propose to predict the future of fashion based on  visual styles
 zon over the last N years, can we predict what salient visual properties the best selling dresses will have NN months  from now? Given a list of trending garments, can we predict  which will remain stylish into the future? Which old trends  are primed to resurface, independent of seasonality?  Computational models able to make such forecasts  would be critically valuable to the fashion industry, in terms  of portraying large-scale trends of what people will be buying months or years from now
They would also benefit  individuals who strive to stay ahead of the curve in their  public persona, e.g., stylists to the stars
However, fashion forecasting is interesting even to those of us unexcited  by haute couture, money, and glamour
This is because  wrapped up in everyday fashion trends are the effects of  shifting cultural attitudes, economic factors, social sharing,  and even the political climate
For example, the hard-edged  flapper style during the prosperous NNN0’s in the U.S
gave  way to the conservative, softer shapes of NNN0’s women’s  wear, paralleling current events such as women’s right to  vote (secured in NNN0) and the stock market crash N years  later that prompted more conservative attitudes [NN]
Thus,  beyond the fashion world itself, quantitative models of style  evolution would be valuable in the social sciences
 While structured data from vendors (i.e., recording purchase rates for clothing items accompanied by meta-data  labels) is relevant to fashion forecasting, we hypothesize  that it is not enough
Fashion is visual, and comprehensive  fashion forecasting demands actually looking at the prodNNNN    ucts
Thus, a key technical challenge in forecasting fashion  is how to represent visual style
Unlike articles of clothing and their attributes (e.g., sweater, vest, striped), which  are well-defined categories handled readily by today’s sophisticated visual recognition pipelines [N, N, NN, NN], styles  are more difficult to pin down and even subjective in their  definition
In particular, two garments that superficially are  visually different may nonetheless share a style
 Furthermore, as we define the problem, fashion forecasting goes beyond simply predicting the future purchase rate  of an individual item seen in the past
So, it is not simply a  regression problem from images to dates
Rather, the forecaster must be able to hypothesize styles that will become  popular in the future—i.e., to generate yet-unseen compositions of styles
The ability to predict the future of styles  rather than merely items is appealing for applications that  demand interpretable models expressing where trends as a  whole are headed, as well as those that need to capture the  life cycle of collective styles, not individual garments
Despite some recent steps to qualitatively analyze past fashion  trends in hindsight [NN, NN, N0, NN, NN], to our knowledge  no existing work attempts visual fashion forecasting
 We introduce an approach that forecasts the popularity  of visual styles discovered in unlabeled images
Given a  large collection of unlabeled fashion images, we first predict  clothing attributes using a supervised deep convolutional  model
Then, we discover a “vocabulary” of latent styles  using non-negative matrix factorization
The discovered  styles account for the attribute combinations observed in the  individual garments or outfits
They have a mid-level granularity: they are more general than individual attributes (pastel, black boots), but more specific than typical style classes  defined in the literature (preppy, Goth, etc.) [NN, NN, NN]
We  further show how to augment the visual elements with text  data, when available, to discover fashion styles
We then  train a forecasting model to represent trends in the latent  styles over time and to predict their popularity in the future
 Building on this, we show how to extract style dynamics  (trendy vs
classic vs
outdated), and forecast the key visual  attributes that will play a role in tomorrow’s fashion—all  based on learned visual models
 We apply our method to three datasets covering six years  of fashion sales data from Amazon for about N0,000 unique  products
We validate the forecasted styles against a heldout future year of purchase data
Our experiments analyze  the tradeoffs of various forecasting models and representations, the latter of which reveals the advantage of unsupervised style discovery based on visual semantic attributes  compared to off-the-shelf CNN representations, including  those fine-tuned for garment classification
Overall, an important finding is that visual content is crucial for securing  the most reliable fashion forecast
Purchase meta-data, tags,  etc., are useful, but can be insufficient when taken alone
 N
Related work  Retrieval and recommendation There is strong practical  interest in matching clothing seen on the street to an online  catalog, prompting methods to overcome the street-to-shop  domain shift [NN, N0, NN]
Beyond exact matching, recommendation systems require learning when items “go well”  together [NN, NN, NN] and capturing personal taste [N] and  occasion relevance [NN]
Our task is very different
Rather  than recognize or recommend garments, our goal is to forecast the future popularity of styles based on visual trends
 Attributes in fashion Descriptive visual attributes are  naturally amenable to fashion tasks, since garments are often described by their materials, fit, and patterns (denim,  polka-dotted, tight)
Attributes are used to recognize articles of clothing [N, NN], retrieve products [NN, NN], and describe clothing [N, NN]
Relative attributes [NN] are explored  for interactive image search with applications to shoe shopping [NN, NN]
While often an attribute vocabulary is defined  manually, useful clothing attributes are discoverable from  noisy meta-data on shopping websites [N] or neural activations in a deep network [N0]
Unlike prior work, we use inferred visual attributes as a conduit to discover fine-grained  fashion styles from unlabeled images
 Learning styles Limited work explores representations of  visual style
Different from recognizing an article of clothing (sweater, dress) or its attributes (blue, floral), styles  entail the higher-level concept of how clothing comes together to signal a trend
Early methods explore supervised  learning to classify people into style categories, e.g., biker,  preppy, Goth [NN, NN]
Since identity is linked to how a  person chooses to dress, clothing can be predictive of occupation [NN] or one’s social “urban tribe” [NN, NN]
Other  work uses weak supervision from meta-data or co-purchase  data to learn a latent space imbued with style cues [NN, NN]
 In contrast to prior work, we pursue an unsupervised approach for discovering visual styles from data, which has  the advantages of i) facilitating large-scale style analysis, ii)  avoiding manual definition of style categories, iii) allowing  the representation of finer-grained styles , and iv) allowing  a single outfit to exhibit multiple styles
Unlike concurrent  work [NN] that learns styles of outfits, we discover styles  for individual garments and, more importantly, predict their  popularity in the future
 Discovering trends Beyond categorizing styles, a few  initial studies analyze fashion trends
A preliminary experiment plots frequency of attributes (floral, pastel, neon) observed over time [NN]
Similarly, a visualization shows the  frequency of garment meta-data over time in two cities [NN]
 The system in [NN] predicts when an object was made.The  collaborative filtering recommendation system of [NN] is enhanced by accounting for the temporal dynamics of fashion,  with qualitative evidence it can capture popularity changes  of items in the past (i.e., Hawaiian shirts gained popularity  NNN    after N00N)
A study in [N0] looks for correlation between  attributes popular in New York fashion shows versus what  is seen later on the street
Whereas all of the above center  around analyzing past (observed) trend data, we propose to  forecast the future (unobserved) styles that will emerge
To  our knowledge, our work is the first to tackle the problem  of visual style forecasting, and we offer objective evaluation  on large-scale datasets
 Text as side information Text surrounding fashion images can offer valuable side information
Tag and garment type data can serve as weak supervision for style  classifiers [NN, NN]
Purely textual features (no visual  cues) are used to discover the alignment between words for  clothing elements and styles on the fashion social website  Polyvore [NN]
Similarly, extensive tags from experts can  help learn a representation to predict customer-item match  likelihood for recommendation [N]
Our method can augment its visual model with text, when available
While  adding text improves our forecasting, we find that text alone  is inadequate; the visual content is essential
 N
Learning and forecasting fashion style  We propose an approach to predict the future of fashion  styles based on images and consumers’ purchase data
Our  approach N) learns a representation of fashion images that  captures the garments’ visual attributes; then N) discovers  a set of fine-grained styles that are shared across images  in an unsupervised manner; finally, N) based on statistics  of past consumer purchases, constructs the styles’ temporal  trajectories and predicts their future trends
 N.N
Elements of fashion  In some fashion-related tasks, one might rely solely on  meta information provided by product vendors, e.g., to analyze customer preferences
Meta data such as tags and  textual descriptions are often easy to obtain and interpret
 However, they are usually noisy and incomplete
For example, some vendors may provide inaccurate tags or descriptions in order to improve the retrieval rank of their products,  and even extensive textual descriptions fall short of communicating all visual aspects of a product
 On the other hand, images are a key factor in a product’s  representation
It is unlikely that a customer will buy a garment without an image no matter how expressive the textual description is
Nonetheless, low level visual features  are hard to interpret
Usually, the individual dimensions  are not correlated with a semantic property
This limits the  ability to analyze and reason about the final outcome and  its relation to observable elements in the image
Moreover,  these features often reside in a certain level of granularity
 This renders them ill-suited to capture the fashion elements  which usually span the granularity space from the most fine  and local (e.g
collar) to the coarse and global (e.g
cozy)
 Semantic attributes serve as an elegant representation  that is both interpretable and detectable in images
Additionally, they express visual properties at various levels of  granularity
Specifically, we are interested in attributes that  capture the diverse visual elements of fashion, like: Colors  (e.g
blue, pink); Fabric (e.g
leather, tweed); Shape (e.g
 midi, beaded); Texture (e.g
floral, stripe); etc
These attributes constitute a natural vocabulary to describe styles in  clothing and apparel
As discussed above, some prior work  considers fashion attribute classification [NN, NN], though  none for capturing higher-level visual styles
 To that end, we train a deep convolutional model for  attribute prediction using the DeepFashion dataset [NN]
 The dataset contains more than N00,000 images labeled  with N,000 semantic attributes collected from online fashion websites
Our deep attribute model has an AlexNet-like  structure [NN]
It consists of N convolutional layers and three  fully connected layers
The last attribute prediction layer is  followed by a sigmoid activation function
We use the cross  entropy loss to train the network for binary attribute prediction
The network is trained using Adam [NN] for stochastic optimization with an initial learning rate of 0.00N and a  weight decay of Ne-N
(see Supp
for details)
 With this model we can predict the presence of M = N, 000 attributes in new images:  ai = fa(xi|θ), (N) such that θ is the model parameters, and ai ∈ R  M where the  mth element in ai is the probability of attribute a m in image  xi, i.e., a m i = p(a  m|xi)
fa(·) provides us with a detailed visual description of a garment that, as results will show,  goes beyond meta-data typically available from a vendor
 N.N
Fashion style discovery  For each genre of garments (e.g., Dresses or T-Shirts),  we aim to discover the set of fine-grained styles that emerge
 That is, given a set of images X = {xi} N i=N we want to  discover the set of K latent styles S = {sk} K k=N that are  distributed across the items in various combinations
 We pose our style discovery problem in a nonnegative matrix factorization (NMF) framework that maintains  the interpretability of the discovered styles and scales efficiently to large datasets
First we infer the visual attributes  present in each image using the classification network described above
This yields an M × N matrix A ∈ RM×N  indicating the probability that each of the N images contains each of the M visual attributes
Given A, we infer the  matrices W and H with nonnegative entries such that:  A ≈ WH where W ∈ RM×K , H ∈ RK×N 
(N) We consider a low rank factorization of A, such that A is  estimated by a weighted sum of K rank-N matrices:  A ≈  K∑  k=N  λk.wk ⊗ hk, (N)  NN0    where ⊗ is the outer product of the two vectors and λk is the weight of the kth factor [NN]
 By placing a Dirichlet prior on wk and hk, we insure  the nonnegativity of the factorization
Moreover, since  ||wk||N = N, the result can be viewed as a topic model with the styles learned by Eq
N as topics over the attributes
That  is, the vectors wk denote common combinations of selected  attributes that emerge as the latent style “topics”, such that  wmk = p(am|sk)
Each image is a mixture of those styles, and the combination weights in hk, when H is column-wise  normalized, reflect the strength of each style for that garment, i.e., hik = p(sk|xi)
Note that our style model is unsupervised which makes  it suitable for style discovery from large scale data
Furthermore, we employ an efficient estimation for Eq
N for large  scale data using an online MCMC based approach [NN]
At  the same time, by representing each latent style sk as a mixture of attributes [aNk, a N k, 


, a  M k ], we have the ability to  provide a semantic linguistic description of the discovered  styles in addition to image examples
Figure N shows examples of styles discovered for two datasets (genres of products) studied in our experiments
 Finally, our model can easily integrate multiple representations of fashion when it is available by adjusting the  matrix A
That is, given an additional view (e.g., based on  textual description) of the images U ∈ RL×N , we augment the attributes with the new modality to construct the new  data representation Á = [A;U] ∈ R(M+L)×N 
Then Á is factorized as in Eq
N to discover the latent styles
 N.N
Forecasting visual style  We focus on forecasting the future of fashion over a NN year time course
In this horizon, we expect consumer  purchase behavior to be the foremost indicator of fashion  trends
In longer horizons, e.g., N-N0 years, we expect more  factors to play a role in shifting general tastes, from the  social, political, or demographic changes to technological  and scientific advances
Our proposed approach could potentially serve as a quantitative tool towards understanding  trends in such broader contexts, but modeling those factors  is currently out of the scope of our work
 The temporal trajectory of a style In order to predict  the future trend of a visual style, first we need to recover the  temporal dynamics which the style went through up to the  present time
We consider a set of customer transactions Q  (e.g., purchases) such that each transaction qi ∈ Q involves one fashion item with image xqi ∈ X 
Let Q  t denote the  subset of transactions at time t, e.g., within a period of one  month
Then for a style sk ∈ S, we compute its temporal trajectory yk by measuring the relative frequency of that  style at each time step:  ykt = N  |Qt|  ∑  qi∈Qt  p(sk|xqi), (N)  for t = N, 


, T 
Here p(sk|xqi) is the probability for style sk given image xqi of the item in transaction qi
 Forecasting the future of a style Given the style temporal trajectory up to time n, we predict the popularity of the  style in the next time step in the future ŷn+N using an exponential smoothing model [N]:  ŷn+N|n = ln  ln = αyn + (N− α)ln−N  ŷn+N|n = n∑  t=N  α(N− α)n−tyt + (N− α) nl0  (N)  where α ∈ [0, N] is the smoothing factor, ln is the smoothing value at time n, and l0 = y0
In other words, our forecast ŷn+N is an estimated mean for the future popularity of the  style given its previous temporal dynamics
 The exponential smoothing model (EXP), with its exponential weighting decay, nicely captures the intuitive notion that the most recent observed trends and popularities of  styles have higher impact on the future forecast than older  observations
Furthermore, our selection of EXP combined  with K independent style trajectories is partly motivated by  practical matters, namely the public availability of product  image data accompanied by sales rates
EXP is defined with  only one parameter (α) which can be efficiently estimated  from relatively short time series
In practice, as we will  see in results, it outperforms several other standard time series forecasting algorithms, specialized neural network solutions, and a variant that models all K styles jointly (see  Sec
N.N)
While some styles’ trajectories exhibit seasonal  variations (e.g
T-Shirts are sold in the summer more than  in the winter), such changes are insufficient with regard of  the general trend of the style
As we show later, the EXP  model outperforms models that incorporate seasonal variations or styles’ correlations for our datasets
 N
Evaluation  Our experiments evaluate our model’s ability to forecast  fashion
We quantify its performance against an array of alternative models, both in terms of forecasters and alternative  representations
We also demonstrate its potential power for  providing interpretable forecasts, analyzing style dynamics,  and forecasting individual fashion elements
 Datasets We evaluate our approach on three datasets collected from Amazon by [N0]
The datasets represent three  garment categories for women (Dresses and Tops&Tees)  and men (Shirts)
An item in these sets is represented with  a picture, a short textual description, and a set of tags (see  Fig
N)
Additionally, it contains the dates each time the  item was purchased
 These datasets are a good testbed for our model since  they capture real-world customers’ preferences in fashion  NNN    Dataset #Items #Transaction  Dresses NN,NNN NN,NNN  Tops & Tees NN,NNN NN,NNN  Shirts NN,NNN NN,NNN  Table N: Statistics of the three datasets from Amazon
 Text Amanda Uprichard Women's  Kiana Dress, Royal, Small  Tags - Women  - Clothing  - Dresses  - Night Out & Cocktail  - Women's Luxury Brands  Text Women's Stripe Scoop Tunic  Tank, Coral, Large  Tags - Women  - Clothing  - Tops & Tees  - Tanks & Camis  Text The Big Bang Theory DC  Comics Slim-Fit T-Shirt   Tags - Men - Clothing - T-Shirts  Figure N: The fashion items are represented with an image, a textual description, and a set of tags
 and they span a fairly long period of time
For all experiments, we consider the data in the time range from January  N00N to December N0NN
We use the data from the years  N00N to N0NN for training, N0NN for validation, and N0NN for  testing
Table N summarizes the dataset sizes
 N.N
Style discovery  We use our deep model trained on DeepFashion [NN]  (cf
Sec
N.N) to infer the semantic attributes for all items in  the three datasets, and then learn K = N0 styles from each
We found that learning around N0 styles within each category is sufficient to discover interesting visual styles that  are not too generic with large within-style variance nor too  specific, i.e., describing only few items in our data
Our  attribute predictions average NN% AUC on a held-out DeepFashion validation set; attribute ground truth is unavailable  for the Amazon datasets themselves
 Fig
N shows NN of the discovered styles in N of the  datasets along with the N top ranked items based on the likelihood of that style in the items p(sk|xi), and the most likely attributes per style (p(am|sk))
As anticipated, our model automatically finds the fine-grained styles within each genre  of clothing
While some styles vary across certain dimensions, there is a certain set of attributes that identify the  style signature
For example, color is not a significant factor in the Nst and Nrd styles (indexed from left to right) of Dresses
It is the mixture of shape, design, and structure  that defines these styles (sheath, sleeveless and bodycon in  Nst, and chiffon, maxi and pleated in Nrd)
On the other hand, the clothing material might dominate certain styles,  like leather and denim in the NNth and NNth style of Dresses
Having a Dirichlet prior for the style distribution over the  attributes induces sparsity
Hence, our model focuses on  the most distinctive attributes for each style
A naive approach (e.g., clustering) could be distracted by the many  visual factors and become biased towards certain properties  like color, e.g., by grouping all black clothes in one style  while ignoring subtle differences in shape and material
 N.N
Style forecasting  Having discovered the latent styles in our datasets, we  construct their temporal trajectories as in Sec
N.N using a  temporal resolution of months
We compare our approach  to several well-established forecasting baselines, which we  group in three main categories:  Naı̈ve These methods rely on the general properties of the  trajectory: N) mean: it forecasts the future values to be equal  to the mean of the observed series; N) last: it assumes the  forecast to be equal to the last observed value; N) drift: it  considers the general trend of the series
 Autoregression These are linear regressors based on the  last few observed values’ “lags”
We consider several variations [N]: N) The linear autoregression model (AR); N) the  AR model that accounts for seasonality (AR+S); N) the vector autoregression (VAR) that considers the correlations between the different styles’ trajectories; N) and the autoregressive integrated moving average model (ARIMA)
 Neural Networks Similar to autoregression, the neural  models rely on the previous lags to predict the future;  however these models incorporate nonlinearity which make  them more suitable to model complex time series
We consider two architectures with sigmoid non-linearity: N) The  feed forward neural network (FFNN); N) and the time  lagged neural network (TLNN) [NN]
 For models that require stationarity (e.g
AR), we consider the differencing order as a hyperparamtere for each  style
All hyperparameters (α for ours, number of lags for  the autoregression, and hidden neurons for neural networks)  are estimated over the validation split of the dataset
We  compare the models based on two metrics: The mean absolute error MAE = N n  ∑n t=N |et|, and the mean absolute  percentage error MAPE = N n  ∑n t=N |  et yt | × N00
Where  et = ŷt − yt is the error in predicting yt with ŷt
 Forecasting results Table N shows the forecasting performance of all models on the test data
Here, all models use the identical visual style representation, namely our  attribute-based NMF approach
Our exponential smoothing  model outperforms all baselines across the three datasets
 Interestingly, the more involved models like ARIMA, and  the neural networks do not perform better
This may be  due to their larger number of parameters and the relatively  short style trajectories
Additionally, no strong correlations  among the styles were detected and VAR showed inferior  performance
We expect there would be higher influence  between styles from different garment categories rather than  between styles within a category
Furthermore, modeling  seasonality (AR+S) does not improve the performance of  the linear autoregression model
We notice that the Dresses  dataset is more challenging than the other two
The styles  NNN    (a) Dresses  (b) Tops & Tees  Figure N: The discovered visual styles on (a) Dresses and (b) Tops & Tees datasets (see Supp for Shirts)
Our model captures the finegrained differences among the styles within each genre and provides a semantic description of the style signature based on visual attributes
 there exhibit more temporal variations compared to the ones  in Tops&Tees and Shirts, which may explain the larger forecast error in general
Nonetheless, our model generates a  reliable forecast of the popularity of the styles for a year  ahead across all data sets
The forecasted style trajectory by  our approach is within a close range to the actual one (only  N to N percentage error based on MAPE)
Furthermore, we  notice that our model is not very sensitive to the number of  styles
When varying K between NN and NN, the relative performance of the forecast approaches is similar to Table N,  with EXP performing the best
 Fig
N visualizes our model’s predictions on four styles  from the Tops&Tees dataset
For trajectories in Fig
Na and  Fig
Nb, our approach successfully captures the popularity  of styles in year N0NN
Styles in Fig
Nc and Fig
Nd are  much more challenging
Both of them experience a reflection point at year N0NN, from a declining popularity to an  increase and vice versa
Still, the predictions made by our     (a) (b)  (c) (d)  Figure N: The forecasted popularity estimated by our model for  N styles from the Tops & Tees dataset
Our model successfully  predicts the popularity of styles in the future and performs well  even with challenging trajectories that experience a sudden change  in direction like in (c) and (d)
 model forecast this change in direction correctly and the error in the estimated popularity is minor
 NNN    Model Dresses Tops & Tees Shirts  MAE MAPE MAE MAPE MAE MAPE  Naı̈ve  mean 0.0NNN NN.N0 0.0NNN NN.NN 0.0NNN N.NN  last 0.0NNN N.NN 0.0NNN N.NN 0.0NN0 N.N0  drift 0.0N0N N.NN 0.0NNN N.N0 0.0NNN N.N0  Autoregression  AR 0.0NNN N.NN 0.0NNN N.N0 0.0NN0 N.NN  AR+S 0.0NN0 NN.NN 0.0NNN N.NN 0.0NNN N.NN  VAR 0.0NN0 N0.NN 0.0NNN NN.NN 0.0NN0 N.NN  ARIMA 0.0NNN NN.0N 0.0NNN N.NN 0.00NN N.NN  Neural Network  TLNN 0.0NNN NN.NN 0.0NNN N.NN 0.0NNN N.NN  FFNN 0.0NNN NN.NN 0.0NNN N0.NN 0.0N0N N.NN  Ours 0.0NNN N.NN 0.0NNN N.NN 0.00NN N.NN  Table N: The forecast error of our approach compared to several  baselines on three datasets
 N.N
Fashion representation  Thus far we have shown the styles discovered by our approach as well as our ability to forecast the popularity of  visual styles in the future
Next we examine the impact of  our representation compared to both textual meta-data and  CNN-based alternatives
 Meta Information Fashion items are often accompanied by information other than the images
We consider  two types of meta information supplied with the Amazon  datasets (Fig
N): N) Tags: which identify the categories, the  age range, the trademark, the event, etc.; N) Text: which provides a description of the item in natural language
For both,  we learn a unique vocabulary of tags and words across the  dataset and represent each item using a bag of words representation
From thereafter, we can employ our NMF and  forecasting models just as we do with our visual attributebased vocabulary
In results, we consider a text-only baseline as well as a multi-modal approach that augments our  attribute model with textual cues
 Visual Attributes are attractive in this problem setting for  their interpretability, but how fully do they capture the visual content? To analyze this, we implement an alternative representation based on deep features extracted from a  pre-trained convolutional neural network (CNN)
In particular, we train a CNN with an AlexNet-like architecture on  the DeepFashion dataset to perform clothing classification  (see Supp
for details)
Since fashion elements can be local properties (e.g., v-neck) or global (e.g., a-line), we use  the CNN to extract two representations at different abstraction levels: N) FCN: features extracted from the last hidden  layer; N) MN: features extracted from the third max pooling  layer after the last convolutional layer
We refer to these as  ClothingNet-FCN and ClothingNet-MN in the following
 Forecasting results The textual and visual cues inherently rely on distinct vocabularies, and the metrics applied for Table N are not comparable across representations
 Model Dresses Tops & Tees Shirts  KL IMP(%) KL IMP(%) KL IMP(%)  Meta Information  Tags 0.0NNN 0 0.0NNN 0 0.00NN 0  Text 0.0NNN NN.N 0.00NN NN.N 0.00NN N0.N  Visual  ClothingNet-FCN 0.0NNN -NNN.N 0.NN -NNNN.N 0.N0NN -N0NN.N  ClothingNet-MN 0.0NNN -NNN.N 0.0NNN -NNN.N 0.0NNN -N0.N  Attributes 0.0N0N NN.N 0.00NN NN.N 0.00NN NN.N  Multi-Modal  Attributes+Tags 0.0NNN -NN.N 0.00NN NN.N 0.00NN NN.N  Attributes+Text 0.00NN N0.N 0.00NN NN.N 0.00NN NN.N  Attr+Tags+Text 0.00NN NN.N 0.00NN NN.N 0.00NN NN.N  Table N: Forecast performance for various fashion representations  in terms of KL divergence (lower is better) and the relative improvement (IMP) over the Tags baseline (higher is better)
Our  attribute-based visual styles lead to much more reliable forecasts  compared to meta data or other visual representations
 Nonetheless, we can gauge their relative success in forecasting by measuring the distribution difference between their  predictions and the ground truth styles, in their respective  feature spaces
In particular, we apply the experimental  setup of Sec
N.N, then record the Kullback-Leibler divergences (KL) between the forecasted distribution and the actual test set distribution
For all models, we apply our best  performing forecaster from Table N (EXP)
 Table N shows the effect of each representation on forecasting across all three datasets
Among all single modality  methods, ours is the best
Compared to the ClothingNet  CNN baselines, our attribute styles are much more reliable
 Upon visual inspection of the learned styles from the CNNs,  we find out that they are sensitive to the pose and spatial  configuration of the item and the person in the image
This  reduces the quality of the discovered styles and introduces  more noise in their trajectories
Compared to the tags alone,  the textual description is better, likely because it captures  more details about the appearance of the item
However,  compared to any baseline based only on meta data, our approach is best
This is an important finding: predicted visual  attributes yield more reliable fashion forecasting than strong  real-world meta-data cues
To see the future of fashion, it  pays off to really look at the images themselves
 The bottom of Table N shows the results when using various combinations of text and tags along with attributes
We  see that our model is even stronger, arguing for including  meta-data with visual data whenever it is available
 N.N
Style dynamics  Having established the ability to forecast visual fashions, we now turn to demonstrating some suggestive applications
Fashion is a very active domain with styles and designs going in and out of popularity at varying speeds and  stages
The life cycle of fashion goes through four main  stages [NN]: N) introduction; N) growth; N) maturity; and finally N) decline
Knowing which style is at which level of  NNN    (a) (b) (c)  (d) (e)  (f)  Figure N: Our approach offers the unique opportunity to examine  the life cycle of visual styles in fashion
Some interesting temporal  dynamics of the styles discovered by our model can be grouped  into: (a) out of fashion; (b) classic; (c) in fashion or (d) trending;  (e) unpopular; and (f) re-emerging styles
 its lifespan is of extreme importance for the fashion industry
Understanding the style dynamics helps companies to  adapt their strategies and respond in time to accommodate  the customers’ needs
Our model offers the opportunity to  inspect visual style trends and lifespans
In Fig
N, we visualize the temporal trajectories computed by our model for  N styles from Dresses
The trends reveal several categories  of styles: N) Out of fashion: styles that are losing popularity at a rapid rate (Fig
Na); N) Classic: styles that are relatively popular and show little variations through the years  (Fig
Nb); N) Trending: styles that are trending and gaining popularity at a high rate (Fig
Nc and d); N) Unpopular:  styles that are currently at a low popularity rate with no sign  of improvement (Fig
Ne); N) Re-emerging: styles that were  popular in the past, declined, and then resurface again and  start trending (Fig
Nf)
 Our model is in a unique position to offer this view point  on fashion
For example, using item popularity and trajectories is not informative about the life cycle of the visual  style
An item lifespan is influenced by many other factors  such as pricing, marketing strategy, and advertising among  many others
By learning the latent visual styles in fashion,  our model is able to capture the collective styles shared by  many articles and, hence, depicts a more realistic popularity trajectory that is less influenced by irregularities experienced by the individual items
 N.N
Forecasting elements of fashion  While so far we focused on visual style forecasting, our  model is capable of inferring the popularity of the individual attributes as well
Thus it can answer questions like:  what kind of fabric, texture, or color will be popular next  year? These questions are of significant interest in the fashion industry (e.g., see the “fashion oracle” World Global  GT  (a) Texture  GT  (b) Shape  Figure N: Our model can predict the popularity of individual fashion attributes using the forecasted styles as a proxy
The forecasted  attributes are shown in color while the ground truth is in black
The  attribute size is relative to its popularity rank
 Style Network [N, N], which thousands of designers rely on  for trend prediction on silhouettes, palettes, etc.)
 We get the attribute popularity p(am|t) at a certain time t in the future through the forecasted popularity of the styles:  p(am|t) = ∑  sk∈S  p(am|sk)p(sk|t) (N)  where p(am|sk) is the probability of attribute am given style sk based on our style discovery model, and p(sk|t) is the forecated probability of style sk at time t
 For the N000 attributes in our visual vocabulary, our  model achieves an intersection with ground truth popularity rank at N0%, NN% and NN% for the Top N0, NN and N0 attributes respectively
Fig
N shows the forecasted texture and  shape attributes for the Dresses test set
Our model successfully captures the most dominant attributes in both groups  of attributes, correctly giving the gist of future styles
 N
Conclusion  In the fashion industry, predicting trends, due to its  complexity, is frequently compared to weather forecasting:  sometimes you get it right and sometimes you get it wrong
 In this work, we show that using our vision-based fashion  forecasting model we get it right more often than not
We  propose a model that discovers fine-grained visual styles  from large scale fashion data in an unsupervised manner
 Our model identifies unique style signatures and provides  a semantic description for each based on key visual attributes
Furthermore, based on user consumption behavior,  our model predicts the future popularity of the styles, and  reveals their life cycle and status (e.g
in- or out of fashion)
 We show that vision is essential for reliable forecasts, outperforming textual-based representations
Finally, fashion  is not restricted to apparel; it is present in accessories, automobiles, and even house furniture
Our model is generic  enough to be employed in different domains where a notion  of visual style is present
 Acknowledgment This research is supported by KIT,  NSF IIS-N0NNNN0, and a gift from Amazon
 NNN    References  [N] Fashion Statistics
https://fashionunited.com/  global-fashion-industry-statistics
N  [N] Trend-Forecasting
http://  fusion.net/story/N0NNNN/  wgsn-trend-forecasting-sarah-owen/
N  [N] WGSN
https://www.wgsn.com/en/
N  [N] T
L
Berg, A
C
Berg, and J
Shih
Automatic Attribute  Discovery and Characterization from Noisy Web Data
In  ECCV, N0N0
N  [N] L
Bossard, M
Dantone, C
Leistner, C
Wengert, T
Quack,  and L
Van Gool
Apparel Classification with Style
In  ACCV, N0NN
N, N  [N] G
E
Box, G
M
Jenkins, G
C
Reinsel, and G
M
Ljung
 Time series analysis: forecasting and control
John Wiley &  Sons, N0NN
N  [N] C
Bracher, S
Heinz, and R
Vollgraf
Fashion DNA: Merging Content and Sales Data for Recommendation and Article  Mapping
In KDD Fashion Workshop, N0NN
N, N  [N] R
G
Brown and R
F
Meyer
The fundamental theorem of  exponential smoothing
Operations Research, N(N):NNN–NNN,  NNNN
N  [N] H
Chen, A
Gallagher, and B
Girod
Describing Clothing  by Semantic Attributes
In ECCV, N0NN
N, N  [N0] K
Chen, K
Chen, P
Cong, W
H
Hsu, and J
Luo
Who  are the devils wearing prada in new york city? In ACM  Multimedia, N0NN
N, N, N  [NN] Q
Chen, J
Huang, R
Feris, L
M
Brown, J
Dong, and  S
Yan
Deep Domain Adaptation for Describing People  Based on Fine-Grained Clothing Attributes
In CVPR, N0NN
 N, N  [NN] K
Davis
Don’t Know Much About History: Everything You  Need to Know About American History but Never Learned
 Harper, N0NN
N  [NN] W
Di, C
Wah, A
Bhardwaj, R
Piramuthu, and N
Sundaresan
Style finder: Fine-grained clothing style detection and  retrieval
In CVPR Workshops, N0NN
N  [NN] J
Faraway and C
Chatfield
Time series forecasting with  neural networks: a comparative study using the airline data
 Applied statistics, pages NNN–NN0, NNNN
N  [NN] R
He and J
McAuley
Ups and Downs: Modeling the Visual  Evolution of Fashion Trends with One-Class Collaborative  Filtering
In WWW, N0NN
N  [NN] W.-L
Hsiao and K
Grauman
Learning the Latent “Look”:  Unsupervised Discovery of a Style-Coherent Embedding  from Fashion Images
In ICCV, N0NN
N  [NN] C
Hu, P
Rai, C
Chen, M
Harding, and L
Carin
Scalable Bayesian Non-Negative Tensor Factorization for Massive Count Data
In ECML PKDD, N0NN
N  [NN] J
Huang, R
Feris, Q
Chen, and S
Yan
Cross-Domain  Image Retrieval With a Dual Attribute-Aware Ranking Network
In ICCV, N0NN
N, N, N  [NN] T
Iwata, S
Watanabe, and H
Sawada
Fashion Coordinates Recommender System Using Photographs from Fashion Magazines
In IJCAI International Joint Conference on  Artificial Intelligence, N0NN
N, N  [N0] M
H
Kiapour, X
Han, S
Lazebnik, A
C
Berg, and T
L
 Berg
Where to Buy It: Matching Street Clothing Photos in  Online Shops
In ICCV, N0NN
N, N  [NN] M
H
Kiapour, K
Yamaguchi, A
C
Berg, and T
L
Berg
 Hipster wars: Discovering Elements of Fashion Styles
In  ECCV, N0NN
N, N  [NN] D
P
Kingma and J
L
Ba
ADAM: A Method for Stochastic  Optimization
In ICLR, N0NN
N  [NN] T
G
Kolda and B
W
Bader
Tensor decompositions and  applications
SIAM review, NN(N):NNN–N00, N00N
N  [NN] A
Kovashka, D
Parikh, and K
Grauman
WhittleSearch:  Image search with relative attribute feedback
In CVPR,  N0NN
N, N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
ImageNet  Classification with Deep Convolutional Neural Networks
In  NIPS, N0NN
N  [NN] I
Kwak, A
Murillo, P
Belhumeur, D
Kriegman, and S
Belongie
From Bikers to Surfers: Visual Recognition of Urban  Tribes
In BMVC, N0NN
N, N  [NN] S
Liu, J
Feng, Z
Song, T
Zhang, H
Lu, C
Xu, and S
Yan
 “Hi, Magic Closet, Tell Me What to Wear !”
In ACM Multimedia, N0NN
N  [NN] S
Liu, Z
Song, G
Liu, C
Xu, H
Lu, and S
Yan
Street-toshop: Cross-scenario clothing retrieval via parts alignment  and auxiliary set
In CVPR, N0NN
N, N  [NN] Z
Liu, S
Qiu, and X
Wang
DeepFashion : Powering Robust Clothes Recognition and Retrieval with Rich Annotations
In CVPR, N0NN
N, N, N, N  [N0] J
McAuley, C
Targett, Q
Shi, and A
van den Hengel
 Image-based Recommendations on Styles and Substitutes
In  ACM SIGIR, N0NN
N  [NN] A
C
Murillo, I
S
Kwak, L
Bourdev, D
Kriegman, and  S
Belongie
Urban tribes: Analyzing group photos from a  social perspective
In CVPR Workshops, N0NN
N  [NN] D
Parikh and K
Grauman
Relative Attributes
In ICCV,  N0NN
N  [NN] E
Simo-Serra, S
Fidler, F
Moreno-Noguer, and R
Urtasun
Neuroaesthetics in Fashion: Modeling the Perception  of Fashionability
In CVPR, N0NN
N, N, N  [NN] E
Simo-Serra and H
Ishikawa
Fashion Style in NNN Floats  : Joint Ranking and Classification using Weak Data for Feature Extraction
In CVPR, N0NN
N, N  [NN] Z
Song, M
Wang, X.-s
Hua, and S
Yan
Predicting Occupation via Human Clothing and Contexts
In ICCV, N0NN
N,  N  [NN] G
B
Sproles
Analyzing fashion life cycles: principles and  perspectives
The Journal of Marketing, pages NNN–NNN,  NNNN
N  [NN] K
Vaccaro, S
Shivakumar, Z
Ding, K
Karahalios, and  R
Kumar
The Elements of Fashion Style
In UIST, N0NN
N  [NN] A
Veit, B
Kovacs, S
Bell, J
McAuley, K
Bala, and S
Belongie
Learning Visual Clothing Style with Heterogeneous  Dyadic Co-occurrences
In ICCV, N0NN
N, N  [NN] S
Vittayakorn, A
C
Berg, and T
L
Berg
When was that  made? In WACV, N0NN
N  [N0] S
Vittayakorn, T
Umeda, K
Murasaki, K
Sudo, T
Okatani,  and K
Yamaguchi
Automatic Attribute Discovery with  Neural Activations
In ECCV, N0NN
N  NNN  https://fashionunited.com/global-fashion-industry-statistics https://fashionunited.com/global-fashion-industry-statistics http://fusion.net/story/N0NNNN/wgsn-trend-forecasting-sarah-owen/ http://fusion.net/story/N0NNNN/wgsn-trend-forecasting-sarah-owen/ http://fusion.net/story/N0NNNN/wgsn-trend-forecasting-sarah-owen/ https://www.wgsn.com/en/   [NN] S
Vittayakorn, K
Yamaguchi, A
C
Berg, and T
L
Berg
 Runway to realway: Visual analysis of fashion
In WACV,  N0NN
N  [NN] K
Yamaguchi, H
Kiapour, and T
Berg
Paper doll parsing:  Retrieving similar styles to parse clothing items
In ICCV,  N0NN
N  [NN] K
Yamaguchi, H
Kiapour, L
Ortiz, and T
Berg
Parsing  clothing in fashion photographs
In CVPR, N0NN
N  [NN] A
Yu and K
Grauman
Just noticeable differences in visual  attributes
In ICCV, N0NN
N, N  NNNIncreasing CNN Robustness to Occlusions by Reducing Filter Support   Increasing CNN Robustness to Occlusions by Reducing Filter Support  Elad Osherov  Technion, Israel  eladosherov@campus.technion.ac.il  Michael Lindenbaum  Technion, Israel  mic@cs.technion.ac.il  Abstract  Convolutional neural networks (CNNs) provide the current state of the art in visual object classification, but they  are far less accurate when classifying partially occluded  objects
A straightforward way to improve classification  under occlusion conditions is to train the classifier using  partially occluded object examples
However, training the  network on many combinations of object instances and occlusions may be computationally expensive
This work proposes an alternative approach to increasing the robustness  of CNNs to occlusion
 We start by studying the effect of partial occlusions on  the trained CNN and show, empirically, that training on  partially occluded examples reduces the spatial support of  the filters
Building upon this finding, we argue that smaller  filter support is beneficial for occlusion robustness
We propose a training process that uses a special regularization  term that acts to shrink the spatial support of the filters
We  consider three possible regularization terms that are based  on second central moments, group sparsity, and mutually  reweighted LN, respectively
When trained on normal (un- occluded) examples, the resulting classifier is highly robust  to occlusions
For large training sets and limited training time, the proposed classifier is even more accurate than  standard classifiers trained on occluded object examples
 N
Introduction  Deep Convolutional Neural Networks [NN] have recently  exhibited remarkable performance in the task of image classification [NN]
The availability of large amounts of annotated data [N], parallel computational resources such as  GPUs, and regularization techniques [N0, N, NN] have contributed greatly to CNN performance
Deeper, more sophisticated, network topologies have continuously provided  state-of-the-art results [NN, NN, N0]
 Nevertheless, CNNs, as well as other visual classification algorithms, are far less accurate when classifying partially occluded objects; see Figure (N)
The decrease in performance is especially severe when the classifier is trained  Figure N: (Left) Classification error rates for AlexNet and  VGGNN under occlusions of various sizes
Note that even  the slightest partial occlusion may significantly reduce classification accuracy
(Right) Examples of partially occluded  examples used for validation
 as usual, on images of mostly unoccluded objects
This  problem could be regarded as a special case of domain adaptation, where the training data and the test data are drawn  from different distributions
 Thus, one possible approach to improving classification  accuracy under partial occlusion is to train the classifier on  partially occluded objects as well [NN]
In the first part of  this work we experimented with this approach and indeed  found that the classification accuracy improved
We also  examined the resulting network and observed that (a) its filters (in all layers) tend to be of smaller spatial support, and  (b) every filter uses more features from the previous layer
 Intuition tells us that smaller spatial support makes the filter more robust to occlusion because the probability that a  random occluder will intersect with the filter support and  adversely affect its response is smaller
But does it really  explain the classifier’s greater robustness to occlusions?  To validate the hypothesis that smaller spatial support is  indeed important for occlusion robustness, we propose, in  the next part of this work, to reduce the spatial support in  a different way, by specialized regularization
We propose  three possible regularization terms that act to shrink the spatial support of the filters
These regularization terms are  based on second central moments, group sparsity, and mutually re-weighted LN, respectively
When trained on nor- mal (unoccluded) examples, the resulting classifier is usually (for large training sets) more occlusion robust than stanNNN0    dard classifiers trained on occluded object examples
This  indeed shows that smaller spatial support is beneficial for  occluded object classification
 Note that the first approach requires a large number of  occluded object examples in order to represent the distribution of all object instances from the class under all typical occluders
To synthesize many occluded object examples from regular images of unoccluded objects, we need to  know the object location within the image, which requires a  large labeling effort
Natural, unsynthesized occlusion examples are rare, requiring labeling as well
In both cases the  number of examples and the computational effort are large
 The proposed classifiers with the specialized regularization,  on the other hand, are trainable on the usual datasets
 Our contribution in this paper is twofold: first we analyze  the effect of training with occlusions on CNN visual classifiers, and in particular show the reduction of the filters’  spatial support, accompanied by an increase in its effective  depth
Then we show that similarly reduced spatial support  of the filters may be obtained by training on unoccluded examples by special regularization
We introduce N different  types of such regularization and show that it improves the  classifier’s robustness to occlusions
 The rest of this paper is organized as follows
We start  by describing some related work in Section N
Section N analyzes the CNN trained on occluded object examples
Section N introduces the occlusion-robust CNN
Finally, we describe our experiments and conclude our work in Sections  N and N
Derivation details and more experimental results  (with AlexNet and VGGNN) are described in the Appendix
 N
Related Work  Partial occlusion is one of the major challenges in visual  object classification
Fukushima [N] suggests that human  vision tends to struggle when the occlusion pattern is unnoticeable
He proposes a neural network algorithm that first  detects the occlusion and then nullifies the corresponding  activation map locations
BoW methods [N, NN, NN] generate local pieces of evidence and therefore should be, in  principle, relatively robust to occlusions
Nonetheless, they  often fail when the object is partially occluded
DPM algorithms [N] still respond positively when one of the parts  is undetected, but their performance deteriorates
Other approaches, which relies on HOG features, model the occlusion as a binary coarse grid and explicitly infers it, using a  computationally expensive algorithm [NN, N]
 CNNs provide the best visual classification results but  their accuracy decreases when the network that was trained  on unoccluded training images is fed with partially occluded test images [NN]
They found that training several  sub-networks for each occlusion ratio is sub-optimal, and  suggest that architectural changes may be required to improve occlusion robustness
The authors of [N] quantify  Figure N: An illustration of how filter support amplifies partial occlusion effect
A N× N image with a partial occlusion (red) is filtered with a N × N filter
NN% of the feature map elements are corrupted
When filtered with a N × N filter, NN% of the feature map elements are corrupted
 the invariance of different features of the network to several transformations
They suggest that weight sparsity may  contribute to occlusion invariance
A recent contribution focuses on face recognition and achieves occlusion robustness  by a specialized loss that relies on classifiers that use localized information [NN]
 N
The Effect of Occlusions on CNNs  Our first goal is to characterize the networks that are  more successful with respect to recognition under partial  occlusion
One example is a network trained on partially  occluded training examples, which is therefore more robust  to occlusions
Since CNNs are composed of linear filters,  they are greatly affected by occlusions
A linear filter that  sums values from all the objects’ regions responds very differently when some of these values change significantly, as  is the case under occlusion
Therefore an occlusion-robust  classifier should use features that do not rely on the spatial support of the entire object but only on part of it
We  are interested in the spatial support of the filters’ receptive  fields
Instead of using a strict definition of spatial support,  namely the set of locations associated with nonzero filter  weights, we prefer softer measures that take into consideration the magnitude and location of the filter weights and are  referred to as effective spatial support; see Section N.N
 To examine the change in effective spatial support, we  trained two CNNs
One was trained in the usual way, with  (unoccluded) images taken from the ImageNet data set [NN]
 The other was trained on both unoccluded and partially occluded images
We are interested in comparing the corresponding filters across the two networks, and more specifically the effective spatial support of their receptive fields
 To establish a correspondence between the filters from both  CNNs, we conducted a somewhat more complex experiment; see Section N.N for details
As expected, we found  that networks trained with occluded examples are better  able to recognize partially occluded objects
We also found  that these networks are composed of filters with smaller spatial support relative to filters in networks trained as usual
 Smaller spatial support means that more neurons would respond independently of the occlusion; see Figure N
We  argue that the smaller spatial support is indeed a major reaNNN    son for the advantage of the occlusion trained networks and  shall provide further evidence for this claim in Section N
 In the rest of this section we elaborate on this finding,  suggest several measures for spatial support, and show empirically that training with occluded examples indeed results  in filters of lower spatial support
 N.N
Measures of Spatial Support  We start by briefly describing the CNN structure we use,  and the associated notations
A CNN is typically composed  of several convolution layers, and then several fully connected layers topped by a classifier
A convolution layer is  composed of K linear filters followed by nonlinearities and  optional pooling stages
Every filter is specified by a ND  matrix of size D × M × N
We will use the tensor nota- tion [N], with a ND Wl tensor, where l represents the layer number
A single element (i.e
a weight) of this tensor is  denoted Wlkdmn, where k is the filter index, d is the filter depth (or input channel), and m,n are the spatial indexes
 We shall be interested in the spatial distribution of the  weight values
Therefore, we focus on sub-filters of size  M × N, corresponding to common k, d values
These ND sub-filters are called kernels
We consider each kernel separately, and characterize the distribution of its weight values  as described below
Then we get more concise characterizations by summing these measures over the different kernels  and the different filters in each layer
 The measures of spatial distribution should not depend  on the magnitude of the weights or on their signs
Therefore, we use their normalized values
Formally, let Wlkd:: be the ND matrix of weight associated with the (k, d)-th kernel
Then, the tensor V of the absolute value of the filter  weights and the tensor V̂ of the normalized filter weights  are specified by:  V l kd:: = |W  l kd::|, V̂  l kd:: =  |Wlkd::|  ||Wlkd::||N 
(N)  The resulting normalized weight kernels may be interpreted  as a (ND) probability distribution
 We shall use two alternative measures for characterizing  the spatial support of the filter weights
 Spatial entropy - Entropy, which is a measure of uncertainty for random variables, may be applied to positive  vectors of unit LN length as a diversity measure
We shall use it here as a measure for the scattering of filter weights,  and refer to it as spatial entropy
Thus, the spatial entropy  HS of the (k, d)-th ND kernel W l kd:: is:  (HS) l k,d = −  ∑  m,n  V̂ l kdmnlog(V̂  l kdmn)
(N)  The entropy of the filter is the average of its kernel entropies, weighted by the relative LN energy:  (HS) l k =  D∑  d=N  H lk,d · ‖Wlkd::‖N ‖Wlk:::‖N  
(N)  (Other types of weighting could be used as well.) Note that  the entropy is indifferent to permutations of the weights,  and therefore it is only indicative of the spatial support
If  the entropy value is large, then the distribution is closer to  uniform and the support is large as well
When it is small,  however, it could correspond to two significant weights in  nearby locations (small spatial support) or to two significant  weights in far locations (large spatial support)
We call this  spatial entropy because it refers to the scattering of weights  along spatial coordinates in the different kernels and not to  their scattering for different filters or depths
 Second central moment - Another measure that can be  used to estimate the scattering of the filter weights is the  second moment
Here, again, we begin by normalizing the  weights so that their sum is one
Then, following rigid body  mechanics, we calculate the center of mass and the second  moment relative to it
A filter with substantial weights far  from its center will have a large second central moment
 Consider a ND discrete grid of masses {wmn}
The ij moment is Mij =  ∑ m,n wmnm  inj 
Specifically, M00 is the zeroth moment, which is simply the sum of the weights,  and MN0,M0N are the first moments
The center of mass is defined as (µx = MN0/M00, µy = M0N/M00)
The second central moments are Cij =  ∑ m,n wmn(m−µx)  i(n−µy) j 
 The latter moments (known as moments of inertia in mechanics) are common measures for the concentration of  mass
 We shall use the absolute and the normalized  weights specified in eq.(N), and treat these filter weights  as point masses
The moments and the associated centers of mass are calculated from the absolute and normalized weights separately for each kernel, and denoted (Mij) l kd, (µx)  l kd, (µy)  l kd, (Cij)  l kd and  (̂Mij) l  kd , (̂µx)  l  kd, (̂µy) l  kd , (̂Cij)  l  kd respectively
 We use the sum of the two normalized second central  moments,  τ lkd = (̂CN0) l  kd + (̂C0N) l  kd, (N)  to characterize the effective spatial support of the kernel
To  characterize a filter, we shall take again a weighted sum of  this measure over all the kernels in the filter,  τ lk =  D∑  d=N  τ lkd · ‖Wkd::‖N ‖Wk:::‖N  
(N)  In principle, we could normalize the filter weights, the corresponding moments, and the weights of the kernels in  eq.(N) differently, using, say, Euclidean norms
The results  NNN    are similar
We kept the LN norm so that the same nor- malization works for the spatial entropy
Interestingly, the  effective spatial support τ lk may be written in a simplified way, using the unnormalized moments:  τ lk = N  ‖Wk:::‖N  D∑  d=N  [(CN0) l kd + (C0N)  l kd]
(N)  See eq.(NN) in the Appendix for details; This simplification  is used later in Section N
 Compared to the entropy based measure, this measure,  based on central moments, is a more direct measure of the  concentration of the filter weights
It explicitly considers  the spatial location associated with the weights and calculates their distance from the center of mass
A large second moment means that some non-negligible weights are  far from the center of mass, implying larger effective spatial support
Therefore, we prefer this measure over the entropy based measure, but shall use both of them below to  strengthen the evidence of the shrinking receptive fields
 N.N
The Effect of Occlusions on Spatial Support  We are interested in the change of effective spatial support caused by training the CNN on images of partially occluded objects
Training two CNN networks separately, one  on occluded objects and one on unoccluded objects, would  result in two sets of unrelated filters
To compare the spatial  support of specific filters, we need to maintain a correspondence between pairs of filters, one from each network
To  maintain this correspondence, we conducted a more careful  experimental procedure, described in Section N.N
This procedure produces two CNN networks with correspondence  between the filters
We calculated the spatial entropy (N)  and the second central moment (N) of every filter, as well as  the fraction of filters for which the measures were larger in  the network trained only with unoccluded examples; see Table N and TableN in the appendix
The results clearly show  that the spatial support tends to be smaller when training  with occluded examples
Note that the differences in spatial  support are actually very small
This is a result of the procedure we use and the price paid for maintaining the correspondence; see Section N.N for more details and for related  results without filter correspondence
 As discussed above, filters with smaller support are less  likely to be influenced by partial occlusion than filters with  larger support
This seems to be the source of the preference for smaller support filters observed when training on  occluded objects
Note, however, that filters with larger  support are more sensitive, in principle, to additional object parts and details, and are therefore potentially more discriminative
Still, the results indicate that the lower sensitivity to occlusion is more significant than the decrease in  discriminative power
 conv layer N N N N0 NN  Fraction of filters in net A  with larger spatial entropy than 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  corresponding filters in net B
 Fraction of filters in net A with  larger Nnd central moments than 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  corresponding filters in net B
 Table N: The fraction of filters in network A, (VGGNN  trained only with unoccluded examples) that have a larger  effective support than the corresponding filters in network  B (trained also with occluded examples)
The spatial support of the filters tends to be lower if the training images are  partially occluded; see appendix A for additional results
 N.N
The Effect of Occlusions on the Effective Depth  The filter spatial support is smaller when training on occluded object images
The influence of the different kernels  on the overall filter response is different as well
To measure the contribution of the various kernels to each filter, we  consider the energy of the kernel weights, ||Wlkd::|| N N
The  normalized energy:  Slkd = ||Wlkd::||  N N∑D  i=N ||W l ki::||  N N  , (N)  serves as a measure of the contribution of every kernel to  the filter response
Note that the normalized energy values  sum to N
The distribution of these contributions within a  filter may be estimated by the depth entropy HD, where the subscript D stands for filter depth
 (HD) l k =  D∑  j=N  −Slkj · log(S l kj)
(N)  This entropy can serve as an indication of the number of  kernels that significantly influence the filter response, and  we refer to it as effective depth
A filter that depends, for  example, on a single kernel, would correspond to zero entropy and minimal effective depth, while a filter depending  on all kernels equally would correspond to logD entropy and maximal effective depth
 In the same way that we calculated the change in the spatial support, we now calculate the fraction of filters where  the depth entropy (N) was larger in the network that was  trained only with unoccluded examples, relative to the network that was trained also with occluded examples
The  results (Table N) indicate that the kernel energy in the network trained also on occluded examples is distributed more  evenly, i.e., more kernels contribute to the response
This  larger effective depth may have two related but different explanations
First, the network trained on partially occluded  NNN    conv layer N N N N0 NN  Fraction of filters in net A with  larger depth entropy than 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  corresponding filters in net B
 Table N: The fraction of filters in network A, (VGGNN  trained only with unoccluded examples) that have a larger  depth entropy than the corresponding filters in network B  (trained also with occluded examples)
When training on  partially occluded examples, more kernels play a meaningful role in the feature extraction process; see appendix A for  additional results
 examples uses a smaller part of the object to derive the intermediate pieces of evidence and therefore requires more  of them to be discriminative
In addition, because different parts of the object may be occluded in different test images, the network should use alternative configurations of  features, which again increases the diversity of the kernels
 Interestingly, this effect is more significant for the deeper  layers; see Table N
Neurons in these layers detect coarser  level features or full object parts
With training under occlusion, they may be collecting alternative sets of evidence,  such as different subsets of the object’s parts
 N
Enforcing Small Spatial Support via Regularization  As discussed in the introduction, and empirically verified  (Section N), a straightforward way to improve classification  under occlusion conditions is to train the classifier using  partially occluded examples
We propose here an alternative approach: training the classifier on normal, unoccluded  examples, with bias to smaller spatial support filters
This  bias, which relies on the observation from the previous section, is implemented by several special regularization terms
 Our motivation is two-fold:  N
Training on many combinations of object instances  and occlusions is computationally expensive and/or requires detailed localization annotation; see Figures N,N
 Therefore, an algorithm trainable only on unoccluded  object instances, but which still provides robustness to  occlusion, is desirable
 N
While we have shown that training on partially occluded examples reduces the filters’ spatial support,  we did not provide evidence that this reduction contributes significantly to occlusion robustness
It could  be that other properties of the network, learned from  the occluded examples, provide this robustness and the  reduced spatial support is only a side effect
By showing that a network trained only on unoccluded object  Figure N: Different occlusion scenarios
When the object’s  location is not given (as is the case in ImageNet), creating  synthetic partial occlusions by placing an occluder in a random location may be problematic
For example, the owl occluder, covering N0% of the full image may partially cover  the tractor target, completely cover it, or not cover it as all
 examples is robust to occlusion, we provide direct evidence that small spatial support provides robustness
 N.N
Convolution layer regularization terms  We shall train CNNs composed of convolution layers and  fully connected layers
The training is carried out by minimizing a loss function:  L = L0 +  L∑  l=N  λlRl, (N)  where L0 is the data loss function, λ l are regularization  strength coefficients, which may differ for each layer, and  Rl is a regularization term for the l-th layer, which penal- izes for large spatial support of the filters in this layer
We  shall consider several alternative regularization options
 N.N.N Regularization by minimizing second central  moments  We first propose to use the second central moment of the  different convolution filters as a regularization term
Thus,  the regularization term Rl becomes:  RlCM =  K∑  k=N  τ lk
(N0)  Following eq.(NN), this term and its derivative become:  RlCM =  K∑  k=N  N  ||wk:::||N  D∑  d=N  [(CN0) l kd + (C0N)  l kd]
(NN)  ∂RlCM  ∂Wlkdij =  [ sign(Wlkdij)  ||wk:::||N −  Wlkdij ||wk:::||NN  ] · [(i− (µx)  l kd)  N + (j − (µy) l kd)  N];  (NN)  see appendix A
Note that the term Wlkdij  ||wk:::||NN is negligible  in comparison to sign(Wlkdij)  ||wk:::||N , in each case where the filter  NNN    is not extremely sparse, which is a reasonable assumption
 This means that the sign of the derivative is determined only  by sign(Wlkdij)
The term N  ||wk:::||N only changes the relative importance of the regularization term within the overall  loss function, and therefore may be replaced by changing  λl
Moreover, ||wk:::||N changes very slowly in practice
Thus, a simplified expression for the derivative, which we  use in the optimization, is:  ∂RlCM ∂Wlkdij  ≈ sign(Wlkdij)[(i− (µx) l kd)  N + (j − (µy) l kd)  N]
 (NN)  This regularization term shrinks weights that are far from  the center of mass of each kernel
 N.N.N Regularization by enforcing group sparsity  Another way to reduce the effective support of the kernels  is to shrink together specific filter weight subsets, chosen  to explicitly reduce the support, instead of shrinking each  filter weight independently using the usual LN regularization
We propose to use structured sparsity regularization,  and specifically group sparsity regularization (also known  as group Lasso [NN]), used for sparse signal representation
 Sparsity with group regularization prefers not only that the  number of non-zero weights will be small, but also that  these weights will come from a minimal number of weight  subsets (or groups); see Figure N in Appendix A for an illustration of the groups used in our experiments
The regularization term Rl in this case is:  RlGS = ∑  k,d  ∑  r  √∑  i,j  (Wlkd::) N
∗Gr, (NN)  where every Gr is an indicator matrix, the same dimension as the kernel, which indicates whether the (i, j)-th weights is a member of the group
The derivative of this term with  respect to Wlkdij is:  ∂RlGS ∂Wlkdij  = ∑  r  WlkdijGr(i, j)√∑ m,n(W  l kd::)  N
∗Gr  
(NN)  It is possible to shrink all the kernels using several sparsity  groups, using the same sparsity group, or using a different  group for each one
In our implementation we randomly  chose a single group for each kernel
 N.N.N Regularization by mutually re-weighted LN  Finally, inspired by the results of [N], which showed that  promoting sparsity in the network weights increases network invariance to various deformations, we suggest using a  variation of weighted LN regularization
It is known that LN regularization induces sparsity, while retaining the desired  Figure N: Generating occluded examples
(Left) An occluding patch is acquired from a different object category using  intelligent scissors and is re-scaled to a certain occlusion  ratio, with respect to the target image
(Right) The target  image is occluded using the previously acquired occluder
 convexity attribute that the L0 norm lacks
Sparsity can be achieved effectively using an iterative sequence of minimizations, known as the iterative reweighted LN minimiza- tion (IRWLN) [N]
Each iteration minimizes a reweighted  LN norm, where the weights used for the next iteration are computed from the value of the current solution
We  propose to use a variation of this algorithm
The original  reweighting scheme uses the size of each entry to calculate  the weight of this entry (in the next iteration) while we use  the other entries of the vector to calculate this weight in the  next iteration
In the context of CNN kernels, for a particular filter weight X in some kernel, we use the other filter weights in the same kernel to calculate the weight of X 
We call this algorithm Mutually Re-WeightedLN (MRWLN) and specify the corresponding regularization term:  RlMRWLN = ∑  k,d  ∑  i,j  |Wlkdij |· ∑  m N=i,n N=j  |Wlkdmn|
(NN)  In comparison to LN regularization, where the weights shrink at the same rate, MRWLN constantly changes the  shrinking rate for each weight, with respect to its spatial  neighbors
MRWLN tends to shrink smaller weights more  rapidly then larger weights, effectively promoting sparsity
 The derivative of RlMRWLN with respect to W l kdij is:  ∂RlMRWLN ∂Wlkdij  = sign(Wlkdij) · ∑  m N=i,n N=j  |Wlkdmn|
(NN)  Note that the weight ∑  m N=i,n N=j |W l kdmn| has a similar effect as the weight N/(|Wlkdmn|+ǫ), which would be used if we followed the reweighting proposed in [N], provided that  the sum (or norm) of the weights in the kernel is constant
 N.N
Fully connected layer regularization  The first fully connected layer is the deepest layer in the  CNN for which the spatial information of the input is explicit
The output of this layer lacks any direct spatial information that can be traced back to the input location of  the pixels
Following the discussion in Section N, we argue  NNN    that recognition of partially occluded objects would benefit  from shrinking the filters’ support
The same regularization  terms, proposed in Section N.N for the convolution filters,  can also be used for the optimization of the first fully connected layer weights, while promoting smaller support and  sparsity; see Section N for experimental results
 N.N
The learning algorithm  The regularization terms presented above are independent with respect to the different kernels, implying that  training using the stochastic gradient descent algorithm is  efficient
Algorithm N describes the SGD training process
 The algorithm depends, via a parameter R, on one of the regularization terms,(N0),(NN) or (NN), and uses the associated derivative (NN),(NN) or (NN)
The algorithm’s other parameters are the learning rate function γt, the batch size B, and the regularization strength coefficients {λl}Ll=N
Addi- tional learning techniques such as momentum, LN regular- ization and batch normalization, may be incorporated
 Algorithm N Small support regularized SGD  N: Input: training set D = {xN:N , yN:N},R,B,γt, {λ l}Ll=N  N: winit ← N(0, σ),winit ∈ R L K×D×M×N  N: while (epoch ≤ numberofepochs) do N: for n = N...BatchNumber do N: wn ← wn−N −  γt B  ∑B b=N[  ∂L0(xb) ∂w + λ  l ∂R(xb) ∂w ]  N: end for  N: epoch← epoch+ N N: end while  N: return w  N
Experiments and Results  We start by describing experiments that demonstrate the  influence of training under occlusion on the properties of the  network, as discussed in Section N
We then turn to evaluating the categorization accuracy of algorithms that use the  proposed specialized regularization
We use three popular  datasets: CIFAR-N0/CIFARN00 [NN], and ImageNet [NN]
 Since, to our knowledge, there is no large scale data set  that contains occlusion annotations, we generated and used  several types of artificial occlusions
The occluders were  crops from image of other categories obtained by running  the intelligent scissors [NN] in random locations
We also  experimented with random rectangles and got similar results
The occluders’ sizes were characterized by the ratio δ between their sizes and the entire image
Note that δ = 0.N may correspond to a square occluder whose sides equal 0.NN of the full image—not a small image part
All experiments  were done using the MatConvNet toolbox [NN]
 N.N
Training with partially occluded examples  In the following, we elaborate on the experimental settings used for obtaining the results in Section N, and show  additional results
To evaluate the effect of training with  partially occluded examples on the spatial support of the  filters, we conducted two types of experiments
 In the first type, we trained N CNN networks as follows
 Our goal here was to maintain a correspondence between  the filters of the two networks
Thus, we first trained both  networks identically for NN epochs on unoccluded examples  (from ImageNet [NN])
We then continued the training for N  epochs, with a learning rate that was N00 times smaller than  that used in the first NN epochs
For these last five epochs,  the first network was trained on unoccluded examples but  the second network was trained on partially occluded examples
In the resulting networks, the corresponding filters  are not identical but essentially extracted features with the  same functionality
A comparison between the properties  of these filters is described in Section N
We experimented  with AlexNet [NN], and VGGNN [NN], both with batch normalization [N0]
 In the experiments of the second type, we trained the  two CNNs separately, one on unoccluded examples and the  other on occluded examples only
Here, correspondence between filters is not known (or does not exist), so comparing  specific filters is meaningless
Thus, we only compared the  average of the effective spatial support, over all filters in the  same layer; see Table N in Appendix A
These results further confirm our findings regarding the smaller filter support  caused by training on occluded objects
 N.N
Regularization for occlusion robustness  N.N.N The CIFAR Datasets  The CIFAR-N0/CIFARN00 datasets [NN] are both drawn  from N0 Million Tiny Images [NN]
They both contain N0K  training examples and N0K test examples, all of which are  NN×NN RGB
CIFARN0 consists of N0 distinct classes while CIFARN00 consists of N00
Both training and test data are  distributed uniformly in both datasets
We evaluated the  recognition accuracy under partial occlusion by training a  LeNet [NN] CNN with different choices of the proposed regularization terms; see Table N
 When training on unoccluded object examples, without regularization, the error associated with classifying occluded objects (Nnd line in Table N) is much higher than that  associated with classifying unoccluded objects (top line)
 As expected, when training on occluded object examples,  the error associated with classifying partially occluded objects decreases (Nrd line)
The proposed classifiers, when  trained on normal (unoccluded examples), reduced the classification error on occluded objects as well, regardless of the  choice of regularization term
This improvement came, as  NNN    LeNet-CIFARN0 LeNet-CIFARN00  δ = 0.N δ = 0.N δ = 0.N δ = 0.N  ut+uv NN.NN NN.NN  ut+ov NN.NN N0.0N N0.NN NN.NN  ot+ov NN.NN NN.NN NN.NN N0.NN  RCM conv NN.NN NN.N0 NN.NN NN.NN  RCM conv+fc NN.00 NN.N0 NN.NN NN.NN  RGS conv NN.N0 NN.N0 NN.NN N0.0N  RGS conv+fc NN.NN NN.0N NN.NN NN.NN  RMRWLN conv NN.0N NN.NN NN.NN NN.NN  RMRWLN conv+fc NN.NN NN.NN NN.NN NN.NN  RLN conv+fc NN.NN NN.NN N0.N0 NN.NN  RIRWLN[N] conv+fc NN.NN NN.NN NN.NN N0.NN  Table N: Recognition error rates (%) on the CIFARN0 and  CIFARN00 datasets
ut: unoccluded train
uv: unoccluded  val
ot: occluded train
ov: occluded val
δ: occlusion ratio
 expected, with smaller filter support
Adding a regularization term on the first fully connected (FC) layer further improves classification results, in most cases
For comparison  we also experimented with LN and IRWLN regularizations
 For CIFARN0, training with occluded examples yielded  better accuracy than our regularization based approach, although, for moderate occlusion (δ = 0.N), the difference is small (for Rcm regularization)
For CIFARN00, the reg- ularization based approach achieved slightly better results
 We associated the difference between these two cases with  the smaller number of examples (N00 vs NK) per class in the  second case, which is probably not enough for training
 For best performance on occluded objects, λ is set to a relatively high value; see Appendix
When tested on unoccluded objects, with this λ value, performance is slightly reduced
(error rate increases by 0.N%)
When setting λ optimally for recognizing unoccluded objects, the results  are better than those obtained with LN regularization (error rate goes down by N.N%)
Therefore, RCM should be pre- ferred also for general classification
This surprising result  is consistent with [NN], where similar improvements were  obtained by training with occlusions
 N.N.N The ImageNet Dataset  The picture changes when experimenting with the  ILSVRCN0NN classification task [N]
We used AlexNet and  VGGNN topologies [NN, NN], with batch normalization [N0]
 We experimented with training several CNNs
The first was  trained on unoccluded object examples and tested on both  unoccluded and occluded validation sets (δ = 0.N)
Next we trained a network with partially occluded object examples, also with a δ = 0.N occlusion ratio, and tested it on the corresponding occluded validation set
Finally, we trained  a network with different regularization terms
For AlexNet,  RCM regularization was used on the first and second conILSVRCNN top-N\top-N AlexNet top-N\top-N VGGNN  ut+uv NN.N\NN.N N.N\NN.N  ut+ov NN.N\N0.N NN.N\NN.N ot+ov NN.N\NN.N NN.N\NN.N Support regularized net NN.0\NN.N NN.N\NN.N  Table N: Recognition error rates (%) on the ILSVRCNN  dataset for AlexNet and VGGNN
ut: unoccluded train
uv:  unoccluded val
ot: occluded train
ov: occluded val
The  last three rows correspond to testing of occluded objects
 Figure N: VGGNN convergence of support-regularized network vs
the common network trained on occluded examples
The regularized network converged N times faster
 volution layers as well as on the first fully connected layer  and RMRWLN was used on the third, fourth and fifth convo- lution layers
For VGGNN we used RCM on the first layer of each cascade
Reducing the support of the first element  in the cascade results in support reduction of the entire cascade
The results are given in Table N
The regularized VGG  network achieved a significant advantage over training on  occluded examples, and its validation accuracy was close  to that obtained without occlusion at all
We associate this  advantage with the smaller size of the objects in ImageNet,  which implies that the simulated occluder might eliminate  the object completely, or not even intersect with it; see Figure N
This makes training with occluded examples less effective, prolonging training significantly; see FigureN
 N
Conclusions  We considered visual classification under occlusion using CNNs
We show that training with partially occluded  objects reduces the spatial support of the CNN filters and  increase their effective depth
Following these observations,  we propose a new learning algorithm that relies on special  regularization and trains with regular unoccluded examples,  while producing a classifier that is robust to occlusions
In  the realistic case of large train sets with weakly annotated  images, it trains faster and is more accurate than training  with occluded objects
This result is surprising since limiting spatial support does not address all aspects of occlusion
 NNN    Acknowledgments  This research was supported by the Israel Science Foun- dation (grant No.NNNN\NN)
 References  [N] E
J
Candes, M
B
Wakin, and S
P
Boyd
Enhancing sparsity by reweighted lN minimization
Journal of Fourier Analysis and Applications, N00N
N, N  [N] G
Csurka, C
Dance, L
Fan, J
Willamowski, and C
Bray
 Visual categorization with bags of keypoints
In Workshop  on Statistical Learning in Computer Vision (ECCV), N00N
N  [N] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
 In Proceedings of the conference on Computer Vision and  Pattern Recognition (CVPR), N00N
N, N  [N] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and D
Ramanan
Object detection with discriminatively trained partbased models
Transactions on Pattern Analysis and Machine Intelligence (PAMI), N0N0
N  [N] K
Fukushima
Recognition of partly occluded patterns: a  neural network model
Biological Cybernetics, N00N
N  [N] T
Gao, B
Packer, and D
Koller
A segmentation-aware  object detection model with occlusion handling
In Proceedings of the conference on Computer Vision and Pattern  Recognition (CVPR), N0NN
N  [N] I
Goodfellow, Y
Bengio, and A
Courville
Deep learning
 Book in preparation for MIT Press, N0NN
N  [N] I
Goodfellow, H
Lee, Q
V
Le, A
Saxe, and A
Y
Ng
Measuring invariances in deep networks
In Advances in Neural  Information Processing Systems (NIPS), N00N
N, N  [N] I
J
Goodfellow, D
Warde-farley, M
Mirza, A
Courville,  and Y
Bengio
Maxout networks
In Proceedings of the International Conference on Machine Learning (ICML), N0NN
 N  [N0] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N, N, N  [NN] A
Krizhevsky and G
Hinton
Learning multiple layers of  features from tiny images
N00N
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in Neural Information Processing Systems (NIPS),  N0NN
N, N, N  [NN] S
Lazebnik, C
Schmid, and J
Ponce
Beyond bags of  features: Spatial pyramid matching for recognizing natural  scene categories
In Proceedings of the conference on Computer Vision and Pattern Recognition (CVPR), N00N
N  [NN] B
B
Le Cun, J
S
Denker, D
Henderson, R
E
Howard,  W
Hubbard, and L
D
Jackel
Handwritten digit recognition  with a back-propagation network
In Advances in Neural  Information Processing Systems (NIPS), NNN0
N, N  [NN] E
N
Mortensen and W
A
Barrett
Intelligent scissors for  image composition
In The NNnd annual conference on Computer Graphics and Interactive Techniques, NNNN
N  [NN] M
Opitz, G
Waltner, G
Poier, H
Possegger, and  H
Bischof
Grid loss: Detecting occluded faces
In European Conference on Computer Vision (ECCV), N0NN
N  [NN] B
Pepik, R
Benenson, T
Ritschel, and B
Schiele
What is  holding back convnets for detection? In German Conference  on Pattern Recognition, N0NN
N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual  Recognition Challenge
International Journal of Computer  Vision (IJCV), N0NN
N, N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
International  Convention on Learning Representations (ICLR), N0NN
N,  N, N  [N0] N
Srivastava, G
E
Hinton, A
Krizhevsky, I
Sutskever, and  R
Salakhutdinov
Dropout: a simple way to prevent neural networks from overfitting
Journal of Machine Learning  Research, NN, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In Proceedings of the  conference on Computer Vision and Pattern Recognition  (CVPR), N0NN
N  [NN] A
Torralba, R
Fergus, and W
T
Freeman
N0 million tiny  images: A large data set for nonparametric object and scene  recognition
Transactions on Pattern Analysis and Machine  Intelligence (PAMI), N00N
N  [NN] J
R
Uijlings, K
E
van de Sande, T
Gevers, and A
W
 Smeulders
Selective search for object recognition
International Journal of Computer Vision (IJCV), N0NN
N  [NN] A
Vedaldi and K
Lenc
Matconvnet – convolutional neural  networks for matlab
In Proceeding of the ACM Int
Conf
on  Multimedia, N0NN
N  [NN] A
Vedaldi and A
Zisserman
Structured output regression  for detection with partial truncation
In Advances in neural  information processing systems(NIPS), N00N
N  [NN] L
Wan, M
Zeiler, S
Zhang, Y
L
Cun, and R
Fergus
Regularization of neural networks using dropconnect
In Proceedings of the International Conference on Machine Learning  (ICML), N0NN
N  [NN] X
Wang, A
Shrivastava, and A
Gupta
A-fast-rcnn: Hard  positive generation via adversary for object detection
Proceedings of the conference on Computer Vision and Pattern  Recognition (CVPR), N0NN
N  [NN] M
Yuan and Y
Lin
Model selection and estimation in regression with grouped variables
Journal of the Royal Statistical Society: Series B (Statistical Methodology), N00N
N  NNN    A
Supplementary Material  A.N
Second central moment regularization deriva- tive  In the following we provide the full derivation of the second central moment regularization term, RCM , that was in- troduced in (NN)
We consider the absolute value of the filter  weights, normalized by the LN norm of the entire filter (note that it differs from V̂ (N)):  Ṽ l  kd:: = |Wlkd::|  ||Wlk:::||N 
(NN)  Let RlCM = (R l CM )  x + (RlCM ) y , where (RlCM )  x and  (RlCM ) y correspond to the horizontal second central moment CN0 and the vertical second central moment C0N, re- spectively
Below we calculate the derivative of (RlCM )  x
 The derivative of (RlCM ) y is symmetric
 The derivative of (RlCM ) x with respect to a specific filter  weight is obtained using the chain rule:  ∂(RlCM ) x  ∂Wlkdij =  ∂(RlCM ) x  ∂Ṽ l  kdij  · ∂Ṽ  l  kdij  ∂Wlkdij 
(NN)  The explicit term for (RlCM ) x is:  (RlCM ) x =  = ∑  k,d  N  ||Wlk:::||N  [∑  m,n  |Wlkdmn|·m N −  ( ∑  m,n|W l kdmn|·m)  N  ∑ m,n|W  l kdmn|  ]  = ∑  k,d  [∑  m,n  |Wlkdmn|·m N  ||Wlk:::||N −  (∑ m,n|W  l  kdmn|·m  ||Wlk:::||N  )N  ∑ m,n|W  l  kdmn|  ||Wlk:::||N  ]  = ∑  k,d  [∑  m,n  Ṽ l  kdmn ·m N −  (∑ m,n Ṽ  l  kdmn ·m )N  ∑ m,n Ṽ  l  kdmn  ] 
 (N0)  This term can now be written using the moments M̃N0,  M̃N0 and M̃00 (which are the moments in the Ṽ l  kdmn values)
Calculating the first element of the derivative, with  respect to Ṽ l  kdij yields:  ∂(RlCM ) x  ∂Ṽ l  kdij  = ∂  ∂Ṽ l  kdij  u=K,v=D∑  u=N,v=N  [ (M̃N0)  l uv −  (M̃NN0) l uv  (M̃00)luv  ]  = [ iN −  N · (M̃N0) l kd · (M̃00)  l kd · i− (M̃  N N0)  l kd  (M̃N00) l kd  ]  = [ iN · (M̃N00)lkd − N · (M̃N0)lkd · (M̃00)lkd · i+ (M̃NN0)lkd  (M̃N00) l kd  ]  =  [ (M̃00)  l kd · i− (M̃N0)  l kd  ]N  (M̃N00) l kd  = [ i− (µ̃x)  l kd  ]N 
 (NN)  Note that (µ̃x) l kd = (µx)  l kd
Deriving the second term of  (NN) yields:  ∂Ṽ l  kdij  ∂Wlkdij =  ∂  ∂Wlkdij  |Wlkdij |  ||Wlk:::||N =  = [sign(Wlkdij) ||wk:::||N  − Wlkdij  ||wk:::||NN  ] 
 (NN)  To sum up, the complete term for the derivative, with  respect to a single filter weight Wlkdij takes the form of:  ∂RlCM  ∂Wlkdij =  [ sign(Wlkdij)  ||wk:::||N −  Wlkdij ||wk:::||NN  ] · [(i− (µx)  l kd)  N + (j − (µy) l kd)  N];  (NN)  A.N
Simplifying the expression for the effective spa- tial support of a filter  In the following we provide further details regarding the  simplification made in Section N eq.(N)-(N) in order to obtain the simplified term in eq.(N), which is used to calculate  the second central moment of a filter
This simplification  allowed us to calculate (CN0) l kd and (C0N)  l kd using the absolute valued weights, instead of the normalized weights as  in (̂CN0) l  kd and (̂C0N) l  kd
Substituting eq.(N) into (N) yield  the following:  D∑  d=N  (̂CN0) l  kd  ‖Wkd::‖N ‖Wk:::‖N  = D∑  d=N  [∑  i,j  |Wlkdij |  ||Wlkd::||N · jN  − ( ∑  i,j  |Wlkdij |  ||Wlkd::||N · j)  N  ∑ m,n  |Wlkdmn| ||Wlkd::||N  ] · ‖Wkd::‖N ‖Wk:::‖N  =  ∑D d=N(CN0)  l kd  ‖Wk:::‖N 
 (NN)  Hence, using the unnormalized second moments is  equivalent up to N‖Wk:::‖N , which is shown in eq.(N)
 NNN    A.N
Additional results supporting the observation that training with occluded examples lowers the spatial supports of the filters and increases their effective depths  In the following, we provide further, more detailed results regarding the experiments presented in section N and  section N
First we provide the complete results of second  central moment and spatial entropy for two VGGNN networks, where the first trained with unoccluded examples,  and the second is based on the first, but is trained on occluded examples near the end of the training process; see  Sections N and N, for details
The following table completes  the results in Table N, Section N.N:  Fraction of filters in net A with Fraction of filters in net A with  larger spatial entropy than larger Nnd central moments than  corresponding filters in net B corresponding filters in net B  conv N 0.NNN 0.NNN  conv N 0.NNN 0.NNN  conv N 0.NNN 0.NNN  conv N 0.NNN 0.NNN  conv N 0.NNN 0.NNN  conv N 0.NNN 0.N0N  conv N 0.NNN 0.NNN  conv N 0.NNN 0.NNN  conv N 0.NNN 0.NNN  conv N0 0.NNN 0.NNN  conv NN 0.NNN 0.NNN  conv NN 0.NNN 0.NNN  conv NN 0.NNN 0.NNN  Table N: The fraction of filters in network A, (VGGNN  trained only with unoccluded examples) that has a larger  effective support than the corresponding filters in network  B (trained also with occluded examples)
The spatial support of the filters tends to be lower if the training images are  partially occluded
 Next we provide the complete results of the same experiment for networks based on AlexNet topology:  conv layer N N N N N  Fraction of filters in net A  with larger spatial entropy than 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  corresponding filters in net B
 Fraction of filters in net A with  larger Nnd central moments than 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  corresponding filters in net B
 Table N: The fraction of filters in network A, (AlexNet  trained only with unoccluded examples) that has a larger  effective support than the corresponding filters in network  B (trained also with occluded examples)
The spatial support of the filters tends to be lower if the training images are  partially occluded
 In the following table we present the effective depth for  the same experiment for networks based on AlexNet topology; see Section N.N, Table N:  conv layer N N N N N  Fraction of filters in net A with  larger depth entropy than 0.NNN 0.NNN 0.NNN 0.NNN 0.NNN  corresponding filters in net B
 Table N: The fraction of filters in network A, (AlexNet  trained only with unoccluded examples) that has a larger  depth entropy than the corresponding filters in network B  (trained also with occluded examples)
When training on  partially occluded examples, more kernels play a meaningful role in the feature extraction process; see appendix A for  additional results
 As mentioned in Section N.N, in the experiments presented above, the differences in spatial support are quite  small
This is a result of the experimental settings, meant to  maintain correspondence between the filters of the two network
We also carried out a different experiment, where we  trained two AlexNet networks
The first was trained only on  unoccluded examples, while the second was trained only on  occluded examples
In this setting, there is no filter correspondence between the two networks
The following table  presents the means and the standard deviations of the Nnd  central moments in these networks
The results show that  the spatial support is indeed lower when training with occluded examples
The difference is larger and statistically  significant in the later layers
 conv layer N N N N N  (µ,σ) for non occ N.NN,N.NN N.NN,0.NN N.NN,0.0N N.NNN,0.0NN N.N0,0.0N  (µ,σ) for occ N.NN,N.NN N.NN,0.NN N.0N,0.0N N.NNN,0.00N N.NN,0.00N  Table N: A comparison of the mean and standard deviations of the second central moment between two AlexNet  networks
The first trained on unoccluded examples while  the second trained only on partially occluded training examples
The comparison is presented with respect to the  different convolution layers
The spatial support is smaller  if the training process is conducted on occluded examples
 NN0    A.N
The group sparsity masks used in the experi- ments
 In the following figure, we present an example of the  group sparsity masks used with the LeNet network which  trained with the CIFAR data sets experiments described in  Section N:  Figure N: Group sparsity masks, similar to those used in our  experiments
White represents unaffected weights, while  blue represents the group of weights meant to decay together
 A.N
A note on setting the regularization strength
 Using substantial experimentation (on LeNetN), we came  to the conclusion that latter layers in the CNN benefit from  larger regularization strength values (λ)
The results cited in Table N were obtained with λ = 0.000N for the first two layers and for larger λ = 0.00N for the last layers
 NNNHydraPlus-Net: Attentive Deep Features for Pedestrian Analysis   HydraPlus-Net: Attentive Deep Features for Pedestrian Analysis  Xihui LiuN,N∗, Haiyu ZhaoN∗, Maoqing TianN, Lu ShengN  Jing ShaoN†, Shuai YiN, Junjie YanN, Xiaogang WangN  NThe Chinese University of Hong Kong NSenseTime Group Limited  shaojing@sensetime.com  Abstract  Pedestrian analysis plays a vital role in intelligent video  surveillance and is a key component for security-centric  computer vision systems
Despite that the convolutional  neural networks are remarkable in learning discriminative features from images, the learning of comprehensive  features of pedestrians for fine-grained tasks remains an  open problem
In this study, we propose a new attentionbased deep neural network, named as HydraPlus-Net (HPnet), that multi-directionally feeds the multi-level attention maps to different feature layers
The attentive deep  features learned from the proposed HP-net bring unique  advantages: (N) the model is capable of capturing multiple attentions from low-level to semantic-level, and (N)  it explores the multi-scale selectiveness of attentive features to enrich the final feature representations for a pedestrian image
We demonstrate the effectiveness and generality of the proposed HP-net for pedestrian analysis on two  tasks, i.e
pedestrian attribute recognition and person reidentification
Intensive experimental results have been provided to prove that the HP-net outperforms the state-of-theart methods on various datasets.N  N
Introduction  Pedestrian analysis is a long-lasting research topic because of the continuing demands for intelligent video  surveillance and psychological social behavior researches
 Particularly, with the explosion of researches about the  deep convolutional neural networks in recent computer vision community, a variety of applications categorized as  the pedestrian analysis, e.g
pedestrian attribute recognition,  person re-identification and etc., have received remarkable  improvements and presented potentialities for practical us∗X
Liu and H
Zhao share equal contribution
†J
Shao is the corresponding author
Nhttps://github.com/xh-liu/HydraPlus-Net  (a) (b) (c) (d)  Figure N
Pedestrian analysis needs a comprehensive feature representation from multi-levels and scales
(a) Semantic-level: attending features around local regions facilitates distinguishing persons that own similar appearances at a glance, such as “long hair”  vs
“short hair” and “long-sleeves” vs
“short-sleeves”
(b) Lowlevel: some patterns like “clothing stride” can be well captured  by low-level features rather than those in high-level
(c-d) Scales:  multi-scale attentive features benefit describing person’s characteristics, where the small-scale attention map in (c) corresponds to  the “phone” and a large-scale one in (d) for a global understanding
 age in modern surveillance system
However, the learning  of feature representation for pedestrian images, as the backbone for all those applications, still confronts critical challenges and needs profound studies
 At first, most traditional deep architectures have not extracted the detailed and localized features complementary  to the high-level global features, which are especially effective for fine-grained tasks in pedestrian analysis
For example, it is difficult to distinguish two instances if no semantic features are extracted around hair and shoulders, as  shown in Fig
N(a)
Also in Fig
N(c), the effective features  should be located within a small-scale head-shoulder region  if we want to detect the attribute “calling”
However, existing arts merely extract global features [NN, NN, N0] and are  hardly effective to location-aware semantic pattern extraction
Furthermore, it is well-known that multi-level features  NNN0  https://github.com/xh-liu/HydraPlus-Net   aid diverse vision tasks [NN, N]
Similar phenomenon has  also happened in the pedestrian analysis, such as the pattern  “clothing stride” shown in Fig
N(b) should be inferred from  low-level features, while the attribute “gender” in Fig
N(d)  is judged by semantic understanding of the whole pedestrian image
Unlike previous approaches that mainly generate the global feature representations, the proposed feature  representation encodes multiple levels of feature patterns as  well as a mixture of global and local information, and thus  it owns a potential capability for multi-level pedestrian attribute recognition and person re-identification
 Facing the drawbacks of recent methods for pedestrian analysis, we try to tackle the general feature learning  paradigm for pedestrian analysis by a multi-directional network, called HydraPlus-Net, which is proposed to better exploit the global and local contents with multi-level feature  fusion of a single pedestrian image
Specifically, we propose a multi-directional attention (MDA) module that aggregates multiple feature layers within the attentive regions  extracted from multiple layers in the network
Since the  attention maps are extracted from different semantic layers, they naturally abstract different levels of visual patterns  of the same pedestrian image
Moreover, filtering multiple  levels of features by the same attention map results in an  effective fusion of multi-level features from a certain local  attention distribution
After applying the MDA to different  layers of the network, the multi-level attentive features are  fused together to form the final feature representation
 The proposed framework is evaluated on two representatives among the pedestrian analysis tasks, i.e
pedestrian  attribute recognition and person re-identification (ReID), in  which attribute recognition focuses on assigning a set of attribute labels to each pedestrian image while ReID aims to  associate the images of one person across multiple cameras  and/or temporal shots
Although pedestrian attribute recognition and ReID pay attention to different aspects of the  input pedestrian image, these two tasks can be solved by  learning a similar feature representation, since they are inherently correlated with similar semantic features and the  success of one task will improve the performance of the  other
Compared with existing approaches, our framework  achieves the state-of-the-art performance on most datasets
 The contributions of this work are three-fold:  (N) A HydraPlus Network (HP-net) is proposed with the  novel multi-directional attention modules to train multilevel and multi-scale attention-strengthened features for  fine-grained tasks of pedestrian analysis
 (N) The HP-net is comprehensively evaluated on pedestrian attribute recognition and person re-identification
 State-of-the-art performances have been achieved with significant improvements against the prior methods
 (N) A new large-scale pedestrian attribute dataset (PAN00K dataset) is collected with the most diverse scenes and  Attentive  Feature Net  Conv layers Incep- Incep- IncepMain Net  G A P  FC O ut pu t  Incep- Incep- IncepIncep- Incep- IncepIncep- Incep- IncepFigure N
A deep HP-Net with a Main Net (M-net) and an Attentive Feature Net (AF-net)
The AF-net comprises three multidirectional attention (MDA) modules (i.e
F(αi), i ∈ Ω)
Each MDA module includes two components: (N) attention map generation with black solid lines, and (N) attentive features by masking  the attention map to different levels of features in hot dash lines
A  global average pooling and one fully-connected layer are applied  to the concatenated features obtained from the M-net and AF-net
 the largest number of samples and instances up-to-date
The  PA-N00K dataset is more informative than the previous col- lections and helpful for various pedestrian analysis tasks
 N
Related Works  Attention models In computer vision, attention models  have been used in tasks such as image caption generation [NN], visual question answering [NN, NN] and object  detection [N]
Mnih et al
[N0] and Xiao et al
[NN] explored hard attention, in which the network attends to a  certain region of the image or feature map
Compared  to non-differentiable hard attention trained by reinforce  algorithms [NN], soft attention which weights the feature  maps is differentiable and can be trained by back propagation
Chen et al
[N] introduced an attention to the  multi-scale features, and Zagoruyko et al
[NN] exploited  attention in knowledge transfer
In this work, we design  a multi-directional attention network for better pedestrian  feature representation and apply it to both pedestrian attribute recognition and re-identification tasks
To the best  of our knowledge, this is the first work to adopt attention  idea in the aforementioned two tasks
 Pedestrian attribute recognition Pedestrian attribute has  been an important research topic recently, due to its  prospective application in video surveillance systems
Convolutional neural networks have achieved great success in  pedestrian attribute recognition
Sudowe et al
[NN] and  Li et al
[NN] proposed that jointly training multiple attributes can improve the performance of attribute recognition
Previous work also investigated the effectiveness of  NNN    (a)  (b)  M -n  e t   la ye  rs A  F -n  e t   la ye  rs  αN  N N N  Figure N
An example of the multi-directional attention (MDA)  module F(αN)
The M-net in (a) is presented with simplified lay- ers (indexed by i ∈ Ω) representing the output layers of three inception blocks
The attention map αN is generated from  block N and multi-directionally masks three adjacent blocks
 utilizing pose and body part information in attribute recognition
Zhang et al
[NN] proposed a pose aligned network  to capture the pose-normalized appearance differences
Different from previous works, we propose an attention structure which can attend to important areas and align body  parts without prior knowledge on body parts or poselets
 Person re-identification Feature extraction and metric  learning [NN, NN] are two main components for person reidentification
The success of deep learning in image classification inspired lots of studies on person ReID [N, NN,  N0, NN, NN, NN, NN, NN, NN]
The filter pairing neural network (FPNN) proposed by Li et al
[NN] jointly handles misalignment, transforms, occlusions and background clutters
 Cheng et al
[N] presented a multi-channel parts-based CNN  to learn body features from the input image
In this paper,  we mainly target on feature extraction and cosine distance  is directly adopted for metric learning
Moreover, attention  masks are utilized in our pipeline to locate discriminative  regions which can better describe each individual
 N
HydraPlus-Net Architecture  The design of the HydraPlus networkN (HP-net) is motivated by the necessity to extract multi-scale features from  multiple levels, so as not only to capture both global and local contents of the input image but also assemble its features  with different levels of semantics
As shown in Fig
N, the  HP-net consists of two parts, one is the Main Net (M-net)  that is a plain CNN architecture, the other is the Attentive  Feature Net (AF-net) including multiple branches of multidirectional attention (MDA) modules applied to different  semantic feature levels
The AF-net shares the same basic convolution architectures as the M-net except the added  NHydra is a water monster with nine heads
In this work, the network  consists of a N-branch in AF-net (N MDA modules and N attention subbranches for each MDA), plus an M-net, so it is called HydraPlus Net
 (a)  (b)  (c)  αN αN αN  αN N  αN N  αN N  αN N  αN N  αN N  αN N  αN N  Figure N
The diversity and semantic selectiveness of attention  maps
(a) The attention maps generated from three adjacent blocks  respond to visual patterns in different scales and levels, in which  αN owns the power to highlight semantic patterns in object level
 (b-c) Different channels in attention maps capture different visual  patterns related to body parts, salient objects and background
 MDA modules
Their outputs are concatenated and then  fused by global average pooling (GAP) and fully connected  (FC) layers
The final output can be projected as the attribute logits for attribute recognition or feature vectors for  re-identification
In principle, any kind of CNN structure  can be applied to construct the HP-net
But in our implementation, we design a new end-to-end model based on  inception vN architecture [N0] because of its excellent  performance in general image-related recognition tasks
As  sketched in Fig
N, each network of the proposed framework contains several low-level convolutional layers and is  followed by three inception blocks
This model seems  simple but is non trivial as it achieves all required abilities  and brings them together to boost the recognition capability
 N.N
Attentive Feature Network  The Attentive Feature Network (AF-net) in Fig
N  comprises three branches of sub-networks augmented by  the multi-directional attention (MDA) modules, namely  F(αi), i ∈ Ω = {N, N, N}, where αi are the attention maps generated from the output features of the inception  block i marked by black solid lines, and are applied to the  output of the kth block (k ∈ Ω = {N, N, N}) in hot dash lines
For each MDA module, there is one link of attention generation and three links for attentive feature construction
Different MDA modules have their attention maps generated  from different inception blocks and then been multiplied to  feature maps of different levels to produce multi-level atNNN    tentive features
An example of a MDA module F(αN) is shown in Fig
N
The main stream network of each AF-net  branch is initialized exactly as the M-net, and thus the attention maps approximately distill similar features as what  the M-net extracts
 It is well known that the attention maps learned from  different blocks vary in scale and detailed structure
For example, the attention maps from higher blocks (e.g
αN) tend  to be coarser but usually figure out the semantic regions like  αN highlights the handbag in Fig
N(a)
But those from lower  blocks (e.g
αN) often respond to local feature patterns and  can catch detailed local information like edges and textures,  just as the examples visualized in Fig
N(a)
Therefore, if  fusing the multi-level attentive features by MDA modules,  we enable the output features to gather information across  different levels of semantics, thus offering more selective  representations
Moreover, the MDA module also differs  from the traditional attention-based models [NN, NN] that  push the attention map back to the same block, and it extends this mechanism by applying the attention maps to adjacent blocks, as shown in lines with varying hot colors in  Fig
N
Applying one single attention map to multiple blocks  naturally let the fused features encode multi-level information within the same spatial distribution which is illustrated  in Section N.N
 More specifically, for a given inception block i, its  output feature map is denoted as Fi ∈ RC×H×W with the width W , height H and C channels
The attention map αi  is generated from Fi by a N × N conv layer with BN and ReLU activation function afterwards, noted as  αi = gatt(F i;θi  att ) ∈ RL×H×W , (N)  where L means the channels of the attention map
In this  paper, we fix L = N for both tasks
And the attentive fea- ture map to the inception block k is an element-wise  multiplication  F̃ i,k l = α  i l ◦ F  k, l ∈ {N, 


, L}
(N)  Each attentive feature map F̃ i,k l is then passed through the  following blocks thereafter, and at the end of MDA module we concatenate the L attentive feature maps as the final  feature representation
We visualized the detailed structure  of an MDA module F(αN) in Fig
N
αN is generated from the inception block N and then applied to feature maps indexed by k ∈ Ω = {N, N, N}, as shown in Fig
N(b)
Note that we prefer the ReLU activation function rather than the  sigmoid function to constrain the attention maps so that  the attentive regions receive more weights, and the contrast  of the attention map is enlarged
More examples and analyses are shown in Sec
N to illustrate the MDA’s effectiveness
 N.N
HP-Net Stage-wise Training  We train the HP-net in a stage-wise fashion
Initially,  a plain M-net is trained to learn the fundamental pedes(a) (b)  F N  αN N  F N  αN N  αN N  αN N  F̃ N,N N  F̃ N,N N  F̃ N,N N  F̃ N,N N  Figure N
Examples of multi-directional attentive features
(a) The  identification of low-level attributes like “upper-clothing pattern”  requires the low-level attention connections, for example, applying  αNN to extract F̃ N,N  N indicating textures onto the T-shirt
(b) But the  semantic or object-level attributes like “phone” require high-level  attention connections such as applying αNN to extract F̃ N,N  N for the  detection of the phone near the ear
 trian features
Then the M-net is duplicated three times  to construct the AF-net with adjacent MDA modules, each  of which following the framework shown in Fig
N
Since  each MDA module consists of three branches where the attention map masks adjacent inception blocks, thus in  each branch we only fine-tune the blocks after the attentionoperated block
After separately fine-tuning three MDA  modules in AF-net, we fix both the M-net and AF-net and  train the remaining GAP and FC layers
The output layer  to minimize losses defined by different tasks, in which the  cross-entropy loss Latt is applied for pedestrian attribute recognition, and softmax loss for person re-identification
 N
Ablation Study On Attentive Deep Features  The advantages of HP-net are its capability of learning  both multi-level attentions and multi-scale attentive features  for a comprehensive feature representation of a pedestrian  image
To better understand these advantages, we analyze  the effectiveness of each component in the network with  qualitative visualization and quantitative comparisons
 N.N
Multi-level Attention Maps  The level of attention maps
The compared exemplars  of attention maps from three layers (i.e
the outputs of the  inception blocks i ∈ Ω = {N, N, N}) are shown in Fig
N(a)
We observe that the attention map from earlier  layer i = N prefers grasping low-level patterns like edges or textures, while those from higher layers i = N or N are more likely to capture semantic visual patterns corresponding to  a specific object (e.g
handbag) or human identity
 The quantity of attention maps
Most previous studies [NN, NN] merely demonstrated the effectiveness of an  attention-based model with a limited number of channels  NNN    NN.N  NN.N  NN.N  NN.N  NN.N  NN.N  (a) (b) (c)  α  F  Figure N
Results of discarding partial attention modules or connections compared with that of the complete network fed with all MDA  modules on VIPeR dataset
The N× N boxes in (a) indicates the indices of different attention maps and their mask directions
The hollow white in each box means the corresponding attentions or directional links have been cut down
Bars are plot by the Top-N accuracy
(b) and  (c) present the qualitative results by the complete network compared with two kinds of partial networks in (a)
For a query image shown in  the middle, Top-N results are shown aside with the correct marked by green and the false alarm are red
Best viewed in color
 (i.e., L = N or N)
In this study, we explore the potential performance of an attention model with increasing channels  in both diversity and consistency
 N) Attention Diversity
Fig
N(b) shows two images of one  single pedestrian captured by two cameras, alongside with  L = N attention channels of αN are presented
From the raw image, it is hard to distinguish these images due to the large  intra-class variations from cluttered background, varying illumination, viewpoint changes and etc
Nevertheless, benefited from the discriminative localization ability of multiple  attention channels from one level, the entire features can  be captured separately with respect to different attentive areas
Compared to a single attention channel, the diversity of  multiple attention channels enriches the feature representations and improves the chance to accurately analyze both  the attributes and identity of one pedestrian
 N) Attention Consistency
We also observe that one attention map generated upon different input samples might  be similarly distributed in spatial domain since they highlight the same semantic parts of a pedestrian
Notwithstanding different pedestrians, shown in Fig
N(b-c), their attention channels αN N  capture the head-shoulder regions and the  channels αN N  infer the background area
Since the consistent  attention maps are usually linked to salient objects, the selectiveness of these attention maps is thus essential on identifying the pedestrian
 N.N
Multi-Directional Attentive Features  Apart from the benefits of the multi-level attention maps,  the effectiveness of the proposed method also lies on the  novel transition scheme
For instance, the pedestrian in  Fig
N(b) holds a phone near the right ear that cannot be  directly captured neither by the feature map FN in a lower  layer i = N, nor by the naı̈ve attentive feature maps F̃N,N N  
 Surprisingly, with the help of a higher level attention map  αN N , the attentive feature map F̃  N,N N  can precisely attend the  region around the phone
On the other hand, the high-level  attention map αN N  might not be able to capture lower-level  visual patterns related to attributes like “upper-clothing pattern”
For example, the attention map αN N  shown in Fig
N(a)  does not point out the local patterns onto the T-shirt, while  on the contrary, the low-level attention map αN N  filters out  F̃ N,N N  that typically reflects these texture patterns
 N.N
Component Analysis  We also demonstrate the cases when dropping partial  attention modules or connections in comparison with the  complete AF-net
As an example, the person ReID on  VIPeR dataset [N] with six typical configurations are compared in Fig
N(a)
The orange bar shown in its bottom indicates the performance with the complete AF-net, while the  yellow one is the M-net which is considered as the baseline  model without the attention modules
The rest four bars are  configured as:  (N) Blue: naı̈ve attention modules per branch
In each  branch of AF-net, a naı̈ve attention module is applied to  extract the attentive features F̃i,i, i ∈ Ω = {N, N, N}
 (N) Cyan: discarding the middle-level attention maps and  attentive features
We discard both the attention maps and  attentive features of the block Nnd, i.e
prune the modules that produce F̃N,k and F̃i,N, ∀i, k ∈ {N, N, N}
 (N) Purple: pruning one branch
It discards the first MDA  module F(αN)
 (N) Light purple: pruning two branches
The first two MDA  modules F(αN) and F(αN) are discarded
 The results clearly prove that either cutting down the  number of MDA modules or connections within this module will pull down the performance, and it is reasonable  that these attention components complement each other to  generate the comprehensive feature representation and thus  gain a higher accuracy
Two examples with Top-N identification results shown in Fig
N(b-c) further demonstrate the  NNN    0.N  0.N  0.N  0.N  0.N  N  Fe m al e  C le rk  B oo ts  Lo ng H ai r  Je an s  Ti gh tT ro us er s  C us to m er  Lo ng Tr ou se rs  Pu si ng V es t  D re ss Sh irt  Le at he rS ho es  C al lin g  C ar ry in gb yH an d  H an dT ru nk  C lo th Sh oe s  C ot to n  G la ss es  Ja ck et  A ge Le ss NN  Sh or tS le ev e  SS B ag  Su it- U p  Sp or tS ho es  B ac kp ac k  Sh or tS ki rt  B la ck H ai r B ox  H an dB ag  TS hi rt H at  O th er A ttc hm en t  Pl as tic B ag  A ge NN -N N  Pu lli ng  A ge NN -N 0  Sw ea te r  Ti gh t  B al dH ea d  C as ua lS ho es  B od yF at  H ol di ng  G at he rin g  M uf fle r  Pa pe rB ag  C ar ry in gb yA rm  B od yT hi n  B od yN or m al  Ta lk in g Sk irt  DeepMar  HP-net  m A  Figure N
Mean accuracy scores for all attributes of RAP dataset by HP-net and DeepMar marked with red and blue bars respectively
The  bars are sorted according to the larger mAs between two methods
The HP-net outperforms DeepMar especially on “glasses” and “hat”  which have the exemplar sample listed aside
The sample in orange provides a failure case of predicting the attribute “talking”
 PETA RAP PA-N00K  # scene - NN NNN  # sample NN,000 NN,NNN N00,000  # attribute NN (+N) NN (+N) NN  # tracklet - - NN,N0N  resolution from NN× NN  to NNN× NNN  from NN× NN  to NNN× NNN  from N0× N00  to NNN× NNN  Table N
Comparison of the proposed PA-N00K dataset with ex- isting datasets
The attribute number listed in the parentheses indicates the multi-class attributes while the one outside means the  number of binary attributes
 effectiveness and indispensability of each component of the  entire AF-net
The complete network is superior to both  the multi-level naı̈ve attention modules (Fig
N(b)) and the  single MDA module (Fig
N(c))
 N
Pedestrian Attribute Recognition  We evaluate our HP-net on two public datasets comparing the state-of-the-art methods
In addition, we further propose a new large-scale pedestrian attribute dataset PA-N00K with larger scene diversities and amount of samples
 N.N
PA-N00K Dataset  Most of existing public pedestrian attribute datasets [N,  NN] only contain a limited number of scenes (at most NN) with no more than N0, 000 annotated pedestrians
To fur- ther evaluate the generality of the proposed method, we  construct a new large-scale pedestrian attribute (PA) dataset  named as PA-N00K with N00, 000 pedestrian images from NNN scenes, and therefore offer a superiorly comprehen- sive dataset for pedestrian attribute recognition
To our best  knowledge, it is to-date the largest dataset for pedestrian attribute recognition
We compare our PA-N00K dataset with  the other two publicly available datasets in Table N
 The samples of one person in PETA dataset [N] are  only annotated once by randomly picking one exemplar image, and therefore share the same annotated attributes even  though some of them might not be visible and some other  attributes are ignored
Another limitation is that the random partition of the training, validation and test sets are  conducted in the whole dataset with no consideration of  the person’s identity across images, which leads to unfair  image assignment of one person in different sets
In RAP  dataset [NN], the high-quality indoor images with controlled  lighting conditions contain much lower variances than those  under unconstrained real scenarios
Moreover, some attributes are even highly imbalanced
 PA-N00K dataset surpasses the the previous datasets both in quantity and diversity, as shown in Table N
We define NN commonly used attributes including global attributes like gender, age, and object level attributes like handbag,  phone, upper-clothing and etc
The PA-N00K dataset was constructed by images captured from real outdoor surveillance cameras which is more challenging
Different from  the existing datasets, the images were collected by sampling  the frames from the surveillance videos, which makes some  future applications available, such as video-based attribute  recognition and frame-level pedestrian quality estimation
 We annotated all pedestrians in each image and abandoned  pedestrians with blurred motion or extreme low resolution  (lower than N0× N00)
The whole dataset is randomly split into training, validation and test sets with a ratio of N : N : N
The samples of one person was extracted along its tracklets  in a surveillance video, and they are randomly assigned to  one of these sets, in which case PA-N00K dataset ensures the attributes are learned independent of the person’s identity
All these sets are guaranteed to have positives and negatives of the NN attributes
Note that this partition based on tracklets is fairer than the partition that randomly shuffles  the images in PETA dataset
 In the following experiments, we employ five evaluation criteriaN including a label-based metric mean accuracy  (mA), and four instance-based metrics, i.e
accuracy, precision, recall and FN-score
To address the issue of imbalanced classes, we adopt a weighted cross-entropy loss funcNCriterion definitions are the same as those in [NN]
 NNN    Dataset RAP PETA PA-N00K  Method ELFmm  FCNmm  FCNmm ACN  DeepMar  Mnet  HPnet  ELFmm  FCNmm  FCNmm ACN  DeepMar  Mnet  HPnet  DeepMar  Mnet  HPnet  mA NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.N N0.NN NN.NN NN.N NN.N NN.NN  Accu NN.NN NN.NN NN.NN NN.NN NN.0N dNN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.0N NN.NN NN.NN N0.NN N0.NN NN.NN  Prec NN.NN NN.NN NN.NN N0.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.0N NN.0N NN.NN NN.NN NN.NN NN.NN NN.N NN.NN  Recall NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.N NN.NN N0.NN NN.0N NN.0N  FN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.0N NN.NN NN NN.NN NN.NN NN.NN NN.NN NN.0N NN.NN NN.NN NN.NN  Table N
Quantitative results(%) on three datasets for pedestrian attribute recognition, compared with previous benchmark methods
 Backpack Short Sleeves Upperbody Splice  0.N0 N Glasses  DeepMar M-net HP-net  0.N0 N Female Side Long Coat  0.N0 N  Long Sleeves Long Coat Trousers  Female 0.N0 N  Upperclothing Pattern Trousers  Hat Side  0.N0 N  Handcarry Backpack Trousers  Back 0.N0 N  Glasses Upperclothing Pattern Shorts  Side  Figure N
Comparison results between DeepMar, M-net and HPnet on partial ground truth attributes annotated for the given examples
Different colors represent different methods
Bars are plot  by the prediction probabilities
 tion introduced by [NN]
 N.N
Comparison with the Prior Arts  We quantitatively and qualitatively compare the performance of the proposed method with the previous state-ofthe-art methods on the previously mentioned three datasets
 The following comparisons keep the same settings as the  prior arts on different datasets respectively
 Quantitative Evaluation
We list the results of each  method on RAP, PETA and PA-N00K datasets in Table N
Six reference methods are selected to be compared with the  proposed model
The first three models are based on SVM  classifier with hand-crafted features (ELF-mm [N, NN]) and  deep-learned features (FCN-mm and FCN-mm) respectively
 ACN [NN] and DeepMar [NN] are CNN models that achieved  good performances by joint training the multiple attributes
 The baseline M-net and the proposed final model significantly outperform the state-of-the-art methods
We are also  interested in the performance of each attribute
The bar in  Fig
N shows the overlapped histograms of the mean accuracy (mA) for all attributes by DeepMar and HP-net
The  bars are sorted in descending order according to the larger  mA between these methods at one attribute
We find that the  envelope superimposing the histogram is always supported  by the HP-net with prominent performance gain against  DeepMar, and is extremely superior on attributes which require fine-grained localization, like glasses and handbags
 Qualitative Evaluation
Besides the quantitative results in  Dataset Market-NN0N [NN] CUHK0N [NN] VIPeR [N]  # identities NN0N NNN0 NNN  # images NNNNN NNNNN NNNN  # cameras N N N  # training IDs NN0 NNN0 NNN  # test IDs NNN N00 NNN  # probe images NNNN N00 NNN  # gallery images NNNNN N00 NNN  Table N
The specifications of three evaluated ReID datasets
 Table N, we also conduct qualitative evaluations for exemplar pedestrian images
As shown in the examples in Fig
N,  sample images from RAP dataset and their attention maps  demonstrate the localizability of the learned attention maps
 Especially in the first image, the attention map highlights  two bags simultaneously
We also notice a failure case on  the attribute “talking” which is irrelevant to a certain region  but requires a global understanding of the whole image
 For the PA-N00K dataset, we show attribute recognition results for several exemplar pedestrian images in Fig
N
 The bars indicate the prediction probabilities
Although  the probabilities of one attribute do not directly imply its  actual recognition confidences, they uncover the discriminative power of different methods as the lower probability  corresponds to ambiguity or difficulty in correctly predicting one attribute
The proposed HP-net reliably predicts  these attributes with region-based saliency, like “glasses”,  “back-pack”, “hat”, “shorts” and “handcarry”
 N
Person Re-identification  Referring to the person re-identification, we also evaluate the HP-net with several reference methods on three publicly available datasets, quantitatively and qualitatively
 N.N
Datasets and Setups  The proposed approach is evaluated on three publicly  standard datasets, including CUHK0N [NN], VIPeR [N], and  Market-NN0N [NN]
A summary about the statistical information of the three datasets are listed in Table N
For the  Market-NN0N dataset, the same data separation strategy is  used as [NN]
For the other datasets, the training, validation  and testing images are sampled based on the strategy introduced in [N0]
The training and validation identities are  guaranteed to have no overlaps with the testing ones for all  NNN    CUHK0N Top-N Top-N Top-N0 Top-N0  PersonNet [NN] NN.N NN.N NN.N NN.N  JSTL [N0] NN.N - - Joint ReID [N] NN.N - - LOMO-XQDA [NN] NN.N - - M-net NN.N NN.N NN.N NN.N  HP-net NN.N NN.N NN.N NN.N  VIPeR Top-N Top-N Top-N0 Top-N0  NFST [NN] NN.N NN.N N0.N NN.0  SCSP [N] NN.N NN.N NN.N NN.N  GOG+XQDA [NN] NN.N NN.N NN.N NN.N  TCP [N] NN.N NN.N NN.N NN.N  M-net NN.N NN.N NN.N NN.N  HP-net NN.N NN.N NN.0 NN.N  Market-NN0N Top-N Top-N Top-N0 Top-N0  WARCA-L [NN] NN.N NN.N NN.0 NN.0  LOMO+CN [NN] NN.N - - S-CNN [NN] NN.N - - BoW-best [NN] NN.N NN.N NN.N NN.0  M-net NN.N NN.N NN.N NN.0  HP-net NN.N NN.N NN.N NN.N  Table N
Experimental results(%) of the proposed HP-net and other  comparisons on three datasets
The CMC Top-N-N-N0-N0 accuracies are reported
The Top-N accuracies of two best performing  approaches are marked in bold
 evaluated datasets
Following the pipeline of JSTL [N0], all  the training samples are combined together to train a single  ReID model from scratch, which can be directly evaluated  on all the testing datasets
 The widely applied cumulative match curve (CMC) metric is adopted for quantitative evaluation
While in the  matching process, the cosine distance is computed between  each query image and all the gallery images, and the ranked  gallery list is returned
All the experiments are conducted  under the setting of single query and the testing procedure  is repeated N00 times to get an average result
 N.N
Performance Comparisons  Quantitative Evaluation
As shown in Table N, the proposed approach is compared with a series of the deep neural networks like PersonNet [NN], the multi-domain CNN  JSTL [N0], the Joint ReID method [N], and the horizontal  occurrence model LOMO-XQDA [NN] on CUHK0N [NN]
 As for the VIPeR [N] dataset, the null space semisupervised learning method NFST [NN], the similarity learning method SCSP [N], the hierarchical Gaussian model  GOG+XQDA [NN], and the triplet loss model TCP [N] are  selected for comparison
The Market-NN0N [NN] dataset is  also evaluated with the metric learning WARCA-L [NN],  a novel Siamese LSTM architecture LOMO+CN [NN], the  Siamese CNN with learnable gate S-CNN [NN], and the bag  of words model BoW-best [NN]
 Besides the results of the proposed approach with complete HP-net, the results of the M-net are also listed as the  baseline for the three datasets
From Table N, we can obProbe TopN TopN TopN TopN TopN  Figure N
Comparison results between HP-net and M-net
For the  probe images, the Top-N retrieval results of HP-net together with  attention maps are shown in the first row, and the results of M-net  are shown in the second row
 serve that the proposed approach achieves the Top-N accuracies of NN.N%, NN.N% and NN.N% on the CUHK0N, ViPeR and Market-NN0N datasets, respectively, and it achieves the  state-of-the-art performance on all the three datasets
Moreover, even though the M-net can achieve quite satisfactory  results on all datasets, the proposed pipeline can further improve the Top-N accuracies by N.N%, N.0%, and N.N% for each dataset, respectively
 Qualitative Evaluation
To highlight the performance of  the proposed method on extracting localized semantic features, one query image together with its Top-N gallery results by the proposed method and the M-net are visualized  in Fig
N
We observe that the proposed approach improves  the rankings of the M-net and gets the correct results
By visualizing the attention maps from HP-Net of the query images and the Top-N gallery images of both methods, we observe that the proposed attention modules can successfully  locate the T-shirt patterns, in which the fine-grained features  are extracted and discriminatingly identify the query person  against the other identities with similar dressing
 N
Conclusion  In this paper, we present a new deep architecture called  HydraPlus network with a novel multi-directional attention  mechanism
Extensive ablation studies and experimental  evaluations have manifested the effectiveness of the HP-net  to learn multi-level and multi-scale attentive feature representations for fine-grained tasks in pedestrian analysis, like  pedestrian attribute recognition and person re-identification
 In the end, a new large-scale attribute dataset PA-N00K is introduced to facilitate various pedestrian analysis tasks
 Acknowledgement This work is supported in part by  SenseTime Group Limited, in part by the General Research  Fund through the Research Grants Council of Hong  Kong under Grants CUHKNNNNNNNN, CUHKNNN0NNNN,  CUHKNNN0NNNN, CUHKNNNNNN, CUHKNNN0N0NN,  CUHKNNN0NNNN, and in part by the Hong Kong Innovation  and Technology Support Programme Grant ITS/NNN/NNFX
 NNN    References  [N] E
Ahmed, M
Jones, and T
K
Marks
An improved deep  learning architecture for person re-identification
In CVPR,  N0NN
N  [N] J
Ba, V
Mnih, and K
Kavukcuoglu
Multiple object recognition with visual attention
arXiv preprint arXiv:NNNN.NNNN,  N0NN
N  [N] D
Chen, Z
Yuan, B
Chen, and N
Zheng
Similarity learning with spatial constraints for person re-identification
In  CVPR, N0NN
N  [N] L.-C
Chen, Y
Yang, J
Wang, W
Xu, and A
L
Yuille
Attention to scale: Scale-aware semantic image segmentation
 In CVPR, N0NN
N  [N] D
Cheng, Y
Gong, S
Zhou, J
Wang, and N
Zheng
Person re-identification by multi-channel parts-based cnn with  improved triplet loss function
In CVPR, N0NN
N, N  [N] M
Cornia, L
Baraldi, G
Serra, and R
Cucchiara
A deep  multi-level network for saliency prediction
arXiv preprint  arXiv:NN0N.0N0NN, N0NN
N  [N] Y
Deng, P
Luo, C
C
Loy, and X
Tang
Pedestrian attribute  recognition at far distance
In Proceedings of the NNnd ACM  international conference on Multimedia, N0NN
N  [N] D
Gray, S
Brennan, and H
Tao
Evaluating appearance  models for recognition, reacquisition, and tracking
In Proc
 IEEE International Workshop on Performance Evaluation  for Tracking and Surveillance (PETS), N00N
N, N, N  [N] D
Gray and H
Tao
Viewpoint invariant pedestrian recognition with an ensemble of localized features
In ECCV, N00N
 N  [N0] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] C
Jose and F
Fleuret
Scalable metric learning via weighted  approximate rank component analysis
arXiv preprint  arXiv:NN0N.00NN0, N0NN
N  [NN] M
Koestinger, M
Hirzer, P
Wohlhart, P
M
Roth, and  H
Bischof
Large scale metric learning from equivalence  constraints
In CVPR, N0NN
N  [NN] D
Li, X
Chen, and K
Huang
Multi-attribute learning for  pedestrian attribute recognition in surveillance scenarios
In  Pattern Recognition (ACPR), N0NN Nrd IAPR Asian Conference on, N0NN
N, N, N  [NN] D
Li, Z
Zhang, X
Chen, H
Ling, and K
Huang
A richly  annotated dataset for pedestrian attribute recognition
arXiv  preprint arXiv:NN0N.0N0NN, N0NN
N  [NN] S
Li, T
Xiao, H
Li, B
Zhou, D
Yue, and X
Wang
Person  search with natural language description
In CVPR, N0NN
N  [NN] W
Li, R
Zhao, T
Xiao, and X
Wang
Deepreid: Deep filter  pairing neural network for person re-identification
In CVPR,  N0NN
N, N, N  [NN] S
Liao, Y
Hu, X
Zhu, and S
Z
Li
Person re-identification  by local maximal occurrence representation and metric  learning
In CVPR, N0NN
N, N  [NN] J
Lu, J
Yang, D
Batra, and D
Parikh
Hierarchical  question-image co-attention for visual question answering
 In NIPS, N0NN
N  [NN] T
Matsukawa, T
Okabe, E
Suzuki, and Y
Sato
Hierarchical gaussian descriptor for person re-identification
In CVPR,  N0NN
N  [N0] V
Mnih, N
Heess, A
Graves, et al
Recurrent models of  visual attention
In NIPS, N0NN
N  [NN] A
Newell, K
Yang, and J
Deng
Stacked hourglass networks for human pose estimation
In ECCV, N0NN
N, N  [NN] B
Prosser, W.-S
Zheng, S
Gong, T
Xiang, and Q
Mary
 Person re-identification by support vector ranking
In BMVC,  N0N0
N  [NN] C
Su, S
Zhang, J
Xing, W
Gao, and Q
Tian
Deep  attributes driven multi-camera person re-identification
In  ECCV, N0NN
N  [NN] P
Sudowe, H
Spitzer, and B
Leibe
Person attribute recognition with a jointly-trained holistic cnn model
In CVPR,  N0NN
N, N, N  [NN] E
Ustinova, Y
Ganin, and V
Lempitsky
Multiregion bilinear convolutional neural networks for person reidentification
arXiv preprint arXiv:NNNN.0NN00, N0NN
N  [NN] R
R
Varior, M
Haloi, and G
Wang
Gated siamese  convolutional neural network architecture for human reidentification
In ECCV, N0NN
N, N  [NN] R
R
Varior, B
Shuai, J
Lu, D
Xu, and G
Wang
A  siamese long short-term memory architecture for human reidentification
In ECCV, N0NN
N  [NN] R
J
Williams
Simple statistical gradient-following algorithms for connectionist reinforcement learning
Machine  learning, N(N-N):NNN–NNN, NNNN
N  [NN] L
Wu, C
Shen, and A
v
d
Hengel
Personnet: person  re-identification with deep convolutional neural networks
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N, N  [N0] T
Xiao, H
Li, W
Ouyang, and X
Wang
Learning deep feature representations with domain guided dropout for person  re-identification
In CVPR, N0NN
N, N, N, N  [NN] T
Xiao, S
Li, B
Wang, L
Lin, and X
Wang
Joint detection and identification feature learning for person search
In  CVPR, N0NN
N  [NN] T
Xiao, Y
Xu, K
Yang, J
Zhang, Y
Peng, and Z
Zhang
 The application of two-level attention models in deep convolutional neural network for fine-grained image classification
 In CVPR, N0NN
N  [NN] H
Xu and K
Saenko
Ask, attend and answer: Exploring  question-guided spatial attention for visual question answering
In ECCV, N0NN
N  [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
C
Courville, R
Salakhutdinov, R
S
Zemel, and Y
Bengio
Show, attend and tell:  Neural image caption generation with visual attention
In  ICML, N0NN
N, N  [NN] S
Zagoruyko and N
Komodakis
Paying more attention to attention: Improving the performance of convolutional neural networks via attention transfer
arXiv preprint  arXiv:NNNN.0NNNN, N0NN
N  [NN] L
Zhang, T
Xiang, and S
Gong
Learning a discriminative null space for person re-identification
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [NN] N
Zhang, M
Paluri, M
Ranzato, T
Darrell, and L
Bourdev
 Panda: Pose aligned networks for deep attribute modeling
In  CVPR, N0NN
N  NNN    [NN] L
Zheng, L
Shen, L
Tian, S
Wang, J
Wang, and Q
Tian
 Scalable person re-identification: A benchmark
In ICCV,  N0NN
N, N  NNNDeep Determinantal Point Process for Large-Scale Multi-Label Classification   Deep Determinantal Point Process for Large-Scale Multi-Label Classification  Pengtao Xie*†, Ruslan Salakhutdinov*, Luntian Mou§ and Eric P
Xing†  *Machine Learning Department, Carnegie Mellon University, USA †Petuum Inc
 §Beijing Key Laboratory of Traffic Engineering, Beijing University of Technology, China {pengtaox,rsalakhu}@cs.cmu.edu, ltmou@bjut.edu.cn, eric.xing@petuum.com  Abstract  We study large-scale multi-label classification (MLC) on  two recently released datasets: Youtube-NM and Open Images that contain millions of data instances and thousands  of classes
The unprecedented problem scale poses great  challenges for MLC
First, finding out the correct label  subset out of exponentially many choices incurs substantial ambiguity and uncertainty
Second, the large data-size  and class-size entail considerable computational cost
To  address the first challenge, we investigate two strategies:  capturing label-correlations from the training data and incorporating label co-occurrence relations obtained from external knowledge, which effectively eliminate semantically  inconsistent labels and provide contextual clues to differentiate visually ambiguous labels
Specifically, we propose a Deep Determinantal Point Process (DDPP) model  which seamlessly integrates a DPP with deep neural networks (DNNs) and supports end-to-end multi-label learning and deep representation learning
The DPP is able to  capture label-correlations of any order with a polynomial  computational cost, while the DNNs learn hierarchical features of images/videos and capture the dependency between  input data and labels
To incorporate external knowledge  about label co-occurrence relations, we impose relational  regularization over the kernel matrix in DDPP
To address  the second challenge, we study an efficient low-rank kernel  learning algorithm based on inducing point methods
Experiments on the two datasets demonstrate the efficacy and  efficiency of the proposed methods
 N
Introduction  Recently two large-scale multi-label datasets have been  released: YouTube-NM [N] and Open Images [NN]
The  YouTube-NM dataset contains about N million videos, each  associated with multiple labels coming from NN00 classes
 These videos are 0.N million hours long and contain N.N billion frames
The Open Images dataset contains about N  DDPP  …… Sky  Ocean  Tiger  Label Embeddings  Sky Ocean  Selected Labels  Sun  Cloud People  Sky  Table  Must/cannot Links   Cloud  Sofa  Tiger Ocean  Figure N
DDPP for multi-label classification
The inputs of DDPP in- clude the image (or video), the embedding vectors of labels and (optional)  must/cannot-links regarding label co-occurrence, and the output is a subset  of selected labels
DDPP captures the correlation among labels using DPP,  characterizes the dependency between the image and labels using DNN,  and incorporates the must/cannot-links via relational regularization
 million images, each annotated with multiple class labels
 The total number of unique labels is N0NN
The scale of  these two datasets is much larger than previous multi-label  datasets such as NUS-WIDE [NN], PASCAL VOCN, SUN  attributes [NN], MediamillN, in terms of both the number of  data instances and the number of classes, bringing in great  challenges for multi-label classification (MLC)
For each  data instance x, a subset of labels S ⊆ Y = {N, · · · ,K} are to be selected to annotate x, where S has exponentially many (NK) choices
The number (K) of unique labels is NN00 in Youtube-NM and N0NN in Open Images, which result in a tremendous combinatorial search space
Finding  out the correct label-subset S∗ in this space requires not only efficient algorithms that can tackle this NP-hard problem in polynomial time, but also modeling techniques that  can effectively resolve the ambiguity and uncertainty when  picking up S∗ from NK choices
In this paper, we aim at addressing these challenges
To  correctly hit S∗ from NK candidates, we investigate two strategies: (N) capturing high-order correlations among labels from the training data; (N) incorporating external prior  knowledge about label co-occurrence relations
Both strategies aim at ruling out candidate subset S where the labels are semantically and contextually inconsistent, thus effecNhttp://host.robots.ox.ac.uk/pascal/VOC/vocN0NN/ Nhttp://mulan.sourceforge.net/datasets-mlc.html  NNNN    Sky Cloud  Sheep  Painting  Sofa  Curtain Bed  Car  Figure N
Labels that are visually less distinguishable can be well discrim- inated by leveraging label correlations learned from training data (left) and  label co-occurrence relations obtained from external knowledge (right)
 tively shrinking the search space and reducing the ambiguity/uncertainty of hitting S∗
As a complement of visual features, the dependency relationship between labels provides additional clues for correct prediction
Visually hardto-distinguish labels could be well differentiated based on  label-correlations
Fig
N presents two examples
In the  left figure, the image region marked with yellow box can  be predicted either as bed or car, with similar confidence
 But compared with car, bed possesses stronger correlation  with other predicted labels including painting, curtain, sofa,  which collectively form an indoor scene
Leveraging labelcorrelation, we can correctly assign bed to the region
The  figure on the right shows a similar example, where the label  co-occurrence relations obtained from external knowledge,  such as “sky is likely to co-occur with cloud, not sheep”,  are leveraged to discriminate the two visually similar labels  cloud and sheep
 Characterizing label correlation [NN, NN, NN, NN, NN, N,  NN, NN] has been widely studied in MLC
While existing methods have demonstrated success on tens or hundreds of labels, they are less capable to deal with thousands of labels
To retain computational efficiency, many  approaches [NN, NN, NN, N, NN] limit the order of labelcorrelations to be less than three and ignore high-order ones
 High-order relations can represent semantics that is difficult to be captured in low-order relations
For instance,  considering two label sets SN = {bed, desk, sofa} and SN = {bed, desk, woods}, both of them exhibit strong second-order correlations: every two labels therein are correlated
But SN also possesses a third-order correlation: the three words collectively represent a furniture topic, while  SN does not
Several methods [NN, NN, N, NN] are proposed to capture high-order correlations, but they either make  strong assumptions that degrade classification performance  [NN, NN, NN], or incur substantial computational cost [N]
 To address this issue, we develop a Deep Determinantal Point Process (DDPP) model (Fig
N) that is not only  highly expressive to capture high-order label-correlations,  but also computationally efficient
In DPP [NN], the selection of label-subset S is based upon the volume of the parallelepiped formed by nonlinear feature vectors of labels in S [NN]
The volume is collectively determined by  the global relationship among all vectors, rather than their  pairwise relations, hence is able to characterize high-order  label dependency
The volume can be computed efficiently  as the determinant of a kernel matrix between the embedding vectors of labels, with polynomial (specifically, cubic)  complexity
Other than capturing label-correlation, DDPP  seamlessly integrates deep neural networks (DNNs) to measure the dependency between input data and labels
The  DNNs learn features from input images/videos and can be  trained in an end-to-end fashion
 Besides label-correlations derived from training data, we  can leverage the co-occurrence relations obtained from external knowledge to distinguish visually ambiguous labels
 For simplicity, we consider binary relations: two labels can  have a must-link suggesting they are very likely to co-occur,  or a cannot-link indicating that they barely co-occur
Many  knowledge sources provide such relations
For instance,  WordNet [NN] contains a lot of “A is a B” relations, such  as apple-fruit and tiger-animal
In this case, if label A is  assigned to the input data, so should be B
To incorporate  these must/cannot links, we impose relational regularization  over the kernel matrix in DDPP
According to the property  of DPP [NN], two labels represented with vectors ai and aj have a better chance to be co-selected if k(ai,aj) – the out- put of the kernel function over them – is small
If label i and j have a must-link, the regularizer encourages k(ai,aj) to be small to promote co-selection
If they share a cannotlink, k(ai,aj) is favored to be large
Lastly, to make DDPP scale to thousands of labels and  millions of data instances presenting in Youtube-NM and  Open Images, we study an efficient algorithm
In DDPP,  for each of the N training instances, one needs to evaluate the determinant and inverse of a data-dependent K×K ker- nel matrix (where K is the number of classes) with a sub- stantial O(NKN) cost
To address this problem, we inves- tigate a scalable low-rank kernel learning algorithm based  on inducing point methods [NN] which seek a low-rank parameterization of the kernel matrix using auxiliary points
 Thereafter, the Woodbury matrix identity can be applied to  compute matrix inverse, reducing the cost from O(KN) to O(MN), where M ≪ K is the number of inducing points
 The major contributions of this paper are:  • We propose a deep DPP method to perform large- scale MLC
DDPP is able to capture high-order labelcorrelations with polynomial computational complexity and facilitates end-to-end deep feature learning
 • To incorporate external knowledge regarding label co- occurrence relations, we propose to impose relational  regularization over the kernel matrix of DDPP
 • We study a low-rank kernel learning algorithm to scale DDPP to thousands of labels and millions of instances
 • Experiments on YouTube-NM and Open Images  NNN    demonstrate the effectiveness and efficiency of the proposed methods
 The rest of the paper is organized as follows
Section  N reviews related works
Section N introduces the DDPP  methods and scalable algorithms
Section N presents experimental results and Section N concludes the paper
 N
Related Works  Multi-label classification (MLC) has been widely studied in computer vision and and machine learning [N0, N, N,  NN, NN, NN, NN, N0, NN, NN, NN, NN, NN, NN, N0]
We present a  brief review from the following perspectives
 Capturing Label Correlations Many approaches have  been proposed to capture label correlations, based on  graphical models, latent space learning and class chaining
 Graphical model (GM) based approaches leverage undirected [NN, NN, NN, NN, N, NN] and directed [NN, NN] GMs  to capture the dependency structure among labels
To retain computational efficiency, many methods limit the order  of correlation to be less than three [NN, NN, NN, N, NN] or  assume label-dependency is linear [NN]
To capture highorder nonlinear correlation, Belanger and McCallum [N]  learn a neural network which takes labels as input and  produces their dependency score
This method is computationally inefficient: during training, an iterative inference procedure needs to be performed over each data  instance
In [NN, NN, NN], hypergraph spectral learning,  Bayesian network structure learning and max-margin structure learning are studied respectively to learn high-order  label-correlations
These methods lack the flexibility to perform deep visual feature learning in an end-to-end manner
 Latent space learning approaches propose to capture label  dependency in a shared hidden space
Linear subspaces  based on low-rankness [NN, NN] and conditional Bernoulli  mixture [NN] cannot capture nonlinear correlations
Nonlinear spaces induced by Restricted Boltzmann machine [NN]  and deep neural networks [NN, N, NN] make a strong assumption: the labels are independent conditioned on the latent  space, which may not hold in practice and leads to inferior  performance
Class chaining methods [NN, NN, NN] organize the classes into a linear chain and predict them in a  greedy manner: the prediction of class i depends on classes N, · · · , i−N
The overall prediction relies on the class order, which is difficult to specify
To address this issue, they propose to average the predictions over a randomly chosen set  of class permutations, which substantially increases computational cost
Another disadvantage is early prediction  errors will be propagated to subsequent classes
 Incorporating Prior Knowledge Several approaches  leverage external knowledge, such as label hierarchy [NN,  NN], label correlation statistics [NN, NN], object bounding  boxes [NN], to boost MLC performance
In [NN], a linear  projection matrix is designed to encode prior knowledge of  label-correlations
In [NN], a prior distribution is defined  to encourage labels with large cosine similarity (computed  from external Wikipedia corpora) to be co-selected
These  knowledge-incorporation methods are model-specific, and  are not applicable to DPP
We propose a new approach tailored to the property of DPP
 Scaling to Large Number of Labels To solve MLC  problems that have a large number of classes, approaches  based on label space dimension reduction (LSDR) or section [NN, N0, N, NN, N] and label hierarchy learning [N, N, NN]  have been studied
LSDR encodes the high-dimensional  label vectors into low-dimensional coding vectors
Then  predictive models are trained from instance features to  codes, which are decoded to recover the original labels
 These methods lack the flexibility to incorporate label cooccurrence knowledge since labels are transformed into a  latent space
In [N], a hierarchy of labels is learned via recursive node-partitioning, which reduces the prediction cost  to sub-linear
The limitation of this approach is its inflexibility to capture label correlations
 DPP for Computer Vision and Deep Kernel Learning  DPP has been applied for several vision tasks, including  video summarization [N0, NN], pedestrian detection [NN],  among others
Our work represents the first one using DPP  [NN] for multi-label classification
Using deep neural networks to parameterize kernel function is studied in [N0, NN]
 In our method, DNN is utilized to parameterize a conditional kernel
 N
Methods  In this section, we present the DDPP model and algorithms for parameter learning and label-subset inference
 N.N
Deep Determinantal Point Process  MLC can be formulated as a subset selection problem:  given the input data x and the K classes Y = {N, · · · ,K}, we aim at selecting a subset S ⊆ Y of labels that best describes x
This is a NP-hard problem since S has in- finitely many choices
The selection of S needs to con- sider two factors: (N) the labels in S should be highly rel- evant to x; (N) the labels should exhibit strong correlation
As stated earlier, incorporating label-correlation effectively  eliminates semantically-inconsistent labels and reduces the  search space of S 
However, it greatly complicates com- putation
One popular model that is able to simultaneously  incorporate these two factors is Conditional Random Field  (CRF) [NN], where the dependency between input data and  labels, together with correlation among labels, is characterized by potential functions
However, CRF involves a partition function which sums over exponentially many configurations and makes inference and learning extremely hard  NNN    Sky  VFN Ocean  LIDN LCN LIDN  ai aj  x  g(ai, x) g(aj , x) k(ai,aj)  k̃(ai,aj |x)  Figure N
In DDPP, the conditional kernel function k̃(ai,aj |x) is the product of a label-label kernel function k(ai,aj) and two label-input score functions g(ai, x) and g(aj , x)
k(ai,aj) is characterized by a label- correlation network (LCN) and g(ai, x) is represented by a visual feature network (VFN) and a label-input dependency network (LIDN)
 when the potential function is of high-order
We aim at  designing methods that are able to capture correlations of  any-order, but also computationally tractable
 To achieve this goal, we resort to Determinantal Point  Process (DPP) [NN], which defines a probability distribution over subsets
Given a set of items {ai} K i=N, each represented with a vector a, DPP computes a kernel matrix  L ∈ RK×K , where Lij = k(ai,aj) and k(·, ·) is a ker- nel function
Then the probability over a subset of items  indexed by S ⊆ {N, · · · ,K} can be defined as  p(S) = det(LS)  det(L+ I) (N)  where LS ≡ [Lij ]i,j∈S denotes the restriction of L to the entries indexed by elements of S and det(·) denotes the determinant of a matrix and I is an identity matrix
The  determinant enables DPP to capture the high-order labeldependency
To understand this, we first present the geometry interpretation of det(LS)
According to the kernel trick [NN], k(ai,aj) can be written as φ(ai)  ⊤φ(aj), where φ(·) is a reproducing kernel feature map [NN]
Then det(LS) is essentially the volume of the parallelepiped formed by  the vectors {φ(ai)|i ∈ S} [NN]
The size of the vol- ume is collectively determined by all these vectors in a  global way, which hence captures the high-order correlation among them
Another way to understand why determinant entails high-order correlation is to expand det(LS) as a sum of terms each involving the multiplication of |S| ker- nel function values, which hence captures label-correlations  of |S|-th order
While able to represent high-order correla- tion, DPP is computationally efficient
DPP’s normalizer  det(L+ I) can be computed in polynomial (cubic) time, as opposed to the exponential complexity in CRF
 In the context of MLC, we apply DPP to capture the correlation among labels: given the representations of K labels {ai}  K i=N (we will discuss how to learn these representations  later on), we compute the kernel matrix L and define probability over label subset according to Eq.(N)
For label-label  kernel function k(ai,aj), we parameterize it using a label correlation network (LCN) where the inputs are ai and aj and the output is a scalar indicating the correlation of the  two labels, as shown in Fig
N
As stated earlier, the selection of labels relies not only on the correlation among  labels, but also the dependency between input data and labels
We use a deep neural network (Fig
N) to define a  score function g(ai, x) to measure the dependency between input image/video x and label i
x is first fed into a visual feature network (VFN) to extract deep features, which then  together with the representation of a label are inputted into  a label-input dependency network (LIDN) to generate a dependency score
To enable end-to-end training of the VFN,  we incorporate g(ai, x) into the kernel function in DPP
On top of the kernel function k(ai,aj) measuring the correla- tion between label i and j, we define a new kernel  k̃(ai,aj |x) = g(ai, x)k(ai,aj)g(aj , x) (N)  which is conditioned on the input x
k̃(ai,aj |x) simul- taneously captures label-input dependency and label-label  correlation
Under this conditional kernel parameterized by  deep networks, we obtain a Deep DPP:  p(S|x) = det(LS(x))  det(L(x) + I) (N)  where Lij(x) = k̃(ai,aj |x)
Given training data {(xn,Sn)}  N n=N where xn is the input  and Sn is the subset of labels assigned to xn, we learn the parameters Θ of DDPP, mainly the weight and bias param- eters in DNNs, by maximizing the data likelihood  maxΘ L({(xn,Sn)} N n=N) =  N∏  n=N p(Sn|xn; Θ) (N)  Since the DPP used in our paper has learnable weight  parameters (those in the neural networks) that are adjusted  during training to best fit the output labels, it is able to  capture any type of relations among labels
This is different from the traditional non-learnable DPP [NN] where the  kernel matrix is computed on fixed feature vectors of data  points and a repulsion effect among data points is favored
 N.N
Learning Label Embeddings  In DDPP, evaluating the label-label kernel function  k(ai,aj) and label-input score function g(ai, x) both re- quire the labels to have vector representations
Inspired  from studies on word embedding [N0], we propose a label  embedding approach that learns an embedding vector ai for  each label i, by exploiting the label co-occurrence patterns in the training data
Given a training instance with labels S , we predict the existence of each label i in S based on other labels S − {i}
Let a¬i =  N |S|−N  ∑ j∈(S−{i}) aj be the average embedding of labels in S − {i}, then the probability  NNN    φ(ai) · φ(aj) ↑  det(LS) ↓  p(S) ↓  φ(ai) · φ(aj) ↓  det(LS) ↑  p(S) ↑  Increasing                          discourages the    co-selection of    label    and     k(ai,aj)  i j  φ(ai)  φ(ai)  φ(ai)  φ(aj)  φ(aj)  φ(aj)  Decreasing                          encourages the    co-selection of    label    and     k(ai,aj)  i j  Figure N
In DPP, the probability of a subset S is proportional to det(LS), which is the volume of the parallelepiped formed by vectors {φ(ai), i ∈ S}
φ(·) is the reproducing kernel feature map
As we increase the in- ner product between φ(ai) and φ(aj) (which is essentially k(ai,aj)), the volume of the parallelepiped decreases and p(S) decreases, which dis- courages the co-selection of label i and j
On the contrary, decreasing  k(ai,aj) encourages the two labels to be co-selected
 that S contains i is  p(i|S − {i}) = exp(a⊤i a¬i)∑  j∈((Y−S)∪{i}) exp(a ⊤ j a¬i)  (N)  We learn these embedding vectors by maximizing the data  likelihood ∏N  n=N  ∏ i∈Sn  p(i|Sn − {i})
Note that the label embeddings could be learned jointly with the weight parameters of VFN, LIDN and LCN
For simplicity, we performed  the learning separately while leaving joint learning to future  study
 N.N
Incorporating External Knowledge on Label Co-occurrence  There exists abundant prior knowledge regarding the cooccurrence relations among labels
In particular, we consider binary relations: if two labels have a must-link, they  co-occur with high probability; if bearing a cannot-link,  they seldom co-occur
Such relations can be obtained from  various knowledge base
Other than the WordNet example  given in Section N, one can derive such relations by computing the correlation statistics on a much larger external  dataset such as textual tags of Flickr images [NN]: two labels  are connected with a must-link if their correlation score is  high and a cannot link otherwise
 We aim to incorporate these externally-obtained cooccurrence relations to boost MLC performance on  Youtube-NM and Open Images
Specifically, we impose  relational regularization over DDPP such that labels with  must-links are encouraged to be co-selected and those with  cannot-links are penalized for co-selection
This regularization approach is designed according to the property of DPP,  which assigns larger probability mass p(S) over a label- subset S where the labels are more mutually “different” (Fig
N)
The “difference” between two labels ai and aj is measured by the kernel function k(ai,aj): the smaller k(ai,aj) is, the more different ai and aj are
To encour- age label i and j to be simultaneously selected into S , we encourage k(ai,aj) to be small to increase p(S)
To discourage simultaneous selection, k(ai,aj) is preferred to be large to decrease p(S)
Denoting M and C the set of la- bel pairs possessing must and cannot links respectively, we  define the following relation-regularized DDPP (RDDPP)  problem  maxΘ L({(xn,Sn)} N n=N)+  λ(− ∑  (i,j)∈M  k(ai,aj) + ∑  (i,j)∈C  k(ai,aj)) (N)  In the second term of the objective function, we encourage  label pair (i, j) with must-link to have smaller k(ai,aj) and those with cannot-link to have larger k(ai,aj)
 N.N
Parameter Learning  To solve the problem defined in Eq.(N) and Eq.(N), we  use gradient descent method to minimize the negative loglikelihood ∑N  n=N(− log det(LSn(xn)) + log det(L(xn) + I))
To compute the gradient of log det(L(xn) + I), one needs to invert the matrix L(xn) + I, with a complex- ity of O(KN) where K is the number of classes
When K is large, the scalability of the algorithm is very pro- hibitive
To address this issue, we leverage a low rank  kernel learning approach based on inducing points methods [NN] and structure exploiting approaches [NN]
The inducing points method introduces a set of auxiliary points  U = {um} M m=N and approximates the kernel k(ai,aj)  as k(ai,aj) ≈ V ⊤ ai,U  V −N U,UVaj ,U where Vai,U is a M dimensional vector whose m-th element is k(ai,um) and VU,U is M × M matrix in which the (m,n)-th entry is k(um,un)
Inspired by [NN], the vector Vai,U can be fur- ther approximated using interpolation: first finding two inducing points ua and ub that closely bound ai, then approximating k(ai,um) as wik(ua,um) + (N − wi)k(ub,um), where wi is a learnable weight
Let wi ∈ R  M denote a  sparse vector with only two non-zeros entries where the a-th and b-th entry are wi and N−wi respectively, then Vai,U can be approximated as w⊤i VU,U 
To this end, the approximation of k(ai,aj) is k(ai,aj) ≈ w ⊤ i VU,UV  −N U,UVU,Uwj =  w ⊤ i VU,Uwj 
Under this approximated kernel, the kernel matrix L(x) can be written as L(x) = WVU,UW ⊤  where W is a K ×M sparse matrix of which the i-th row is g(ai, x)w  ⊤ i 
According to the Woodbury matrix identity, the inverse of WVU,UW ⊤ + I can be computed as  (WVU,UW ⊤ + I)−N = I −W(V−NU,U +W  ⊤ W)−NW⊤  where the dominating computation is inverting the M ×M matrix V−NU,U + W  ⊤ W with a complexity of O(MN)
M  is typically much smaller than K, hence complexity can be reduced greatly
Since W is very sparse where each row  contains only two non-zeros, the matrix multiplication involving W can be performed very efficiently
 N.N
Inference  Given the learned model parameters, we perform MLC  by inferring the mode of the conditional probability p(S|x)
 NNN    Given the input data x, we compute the conditional kernel matrix L(x), then select the optimal subset of labels S∗ as S∗ = argmaxS p(S|x) = argmaxS log det(LS(x))
This is a NP-hard problem since the search space {S|S ⊆ Y} is exponential
To address this challenge, we use an approximate inference algorithm proposed by [NN], which (N) first  relaxes the original 0/N integer programming problem into a  continuous one; (N) then solves the continuous optimization  problem in polynomial time; (N) finally rounds the continuous solution back to the 0/N binary solution
Please refer to  the supplements for details
 N
Experiments  In this section, we present experimental results on the  Youtube-NM and Open Images datasets
Due to space limit,  some results are deferred to the supplements
 N.N
Datasets  The YouTube-NM [N] dataset contains ∼N million videos, each annotated with multiple labels from N,N00  classes
The average number of annotations per video is N.N
 These videos are 0.N million hours long and contain ∼N.N billion frames in total
The dataset is split into a training set  with ∼N.N million videos, a validation set with ∼N.N mil- lion videos and a hidden test set with ∼0.N million videos
Two types of features are provided for this dataset: framelevel and video-level
To extract frame-level features, each  video is decoded at N frame-per-second up to the first NN0  seconds
The decoded frames are fed into the Inception  vN network [NN] pre-trained on ImageNet [NN] where the  N0NN-dimensional ReLu activation of the last hidden layer  (layer name pool N reshape) is utilized as frame-level features
Feature dimension is reduced to N0NN using PCA  (+ whitening) followed by quantization (N byte per coefficient)
To obtain video-level features, the first-, secondorder and ordinal statistics of frame-level features are computed and normalized
Please refer to [N] for details
 The Open Images [NN] dataset contains ∼N million im- ages which are annotated with labels spanning N0NN classes
 The average number of labels per image is N.NN
The dataset  is split into a training set (N,0NN,NNN images) and a validation set (NNN,0NN images)
 For both datasets, testing is performed on the validation  set, which is untouched during model training
The two  datasets have been updated since their first release
We used  the first version released in September N0NN
 N.N
Experimental Setup  Hyperparameters DDPP performs visual representation  learning on Youtube-NM frame-level features and Open Image raw pixels
For Youtube-NM, the visual feature network  (VFN) in DDPP is configured as a N-layer N0NN-units LSTM  network [NN]
On Open Images, the VFN is chosen to be  the Inception vN network [NN]
For Youtube-NM video-level  features, they are directly fed into the label-input dependency network (LIDN) in DDPP without further representation learning
For both datasets, the LIDN is configured  to be a fully-connected network with N hidden layers where  the number of units in the first and second layer is N0NN  and NNN respectively and the activation function is ReLU
It  takes the concatenation of label embedding and visual representation as inputs and produces a dependency score
The  label-correlation network (LCN) has N hidden layers where  the number of units in the first and second layer is N00  and N00 respectively and the activation function is ReLU
 It takes each label-embedding vector as input and produces  a N00-dimensional latent representation
Then a linear kernel is applied to the latent representations of two labels
The  dimension of label embeddings is set to N00
The regularization parameter in RDDPP is set to 0.N for Youtube-NM  and 0.0N for Open Images
We use AdaGrad [NN] with a  learning rate of 0.N and batch size of NN to learn model parameters
In LSTM training, the network is unrolled for  N0 iterations
In low-rank kernel learning, the number of inducing points is set to N00
The hyperparameters of baseline  methods are deferred to the supplements
 Baselines For Youtube-NM experiments, we compare  with the following baselines on frame-level features:  • Logistic regression with average pooling (LR-Avg) [N]: train NN00 one-vs-rest LR classifiers for each class based  on frame-level features; at test time, prediction scores on  individual frames are averaged into a video-level score
 • Deep bag of frames [N] with independent labels (DBoF- IL), and long short-term memory network [NN, NN, N] with  independent labels (LSTM-IL): use DBoF and LSTM to  encode frame features; the output labels are treated as  independent, each associated with a sigmoid unit and a  binary cross-entropy loss
 • Structured prediction energy networks (SPEN) [N]
 and the following baselines on video-level features:  • LR, Support Vector Machine (SVM), Mixture of Experts (MoE) [N]: learn one-vs-rest LR, SVM and MoE classifiers on video-level features
 • Multi-label learning by exploiting label dependency (LEAD) [NN], principle label space transformation  (PLST) [NN], clique generating machine (CGM) [NN]
 For Open Images, we compare with:  • LR and SVM: use the Inception vN network pre-trained on ImageNet to extract image features, then learn one-vsrest LR or SVM classifiers for each class
 NNN    Feature Methods MAP Hit@N PERR  Frame  level  LR-Avg [N] NN.0 N0.N NN.N  DBoF-IL [N] NN.N NN.N NN.N  LSTM-IL [N] NN.N NN.N NN.N  SPEN [N] NN.N NN.N NN.N  DDPP-Sep NN.N NN.N NN.N  DDPP N0.N NN.N NN.N  RDDPP NN.N NN.N N0.N  Video  level  LR [N] NN.N N0.N NN.0  SVM [N] NN.0 NN.N NN.N  MoE [N] NN.N NN.N NN.N  LEAD [NN] N0.N NN.N NN.N  PLST [NN] N0.N NN.N NN.N  CGM [NN] NN.N NN.N NN.N  DDPP NN.N NN.N NN.N  RDDPP NN.N NN.N NN.N  Table N
MLC Performance (%) on the Youtube-NM Validation Set  • CNN with independent labels (CNN-IL): replace the softmax output layer in Inception vN network with N0NNway sigmoid layer
 • Deep convolutional ranking (DCR) [NN], CNN-RNN [NN], conditional graphical Lasso (CGL) [NN], multi-task  label cleaning (MTLC) [NN]
 Evaluation Metrics Following [N, NN], we use mean  average precision (MAP), Hit@k and precision at equal recall rate (PERR) to evaluate the MLC performance
 The AP for each class c is defined as AP (c) =  N/M ∑N  n=N p(c, n)I(c, n) where p(c, n) is the precision for class c when retrieving n annotations, I(c, n) is an indicator function that is N if the ground truth for class c and the im- age at rank n is positive
N is the size of the validation set and M is the number of positives
The MAP of C classes is N/C  ∑C c=N AP (c)
Hit@k is the percentage of test instances that contain at least one of the groundtruth labels in  the top k predictions
For the definition of PERR, please refer to [N]
These metrics require DDPP to assign a confidence score to each predicted label
We use the continuous  values obtained in step N of the inference algorithm (Section  N.N) to represent the confidence
 External Knowledge on Label Co-occurrence Relations We harvest label co-occurrence relations from the  YFCCN00M dataset [NN], which contains ∼NN.N million photos and ∼0.N million videos from Flickr, each annotated with multiple textual tags
On YFCCN00M tags, we compute the pointwise mutual information (PMI) [NN] between  each pair of classes in Youtube-NM or Open Images and assign must-links to N000 class-pairs that have the largest PMI  and cannot-links to N000 pairs with the smallest PMI
 N.N
Results  Table N shows the MLC performance on the Youtube-NM  dataset
As can be seen, our method DDPP outperforms  the baselines with a large margin, on both the frame-level  features and video-level features
At frame level, LSTMIL and DDPP both leverage LSTM networks to learn visual representations
Their major difference is: LSTMMethods MAP  LR NN.N  SVM NN.N  CNN-IL NN.N  DCR [NN] NN.N  CNN-RNN [NN] NN.N  CGL [NN] NN.N  MTLC [NN] NN.N  DDPP-Sep NN.N  DDPP NN.N  RDDPP NN.N  Table N
MLC Performance on the Open Images Validation Set  IL treats the output labels as independent while DDPP  aims at capturing high-order label-correlations
As a result,  DDPP achieves much better performance than LSTM-IL
 Similar to LSTM-IL, DBof-IL ignores label-correlations,  which loses the semantic and contextual clues among labels
SPEN uses deep networks to capture nonlinear labeldependency
While outperforming other baselines, it is inferior to DDPP
One possible reason is: during parameter learning of SPEN, an approximated inference procedure  is conducted over each training instance, which may incur  large approximation errors
On the contrary, no inference is  needed when learning DDPP parameters
LR-Avg reduces  video-level MLC into frame-level MLC, which fails to consider the inter-frame relations, hence leading to inferior performance
 The importance of capturing high-order labelcorrelations is reflected on the video-level features as  well
LR, SVM and MoE ignore label-correlations, hence  performing less well
LEAD, PLST and CGM aim at  exploiting high-order label dependency based on Bayesian  network structure learning, label space dimension reduction  and clique generation
The experiments show that they are  less effective than DDPP
The possible reasons are: (N)  LEAD performs structure learning and parameter learning  separately while DDPP learns label-correlations and labelinput dependency in a joint manner; (N) PLST projects  the labels into a linear subspace, hence can only capture  linear dependency while DDPP uses kernel methods to  exploit nonlinear dependency; (N) Similar to SPEN, CGM’s  parameter learning involves approximate inference on  training data, which may incur considerable approximation  errors, while DDPP can avoid this
Besides, these methods  lack the flexibility to perform end-to-end feature learning  and hence cannot be applied to frame-level features
 In DDPP, capturing label-correlations and learning visual features are performed jointly
To evaluate whether  joint learning is better than performing the two tasks  separately, we compare DDPP with a model variant  which first learns visual features without considering labelcorrelation, then feeds the learned features to DPP to perform correlation-aware MLC
The first step is performed on  the frame-level features using LSTM-IL
We refer to this  method as DDPP Separation (DDPP-Sep)
As shown in  NNN    NN.N  N0  N0.N  NN  NN.N  NN  0 N000 N000 N0000 NN000 N0000  M A  P    Number of Must/Cannot Links   Figure N
MAP versus the total number of must/cannot links, on  the Youtube-NM frame-level features
 Table N, DDPP-Sep performs worse than DDPP, which corroborates the merit of joint learning
 By incorporating label co-occurrence knowledge, RDDPP improves DDPP greatly
To further study the effect  of knowledge-incorporation, we measure how the MAP  varies as we gradually add must/cannot links
Class-pairs  are ranked according to their PMI in descending order
 Must and cannot links are added by visiting the ranked list  from top to bottom and from bottom to top respectively
 The number of must-links is equal to that of cannot links
 Fig
N shows the results on Youtube-NM frame-level features
MAP consistently increases when the link number  is increased from N,000 to N0,000, which demonstrates the  benefits of incorporating prior knowledge and the efficacy  of relational regularization in RDDPP to realize that
Further increasing the link number does not improve MAP, possibly because the links added later contain more noise
 Table N shows the MAP on Open Images, where DDPP  and RDDPP outperform baselines
LR, SVM, CNN-IL and  MTLC learn independent classifiers for each class and ignore label-correlations
DCR uses a ranking-loss to differentiate relevant and irrelevant labels, but is less capable of capturing fine-grained correlations among relevant  labels
CNN-RNN characterizes label-dependency using  class chaining, where a proper class-order is difficult to  specify
CGL captures pairwise correlations using conditional graphical Lasso, but is unable to exploit highorder label-dependency
DDPP performs better than DDPPSep by jointly capturing label-correlation and learning visual features
RDDPP outperforms DDPP by incorporating  label-correlation knowledge
More results on Open Images  are in the supplements
 N.N
Computational Time  The experiments were conducted on two clusters: a N0machines GPU (Titan X) cluster which ran deep learning  (DL) experiments and a NN-machines CPU cluster for nonDL experiments
We compare the training time of DDPP  with DL methods
To verify the efficiency-gain brought  by low rank kernel learning (Section N.N), we also compare with the case where the kernel matrix in DDPP is  of full rank (DDPP-FullRank)
Each model is trained using a distributed system that adopts a data-parallel strategy
Table N shows the convergence time of different modMethods Youtube-NM Open Images  DBoF-IL [N] N.N –  LSTM-IL [N] N.N –  SPEN [N] NN.N –  CNN-IL – N.N  DCR [NN] – N.N  CNN-RNN [NN] – N.N  MTLC [NN] – N0.N  DDPP-FullRank NN.N NN.N  DDPP N.N N0.N  RDDPP N.N NN.N  Table N
Convergence Time (Hours)  els on the GPU/CPU cluster, from which we observe: (N)  DDPP, which by default adopts low-rank kernel learning,  is much more efficient than DDPP-FullRank; the computational complexity of DDPP and DDPP-FullRank is O(MN) and O(KN) respectively, where the number of inducing points M is much smaller than the number of classes K
(N) RDDPP, though adding an additional regularizer to DDPP,  does not incur substantial extra cost
(N) While being able to  capture high-order label-correlations, DDPP’s convergence  time is comparable with that of other deep learning methods which assume label-independence, such as DBoF-IL,  LSTM-IL and CNN-IL
(N) DDPP is much more efficient  than SPEN which involves a costly inference procedure  in parameter learning; no inference is needed in learning  DDPP parameters
 N
Conclusions and Future Works  We study the large-scale multi-label classification on two  recently released datasets: Youtube-NM and Open Images
 To capture the high-order correlation among labels while  retaining computational efficiency, we propose Deep Determinantal Point Process (DDPP) that seamlessly integrates  DPP and deep neural networks (DNNs) and supports endto-end learning
DPP is able to capture label-correlation  of arbitrary order within polynomial computational time  while DNNs play the role of representation learning of images and videos
To incorporate prior knowledge regarding label co-occurrence relations, we impose relational regularization over DDPP’s kernel matrix
A low-rank kernel  learning algorithm is investigated to scale DDPP to millions  of instances and thousands of labels
Experiments on the  two datasets demonstrate the efficacy and efficiency of our  methods
For future works, we plan to investigate the noisy  and missing label problem [NN] presenting in Open Images  and leverage label hierarchy to improve MLC performance
 Acknowledgements  P.X and E.X are supported by National Institutes of Health  PN0DA0NNNNN, R0NGMNNNNNN, National Science Foundation IISNNNNNNN, DARPA FANNNN0NC000N
L.M is supported by National Natural Science Foundation of China  No.NNNNN0NN
 NN0    References  [N] S
Abu-El-Haija, N
Kothari, J
Lee, P
Natsev, G
Toderici,  B
Varadarajan, and S
Vijayanarasimhan
Youtube-Nm: A  large-scale video classification benchmark
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N, N, N  [N] R
Agrawal, A
Gupta, Y
Prabhu, and M
Varma
Multi-label  learning with millions of labels: Recommending advertiser  bid phrases for web pages
In WWW, N0NN
N  [N] K
Balasubramanian and G
Lebanon
The landmark selection method for multiple output prediction
ICML, N0NN
N  [N] D
Belanger and A
McCallum
Structured prediction energy  networks
In ICML, N0NN
N, N, N, N, N  [N] S
Bengio, J
Weston, and D
Grangier
Label embedding  trees for large multi-class tasks
In NIPS, N0N0
N  [N] W
Bi and J
T.-Y
Kwok
Efficient multi-label classification  with many labels
In ICML, N0NN
N  [N] S
S
Bucak, R
Jin, and A
K
Jain
Multi-label learning with  incomplete class assignments
In CVPR, N0NN
N  [N] S
S
Bucak, P
K
Mallapragada, R
Jin, and A
K
Jain
Efficient multi-label ranking for multi-class learning: application to object recognition
In ICCV, N00N
N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected crfs
ICLR, N0NN
N, N  [N0] Y
Chen and H
Lin
Feature-aware label space dimension  reduction for multi-label classification
In NIPS, N0NN
N  [NN] W
Cheng, E
Hüllermeier, and K
J
Dembczynski
Bayes  optimal multilabel classification via probabilistic classifier  chains
In ICML, N0N0
N  [NN] T.-S
Chua, J
Tang, R
Hong, H
Li, Z
Luo, and Y
Zheng
 Nus-wide: a real-world web image database from national  university of singapore
In CIVR, N00N
N  [NN] K
W
Church and P
Hanks
Word association norms, mutual  information, and lexicography
Computational linguistics,  NN(N):NN–NN, NNN0
N  [NN] J
Deng, W
Dong, R
Socher, L.-J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
In  CVPR, N00N
N  [NN] J
Duchi, E
Hazan, and Y
Singer
Adaptive subgradient methods for online learning and stochastic optimization
 Journal of Machine Learning Research, N0NN
N  [NN] X
Geng and L
Luo
Multilabel ranking with inconsistent  rankers
In CVPR, N0NN
N  [NN] M
George and C
Floerkemeier
Recognizing products: A  per-exemplar multi-label image classification approach
In  ECCV, N0NN
N  [NN] N
Ghamrawi and A
McCallum
Collective multi-label classification
In CIKM, N00N
N, N  [NN] J
Gillenwater, A
Kulesza, and B
Taskar
Near-optimal map  inference in determinantal point process
In NIPS, N0NN
N  [N0] B
Gong, W.-L
Chao, K
Grauman, and F
Sha
Diverse  sequential subset selection for supervised video summarization
In NIPS, N0NN
N  [NN] Y
Gong, Y
Jia, T
Leung, A
Toshev, and S
Ioffe
Deep  convolutional ranking for multilabel image annotation
arXiv  preprint arXiv:NNNN.NNNN, N0NN
N, N  [NN] Y
Guo and S
Gu
Multi-label classification using conditional dependency networks
In IJCAI, N0NN
N, N  [NN] B
Hariharan, L
Zelnik-Manor, M
Varma, and S
Vishwanathan
Large scale max-margin multi-label classification  with priors
In ICML, N0N0
N, N  [NN] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural computation, NNNN
N  [NN] D
J
Hsu, S
Kakade, J
Langford, and T
Zhang
Multi-label  prediction via compressed sensing
In NIPS, N00N
N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] S
Ji, L
Tang, S
Yu, and J
Ye
Extracting shared subspace  for multi-label classification
In KDD, N00N
N, N  [NN] L
Jing, L
Yang, J
Yu, and M
K
Ng
Semi-supervised  low-rank mapping learning for multi-label classification
In  CVPR, N0NN
N  [NN] A
Kanehira and T
Harada
Multi-label ranking from positive and unlabeled data
In CVPR, N0NN
N  [N0] F
Kang, R
Jin, and R
Sukthankar
Correlated label propagation with application to multi-label learning
In CVPR,  N00N
N  [NN] P
Krähenbühl and V
Koltun
Efficient inference in fully  connected crfs with gaussian edge potentials
In Advances  in Neural Information Processing Systems, pages N0N–NNN,  N0NN
N, N  [NN] I
Krasin, T
Duerig, N
Alldrin, A
Veit, S
Abu-El-Haija,  S
Belongie, D
Cai, Z
Feng, V
Ferrari, V
Gomes, A
Gupta,  D
Narayanan, C
Sun, G
Chechik, and K
Murphy
Openimages: A public dataset for large-scale multi-label and  multi-class image classification
Dataset available from  https://github.com/openimages, N0NN
N, N  [NN] A
Kulesza and B
Taskar
Learning determinantal point processes
UAI, N0NN
N, N, N  [NN] A
Kulesza, B
Taskar, et al
Determinantal point processes  for machine learning
Foundations and Trends R© in Machine  Learning, N0NN
N, N  [NN] J
Lafferty, A
McCallum, F
Pereira, et al
Conditional random fields: Probabilistic models for segmenting and labeling  sequence data
In ICML, N00N
N  [NN] D
Lee, G
Cha, M.-H
Yang, and S
Oh
Individualness and  determinantal point processes for pedestrian detection
In  ECCV, N0NN
N  [NN] C
Li, B
Wang, V
Pavlu, and J
Aslam
Conditional bernoulli  mixtures for multi-label classification
In ICML, N0NN
N  [NN] Q
Li, M
Qiao, W
Bian, and D
Tao
Conditional graphical  lasso for multi-label image classification
In CVPR, N0NN
N,  N, N  [NN] X
Li, F
Zhao, and Y
Guo
Conditional restricted boltzmann  machines for multi-label learning with incomplete labels
In  AISTATS, N0NN
N  [N0] T
Mikolov, I
Sutskever, K
Chen, G
S
Corrado, and  J
Dean
Distributed representations of words and phrases  and their compositionality
In NIPS, N0NN
N  [NN] G
A
Miller
Wordnet: a lexical database for english
Communications of the ACM, NN(NN):NN–NN, NNNN
N  NNN    [NN] G
Patterson, C
Xu, H
Su, and J
Hays
The sun attribute  database: Beyond categories for deeper scene understanding
 IJCV, N0N(N-N):NN–NN, N0NN
N  [NN] Y
Prabhu and M
Varma
Fastxml: A fast, accurate and stable tree-classifier for extreme multi-label learning
In KDD,  N0NN
N  [NN] V
Ranjan, N
Rasiwasia, and C
Jawahar
Multi-label crossmodal retrieval
In ICCV, N0NN
N  [NN] J
Read, B
Pfahringer, G
Holmes, and E
Frank
Classifier chains for multi-label classification
Machine learning,  N0NN
N  [NN] J
Rousu, C
Saunders, S
Szedmak, and J
Shawe-Taylor
 Kernel-based learning of hierarchical multilabel classification models
JMLR, N00N
N  [NN] B
Schölkopf and A
J
Smola
Learning with kernels: support vector machines, regularization, optimization, and beyond
MIT press, N00N
N  [NN] B
W
Silverman
Some aspects of the spline smoothing approach to non-parametric regression curve fitting
Journal  of the Royal Statistical Society
Series B (Methodological),  pages N–NN, NNNN
N, N  [NN] L
Sun, S
Ji, and J
Ye
Hypergraph spectral learning for  multi-label classification
In KDD, N00N
N, N  [N0] G
Sundaramoorthi and B.-W
Hong
Fast label: Easy and efficient solution of joint multi-label and estimation problems
 In CVPR, N0NN
N  [NN] F
Tai and H
Lin
Multilabel classification with principal  label space transform
Neural Computation, N0NN
N, N, N  [NN] M
Tan, Q
Shi, A
van den Hengel, C
Shen, J
Gao, F
Hu,  and Z
Zhang
Learning graph structure for multi-label image  classification via clique generation
In CVPR, N0NN
N, N, N  [NN] B
Thomee, D
A
Shamma, G
Friedland, B
Elizalde, K
Ni,  D
Poland, D
Borth, and L.-J
Li
YfccN00m: The new  data in multimedia research
Communications of the ACM,  NN(N):NN–NN, N0NN
N, N  [NN] A
Veit, N
Alldrin, G
Chechik, I
Krasin, A
Gupta, and  S
Belongie
Learning from noisy large-scale datasets with  minimal supervision
CVPR, N0NN
N, N  [NN] C
Vens, J
Struyf, L
Schietgat, S
Džeroski, and H
Blockeel
Decision trees for hierarchical multi-label classification
 Machine Learning, N00N
N  [NN] J
Wang, Y
Yang, J
Mao, Z
Huang, C
Huang, and W
Xu
 Cnn-rnn: A unified framework for multi-label image classification
In CVPR, N0NN
N, N, N, N  [NN] Z
Wang, B
Du, L
Zhang, L
Zhang, M
Fang, and D
Tao
 Multi-label active learning based on maximum correntropy  criterion: Towards robust and discriminative labeling
In  ECCV, N0NN
N  [NN] Y
Wei, W
Xia, J
Huang, B
Ni, J
Dong, Y
Zhao, and  S
Yan
Cnn: Single-label to multi-label
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N  [NN] A
Wilson and H
Nickisch
Kernel interpolation for scalable  structured gaussian processes (kiss-gp)
In ICML, N0NN
N  [N0] A
G
Wilson, Z
Hu, R
Salakhutdinov, and E
P
Xing
Deep  kernel learning
AISTATS, N0NN
N  [NN] B
Wu, S
Lyu, and B
Ghanem
Ml-mg: multi-label learning  with missing labels using a mixed graph
In ICCV, N0NN
N  [NN] X
Xue, W
Zhang, J
Zhang, B
Wu, J
Fan, and Y
Lu
 Correlative multi-label multi-instance image annotation
In  ICCV, N0NN
N  [NN] H
Yang, J
Tianyi Zhou, Y
Zhang, B.-B
Gao, J
Wu, and  J
Cai
Exploit bounding box annotations for multi-label object recognition
In CVPR, N0NN
N  [NN] H
Yang, J
T
Zhou, and J
Cai
Improving multi-label learning with missing labels by structured semantic correlations
 In ECCV, N0NN
N  [NN] C.-K
Yeh, W.-C
Wu, W.-J
Ko, and Y.-C
F
Wang
Learning  deep latent spaces for multi-label classification
N0NN
N  [NN] J
Yue-Hei Ng, M
Hausknecht, S
Vijayanarasimhan,  O
Vinyals, R
Monga, and G
Toderici
Beyond short snippets: Deep networks for video classification
In CVPR, N0NN
 N  [NN] K
Zhang, W
Chao, F
Sha, and K
Grauman
Video summarization with lstm
In ECCV, N0NN
N  [NN] M.-L
Zhang and K
Zhang
Multi-label learning by exploiting label dependency
In KDD, N0N0
N, N, N, N  [NN] F
Zhao, Y
Huang, L
Wang, and T
Tan
Deep semantic ranking based hashing for multi-label image retrieval
In CVPR,  N0NN
N  [N0] K
Zhao, W.-S
Chu, and H
Zhang
Deep region and multilabel learning for facial action unit detection
In CVPR, N0NN
 N  NNNNeural Person Search Machines   Neural Person Search Machines  Hao Liu N Jiashi FengN Zequn JieN Karlekar JayashreeN  Bo Zhao N Meibin QiN Jianguo JiangN Shuicheng YanN,N  NHefei University of Technology NNational University of Singapore NTencent AI Lab N Panasonic R&D Center Singapore NSouthwest Jiaotong University NNN0 AI Institute  {hfut.haoliu, zequn.nus}@gmail.com, elefjia@nus.edu.sg, karlekar.jayashree@sg.panasonic.com  zhaobo@my.swjtu.edu.cn, {qimeibin, jgjiang}@hfut.edu.cn, yanshuicheng@NN0.cn  Abstract  We investigate the problem of person search in the wild in  this work
Instead of comparing the query against all candidate regions generated in a query-blind manner, we propose to recursively shrink the search area from the whole  image till achieving precise localization of the target person, by fully exploiting information from the query and contextual cues in every recursive search step
We develop the  Neural Person Search Machines (NPSM) to implement such  recursive localization for person search
Benefiting from  its neural search mechanism, NPSM is able to selectively  shrink its focus from a loose region to a tighter one containing the target automatically
In this process, NPSM employs an internal primitive memory component to memorize  the query representation which modulates the attention and  augments its robustness to other distracting regions
Evaluations on two benchmark datasets, CUHK-SYSU Person  Search dataset and PRW dataset, have demonstrated that  our method can outperform current state-of-the-arts in both  mAP and top-N evaluation protocols
 N
Introduction  Person search [NN, NN] aims to localize a specific person matching the provided query in gallery images or video  frames
It is a new and challenging task that requires to address person detection and re-identification simultaneously
 It has many important applications in video surveillance and  security, such as cross-camera visual tracking [NN] and person verification [NN]
But it is difficult in real-world scenarios due to various distracting factors including large appearance variance across multiple cameras, low resolution,  cluttered background, unfavorable camera setting, etc
 To date, only a few methods have been proposed to address this task
In the pioneer work [NN], Xiao et al
adopted  the end-to-end person search model based on the proposed  Online Instance Matching (OIM) loss function to jointly  (a) Search process of previous methods  (b) Search process of the NPSM  Figure N
Demonstration of person search process for one gallery  image in previous methods and our proposed method
(a) The  search process of previous methods
The query person is one-byone compared with the detection results for one gallery image;  then the search result ranked at the first place is obtained
The  red boxes indicate the wrong matched results while the green box  represents the truly matched person
(b) The search process of the  NPSM
When a target person is searched within a whole scene, the  search scope on which attention is focused is recursively shrinking with guidance from memory of the query person’s appearance,  which is marked in red boxes
 train person detection and re-identification networks
The  recent work [NN] also follows a similar pipeline
Generally,  all of the previous person search methods are based on such  a simple two-stage search strategy: first to detect all candidate persons within an image and then to perform exhaustive comparison between all possible pairs of the query and  the candidates to output a search result ranked at the first  place within the searched images
This pipeline has some  drawbacks
Firstly, if the target person has distracting factors around, e.g., another person with similar appearance,  the search accuracy would be affected by the distracting  NNN    factors
Secondly, extra error, such as inaccurate detection,  would be introduced by the two isolated frameworks, i.e
 person detection and re-identification
See Figure N (a) for  demonstration
The red boxes indicate the wrong matched  results while the green box represents the truly matched person
 For person search, it is commonly assumed that within  an image, the target person only appears at a single location
Such instance-level exclusive cues imply that instead  of examining all possible persons, a more effective strategy  is to only search within the regions possibly containing the  target person in a coarse-to-fine manner
This is similar to  human neural system for processing complex visual information [N,NN]
More concretely, after seeing and remembering the appearance of a target person, one usually shrinks  his search area from a large scope to a small one and performs matching with his memory in details within the small  scope with more effort
Such a coarse-to-fine search process  is intuitively useful for existing person search solutions
 Inspired by above observations, we propose a new and  more effective person search strategy and develop the Neural Person Search Machines (NPSM)
Compared to the  search process in previous methods, our NPSM (Figure N  (b)) takes the query person as memory to recursively guide  the model to shrink the search region and judge whether  the current region contains the target person or not
This  process would include more contextual cues beneficial for  person matching
In Figure N (b), the red box in each image  from left to right corresponds to a region that can be focused  on, and the arrow indicates a search process which can be  considered as the continuous shrinkage of the focus region
 Additionally, those irrelevant regions can be ignored after  every shrinkage of a subregion from a big region, which  can reduce the interference of other unimportant regions
 To model the above person search process, we need to  solve the following two non-trivial problems: N) integrating information of the query person into the search process  as memory to exclude interference from impossible candidates; N) judging which subregion should be focused on in  the bigger region at each recursive step in the coarse-to-fine  search process under the guidance of memory
 For localizing the target person in a sequence correctly  and fully exploiting the context information, we propose  a neural search architecture to selectively concentrate on  an effective subregion of the input region, and meanwhile  ignore other perceived information from distracting subregions in a recursive way
Take the third subregion of  the search process in Figure N (b) for example, the proposed NPSM would highlight the truly matched person at  the left side of the region and ignore the similar person at the  right side
Considering the specific ability of Long ShortTerm Memory (LSTM) [N0] to partially allow or deny information to flow into or out of its memory component, we  build our Neural Search Networks (NSN) upon Convolutional LSTM (Conv-LSTM) [NN] units which are capable  of preserving spatial information from the spatio-temporal  sequences
 Different from the vanilla Conv-LSTM, we augment our  NSN by equipping it with external primitive memory that  contains appearance information of the query and helps  identify the candidate regions at the coarse level and discards irrelevant regions
The external primitive memory  thus enables the query to be involved in the representation  learning for person search as well as the recursive search  process with region shrinkage
 To sum up, in this work we go beyond the standard  LSTM based models and propose a new person search  approach called Neural Person Search Machines (NPSM)  based on the Conv-LSTM [NN], which contains the context  information of each person and employs the external memory about the query person to guide the model to attend to  the right region
Our approach is able to achieve better performance compared with other methods, as validated by experimental results
 We make the following contributions to person search:  N) We redefine the person search process as a detection free  procedure of recursively focusing on the right regions
 N) We coin a novel method more robust to distracting factors  benefiting from contextual information
 N) We propose a new neural search model that can integrate the query person information into primitive memory  to guide the model to recursively focus on the effective regions
 N
Related Work  Person search can be regarded as the combination of  person re-identification and person detection
Most of existing works of person re-identification focus on designing hand-crafted discriminative features [N, N, NN], learning  deep learning based high-level features [N,NN,NN,NN,NN,NN]  and learning distance metrics [NN, NN, N0, NN, NN]
Recent deep learning based person re-identification methods  [N, NN, NN, NN] re-design the structure of the deep network  to improve performance
For instance, [N] designed two  novel layers to capture relationships between two views of a  person pair
Among distance metric learning methods, [NN]  proposed KISSME (KISS MEtric) to learn a distance metric  from equivalence constraints
Additionally, [NN] proposed  to solve the person re-id problem by learning a discriminative null space of the training samples while [NN] proposed a  method learning a shared subspace across different scales to  address the low resolution person re-identification problem
 For person detection, Deformable Part Model  (DPM) [N], Aggregated Channel Features (ACF) [N]  and Locally Decorrelated Channel Features (LDCF) [NN]  are three representative methods relying on hand-crafted  NNN    features and linear classifiers to detect pedestrians
Recently, several deep learning-based frameworks have been  proposed
In [NN], DeepParts was proposed to handle occlusion with an extensive part pool
Besides, [N] proposed the  CompACT boosting algorithm learning complexity-aware  detector cascades for person detection
In our knowledge,  two previous works [NN, NN] address person search by  fusing person re-identification and detection into an integral pipeline to consider whether any complementarity  exists between the two tasks
[NN] developed an end-to-end  person search framework to jointly handle both aspects  with the help of Online Instance Matching (OIM) loss  while [NN] proposed ID-discriminative Embedding (IDE)  and Confidence Weighted Similarity (CWS) to improve  the person search performance
However, these two works  simply focus on how the interplay of pedestrian detection  and person re-identification affects the overall performance,  and they still isolate the person search into two individual  components (detection and re-identification), which would  introduce extra error, e.g
inaccurate detection
In this  paper, we regard person search as a detection-free process  of gradually removing interference or irrelevant target  persons for the query person
 Recently, LSTM based attention methods have shown  good performance in image description [NN], action recognition [NN, NN] and person re-identification [NN]
In [NN],  Xu et al
showed how the learned attention can be exploited  to give more interpretability into the model generation process, while [NN, NN] adopted attention methods to recognize  important elements in video frames based on the action that  is being performed
Moreover, [NN] formulated an attention  model as a triplet recurrent neural network which dynamically generates comparative attention location maps for person re-identification
Analogously, our proposed NPSM  also has such a locally emphasizing property, but NPSM is  a query-aware model while the above attention-based methods all adopt a blind attention mechanism
 N
Proposed Neural Person Search Machines  In this section, we present the architecture details of the  proposed Neural Person Search Machines (NPSM), and explain how it works with the primitive memory modeling to  facilitate person search
 N.N
Architecture Overview  The overall architecture is shown in Figure N
It consists of two components, i.e
Primitive Memory and Neural  Search Networks
We propose to solve person search by recursively shrinking the search area from the whole image  to the precise bounding box of the person of interest
And  each region in the shrinking search process would contain  the contextual information of the final search result
Besides recursively utilizing the contextual cues, NPSM provides extra robustness to interference from other distracting  subregions for the model in the search process
 The proposed NPSM is trained end-to-end to learn to  make a decision on the subregion attention at each recursive  step and finally localize the person of interest
The Neural  Search Network enables the model to automatically focus  on relevant regions, and the Primitive Memory that models the representation of the query person continuously provides extra cues for every search step and facilitates more  precise localization of persons
After performing the recursive region shrinkage, the model reaches a search result with the biggest confidence as the final search result  with an gallery image
Note that, different from previous  works [NN, NN], our method is detection-free and includes  no Non-Maximum Suppression (NMS), as it is a search process performing simultaneous region shrinking and person  identification
When the searching is finished, there will be  only one bounding box person search result left
 N.N
Person Search with NPSM  As aforementioned, we redefine the person search process as the recursive region shrinking process
It is equivalent to recursively focusing on a subregion containing the  person of interest from a bigger region
Here we describe  the details of our proposed NPSM and explain how to perform the recursive region shrinking to search the target person for each gallery image
 N.N.N Neural Search Networks  Learning to search for a person from a big region to a specific person region within the gallery image can be deemed  as a sequence modeling problem
Specifically, the shrinking regions constitute a sequence
Thus a natural choice  for the model candidates is the Recurrent Neural Network  (RNN) or LSTM based RNN
However, the vanilla LSTM  [N0] only models sequence information through fully connected layers and requires vectorizing ND feature maps
 This would result in the loss of spatial information, harming  person localization performance
In order to preserve the  spatial structure of the regions over the shrinking process  shown in Figure N, we design a new network called Neural Search Network (NSN) based on Convolutional LSTM  (Conv-LSTM) [NN] for each recursive step
Conv-LSTM  replaces the fully connected multiplicative operations in an  LSTM unit with convolutional operations
Different from  it, the NSN has an additional memory component recording  the query
 Conv-LSTM can be used for building attention networks  that can learn to pay attention to critical regions within feature maps
Thus, Conv-LSTM based NSN is also equipped  with attention mechanism to learn to gradually shrink the  region and selectively memorize the contextual information  contained in the searched bigger region at each recursive  NNN    Figure N
Architecture of our proposed Neural Person Search Machines (NPSM)
It consists of two components, i.e
Primitive Memory  and Neural Search Networks
It works by recursively shrinking the search area from the whole image to the precise bounding box of the  person of interest under the guidance (orange dotted lines) of Primitive Memory
And each region in the shrinking search process would  contain the contextual information of the final search result
Red boxes denote the shrinking regions highlighted at different recursive steps  in our NPSM
“ResN0 PartN” corresponds to the convN to convN N of ResNet-N0 while “ResN0 PartN” represents the convN N to convN N of  ResNet-N0
Best viewed in color
 step
However, our neural search model has a unique feature that distinguishes it from a plain attention model: in  addition to gallery images, a query illustrating the search  target is also input to the search network
Traditional attention networks cannot well model such extra cues
In this  work, we propose to model such query information into the  primitive memory in order to facilitate person search
 We now elaborate on the new Neural Search Networks  (NSN) of our NPSM, tailored for the person search task
In  the NSN component, the query person information, denoted  as q, is integrated into the computation within gates and  cell states in a way to bias the updating of internal states towards emphasizing information relevant to the query while  forgetting irrelevant information
Here the query feauture  q is extracted from the query image through the pre-trained  “ResN0 partN” (convN to convN N of ResNet-N0 [N]) which is  the same as the one extracting features from gallery images
 The cell and gates in the NSN are defined as  it = σ (wxi ∗ xt +whi ∗ ht−N +wqi ∗ q+ bi)  ft = σ (wxf ∗ xt +whf ∗ ht−N +wqf ∗ q+ bf )  ot = σ (wxo ∗ xt +who ∗ ht−N +wqo ∗ q+ bo) (N)  gt = tanh (wxc ∗ xt +whc ∗ ht−N +wqc ∗ q+ bc)  ct = ft ⊙ ct−N + it ⊙ gt  ht = ot ⊙ tanh (ct) ,  where ∗ represents the convolutional operation and ⊙ is the Hadamard product, wx∼, wh∼ are two-dimensional convolutional kernels and xt which is the feature map of the region highlighted by the previous time-step denotes the input  at time step t
The input gate, forget gate, output gate, hidden state and memory cell are denoted as it, ft, ot, ht, ct respectively, which are all three-dimensional tensors retaining  spatial dimensions
With their control, the contextual information can be selectively memorized
Note the query person information q is independent of the time step t, therefore serving as the global primitive memory that guides the  person search procedure continuously
The effect of such  memory information over the states is modeled through the  parameter wq∼
 N.N.N Region Shrinkage with Primitive Memory  As stated above, the goal of NPSM is to effectively shrink  regions containing the target person based on the neural  search mechanism, guided by the primitive memory
That  is, the NPSM will decide which local region should be focused on at each recursive step in the search process as  shown in Figure N
Through this way, more context information would be included from a large region and the  number of irrelevant person candidates with the target person would be recursively reduced in the search process
In  this subsection, we introduce how the subregion of each recursive time-step is generated and shrunk from the bigger  region of the previous time-step
 Here we define the region covered by the highlighted  proposal bounding boxes induced by current attention map  as follows:  NNN    R = (min(θxN),min(θyN),max(θxN),max(θyN)),  where θxN,θyN,θxN,θyN are the top left and lower right corner coordinates of all the highlighted bounding boxes from a  predefined collection, generated by an unsupervised object  proposal model (e.g., Edgeboxes [NN])
Then we separate  the region R into several candidate subregions according to  the relationship of each contained bounding box in the region R
In this paper, we choose the Euclidean distance as  the metric of the relationship defined as  d(a,b) =  √  ∑N  i=N (ai − bi)N, (N)  where a and b are the centre coordinates of two proposal bounding boxes A and B respectively
Specifically,  a = (aN, aN), b = (bN, bN), aN = axN + 0.N (axN − axN), aN = ayN+0.N (ayN − ayN), bN = bxN+0.N(bxN−bxN), bN = byN + 0.N(byN − byN)
(axN, ayN) and (axN, ayN) are the top left and lower right coordinates of bounding box A while  (bxN, byN) and (bxN, byN) are the top left and lower right co- ordinates of bounding box B
Then the proposal bounding  boxes can be grouped into C clusters according to their relationships
The corresponding subregions covered by proposals are Rsub(C)
We denote the parent region to generate  subregions Rsub(C) as R par
 At each recursive step t, the proposed NSN outputs an attention map which predicts the scores (reflecting confidence  on containing the target person given the primitive memory  information) of shrinking to region Rsubt,(C) after NSN taking  input the parent region R par t−N at the previous step t− N
 More specifically, at each time step (corresponding to  shrinking to one region), NSN takes input the query person feature q and the region feature xt extracted from pretrained “ResN0 partN” which denotes the convN to convN N  of ResNet-N0 [N]
Here, we add a Region of Interest (ROI)  pooling layer following “ResN0 partN” to make sure the regions of different sizes can have feature maps of the same  size K×K×D
Compared with the standard LSTM based model relying on multi-layer perceptron, NSN uses convolutional layers to integrate the region representation with  primitive memory and produce attention maps
Specifically,  at each time step t, an attention score map of size K × K for K ×K locations is computed:  zt = wz ∗ tanh (wqa ∗ q+wha ∗ ht + ba) (N)  l i,j t =  exp(zijt ) ∑  i  ∑  j exp(z ij t )  
(N)  The score for location (i, j) is denoted as li,jt 
Then, in the process of region shrinkage, the NSN computes the average scores of different subregions from the  parent region
NSN highlights the subregion with the maximum score as the region to be searched in the next step
 This computation would be performed many times until the  search path reaches the final proposal
The average score of  the subregion is computed as follows:  St = N  m · n  m ∑  i=N  n ∑  j=N  l i,j t , (N)  where m and n are the height and the width of the subregion respectively
l i,j t corresponds to the score map defined in Eqn
(N) generated on the parent region
In other  words, our model does not stick to the single region
If  some regions not highlighted before receive higher attention at certain search step, our model would jump to that  region with higher intra-region confidence scores
In this  way, the accumulative error in the shrinkage process can be  alleviated
Note that our NSN serves as a region shrinkage  method
In other words, our NSN only outputs the most  similar proposal with the query person in each gallery image
Therefore, the features of the query person image and  the final search result are extracted from the “Identification Net” (orange boxes in Figure N) of the trained model  when the searching is finished
Here, the “Identification  Net” takes input the output of “ResN0 PartN” (pink boxes  in Figure N) representing the convN N to convN N of ResNetN0
And it consists of a global average pooling layer and  a NNN-dimension Fully Connected layer
Then the cosine  similarity between the features of the query person and the  final person search result is computed for evaluation
 N.N
Training Strategy  Here we detail the training of the proposed model
 Firstly, we use the architecture proposed in OIM [NN] to pretrain the Fully Convolutional Networks (FCN) including  both “ResN0 partN” and “ResN0 partN’
Then for the region  at each recursive time-step, the feature is extracted from the  ROI pooling layer after the pre-trained “ResN0 partN”
After  that, all the features are fed to the NSN and we add a convolutional layer of size N×N×N after output of each time step to calculate the “region shrinkage loss”
Here we adopt segmentation alike softmax loss as the “region shrinkage loss”
 The supervision label of each time step is defined as  Ut =  {  N, if G ∈ Rt  0, otherwise, (N)  where G is the ground truth bounding box of the target person while Rt is the region box reached at the tth time step
 This training strategy enables the proposed network to produce proper attention maps that fall into the region containing the target person as tight as possible
In other words,  our NPSM is expected to predict the probability of the target person appearing at each location in a gallery image
 Besides, to make the learned feature more discriminative, we add an identification loss following the “Identification Net”, which takes input the output feature u of “IdentiNNN    fication Net” and is defined as  P (z = c|u) = exp(Scu)  ∑  k exp(Sku) , (N)  Liden = −log(P (z = c|u))
(N)  where there are a total of I identities, z is the identity of the  person, and S is the softmax weight matrix while Sc and Sk represent the cth and kth column of it, respectively
 N
Experiments  N.N
Datasets and Evaluation Protocol  N.N.N Datasets  CUHK-SYSU: CUHK-SYSU [NN] is a large-scale person  search dataset with diverse scenes, containing NN,NNN images, N,NNN different persons, and NN,NNN annotated pedestrians bounding boxes
Each selected query person appears  in at least two images captured from different viewpoints
 The images present large variations in viewpoint, lighting,  resolution, occlusion and background, intensively reflecting  the real application scenarios and scene diversity
We use  the official training/test split provided by the dataset
The  training set contains NN,N0N images and N,NNN query persons
Within the testing set, the query set contains N,N00  persons and the gallery contains N,NNN images in total
 PRW: The PRW dataset [NN] is extracted from one N0-hour  video captured on a university campus
The dataset includes  NN,NNN video frames of scenes captured by N cameras
In  total NN,NNN frames are manually annotated, giving NN,NN0  pedestrian bounding boxes
Among them, NN,N0N pedestrians are annotated with NNN IDs
It provides N,NNN frames of  NNN different persons for training
The provided testing set  contains N,0NN query persons and a gallery of N,NNN images
 N.N.N Evaluation Protocol  We adopt the mean Averaged Precision (mAP) and the top-N  matching rate as performance metrics, which are also used  in OIM [NN] and [NN]
Using the mAP metric, person search  performance is evaluated in a similar way as detection, reflecting the accuracy of detecting the query person from  the gallery images
The top-N matching rate treats person  search as a ranking and localization problem
A matching is  counted if a bounding box among the top-N predicted boxes  overlaps with the ground truth larger than the threshold 0.N
 N.N
Implementation Details  In this paper, the Fully Convolutional Networks (FCN)  including both “ResN0 partN” and “ResN0 partN” are pretrained by using the architecture proposed in OIM [NN]
For  the input region at each time-step, we apply an ROI pooling  layer on the convN N convolutional features of it to normal- ize all the features to the same size of NN × NN × N0NN
For query person images, we also extract their NN × NN × N0NN  convolutional features in the same way
These features are  then fed into the NPSM architecture
In particular, within  NSN, the convolutional kernels for input-to-input states and  state-to-state transitions are fixed as N × N with N0NN chan- nels
At each recursive search step, we set the number C  of subregions covered by clustered proposals to N
We implement our network using Theano [NN] and Caffe [NN] deep  learning framework
The training of the NPSM converges in  roughly N0 hours for CUHK-SYSU dataset and N0 hours for  PRW dataset on on a machine with NNGB memory, NVIDIA  GeForce GTX TITAN X GPU and Intel iN-NNN0K Processor
The initial learning rate is 0.00N and decays at the rate  of 0.N for the weight updates of RMSProp [N0]
Additionally, we manually augment the data by performing random  ND translation
The speed of our method is close to realtime
For one gallery image, our model takes round Ns to  output a final searched result
However, overhead of ranking over gallery is dominating
For the CUHK-SYSU with  gallery size of N00, calculating cosine similarity between  search result from all the gallery images and query for ranking takes round N0s
For the PRW with N,NNN gallery images, ranking over gallery takes round NN minutes
 N.N
Ablation Study  In this subsection, we perform several analytic experiments on CUHK-SYSU benchmark to investigate the contribution of each component in our proposed NPSM architecture
We analyze attention prediction, contextual cue and  primitive memory of query person
In total we have three  variants by training the model based on different combinations of the above factors
And the gallery size is set to N00
 The details and corresponding results are shown in Table N
 As aforementioned, we employ the framework in  OIM [NN] which involves none of three factors, as the baseline
Based on this framework, the results of OIM [NN]  are obtained
In the method named “NPSM w/o C”, we  remove the “contextual cue and primitive memory integration” part (corresponding to Eqn
(N)) of the NSN in our  proposed NPSM
Instead, at each recursive step, we replace  the “contextual que and primitive memory integration” part  with a N × N × N0NN convolutional layer followed by the concatenation of the FCN (“ResN0 partN”) feature map of  the query person (primitive memory) and the current step  region (q and xt)
Moreover, for each recursive step, we  only keep the shrinking region generation method and the  attention score prediction model (Eqn
(N) and (N) ) to predict the attention score map
This setting makes our NPSM  a simple version without contextual cues involved but still  with the attention prediction ability
Furthermore, in the  method named “NPSM w/o A&C”, we further remove the  attention prediction model and only generate the shrinking  region as the input of each recursive step and add a N0NNdimension fully connected (FC) layer and a N-dimension  NNN    Table N
Results of ablation study on CUHK-SYSU dataset with  N00 gallery size setting
Legend: A: Attention prediction model,  C: Contextual cue, P: Primitive memory of query person
“w/o  A&C’ and “w/o C” are short for “without Attention prediction  model and Contextual cue” and “without Contextual cue but with  Attention prediction model” respectively
 A C P mAP(%) top-N(%)  OIM (Baseline) ✕ ✕ ✕ NN.N NN.N  NPSM w/o A&C ✕ ✕ ✓ NN.N NN.N  NPSM w/o C ✓ ✕ ✓ NN.N NN.N  NPSM ✓ ✓ ✓ NN.N NN.N  FC layer after the output (concatenation of the FCN feature map of query person (primitive memory) and the current region) of each recursive step
And the N-dimension FC  layer aims at predicting the score of each highlighted subregion from the parent region
From comparison between  the results of “OIM” and “NPSM w/o A&C”, we can see  that simply using primitive memory of query without contextual cues involved to search for a target person in the  recursive way can not achieve satisfactory results
From the  result of “NPSM w/o C” , we find that the sightly higher  performance is achieved than the baseline due to usage of  the attention model which can introduce more spatial location information than the “NPSM w/o A&C”
However,  both “NPSM w/o A&C” and “NPSM w/o C” lack a contextual cue memory mechanism
In other words, the above  methods are unable to memorize the context information  provided in a larger region through previous recursive steps
 From the result of “NPSM” overtaking the baseline method  “OIM” by N.N% and N.N% for mAP and top-N evaluation  protocol, we find that the neural search mechanism induced  by our proposed NPSM is beneficial for person search performance, and memory of query person can also effectively  guide the neural search model to find the right person
 N.N
Comparison with State-of-the-art Methods  We compare NPSM with several state-of-the-arts, including end-to-end person search ones proposed by Xiao  et al
[NN] and Zheng et al
[NN] and some other methods  combining commonly used pedestrian detectors (DPM [N],  ACF [N], CCF [NN], LDCF [NN] and their respective R-CNN  [N]) with hand-crafted features (BoW [N0], LOMO [NN],  DenseSIFT-ColorHist (DSIFT) [NN]) and distance metrics  (KISSME [NN], XQDA [NN])
 N.N.N Results on CUHK-SYSU  We report the person search performance on CUHK-SYSU  with N00 gallery size setting in Table N, where “CNN” represents the detector part (Faster-RCNN [NN] with ResNetN0) and “IDNet” denotes the re-identification part in the  framework of OIM [NN]
Compared with CNN+IDNet,  Figure N
Test mAPs of different approaches under different  gallery sizes
 the OIM achieves performance improvement by introducing  joint optimization of the detection and identification parts,  but still follows the isolated “detection+re-identification”  two-stage strategy in the person search process
Comparatively, our proposed NPSM is a detection-free method and  solves localization and re-identification of the query person simultaneously by introducing the query-aware region  shrinkage mechanism which can include more contextual  information beneficial for search accuracy
It can be verified by results shown in Table N
NPSM beats all compared  methods consistently for both the mAP and top-N metrics
 Table N
Comparison of NPSM’s performance on CUHK-SYSU  with N00 gallery size setting with the state-of-the-arts
 Method mAP(%) top-N(%)  ACF [N]+DSIFT [NN]+Euclidean NN.N NN.N  ACF+DSIFT+KISSME [NN] NN.N NN.N  ACF+BoW [N0]+Cosine NN.N NN.N  ACF+LOMO+XQDA [NN] NN.N NN.N  ACF+IDNet [NN] NN.N NN.0  CCF [NN]+DSIFT+Euclidean NN.N NN.N  CCF+DSIFT+KISSME NN.N NN.N  CCF+BoW+Cosine NN.N NN.N  CCF+LOMO+XQDA NN.N NN.N  CCF+IDNet N0.N NN.N  CNN [NN]+DSIFT+Euclidean NN.N NN.N  CNN+DSIFT+KISSME NN.N NN.N  CNN+BoW+Cosine NN.N NN.N  CNN+LOMO+XQDA NN.N NN.N  CNN+IDNet NN.N NN.N  OIM [NN](Baseline) NN.N NN.N  NPSM NN.N NN.N  Moreover, Figure N shows the mAP of the compared  methods with different gallery sizes, including [N0, N00,  N00, N000, N000, N000]
One can see that the mAP drops  gradually as the gallery size increases, but our method can  still outperform all other methods under different gallery  size settings
In particular, NPSM improves average performance per gallery size over OIM [NN] by around N%
 NNN    Ground truth Query image Attention maps  Figure N
Attention maps learned by our NPSM model for different testing person samples in CUHK-SYSU and PRW dataset
The first  three rows are from CUHK-SYSU, while the bottom row is from PRW
Green boxes represent the ground truth boxes while red boxes are  the region bounding boxes highlighted by our NPSM model
 N.N.N Results on PRW  On PRW dataset, we conduct experiments to compare  NPSM with some state-of-the-art methods combining  different detectors (respective R-CNN [N] detectors of  DPM [N], CCF [NN],ACF [N], LDCF [NN]) and recognizers  (LOMO, XQDA [NN], IDEdet, CWS [NN])
Among them,  AlexNet [NN] is exploited as the base network for the RCNN detector
Although VGGNet [NN] and ResNet [N] have  more parameters and are deeper than AlexNet, according  to [NN], AlexNet can achieve better performance than the  other two for DPM and ACF incorporating different recognizers
The results are shown in Table N
Because the  OIM method is the baseline of our NPSM, we implement  the source code provided in OIM [NN] to obtain the baseline  result on the PRW dataset
Compared with the result, our  NPSM outperforms it by N.N% and N.N% for mAP and top-N  accuracy separately
Besides, compared with all other stateof-the-arts considering five bounding boxes per gallery image, our method achieves better performance by only keeping one bounding box for testing per gallery image
 In Figure N, we visualize some attention maps produced  by our NPSM for testing samples from CUHK-SYSU and  PRW datasets, which are all ranked top N in search results
 The first three rows are from CUHK-SYSU, while the bottom row is from PRW
We observe that our NPSM can effectively shrink the search region to the correct person region  guided by primitive memory of the query person
 N
Conclusions  In this work, we introduced a novel neural person search  machine solving person search through recursively localizTable N
Comparison of NPSM’s performance on PRW with stateof-the-arts
 Method mAP(%) top-N(%)  DPM-Alex+LOMO+XQDA [NN] NN.0 NN.N  DPM-Alex+IDEdet [NN] N0.N NN.N  DPM-Alex+IDEdet+CWS [NN] N0.N NN.N  ACF-Alex+LOMO+XQDA N0.N N0.N  ACF-Alex+IDEdet NN.N NN.N  ACF-Alex+IDEdet+CWS NN.N NN.N  LDCF+LOMO+XQDA NN.0 NN.N  LDCF+IDEdet NN.N NN.N  LDCF+IDEdet+CWS NN.N NN.N  OIM(Baseline) NN.N NN.N  NPSM NN.N NN.N  ing effective regions, with guidance from the memory of the  query person
Extensive experiments on two public benchmarks demonstrated its superiority over state-of-the-arts in  most cases and the benefit to recognition accuracy in person  search
 Acknowledgment  This work was supported in part by the National Natural Science Foundation of China under Grant NNNNNNNN,  Grant NNNNNNN0, and Grant NNNNN00N
The work of Jiashi  Feng was partially supported by NUS startup R-NNN-000C0N-NNN, MOE Tier-I R-NNN-000-CNN-NNN and IDS R-NNN000-CNN-NNN
 N00    References  [N] E
Ahmed, M
Jones, and T
K
Marks
An improved deep learning  architecture for person re-identification
In IEEE CVPR, pages NN0N–  NNNN, N0NN
 [N] J
R
Anderson
Cognitive psychology and its implications
WH  Freeman/Times Books/Henry Holt & Co, NNN0
 [N] Z
Cai, M
Saberian, and N
Vasconcelos
Learning complexityaware cascades for deep pedestrian detection
In IEEE CVPR, pages  NNNN–NNNN, N0NN
 [N] P
Dollár, R
Appel, S
Belongie, and P
Perona
Fast feature pyramids  for object detection
IEEE TPAMI, NN(N):NNNN–NNNN, N0NN
 [N] M
Farenzena, L
Bazzani, A
Perina, V
Murino, and M
Cristani
 Person re-identification by symmetry-driven accumulation of local  features
In IEEE CVPR, pages NNN0–NNNN
IEEE, N0N0
 [N] P
F
Felzenszwalb, R
B
Girshick, D
McAllester, and D
Ramanan
 Object detection with discriminatively trained part-based models
 IEEE TPAMI, NN(N):NNNN–NNNN, N0N0
 [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Region-based convolutional networks for accurate object detection and segmentation
 IEEE TPAMI, NN(N):NNN–NNN, N0NN
 [N] D
Gray and H
Tao
Viewpoint invariant pedestrian recognition  with an ensemble of localized features
In ECCV, pages NNN–NNN
 Springer, N00N
 [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for  image recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages NN0–NNN, N0NN
 [N0] S
Hochreiter and J
Schmidhuber
Long short-term memory
Neural  computation, N(N):NNNN–NNN0, NNNN
 [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick,  S
Guadarrama, and T
Darrell
Caffe: Convolutional architecture for  fast feature embedding
arXiv preprint arXiv:NN0N.N0NN, N0NN
 [NN] M
Köstinger, M
Hirzer, P
Wohlhart, P
M
Roth, and H
Bischof
 Large scale metric learning from equivalence constraints
In IEEE  CVPR, pages NNNN–NNNN, N0NN
 [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet classification with deep convolutional neural networks
In NIPS, pages N0NN–  NN0N, N0NN
 [NN] W
Li, R
Zhao, T
Xiao, and X
Wang
Deepreid: Deep filter pairing  neural network for person re-identification
In IEEE CVPR, pages  NNN–NNN, N0NN
 [NN] X
Li, W.-S
Zheng, X
Wang, T
Xiang, and S
Gong
Multi-scale  learning for low-resolution person re-identification
In IEEE ICCV,  pages NNNN–NNNN, N0NN
 [NN] Z
Li, E
Gavves, M
Jain, and C
G
Snoek
Videolstm convolves, attends and flows for action recognition
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [NN] S
Liao, Y
Hu, X
Zhu, and S
Z
Li
Person re-identification by local maximal occurrence representation and metric learning
In IEEE  CVPR, pages NNNN–NN0N, N0NN
 [NN] H
Liu, J
Feng, M
Qi, J
Jiang, and S
Yan
End-to-end comparative attention networks for person re-identification
IEEE TIP,  NN(N):NNNN–NN0N, N0NN
 [NN] H
Liu, Z
Jie, K
Jayashree, M
Qi, J
Jiang, S
Yan, and J
Feng
 Video-based person re-identification with accumulative motion context
arXiv preprint arXiv:NN0N.00NNN, N0NN
 [N0] H
Liu, M
Qi, and J
Jiang
Kernelized relaxed margin components  analysis for person re-identification
IEEE Signal Processing Letters,  NN(N):NN0–NNN, N0NN
 [NN] W
Nam, P
Dollar, and J
H
Han
Local decorrelation for improved  pedestrian detection
NIPS, N:NNN–NNN, N0NN
 [NN] J
Niño-Castañeda, A
Frı́as-Velázquez, N
B
Bo, M
Slembrouck,  J
Guan, G
Debard, B
Vanrumste, T
Tuytelaars, and W
Philips
 Scalable semi-automatic annotation for multi-camera person tracking
IEEE TIP, NN(N):NNNN–NNNN, N0NN
 [NN] B
A
Olshausen, C
H
Anderson, and D
C
Van Essen
A neurobiological model of visual attention and invariant pattern recognition  based on dynamic routing of information
Journal of Neuroscience  the Official Journal of the Society for Neuroscience, NN(NN):NN00–NN,  NNNN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards realtime object detection with region proposal networks
In NIPS, pages  NN–NN, N0NN
 [NN] S
Sharma, R
Kiros, and R
Salakhutdinov
Action recognition using  visual attention
arXiv preprint arXiv:NNNN.0NNNN, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks  for large-scale image recognition
arXiv preprint arXiv:NN0N.NNNN,  N0NN
 [NN] D
Tao, Y
Guo, M
Song, Y
Li, Z
Yu, and Y
Y
Tang
Person reidentification by dual-regularized kiss metric learning
IEEE TIP,  NN(N):NNNN–NNNN, N0NN
 [NN] Theano Development Team
Theano: A Python framework for  fast computation of mathematical expressions
arXiv e-prints,  abs/NN0N.0NNNN, May N0NN
 [NN] Y
Tian, P
Luo, X
Wang, and X
Tang
Deep learning strong parts  for pedestrian detection
In IEEE ICCV, pages NN0N–NNNN, N0NN
 [N0] T
Tieleman and G
Hinton
Lecture N.N-rmsprop: Divide the gradient  by a running average of its recent magnitude
COURSERA: Neural  Networks for Machine Learning, N(N), N0NN
 [NN] L
Wu, C
Shen, and A
v
d
Hengel
Personnet: Person reidentification with deep convolutional neural networks
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
 [NN] T
Xiao, H
Li, W
Ouyang, and X
Wang
Learning deep feature representations with domain guided dropout for person re-identification
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] T
Xiao, S
Li, B
Wang, L
Lin, and X
Wang
Joint detection and  identification feature learning for person search
arXiv:NN0N.0NNN0,  N0NN
 [NN] S
Xingjian, Z
Chen, H
Wang, D.-Y
Yeung, W.-k
Wong, and W.-c
 Woo
Convolutional lstm network: A machine learning approach for  precipitation nowcasting
In NIPS, pages N0N–NN0, N0NN
 [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
Courville, R
Salakhudinov,  R
Zemel, and Y
Bengio
Show, attend and tell: Neural image caption generation with visual attention
In IEEE ICCV, pages N0NN–  N0NN, N0NN
 [NN] B
Yang, J
Yan, Z
Lei, and S
Z
Li
Convolutional channel features
 In IEEE ICCV, pages NN–N0, N0NN
 [NN] Y
Yang, S
Liao, Z
Lei, and S
Z
Li
Large scale similarity learning  using similar pairs for person verification
In AAAI, N0NN
 [NN] L
Zhang, T
Xiang, and S
Gong
Learning a discriminative null  space for person re-identification
arXiv preprint arXiv:NN0N.0NNNN,  N0NN
 [NN] R
Zhao, W
Ouyang, and X
Wang
Unsupervised salience learning  for person re-identification
In IEEE CVPR, pages NNNN–NNNN, N0NN
 [N0] L
Zheng, L
Shen, L
Tian, S
Wang, J
Wang, and Q
Tian
Scalable  person re-identification: A benchmark
In IEEE ICCV, pages NNNN–  NNNN, N0NN
 [NN] L
Zheng, H
Zhang, S
Sun, M
Chandraker, and Q
Tian
Person  re-identification in the wild
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object proposals  from edges
In ECCV, pages NNN–N0N
Springer, N0NN
 N0NVisual Semantic Planning Using Deep Successor Representations   Visual Semantic Planning using Deep Successor Representations  Yuke Zhu∗N Daniel Gordon∗∗N Eric KolveN Dieter FoxN  Li Fei-FeiN Abhinav GuptaN,N Roozbeh MottaghiN Ali FarhadiN,N  NAllen Institute for Artificial Intelligence NCarnegie Mellon University NStanford University NUniversity of Washington  Abstract  A crucial capability of real-world intelligent agents is  their ability to plan a sequence of actions to achieve their  goals in the visual world
In this work, we address the problem of visual semantic planning: the task of predicting a  sequence of actions from visual observations that transform  a dynamic environment from an initial state to a goal state
 Doing so entails knowledge about objects and their affordances, as well as actions and their preconditions and effects
We propose learning these through interacting with a  visual and dynamic environment
Our proposed solution involves bootstrapping reinforcement learning with imitation  learning
To ensure cross task generalization, we develop a  deep predictive model based on successor representations
 Our experimental results show near optimal results across a  wide range of tasks in the challenging THOR environment
 N
Introduction  Humans demonstrate levels of visual understanding that  go well beyond current formulations of mainstream vision  tasks (e.g
object detection, scene recognition, image segmentation)
A key element to visual intelligence is the ability to interact with the environment and plan a sequence of  actions to achieve specific goals; This, in fact, is central to  the survival of agents in dynamic environments [N, NN]
 Visual semantic planning, the task of interacting with  a visual world and predicting a sequence of actions that  achieves a desired goal, involves addressing several challenging problems
For example, imagine the simple task  of putting the bowl in the microwave in the  visual dynamic environment depicted in Figure N
A successful plan involves first finding the bowl, navigating to it,  then grabbing it, followed by finding and navigating to the  microwave, opening the microwave, and finally putting the  bowl in the microwave
 The first challenge in visual planning is that performing  ∗indicates equal contribution
 N  navigate to bowl  N  N  p ic  k  u  p  b  o w  l  navigate to microwave  open microwave  N  put bowl in microwave  Initial State  Overhead view of Visual Dynamic Environment Task: Put bowl in microwave  N  Figure N
Given a task and an initial configuration of a scene, our  agent learns to interact with the scene and predict a sequence of  actions to achieve the goal based on visual inputs
 each of the above actions in a visual dynamic environment  requires deep visual understanding of that environment, including the set of possible actions, their preconditions and  effects, and object affordances
For example, to open a microwave an agent needs to know that it should be in front  of the microwave, and it should be aware of the state of  the microwave and not try to open an already opened microwave
Long explorations that are required for some tasks  imposes the second challenge
The variability of visual observations and possible actions makes naı̈ve exploration intractable
To find a cup, the agent might need to search several cabinets one by one
The third challenge is emitting a  sequence of actions such that the agent ends in the goal state  and the effects of the preceding actions meet the preconditions of the proceeding ones
Finally, a satisfactory solution  to visual planning should enable cross task transfer; previous knowledge about one task should make it easier to learn  the next one
This is the fourth challenge
 In this paper, we address visual semantic planning as a  policy learning problem
We mainly focus on high-level  actions and do not take into account the low-level details of  motor control and motion planning
Visual Semantic Planning (VSP) is the task of predicting a sequence of semantic  actions that moves an agent from a random initial state in a  NNN    visual dynamic environment to a given goal state
 To address the first challenge, one needs to find a way  to represent the required knowledge of objects, actions,  and the visual environment
One possible way is to learn  these from still images or videos [NN, NN, NN]
But we argue that learning high-level knowledge about actions and  their preconditions and effects requires an active and prolonged interaction with the environment
In this paper, we  take an interaction-centric approach where we learn this  knowledge through interacting with the visual dynamic environment
Learning by interaction on real robots has limited scalability due to the complexity and cost of robotics  systems [NN, N0, NN]
A common treatment is to use  simulation as mental rehearsal before real-world deployment [N, NN, NN, NN, NN]
For this purpose, we use the  THOR framework [NN], extending it to enable interactions  with objects, where an action is specified as its pre- and  post-conditions in a formal language
 To address the second and third challenges, we cast VSP  as a policy learning problem, typically tackled by reinforcement learning [NN, NN, NN, N0, NN, NN]
To deal with the large  action space and delayed rewards, we use imitation learning to bootstrap reinforcement learning and to guide exploration
To address the fourth challenge of cross task generalization [NN], we develop a deep predictive model based  on successor representations [N, NN] that decouple environment dynamics and task rewards, such that knowledge from  trained tasks can be transferred to new tasks with theoretical  guarantees [N]
 In summary, we address the problem of visual semantic  planning and propose an interaction-centric solution
Our  proposed model obtains near optimal results across a spectrum of tasks in the challenging THOR environment
Our  results also show that our deep successor representation offers crucial transferability properties
Finally, our qualitative results show that our learned representation can encode  visual knowledge of objects, actions, and environments
 N
Related Work  Task planning
Task-level planning [N0, NN, N0, NN, NN]  addresses the problem of finding a high-level plan for performing a task
These methods typically work with highlevel formal languages and low-dimensional state spaces
In  contrast, visual semantic planning is particularly challenging due to the high dimensionality and partial observability  of visual input
In addition, our method facilitates generalization across tasks, while previous methods are typically  designed for a specific environment and task
 Perception and interaction
Our work integrates perception and interaction, where an agent actively interfaces with  the environment to learn policies that map pixels to actions
 The synergy between perception and interaction has drawn  an increasing interest in the vision and robotics community
 Recent work has enabled faster learning and produced more  robust visual representations [N, NN, NN] through interaction
 Some early successes have been shown in physical understanding [N, NN, NN, NN] by interacting with an environment
 Deep reinforcement learning
Recent work in reinforcement learning has started to exploit the power of deep neural networks
Deep RL methods have shown success in several domains such as video games [NN], board games [NN],  and continuous control [N0]
Model-free RL methods (e.g.,  [N0, NN, NN]) aim at learning to behave solely from actions and their environment feedback; while model-based  RL approaches (e.g., [N, NN, N0]) also estimate a environment model
Successor representation (SR), proposed  by Dayan [N], can be considered as a hybrid approach of  model-based and model-free RL
Barreto et al
[N] derived  a bound on value functions of an optimal policy when transferring policies using successor representations
Kulkarni et  al
[NN] proposed a method to learn deep successor features
 In this work, we propose a new SR architecture with significantly reduced parameters, especially in large action spaces,  to facilitate model convergence
We propose to first train  the model with imitation learning and fine-tune with RL
It  enables us to perform more realistic tasks and offers significant benefits for transfer learning to new tasks
 Learning from demonstrations
Expert demonstrations  offer a source of supervision in tasks which must usually be  learned with copious random exploration
A line of work interleaves policy execution and learning from expert demonstration that has achieved good practical results [N, NN]
 Recent works have employed a series of new techniques  for imitation learning, such as generative adversarial networks [NN, NN], Monte Carlo tree search [NN] and guided  policy search [NN], which learn end-to-end policies from  pixels to actions
 Synthetic data for visual tasks
Computer games and  simulated platforms have been used for training perceptual tasks, such as semantic segmentation [NN], pedestrian detection [NN], pose estimation [NN], and urban driving [N, NN, NN, NN]
In robotics, there is a long history of  using simulated environments for learning and testing before real-world deployment [NN]
Several interactive platforms have been proposed for learning control with visual  inputs [N, NN, NN, NN, NN]
Among these, THOR [NN] provides high-quality realistic indoor scenes
Our work extends THOR with a new set of actions and the integration  of a planner
 N
Interactive Framework  To enable interactions with objects and with the environment, we extend the THOR framework [NN], which has been  used for learning visual navigation tasks
Our extension inNNN    Look Down  Pick Up egg  Open microwave  Navigate to microwave  Close  fridge  Put tomato  Before After Before After  Look Up  Figure N
Example images that demonstrate the state changes before and after an object interaction from each of the six action  types in our framework
Each action changes the visual state and  certain actions may enable further interactions such as opening the  fridge before taking an object from it
 cludes new object states, and a discrete description of the  scene in a planning language [NN]
 N.N
Scenes  In this work, we focus on kitchen scenes, as they allow  for a variety of tasks with objects from many categories
 Our extended THOR framework consists of N0 individual  kitchen scenes
Each scene contains an average of NN distinct objects with which the agent can interact
The scenes  are developed using the Unity ND game engine
 N.N
Objects and Actions  We categorize the objects by their affordances [NN], i.e.,  the plausible set of actions that can be performed
For the  tasks of interest, we focus on two types of objects: N) items  that are small objects (mug, apple, etc.) which can be picked  up, held, and moved by the agent to various locations in the  scene, and N) receptacles that are large objects (table, sink,  etc.) which are stationary and can hold a fixed capacity of  items
A subset of receptacles, such as fridges and cabinets,  are containers
These containers have doors that can be  opened and closed
The agent can only put an item in a  container when it is open
We assume that the agent can  hold at most one item at any point
We further define the  following actions to interact with the objects:  N
Navigate {receptacle}: moving from the current lo- cation of the agent to a location near the receptacle;  N
Open {container}: opening the door of a container in front of an agent;  N
Close {container}: closing the door of a container in front of an agent;  N
Pick Up {item}: picking up an item in field of view; N
Put {receptacle}: putting a held item in the receptacle; N
Look Up and Look Down: tilting the agent’s gaze N0  degrees up or down
 In summary, we have six action types, each taking a corresponding type of action arguments
The combination of  actions and arguments results in a large action set of N0 per  scene on average
Fig
N illustrates example scenes and the  six types of actions in our framework
Our definition of action space makes two important abstractions to make learning tractable: N) it abstracts away from navigation, which  can be tackled by a subroutine using existing methods such  as [NN]; and N) it allows the model to learn with semantic actions, abstracting away from continuous motions, e.g.,  the physical movement of a robot arm to grasp an object
A  common treatment for this abstraction is to “fill in the gaps”  between semantic actions with callouts to a continuous motion planner [N0, NN]
It is evident that not all actions can be  performed in every situation
For example, the agent cannot pick up an item when it is out of sight, or put a tomato  into fridge when the fridge door is closed
To address these  requirements, we specify the pre-conditions and effects of  actions
Next we introduce an approach to declaring them  as logical rules in a planning language
These rules are only  encoded in the environment but not exposed to the agent
 Hence, the agent must learn them through interaction
 N.N
Planning Language  The problem of generating a sequence of actions that  leads to the goal state has been formally studied in the field  of automated planning [NN]
Planning languages offer a  standard way of expressing an automated planning problem  instance, which can be solved by an off-the-shelf planner
 We use STRIPS [NN] as the planning language to describe  our visual planning problem
 In STRIPS, a planning problem is composed of a description of an initial state, a specification of the goal  state(s), and a set of actions
In visual planning, the initial  state corresponds to the initial configuration of the scene
 The specification of the goal state is a boolean function that  returns true on states where the task is completed
Each action is defined by its precondition (conditions that must be  satisfied before the action is performed) and postcondition  (changes caused by the action)
The STRIPS formulation  enables us to define the rules of the scene, such as object  affordances and causality of actions
 N
Our Approach  We first outline the basics of policy learning in Sec
N.N
 Next we formulate the visual semantic planning problem as  a policy learning problem and describe our model based on  successor representation
Later we propose two protocols  of training this model using imitation learning (IL) and reinforcement learning (RL)
To this end, we use IL to bootstrap  our model and use RL to further improve its performance
 N.N
Successor Representation  We formulate the agent’s interactions with an environment as a Markov Decision Process (MDP), which can be  specified by a tuple (S,A, p, r, γ)
S and A are the sets of  NNN    openaction typeone-hot  visual observation  s  a cabinetaction argumentone-hot  a,s  a,s  w  ra,s  Qa,s  immediate reward  Q value  : dot product  : convolution layer  : fully-connected layer  internal state one-hot inventory  state-action feature  successor feature  reward  predictor   vector  Figure N
An overview of the network architecture of our successor representation (SR) model
Our network takes in the current state as  well as a specific action and predicts an immediate reward ra,s as well as a discounted future reward Qa,s, performing this evaluation for  each action
The learned policy π takes the argmax over all Q values as its chosen action
 states and actions
For s ∈ S and a ∈ A, p(s0|s, a) de- fines the probability of transiting from the state s to the next  state s0 ∈ S by taking action a
r(s, a) is a real-value func- tion that defines the expected immediate reward of taking  action a in state s
For a state-action trajectory, we define  the future discounted return R = PN  i=0 γ ir(si, ai), where  γ ∈ [0, N] is called the discount factor, which trades off the importance of immediate rewards versus future rewards
 A policy ⇡ : S → A defines a mapping from states to actions
The goal of policy learning is to find the optimal policy ⇡⇤ that maximizes the future discounted return R starting from state s0 and following the policy ⇡ ⇤
 Instead of directly optimizing a parameterized policy, we  take a value-based approach
We define a state-action value  function Qπ : S ×A → R under a policy ⇡ as  Qπ(s, a) = Eπ[R|s0 = s, a0 = a], (N)  i.e., the expected episode return starting from state s, taking  action a, and following policy ⇡
The Q value of the optimal  policy ⇡⇤ obeys the Bellman equation [NN]:  Qπ ⇤  (s, a) = Eπ ⇤  [r(s, a) + γmax a0  Q(s0, a0)] (N)  In deep Q networks [NN], Q functions are approximated by a  neural networkQ(s, a|✓), and can be trained by minimizing the `N-distance between both sides of the Bellman equation  in Eq
(N)
Once we learn Qπ ⇤  , the optimal action at state s  can be selected by a⇤ = argmaxaQ π⇤(s, a)
 Successor representation (SR), proposed by Dayan [N],  uses a similar value-based formulation for policy learning
It differs from traditional Q learning by factoring the  value function into a dot product of two components: a reward predictor vector w and a predictive successor feature   (s, a)
To derive the SR formulation, we start by factoring the immediate rewards such that  r(s, a) = φ(s, a)Tw, (N)  where φ(s, a) is a state-action feature
We expand Eq
(N) using this reward factorization:  Qπ(s, a) = Eπ[  NX  i=0  γir(si, ai)|s0 = s, a0 = a]  = Eπ[ NX  i=0  γiφ(si, ai) T w|s0 = s, a0 = a]  = Eπ[ NX  i=0  γiφ(si, ai)|s0 = s, a0 = a] T w  =  π(s, a)Tw (N)  We refer to  (s, a)π = Eπ[ PN  i=0 γ iφs,a|s0 = s, a0 = a] as  the successor features of the pair (s, a) under policy ⇡
Intuitively, the successor feature  π(s, a) summarizes  the environment dynamics under a policy ⇡ in a state-action  feature space, which can be interpreted as the expected future “feature occupancy”
The reward predictor vector w  induces the structure of the reward functions, which can be  considered as an embedding of a task
Such decompositions  have been shown to offer several advantages, such as being  adaptive to changes in distal rewards and apt to option discovery [NN]
A theoretical result derived by Barreto et al
 implies a bound on performance guarantee when the agent  NNN    transfers a policy from a task t to a similar task t0, where  task similarity is determined by the `N-distance of the corresponding w vectors between these two tasks t and t0 [N]
 Successor representation thus provides a generic framework  for policy transfer in reinforcement learning
 N.N
Our Model  We formulate the problem of visual semantic planning  as a policy learning problem
Formally, we denote a task by  a Boolean function t : S → {0, N}, where a state s com- pletes the task t iff t(s) = N
The goal is to find an optimal policy ⇡⇤, such that given an initial state s0, ⇡  ⇤ generates  a state-action trajectory T = {(si, ai) | i = 0 


T} that  maximizes the sum of immediate rewards PT−N  i=0 r(si, ai), where t(s0...T−N) = 0 and t(sT ) = N
 We parameterize such a policy using the successor representation (SR) model from the previous section
We develop a new neural network architecture to learn φ,  and  w
The network architecture is illustrated in Fig
N
In  THOR, the agent’s observations come from a first-person  RGB camera
We also pass the agent’s internal state as  input, expressed by one-hot encodings of the held object  in its inventory
The action space is described in Sec
N.N
 We start by computing embedding vectors for the states and  the actions
The image is passed through a N-layer convolutional encoder, and the internal state through a N-layer  MLP, producing a state embedding µs = f(s; ✓cnn, ✓int)
The action a = [atype, aarg] is encoded as one-hot vec- tors and passed through a N-layer MLP encoder that produces an action embedding µa = g(atype, aarg; ✓mlp)
We fuse the state and action embeddings and generate the stateaction feature φs,a = h(µs, µa; ✓r) and the successor fea- ture  s,a = m(µs, µa; ✓q) in two branches
The network predicts the immediate reward rs,a = φ  T s,aw and the Q  value under the current policy Qs,a =   T s,aw using the decomposition from Eq
(N) and (N)
 N.N
Imitation Learning  Our SR-based policy can be learned in two fashions
 First, it can be trained by imitation learning (IL) under the  supervision of the trajectories of an optimal planner
Second, it can be learned by trial and error using reinforcement learning (RL)
In practice, we find that the large action  space in THOR makes RL from scratch intractable due to  the challenge of exploration
The best model performance is  produced by IL bootstrapping followed by RL fine-tuning
 Given a task, we generate a state-action trajectory:  T = {(s0, a0), {(sN, aN), 


, (sT−N, aT−N), (sT , ∅)} (N)  using the planner from the initial state-action pair (s0, a0)  to the goal state sT (no action is performed at terminal  states)
This trajectory is generated on a low-dimensional  Input Remapping  From planner trajectory to training data  {(s0,a0), (sN,aN), (sN,aN), …,  (sT,∅)}  {(        , a0), (        , aN), (        , aN), …, (        , ∅)}sT  sN  sN  input remapping  Action:   open (microwave)  Preconditions:   near (microwave), is_closed (microwave)  Postconditions:   is_open (microwave)  s0  sN  a0  Planning  Figure N
We use a planner to generate a trajectory from an initial state-action pair (s0, a0) to a goal state sT 
We describe each scene in a STRIPS-based planning language, where actions are  specified by their pre- and post-conditions (see Sec
N.N)
We perform input remapping, illustrated in the blue box, to obtain the  image-action pairs from the trajectory as training data
After performing an action, we update the plan and repeat
 state representation in the STRIPS planner (Sec
N.N)
Each  low-dimensional state corresponds to an RGB image, i.e.,  the agent’s visual observation
During training, we perform  input remapping to supervise the model with image-action  pairs rather than feeding the low-dimensional planner states  to the network
To fully explore the state space, we take  planner actions as well as random actions off the optimal  plan
After each action, we recompute the trajectory
This  process of generating training data from a planner is illustrated in Fig
N
Each state-action pair is associated with a  true immediate reward r̂s,a
We use the mean squared loss  function to minimize the error of reward prediction:  Lr = N  T  T−NX  i=0  (r̂s,a − φ T s,aw)  N
(N)  Following the REINFORCE rule [NN], we use the discounted return along the trajectory T as an unbiased esti- mate of the true Q value: Q̂s,a ≈  PT−N i=0 γ  ir̂s,a
We use the  mean squared loss to minimize the error of Q prediction:  LQ = (Q̂s,a −   T s,aw)  N (N)  The final loss on the planner trajectory T is the sum of the reward loss and the Q loss: LT = Lr +LQ
Using this loss signal, we train the whole SR network on a large collection  of planner trajectories starting from random initial states
 N.N
Reinforcement Learning  When training our SR model using RL, we can still use  the mean squared loss in Eq
(N) to supervise the learning  of reward prediction branch for φ and w
However, in absence of expert trajectories, we would need an iterative way  to learn the successor features  
Rewriting the Bellman  equation in Eq
(N) with the SR factorization, we can obtain  an equality on φ and  :   π ⇤  (s, a) = Eπ ⇤  [φ(s, a) + γ (s0, a0)] (N)  NNN    where a0 = argmaxa  (s 0, a)Tw
Similar to DQN [NN],  we minimize the `N-loss between both sides of Eq
(N):  LSR = E π[(φs,a + γ s0,a0 −    π s,a)  N] (N)  We use a similar procedure to Kulkarni et al
[NN] to train  our SR model
The model alternates training between the  reward branch and the SR branch
At each iteration, a minibatch is randomly drawn from a replay buffer of past experiences [NN] to perform one SGD update
 N.N
Transfer with Successor Features  A major advantage of successor features is its ability  to transfer across tasks by exploiting the structure shared  by the tasks
Given a fixed state-action representation φ,  let Mφ be the set of all possible MDPs induced by φ and  all instantiations of the reward prediction vectors w
Assume that ⇡⇤i is the optimal policy of the i-th task in the set  {Mi ∈ Mφ|i = N, 


n}
Let Mn+N to be a new task
We  denote Q π⇤ i  n+N as the value function of executing the optimal  policy of the task Mi on the new task Mn+N, and Q̃ π⇤ i  n+N as  an approximation ofQ π⇤ i  n+N by our SR model
Given a bound  on the approximations such that  |Q π⇤ i  n+N(s, a)−Q̃ π⇤ i  n+N(s, a)| ≤ ✏ ∀s ∈ S, a ∈ A, i = N, 


, n,  we define a policy ⇡0 for the new task Mn+N using Q̃N,...,n,  where ⇡0(s) = argmaxa maxi Q̃ π⇤ i  n+N(s, a)
Theorem N in Barreto et al
[N] implies a bound of the gap between value  functions of the optimal policy ⇡⇤n+N and the policy ⇡ 0:  Q π⇤ n+N  n+N (s, a)−Q π0  n+N(s, a) ≤ Nφm N− γ  (min i  ||wi−wn+N||+✏),  where φm = maxs,a ||φ(s, a)||
This result serves the the- oretical foundation of policy transfer in our SR model
In  practice, when transferring to a new task while the scene  dynamics remain the same, we freeze all model parameters  except the single vector w
This way, the policy of the new  task can be learned with substantially higher sample efficiency than training a new network from scratch
 N.N
Implementation Details  We feed a history of the past four observations, converted  to grayscale, to account for the agent’s motions
We use a  time cost of −0.0N to encourage shorter plans and a task completion reward of N0.0
We train our model with imita- tion learning for N00k iterations with a batch size of NN, and  a learning rate of Ne-N
We also include the successor loss in  Eq
(N) during imitation learning, which helps learn better  successor features
We subsequently fine-tune the network  with reinforcement learning with N0,000 episodes
 N
Experiments  We evaluate our model using the extended THOR framework on a variety of household tasks
We compare our  method against standard reinforcement learning techniques  as well as with non-successor based deep models
The tasks  compare the different methods’ abilities to learn across  varying time horizons
We also demonstrate the SR network’s ability to efficiently adapt to new tasks
Finally, we  show that our model can learn a notion of object affordance  by interacting with the scene
 N.N
Quantitative Evaluation  We examine the effectiveness of our model and baseline  methods on a set of tasks that require three levels of planning complexity in terms of optimal plan length
 Experiment Setup We explore the two training protocols  introduced in Sec
N to train our SR model:  N
RL: we train the model solely based on trial and error,  and learn the model parameters with RL update rules
 N
IL: we use the planner to generate optimal trajectories  starting from a large collection of random initial stateaction pairs
We use the imitation learning methods to  train the networks using supervised losses
 Empirically, we find that training with reinforcement  learning from scratch cannot handle the large action space
 Thus, we report the performance of our SR model trained  with imitation learning (SR IL) as well as with additional  reinforcement learning fine-tuning (SR IL + RL)
 We compare our SR model with the state-of-the-art deep  RL model, ANC [NN], which is an advantage-based actorcritic method that allows the agent to learn from multiple  copies of simulation while updating a single model in an  asynchronous fashion
ANC establishes a strong baseline  for reinforcement learning
We further use the same architecture to obtain two imitation learning (behavior cloning)  baselines
We use the same ANC network structure to train  a softmax classifier that predicts the planner actions given  an input
The network predicts both the action types (e.g.,  Put) and the action arguments (e.g., apple)
We call this  baseline CLS-MLP
We also investigate the role of memory  in these models
To do this, we add an extra LSTM layer to  the network before action outputs, called CLS-LSTM
We  also include simple agents that take random actions and take  random valid actions at each time step
 Levels of task difficulty We evaluate all of the models  with three levels of task difficulty based on the length of the  optimal plans and the source of randomization:  N
Level N (Easy): Navigate to a container and  toggle its state
A sample task would be go to  NNN    Easy Medium Hard  Success Rate Mean (σ) Episode Length Success Rate Mean (σ) Episode Length Success Rate Mean (σ) Episode Length  Random Action N.00 NNN.NN (NNN.NN) 0.00 - 0.0N NNNN.0N (NNN.NN)  Random Valid Action N.00 NN.0N (NN.0N) 0.0N NNNN.N0 (NNN.N0) 0.NN NNNN.NN (NN0N.NN)  ANC [NN] 0.NN N0N.NN (NNN.0N) 0.00 - 0.0N NNNN.NN (NNN0.N0)  CLS-MLP N.00 N.NN (0.N0) 0.NN NNN.NN (N00.NN) 0.NN NNN.NN (N0N.NN)  CLS-LSTM N.00 N.NN (0.NN) 0.N0 NNN.0N (N0N.NN) 0.NN NNN.NN (NNN.N0)  SR IL (ours) N.00 N.N0 (N.0N) 0.N0 NN.NN (NN.NN) 0.NN NN.NN (NN.NN)  SR IL + RL (ours) N.00 N.NN (N.0N) 0.N0 NN.NN (N.NN) - Optimal planner N.00 N.NN (N.0N) N.00 NN.N0 (N.NN) N.00 NN.NN (N.0N)  Table N
Results of evaluating the model on the easy, medium, and hard tasks
For each task, we evaluate how many out of the N00 episodes  were completed (success rate) and the mean and standard deviation for successful episode lengths
The numbers in parentheses show the  standard deviations
We do not fine-tune our SR IL model for the hard task
 the microwave and open it if it is  closed, close it otherwise
The initial  location of the agent and all container states are  randomized
This task requires identifying object  states and reasoning about action preconditions
 N
Level N (Medium): Navigate to multiple receptacles,  collect items, and deposit them in a receptacle
A  sample task here is pick up three mugs from  three cabinets and put them in the  sink
Here we randomize the agent’s initial location,  while the item locations are fixed
This task requires a  long trajectory of correct actions to complete the goal
 N
Level N (Hard): Search for an item and put it in a  receptacle
An example task is find the apple  and put it in the fridge
We randomize  the agent’s location as well as the location of all items
 This task is especially difficult as it requires longerterm memory to account for partial observability, such  as which cabinets have previously been checked
 We evaluate all of the models on N0 easy tasks, N medium  tasks, and N hard tasks, each across N00 episodes
Each  episode terminates when a goal state is reached
We consider an episode fails if it does not reach any goal state  within N,000 actions
We report the episode success rate  and mean episode length as the performance metrics
We  exclude these failed episodes in the mean episode length  metric
For the easy and medium tasks, we train the imitation learning models to mimic the optimal plans
However  for the hard tasks, imitating the optimal plan is infeasible, as  the location of the object is uncertain
In this case, the target object is likely to hide in a cabinet or a fridge which the  agent cannot see
Therefore, we train the models to imitate  a plan which searches for the object from all the receptacles  in a fixed order
For the same reason, we do not perform RL  fine-tuning for the hard tasks
 Table N summarizes the results of these experiments
 Pure RL-based methods struggle with the medium and hard  tasks because the action space is so large that naı̈ve exploration rarely, if ever, succeeds
Comparing CLS-MLP and  CLS-LSTM, adding memory to the agent helps improving  Figure N
We compare updating w with retraining the whole network for new hard tasks in the same scene
By using successor  features, we can quickly learn an accurate policy for the new item
 Bar charts correspond to the episode success rates, and line plots  correspond to successful action rate
 success rate on medium tasks as well as completing tasks  with shorter trajectories in hard tasks
Overall, the SR methods outperform the baselines across all three task difficulties
Fine-tuning the SR IL model with reinforcement learning further reduces the number of steps towards the goal
 More qualitative results can be found in the video.N  N.N
Task Transfer  One major benefit of the successor representation decomposition is its ability to transfer to new tasks while only  retraining the reward prediction vector w, while freezing  the successor features
We examine the sample efficiency of  adapting a trained SR model on multiple novel tasks in the  same scene
We examine policy transfer in the hard tasks,  as the scene dynamics of the searching policy retains, even  when the objects to be searched vary
We evaluate the speed  at which the SR model converges on a new task by finetuning the w vector versus training the model from scratch
 We take a policy for searching a bowl in the scene and substituting four new items (lettuce, egg, container, and apple)  in each new task
Fig
N shows the episode success rates  (bar chart) and the successful action rate (line plot)
By fineNLink to supplementary video: https://goo.gl/vXsbQP  NNN  https://goo.gl/vXsbQP   Figure N
We compare the different models’ likelihood of performing a successful action during execution
ANC suffers from  the large action space due to naı̈ve exploration
Imitation learning models are capable of differentiating between successful and  unsuccessful actions because the supervised loss discourages the  selection of unsuccessful actions
 tuning w, the model quickly adapts to new tasks, yielding  both high episode success rate and successful action rate
In  contrast, the model trained from scratch takes substantially  longer to converge
We also experiment with fine-tuning the  entire model, and it suffers from similar slow convergence
 N.N
Learning Affordances  An agent in an interactive environment needs to be able  to reason about the causal effects of actions
We expect our  SR model to learn the pre- and post-conditions of actions  through interaction, such that it develops a notion of affordance [NN], i.e., which actions can be performed under a  circumstance
In the real world, such knowledge could help  prevent damages to the agent and the environment caused  by unexpected or invalid actions
 We first evaluate each network’s ability to implicitly  learn affordances when trained on the tasks in Sec
N.N
In  these tasks, we penalize unnecessary actions with a small  time penalty, but we do not explicitly tell the network which  actions succeed and which fail
Fig
N illustrates that a standard reinforcement learning method cannot filter out unnecessary actions especially given delayed rewards
Imitation  learning methods produce significantly fewer failed actions  because they can directly evaluate whether each action gets  them closer to the goal state
 We also analyze the successor network’s capability of explicitly learning affordances
We train our SR model with  reinforcement learning, by executing a completely random  policy in the scene
We define the immediate reward of issuing a successful action as +N.0 and an unsuccessful one as −N.0
The agent learns in N0,000 episodes
Fig
N shows a t-SNE [NN] visualization of the state-action features φs,a
 We see that the network learns to cluster successful state acOpen Microwave  Close    Microwave  Successful Action  Unsuccessful Action  Figure N
Visualization of a t-SNE [NN] embedding of the stateaction vector φs,a for a random set of state-action pairs
Successful state-action pairs are shown in green, and unsuccessful pairs in  orange
The two blue circles highlight portions of the embedding  with very similar images but different actions
The network can  differentiate successful pairs from unsuccessful ones
 tion pairs (shown in green) separate from unsuccessful pairs  (orange)
The network achieves an ROC-AUC of 0.NN on  predicting immediate rewards over random state-action actions, indicating that the model can differentiate successful  and unsuccessful actions by performing actions and learning from their outcomes
 N
Conclusions  In this paper, we argue that visual semantic planning is  an important next task in computer vision
Our proposed  solution shows promising results in predicting a sequence  of actions that change the current state of the visual world  to a desired goal state
We have examined several different tasks with varying degrees of difficulty and show that  our proposed model based on deep successor representations achieves near optimal results in the challenging THOR  environment
We also show promising cross-task knowledge transfer results, a crucial component of any generalizable solution
Our qualitative results show that our learned  successor features encode knowledge of object affordances,  and action pre-conditions and post-effects
Our next steps  involve exploring knowledge transfer from THOR to realworld environments as well as examining the possibilities  of more complicated tasks with a richer set of actions
 Acknowledgements: This work is in part supported by ONR N000NNNN-N-0NN0, ONR MURI N000NN-NN-N-N00N, NSF IIS-NNNN0NN, NSFNNNN0NN, NRI-NNNNNNN, NSF IIS-NNNN0NN, a Siemens grant, the Intel Science and Technology Center for Pervasive Computing (ISTC-PC), Allen  Distinguished Investigator Award, and the Allen Institute for Artificial Intelligence
 NN0    References  [N] P
Agrawal, A
Nair, P
Abbeel, J
Malik, and S
Levine
 Learning to poke by poking: Experiential learning of intuitive physics
arXiv, N0NN
N  [N] M
L
Anderson
Embodied cognition: A field guide
Artificial intelligence, N00N
N  [N] A
Barreto, R
Munos, T
Schaul, and D
Silver
Successor  features for transfer in reinforcement learning
arXiv, N0NN
 N, N, N  [N] M
G
Bellemare, Y
Naddaf, J
Veness, and M
Bowling
 The arcade learning environment: An evaluation platform for  general agents
JAIR, N0NN
N  [N] C
Chen, A
Seff, A
Kornhauser, and J
Xiao
Deepdriving:  Learning affordance for direct perception in autonomous  driving
In ICCV, pages NNNN–NNN0, N0NN
N  [N] H
Daumé, J
Langford, and D
Marcu
Search-based structured prediction
Machine learning, NN(N):NNN–NNN, N00N
 N  [N] P
Dayan
Improving generalization for temporal difference  learning: The successor representation
Neural Computation, NNNN
N, N  [N] M
P
Deisenroth and C
E
Rasmussen
Pilco: A modelbased and data-efficient approach to policy search
In ICML,  N0NN
N  [N] M
Denil, P
Agrawal, T
D
Kulkarni, T
Erez, P
Battaglia,  and N
de Freitas
Learning to perform physics experiments  via deep reinforcement learning
arXiv, N0NN
N  [N0] C
Dornhege, M
Gissler, M
Teschner, and B
Nebel
Integrating symbolic and geometric planning for mobile manipulation
In SSRR, N00N
N  [NN] A
Dosovitskiy and V
Koltun
Learning to act by predicting  the future
In ICLR, N0NN
N  [NN] A
Fathi and J
M
Rehg
Modeling actions through state  changes
In CVPR, N0NN
N  [NN] R
E
Fikes and N
J
Nilsson
Strips: A new approach to the  application of theorem proving to problem solving
Artificial  intelligence, NNNN
N, N  [NN] M
Ghallab, D
Nau, and P
Traverso
Automated Planning:  theory and practice
N00N
N  [NN] J
J
Gibson
The ecological approach to visual perception:  classic edition
N0NN
N, N  [NN] S
Gu, E
Holly, T
Lillicrap, and S
Levine
Deep reinforcement learning for robotic manipulation with asynchronous  off-policy updates
In ICRA, N0NN
N  [NN] X
Guo, S
Singh, H
Lee, R
L
Lewis, and X
Wang
Deep  learning for real-time atari game play using offline montecarlo tree search planning
In NIPS, N0NN
N  [NN] A
Handa, V
Patraucean, V
Badrinarayanan, S
Stent, and  R
Cipolla
Understanding real world indoor scenes with  synthetic data
In CVPR, N0NN
N  [NN] J
Ho and S
Ermon
Generative adversarial imitation learning
In NIPS, N0NN
N  [N0] L
P
Kaelbling and T
Lozano-Pérez
Hierarchical task and  motion planning in the now
In ICRA, N0NN
N, N  [NN] M
Kempka, M
Wydmuch, G
Runc, J
Toczek, and  W
Jakowski
ViZDoom: A doom-based AI research platform for visual reinforcement learning
In CIG, N0NN
N  [NN] H
J
Kim, M
I
Jordan, S
Sastry, and A
Y
Ng
Autonomous  helicopter flight via reinforcement learning
In NIPS, N00N
 N  [NN] J
Kober, J
A
Bagnell, and J
Peters
Reinforcement learning  in robotics: A survey
IJRR, N0NN
N  [NN] T
D
Kulkarni, A
Saeedi, S
Gautam, and S
J
Gershman
 Deep successor reinforcement learning
arXiv, N0NN
N, N, N,  NN  [NN] B
M
Lake, T
D
Ullman, J
B
Tenenbaum, and S
J
Gershman
Building machines that learn and think like people
 arXiv, N0NN
N  [NN] A
Lerer, S
Gross, and R
Fergus
Learning physical intuition of block towers by example
In ICML, N0NN
N  [NN] S
Levine, C
Finn, T
Darrell, and P
Abbeel
End-to-end  training of deep visuomotor policies
JMLR, N0NN
N  [NN] W
Li, S
Azimi, A
Leonardis, and M
Fritz
To fall or not  to fall: A visual approach to physical stability prediction
 arXiv, N0NN
N  [NN] Y
Li, J
Song, and S
Ermon
Inferring the latent structure  of human decision-making from raw visual inputs
arXiv  preprint arXiv:NN0N.0NNN0, N0NN
N  [N0] T
P
Lillicrap, J
J
Hunt, A
Pritzel, N
Heess, T
Erez,  Y
Tassa, D
Silver, and D
Wierstra
Continuous control with  deep reinforcement learning
ICLR, N0NN
N  [NN] L
v
d
Maaten and G
Hinton
Visualizing data using t-sne
 Journal of Machine Learning Research, N(Nov):NNNN–NN0N,  N00N
N  [NN] M
Malmir, K
Sikka, D
Forster, J
R
Movellan, and G
Cottrell
Deep q-learning for active recognition of germs: Baseline performance on a standardized dataset for active learning
In BMVC, N0NN
N  [NN] J
Marin, D
Vázquez, D
Gerónimo, and A
M
López
 Learning appearance in virtual scenarios for pedestrian detection
In CVPR, N0N0
N  [NN] V
Mnih, A
P
Badia, M
Mirza, A
Graves, T
P
Lillicrap,  T
Harley, D
Silver, and K
Kavukcuoglu
Asynchronous  methods for deep reinforcement learning
In ICML, N0NN
N,  N, N  [NN] V
Mnih, K
Kavukcuoglu, D
Silver, A
A
Rusu, J
Veness,  M
G
Bellemare, A
Graves, M
Riedmiller, A
K
Fidjeland,  G
Ostrovski, et al
Human-level control through deep reinforcement learning
Nature, N0NN
N, N, N, NN  [NN] R
Mottaghi, M
Rastegari, A
Gupta, and A
Farhadi
“what  happens if...” learning to predict the effect of forces in images
In ECCV, N0NN
N  [NN] A
Noë and J
K
ORegan
On the brain-basis of visual consciousness: A sensorimotor account
Vision and mind: Selected readings in the philosophy of perception, N00N
N  [NN] J
Papon and M
Schoeler
Semantic pose using deep networks trained on synthetic rgb-d
In ICCV, N0NN
N  [NN] L
Pinto, D
Gandhi, Y
Han, Y.-L
Park, and A
Gupta
The  curious robot: Learning visual representations via physical  interactions
In ECCV, N0NN
N  [N0] L
Pinto and A
Gupta
Supersizing self-supervision: Learning to grasp from N0k tries and N00 robot hours
In ICRA,  N0NN
N  NNN    [NN] S
R
Richter, V
Vineet, S
Roth, and V
Koltun
Playing for  data: Ground truth from computer games
In ECCV, N0NN
N  [NN] G
Ros, L
Sellart, J
Materzynska, D
Vazquez, and  A
Lopez
The SYNTHIA Dataset: A large collection of  synthetic images for semantic segmentation of urban scenes
 In CVPR, N0NN
N  [NN] S
Ross, G
J
Gordon, and D
Bagnell
A reduction of imitation learning and structured prediction to no-regret online  learning
In International Conference on Artificial Intelligence and Statistics, pages NNN–NNN, N0NN
N  [NN] J
Schmidhuber
An on-line algorithm for dynamic reinforcement learning and planning in reactive environments
 In IJCNN, NNN0
N  [NN] A
Shafaei, J
J
Little, and M
Schmidt
Play and learn: using  video games to train computer vision models
arXiv, N0NN
 N  [NN] D
Silver, A
Huang, C
J
Maddison, A
Guez, L
Sifre,  G
van den Driessche, J
Schrittwieser, I
Antonoglou,  V
Panneershelvam, M
Lanctot, et al
Mastering the game  of go with deep neural networks and tree search
Nature,  N0NN
N  [NN] S
Srivastava, E
Fang, L
Riano, R
Chitnis, S
J
Russell,  and P
Abbeel
Combined task and motion planning through  an extensible planner-independent interface layer
In ICRA,  N0NN
N, N  [NN] S
Srivastava, L
Riano, S
Russell, and P
Abbeel
Using classical planners for tasks with continuous operators in  robotics
In ICAPS, N0NN
N  [NN] R
S
Sutton and A
G
Barto
Reinforcement learning: An  introduction
NNNN
N, N, N  [N0] A
Tamar, S
Levine, and P
Abbeel
Value iteration networks
 In NIPS, N0NN
N  [NN] S
D
Tran and L
S
Davis
Event modeling and recognition  using markov logic networks
In ECCV, N00N
N  [NN] X
Wang, A
Farhadi, and A
Gupta
Actions ˜ transformations
In CVPR, N0NN
N  [NN] B
Wymann, C
Dimitrakakis, A
Sumner, E
Espié, and  C
Guionneau
Torcs: The open racing car simulator
N0NN
 N  [NN] Y
Zhu, R
Mottaghi, E
Kolve, J
J
Lim, A
Gupta, L
FeiFei, and A
Farhadi
Target-driven visual navigation in indoor scenes using deep reinforcement learning
ICRA, N0NN
 N, N, NN  NNNReasoning About Fine-Grained Attribute Phrases Using Reference Games   Reasoning about Fine-grained Attribute Phrases using Reference Games  Jong-Chyi Su∗ Chenyun Wu∗ Huaizu Jiang Subhransu Maji University of Massachusetts, Amherst  {jcsu,chenyun,hzjiang,smaji}@cs.umass.edu  Abstract  We present a framework for learning to describe finegrained visual differences between instances using attribute  phrases
Attribute phrases capture distinguishing aspects  of an object (e.g., “propeller on the nose” or “door near  the wing” for airplanes) in a compositional manner
Instances within a category can be described by a set of these  phrases and collectively they span the space of semantic attributes for a category
We collect a large dataset of such  phrases by asking annotators to describe several visual differences between a pair of instances within a category
We  then learn to describe and ground these phrases to images  in the context of a reference game between a speaker and a  listener
The goal of a speaker is to describe attributes of an  image that allows the listener to correctly identify it within  a pair
Data collected in a pairwise manner improves the  ability of the speaker to generate, and the ability of the listener to interpret visual descriptions
Moreover, due to the  compositionality of attribute phrases, the trained listeners  can interpret descriptions not seen during training for image retrieval, and the speakers can generate attribute-based  explanations for differences between previously unseen categories
We also show that embedding an image into the  semantic space of attribute phrases derived from listeners  offers N0% improvement in accuracy over existing attributebased representations on the FGVC-aircraft dataset
 N
Introduction  Attribute-based representations have been used for describing instances within a basic-level category as they often  share a set of high-level properties
These attributes serve  as basis for human-centric tasks such as retrieval and categorization [NN, NN, NN], and for generalization to new categories based on a description of their attributes [NN, NN, NN,  NN]
However, most prior work has relied on a fixed set of  attributes designed by experts
This limits their scalability  to new domains since collecting expert annotations are expensive, and results in models that are less robust to noisy  open-ended descriptions provided by a non-expert user
 ∗Authors contributed equally  Facing right In the air  Closed cockpit White and green  Propeller spinning  Facing left On the ground Open cockpit  White and blue color Propeller stopped  vs
vs
vs
vs
vs
 Red plane  Speaker  Listener  Figure N
Left: Each annotation in our dataset consists of five pairs  of attribute phrases
Right: A reference game played between a  speaker who describes an attribute of an image within a pair and a  listener whose goal is to pick the right one
 Instead of discrete attributes this work investigates the use  of attribute phrases for describing instances
Attribute  phrases are short sentences that describe a unique semantic visual property of an object (e.g., “red and white color”,  “wing near the top”)
Like captions, they can describe properties in a compositional manner, but are typically shorter  and only capture a single aspect
Like attributes, they are  modular, and can be combined in different ways to describe instances within a category
Their compositionality allows the expression of large number of properties in  a compact manner
For example, colors of objects, or their  parts, can be expressed by combining color terms (e.g., “red  and white”, “green and blue”, etc.)
A collection of these  phrases constitutes the semantic space of describable attributes and can be used as a basis for communication between a human and computer for various tasks
 We begin by collecting a dataset of attribute phrases by  asking annotators to describe five visual differences between random pairs of airplanes from the OID airplane  dataset [NN]
Each difference is of the form “PN vs
PN”  with phrases PN and PN corresponding to the properties of  the left and right image respectively (Figure N)
By collecting multiple properties at a time we obtain a diverse set  of describable attributes
Moreover, phrases collected in a  contrastive manner reveal attributes that are better suited for  fine-grained discrimination
The two phrases in a comparison describe the same underlying attribute (e.g., round nose  and pointy nose both describe the shape), and reflect an axis  NNN    of comparison in the underlying semantic space
We then  analyze the ability of automatic methods to generate these  attribute phrases using the collected dataset
In particular  we learn to generate descriptions and ground them in images in the context of a reference game (RG) between a  speaker S and a listener L (Figure N)
S is provided with  a pair of images {IN, IN} and produces a visual difference of the form PN (or “PN vs
PN”)
L’s goal is to identify  which of the two images corresponds to PN
Reference  games have been widely used to collect datasets describing  objects within a scene
This work employs the framework  to generate and reason about compositional language-based  attributes for fine-grained visual categorization
 Our experiments show that a speaker trained to describe  visual differences displays remarkable pragmatic behavior  allowing a neural listener to rank the correct image with  NN.N% top-N accuracy in the RG compared with N0.N% of a  speaker trained to generate captions non-contrastively
We  also investigate a family of pragmatic speakers who generate descriptions by jointly reasoning about the listener’s  ability to interpret them, based on the work of Andreas and  Klein [N]
Contrastively trained pragmatic speakers offer  significant benefits (on average N% higher top-N accuracy  in RG across listeners) over simple pragmatic speakers
The  resulting speakers can be used to generate attribute-based  explanations for differences between two categories
Moreover, given a set of attribute phrases, the score of an image  with respect to each phrase according to a listener provides  a natural embedding of the image into the space of semantic  attributes
On the task of image classification on the FGVC  aircraft dataset [N0] this representation outperforms existing attribute-based representations by N0% accuracy
 In summary, we show that reasoning about attribute phrases  via reference games is a practical way of discovering and  grounding describable attributes for fine-grained categories
 We validate our approach on a dataset of N,NNN images with  N,N00 pairs for a total of NN,000 phrases (Section N)
We  systematically evaluate various speakers and listeners using the RG and human studies (Section N.N-N.N), investigate  the effectiveness of attribute phrases on various recognition  tasks (Section N.N-N.N), and conclude in Section N
 N
Related Work  Attribute-based representations
Attributes have been  widely used in the computer vision as an intermediate, interpretable representation for high-level recognition
They often represent properties that can be shared across categories,  e.g., both a car and bicycle have wheels, or within a subordinate category, e.g., birds can be described by the shape of  their beak
Due to their semantic nature they have been used  for learning interpretable classifiers [NN, NN], attribute-based  retrieval systems [N], as high-level priors for unseen categories for zero-shot learning [NN, NN], and as a means for  communication in an interactive recognition system [NN]
 A different line of work has explored the question of discovering task-specific attributes
Berg et al
[N] discover  attributes by mining frequent n-grams in captions
Parikh  and Grauman [NN] ask annotators to name directions that  maximally separate the data according to some underlying features
Other approaches [NN, NN, N] have mined  phrases from online text repositories to discover commonsense knowledge about properties of categories (e.g., cars  have doors)
For a detailed description of the above methods see this recent survey [NN]
 The interface for collecting attribute phrases is based on our  earlier work [NN], which showed that annotations collected  in a pairwise manner could be analyzed to discover a lexicon of parts and attributes
This work extends the prior  work in a several ways
We (a) consider the complete problem of generating and interpreting attribute phrases on a significantly larger dataset, (b) systematically evaluate speaker  and listener models on the data, and (c) show their utility in  various recognition tasks
 Referring expression comprehension and generation
 Modern captioning systems [NN, N0, NN] produce descriptions by using encoder-decoder architectures, typically consisting of a convolutional network for encoding an image  and a recurrent network for decoding a sentence
A criticism of these tasks is that captions in existing datasets  (e.g., MS COCO dataset [NN]) can be generated by identifying the dominant categories and relying on a language  model
State-of-the-art systems are often matched by simple nearest-neighbor retrieval approaches [N, N0]
Visual  question-answering systems [N] face a similar issue that  most questions can be answered by relying on commonsense knowledge (e.g., the sky is often blue)
Some recent  attempts have been made to address these issues [NN]
 Tasks where annotators are asked to describe an object in an  image such that another can correctly identify it provides a  way to collect context-sensitive captions [N0]
These tasks  have been widely studied in the linguistics community in  an area called pragmatics (see Grice’s maxims [NN])
Much  prior work in computer vision has focused on generating  referring expressions to distinguish an object within an image [NN, N0, NN, NN, NN, NN, NN, NN]
More recently, referring  expression generation have been extended to interactive dialogue systems [N, N]
In contrast, our work aims to collect  and generate referring expressions for fine-grained discrimination between instances
 For the task of fine-grained recognition, the work of Reed et  al
[NN] is the most related to ours
They ask annotators on  Amazon Mechanical Turk to describe properties of birds  NNN    and flowers, and use the data to train models of images  and text
They show the utility of such models for zeroshot recognition where a description of a novel category is  provided as supervision, and for text-based image retrieval
 Another recent work [NN] showed that referring expressions  for images within a set can be generated simply by enforcing separation of image probabilities during decoding using  beam search
However, their model was trained on context agnostic captions
Our work takes a different approach
 First, we collect attribute phrases in a contrastive manner  that encourages pragmatic behavior among annotators
Second, we ask annotators to provide multiple attribute descriptions, which as we described earlier, allows modular reuse  across instances, and serves as an intermediate representation for various tasks
Attribute phrases capture the spectrum between basic attributes and detailed captions
Like  “visual phrases” [NN] they capture frequently occurring relations between basic attributes
 N
Method  The framework used to collect our dataset is described in  Section N.N
Various speaker and listener models are described in Section N.N and Section N.N respectively
 N.N
A dataset of attribute phrases  We rely on human annotators to discover the space of descriptive attributes
Our annotations are collected on images  from the OID aircraft dataset [NN]
The annotations are organized into NN00 image pairs (NNNN images) in training set,  NNN0 pairs (NNN0 images) in validation set, and NNN0 pairs  (NN0N images) in test set
Each pair is chosen by picking  two different images uniformly at random within the provided split in the OID aircraft dataset
 Annotators from Amazon Mechanical Turk are asked to describe five properties in the form “PN vs
PN”, each corresponding to a different aspect of the objects in the left and  the right image respectively
We also provide some examples as guidance to the annotators
The interface shown in  Figure N is lightweight and allows rapid deployment compared to existing approaches for collecting attribute annotations where an expert decides the set and semantics of attributes ahead of time
However, the resulting annotations  are noisier and reflect the diversity of open-ended languagebased descriptions
A second pass over the data is done to  check for consistency, after which about NN% of the description pairs were discarded
 Figure N shows an example of our dataset (more examples  are in the supplementary material)
Annotations describe  the shapes of parts (nose, wings and tail), relative sizes, orientation, colors, types of engines, etc
Most descriptions are  short with an average length of N.N words on each side, although about N.N% of them have more than N words
These  Describe differences between the two aeroplane images  Instructions:   Annotate each one of the three tasks Press Next to move to the next pair and Submit once done
If the images do not display, your browser may not support this interface
Try the latest Chrome, Safari or Firefox browsers
 Click here to see example answers before you start
    VS        List N differences between the two images N
   VS     N
   VS     N
   VS     N
   VS     N
   VS        pair N of N  Previous Next  Figure N
The annotation interface used to collect five different attribute phrase pairs adapted from [NN]
Amazon mechanical turkers were paid $0.NN for annotating three pairs
 are qualitatively different from image captions which are  typically longer and more grammatical
However, each annotation provides five different attribute pairs
 The OID dataset also comes with a set of expert-designed  attributes
A comparison with OID attributes shows that  attribute phrases capture novel properties that describe the  relative arrangement of parts (e.g., “door above the wing”,  “wing on top”), color combinations, relative sizes, shape,  and number of parts (e.g., “big nose”, “more windows”,  etc.) Section N.N shows a visualization of the space of attribute phrases
Section N.N provides a direct comparison of  OID attributes and those derived from our data on the task  of FGVC-aircraft variant classification [N0]
 N.N
Speaker models  A speaker maps visual inputs to attribute phrases
We consider two speakers; a simple speaker (SS) that takes a single  image as input and produces a description, and a discerning  speaker (DS) that takes two images as input and produces a  single (or a pair of) description(s)
 Both our speaker models are based on the show-and-tell  model [NN] developed for image captioning
Images are  encoded using a convolutional network and decoded into  a sentence using a recurrent network over words
We use  one-hot encoding for NN0 words with frequency greater than  N in the training set
We consider fcN layer outputs of the VGG-NN network [NN] plus two fully-connected layers  with ReLU units [NN] on top as the image feature, and a  LSTM model [NN] with N0NN hidden units to generate the  NN0    sentences
The image feature is fed into the LSTM not only  as the initial input, but also in each state input together with  word embeddings
This led to an improved speaker in our  experiments
For the discerning speaker, we concatenate  two image features as input to the LSTM
At test time we  apply beam search with beam size N0 and get N0 output de- scriptions from each image (pair)
Although the discerning  speaker is trained to generate phrase pairs, we can simply  take the first (or second) half of the pair and evaluate it in  the same way as a simple speaker
 We also consider a pragmatic speaker that generates contrastive captions by reasoning about the listener’s ability to  pick the correct image based on the description
Andreas  and Klein [N] proposed a simple strategy to do so by reranking descriptions of an image based on a weighted combination of (a) fluency – the score assigned by the speaker,  and (b) accuracy – the score assigned by the listener on  the referred image
Various pragmatic speakers are possible  based on the choice of speakers and listeners
The details  are described in Section N.N
 Optimization details: Our implementation is based on Tensorflow [N]
The descriptions are truncated at length NN when training the LSTM
The VGG-NN network is initialized with weights pre-trained on ImageNet dataset [NN]
We  first fix the VGG-NN weights and train the rest of the network, using Adam optimizer [NN] with initial learning rate  0.00N, βN = 0.N, βN = 0.NNN and ✏ = N.0 × N0 −N
We  have batch normalization [NN] in fully connected layers after VGG-NN, and drop out with rate 0.N in LSTM
We use batch size NN for N0000 steps (∼NN epochs)
Second, we fine tune the whole network with initial learning rate modified to N× N0−N, batch size NN for another N0000 steps
 N.N
Listener models  A listener interprets a single (or a pair of) attribute  phrase(s), and picks an image within a pair by measuring  the similarity between the phrase(s) and images in a common embedded space
Once again we consider two listeners: a simple listener (SL) that interprets a single phrase,  and a discerning listener (DL) that interprets a phrase pair
 The simple listener models the score of the image IN within  a pair (IN, IN) for a phrase P as:  p(IN|P ) = σ(φ(IN) T ✓(P), φ(IN)  T ✓(P))
 Here φ and ✓ are embeddings of the image and the phrase respectively, and σ is the softmax function σ(x, y) = exp(x)/(exp(x)+exp(y))
Similarly, a discerning listener models the score of an image by comparing it with an embedding of the phrase pair ✓([PN vs
PN])
A simple way to construct a discerning listener from a simple listener is by  averaging the predictions from the left and right phrases,  i.e., p(I|[PN vs
PN]) = (p(I|PN) + p(I|PN)) /N
 We follow the setup of the speaker to embed phrases and  use the final state of a LSTM with N0NN hidden nodes as the  phrase embedding
The vocabulary of words is kept identical
For image features, once again we use the fcN layer of  the VGG-NN network and add a fully-connected layer with  N0NN units and ReLU activation
The parameters are learned  by minimizing the cross-entropy loss
 We also evaluate two variants of the simple listener, SLr and  SL, based on whether it is trained on non-contrastive data  (IN, IN, PN) where IN is a random image within the training  set, or the contrastive data where IN is the other image in the  annotation pair
 Optimization details: We first fix the VGG-NN network and  use Adam optimizer with initial learning rate = 0.00N, βN = 0.N, batch size = NN for N000 steps (N000 steps for SLr model), then fine-tune the entire model with a learning rate  N× N0−N for another N000-N0000 steps
 Human listener
We also consider human annotators to  perform the task of the listener in the RG
For each generated phrase that describes one image out of an image  pair, we let three users to pick which image out of the pair  the phrase is referring to
However, unlike (most) human  speakers, neural speakers can produce irrelevant descriptions
Thus, in addition to the choice of left and right image,  users have the option to say “not sure” when the description  is ambiguous
If two or more users out of three picked the  same image, we say the human listener is certain about the  choice, otherwise we say the human listener is uncertain
 The interface is shown in the supplementary material
 N
Results  We evaluate various listeners and speakers on the dataset we  collected in terms of their accuracy in the RG in Section N.N  and Section N.N respectively
We then evaluate their effectiveness on a fine-grained classification task in Section N.N,  visualize the space of attribute phrases discovered from the  data in Section N.N, for text-based image retrieval in Section N.N, and for generating visual explanations for differences between categories in Section N.N
 N.N
Evaluating listeners  We first evaluate various listeners on human-generated  phrases
For simple listeners, each annotation provides ten  different reference tasks (IN, IN, P) → {0,N} corresponding to five different left and right attribute phrases
Each task  is evaluated independently and accuracy is measured as the  fraction of correct references made by the listener
Similarly, discerning listeners are evaluated by replacing P with  “PN vs
PN” or “PN vs
PN”
 NNN    Accuracy using human speakers
The results are shown  in Table N
Training on contrastive data improves the accuracy of the simple listener slightly from NN.N% (SLr) to  NN.N% (SL) on the test set
Discerning listeners see both  phrases at once and naturally perform better
There is almost no difference between a discerning listener that combines two simple listeners by averaging their predictions  (N×SL), and one that interprets the two phrases at once (DL)
The results indicate that on our dataset the listener’s  task is relatively easy and contrastive data does not provide  any significant benefits
As a reference the accuracy of a human listener is close to N00% on human-generated phrases
 Input Speaker Listener Val Test  PN Human SLr NN.N NN.N  SL NN.N NN.N  PN vs
PN Human DL NN.N NN.N  N×SL NN.N NN.N Table N
Accuracy (%) of various listeners in the RG using attribute  phrases provided by a human speaker
 Are the top attributes more salient? As annotators are  asked to describe five different attributes they might pick  ones that are more salient first
We evaluate this hypothesis  by measuring the accuracy of the listener (SL) on phrases as  a function of the position of the annotation in the interface  ranging from one for the top attribute to five for the last one
 The results are shown in Table N
The accuracy decreases  monotonically from one to five suggesting that the first attribute phrase is easier for the listener to discriminate
We  are uncertain if this is because the attributes near the top  are more discriminative, or because the listener is better at  interpreting these as they are likely to be more frequent in  the training data
Nevertheless, attribute saliency is a signal  we did not model explicitly and may be used to train better  speakers and listeners (e.g., see Turakhia and Parikh [NN])
 N N N N N  Val NN.N NN.N NN.N NN.N NN.N  Test NN.N NN.N NN.N NN.0 NN.N Table N
Accuracy (%) of the simple listener (SL) on RG using  human-generated attribute phrases at positions one through five  across the validation and test set
The accuracy decreases monotonically from one to five suggesting that the top attribute phrases  are easier to discriminate
 N.N
Evaluating speakers  We use simple listeners, SL and SLr, and the human listener to evaluate speakers
As described in Section N.N we  use beam search to generate N0 descriptions for each image pair and evaluate them individually using various listeners
 The discerning speaker generates phrase pairs but we simply take the first and second half separated by “vs.”, a special word in the vocabulary, and evaluate it using a simple  listener (that sees only one phrase)
If the word “vs.” is  missing in the generated output we simply consider the entire sentence as the PN
Only N out of NNN00 phrase pairs did  not contain the “vs.” token
 For evaluation with humans we collect three independent  annotations on a subset of N00 image pairs (with N0 descriptions each) out of the full test set
The listeners are considered to be correct when the probability of the correct image  is greater than half
For human listener, we report the accuracy of when there is a majority agreement on the correct  image, i.e., when two or more users picked the correct image
For direct comparison with the simple speaker models,  we also report the human listener accuracy when they are allowed to guess
This is the sum of earlier accuracy, and half  of the cases when there is no majority agreement
Human  annotators are uncertain when the generated descriptions  are not fluent or when they are not discriminative
Therefore, a better human accuracy reflects speaker quality both  in terms of fluency and discriminativeness
Some examples  of the generated attribute phrases using various speakers are  shown in Figure N
 Ground Truth:  N) small size VS large size  N) single seat VS more seated  N) facing left VS facing right  N) private VS commercial  N) wings at the top VS wings at the bottom  DS:  N) private plane VS commercial plane (p=0.NNNN)  N) private VS commercial (p=0.NNNN)  N) small plane VS large plane (p=0.0N0N)  N) facing left VS facing right (p=0.0NNN)  N) short VS long (p=0.0NN0)  N) white VS red (p=0.0NNN)  N) high wing VS low wing (p=0.0NNN)  N) small VS large (p=0.0NNNN)  N) glider VS jetliner (p=0.0NN0)  N0) white and blue color VS white red and         blue color (p=0.0NNN)  SS:  N) no engine (p=0.NNNN)  N) small (p=0.NN00)  N) private plane (p=0.0NN0)  N) on the ground (p=0.0NNN)  N) propellor engine (p=0.0NNN)  N) on ground (p=0.0NN0)  N) glider (p=0.0NNN)  N) white color (p=0.0NNN)  N) small plane (p=0.0NNN)  N0) no propeller (p=0.0NNN)  Figure N
Example output of simple speaker SS and discerning  speaker DS
Simple speaker takes the left image in the green box  as input, while the discerning speaker takes both images as input
 In brackets are the probabilities according to the speaker
 Accuracy of various speakers and listeners
Results  on the full test set (Test) and the human-evaluated subset  (Test*) are shown in Table N
The accuracy of discerning speaker exceeds that of simple speaker by more than  N0% no matter which listener to use
This result suggests that data collected contrastively using our annotation task  allows direct training of speaker models that show remarkable context-sensitive behavior
Somewhat surprisingly we  also see that the simple listeners are more accurate than the  human listener when evaluated on descriptions generated  by our speaker models
This is because humans tend to be  more cautious in the reference game
For example, simple listeners will accept yellowish grass being referred to  as “concrete” compared to green grass, but humans tend to  view it as an unclear reference
 NNN    Accuracy (%)  SLr SL Human  Top Test∗ Test Test∗ Test Test∗  SS  N NN.0 NN.N NN.0 NN.N NN.0 (NN.0)  N N0.0 NN.N NN.0 N0.N NN.N (NN.N)  N0 NN.0 NN.N NN.N N0.0 NN.N (NN.N)  DS  N NN.0 NN.N NN.0 NN.N NN.0 (NN.N)  N NN.N N0.N NN.N NN.N N0.N (NN.N)  N0 NN.N NN.N N0.0 N0.N NN.N (NN.0) Table N
Accuracy in the RG using different speakers and listeners
 Test represents the full test set consisting of NNN0 image pairs
 Test∗ represents a subset of N00 test set image pairs for which we  collected human listener results
For the human listener, we report  the accuracy when there is a majority agreement, and accuracy  with guessing (in brackets)
DS is significantly better at generating  discriminative attribute phrases than SS
 Does pragmatics help? Given that our discerning  speaker can generate highly accurate contrastive descriptions, we investigate if additional benefits can be achieved if  the speaker jointly reasons about the listener’s ability to interpret the descriptions
We employ the pragmatic speaker  model of Andreas and Klein [N] where a simple speaker  generates descriptions that are reranked by a simple listener using a weighted combination of speaker and listener  scores
In particular, we rerank the output N0 sentences from speakers by the probabilities from simple listeners
 We combine the listener probability pl and speaker beamsearch probability ps as p = p λ s · p  (N−λ) l  , and pick the optimal λ on a validation set annotated by a human listener
We found that the optimal λ is close to 0, so we decided to use pl only for reranking on test set
 In Table N, we report the accuracy of top k sentences (k = N, N, N) of the human listener and the results after rerank- ing on the Test* set
When using the listener score from  SLr the average accuracy of the top five generated descriptions after reranking improves dramatically from NN.N% to  NN.N% for the simple speaker
The accuracy of the discerning speaker also improves to N0%
This suggests that better pragmatics can be achieved if both the speaker and listener are trained in a contrastive manner
Surprisingly the  contrastively-trained simple listener SL is less effective at  reranking than SLr
We believe this is because the SL overfits on the human speaker descriptions and is less effective  when used with neural speakers
 Figure N shows an example pair and the output of different  speakers
Simple speaker suffers from generating descriptions that are true to the target image, but fail to differentiate  two images
Discerning speaker can mostly avoid this mistake
Reranking by listeners can move better sentences to  the top and improves the quality of top sentences
 Human listener accuracy (%)  Reranker listener  Top None SLr SL  SS  N NN.0 (NN.0) NN.0 (NN.0) NN.0 (NN.0)  N NN.N (NN.N) NN.N (NN.N) N0.N (NN.N)  N NN.N (NN.N) NN.N (NN.0) NN.N (NN.N)  DS  N NN.0 (NN.N) NN.0 (NN.N) NN.0 (NN.0)  N N0.N (NN.N) N0.0 (NN.N) NN.N (NN.N)  N NN.N (NN.N) NN.N (NN.N) NN.N (NN.N) Table N
Accuracy of pragmatic speakers with human listeners on  the Test* set
After generating the descriptions by the speaker  model (either SS or DS), we use the listener model (SLr or SL) to  rerank them
We report the accuracy based on human listener from  the user study
We report both the accuracy when there is majority  agreement, and accuracy with guessing (in brackets)
Pragmatic  speakers are strictly better than non-pragmatic ones
 N.N
Fine-grained classification with attributes  We compare the effectiveness of attribute phrases to existing attributes in the OID dataset on the task of fine-grained  classification on the FGVC aircraft dataset [N0]
The OID  dataset is designed with attributes in mind and has long-tail  distribution over aircraft variants with NNNN models, while  the FGVC dataset is designed for fine-grained classification  task with N00 variants each with N00 images
Both datasets  are based on the images from the airliners.net website and have a few overlapping images
We exclude the  NNN images from the FGVC test set that appear in the OID  training+validation set in our evaluation
 There are NN attributes in the OID dataset organized into  NN categories
We exclude three attributes – two referring to the airline label and model, most of which have  only one training examples per category, and another that  is rare
We then trained linear classifiers to predict each  attribute using the fcN layer feature of the VGG-NN network
Using the same features and trained classifiers, we  construct a NN dimensional embedding of the FGVC images into the space of OID attributes
The attribute classifiers based on the VGG-NN network features are fairly accurate (NN% mean AP across attributes) and outperforms the  Fisher vector baseline included in the OID dataset paper
 For the attribute phrase embeddings, we first obtain the K  most frequent ones in our training set
Given an image I,  we compute the score φ(I)T ✓(P) for each phrase P from a listener as the embedding
For a fair comparison the image  features are kept identical to the OID attribute classifiers
 We also explore an opponent attribute space, where instead  of top phrases we consider the top phrase pairs
Phrase pairs  represent an axis of comparison, e.g., “small vs
medium”,  or “red and blue vs
red and white”, and are better suited for  describing relative attributes
We use the discerning listener  for the embedding on the opponent attribute space
 NNN  airliners.net   SS:   ✔ passenger plane   ?  white   ✔ jet engine   ?  facing right   ✔ commercial plane   ?  _UNK   ?  on the ground   ✔ large   ✔ large size   ✔ on runway  DS:  ✔ commercial plane   ?  facing right   ✔ turbofan engine   ✔ on concrete   ✔ t tail   ✔ jet engine   ✔ twin engine   ✔ multi seater   ✔ white and red   ✔ white colour with red stripes  SS + SLr:  ✔ commercial plane   ✔ large   ✔ large size   ✔ jet engine   ✔ on runway   ✔ passenger plane   ?  on the ground   ?  _UNK   ?  white   ?  facing right  DS + SLr:  ✔ commercial plane   ✔ jet engine   ✔ turbofan engine   ✔ twin engine   ✔ on concrete   ✔ multi seater   ✔ t tail   ✔ white and red   ?  facing right   ✔ white colour with red stripes  Figure N
An example output of various speakers
Given the image pair, we use SS and DS to generate descriptions of the top left image
 Outputs from SS and DS are listed in the order of probabilities from speaker beam search
Outputs of SS+SLr and DS+SLr are reranked  by SLr 
Green checks mean human listener picks correct image with certain, while question marks mean human listener is uncertain which  image is referred to
The results indicate that DS is better than SS, and reranking using listeners improves the quality of top sentences
 Figure N
Classification accuracy on FGVC aircraft dataset using  the NN dimensional OID attributes and varying number of attribute  phrases
See Section N.N for details
 Figure N shows a comparison of OID attributes and attribute  phrases for various listeners and number of attributes
For  the same number of attributes as the OID dataset, attribute  phrases are NN% better
With N00 attributes the accuracy  improves to NN%, about N0% better than OID
These results indicate that attribute phrases provide a better coverage of the space of discriminative directions
The two simple listeners perform equally well and the opponent attribute  space does not offer any additional benefits
 N.N
Visualizing the space of descriptive attributes  We visualize the space of the N00 most frequent phrases  in the training set using the embedding of the simple listener model projected from N0NN dimensions to N using t- SNE [NN] in Figure N
Various semantically related phrases  are clustered into groups
The cluster on the top right reflects color combinations; Phrases such as “less windows”  and “small plane” are nearby (bottom right)
Visualizations  of the learned embeddings of images φ(I) and opponent attribute phrases ✓([PN vs
PN]) are provided in the supple- mentary material
 TensorBoard SCALARS IMAGES AUDIO GRAPHS DISTRIBUTIONS HISTOGRAMS EMBEDDINGS  DATA  Checkpoint: ./embedding.ckpt  Metadata: metadata_sentence.tsv  T-SNE PCA CUSTOM  Dimension ND  Perplexity N0  Learning rate N0  Re‑run  Stop  Iteration: N00N   How to use t-SNE effectively
 Show All Data  Isolate N points  Clear selection     facing right  small plane  commercial plane  in the air  white color  white  turbofan engine  propellor engine  propeller engine  single engine  on ground  ying in the air  military plane  small  large plane  jet engine  big plane  at nose  propeller  ying  commercial  outside  in air  large   N tensor found  sentence_feat   Label by  Class   Color by  No color map  Sphereize data   ND  Points: N00 Dimension: N0NN Selected NNN points  Search .*  by  Class  BOOKMARKS (N)   tsne_N0_N000  Figure N
Visualization of the N00 most frequent descriptions
 Each attribute is embedded into a N0NN dimensional space using  the simple listener SL and projected into two dimensions using  t-SNE [NN]
(Best viewed digitally with zoom.)  N.N
Image retrieval with descriptive attributes  The listeners also allows us to retrieve an image given one  or more attribute phrases
Given a phrase P we rank the images in the test set by the listener scores φ(I)T ✓(P)
Figure N shows some query phrases and the NN most similar images  retrieved from the test set
These results were obtained by  simply concatenating all the query phrases to obtain a single  phrase
More sophisticated schemes for combining scores  from individual phrase predictions are likely to improve results [N0]
Our model can retrieve images with multiple attribute phrases well even though the composition of phrases  does not appear in the training set
For example, “red and  blue” only shows five times in total of NN, 000 phrases in the training set, “pointy nose” and “on the runway” are never  seen in a single phrase together
 NNN    red and blue pointy nose; on the runway red plane; many windows; facing right  Figure N
Top NN images ranked by the listener for various attribute phrases as queries (shown on top)
We rank the images by the scores  from the simple listener on the concatenation of the attribute phrases
The images are ordered from top to bottom, left to right
 small plane  military plane  grey  single engine  pointed nose  fighter jet  grey color  gray  no windows on body  gray color  commercial plane  big plane  twin engine  rounded nose  commercial  turbofan engine  passenger jet  commercial jet  white  white color  F/A-NN Yak-NN  large plane  more windows  commercial plane  more windows on body  big plane  commercial  jet engine  turbofan engine  engines under wings  on ground  private plane  less windows  medium plane  propellor engine  fewer windows on body  small plane  private  propeller engine  stabilizer on top of tail  british airways  NNN-N00 ATR-NN  Figure N
Top N0 discriminative attribute phrases for pairs of categories from FGVC aircraft dataset
Descriptions are generated by the  discerning speaker for each pair of images in the first and second category
The phrases sorted by the occurrence frequency provides an  attribute-based explanation of the visual difference between two categories
 N.N
Generating attribute explanations  The pairwise reasoning of a speaker can be extended to analyze an instance within a set by aggregating speaker utterances across all pairs that include the target
Similarly one  can describe differences between two sets by considering all  pairs of instances across the two sets
We use this to generate attribute-based explanations for visual differences between two categories
We select two categories A,B from FGVC aircraft dataset and randomly choose ten images  from each category
For each image pair (IN ∈ A, IN ∈ B), we generate ten phrase pairs using our discerning speaker
 We then sort unique phrases primarily by their image frequency (number of images from target category described  by the given description minus that from the opposite category), and when tied secondarily by their phrase frequency  (number of occurrences of the phrase in target category minus that in the opposite category.) The top ten attribute  phrases for the two categories for an example pair of categories are shown in Figure N
The algorithm reveals several  discriminative attributes between two such as “engine under wings” for NNN-N00, and “stabilizer on top of tail” for  ATR-NN
 N
Conclusion  We analyzed attribute phrases that emerge when annotators describe visual differences between instances within  a subordinate category (airplanes), and showed that speakers and listeners trained on this data can be used for various human-centric tasks such as text-based retrieval and  attribute-based explanations of visual differences between  unseen categories
Our experiments indicate that pragmatic  speakers that combine listeners and speakers are effective  on the reference game [N], and speakers trained on contrastive data offers significant additional benefits
We also  showed that attribute phrases are modular and can be used  to embed images into an interpretable semantic space
The  resulting attribute phrases are highly discriminative and outperform existing attributes on FGVC aircraft dataset on the  fine-grained classification task
 Acknowledgement: This research was supported in part  by the NSF grants NNNNNNN and NNNNNNN, and a faculty gift  from Facebook
The experiments were performed using  equipment obtained under a grant from the Collaborative  R&D Fund managed by the Massachusetts Tech Collaborative and GPUs donated by NVIDIA
 NNN    References  [N] M
Abadi, A
Agarwal, P
Barham, E
Brevdo, Z
Chen,  C
Citro, G
S
Corrado, A
Davis, J
Dean, M
Devin, et al
 Tensorflow: Large-scale machine learning on heterogeneous  distributed systems
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 N  [N] Z
Akata, S
Reed, D
Walter, H
Lee, and B
Schiele
Evaluation of output embeddings for fine-grained image classification
In Computer Vision and Pattern Recognition (CVPR),  N0NN
N  [N] J
Andreas and D
Klein
Reasoning About Pragmatics with  Neural Listeners and Speakers
Conference on Empirical  Methods in Natural Language Processing (EMNLP), N0NN
 N, N, N, N  [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra, C
L
Zitnick, and D
Parikh
VQA: Visual Question Answering
In  International Conference on Computer Vision (ICCV), N0NN
 N  [N] T
L
Berg, A
C
Berg, and J
Shih
Automatic attribute discovery and characterization from noisy web data
In European Conference on Computer Vision (ECCV), N0N0
N  [N] H
Chen, A
Gallagher, and B
Girod
Describing clothing by  semantic attributes
In European Conference on Computer  Vision (ECCV), N0NN
N  [N] A
Das, S
Kottur, K
Gupta, A
Singh, D
Yadav, J
M
F
 Moura, D
Parikh, and D
Batra
Visual dialog
In Computer  Vision and Pattern Recognition (CVPR), N0NN
N  [N] H
de Vries, F
Strub, S
Chandar, O
Pietquin, H
Larochelle,  and A
Courville
Guesswhat?! visual object discovery  through multi-modal dialogu
In Computer Vision and Pattern Recognition (CVPR), N0NN
N  [N] J
Devlin, S
Gupta, R
Girshick, M
Mitchell, and C
L
Zitnick
Exploring nearest neighbor approaches for image captioning
arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [N0] J
Donahue, L
Anne Hendricks, S
Guadarrama,  M
Rohrbach, S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual  recognition and description
In Computer Vision and Pattern  Recognition (CVPR), N0NN
N  [NN] A
Farhadi, I
Endres, and D
Hoiem
Attribute-centric recognition for cross-category generalization
In Computer Vision  and Pattern Recognition (CVPR), N0N0
N, N  [NN] A
Farhadi, I
Endres, D
Hoiem, and D
Forsyth
Describing  objects by their attributes
In Computer Vision and Pattern  Recognition (CVPR), N00N
N, N  [NN] H
P
Grice
Logic and conversation
NNNN
N  [NN] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural computation, N(N):NNNN–NNN0, NNNN
N  [NN] R
Hu, H
Xu, M
Rohrbach, J
Feng, K
Saenko, and T
Darrell
Natural language object retrieval
Computer Vision and  Pattern Recognition (CVPR), N0NN
N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 In International Conference on Machine Learning (ICML),  N0NN
N  [NN] H
Izadinia, F
Sadeghi, S
K
Divvala, H
Hajishirzi, Y
Choi,  and A
Farhadi
Segment-phrase table for semantic segmentation, visual entailment and paraphrasing
In International  Conference on Computer Vision (ICCV), N0NN
N  [NN] D
Jayaraman and K
Grauman
Zero-shot recognition with  unreliable attributes
In Neural Information Processing Systems (NIPS), N0NN
N  [NN] J
Johnson, B
Hariharan, L
van der Maaten, L
Fei-Fei, C
L
 Zitnick, and R
Girshick
CLEVR: A diagnostic dataset for  compositional language and elementary visual reasoning
In  Computer Vision and Pattern Recognition (CVPR), N0NN
N  [N0] S
Kazemzadeh, V
Ordonez, M
Matten, and T
L
Berg
 ReferItGame: Referring to objects in photographs of natural scenes
In Conference on Empirical Methods in Natural  Language Processing (EMNLP), N0NN
N  [NN] D
Kingma and J
Ba
ADAM: A method for stochastic optimization
In International Conference on Learning Representations (ICLR), N0NN
N  [NN] R
Kiros, R
Salakhutdinov, and R
S
Zemel
Unifying  visual-semantic embeddings with multimodal neural language models
Transactions of the Association for Computational Linguistics (TACL), N0NN
N  [NN] A
Kovashka, D
Parikh, and K
Grauman
WhittleSearch:  Image search with relative attribute feedback
In Computer  Vision and Pattern Recognition (CVPR), N0NN
N, N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Neural Information Processing Systems (NIPS), N0NN
N  [NN] C
H
Lampert, H
Nickisch, and S
Harmeling
Attributebased classification for zero-shot visual object categorization
IEEE Transactions on Pattern Analysis and Machine  Intelligence (TPAMI), NN(N):NNN–NNN, N0NN
N, N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft COCO: Common objects in context
In European Conference on Computer Vision (ECCV), N0NN
N  [NN] R
Luo and G
Shakhnarovich
Comprehension-guided referring expressions
In Computer Vision and Pattern Recognition (CVPR), N0NN
N  [NN] S
Maji
Discovering a lexicon of parts and attributes
In  Workshop on Parts and Attributes, European Conference on  Computer Vision (ECCV), N0NN
N, N  [NN] S
Maji
A taxonomy of part and attribute discovery techniques
In Visual Attributes, pages NNN–NNN
Springer International Publishing, N0NN
N  [N0] S
Maji, E
Rahtu, J
Kannala, M
Blaschko, and A
Vedaldi
 Fine-grained visual classification of aircraft
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N, N, N  [NN] J
Mao, J
Huang, A
Toshev, O
Camburu, A
Yuille, and  K
Murphy
Generation and comprehension of unambiguous  object descriptions
Computer Vision and Pattern Recognition (CVPR), N0NN
N  [NN] M
Mitchell, K
Van Deemter, and E
Reiter
Generating expressions that refer to visible objects
In HLT-NAACL, N0NN
 N  [NN] V
K
Nagaraja, V
I
Morariu, and L
S
Davis
Modeling  context between objects for referring expression understandNNN    ing
In European Conference on Computer Vision (ECCV),  N0NN
N  [NN] V
Nair and G
E
Hinton
Rectified linear units improve restricted boltzmann machines
In International Conference  on Machine Learning (ICML), N0N0
N  [NN] D
Parikh and K
Grauman
Interactively building a discriminative vocabulary of nameable attributes
In Computer Vision  and Pattern Recognition (CVPR), N0NN
N  [NN] D
Parikh and K
Grauman
Relative attributes
In International Conference on Computer Vision (ICCV), N0NN
N  [NN] S
Reed, Z
Akata, H
Lee, and B
Schiele
Learning deep  representations of fine-grained visual descriptions
In Computer Vision and Pattern Recognition (CVPR), N0NN
N  [NN] F
Sadeghi, S
K
Kumar Divvala, and A
Farhadi
VISKE:  Visual knowledge extraction and question answering by visual verification of relation phrases
In Computer Vision and  Pattern Recognition (CVPR), N0NN
N  [NN] M
A
Sadeghi and A
Farhadi
Recognition using visual phrases
In Computer Vision and Pattern Recognition  (CVPR), N0NN
N, N  [N0] W
J
Scheirer, N
Kumar, P
N
Belhumeur, and T
E
Boult
 Multi-attribute spaces: Calibration for attribute fusion and  similarity search
In Computer Vision and Pattern Recognition (CVPR), N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
CoRR,  abs/NN0N.NNNN, N0NN
N  [NN] N
Turakhia and D
Parikh
Attribute dominance: What pops  out? In Computer Vision and Pattern Recognition (CVPR),  N0NN
N  [NN] L
van der Maaten and G
Hinton
Visualizing data using tsne
Journal of Machine Learning Research (JMLR), N(Nov),  N00N
N  [NN] A
Vedaldi, S
Mahendran, S
Tsogkas, S
Maji, B
Girshick,  J
Kannala, E
Rahtu, I
Kokkinos, M
B
Blaschko, D
Weiss,  B
Taskar, K
Simonyan, N
Saphra, and S
Mohamed
Understanding objects in detail with fine-grained attributes
In  Computer Vision and Pattern Recognition (CVPR), N0NN
N,  N  [NN] R
Vedantam, S
Bengio, K
Murphy, D
Parikh, and  G
Chechik
Context-aware captions from context-agnostic  supervision
In Computer Vision and Pattern Recognition  (CVPR), N0NN
N  [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In Computer Vision  and Pattern Recognition (CVPR), N0NN
N, N  [NN] C
Wah, G
Van Horn, S
Branson, S
Maji, P
Perona, and  S
Belongie
Similarity comparisons for interactive finegrained categorization
In Computer Vision and Pattern  Recognition (CVPR), N0NN
N  [NN] L
Yu, P
Poirson, S
Yang, A
C
Berg, and T
L
Berg
Modeling context in referring expressions
In European Conference on Computer Vision (ECCV), N0NN
N  [NN] L
Yu, H
Tan, M
Bansal, and T
L
Berg
A joint speakerlistener-reinforcer model for referring expressions
In Computer Vision and Pattern Recognition (CVPR), N0NN
N  [N0] P
Zhang, Y
Goyal, D
Summers-Stay, D
Batra, and  D
Parikh
Yin and Yang: Balancing and answering binary  visual questions
In Computer Vision and Pattern Recognition (CVPR), N0NN
N  NNNFlow-Guided Feature Aggregation for Video Object Detection   Flow-Guided Feature Aggregation for Video Object Detection  Xizhou ZhuN,N∗ Yujie WangN∗ Jifeng DaiN Lu YuanN Yichen WeiN  NUniversity of Science and Technology of China NMicrosoft Research  ezra0N0N@mail.ustc.edu.cn {v-yujiwa,jifdai,luyuan,yichenw}@microsoft.com  Abstract  Extending state-of-the-art object detectors from image to  video is challenging
The accuracy of detection suffers from  degenerated object appearances in videos, e.g., motion blur,  video defocus, rare poses, etc
Existing work attempts to exploit temporal information on box level, but such methods  are not trained end-to-end
We present flow-guided feature  aggregation, an accurate and end-to-end learning framework for video object detection
It leverages temporal coherence on feature level instead
It improves the per-frame  features by aggregation of nearby features along the motion paths, and thus improves the video recognition accuracy
Our method significantly improves upon strong singleframe baselines in ImageNet VID [NN], especially for more  challenging fast moving objects
Our framework is principled, and on par with the best engineered systems winning the ImageNet VID challenges N0NN, without additional  bells-and-whistles
The code would be released
 N
Introduction  Recent years have witnessed significant progress in object detection [NN]
State-of-the-art methods share a similar  two-stage structure
Deep Convolutional Neural Networks  (CNNs) [NN, NN, N0, NN] are firstly applied to generate a  set of feature maps over the whole input image
A shallow  detection-specific network [NN, N0, N0, NN, N] then generates  the detection results from the feature maps
 These methods achieve excellent results in still images
 However, directly applying them for video object detection  is challenging
The recognition accuracy suffers from deteriorated object appearances in videos that are seldom observed in still images, such as motion blur, video defocus,  rare poses, etc (See an example in Figure N and more in Figure N)
As quantified in experiments, a state-of-the-art stillimage object detector (R-FCN [N] + ResNet-N0N [NN]) deteriorates remarkably for fast moving objects (Table N (a))
 ∗This work is done when Xizhou Zhu and Yujie Wang are interns at  Microsoft Research Asia  Nevertheless, the video has rich information about the  same object instance, usually observed in multiple “snapshots” in a short time
Such temporal information is exploited in existing video object detection methods [NN, NN,  NN, NN] in a simple way
These methods firstly apply object detectors in single frames and then assemble the detected bounding boxes across temporal dimension in a dedicated post processing step
This step relies on off-theshelf motion estimation such as optical flow, and handcrafted bounding box association rules such as object tracking
In general, such methods manipulate the single-frame  detection boxes with mediocre qualities but do not improve  the detection quality
The performance improvement is  from heuristic post-processing instead of principled learning
There is no end-to-end training
In this work, these  techniques are called box level methods
 We attempt to take a deeper look at video object detection
We seek to improve the detection or recognition quality by exploiting temporal information, in a principled way
 As motivated by the success in image recognition [NN], feature matters, and we propose to improve the per-frame feature learning by temporal aggregation
Note that the features of the same object instance are usually not spatially  aligned across frames due to video motion
A naive feature  aggregation may even deteriorate the performance, as elaborated in Table N (b) later
This suggests that it is critical to  model the motion during learning
 In this work, we propose flow-guided feature aggregation (FGFA)
As illustrated in Figure N, the feature extraction network is applied on individual frames to produce the  per-frame feature maps
To enhance the features at a reference frame, an optical flow network [N] estimates the motions between the nearby frames and the reference frame
 The feature maps from nearby frames are warped to the reference frame according to the flow motion
The warped features maps, as well as its own feature maps on the reference  frame, are aggregated according to an adaptive weighting  network
The resulting aggregated feature maps are then  fed to the detection network to produce the detection result  on the reference frame
All the modules of feature extraction, flow estimation, feature aggregation, and detection are  NN0N    flow fieldflow field  aggregation  …  … …  …  �featurewarping � featurewarping  filter #NNN0  t-N0  t  filter #NNN0  t  aggregated feature maps detection result filter #NNN0  t t  t+N0  filter #NNN0  t+N0  t-N0  Figure N
Illustration of FGFA (flow-guided feature aggregation)
 For each input frame, a feature map sensitive to “cat” is visualized
 The feature activations are low at the reference frame t, resulting in  detection failure in the reference frame
The nearby frames t− N0 and t + N0 have high activations
After FGFA, the feature map at the reference frame is improved and detection on it succeeds
 trained end-to-end
 Compared with box level methods, our approach works  on feature level, performs end-to-end learning and is complementary (e.g., to Seq-NMS [NN])
It improves the perframe features and generates high quality bounding boxes
 The boxes can be further refined by box-level methods
 Our approach is evaluated on the large-scale ImageNet  VID dataset [NN]
Rigorous ablation study verifies that it  is effective and significantly improves upon strong singleframe baselines
Combination with box-level methods produces further improvement
We report object detection accuracy on par with the best engineered systems winning  the ImageNet VID challenges, without additional bellsand-whistles (e.g., model ensembling, multi-scale training/testing, etc.)
 In addition, we perform an in-depth evaluation according to the object motion magnitude
The results indicate  that the fast moving objects are far more challenging than  slow ones
This is also where our approach gains the most
 Our method can make effective use of the rich appearance  information in the varied snapshots of fast moving objects
 N
Related Work  Object detection from image
State-of-the-art methods for general object detection [N0, N0, NN, N] are mainly  based on deep CNNs [NN, NN, N0, NN]
In [NN], a multistage pipeline called Regions with Convolutional Neural  Networks (R-CNN) is proposed for training deep CNN to  classify region proposals for object detection
To speedup,  ROI pooling is introduced to the feature maps shared on  the whole image in SPP-Net [NN] and Fast R-CNN [N0]
 In Faster R-CNN [N0], the region proposals are generated  by the Region Proposal Network (RPN), and features are  shared between RPN and Fast R-CNN
Most recently, RFCN [N] replaces ROI pooling operation on the intermediate feature maps with position-sensitivity ROI pooling operation on the final score maps, pushing the feature sharing  to an extreme
 In contrast to these methods of still-image object detection, our method focuses on object detection in videos
It  incorporates temporal information to improve the quality of  convolutional feature maps, and can easily benefit from the  improvement of still-image object detectors
 Object detection in video
Recently, ImageNet introduces a new challenge for object detection from videos  (VID), which brings object detection into the video domain
 In this challenge, nearly all of existing methods incorporate  temporal information only on the final stage “ boundingbox post-processing”
T-CNN [NN, NN] propagates predicted  bounding boxes to neighboring frames according to precomputed optical flows, and then generates tubelets by applying tracking algorithms from high-confidence bounding  boxes
Boxes along the tubelets are re-scored based on  tubelets classification
Seq-NMS [NN] constructs sequences  along nearby high-confidence bounding boxes from consecutive frames
Boxes of the sequence are re-scored to the  average confidence, other boxes close to this sequence are  suppressed
MCMOT [NN] formulates the post-processing  as a multi-object tracking problem
A series of hand-craft  rules (e.g., detector confidences, color/motion clues, changing point detection and forward-backward validation) are  used to determine whether bounding boxes belong to the  tracked objects, and to further refine the tracking results
 Unfortunately, all of these methods are multi-stage pipeline,  where results in each stage would rely on the results from  previous stages
Thus, it is difficult to correct errors produced by previous stages
 By contrast, our method considers temporal information  at the feature level instead of the final box level
The entire system is end-to-end trained for the task of video object detection
Besides, our method can further incorporate  such bounding-box post-processing techniques to improve  the recognition accuracy
 Motion estimation by flow
Temporal information  in videos requires correspondences in raw pixels or feaN0N    tures to build the relationship between consecutive frames
 Optical flow is widely used in many video analysis and  processing
Traditional methods are dominated by variational approaches [N, NN], which mainly address small  displacements [NN]
The recent focus is on large displacements [N], and combinatorial matching (e.g., DeepFlow [NN], EpicFlow [NN]) has been integrated into the variational approach
These approaches are all hand-crafted
 Deep learning based methods (e.g., FlowNet [N] and its  successors [NN, NN]) have been exploited for optical flow  recently
The most related work to ours is deep feature  flow [NN], which shows the information redundancy in  video can be exploited to speed up video recognition at minor accuracy drop
It shows the possibility of joint training  the flow sub-network and the recognition sub-network
 In this work, we focus on another aspect of associating  and assembling the rich appearance information in consecutive frames to improve the feature representation, and then  the video recognition accuracy
We follow the design of  deep feature flow to enable feature warping across frames
 Feature aggregation
Feature aggregation is widely  used in action recognition [NN, N0, NN, NN, NN, N, NN, NN]  and video description [N, NN]
On one hand, most of  these work [NN, NN, NN, N, NN, N, N, NN] use recurrent neural network (RNNs) to aggregate features from consecutive frames
On the other hand, exhaustive spatial-temporal  convolution is used to directly extract spatial-temporal features [NN, NN, NN, NN]
However, the convolutional kernel size in these methods may limit the modeling of fastmoving objects
To address this issue, a large kernel size  should be considered, but it will greatly increase the parameter number, brining issues of overfitting, computational  overhead and memory consumption
By contrast, our approach relies on flow-guided aggregation, and can be scalable to different types of object motion
 Visual tracking
Recently, deep CNNs have been used  for object tracking [NN, NN] and achieved impressive tracking accuracy
When tracking a new target, a new network  is created by combining the shared layers in the pre-trained  CNN with a new binary classification layer, which is online  updated
Tracking is apparently different from the video  object detection task, because it assumes the initial localization of an object in the first frame and it does not require  predicting class labels
 N
Flow Guided Feature Aggregation  N.N
A Baseline and Motivation  Given the input video frames {Ii}, i = N, 


,∞, we aim to output object bounding boxes on all the frames, {yi}, i = N, 


,∞
A baseline approach is to apply an off-the-shelf object detector to each frame individually
 Modern CNN-based object detectors share a similar  rare poses  … …  video defocus  … …  motion blur  … …  part occlusion  … …  Figure N
Typical deteriorated object appearance in videos
 structure [NN, N0, N0, NN, N]
A deep convolutional subnetwork Nfeat, is applied on the input image I , to produce feature maps f = Nfeat(I) on the whole image
A shal- low detection-specific sub-network, Ndet, is applied on the feature maps to generate the output, y = Ndet(f)
 Video frames contain drastic appearance changes of the  same object instance, as exemplified in Figure N
Detection  on single frames generates unstable results and fails when  appearance is poor
Figure N presents an example
The feature responses for “cat” category are low at the reference  frame t due to motion blur
This causes single frame detection failure
Observing that the nearby frames t − N0 and t+N0 have high responses, their features can be propagated to the reference frame
After the features on the reference  frame is enhanced, detection on it succeeds
 Two modules are necessary for such feature propagation  and enhancement: N) motion-guided spatial warping
It estimates the motion between frames and warps the feature  maps accordingly
N) feature aggregation module
It figures  out how to properly fuse the features from multiple frames
 Together with the feature extraction and detection networks,  these are the building blocks of our approach
They are  elaborated below
 N.N
Model Design  Flow-guided warping
As motivated by [NN], given a  reference frame Ii and a neighbor frame Ij , a flow field  Mi→j = F(Ii, Ij) is estimated by a flow network F (e.g., FlowNet [N])
 The feature maps on the neighbor frame are warped to  the reference frame according to the flow
The warping  function is defined as  fj→i = W(fj ,Mi→j) = W(fj ,F(Ii, Ij)), (N)  where W(·) is the bilinear warping function applied on all the locations for each channel in the feature maps, and fj→i denotes the feature maps warped from frame j to frame i
 NN0    Feature aggregation
After feature warping, the reference frame accumulates multiple feature maps from  nearby frames (including its own)
These feature maps  provide diverse information of the object instances (e.g.,  varied illuminations/viewpoints/poses/non-rigid deformations)
For aggregation, we employ different weights at different spatial locations and let all feature channels share the  same spatial weight
The N-D weight maps for warped features fj→i are denoted as wj→i
The aggregated features at  the reference frame f̄i is then obtained as  f̄i = ∑i+K  j=i−K wj→ifj→i, (N)  where K specifies the range of the neighbor frames for aggregation (K = N0 by default)
Equation (N) is similar to the formula of attention models [NN], where varying weights  are assigned to the features in the memory buffer
 The aggregated features f̄i are then fed into the detection  sub-network to obtain the results,  yi = Ndet(f̄i)
(N)  Compared to the baseline and previous box level methods, our method aggregates information from multiple  frames before producing the final detection results
 Adaptive weight
The adaptive weight indicates the importance of all buffer frames [Ii−K , 


, Ii+K ] to the refer- ence frame Ii at each spatial location
Specifically, at location p, if the warped features fj→i(p) is close to the features fi(p), it is assigned to a larger weight
Otherwise, a smaller weight is assigned
Here, we use the cosine similarity metric [NN] to measure the similarity between the warped features and the features extracted from the reference frame
 Moreover, we do not directly use the convolutional features obtained from Nfeat(I)
Instead, we apply a tiny fully convolutional network E(·) to features fi and fj→i, which projects the features to a new embedding for similarity measure and is dubbed as the embedding sub-network
 We estimate the weight by  wj→i(p) = exp( fej→i(p) · f  e i (p)  |fej→i(p)||f e i (p)|  ), (N)  where fe = E(f) denotes embedding features for sim- ilarity measurement, and the weight wj→i is normalized for every spatial location p over the nearby frames,∑i+K j=i−K wj→i(p) = N
The estimation of weight could  be viewed as the process that the cosine similarity between  embedding features passes through the SoftMax operation
 N.N
Training and Inference  Inference
Algorithm N summarizes the inference algorithm
Given an input video of consecutive frames {Ii} and  Algorithm N Inference algorithm of flow guided feature aggregation for video object detection
 N: input: video frames {Ii}, aggregation range K N: for k = N to K + N do ⊲ initialize feature buffer N: fk = Nfeat(Ik) N: end for  N: for i = N to ∞ do ⊲ reference frame N: for j = max(N, i−K) to i+K do ⊲ nearby frames N: fj→i = W(fj ,F(Ii, Ij)) ⊲ flow-guided warp N: fej→i, f  e i = E(fj→i, fi) ⊲ compute embedding features  N: wj→i = exp( fej→i·f  e i  |fe j→i  ||fe i | ) ⊲ compute aggregation weight  N0: end for  NN: f̄i = ∑i+K  j=i−K wj→ifj→i ⊲ aggregate features  NN: yi = Ndet(f̄i) ⊲ detect on the reference frame NN: fi+K+N = Nfeat(Ii+K+N) ⊲ update feature buffer NN: end for  NN: output: detection results {yi}  the specified aggregation range K, the proposed method sequentially processes each frame with a sliding feature buffer  on the nearby frames (of length NK + N in general, except for the beginning and the ending K frames)
At initial, the  feature network is applied on the beginning K + N frames to initialize the feature buffer (LN-LN in Algorithm N)
Then  the algorithm loops over all the video frames to perform  video object detection, and to update the feature buffer
For  each frame i as the reference, the feature maps of the nearby  frames in the feature buffer are warped with respect to it,  and their respective aggregation weights are calculated (LNLN0)
Then the warped features are aggregated and fed to  the detection network for object detection (LNN-LNN)
Before taking the (i+N)-th frame as the reference, the feature maps are extracted on the (i+K+N)-th frame and are added to the feature buffer (LNN)
 As for runtime complexity, the ratio of the proposed  method versus the single-frame baseline is as  r = N + (NK + N) · (O(F) +O(E) +O(W))  O(Nfeat) +O(Ndet) , (N)  where O(·) measures the function complexity
Typically, the complexity of Ndet, E and W can be ignored when they are compared with Nfeat
The ratio is approximated as: r ≈  N+ (NK+N)·O(F) O(Nfeat)  
The increased computation mostly comes  from F 
This is affordable, because the complexity of F is also much lower than that of Nfeat in general
 Training
The entire FGFA architecture is fully differentiable and can be trained end-to-end
The only thing to  note is that the feature warping module is implemented by  bilinear interpolation and also fully differentiable w.r.t
both  of the feature maps and the flow field
 Temporal dropout
In SGD training, the aggregation  range number K is limited by memory
We use a large K in  NNN    inference but a small K(= N by default) in training
This is no problem as the adaptive weights are properly normalized  during training and inference, respectively
Note that during  training, the neighbor frames are randomly sampled from a  large range that is equal to the one during inference
As an  analogy to dropout [NN] technique, this can be considered as  a temporal dropout, by discarding random temporal frames
 As evidenced in Table N, this training strategy works well
 N.N
Network Architecture  We introduce the incarnation of different sub-networks  in our FGFA model
 Flow network
We use FlowNet [N] (“simple” version)
 It is pre-trained on the Flying Chairs dataset [N]
It is applied on images of half resolution and has an output stride  of N
As the feature network has an output stride of NN (see  below), the flow field is downscaled by half to match the  resolution of the feature maps
 Feature network
We adopt the state-of-the-art ResNet  (-N0 and -N0N) [NN] and Inception-Resnet [NN] as the feature network
The original Inception-ResNet is designed for  image recognition
To resolve feature misalignment issue  and make it proper for object detection, We utilize a modified version dubbed as “Aligned-Inception-ResNet”, which  is described in [N]
The ResNet-N0, ResNet-N0N, and the  Aligned-Inception-ResNet models are all pre-trained on ImageNet classification
 The pretrained models are crafted into feature networks  in our FGFA model
We slightly modify the nature of three  models for object detection
We remove the ending average pooling and the fc layer, and retain the convolution layers
To increase the feature resolution, following the practice in [N, N], the effective stride of the last block is changed  from NN to NN
Specially, at the beginning of the last block  (“convN” for both ResNet and Aligned-Inception-ResNet),  the stride is changed from N to N
To retain the receptive  field size, the dilation of the convolutional layers (with kernel size > N) in the last block is set as N
Finally, a randomly  initialized N× N convolution is applied on top to reduce the feature dimension to N0NN
 Embedding network
It has three layers: a N× N× NNN convolution, a N× N× NNN convolution, and a N× N× N0NN convolution
It is randomly initialized
 Detection network
We use state-of-the-art R-FCN [N]  and follow the design in [NN]
On top of the N0NN-d feature  maps, the RPN sub-network and the R-FCN sub-network  are applied, which connect to the first NNN-d and the last  NNN-d features respectively
N anchors (N scales and N aspect  ratios) are utilized in RPN, and N00 proposals are produced  on each image
The position-sensitive score maps in R-FCN  are of N× N groups
 N
Experiments  N.N
Experiment Setup  ImageNet VID dataset [NN]
It is a prevalent large-scale  benchmark for video object detection
Following the protocols in [NN, NN], model training and evaluation are performed on the N,NNN video snippets from the training set and  the NNN snippets from the validation set, respectively
The  snippets are fully annotated, and are at frame rates of NN or  N0 fps in general
There are N0 object categories
They are  a subset of the categories in the ImageNet DET dataset
 Slow, medium, and fast motion
For better analysis, the  ground truth objects are categorized according to their motion speed
An object’s speed is measured by its averaged  intersection-over-union (IoU) scores with its corresponding  instances in the nearby frames (±N0 frames)
The indicator is dubbed as “motion IoU”
The lower the motion IoU is,  the faster the object moves
Figure N presents the histogram  of all motion IoU scores
According to the score, the objects are divided into slow (score > 0.N), medium (score ∈ [0.N, 0.N]), and fast (score < 0.N) groups, respectively
Examples from various groups are shown in Figure N
 In evaluation, besides the standard mean averageprecision (mAP) scores, we also report the mAP scores over  the slow, medium, and fast groups, respectively, denoted as  mAP(slow), mAP(medium), and mAP(fast)
This provides  us a more detailed analysis and in-depth understanding
 Implementation details
During training, following  [NN, NN], both the ImageNet DET training and the ImageNet  VID training sets are utilized
Two-phase training is performed
In the first phase, the feature and the detection networks are trained on ImageNet DET, using the annotations  of the N0 categories as in ImageNet VID
SGD training is  performed, with one image at each mini-batch
NN0K iterations are performed on N GPUs, with each GPU holding  one mini-batch
The learning rates are N0−N and N0−N in the first N0K and in the last N0K iterations, respectively
In  the second phase, the whole FGFA model is trained on ImageNet VID, where the feature and the detection networks  are initialized from the weights learnt in the first phase
N0K  iterations are performed on N GPUs, with learning rates of  N0−N and N0−N in the first N0K and in the last N0K iterations, respectively
In both training and inference, the images are  resized to a shorter side of N00 pixels for the feature network, and a shorter side of N00 pixels for the flow network
 Experiments are performed on a workstation with Intel ENNNN0 vN CPU N.NGHz and Nvidia KN0 GPU
 N.N
Ablation Study  FGFA Architecture Design Table N compares our  FGFA with the single-frame baseline and its variants
 Method (a) is the single-frame baseline
It has a mAP  NN.N% using ResNet-N0N
It is close to the NN.N% mAP  NNN    Nfeat ResNet-N0 ResNet-N0N  methods (a) (b) (c) (d) (e) (a) (b) (c) (d) (e)  multi-frame feature aggregation? X X X X X X X X  adaptive weights? X X X X X X  flow-guided? X X X X  end-to-end training? X X X X X X  mAP (%) N0.N NN.N↓N.0 NN.N↑N.N NN.0↑N.N NN.N↑N.N NN.N NN.0↓N.N NN.N↑0.N NN.N↑N.N NN.N↑N.N  mAP (%) (slow) NN.N NN.N↑N.N NN.N↑N.N NN.N↑N.N NN.N↑N.0 NN.N NN.N↓0.N NN.N↓0.N NN.N↑N.N NN.N↑0.N  mAP (%) (medium) NN.N NN.N↑N.N NN.N↑N.N NN.N↑N.0 NN.N↑N.N NN.N NN.N↑N.N NN.N↑N.0 NN.N↑N.N NN.N↑N.0  mAP (%) (fast) N0.N NN.N↓N.N N0.N↑0.N NN.0↑N.N NN.N↑N.N NN.N NN.N↓N.N NN.N↑0.N NN.N↑N.N NN.N↑N.N  runtime (ms) N0N N0N NN0 NNN NNN NNN NNN N0N NNN NNN  Table N
Accuracy and runtime of different methods on ImageNet VID validation, using ResNet-N0 and ResNet-N0N feature extraction  networks
The relative gains compared to the single-frame baseline (a) are listed in the subscript
 N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0.N 0 motion IoU  0  0.N  0.N  0.N  0.N  0.0N  0.NN  0.NN  0.NN  pr op  or tio  n  slow NN.N%  medium NN.N%  fast NN.N%  Figure N
Histogram of the motion IoUs of all ground truth object  instances, and the division of slow, medium and fast groups
 …  …  …  …  …  …  tt-N0 t+N0  slow  medium  fast  Figure N
Example video snippets of object instances with slow,  medium and fast motions
The motion IoUs are 0.NN, 0.NN and  0.NN, respectively
 instance size small middle large  mAP (%) NN.N NN.N NN.N  mAP (%) (slow) NN.N NN.N NN.N  mAP (%) (medium) NN.N NN.N N0.N  mAP (%) (fast) NN.N NN.N NN.N  Table N
Detection accuracy of small (area< N0N pixels), medium (N0N ≤area≤ NN0Npixels), and large (area> NN0Npixels) object instances of the single-frame baseline (entry (a)) in Table N
 -N0 -N 0 N N0 0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  ad ap  tiv e  w ei  gh t  offset from reference frame  slow medium fast   -N0 -N 0 N N0 0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  0.0N  0.0NN  offset from reference frame  ad ap  tiv e  w ei  gh t  slow medium fast   Figure N
Adaptive weight distribution over frames
Left: entry  without flow-guided feature warping (Table N (c)); Right: entry  with flow-guided feature warping (Table N (d))
The histogram is  performed within the boxes of instances with varying motions
 in [NN], which is also based on R-FCN and ResNet-N0N
 This indicates that our baseline is competitive and serves as  a valid reference for evaluation
Note that we do not add  bells and whistles like multi-scale training/testing, exploiting context information, model ensemble, etc., in order to  facilitate comparison and draw clear conclusions
 Evaluation on motion groups shows that detecting fast  moving objects is very challenging: mAP is NN.N% for slow  motion, and it drops to NN.N% for fast motion
As objects of  different sizes may have different motion speed, we further  analyze the influence of the object size
Table N presents the  mAP scores of small, middle, and large objects of different  motion speeds
It shows that “fast motion” is an intrinsic  challenge, irrespective to how large the object is
 Method (b) is a naive feature aggregation approach and a  degenerated variant of FGFA
No flow motion is used
The  flow map Mi→j is set to all zeros in Eq
(N)
No adaptive weighting is used
The weight wi→j is set to N  NK+N in  Eq
(N)
The variant is also trained end-to-end in the same  way as FGFA
The mAP decreases to NN.0% using ResNetN0N, N.N% shy of baseline (a)
The decrease for fast motion  (NN.N% → NN.N%) is much more significant than that for slow motion (NN.N% → NN.N%)
It indicates that it is criti- cal to consider motion in video object detection
 NNN    # training frames N* N  # testing frames N N N NN NN NN* NN N N N NN NN NN NN  mAP (%) N0.N NN.N NN.N NN.N NN.N NN.0 NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  runtime (ms) N0N NN0 N0N NNN NNN NNN NNN N0N NN0 N0N NNN NNN NNN NNN  Table N
Results of using different number of frames in training and inference, using ResNet-N0
Default parameters are indicated by *
 method feature network mAP (%) runtime (ms)  single-frame baseline  ResNet-N0N  NN.N NNN  + MGP NN.N NNN*  + Tubelet rescoring NN.N NNNN  + Seq-NMS NN.N NNN*  FGFA  ResNet-N0N  NN.N NNN  + MGP NN.N N0NN*  + Tubelet rescoring NN.N NNNN  + Seq-NMS NN.N NNN*  FGFA AlignedInception-ResNet  NN.N NNN  + Seq-NMS N0.N NNN*  Table N
Results of baseline method and FGFA before and after combination with box level techniques
As for runtime, entry marked with * utilizes CPU implementation of box-level techniques
 Method (c) adds the adaptive weighting module into (b)
 It obtains a mAP NN.N%, N.N% higher than that of (b)
Note  that adding the adaptive weighting scheme is of little help  for mAP (slow) and mAP (medium), but is important for  mAP (fast) (NN.N% → NN.N%)
Figure N (Left) shows that the adaptive weights for the fast moving instances concentrate on the frames close to the reference, which have relatively small displacement w.r.t
the reference in general
 Method (d) is the proposed FGFA method, which adds  the flow-guided feature aggregation module to (c)
It increases the mAP score by N% to NN.N%
The improvement  for fast motion is more significant (NN.N% → NN.N%)
Fig- ure N shows that the adaptive weights in (d) distribute more  evenly over neighbor frames than (c), and it is most noticable for fast motion
It suggests that the flow-guided feature aggregation effectively promotes the information from  nearby frames in feature aggregation
The proposed FGFA  method improves the overall mAP score by N.N%, and mAP  (fast) by N.N% compared to the single-frame baseline (a)
 Some example results are shown in Figure N
 Method (e) is a degenerated version of (d) without using  end-to-end training
It takes the feature and the detection  sub-networks from the single-frame baseline (a), and the  pre-trained off-the-shelf FlowNet
During training, these  modules are fixed and only the embedding sub-network is  learnt
It is clearly worse than (d)
This indicates the importance of end-to-end training in FGFA
 As to runtime, the proposed FGFA method takes NNNms  to process one frame, using ResNet-N0N and FlowNet
It is  slower than the single-frame baseline (NNNms) because the  flow network is evaluated NK + N(K = N0) times for each frame
To reduce the number of evaluation, we also experimented with another version of FGFA, in which the flow  network is only applied on adjacent frame pairs
The flow  field between non-adjacent frames is obtained by compositing the intermediate flow fields
In this way, the flow field  computation on each adjacent frame pair can be re-used for  different reference frames
The per-frame computation time  of FGFA is reduced to NNNms, much faster than NNNms
The  accuracy is slightly decreased (∼ N%) due to error accumu- lation in flow field composition
 # frames in training and inference Due to memory issues, we use the lightweight ResNet-N0 in this experiment
 We tried N and N frames in each mini-batch during SGD  training (N frame reaches the memory cap), and N, N, N, NN,  NN, NN, and NN frames in inference
Results in Table N show  that training with N and N frames achieves very close accuracy
This verifies the effectiveness of our temporal dropout  training strategy
In inference, as expected, the accuracy improves as more frames are used
The improvement saturates  at NN frames
By default, we sample N frames in training and  aggregate over NN frames in inference
 N.N
Combination with Box-level Techniques  Our approach focuses on improving feature quality and  recognition accuracy in video frames
The output object  boxes can be further improved by previous box-level techniques as post-processing
In particular, we tested three  prevalent techniques, namely, motion guided propagation  (MGP) [NN], Tubelet rescoring [NN], and Seq-NMS [NN]
 Note that MGP and Tubelet rescoring are used in the winning entry of ImageNet VID challenge N0NN [NN]
We utilized the official public code for MGP and Tubelet rescoring, and re-implemented Seq-NMS
 Table N presents the results
The three techniques  are firstly combined with our single-frame baseline using  ResNet-N0N model
They all improve the baseline
This indicates that such post-processing techniques are effective
 Between them, Seq-NMS obtains the largest gain
When  they are combined with FGFA using ResNet-N0N model,  no improvement is observed for MGP and Tubelet rescoring
However, Seq-NMS is still effective (mAP increased to  NN.N%)
By using Aligned-Inception-ResNet as the feature  network, the mAP of FGFA+Seq-NMS is further improved  NNN    horse: 0.N0  baseline  baseline  baseline  FGFA  FGFA  FGFA  horse: 0.NN horse: 0.NN  horse: 0.NN  horse: 0.NN  horse: 0.NN horse: 0.NN horse: 0.NN  car: 0.NN  horse: 0.NN  horse: 0.NN  horse: 0.NN  dog: 0.NN  horse: 0.NN  dog: 0.NN  horse: 0.NN  horse: 0.NN  bicycle: 0.NN  horse: 0.NN  horse: 0.NN  fox: 0.NN  fox: 0.NN  fox: 0.NN  fox: 0.NN  fox: 0.NN  fox: 0.NN  fox: 0.N0  lion: 0.NN antelope: 0.NNdog: 0.NN  fox: 0.N0  Figure N
Example video clips where the proposed FGFA method improves over the single-frame baseline (using ResNet-N0N)
The  green and yellow boxes denote correct and incorrect detections, respectively
More examples are available at https://youtu.be/  RNhNDbTPvVg
 to N0.N%, showing that Seq-NMS is highly complementary  to FGFA
 Comparison with state-of-the-art systems Unlike image object detection, the area of video object detection lacks  principled metrics [NN] and guidelines for evaluation and  comparison
Existing leading entries in ImageNet VID  challenge N0NN and N0NN show impressive results, but they  are complex and highly engineered systems with various  bells and whistles
This makes direct and fair comparison  between different works difficult
 This work aims at a principled learning framework for  video object detection instead of the best system
The solid  improvement of FGFA over a strong single frame baseline verifies the effectiveness of our approach
As a reference, the winning entry of ImageNet VID challenge N0NN  (NUIST Team) [NN] obtains NN.N% mAP on ImageNet VID  validation
It uses various techniques like model ensembling, cascaded detection, context information, and multiscale inference
In contrast, our approach does not use these  techniques (only Seq-NMS is used) and achieves best mAP  at N0.N%
Thus, we conclude that our approach is highly  competitive with even the currently best engineered system
 N
Conclusion and Future Work  This work presents an accurate, end-to-end and principled learning framework for video object detection
Because our approach focuses on improving feature quality,  it would be complementary to existing box-level framework for better accuracy in video frames
Several important aspects are left for further exploration
Our method  slows down a bit, and it would be possibly sped up by more  lightweight flow networks
There is still large room to be  improved in fast object motion
More annotation data (e.g.,  YouTube-BoundingBoxes [NN]) and precise flow estimation  may be benefit to improvements
Our method can further  leverage better adaptive memory scheme in the aggregation  instead of the attention model used
We believe these open  questions will inspire more future work
 NNN  https://youtu.be/RNhNDbTPvVg https://youtu.be/RNhNDbTPvVg   References  [N] N
Ballas, L
Yao, C
Pal, and A
Courville
Delving deeper  into convolutional networks for learning video representations
In ICLR, N0NN
N  [N] T
Brox, A
Bruhn, N
Papenberg, and J
Weickert
High accuracy optical flow estimation based on a theory for warping
 In ECCV, N00N
N  [N] T
Brox and J
Malik
Large displacement optical flow: descriptor matching in variational motion estimation
TPAMI,  N0NN
N  [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected crfs
In ICLR, N0NN
N  [N] J
Dai, Y
Li, K
He, and J
Sun
R-fcn: Object detection via  region-based fully convolutional networks
In NIPS, N0NN
 N, N, N, N  [N] J
Dai, H
Qi, Y
Xiong, Y
Li, G
Zhang, H
Hu, and  Y
Wei
Deformable convolutional networks
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N  [N] J
Donahue, L
Anne Hendricks, S
Guadarrama,  M
Rohrbach, S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual  recognition and description
In CVPR, N0NN
N  [N] A
Dosovitskiy, P
Fischer, E
Ilg, P
Hausser, C
Hazirbas,  V
Golkov, P
v.d
Smagt, D
Cremers, and T
Brox
Flownet:  Learning optical flow with convolutional networks
In ICCV,  N0NN
N, N, N  [N] M
Fayyaz, M
Hajizadeh Saffar, M
Sabokrou, M
Fathy,  R
Klette, and F
Huang
Stfcn: Spatio-temporal fcn for semantic video segmentation
In ACCV Workshop, N0NN
N  [N0] R
Girshick
Fast r-cnn
In ICCV, N0NN
N, N, N  [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In CVPR, N0NN
N, N, N  [NN] W
Han, P
Khorrami, T
Le Paine, P
Ramachandran,  M
Babaeizadeh, H
Shi, J
Li, S
Yan, and T
S
Huang
 Seq-nms for video object detection
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N, N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Spatial pyramid pooling  in deep convolutional networks for visual recognition
In  ECCV, N0NN
N, N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N, N, N  [NN] B
K
Horn and B
G
Schunck
Determining optical flow
In  Artificial intelligence, NNNN
N  [NN] N
Hyeonseob and H
Bohyung
Learning multi-domain convolutional neural networks for visual tracking
In CVPR,  N0NN
N  [NN] E
Ilg, N
Mayer, T
Saikia, M
Keuper, A
Dosovitskiy, and  T
Brox
Flownet N.0: Evolution of optical flow estimation  with deep networks
In CVPR, N0NN
N  [NN] K
Kang, H
Li, J
Yan, X
Zeng, B
Yang, T
Xiao, C
Zhang,  Z
Wang, R
Wang, X
Wang, and W
Ouyang
T-cnn:  Tubelets with convolutional neural networks for object detection from videos
arXiv preprint arxiv:NN0N.0NNNN, N0NN
 N, N, N, N  [NN] K
Kang, W
Ouyang, H
Li, and X
Wang
Object detection  from video tubelets with convolutional neural networks
In  CVPR, N0NN
N, N  [N0] A
Kar, N
Rai, K
Sikka, and G
Sharma
Adascan: Adaptive scan pooling in deep convolutional neural networks for  human action recognition in videos
In CVPR, N0NN
N  [NN] A
Karpathy, G
Toderici, S
Shetty, T
Leung, R
Sukthankar,  and L
Fei-Fei
Large-scale video classification with convolutional neural networks
In CVPR, N0NN
N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, N0NN
N, N  [NN] B
Lee, E
Erdenee, S
Jin, M
Y
Nam, Y
G
Jung, and P
K
 Rhee
Multi-class multi-object tracking using changing point  detection
In ECCV, N0NN
N, N, N  [NN] Z
Li, E
Gavves, M
Jain, and C
G
Snoek
Videolstm  convolves, attends and flows for action recognition
arXiv  preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] W
Lijun, O
Wanli, W
Xiaogang, and L
Huchuan
Visual  tracking with fully convolutional networks
In ICCV, N0NN
 N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
Reed, C.-Y
 Fu, and A
C
Berg
Ssd: Single shot multibox detector
In  ECCV, N0NN
N, N, N  [NN] C
Luo, J
Zhan, L
Wang, and Q
Yang
Cosine normalization: Using cosine similarity instead of dot product in neural  networks
arXiv preprint arXiv:NN0N.0NNN0, N0NN
N  [NN] A
Ranjan and M
J
Black
Optical flow estimation using a  spatial pyramid network
arXiv preprint arXiv:NNNN.00NN0,  N0NN
N  [NN] E
Real, J
Shlens, S
Mazzocchi, X
Pan, and V
Vanhoucke
Youtube-boundingboxes: A large high-precision  human-annotated data set for object detection in video
In  CVPR, N0NN
N  [N0] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
N, N, N  [NN] J
Revaud, P
Weinzaepfel, Z
Harchaoui, and C
Schmid
 Epicflow: Edge-preserving interpolation of correspondences  for optical flow
In CVPR, N0NN
N  [NN] A
M
Rush, S
Chopra, and J
Weston
A neural attention  model for abstractive sentence summarization
In EMNLP,  N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
Berg, and F.-F
Li
Imagenet large scale visual recognition  challenge
In IJCV, N0NN
N, N, N  [NN] S
Sharma, R
Kiros, and R
Salakhutdinov
Action recognition using visual attention
In ICLR Workshop, N0NN
N  [NN] M
Siam, S
Valipour, M
Jagersand, and N
Ray
Convolutional gated recurrent networks for video segmentation
 arXiv preprint arXiv:NNNN.0NNNN, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
In ICLR, N0NN
 N, N  [NN] N
Srivastava, G
E
Hinton, A
Krizhevsky, I
Sutskever, and  R
Salakhutdinov
Dropout: a simple way to prevent neural  networks from overfitting
In JMLR, N0NN
N  NNN    [NN] L
Sun, K
Jia, D.-Y
Yeung, and B
E
Shi
Human action  recognition using factorized spatio-temporal convolutional  networks
In ICCV, N0NN
N  [NN] C
Szegedy, S
Ioffe, V
Vanhoucke, and A
Alemi
InceptionvN, inception-resnet and the impact of residual connections  on learning
arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [N0] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
N, N  [NN] D
Tran, L
Bourdev, R
Fergus, L
Torresani, and M
Paluri
 Learning spatiotemporal features with Nd convolutional networks
In ICCV, N0NN
N  [NN] D
Tran, L
Bourdev, R
Fergus, L
Torresani, and M
Paluri
 Deep endNend voxelNvoxel prediction
In CVPR Workshop,  N0NN
N  [NN] J
Weickert, A
Bruhn, T
Brox, and N
Papenberg
A survey  on variational optic flow methods for small displacements
 In Mathematical models for registration and applications to  medical imaging
N00N
N  [NN] P
Weinzaepfel, J
Revaud, Z
Harchaoui, and C
Schmid
 Deepflow: Large displacement optical flow with deep matching
In ICCV, N0NN
N  [NN] J
Yang, H
Shuai, Z
Yu, R
Fan, Q
Ma, Q
Liu,  and J
Deng
Efficient object detection from videos
 http://image-net.org/challenges/talks/  N0NN/ImagenetN0NNVID.pptx, N0NN
N  [NN] L
Yao, A
Torabi, K
Cho, N
Ballas, C
Pal, H
Larochelle,  and A
Courville
Describing videos by exploiting temporal  structure
In ICCV, N0NN
N  [NN] J
Yue-Hei Ng, M
Hausknecht, S
Vijayanarasimhan,  O
Vinyals, R
Monga, and G
Toderici
Beyond short snippets: Deep networks for video classification
In CVPR, N0NN
 N  [NN] H
Zhang and N
Wang
On the stability of video detection  and tracking
arXiv preprint arXiv:NNNN.0NNNN, N0NN
N  [NN] X
Zhu, Y
Xiong, J
Dai, L
Yuan, and Y
Wei
Deep feature  flow for video recognition
In CVPR, N0NN
N, N, N  NNN  http://image-net.org/challenges/talks/N0NN/Imagenet N0NN VID.pptx http://image-net.org/challenges/talks/N0NN/Imagenet N0NN VID.pptxLearning Visual N-Grams From Web Data   Learning Visual N-Grams from Web Data  Ang Li∗  University of Maryland  College Park, MD N0NNN, USA  angli@umiacs.umd.edu  Allan Jabri Armand Joulin Laurens van der Maaten  Facebook AI Research  NN0 Broadway, New York, NY N00NN, USA  {ajabri,ajoulin,lvdmaaten}@fb.com  Abstract  Real-world image recognition systems need to recognize  tens of thousands of classes that constitute a plethora of visual concepts
The traditional approach of annotating thousands of images per class for training is infeasible in such a  scenario, prompting the use of webly supervised data
This  paper explores the training of image-recognition systems on  large numbers of images and associated user comments,  without using manually labeled images
In particular, we  develop visual n-gram models that can predict arbitrary phrases that are relevant to the content of an image
Our  visual n-gram models are feed-forward convolutional net- works trained using new loss functions that are inspired by  n-gram models commonly used in language modeling
We demonstrate the merits of our models in phrase prediction,  phrase-based image retrieval, relating images and captions,  and zero-shot transfer
 N
Introduction  Research on visual recognition models has traditionally  focused on supervised learning models that consider only a  small set of discrete classes, and that learn their parameters  from datasets in which (N) all images are manually annotated for each of these classes and (N) a substantial number of annotated images is available to define each of the  classes
This tradition dates back to early image-recognition  benchmarks such as CalTech-N0N [NN] but is still common  in modern benchmarks such as ImageNet [NN] and COCO  [NN]
The assumptions that are implicit in such benchmarks  are at odds with many real-world applications of imagerecognition systems, which often need to be deployed in  an open-world setting [N]
In the open-world setting, the  number of classes to recognize is potentially very large and  class types are wildly varying [NN]: they include generic  objects such as “dog” or “car”, landmarks such as “Golden  Gate Bridge” or “Times Square”, scenes such as “city park”  ∗This work was done while Ang Li was at Facebook AI Research
 Predicted n-grams lights  Burning Man  Mardi Gras  parade in progress  Predicted n-grams GP  Silverstone Classic  Formula N  race for the  Predicted n-grams navy yard  construction on the  Port of San Diego  cargo  Figure N
Four high-scoring visual n-grams for three images in our  test set according to our visual n-gram model, which was trained  solely on unsupervised web data
We selected the n-grams that  are displayed in the figure from the five highest scoring n-grams  according to our model, in such a way as to minimize word overlap  between the n-grams
For all figures in the paper, we refer the  reader to the supplementary material for license information
 or “street market”, and actions such as “speed walking” or  “public speaking”
The traditional approach of manually  annotating images for training does not scale well to the  open-world setting because of the amount of effort required  to gather and annotate images for all relevant classes
To  circumvent this problem, several recent studies have tried to  use image data from photo-sharing websites such as Flickr  to train their models [N, N, NN, NN, NN, NN, NN, NN, NN]: such  images have no manually curated annotations, but they do  have metadata such as tags, captions, comments, and geolocations that provide weak information about the image  content, and are readily available in nearly infinite numbers
 NNNN    In this paper, we follow [NN] and study the training  of models on images and their associated user comments  present in the YFCCN00M dataset [NN]
In particular, we  aim to take a step in bridging the semantic gap between vision and language by predicting phrases that are relevant to  the contents of an image
We develop visual n-gram models  that, given an image I, assign a likelihood p(w|I) to each possible phrase (n-gram) w
Our models are convolutional networks trained using a loss function that is motivated by  n-gram smoothers commonly used in language modeling [NN, NN]: we develop a novel, differentiable loss function  that optimizes trainable parameters for frequent n-grams, whereas for infrequent n-grams, the loss is dominated by the predicted likelihood of smaller “sub-grams”
The resulting visual n-gram models have substantial advantages over prior open-world visual models [NN]: they recognize  landmarks such as “Times Square”, they differentiate between ‘Washington DC” and the “Washington Nationals”,  and they distinguish between “city park” and “Park City”
 The technical contributions of this paper are threefold:  (N) we are the first to explore the prediction of n-grams rel- evant to image content using convolutional networks, (N) we  develop a novel, differentiable smoothing layer for such networks, and (N) we provide a simple solution to the out-ofvocabulary problem of traditional image-recognition models
We present a series of experiments to demonstrate the  merits of our proposed model in image tagging, image retrieval, image captioning, and zero-shot transfer
 N
Related Work  There is a substantial body of prior work that is related to this study, in particular, work on (N) learning from  weakly supervised web data, (N) relating image content and  language, and (N) language modeling
We give a (nonexhaustive) overview of prior work below
 Learning from weakly supervised web data
Several  prior studies have used Google Images to obtain large collections of (weakly) labeled images for the training of vision  models [N, N, NN, NN, NN, NN, NN]
We do not opt for such an  approach here because it is very difficult to understand the  biases it introduces, in particular, because image retrieval  by Google Images is likely aided by a content-based image  retrieval model itself
This introduces the real danger that  training on data from Google Images amounts to replicating  an existing black-box vision system
Various other studies  have used data from photo-sharing websites such as Flickr  for training; for instance, to train hierarchical topic models [NN] or multiple-instance learning SVMs [NN], to learn  label distribution models [NN, NN], to finetune pretrained  convolutional networks [NN], and to train weak classifiers  that produce additional visual features [NN]
Like this study,  [NN] trains convolutional networks on the image-comment  pairs
Our study differsN from [NN] in that we do not just  consider single words, as a result of which our models distinguish between, e.g., “city park” and “Park City”
 Relating image content and language
Our approach  is connected to a wide body of work that aims at bridging the semantic gap between vision and language [NN]
In  particular, many studies have explored this problem in the  context of image captioning
Most image-captioning systems train a recurrent network or maximum entropy language model on top of object classifications produced by a  convolutional network; the models are either trained separately [NN, NN, NN] or end-to-end [NN, NN]
We do not consider recurrent networks in our study because test-time inference in such networks is slow, which hampers the deployment of such models in real-world applications
An  image-captioning study that is closely related to our work  is [NN], which trains a bilinear model that outputs phrase  probabilities given an image feature and combines the relevant phrases into a caption using a collection of heuristics
 Several other works have explored joint embedding of images and text, either at the word level [N0] or at the sentence level [NN, N0]
What distinguishes our study is that  prior work is generally limited in the variety of visual concepts it can deal with; these studies rely on vision models  that recognize only small numbers of classes and / or on  the availability of “ground-truth” captions that describe the  image content — such captions are very different from a  typical user comment on Flickr
In contrast to prior work,  we consider the open-world setting with very large numbers of visual concepts, and we do not rely on ground-truth  captions provided by human annotators
Our study is most  similar to that of [N0], which uses n-gram to generate image descriptions; unlike [N0], we we do not rely on separately  trained image-classification pipelines
Instead, we train our  model end-to-end on a dataset without ground-truth labels
 Language models
Several prior studies have used  phrase embeddings for natural language processing tasks  such as named entity recognition [NN], text classification [NN, NN, NN], and machine translation [NN, NN]
These  studies differ from our work in that they focus solely on  language modeling and not on visual recognition
Our  models are inspired by smoothing techniques used in traditional n-gram language modelsN, in particular, Jelinek- Mercer smoothing [NN]
Our models differ from traditional  n-gram language models in that they are image-conditioned and parametric: whereas n-gram models count the fre- quency of n-grams in a text corpus to produce a distribu- tion over phrases or sentences, our model measures phrase  likelihoods by evaluating inner products between image features and learned parameter vectors
 NIndeed, the models in [NN] are a special case of our models in which  only unigrams are considered
NA good overview of these techniques is given in [N, NN]
 NNNN    N
Learning Visual N-Gram Models  Below, we describe the dataset we use in our experiments, the loss functions we optimize, and the training procedure we use for optimization
 N.N
Dataset  We train our models on the YFCCN00M dataset, which  contains NN.N million images and associated multi-lingual  user comments [NN]
We applied a simple language detector  to the dataset to select only images with English user comments, leaving a total of N0 million examples for training  and testing
We preprocessed the text by removing punctuations, and we added [BEGIN] and [END] tokens at the  beginning and end of each sentence
We preprocess all images by rescaling them to NNN×NNN pixels (using bicubic interpolation), cropping the central NNN×NNN, subtracting the mean pixel value of each image, and dividing by the  standard deviation of the pixel values
 For most experiments, we use a dictionary of all English  n-grams (with n between N and N) with more than N, 000 oc- currences in the N0 million English comments
This dictionary contains NNN, N0N n-grams: NN, NNN unigrams, NN, NN0 bigrams, NN, NN0 trigrams, NN, NNN four-grams, and NN, NNN five-grams
We emphasize that the smoothed visual n-gram models we describe below are trained and evaluated on all  n-grams in the dataset, even if these n-grams are not in the dictionary
However, whereas the probability of indictionary n-grams is primarily a function of parameters that are specifically tuned for those n-grams, the probability of out-of-dictionary n-grams is composed from the proba- bility of smaller in-dictionary n-grams (details below)
 N.N
Loss functions  The main contribution of this paper is in the loss functions we use to train our phrase prediction models
In  particular, we explore (N) a naive n-gram loss that measures the (negative) log-likelihood of in-dictionary n-grams that are present in a comment and (N) a smoothed n-gram  loss that measures the (negative) log-likelihood of all n- grams, even if these n-grams are not in the dictionary
This loss uses smoothing to assign non-zero probabilities to  out-of-dictionary n-grams; specifically, we experiment with Jelinek-Mercer smoothing [NN]
 Notation
We denote the input image by I and the image features extracted by the convolutional network with  parameters θ by φ(I; θ) ∈ RD
We denote the n-gram dic- tionary that our model uses by D and a comment containing K words by w ∈ [N, C]K , where C is the total number of words in the (English) language
We denote the n-gram that ends at the i-th word of comment w by wii−n+N and the i- th word in comment w by wii 
Our predictive distribution  is governed by a n-gram embedding matrix E ∈ RD×|D|
With a slight abuse of notation, we denote the embedding  corresponding to a particular n-gram w by ew
For brevity, we omit the sum over all image-comment pairs in the training / test data when writing loss functions
 Naive n-gram loss
The naive n-gram loss is a standard multi-class logistic loss over all n-grams in the dictionary D
The loss is summed over all n-grams that appear in the sentence w; that is, n-grams that do not appear in the dic- tionary are ignored:  ℓ(I, w; θ,E) = −  n ∑  m=N  K ∑  i=n  I [  wii−m+N ∈ D ]  log pobs (  wii−m+N|φ(I; θ);E )  ,  where the observational likelihood pobs(·) is given by a softmax distribution over all in-dictionary n-grams w that is governed by the inner product between the image features  φ(I; θ) and the n-gram embeddings:  pobs (w|φ(I; θ);E) = exp  (  −e⊤wφ(I; θ) )  ∑  w′∈D exp (  −e⊤w′φ(I; θ) ) 
 The image features φ(I; θ) are produced by a convolutional network φ(·), which we describe in more detail in N.N
 The naive n-gram loss cannot do language modeling be- cause it does not model a conditional probability
To circumvent this issue, we construct an ad-hoc conditional distribution based on the scores produced by our model at prediction time using a “stupid” back-off model [N]:  p (  wii|w i−N i−n+N  )  ∝  {  pobs (  wii|w i−N i−n+N  )  , if wii−n+N ∈ D  λp (  wii|w i−N i−n+N  )  , otherwise
 For brevity, we dropped the conditioning on φ(I; θ) and E
 Jelinek-Mercer (J-M) loss
The simple n-gram loss has two main disadvantages: (N) it ignores out-of-dictionary n- grams entirely during training and (N) the parameters E that  correspond to infrequent in-dictionary words are difficult to  pin down
Inspired by Jelinek-Mercer smoothing, we propose a loss function that aims to address both these issues:  ℓ(I, w; θ,E) = −  K ∑  i=N  log p (  wii|w i−N i−n+N, φ(I; θ);E  )  ,  where the likelihood of a word conditioned on the (n−N) words appearing before it is defined as:  p (  wii|w i−N i−n+N  )  = λpobs (  wii|w i−N i−n+N  )  +(N−λ)p (  wii|w i−N i−n+N  )  
 Herein, we removed the conditioning on φ(I; θ) and E for brevity
The parameter 0 ≤ λ ≤ N is a smoothing  NNNN    constant that governs how much of the probability mass  from (n−N)-grams is (recursively) transferred to both in- dictionary and out-of-dictionary n-grams
The probability mass transfer prevents the Jelinek-Mercer loss from assigning zero probability (which would lead to infinite loss) to  out-of-vocabulary n-grams, and it allows it to learn from low-frequency and out-of-vocabulary n-grams
 The Jelinek-Mercer loss proposed above is different  from traditional is Jelinek-Mercer smoothing: in particular,  it is differentiable with respect to both E and θ
As a result, the loss can be backpropagated through the convolutional  network
In particular, the loss gradient with respect to φ is given by:  ∂ℓ  ∂φ = −  K ∑  i=N  p (  wii|w i−N i−n+N, φ(I; θ);E  ) ∂p  ∂φ ,  where the partial derivatives are given by:  ∂p  ∂φ = λ  ∂pobs ∂φ  + (N− λ) ∂p  ∂φ  ∂pobs ∂φ  = pobs(w|φ(I; θ);E) (E[ew′ ]w′∼pobs − ew) 
 This error signal can be backpropagated directly through the  convolutional network φ(·)
 N.N
Training  The core of our visual recognition models is formed by  a convolutional network φ(I; θ)
For expediency, we opt for a residual network [NN] with NN layers
Our networks are initialized by an Imagenet-trained network, and trained to  minimize the loss functions described above using stochastic gradient descent using a batch size of NNN for N0 epochs
In all experiments, we employ Nesterov momentum of 0.N, a weight decay of 0.000N, and an initial learning rate of 0.N; the learning rate is divided by N0 whenever the training loss stabilizes (until a minimum learning rate of 0.00N)
 A major bottleneck in training is the large number of outputs of our observation model: doing a forward-backward  pass with NNN inputs (the image features) and NNN, N0N out- puts (the n-grams) is computationally intensive
To circum- vent this issue, we follow [NN] and perform stochastic gradient descent over outputs [N]: we only perform the forwardbackward pass for a random subset (formed by all positive  n-grams in the batch) of the columns of E
This simple ap- proximation works well in practice, and it can be shown to  be closely related to the exact loss [NN]
 N
Experiments  Below, we present the four sets of experiments we performed to assess the performance of our visual n-gram models in: (N) phrase-level image tagging, (N) phrase-based  image retrieval, (N) relating images and captions, and (N)  zero-shot transfer
 Loss / Smoothing “Stupid” back-off Jelinek-Mercer  Imagenet + linear NNN NNN  Naive n-gram NNN NNN Jelinek-Mercer NNN NNN  Table N
Perplexity of visual n-gram models averaged over  YFCCN00M test set of N0, 000 images (evaluated on in-dictionary  words only)
Results for two losses (rows) with and without  smoothing at test time (columns)
Lower is better
 N.N
Phrase-level image tagging  We first gauge whether relevant comments for images  have high likelihood under our visual n-gram models
Specifically, we measure the perplexity of predicting the  correct words in a comment on a held-out test set of N0, 000 images, and average this perplexity over all images in the  test set
The perplexity of a model is defined as NH(p), where H(p) is the cross-entropy:  H(p) = − N  K  K ∑  i=N  logN p (  wii|w i−N i−n+N, φ(I; θ);E  )  
 We only consider in-dictionary unigrams in our perplexity measurements
As is common in language modeling [NN], we assume a uniform conditional distribution  pobs (  wii|w i−N i−n+N  )  for n-grams whose prefix is not in the  dictionary (i.e., for n-grams for which wi−Ni−n+N /∈ D)
Based on the results of preliminary experiments on a held-out validation set, we set λ=0.N in the Jelinek-Mercer loss
We compare models that use either of the two loss  functions (the naive in-dictionary n-gram loss and Jelinek- Mercer loss) with a baseline trained with a linear layer on  top of Imagenet-trained visual features trained using naive  n-gram loss
We consider two settings of our models at prediction time: (N) a setting in which we use the “stupid”  back-off model with λ=0.N; and (N) a setting in which we smooth the p(·) predictions using Jelinek-Mercer smooth- ing (as described above) using λ=0.N
 The resulting perplexities for all experimental settings  are presented in Table N
From the results presented in the  table, we observe that: (N) the use of smoothing losses for  training image-based phrase prediction models leads to better models than the use of a naive n-gram loss; and (N) the use of additional smoothing at test time may further reduce  the perplexity of the n-gram model
The former effect is the result of the ability of smoothing losses to direct the learning signal to the most relevant n-grams instead of equally spreading it over all n-grams that are present in the target
The latter effect is the result of the ability of predictiontime smoothing to propagate the probability mass from indictionary n-grams to relevant out-of-dictionary n-grams
To obtain more insight into the phrase-prediction performance of our models, we also assess our model’s ability  NNNN    Model R@N R@N R@N0 Accuracy  Imagenet + linear N.0 N0.N NN.N NN.N  Naive n-gram N.N NN.N NN.N NN.N Jelinek-Mercer N.N NN.0 NN.N NN.0  Table N
Phrase-prediction performance on YFCCN00M test set  of N0, 000 images measured in terms of recall@k at three cut-off  levels k (lefthand-side; see text for details) and the percentage of  correctly predicted n-grams according to human raters (righthandside) for one baseline model and two of our phrase prediction models
Higher is better
 to predict relevant phrases (n-grams) for images
To cor- rect for variations in the marginal frequency of n-grams, we calibrate all log-likelihood scores by subtracting the average log-likelihood our model predicts on a large collection  of held-out validation images
We predict n-gram phrases for images by outputting the n-grams with the highest cal- ibrated log-likelihood score for an image
Examples of the  resulting n-gram predictions are shown in Figure N
 We quantify phrase-prediction performance in terms of  recall@k on a set of N0, 000 images from the YFCCN00M test set
We define recall@k as the average percentage of n- grams appearing in the comment that are among the k front- ranked n-grams when the n-grams are sorted according to their score under the model
In this experiment and all experiments hereafter, we only present results where the same  smoothing is used at training and at prediction time: that  is, we use the “stupid” back-off model on the predictions  of naive n-grams models and we smooth the predictions of Jelinek-Mercer models using Jelinek-Mercer smoothing
 As a baseline, we consider a linear multi-class classifier  over n-grams (i.e., using naive n-gram loss) trained on fea- tures produced by an Imagenet-trained convolutional network
The results are shown in the lefthand-side of Table N
 Because the n-grams in the YFCCN00M test set are noisy targets (many words that are relevant to the image  content are not present in the comments), we also performed  an experiment on Amazon Mechanical Turk in which we  asked two human raters whether or not the highest-scoring  n-gram was relevant to the content of the image
We filter out unreliable raters based on their response time, and for  each of our models, we measure the percentage of retrieved  n-grams that is considered relevant by the remaining raters
The resulting accuracies of the visual n-gram models are reported in the righthand-side of Table N
 The results presented in the table are in line with the  results presented in Table N: they show that the use of a  smoothing loss substantially improves the results compared  to baseline models based on the naive n-gram loss
In par- ticular, the relative performance in recall@k between our best model and the Imagenet-trained baseline model is approximately N0%
The merits of the Jelinek-Mercer loss are  Cut-off value k 0 N N0 NN N0 NN N0  R e c a ll@  k  0  N  N0  NN  N0  NN  N0 Unigram (NNK) Bigram (N0K) Trigram (NNNK) Four-gram (NN0K) Five-gram (NNNK)  Figure N
Recall@k on n-gram retrieval of five models with increasing maximum length of n-grams included in the dictionary  (n=N, 


, N), for varying cut-off values k
The dictionary size of  each of the models is shown between brackets
Higher is better
 confirmed by our experiment on Mechanical Turk: according to human annotators, NN.0% of the predicted phrases is relevant to the visual content of the image
 Next, we study the performance of our Jelinek-Mercer  model as a function of n; that is, we investigate the effect of including longer n-grams in our model on the model perfor- mance
As before, we measure recall@k of n-gram retrieval as a function of the cut-off level k, and consider models with unigrams to five-grams
Figure N presents the results of this  experiment, which shows that the performance of our models increases as we include longer n-grams in the dictionary
The figure also reveals diminishing returns: the improvements obtained from going beyond trigrams are limited
 N.N
Phrase-based image retrieval  In the second set of experiments, we measure the ability  of the system to retrieve relevant images for a given n-gram query
Specifically, we rank all images in the test set according to the calibrated log-likelihood our models predict  for the query-image pairs
 In Figure N, we show examples of twelve images that  are most relevant from a set of NNN, NNN YFCCN00M test images (according to our model) for four different n-gram queries; we manually picked these n-grams to demonstrate the merits of building phrase-level image recognition models
The figure shows that the model has learned accurate  visual representations for n-grams such as “Market Street” and “street market”, as well as for “city park” and “Park  City” (see the caption of Figure N for details on the queries)
 We show a second set of image retrieval examples in Figure N, which shows that our model is able to distinguish visual concepts related to Washington: namely, between the  state, the city, the baseball team, and the hockey team
 As in our earlier experiments, we quantify the imageretrieval quality of our model on a set of N0, 000 test images  NNNN    Market Street City park  Park CityStreet market  Figure N
Four highest-scoring images for n-gram queries “Market Street”, “street market”, “city park”, and “Park City” from  a collection of NNN, NNN YFCCN00M images
Market Street is a  common street name, for instance, it is one of the main thoroughfares in San Francisco
Park City (Utah) is a popular winter sport  destination
The figure only shows images from the YFCCN00M  dataset whose license allows reproduction
We refer to the supplementary material for detailed copyright information
 from the YFCCN00M dataset by measuring the precision  and recall of retrieving the correct image given a query n- grams
We compute a precision-recall curve by averaging  over the N0, 000 n-gram queries that have the highest tf- idf value in the YFCCN00M dataset: the resulting curve is  shown in Figure N
The results from this experiment are in  accordance with the previous results: the naive n-gram loss substantially outperforms our Imagenet baseline, which in  turn, is outperformed by the model trained using JelinekMercer loss
Admittedly, the precisions we obtain are fairly  low even in the low-recall regime
This low recall is the result of the false-negative noise in the “ground truth” we use  for evaluation: an image that is relevant to the n-gram query may not be associated with that n-gram in the YFCCN00M dataset, as a result of which we may consider it as “incorrect” even when it ought to be correct based on the visual  content of the image
 N.N
Relating Images and Captions  In the third set of experiments, we study to whether visual n-gram models can be used for relating images and captions
While many image-conditioned language models  have focused on caption generation, accurately measuring  the quality of a model is still an open problem: most current  metrics poor correlated with human judgement [N]
Therefore, we focus on caption-based retrieval tasks instead: in  particular, we evaluate the performance of our models in  caption-based image retrieval and image-based caption reModel R@N R@N R@N0 Accuracy  Imagenet + linear N.N N.N N.N NN.N  Naive n-gram N.N N.N N.N NN.0 Jelinek-Mercer N.N NN.N NN.N NN.N  Table N
Caption retrieval performance on YFCCN00M test set of  N0, 000 images measured in terms of recall@k at three cut-off  levels k (lefthand-side; see text for details) and the percentage of  correctly retrieved captions according to human raters (righthandside) one baseline model and two of our phrase prediction models
 Higher is better
 Image retrieval COCO-NK Flickr-N0K  R@N R@N R@N0 R@N R@N R@N0  Retrieval models  Karpathy et al
[N0] – – – N0.N N0.N NN.N  Klein et al
[NN] NN.N NN.N NN.0 NN.0 NN.N NN.0  Deep CCA [NN] – – – NN.N NN.N NN.N  Wang et al
[N0] – – – NN.N N0.N NN.N  Language models  STD-RNN [N0] – – – N.N NN.N NN.N  BRNN [NN] N0.N NN.N NN.N NN.N NN.N N0.N  Kiros et al
[NN] – – – NN.N NN.0 NN.N  NIC [NN] – – – NN.0 – NN.0  Ours  Naive n-gram 0.N N.N N.N N.0 N.N N.N Jelinek-Mercer N.0 NN.N NN.N N.N NN.N NN.N  J-M + finetuning NN.0 NN.0 N0.N NN.N NN.N N0.N  Table N
Recall@k (for three cut-off levels k) of caption-based image retrieval on the COCO-NK and Flickr-N0K datasets for eight  baseline models and our models (with and without finetuning)
 Baselines are separated in models dedicated to retrieval (top) and  image-conditioned language models (bottom)
Higher is better
 trieval
In caption-based image retrieval, we rank images  according to their log-likelihood for a particular caption and  measure recall@k: the percentage of queries for which the correct image is among the k first images
 We first perform an experiment on N0, 000 images and comments from the YFCCN00M test set
In addition to  recall@k, we also measure accuracy by asking two human raters to assess whether the retrieved caption is relevant to  the image content
The results of these experiments are presented in Table N: they show that the strong performance of  our visual n-gram models extends to caption retrievalN
Ac- cording to human raters, our best model retrieves a relevant  caption for NN.N% of the images in the test set
To assess if visual n-grams help, we also experiment with a unigram model [NN] with a dictionary size of NNN, N0N
We find that  NWe also performed experiments with a neural image captioning model  that was trained on COCO [NN], but this model performs poorly: it obtains  a recall@k of 0.N, N.0, and N.N for k=N, N, and N0, respectively
This is  because many of the words that appear in YFCCN00M are not in COCO
 NNNN    Washington State Washington DC Washington Nationals Washington Capitals  Figure N
Four highest-scoring images for n-gram queries “Washington State”, “Washington DC”, “Washington Nationals”, and “Washington Capitals” from a collection of NNN, NNN YFCCN00M test images
Washington Nationals is a Major League Baseball team; Washington  Capitals is a National Hockey League hockey team
The figure only shows images from the YFCCN00M dataset whose license allows  reproduction
We refer to the supplementary material for detailed copyright information
 Recall 0 N0 N0 N0 N0 N00  P re  c is  io n  0  N  N  N  N  N  N Imagenet + linear Naive n-gram Jelinek-Mercer  Figure N
Precision-recall curve for phrase-based image retrieval of  our models on YFCCN00M test set of N0, 000 images one baseline  model and two of our phrase-prediction models
The curves were  obtained by averaging over the N0, 000 n-gram queries with the  highest tf-idf value
 this model performs worse than visual n-gram models: its recall@k scores of are N.N, N.N, and N.N, respectively
 To facilitate comparison with existing methods, we also  perform experiments on the COCO-NK and Flickr-N0K  datasets [NN, NN] using visual n-gram models trained on YFCCN00MN
The results of these experiments are presented in Table N; they show that our model performs  roughly on par with the state-of-the-art based on language  models on both datasets
We emphasize that our models have much larger vocabularies than the baseline models, which implies the strong performance of our models  likely generalizes to a much larger visual vocabulary than  the vocabulary required to perform well on COCO-NK and  Flickr-N0K
Like other language models, our models perform worse on the Flickr-N0K dataset than dedicated retrieval models [N0, NN, N0, NN]
Interestingly, our model  does perform on par with a state-of-the-art retrieval model  [NN] on COCO-NK
 NPlease see supplementary materials for additional results in COCO-NK  and additional baseline models for relating images and captions
 Caption retrieval COCO-NK Flickr-N0K  R@N R@N R@N0 R@N R@N R@N0  Retrieval models  Karpathy et al
[N0] – – – NN.N N0.N NN.N  Klein et al
[NN] NN.N N0.N NN.N NN.0 NN.0 NN.N  Deep CCA [NN] – – – NN.N NN.N NN.N  Wang et al
[N0] – – – N0.N NN.N NN.N  Language models  STD-RNN [N0] – – – N.N NN.N NN.N  BRNN [NN] NN.N NN.N NN.0 NN.N NN.N NN.N  Kiros et al
[NN] – – – NN.0 N0.N NN.N  NIC [NN] – – – NN.0 – NN.0  Ours  Naive n-gram 0.N N.N N.N N.N N.N N.N Jelinek-Mercer N.N NN.N NN.N NN.N NN.N NN.N  J-M + finetuning NN.N NN.N NN.N NN.N NN.N NN.0  Table N
Recall@k (for three cut-off levels k) of caption retrieval  on the COCO-NK and Flickr-N0K datasets for eight baseline systems and our visual n-gram models (with and without finetuning)
 Baselines are separated in models dedicated to retrieval (top) and  image-conditioned language models (bottom)
Higher is better
 We also perform image-based caption retrieval experiments: we retrieve captions by ranking all captions in the  COCO-NK and Flick-N0K test set according to their loglikelihood under our model
The results of this experiment  are presented in Table N, which shows that our model performs on par with state-of-the-art image-conditioned language models on caption retrieval
Like all other language  models, our model performs worse than approaches tailored  towards retrieval on the Flickr-N0K dataset
On COCO-NK,  visual n-grams perform on par with the state-of-the-art
 N.N
Zero-Shot Transfer  Because our models are trained on approximately N0  million photos and comments, they have learned to recognize a wide variety of visual concepts
To assess the ability of our models to recognize visual concepts out-of-theNNNN    aYahoo Imagenet SUN  Class mode (in dictionary) NN.N 0.N NN.0  Class mode (all classes) NN.N 0.N N.N  Jelinek-Mercer (in dictionary) NN.N NN.N NN.N  Jelinek-Mercer (all classes) NN.N NN.N NN.0  Table N
Classification accuracies on three zero-shot transfer learning datasets on in-dictionary and on all classes
The number of  in-dictionary classes is N0 out of NN for aYahoo, NNN out of N, 000  for Imagenet, and NN0 out of NN0 for SUN
Higher is better
 box, we perform a series of zero-shot transfer experiments
 Unlike traditional zero-shot learners (e.g., [N, NN, NN]), we  simply apply the Flickr-trained models on a test set from  a different dataset
We automatically match the classes in  the target dataset with the n-grams in our dictionary
We perform experiments on the aYahoo dataset [NN], the SUN  dataset [NN], and the Imagenet dataset [N0]
For a test image,  we rank the classes that appear in each dataset according to  the score our model assigns to the corresponding n-grams, and predict the highest-scoring class for that image
We report the accuracy of the resulting classifier in Table N in two  settings: (N) a setting in which performance is measured  only on in-dictionary classes and (N) a setting in which performance is measured on all classes
 The results of these experiments are shown in Table N
 For reference, we also present the performance of a model  that always predicts the a-priori most likely class
The  results reveal that, even without any finetuning or recalibration, non-trivial performances can be obtained on  generic vision tasks
The performance of our models is  particularly good on common classes such as those in the  aYahoo dataset for which many examples are available in  the YFCCN00M dataset
The performance of our models  is worse on datasets that involve fine-grained classification  such as Imagenet, for instance, because YFCCN00M contains few examples of specific, uncommon dog breeds
 N
Discussion and Future Work  Visual n-grams and recurrent models
This study has presented a simple yet viable alternative to the common  practice of training a combination of convolutional and recurrent networks to relate images and language
Our visual  n-gram models differ in several key aspects from models based on recurrent networks
Visual n-gram models are less suitable for caption generationN [NN] but they are much more  efficient to evaluate at inference time, which is very important in real-world applications of these models
Moreover,  visual n-gram models can be combined with class activation  NOur model achieves a METEOR score [NN] of NN.N on COCO captioning with a test set of N, 000 images, versus NN.N for a nearest neighbor  baseline method and NN.N for a recurrent network [NN]
 on the grass the grass red brick the football being pushed  open mike a string her hair the equipment performing at the  the table sitting around a plate friends at the the foliage  Figure N
Discriminative regions of five n-grams for three images,  computed using class activation mapping [NN, N0]
 mapping [NN, N0] to perform visual grounding of n-grams, as shown in Figure N
Such grounding is facilitated by the  close relation between predicting visual n-grams and stan- dard image classification
This makes visual n-gram mod- els more amenable to transfer to new tasks than approaches  based on recurrent models, as demonstrated by our zeroshot transfer experiments
 Learning from web data
Another important aspect that  discerns our work from most approaches in vision is that our  models are capable of being learned purely from web data,  without any manual data annotation
We believe that this  type of training is essential if we want to construct models  that are not limited to a small visual vocabulary and that are  readily applicable to real-world computer-vision tasks
Indeed, this paper fits in a recent line of work [N, NN] that abandons the traditional approach of gathering images, manually  annotating them for a small visual vocabulary, and training  and testing on the resulting image-target distribution
As  a result, models such as ours may not necessarily achieve  state-of-the-art results on established benchmarks, because  they did not learn to exploit the biases of those benchmarks  as well [NN, NN, NN]
Such “negative” results highlight the  necessity of developing less biased benchmarks that provide  more signal on progress towards visual understanding
 Future work
The Jelinek-Mercer loss we studied in this  paper is based on just one of many n-gram smoothers [NN]
In future work, we plan to perform an in-depth comparison of different smoothers for the training of convolutional  networks
In particular, we will consider loss functions  based as absolute-discounting smoothing such as KneserNey smoothing [NN], as well as back-off models [NN]
We  also plan to explore the use of visual n-gram models in sys- tems that operate in open-world settings, combining them  with techniques for zero-shot and few-shot learning
Finally, we aim to use our models in tasks that require recognition of a large variety of visual concepts and relations between them, such as visual question answering [N, NN], visual Turing tests [NN], and scene graph prediction [NN]
 NNN0    References  [N] P
Anderson, B
Fernando, M
Johnson, and S
Gould
 SPICE: Semantic propositional image caption evaluation
In  ECCV, N0NN
 [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra, C
Zitnick, and D
Parikh
VQA: Visual question answering
In  ICCV, N0NN
 [N] A
Bendale and T
Boult
Towards open world recognition
 In CVPR, N0NN
 [N] Y
Bengio and J.-S
Senecal
Quick training of probabilistic  neural nets by importance sampling
In AISTATS, N00N
 [N] T
Berg and D
Forsyth
Animals on the web
In CVPR, N00N
 [N] T
Brants, A
Popat, P
Xu, F
Och, and J
Dean
Large language models in machine translation
In EMNLP, pages NNN–  NNN, N00N
 [N] S
Changpinyo, W.-L
Chao, B
Gong, and F
Sha
Synthesized classifiers for zero-shot learning
In CVPR, N0NN
 [N] S
Chen and J
Goodman
An empirical study of smoothing  techniques for language modeling
In ACL, NNNN
 [N] X
Chen and A
Gupta
Webly supervised learning of convolutional networks
In ICCV, N0NN
 [N0] J
Deng, W
Dong, R
Socher, L
J
Li, K
Li, and L
FeiFei
Imagenet: A large-scale hierarchical image database
In  CVPR, N00N
 [NN] M
Denkowski and A
Lavie
Meteor universal: Language  specific translation evaluation for any target language
In  EACL N0NN Workshop on Statistical Machine Translation,  N0NN
 [NN] E
Denton, J
Weston, M
Paluri, L
Bourdev, and R
Fergus
 User conditional hashtag prediction for images
In KDD,  N0NN
 [NN] S
Divvala, A
Farhadi, and C
Guestrin
Learning everything  about anything: Webly-supervised visual concept learning
 In CVPR, N0NN
 [NN] J
Donahue, L
Hendricks, S
Guadarrama, M
Rohrbach,  S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual recognition and description
In CVPR, N0NN
 [NN] H
Fang, S
Gupta, F
Iandola, R
Srivastava, L
Deng, P
Dollar, J
Gao, X
He, M
Mitchell, J
Platt, C
Zitnick, and  G
Zweig
From captions to visual concepts and back
In  CVPR, N0NN
 [NN] A
Farhadi, I
Endres, D
Hoiem, and D
Forsyth
Describing  objects by their attributes
In CVPR, N00N
 [NN] A
Farhadi, M
Hejrati, M
Sadeghi, P
Young, C
Rashtchian,  J
Hockenmaier, and D
Forsyth
Every picture tells a story:  Generating sentences from images
In ECCV, N0N0
 [NN] L
Fei-Fei, R
Fergus, and P
Perona
One-shot learning of  object categories
TPAMI, N00N
 [NN] R
Fergus, L
Fei-Fei, P
Perona, and A
Zisserman
Learning  object categories from internet image searches
Proceedings  of the IEEE, N0N0
 [N0] A
Frome, G
Corrado, J
Shlens, S
Bengio, J
Dean, and  T
Mikolov
Devise: A deep visual-semantic embedding  model
In NIPS, N0NN
 [NN] D
Geman, S
Geman, N
Hallonquist, and L
Younes
Visual  Turing test for computer vision systems
Proceedings of the  National Academy of Sciences, NNN(NN):NNNN–NNNN, N0NN
 [NN] J
Goodman
A bit of progress in language modeling
Computer Speech & Language, NN(N):N0N–NNN, N00N
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
 [NN] H
Izadinia, B
Russell, A
Farhadi, M
Hoffman, and  A
Hertzmann
Deep classifiers from image tags in the  wild
In Proceedings of the N0NN Workshop on CommunityOrganized Multimodal Mining: Opportunities for Novel Solutions, pages NN–NN
ACM, N0NN
 [NN] A
Jabri, A
Joulin, and L
van der Maaten
Revisiting visual  question answering baselines
In ECCV, N0NN
 [NN] F
Jelinek and R
Mercer
Interpolated estimation of markov  source parameters from sparse data
In Workshop on Pattern  Recognition in Practice, NNN0
 [NN] A
Joulin, E
Grave, P
Bojanowski, and T
Mikolov
Bag of  tricks for efficient text classification
In arXiv:NN0N.0NNNN,  N0NN
 [NN] A
Joulin, L
van der Maaten, A
Jabri, and N
Vasilache
 Learning visual features from large weakly supervised data
 In ECCV, N0NN
 [NN] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, N0NN
 [N0] A
Karpathy, A
Joulin, and L
Fei-Fei
Deep fragment embeddings for bidirectional image sentence mapping
In NIPS,  N0NN
 [NN] S
M
Katz
Estimation of probabilities from sparse data  for the language model component of a speech recognizer
 ICASSP, NNNN
 [NN] J
R
Kiros, R
Salakhutdinov, and R
S
Zemel
Unifying visual-semantic embeddings with multimodal neural language models
CoRR, abs/NNNN.NNNN, N0NN
 [NN] B
Klein, G
Lev, G
Sadeh, and L
Wolf
Fisher vectors  derived from hybrid gaussian-laplacian mixture models for  image annotation
In CVPR, N0NN
 [NN] R
Kneser and H
Ney
Improved backing-off for m-gram  language modeling
In ICASSP, NNNN
 [NN] E
Kodirov, T
Xiang, Z
Fu, and S
Gong
Unsupervised  domain adaptation for zero-shot learning
In ICCV, N0NN
 [NN] R
Krishna, Y
Zhu, O
Groth, J
Johnson, K
Hata, J
Kravitz,  S
Chen, Y
Kalantidis, L
Jia-Li, D
A
Shamma, M
Bernstein, and L
Fei-Fei
Visual genome: Connecting language  and vision using crowdsourced dense image annotations
 IJCV, N0NN
 [NN] R
Lebret, P
Pinheiro, and R
Collobert
Phrase-based image  captioning
In ICML, N0NN
 [NN] L.-J
Li and L
Fei-Fei
Optimol: Automatic online picture  collection via incremental model learning
IJCV, N0N0
 [NN] Q
Li, J
Wu, and Z
Tu
Harvesting mid-level visual concepts  from large-scale internet images
In CVPR, N0NN
 [N0] S
Li, G
Kulkarni, T
Berg, A
Berg, and Y
Choi
Composing simple image descriptions using web-scale n-grams
In  CoNLL, N0NN
 [NN] Y
Li, J
Yang, Y
Song, L
Cao, J
Luo, and J
Li
Learning from noisy labels with distillation
In arXiv NN0N.0NNNN,  N0NN
 NNNN    [NN] T
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and L
Zitnick
Microsoft COCO: Common objects in context
In ECCV, N0NN
 [NN] J
Mao, W
Xu, Y
Yang, J
Wang, and A
Yuille
Deep captioning with multimodal recurrent neural networks
In ICLR,  N0NN
 [NN] T
Mikolov
Statistical Language Models based on Neural  Networks
PhD thesis, Brno University of Technology, N0NN
 [NN] A
Passos, V
Kumar, and A
McCallum
Lexicon infused  phrase embeddings for named entity resolution
In CoNLL,  N0NN
 [NN] M
Rubinstein, A
Joulin, J
Kopf, and C
Liu
Unsupervised  joint object discovery and segmentation in internet images
 In CVPR, N0NN
 [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
Berg, and L
Fei-Fei
Imagenet large scale visual recognition challenge
IJCV, N0NN
 [NN] R
Selvaraju, A
Das, R
Vedantam, and M
Cogswell
Gradcam: Why did you say that? visual explanations from deep  networks via gradient-based localization
In arXiv Preprint  NNN0.0NNNN, N0NN
 [NN] A
Smeulders, M
Worring, S
Santini, A
Gupta, and R
Jain
 Content-based image retrieval at the end of the early years
 TPAMI, NN(NN):NNNN–NNN0, N000
 [N0] R
Socher, A
Karpathy, Q
Le, C
D
Manning, and A
Ng
 Grounded compositional semantics for finding and describing images with sentences
TACL, N0NN
 [NN] B
Sturm
A simple method to determine if a music information retrieval system is a horse
IEEE Transactions on  Multimedia, NN(N):NNNN–NNNN, N0NN
 [NN] B
Sturm
Horse taxonomy and taxidermy
HORSEN0NN,  N0NN
 [NN] D
Tang, F
Wei, B
Qin, M
Zhou, and T
Liu
Building  large-scale twitter-specific sentiment lexicon: A representation learning approach
In COLING, N0NN
 [NN] K
Tang, A
Joulin, L.-J
Li, and L
Fei-Fei
Co-localization  in real-world images
In CVPR, N0NN
 [NN] B
Thomee, D
Shamma, G
Friedland, B
Elizalde, K
Ni,  D
Poland, D
Borth, and L.-J
Li
YfccN00m: The new  data in multimedia research
Communications of the ACM,  NN(N):NN–NN, N0NN
 [NN] L
Torresani, M
Szummer, and A
Fitzgibbon
Efficient object category recognition using classemes
In ECCV, N0N0
 [NN] A
Veit, N
Alldrin, G
Chechik, I
Krasin, A
Gupta, and  S
Belongie
Learning from noisy large-scale datasets with  minimal supervision
In arXiv NN0N.0NNNN, N0NN
 [NN] S
Vijayanarasimhan and K
Grauman
Keywords to visual  categories: Multiple-instance learning for weakly supervised  object categorization
In CVPR, N00N
 [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In CVPR, N0NN
 [N0] L
Wang, Y
Li, and S
Lazebnik
Learning deep structurepreserving image-text embeddings
In CVPR, N0NN
 [NN] S
Wang and C
D
Manning
Baselines and bigrams: Simple,  good sentiment and topic classification
In ACL, N0NN
 [NN] X.-J
Wang, L
Zhang, X
Li, and W.-Y
Ma
Annotating  images by mining image search results
TPAMI, N00N
 [NN] J
Xiao, J
Hays, K
Ehinger, A
Oliva, and A
Torralba
Sun  database: Large-scale scene recognition from abbey to zoo
 In CVPR, N0N0
 [NN] T
Xiao, T
Xia, Y
Yang, C
Huang, and X
Wang
Learning  from massive noisy labeled data for image classification
In  CVPR, N0NN
 [NN] F
Yan and K
Mikolajczyk
Deep correlation for matching  images and text
In CVPR, N0NN
 [NN] P
Young, A
Lai, M
Hodosh, and J
Hockenmaier
From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions
In ACL,  N0NN
 [NN] L
Yu, E
Park, A
Berg, and T
Berg
Visual madlibs: Fill in  the blank description generation and question answering
In  ICCV, N0NN
 [NN] J
Zhang, S
Liu, M
Li, M
Zhou, and C
Zong
Bilinguallyconstrained phrase embeddings for machine translation
In  ACL, N0NN
 [NN] Z
Zhang and V
Saligrama
Zero-shot recognition via structured prediction
In ECCV, N0NN
 [N0] B
Zhou, A
Khosla, A
Lapedriza, A
Oliva, and A
Torralba
Learning deep features for discriminative localization
 In CVPR, N0NN
 [NN] W
Zou, R
Socher, D
Cer, and C
Manning
Bilingual  word embeddings for phrase-based machine translation
In  EMNLP, N0NN
 NNNNDeNet: Scalable Real-Time Object Detection With Directed Sparse Sampling   DeNet: Scalable Real-time Object Detection with Directed Sparse Sampling  Lachlan Tychsen-Smith, Lars Petersson  CSIRO (DataNN)  N London Circuit, Canberra, ACT, NN0N  Lachlan.Tychsen-Smith@dataNN.csiro.au, Lars.Petersson@dataNN.csiro.au  Abstract  We define the object detection from imagery problem  as estimating a very large but extremely sparse bounding  box dependent probability distribution
Subsequently we  identify a sparse distribution estimation scheme, Directed  Sparse Sampling, and employ it in a single end-to-end CNN  based detection model
This methodology extends and formalizes previous state-of-the-art detection models with an  additional emphasis on high evaluation rates and reduced  manual engineering
We introduce two novelties, a corner based region-of-interest estimator and a deconvolution  based CNN model
The resulting model is scene adaptive,  does not require manually defined reference bounding boxes  and produces highly competitive results on MSCOCO, Pascal VOC N00N and Pascal VOC N0NN with real-time evaluation rates
Further analysis suggests our model performs particularly well when finegrained object localization is desirable
We argue that this advantage stems from  the significantly larger set of available regions-of-interest  relative to other methods
Source-code is available from:  https://github.com/lachlants/denet  N
Introduction  Feed-forward neural networks exhibit good convergence  properties given a random initialization under stochastic  gradient descent (SGD) and, given an appropriate network  design and training regime, can generalize well to previously unseen data [N]
In particular, convolutional neural  networks (CNNs) built from interleaved convolution and  pooling layers with ReLU activation functions have set numerous benchmarks in computer vision tasks [N] [N] [N0]
 A number of methodologies have been developed to map  their state-of-the-art dense regression and classification capabilities to the problem of identifying axis aligned bounding boxes of object instances in images
In particular we  highlight the relatively slow region based CNN approaches  (R-CNN [N], Faster R-CNN [NN]) and the more recent work  on real-time detection (YOLO [NN], SSD [NN])
 Input  Image Base  CNN  Sparse  Sample Classify  CNN BBox  Update  Pr(s|BS)  Corner  Detect Pr(t|k, y, x)  Sampling  Bounding  Boxes  Figure N
A high level flow diagram depicting the DeNet methodology
The CNN’s are highlighted in blue, the novel components  in purple and the outputs in yellow
The sampling bounding box  dependency BS (highlighted in red) is held constant during back  propagation to produce an end-to-end trained model
The corner distribution and final classification distribution are jointly optimized using cross entropy loss
 Rather than focusing on obtaining state-of-the-art accuracy in a competition environment (i.e
computationally unconstrained) in this paper we emphasis the dual task of obtaining the best detection performance at a predefined evaluation rate i.e
N0 Hz and N0 Hz
The primary contributions  made within this paper include:  • An improved theoretical understanding of modern detection methods and a generic framework in which to  describe them i.e
Directed Sparse Sampling
 • A novel, fast, region-of-interest estimator which  doesn’t require manually defined reference bounding  boxes
 • A novel application of deconvolution layers which  greatly improves evaluation rates
 • Six implementations of our method demonstrating  competitive detection performance on a range of  benchmarks
 • An easily extended Theano based code release to facilitate the research community
 NNNN    N.N
Related work  In region based CNN detection (R-CNN) [N] the image  is first preprocessed with a region proposal algorithm e.g
 selective search [NN], region proposal network (RPN) [NN],  etc
This algorithm identifies image regions (i.e
bounding  boxes) of interest (RoIs) which are then rescaled to fixed dimensions (normalizing scale and aspect ratio) and fed into  a CNN based classifier
The CNN assigns a probability  that the region bounds an object of interest or the null class  and, via linear regression, identifies an improved bounding  box
This approach has demonstrated state-of-the-art results, however, it is very expensive to train and evaluate, requiring multiple full CNN evaluations (one per region proposal) and an often expensive pre-processing step
Since the  majority of CNN computation occurs in the first few layers,  Fast R-CNN [N] addressed these issues by applying a shallow CNN to the image and then, for each region, extracting  fixed sized features from the generated feature map for the  final classification
In Faster R-CNN [NN] the region proposal algorithm was integrated into the CNN providing an  end-to-end solution, improved timings and demonstrating  that both tasks (region proposal and classification) shared  similar underlying features
Despite these improvements, to  our knowledge, region based CNN’s have not been demonstrated operating near real-time frequencies
 In You Only Look Once (YOLO) [NN] they depart from  the algorithmically defined region based approaches described above, opting instead for a predefined, regular grid  of detectors
In effect they merged the region classification  problem into the region proposal network (RPN) first proposed in Faster R-CNN
With this approach the CNN is only  evaluated once to produce the outcomes for all detectors  resulting in significantly reduced training and evaluation  times
In Single Shot Detector (SSD) [NN] this approach  was further refined with an improved network design and  training methodology to demonstrate comparable results to  the region based methods
We note that the considerable  improvements achieved with SSD required scene dependent  engineering to manually predefine the most likely set of regions within the image to contain an object, a flaw shared  with the Faster R-CNN region proposal network
In particular, SSD demonstrated an improvement of N.N% MAP [NN] by the addition of four aspect ratios to the predefined regions on the Pascal VOCN00N [N] dataset, highlighting the  importance of manual engineering in modern state-of-theart detector designs
Without going into too much detail,  we note that in practice manually engineered solutions typically limit scalability and adaptiveness to different problem  sets (without an expensive re-engineering process)
 The primary differentiator between these methods lies  in how each method identifies and treats the regions to be  classified
R-CNN based methods sample regions sparsely  based on an algorithmic preprocessing step and normalize  the region of interest while YOLO based approaches perform dense sampling with a manually defined grid of detectors without image normalization
Often dense methods  are well suited to current implementations and, therefore  offer a significant timing advantage over sparse methods
 However, in this work, we demonstrate a novel model design which combines the ease of training, scene adaptability  and classification accuracy of the sparse region-based approaches with the fast training and evaluation of the dense  non-region based methods
 N.N
Probabilistic Object Detection  We formulate the probabilistic multiclass detection problem as first estimating the distribution Pr(s|B, I) where s ∈ C ∩ {null} is a random variable indicating the pres- ence of an instance of class c ∈ C or the null class (in- dicating no instances) which is sufficiently bounded by the  box B = {x, y, w, h} and I is the input image (omitted in subsequent derivations)
This formulation incorporates the  assumption that only a single instance of a class can occupy  each bounding box
We note that this definition does not  seek to perform instance assignment, but can be used as an  input to an algorithm that does e.g
Non-Max Suppression
 Given a suitable neural network design we assert that  Pr(s|B) can be estimated from training data with class bounding box annotations
However, since the number of  unique bounding boxes is given by |B| ∝ XYWH where (X,Y ) are the number of image positions and (W,H) the range of bounding box dimensions the naive solution quickly becomes intractable
For instance, assuming  the most common settings for the ImageNet dataset, N000 classes and NNN × NNN images, and considering all valid bounding boxes within the image, expressing this distribution requires approximately NNN × N0N values or N.NTB in NNbit float format
Clearly this is an intractable problem  with current hardware
 At the cost of localization accuracy, subsampling the  output bounding boxes is a valid approach
For instance,  by careful dataset dependent manual engineering, Faster RCNN and YOLO based approaches subsample the distribution to the order of N0N to N0N bounding boxs [NN] [NN]
These boxes are then refined by estimating only the most  likely bounding box in a local region via linear regression
 As an alternative to large scale subsampling, we sought to  exploit the fact that, due to occlusion and other factors, we  expect a very small subset of bounding boxes to contain  class instances other than the null class
Subsequently, we  have developed a solution based on the state-of-the-art regression capabilities of a single end-to-end CNN which estimates the highly sparse distribution Pr(s|B) in a real-time (or computationally constrained) operational environment
 NNN    N
Directed Sparse Sampling (DSS)  We use the term Directed Sparse Sampling to refer to the  method of a applying a jointly optimized two stage CNN  where one stage estimates the likely locations where userdefined interesting values occur and the other sparsely classifies the identified values e.g
in R-CNN based models  (including R-FCN and DeNet) we estimate the bounding  boxes which are most likely to include a non-null class assignment, then run a classifer over these bounding boxs
 N.N
Corner-based RoI Detector  Here we introduce the concept of bounding box corner  estimation for efficient region-of-interest (RoI) estimation
 In our methodology, this task is performed by estimating  the likelihood that each position in the image contains an  instance of one of N corner types i.e
Pr(t|k, y, x) where t is a binary variable indicating the presence of a corner of  type k ∈ {top left, top right, bottom left, bottom right} at position (x, y) in the input image
We assert that due to the natural translation invariance of the problem, estimating  the corner distribution can be efficiently performed with a  standard CNN design trained on bounding box annotated  image data (e.g
MSCOCO [NN], Pascal VOC [N], etc)
 With the corner distribution defined we estimate the likelihood that a bounding box B contains an instance by apply- ing a Naive Bayesian Classifier to each corner of the bounding box:  Pr(s N= null|B) ∝ ∏  k  Pr(t|k, yk, xk) (N)  where (xk, yk) = fk(B) indicates the bounding box po- sition associated with each corner type k
For ease of im- plementation we define the N×N bounding boxes with the largest non-null probability Pr(s N= null|B) as the sampling bounding boxes BS 
The user defined variable N balances the maximum number of detections the model can handle  with the computational and memory requirements
 With the potentially non-null bounding boxes estimated,  we pass a feature vector of predefined length from the corner detector model to the final classification stages
Therefore, the final classification stage is a function of the form  f : ᾱB → Pr(s|B) where ᾱB is a feature vector uniquely identified by the sampling bounding box B ∈ BS 
It is important that the feature is uniquely associated with each  bounding box, otherwise the classifier will have no information to distinguish between bounding boxes with the same  ᾱB 
Exactly how to construct the feature vector is still a matter of debate [N, NN] however we construct ᾱB by con- catenating together the nearest neighbour sampling features  at predefined locations relative to each sampling bounding  box (e.g
bounding box corners, center, etc) in addition to  the bounding box width and height
The bounding box center position was omitted from the feature vector such that  the classifier would be agnostic to image offsets
 N.N
Training  During training, the model is initially forward propagated to generate the sampling bounding boxes BS as de- scribed in the previous subsection
In addition, we augment  the sampling bounding boxes with the ground truth bounding boxes and randomly generated samples
We then propagate the activations ᾱB associated with the augmented set of sampling bounding boxes through the rest of the model  to produce the final classification distribution Pr(s|BS) and updated bounding box parameters
The set of sampling  bounding boxes BS is held constant during gradient estima- tion to enable end-to-end training, therefore the corner detector network is optimized in conjunction with the bounding box classification and estimation task
Since forward  propagation is a necessary preprocessing step in the back  propagation based SGD policy typically used to optimize  neural networks, the DeNet method introduces no penalty  to training time over a standard dense network
 The DeNet model jointly optimizes over the corner  probability distribution, final classification distribution and  bounding box regression cost, i.e
 Cost = λt Λt  ∑  k,y,x  φ(t|k, y, x) ln(Pr(t|k, y, x))+  λs Λs  ∑  B∈BS  φ(s|B) ln(Pr(s|B))+  λb Λb  ∑  i  SoftLN(φB,i − βi)  (N)  where φ(...) are the ground truth corner and classifi- cation distributions, φB,i = {xi, yi, wi, hi} the ground truth bounding boxes, (λs, λt, λb) are user defined con- stants indicating the relative strength of each component,  (Λs,Λt,Λb) are constants normalizing each component to N given the model initialization and SoftLN(x) is defined in [N]
The corner distribution φ(t|k, y, x) is identified by mapping each groundtruth instance’s corners to a single position in the corner map, corners out of bounds are simply discarded
The detection distribution φ(s|B) is identi- fied by calculating the intersection over union (IoU) overlap  between the groundtruth bounding boxes and the sampling  bounding boxes BS 
Following standard practice, the re- gression target bounding box φB is identified by selecting the ground truth bounding box with the largest IoU overlap
 NN0    N.N
Detection Model  Residual neural networks [N] have demonstrated impressive regression capabilities on a number of large scale  datasets
In particular the N0N layer Residual Network  model (ResNet-N0N) achieved state of the art performance  on the ILSVRCN0NN [NN] and MSCOCO [NN] datasets when  combined with Faster R-CNN
As the base model to our  networks we selected the NN layer, NNM parameter ResNetNN model (DeNet-NN) and the N0N layer, NNM parameter  ResNet-N0N model (DeNet-N0N)
 To each base model we modified the input size to NNN× NNN pixels, removed the final mean pooling and fully con- nected layers and appended two deconvolution [NN] layers followed by a corner detector
The corner detector is  responsible for generating the corner distribution and produces a feature sampling map via a learnt linear projection  with Fs features at each spatial position
The deconvolu- tion [NN] layers efficiently reintroduce spatial information  that was lost in the base model such that the feature map and  corner probability distribution can be defined at a greater  spatial resolution i.e
NN × NN compared to NN × NN with- out
This results in a NN × NN pixel minimum size for each sampling bounding box
 Following the corner detector is the sparse layer which  observes the corners identified by the corner detector and  generates a set of sampling bounding boxes (RoIs)
The  RoIs are used to extract a set of N×N feature vectors from the feature sampling maps
In this case, we are sparsely  sampling NN bounding boxes from a set of N.NM valid bounding boxes
A feature vector is constructed by extracting the nearest neighbour sampling features associated with  a N × N grid plus the bounding box width and height
This produces a feature with N × N × Fs + N values
We found that nearest neighbour sampling was sufficient because the  feature sampling maps have the same, relatively high, spatial resolution as the bounding box corners
Finally, the  feature vectors are propagated through a relatively shallow  fully connected network to generate the final classification  and fine tuned bounding box for each sampling RoI
 In Table N and N we describe the additional layers appended to the base models with the following definitions:  • Conv: Convolves a series of ND filters over the input activations
Filter weights were initialized via the  normal distribution N (0, σ) with σN = N/(nfnxny) where nf is the number of filters and (nx, ny) their spatial shape [N]
Following each convolution is batch  normalization [N] then the ReLU activation function
 • Deconv: Applies a learnt deconvolution [NN] (upsampling) operation followed by ReLU activation
In this  case it is equivalent to upscaling both spatial dimensions then applying a Conv layer
 Model F0 FN FN FN FN FN FN FN DeNet-NN NNN NNN NNN NN0N NNNN N0NN NNN NNN  DeNet-N0N N0NN NNN NNN NNNN N0NN NNNN N0NN NNN  Table N
Filter parameters used for DeNet models
See Table N  Layer Input Shape Filters Shape Stride  ResNet-NN or ResNet-N0N [N] base model
 Deconv NN× NN× F0 FN N× N N× N Deconv NN× NN× FN FN N× N N× N Corner NN× NN× FN - - - Sparse - - - Conv N ×N × FN FN N× N N× N Conv N ×N × FN FN N× N N× N Conv N ×N × FN FN N× N N× N Conv N ×N × FN FN N× N N× N Classifier N ×N × FN - - Table N
DeNet: A ResNet derived model for DSS Object Detection with a NNN×NNN input image
Layers in the base models above the line are initialized with a pretrained ResNet-NN or ResNet-N0N  ImageNet N0NN classification model
 • Corner: Estimates a corner distribution via the softmax function and produces a sampling feature map
 See Section N
 • Sparse: Identifies sampling bounding boxes from corner distribution and produces a fixed size sampling feature from the sampling feature maps
 • Classifier: Maps activations to the desired probability distribution via the softmax function and generates  bounding box targets
 For DeNet-NN we use a ResNet-NN base model and Fs = NN to produce a feature vector of NN0N values and a total of NNM parameters
The DeNet-N0N model uses a ResNet-N0N  base model and increased the number of filters by approximately N.N× for the appended layers (See Table N)
These changes produce a sparse feature vector of NNNN values and a total of NNM parameters
 N.N.N Skip Layer Variant  As an extension we considered augmenting the DeNet models with skip layers
In recent work, skip layers have demonstrated consistent improvements in classification [N], detection [N0] and semantic segmentation [N] and, more generally, are an integral component to highway [NN] and residual  networks [N]
In this case, these layers connect the Deconv  layer with the final layer in the base model which has the  same spatial dimensions
Our implementation follows [N0],  each skip layer performs a linear projection of the source  features to the destination feature dimensions and simply  adds the resulting feature maps (before activation)
 NNN    N.N.N Wide Variant  In this model we modified the skip model variants to use  a NNN × NNN spatial resolution for the corner and feature sampling maps by the addition of another Deconv and skip  layer
We also increased N to NN to produce NN0N RoIs
In the current implementation, this approach comes with a  considerable timing cost due to the increased classification  burden and the CPU bound algorithm for identifying RoIs
 With further engineering (e.g
deduplication) we believe  these costs could be reduced
 N
Implementation Details  Our models are implemented within our Theano based  CNN library called DeNet
The source-code is available  from: https://github.com/lachlants/denet  N.N
Training methodology  In all experiments we used Nesterov style SGD [NN] with  an initial learning rate of 0.N, momentum of 0.N and weight  decay of 0.000N (only applied to weights)
A batch size  of NNN was employed for both models with NN samples per  GPU iteration
The learning rate was divided by N0 at epoch  N0 and epoch N0 and a total of N0 training epochs were performed
Note that, apart from the batch size changes, these  hyperparameters are identical to those used when training  the original residual networks for classification [N]
No online hard negative mining [NN] or other gradient optimization techniques were applied, however, we observed some  instances of overtraining on Pascal VOC
In response, to  increase exposure to negative samples, we introduced N0% randomly generated bounding box samples during training
 An augmentation strategy very similar to GoogLeNet  [N0] was employed to improve model generalization to different scales and translations
For each sample, a black border was added to the smallest dimension to produce a square  image
At test time, this image was scaled to NNN×NNN pix- els using bilinear sampling, during training a random crop  was selected with an area between (0.0N, N.0) relative to the border image and an aspect ratio between (N/N, N/N)
The random crop was discarded and a new one generated if no  ground truth objects overlapped with the crop by at least  N0%
This process was repeated up to N0 times and, as  a fallback, the entire bordered image was returned
As in  testing, the resulting crop was scaled to NNN × NNN pixels
Random photometric (contrast, saturation and brightness)  and mirror augmentation was also employed [N0]
 N.N
Identifying Sampling Bounding Boxes (RoIs)  A simple algorithm was developed to quickly search the  corner distribution for non-null bounding boxes:  N
Search the corner distribution for corners {k, y, x} ∈ Cλ where Pr(t = N|k, y, x) > λ
 N
For each corner type, select the M corners with the greatest likelihood CM ⊆ Cλ  N
Generate a set of unique bounding boxes by matching  every corner within CM of type top− left with every one of type bottom− right
 N
Calculate the probability of each bounding box being  non-null via Equation N
 N
Repeat steps N and N with corners of type top− right and bottom− left
 N
Sort bounding boxes by probability and keep the NN  largest to produce the sampling bounding boxes BS 
 Since the vast majority of corners are culled in step N this  method obtains a significant speed up beyond the naive  brute force method i.e
testing every possible bounding box
 N
Results and Analysis  In this section we compare our design with previously  published models
We note that in some cases, an applesto-apples comparison is difficult due to the wide range of  base models, data augmentation schemes and dataset merging
In particular, we note that SSD utilize larger batch sizes  while R-CNN models have larger input resolutions (on average)
All our DeNet timing results are provided for a single Titan X GPU (CuDNN NNN0) with a batch size of Nx,  the same settings used in SSD
For brevity we include only  three flavours of the non real-time Faster R-CNN model,  the original RPN (VGG), the ResNet-N0N extension RPN+  (ResNet-N0N) and R-FCN for comparison (highlighted in  grey in the tables)
We note that due to implementation restrictions RPN based models are tested with a single image  per batch
 Model Max
Input BS L Param
 RPN (VGG) [NN] N000× N00 N NN NNNM RPN+ (ResNet) [N] N000× N00 N N00 NNM R-FCN [N] N000× N00 N N00 NNM Fast YOLO [NN] NNN× NNN N N NM YOLO [NN] NNN× NNN N NN N0M SSDN00 [NN] N00× N00 N NN NNM SSDNNN [NN] NNN× NNN N NN NNM DeNet-NN (ours) NNN× NNN N NN NNM DeNet-N0N (ours) NNN× NNN N N0N NNM  Table N
Model overview detailing maximum input image sizes,  batch size at test time (BS), number of activation layers (L) and  approximate number of parameters
 In Table N we provide a broad overview of the baseline  models
We note that despite an increased number of layers  and parameters the DeNet models obtain improved evaluation rates (See Section N.N)
 NNN    λs N N N N N N N  λt N N0 N0 N00 N00 N00 N00  λb N N N0 0.N N N0 N  MAP (%) NN.N NN.N NN.N NN.N NN.N NN.N N0.N  Table N
Optimizing cost hyperparameters {λs, λt, λb}, see Equa- tion N
MAP is provided for Pascal VOC N00N val dataset
 Sample BBoxs NN NNN NNN N00 NNN NNN N0NN  Coverage (%) NN NN NN NN NN NN NN  Eval
Rate (Hz) NN N0 NN NN NN NN NN  Table N
Sample bounding boxes vs coverage over training dataset  and evaluation rate
 N,0NNNNNNNNNNN0 NN  NN  NN  N0  NN  NN  #Sample BBoxs @ Test Time  M A  P (%  )  BBox=NN BBox=NNN  BBox=NNN BBox=NNN  BBox=N0NN  Figure N
The MAP on the Pascal VOC N00N validation dataset  with varying number of bounding box samples during training (see  legend) and testing (displayed on x-axis)
 N.N
Hyperparameter Optimization  For the following we used the DeNet-NN model and  trained it on Pascal VOC N00N train and Pascal VOC  N0NN trainval (NN,0NN images), for testing we used Pascal VOC N00N val (N,NN0 images)
The same DeNet-NN  model initialization was used for all experiments
We applied the training procedure described in Section N.N except  with a batch size of NN
 In Table N we performed a coarse search over the corner and bounding box regression cost parameters λt and λb
Best results were achieved by setting λs = N, λt = N00 and λb = N, these were applied in all subsequent exper- iments
Next, we investigated the model behaviour with  varying numbers of sampling bounding boxes
In particular,  we trained a set of models with N = {N, NN, NN, NN, NN}
At test time, we took each of these models and varied N from N to NN to produce Figure N
In Table N, we provide the  model evaluation rate and coverage (percentage of ground  DeNet-NN DeNet-N0N  Estimate corners N.N ms NN% NN.Nms N0%  Generate RoI N.N ms NN% N.N ms N%  Classify RoI N.N ms NN% N.N ms NN%  Estimate instances 0.N ms N% 0.N ms 0%  Total (per image) NN.N ms - NN.0 ms Table N
A coarse timing breakdown per image for DeNet models  at test time on a Titan X GPU
 truth with a sampling bounding box with IoU > 0.N) over the training set that was obtained by the RoI estimator described in Section N
As expected, we observed a consistently improving MAP with diminishing returns above NNN  when training with a larger number of sampling bounding  boxes
In general we observed an improved MAP with  increased testing bounding boxes at the cost of evaluation  rates
For subsequent experiments we set N = NN for both training and testing
 N.N
Timing Breakdown and Evaluation Rates  In Table N we present a coarse analysis of the timing for  both DeNet models
We broke the timing into N sequentially  executed stages:  N
Estimate corners: Images are uploaded to the GPU  and fed through the base network generating the corner distribution and sampling feature maps
The corner  distribution is transferred from GPU to CPU memory
 N
Generate RoI: The sampling bounding boxes (RoIs)  are generated from the corner distribution
 N
Classify RoI: The final classification CNN is executed, the classification distribution and bounding box  regression outputs are transferred from GPU to CPU
 N
Estimate instances: Non-Max Supression is run over  the resulting detection hits producing a de-duplicated  list of detections for each image
 We observe that the vast majority of time is spent evaluating the base network to generate corners
Also, note that  the CPU bound Generate RoI stage timing can vary substantially between different samples and may require additional tuning depending on application
Furthermore, we  wish to emphasize a number of important features of the  DeNet model which makes it significantly faster than most  other baseline models:  • Deconvolution: Spatial information is increased via  deconvolution layers as opposed to the atrous modified models used in R-FCN and SSD
This method introduces spatial information significantly later in the  model, greatly improving evaluation rates
 NNN    • Fast RoI Features: Features are extracted via a simple nearest neighbour sampling method, limiting the  the number of feature reads to NN per RoI
Some RPN  variants use pooling which varies from NN-NN0 per RoI
 • Input Image Dimensions: DeNet scales all images  to NNNxNNN pixels, whereas RPN based methods use a  varying input size up to N000xN00 pixels
 • Batching: Our models are tested with Nx samples per  batch (same as SSD)
This improves GPU utilization
 With these improvements in timing we are able to use a  more expressive base model for the same evaluation rate
 N.N
RoI Coverage Comparison  In Table N
we provide the coverage obtained by by  the top N00 RoIs for RPN, R-FCN and DeNet methods
 We observe that given a relatively low number of RoIs,  RPN (VGG) and R-FCN provide better coverage at low IoU  thresholds, however with increasing IoU the DeNet models  provide significantly improved coverage
 Top N00 Coverage@IoU (%)  Model 0.N 0.N 0.N 0.N 0.N  RPN (VGG) NN.NN NN.NN NN.NN NN.NN N.NN  R-FCN NN.NN NN.NN NN.NN NN.NN N.NN  DeNet-NN N0.NN NN.00 NN.NN NN.NN NN.NN  DeNet-N0N NN.NN NN.NN NN.NN NN.NN NN.NN  Table N
Coverage on Pascal VOC N00N test using N00 sample  bounding boxs (RoI proposals)
 We note that RPN / R-FCN utilize bounding box regression and deduplication methods in their RoI proposal networks, these factors improve coverage with low numbers of  proposals
As demonstrated in the following sections, the  DeNet RoI coverage results do not necessarily translate to a  reduced MAP for the full model, which includes NMS and  bounding box regression, at lower IoU thresholds
 N.N
MSCOCO  The Microsoft Common Object in Context [NN] dataset  consists of NNK training and N0K validation images distributed across N0 classes
For testing, the dataset includes  an N0K test dataset from which a user known subset of N0K  images forms the test-devN0NN set and an unknown  subset of N0K images forms testN0NN allowing only N  evaluations
Due to the dataset size, number of classes  and relatively small size of the object instances within the  images, MSCOCO is a considerably more difficult dataset  compared to the Pascal VOC challenges
The primary evaluation metric for MSCOCO is the integral of the MAP over  the detection matching parameter IoU=0.N to IoU=0.NN
 This metric places a greater emphasis on localization performance compared to the Pascal datasets
We found that  setting λt = N0 for DeNet-N0N was necessary for conver- gence, this is likely due to the greater number of corners  present within each image on average compared to the validation experiments
Training took ∼N days with N× Tesla PN00 GPUs for DeNet-NN and ∼N.N days with N× Tesla PN00 GPUs for DeNet-N0N
 In Table N we provide the precision and recall results  for our models on test-devN0NN
The DeNet models  demonstrate a clear advantage over other high evaluation  rate implementations e.g
our real-time DeNet-NN model  beats SSDN00 by N.N% MAP at the same evaluation rate and SSDNNN by N.N% at more than twice the evaluation rate
The DeNet-N0N model furthers this advantage and is  only beaten by the very slow competition style RPN+ model  utilizing multi-scale evaluation and bounding box refinement
At the time of writing, the DeNet-N0N model obtains  a result good enough to be in the top-N0 on the MSCOCO  competition leaderboard which doesn’t consider evaluation  time
The skip model variants consistently improved performance on small and medium sized objects (see AR and AP  for small and medium area objects in table) with a minor  cost to large objects and evaluation rate
The wide variants further improved small object detection and fine object  localization at the cost of evaluation rate
Near identical  results were obtained on MSCOCO test-stdN0NN e.g
 we obtained a MAP@[0.N:0.NN] of NN.N% and NN.N% for  DeNet-NN and DeNet-N0N respectively
Analysis suggests  our advantage stems from improved large object detection  and finegrain object localization performance, as reflected  in the MAP@IoU=0.NN result
We argue this is an outcome  of the much larger range of candidate RoI’s our method produces e.g
the vanilla DeNet models can select from a possible set of N.N × N0N bounding boxes while SSD utilizes N.N× N0N
Utilizing such a large set of candidate bounding boxes would likely be intractable with the dense evaluation  methods used in the YOLO and RPN derived models
 N.N
Pascal VOC N00N  We combined the trainval samples from Pascal VOC  N00N and N0NN [N] (denoted 0N+NN in table) to produce  NN,NNN training samples
For testing we used Pascal VOC  N00N test containing N,NNN samples
We note that this  dataset is considerably smaller than MSCOCO and therefore more susceptible to overtraining and image augmentation methods
Training time was ∼NN hours for DeNet-NN  using N× Tesla PN00s and ∼N0 hours for DeNet-N0N using N× Tesla PN00s
In Table N, we provide the MAP and tim- ing results
We observed the skip layer variant DeNet-NN  improving upon SSDN00’s peak MAP by N.N% and N0Hz
In the near real-time domain DeNet-N0N matches SSDNNN  at a higher evaluation rate
 NNN    Eval
AP@IoU (%) AP@Area (%) AR@Dets (%) AR@Area (%)  Model Rate 0.N:0.NN 0.N 0.NN S M L N N0 N00 S M L  RPN (VGG) N Hz NN.N NN.N - - - - - - - - - RPN+ (ResNet) <N Hz NN.N NN.N - NN.N NN.N N0.N - - - - - - R-FCN N Hz NN.N NN.N - N0.N NN.N NN.0 - - - - - SSDN00 NN Hz NN.N NN.N NN.N N.N NN.N NN.N NN.N NN.N NN.N N.N NN.N NN.N  SSDNNN NN Hz NN.N NN.N NN.N N.0 NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.0  DeNet-NN NN Hz NN.N NN.N NN.N N.N N0.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.0  DeNet-NN (skip) NN Hz NN.N NN.N NN.N N.N N0.N NN.0 NN.N NN.0 NN.N NN.N NN.N NN.N  DeNet-NN (wide) NN Hz N0.0 NN.N NN.N N0.N N0.N NN.N NN.N NN.N N0.N NN.0 NN.N N0.N  DeNet-N0N NN Hz NN.N N0.N NN.N N.N NN.N N0.N NN.N NN.N N0.N NN.N NN.N NN.N  DeNet-N0N (skip) NN Hz NN.N NN.N NN.N N0.N NN.N N0.N NN.N N0.N N0.N NN.N NN.N NN.N  DeNet-N0N (wide) NN Hz NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
MSCOCO average precision (AP) and average recall (AR) results evaluated on test-devN0NN dataset
 Model Dataset Eval
Rate MAP  RPN (VGG) 0N+NN N Hz NN.N%  RPN (ResNet) 0N+NN N Hz NN.N%  R-FCN 0N+NN N Hz N0.N%  Fast YOLO 0N+NN NNN Hz NN.N%  YOLO 0N+NN NN Hz NN.N%  SSDN00 0N+NN NN Hz NN.N%  SSDNNN 0N+NN NN Hz NN.N%  DeNet-NN 0N+NN NN Hz NN.N%  DeNet-NN (skip) 0N+NN NN Hz NN.N%  DeNet-N0N 0N+NN NN Hz NN.0%  DeNet-N0N (skip) 0N+NN NN Hz NN.N%  Table N
Pascal VOC N00N mean average precision and timing
 N.N
Pascal VOC N0NN  In this experiment we combine trainvaltest from  Pascal VOC N00N and trainval from Pascal VOC N0NN  [N] (denoted 0N++NN in table) to produce NN,N0N training  samples
Test scores are evaluated on N0,NNN samples by the  Pascal VOC N0NN testing server
For this dataset the DeNetNN model matches SSDN00, however, for reasons unknown,  DeNet-N0N demonstrates results below SSDNNN
For reference, we note that DeNet-N0N obtains results near identical to the other ResNet-N0N based model, RPN (ResNet)  with an order of magnitude improvement in evaluation rate
 Training time was ∼NN hours for DeNet-NN with N× Tesla PN00s and ∼NN hours for DeNet-N0N with N× Tesla PN00s
 N
Conclusion  In this work, we describe a framework for sparse estimation with CNNs and present a novel region-of-interest  detector and classification model which reduces manual  engineering and improves state-of-the-art detection performance with real-time and near real-time evaluation rates
 Model Dataset Eval
Rate MAP  RPN (VGG) 0N++NN N Hz N0.N%  RPN (ResNet) 0N++NN N Hz NN.N%  R-FCN 0N++NN N Hz NN.N%  YOLO 0N++NN NN Hz NN.N%  SSDN00 0N++NN NN Hz NN.N%  SSDNNN 0N++NN NN Hz NN.N%  DeNet-NN 0N++NN NN Hz NN.N%  DeNet-N0N 0N++NN NN Hz NN.N%  Table N0
Pascal VOC N0NN mean average precision and timing
 Utilizing deconvolution and skip layers first described in  the context of semantic segmentation, we demonstrated a  highly computationally efficient model with tightly coupled  RoI, class prediction and bounding box regression
We provide further evidence that skip connections consistently improved detection rates for small and medium sized objects
 While the wide model variant highlighted the importance of  corner map resolution for small and medium sized objects,  and provides a natural pathway for future development
 Analysis suggests our model performs particularly well  when finer object localization is desirable
We propose that  the improved localization is due to the much larger set of  possible sampling bounding boxes that are feasible with  our sparse sampling method i.e
N.N × N0N compared to less than N.N × N0N for SSDNNN and RPN
This feature al- lows the model to potentially select a bounding box (before bounding box regression) which is significantly closer  to the ground truth
Furthermore, since we no longer define a set of reference bounding boxes, this approach has reduced manual engineering requirements and can adapt well  to problems which utilize bounding boxes with a very large  range of aspect ratios and scales e.g
rotationally variant or  non-rigid objects
 NNN    References  [N] M
Everingham, L
Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
Int
J
Comput
Vision, NN(N):N0N–NNN, June N0N0
N,  N, N, N  [N] G
Ghiasi and C
C
Fowlkes
Laplacian pyramid reconstruction and refinement for semantic segmentation
In European  Conference on Computer Vision, pages NNN–NNN
Springer,  N0NN
N  [N] R
Girshick
Fast r-cnn
In Proceedings of the IEEE International Conference on Computer Vision, pages NNN0–NNNN,  N0NN
N, N  [N] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Regionbased convolutional networks for accurate object detection  and segmentation
IEEE transactions on pattern analysis  and machine intelligence, NN(N):NNN–NNN, N0NN
N, N  [N] K
He, X
Zhang, S
Ren, and J
Sun
Delving deep into  rectifiers: Surpassing human-level performance on imagenet  classification
In Proceedings of the IEEE international conference on computer vision, pages N0NN–N0NN, N0NN
N  [N] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NN0–NNN, N0NN
N, N, N  [N] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
 CoRR, abs/NN0N.0NNNN, N0NN
N  [N] Y
LeCun, L
Bottou, Y
Bengio, and P
Haffner
Gradientbased learning applied to document recognition
Proceedings of the IEEE, NN(NN):NNNN–NNNN, November NNNN
N  [N] Y
Li, K
He, J
Sun, et al
R-fcn: Object detection via regionbased fully convolutional networks
In Advances in Neural  Information Processing Systems, pages NNN–NNN, N0NN
N, N  [N0] T
Lin, P
Dollár, R
B
Girshick, K
He, B
Hariharan, and  S
J
Belongie
Feature pyramid networks for object detection
CoRR, abs/NNNN.0NNNN, N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European Conference on Computer Vision, pages NN0–NNN
Springer, N0NN
N, N, N  [NN] W
Liu, D
Anguelov, D
Erhan, C
Szegedy, S
E
Reed,  C
Fu, and A
C
Berg
SSD: single shot multibox detector
 CoRR, abs/NNNN.0NNNN, N0NN
N, N, N  [NN] H
Noh, S
Hong, and B
Han
Learning deconvolution network for semantic segmentation
In Proceedings of the IEEE  International Conference on Computer Vision, pages NNN0–  NNNN, N0NN
N  [NN] J
Redmon, S
Divvala, R
Girshick, and A
Farhadi
You  only look once: Unified, real-time object detection
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNN–NNN, N0NN
N, N, N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  Advances in neural information processing systems, pages  NN–NN, N0NN
N, N, N, N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
S
Bernstein,  A
C
Berg, and F
Li
Imagenet large scale visual recognition  challenge
CoRR, abs/NN0N.0NNN, N0NN
N  [NN] A
Shrivastava, A
Gupta, and R
Girshick
Training regionbased object detectors with online hard example mining
In  Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNN–NNN, N0NN
N  [NN] R
K
Srivastava, K
Greff, and J
Schmidhuber
Training  very deep networks
In Advances in neural information processing systems, pages NNNN–NNNN, N0NN
N  [NN] I
Sutskever, J
Martens, G
E
Dahl, and G
E
Hinton
On the  importance of initialization and momentum in deep learning
 ICML (N), NN:NNNN–NNNN, N0NN
N  [N0] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In Proceedings of the IEEE  Conference on Computer Vision and Pattern Recognition,  pages N–N, N0NN
N, N  [NN] J
R
R
Uijlings, K
E
A
van de Sande, T
Gevers, and  A
W
M
Smeulders
Selective search for object recognition
 International Journal of Computer Vision, N0N(N):NNN–NNN,  N0NN
N  NNNHigher-Order Integration of Hierarchical Convolutional Activations for Fine-Grained Visual Categorization   Higher-order Integration of Hierarchical Convolutional Activations for  Fine-grained Visual Categorization  Sijia CaiN, Wangmeng ZuoN, Lei ZhangN∗  NDept
of Computing, The Hong Kong Polytechnic University, Hong Kong, China NSchool of Computer Science and Technology, Harbin Institute of Technology, Harbin, China  {csscai, cslzhang}@comp.polyu.edu.hk, cswmzuo@gmail.com  Abstract  The success of fine-grained visual categorization  (FGVC) extremely relies on the modeling of appearance  and interactions of various semantic parts
This makes  FGVC very challenging because: (i) part annotation and  detection require expert guidance and are very expensive;  (ii) parts are of different sizes; and (iii) the part interactions are complex and of higher-order
To address these issues, we propose an end-to-end framework based on higherorder integration of hierarchical convolutional activations  for FGVC
By treating the convolutional activations as local descriptors, hierarchical convolutional activations can  serve as a representation of local parts from different scales
 A polynomial kernel based predictor is proposed to capture higher-order statistics of convolutional activations for  modeling part interaction
To model inter-layer part interactions, we extend polynomial predictor to integrate hierarchical activations via kernel fusion
Our work also provides  a new perspective for combining convolutional activations  from multiple layers
While hypercolumns simply concatenate maps from different layers, and holistically-nested network uses weighted fusion to combine side-outputs, our approach exploits higher-order intra-layer and inter-layer relations for better integration of hierarchical convolutional  features
The proposed framework yields more discriminative representation and achieves competitive results on the  widely used FGVC datasets
 N
Introduction  Deep convolutional neural networks (CNNs) have  emerged as the new state-of-the-art for a wide range of visual recognition tasks
Nevertheless, it remains quite challenging to derive the effective discriminative representation for fine-grained visual categorization (FGVC), primar∗This work is supported by HK GRC GRF grant (PolyU NNNNNN/NNE)  and China NSFC grant (no
NNNNNNNN)
 ily due to subtle semantic differences between sub-ordinate  categories
Conventional CNNs usually deploy the fully  connected layers to learn global semantic representation and  may not be suitable to FGVC
Therefore, leveraging local  discriminative patterns in CNN is crucial to obtain more  powerful representation, and recently has been intensively  studied for FGVC
 Part-based representations [NN, N, NN, NN, NN] built on  CNN features have been a predominant trend in FGVC
 Such methods follow a detection module consisting of part  detection and appearance modeling to extract regional features on deeper convolutional layers in R-CNN [NN] based  scenario
Then global appearance structure is incorporated  to pool these regional features
Although these methods  have yielded rich emporical returns, they still pose the following issues: (N) A considerable number of part-based  methods [NN, N, NN] heavily rely on the detailed part annotations to train accurate part detectors, which is costly  and further limits the scalability for large-scale datasets;  moreover, identifying discriminative parts for specific finegrained objects is quite challenging and often requires interaction with human or expert knowledge [N, N0]; (N) The discriminative semantic parts in images often appear at different scales
As each spatial unit in the deeper convolutional  layer corresponds to a specific receptive field, activations  from a single convolutional layer are limited in describing  various parts with different sizes; (N) Exploiting the joint  configuration of individual object parts is very important for  object appearance modeling
A few works introduce additional geometric constraints for object parts including the  popular deformable parts model [NN], constellation model  [NN] and order-shape constraint [NN]
One key disadvantage  of these approaches is that they only characterize the firstorder occurrences and relationships of very few parts, however, cannot be readily applied to model objects with more  parts
Consequently, our focus is to capture the higher-order  statistics of those semantic parts at different scales, and thus  provide a more flexible way for global appearance modeling  without the help of part annotation
 NNNN    Figure N
Visualization of several activation maps that corresponds  to large responses of the sum-pooled vectors of two activation layers reluN N and reluN N in VGG-NN model
 In recent works [NN, NN], the deeper convolutional filters are regarded as weak part detectors and the corresponding activations as the responses of detection, shown in Fig
 N
Motivated by this observation, instead of part annotations and explicit appearance modeling, we straightforwardly exploit the higher-order statistics from the convolutional activations
We first provide a perspective of matching kernel to understand the widely adopted mapping and  pooling schemes on convolutional activations in conjunction with linear classifier
Linear mapping and direct pooling only capture the occurrence of parts
In order to capture  the higher-order relations among parts, it is better to explore local non-linear matching kernels to characterize the  higher-order part interactions (e.g., co-occurrence)
However, designing an appropriate CNN architechture that can  be plugged with non-linear local kernels in an end-to-end  manner is non-trivial
The kernel scheme is required to  have explicit non-linear maps and be differentiable to facilitate back-propagation
One representative work is convolutional kernel network (CKN) [NN], which provides a  kernel approximation scheme to interpret CNNs
A related  polynomial network [NN] is to utilize polynomial activation  functions as alternatives of ReLU in CNNs to learn nonlinear interations of feature variables
Similarly, we leverage the polynomial kernel to serve in modeling higher-level  part interactions and derive the polynomial modules that allow trainable structure built on CNNs
 With the kernel scheme, we extend our framework for  higher-order integration of hierarchical convolutional activations
The effectiveness of fusing hierarchical features in  CNNs has been widely reported in visual recognition
The  benefits come from both the different discriminative capacities of multiple convolutional layers and the coarse-to-fine  object description
However, the existing methods simply  concatenate or sum multiple activations into a holistic representation [NN], or adopt a decision level fusion to combine  side-outputs from different layers [NN, NN]
These methods,  however, are limited in exploiting the intrinsic higher-order  relationships of convolutional activations in either the intralayer level or the inter-layer level
By using the kernel fusion on hierarchical convolutional activations, we can construct a richer image representation for cross-layer integration
Compared with the related works that perform feature  fusion via learning multiple networks [N, NN, NN], our framework is easy to construct and more effective for FGVC
 N
Related work  N.N
Feature encoding in CNNs  Applying encoding techniques for the local convolutional activations in CNNs has shown significant improvements compared with the fully-connected outputs [N, NN]
 In this case, the Vectors of Locally Aggregated Descriptors (VLAD) and Fisher Vectors (FV) as high-order statistics based representation can be readily applied
Gong et  al
[NN] propose to use VLAD to encode local features extracted from multiple regions of an image
In [N, N, NN],  the values of FV encoding on convolutional activations are  discovered for scene, texture and video recognition tasks
 However, regarding feature encoding as an isolated component is not the optimal choice for CNNs
Therefore, Lin et  al
[NN] propose a bilinear CNN (B-CNN) as codebook-free  coding that allows end-to-end training for FGVC
The very  recent work in [N] builds a weakly place recognition system by introducing a generalized VLAD layer that can be  trained with off-the-shelf CNN models
An alternative for  feature mapping is to exploit kernel approximation feature  embedding
Yang et al
[NN] introduce adaptive Fastfood  transform in their deep fried convnets to replace the fullyconnected layers, which is a generalization of the Fastfood  transform for approximating kernels [NN]
Gao et al
[NN]  implement an end-to-end structure to approximate degree-N  homogeneous polynomial kernel by utilizing random features and sketch techniques
 N.N
Feature fusion in CNNs  Compared with the fully connected layers capturing the  global semantic information, convolutional layers preserve  more instance-level details and exhibit diverse visual contents as well as different discriminative capacities, which  are more meaningful to the fine-grained recognition task  [N]
Recently a few works attempt to investigate the effectiveness of exploiting features from different convolutional  layers [NN, NN]
Long et al
[NN] combine the feature maps  from intermediate level and high level convolutional layers  in their fully convolutional network to provide both finer  details and higher-level semantics for better image segmentation
Hariharan et al
[NN] introduce hypercolumns for  localization and segmentation, where convolutional activations at a pixel of different feature maps are concatenated  as a vector as a pixel descriptor
Similarly, Xie and Tu  [NN] present a holistically-nested edge detection scheme in  which the sideoutputs are added after several lower convoNNN    lutional layers to provide deep supervision for predicting  edges at multiple scales
 N
Kernelized convolutional activations  Most part-based CNN methods for FGVC consist of two  components: (i) feature extraction for semantic parts on the  last convolutional layer, and (ii) spatial configuration modeling for those parts to produce discriminative image representation
In this work, we treat the convolutional filter  as part detector, and then the convolutional activations in  a single spatial position can be considered as the part descriptions
Therefore, instead of explicit part extraction, we  introduce polynomial predictor to integrate a family of local matching kernels for modeling higher-order part interactions and derive powerful representation for FGVC
 N.N
Matching kernel and polynomial predictor  Suppose that an image I is passed by a plain CNN,  and we denote the ND activations X ∈ RK×M×N ex- tracted from some specific convolutional layer as a set of  K-dimensional descriptors {xp}p∈Ω, where K is the num- ber of feature channels, xp represents the descriptor at a  particular position p over the set Ω of valid spatial locations (|Ω| = M ×N )
We first consider the matching scheme K for activation sets X and X̄ from two images, in which the  set similarity is measured via aggregating all the pairwise  similarities among the local descriptors:  K(X , X̄ ) = Agg({k(xp, x̄p̄)}p∈Ω,p̄∈Ω̄) = ψ(X )Tψ(X̄ ), (N)  where k(·) is some kernel function between individual de- scriptors of two activation sets, Agg(·) is some set-based aggregation function, ψ(X ) and ψ(X̄ ) are the vector repre- setations for sets
It is worth noting that the construction of  K presented above is decomposed into two steps in CNNs: feature mapping and feature aggregation
The mapping step  maps each local descriptor x ∈ RK as φ(x) ∈ RD in elaborated feature space
The aggregating step produces an  image-level representation ψ(X ) from the set {φ(xp)}p∈Ω through some pooling function g(·)
 The key for FGVC is to discover and represent those  local regions which share common appearances within the  same category while exhibiting distinctive difference across  categories
Based on the matching scheme K in Eqn
(N), appropriate pooling operators have been designed to efficiently prune non-discriminative matching subset while retaining those highly discriminative ones into image representation
Among them, sum pooling assigns equal weights  to each position, and does not emphasize any position
Max  pooling only considers the most significant position, which  results in enormous information loss and is prone to small  interference
Other pooling operators such as generalized  max pooling [NN] and ℓp-norm pooling [N0] may be effective in discovering informative regions, but the feasible endto-end schemes are unclear
Our attention is to model the  higher-order relationships for discriminative representation  of local patch and design suitable local mapping function  φ which can be stacked upon CNN for end-to-end training
 Thus, we simply adopt g(·) as the global sum pooling, in which case we denote it as:  ψ(X ) = g({φ(xp)}p∈Ω) = ∑  p∈Ω  φ(xp)
(N)  The above matching underpinning highlights the advantage  of generating image-level representation compatible with  linear predictors, which can be interpreted as the linear  combination of all local compositions accordingly:  f(x) = 〈w, φ(x)〉, (N) wherew is the parameter of predictor, we omit the bias term  and position subscript p here for later convenience
As our  aim is to capture more complex and higher-order relationships among parts, to this end, we propose the following  polynomial predictor:  f(x) =  K∑  k=N  wkxk +  R∑  r=N  ∑  kN,...,kr  WrkN,...,kr ( r∏  s=N  xks), (N)  whereR is the maximal degree of part interactions, Wr is a  r-order tensor which contains the weights of degree-r variable combinations in x
For instance, when r = N, Wi,j,k is the weight of xixjxk
We discuss different polynomial  predictors as well as their corresponding kernels as follows:  N) Linear kernel: k(x, x̄) = 〈x, x̄〉 is the most simple kernel that refers to an identity map φ : x N→ x, which is identical to the polynomial predictor of degree-N: f(x) =∑K  k=N wkxk
 N) Homogeneous polynomial kernel: k(x, x̄) = 〈x, x̄〉r has shown the superiority in characterizing the in- trinsic manifold structure of dense local descriptors [N]
The  induced non-linear map φ : x N→ ⊗rx, where ⊗rx is a tensor defined by the r-order self-outer product [NN] of x,  is able to model all the degree-r interactions between variables
Its polynomial predictor obeys the following form:  f(x) = ∑  kN,...,kr  WrkN,...,kr ( r∏  s=N  xks)
(N)  Notice that the polynomial predictor of degree-N homogeneous polynomial kernel is defined as ∑  i,j Wi,jxixj ,  which captures all pairwise/second-order interactions between variables and is an increasingly popular model in  classification tasks [NN]
 N) Positive definite kernel: as discussed in [NN], the  positive definite kernel k(x, x̄) : (x, x̄) N→ f(〈x, x̄〉) de- fines an analytic function which admits a Maclaurin expansion with only nonnegative coefficients, i.e., f(x) =  NNN    ∑∞ r=0 arx  r, ar ≥ 0
For instance, a non-homogeneous degree-N polynomial kernel (〈x, x̄〉 + N)N corresponds to a polynomial predictor that captures all single and pairwise  interactions between variables
It also indicates that the positive definite kernel can be arbitrarily accurate approximation of polynomial kernels in principle of sufficiently high  degree polynomial expansions for target functions
 N.N
Tensor learning for polynomial kernels  Before deriving the end-to-end CNN architecture for  learning the parameters in Eqn
(N), we first reformulate  the polynomial predictor into a more concise tensor form:  f(x) = 〈w,x〉+ R∑  r=N  〈Wr,⊗rx〉, (N)  where 〈W ,V〉 is the inner product of two same-sized ten- sors W ,V ∈ RKN×···×Kr , which is defined as the sum of the products of their entries
It is observed that the tensor  ⊗rx comprises all the degree-r monomials in x
There- fore, any degree-r homogeneous polynomial predictor satisfies 〈Wr,⊗rx〉 for some r-order tensor Wr; likewise, any r-order tensor Wr determines a degree-r homogenous  polynomial predictor
This equivalence between polynomials and tensors motivates us to transform the parameter  learning of polynomial predictor into tensor learning
 Rather than estimating the variable interations in tensors  independently, an alternative method is tensor decomposition [NN] which breaks the independence of interaction parameters and estimates the reliable interaction parameters  under high sparsity
Tensor decomposition is widely used  in tensor machines [NN] for sparse data based regression,  which circumvents the parameter storage issue and achieves  better generalization in practice
We then embrace the rankone tensor decomposition [NN] in our next step of tensor  learning for consideration of two aspects: the high sparsity  of activations in deeper layers of CNNs and the parameter  sharing of convolutional filters
 We first briefly review the notations and definitions in the  area of rank-one tensor decomposition: the outer product  of vectors uN ∈ RKN , 


,ur ∈ RKr is the KN × · · · × Kr rank-one tensor that satisfies (uN ⊗ · · · ⊗ ur)kN...,kr = (uN)kN · · · (ur)kr 
The rank-one decomposition for a tensor W is defined as W =  ∑D d=N α  dudN ⊗ · · · ⊗ udr , where αd is the weight for d-th rank-one tensor, D is the rank of  the tensor if D is minimal
We then apply the rank-one  approximation [NN] for each r-order tensor Wr and present  the following alternative form of polynomial predictor:  f(x) = 〈w,x〉+ R∑  r=N  〈 Dr∑  d=N  αr,du r,d N ⊗ · · · ⊗ ur,dr ,⊗rx〉
(N)  In order to learn w, αr,d and ur,ds (r = N, 


, R, s = N, 


, r, d = N, 


, Dr), in next section, we show that all  the parameters can be absorbed into the conventional trainable modules in CNNs
 N.N
Trainable polynomial modules  According to the tensor algebra, the Eqn
(N) can be further rewritten as:  f(x) = 〈w,x〉+ R∑  r=N  Dr∑  d=N  αr,d r∏  s=N  〈ur,ds ,x〉 (N)  = 〈w,x〉+ R∑  r=N  〈αr, zr〉 (N)  where the d-th element of the vector zr ∈ RDr is∏r s=N〈ur,ds ,x〉 which characterizes the degree-r variable  interactions under a single rank-one tensor basis
αr = [αr,N, 


, αr,D  r  ]T is the associated weight vector of all Dr rank-one tensors
A key observation of Eqns
(N)  (N) is that we are able to decouple the parameters into  {w,αN, 


,αR} and {{ur,ds }s=N,...,r;d=N,...Dr}r=N,...,R
Notice that for each s, we can first deploy {ur,ds }d=N,...Dr as a set ofDr N×N convolutional filters on X to generate a set of feature maps Zrs of dimension D  r ×M ×N 
Then, the feature maps {Zrs}s=N,...,r from different ss are combined by element-wise product to obtain Zr = ZrN ⊙ · · · ⊙ Zrr
Therefore, {ur,ds }s=N,...,r;d=N,...Dr can be treated as a poly- nomial module in learning degree-r polynomial features
 As for the former parameter group, it can be easily embedded into the learning of the classifier for the concatenated  polynomial features
Refering to Eqn
(N), the derivatives  for x and each degree-r convolutional filter ur,ds in back  propagation process can be achieved by:  ∂ℓ  ∂x =  ∂ℓ  ∂yr  Dr∑  d=N  r∑  s=N  ( ∏  t N=s  〈ur,dt ,x〉)ur,ds (N0)  ∂ℓ  ∂u r,d s  = ∂ℓ  ∂yr ( ∏  t N=s  〈ur,dt ,x〉)x (NN)  where yr = g(Zr) = g({zr}) is the pooled feature rep- resentation for degree-r polynomial module, ℓ is the loss  associated with yr
On this basis, we can embrace those  polynomial modules with the trainable CNN architectures  and are able to model the higher-order part statistics of any  degree
Even though the dominant level of those highlycorrelated parts will be enhanced with a larger r, the highorder tensor usually needs large Dr to guarantee a good approximation
Therefore, a relative small degree r should  be considered in practice because a high-degree polynomial  module increases the computational cost in back propagation, i.e., Eqns
(N0) (NN), and the induced high dimensionality of feature would cause over-fitting
 NNN    Figure N
Illustration of our integration framework
The convolutional activation maps are concatenated as X = concat(X N, 


,XL) and fed into different branches
For r-th branch (r ≥ N), the degree-r polynomial module consisting of r groups of N× N convolutional filters is deployed to obtain r sets of feature maps {Zrs}s=N,...,r 
Then {Z  r s}s=N,...,r are integrated as Z  r by applying element-wise product  ⊙
At last, X and all Zrs are concatenated as the degree-r polynomial features, following by sum pooling layer to obtain the pooled representation y = concat(yN, 


,yL) with the dimension of  ∑R r=N  Dr (DN denotes the channel number of X ), and softmax layer
 N
Hierarchical convolutional activations  N.N
Higher-order integration using kernel fusion  The polynomial predictor provides a good measure for  the highly-correlated parts but the activations on individual  convolutional layer are not sufficient to describe the part relations from different levels of abstraction and scale
Consequently, we investigate a kernel fusion scheme to combine  the hierarchical convolutional activations
Suppose that the  local activation descriptor sets from L convolutional layers  at spatial correspondences for two images are denoted as  ψI : {xl}Ll=N and ψĪ : {x̄l}Ll=N
Then we generalize φ under linear factorization to fuse the local activations from  multiple convolutional layers as below:  k(ψI ,ψĪ) = 〈φ({xl}Ll=N), φ({x̄l}Ll=N)〉  =  L∑  l=N  ηl〈φl(xl), φl(x̄l)〉, (NN)  where ηl is the weight for the matching scores in l-th layer
 The above kernel fusion can be re-interpreted as performing polynomial feature extraction at each layer and fusing  them in latter phase
Recently, hypercolumn [NN] suggests  a simple feature concatenation manner to combine different  feature maps in CNNs for pixel-level classification, which  motivates us to adopt the similar way in our polynomial kernel fusion
Thereby, we assume a holistic mapping φ for  all layers, i.e., ∑L  l=N  √ ηlφ  l(xl) → φ(concat(xN, 


,xL)) with weights  √ ηls be merged into element-wise scale layers
It should be noted that the spatial resolutions of different convolutional layers need to be consistent for concatenation operation
Alternatively, we can add pooling layers  or spatial resampling layers to meet this requirement
In this  sense, the expansion of φ by Eqn
(N) yields two groups of  variable interactions: ∏  kl xlkl that characterizes the interactions of parts in the l-th layer; and ∏  kl,kq xlklx  q kq  (where  l N= q) that captures additional information of multi-scale part relations from the l-th layer and q-th layer
 N.N
Integration architecture for deeper layers  Although the kernel fusion scheme enables polynomial  predictor for integrating hierarchical convolutional activations, it may not perform and scale well in case where large  numbers of layers involoved
We argue that only the convolutional activations from very deep layers refer to the responses of discriminative semantic parts
That is consistent  with the recent studies [NN, NN] which regard the convolutional filters in deeper layers as weak part detectors
In our  experiments, we demonstrate that the integration of the last  three convolutional activation layers (i.e., reluN N, reluN N,  and reluN N in VGG-NN model [NN]) is fairly effective to  obtain satisfactory performance
Even though more lower  layers could be involved, the effect is less obvious on the  improvement but higher computational complexity on both  training and testing phases
Fig
N presents our CNN architecture for integrating multiple convolutional layers
Compared with the B-CNN methods [NN, NN] focusing only on  the degree-N part statistics, our approach provides a general solution to model complex part interactions from hierarchical layers in differnt degrees and its superiority will be  demonstrated in experiments
 N
Experiments  In this section, we evaluate the effectiveness of our proposed integration framework on three fine-grained categorization datasets: Caltech-UCSD Bird-N00-N0NN (CUB)  [NN], Aircraft [NN] and Cars [NN]
The experimental comparisons with state-of-the-art methods indicate that effective feature integration from CNN is a promising solution  for FGVC in contrast with the requirements of massive exNNN    ternal data or detailed part annotation
 N.N
Datasets and Implementation Details  CUB dataset contains NN,NNN bird images
There are  altogether N00 bird species and the number of images per  class is about N0
The significant variations in pose, viewpoint and illumination inside each class make this dataset  very challenging
We adopt the publicly available split [NN],  which use nearly half of the dataset for training and the  other half for testing
 Aircraft dataset has N00 different aircraft model variants, giving N00 images for each model
The aircrafts appear at different scales, design structures and appearances
 We adopt the training/testing split protocol provideded by  [NN] to perform our experiments
 Cars dataset consists of NN,NNN images from NNN car  classes
Each class has about N0 images with different car  sizes and heavy clutter background
We use the same split  provided by [NN], divided with N,NNN images for training  and N,0NN images for testing
 Implementation details: our networks on all datasets  are fine-tuned on the VGG-NN model pretrained on  ILSVRC-N0NN dataset [NN] for fair comparison with most  state-of-the-art FGVC methods
The framework can be  also applied to the recently proposed network architectures  such as Inception [NN] and ResNet [NN]
We remove the  last three fully-connnected layers and construct a directed  acyclic graph (DAG) to combine all the components in our  framework
Before fed into softmax layer, we first pass  pooled polynomial features through ℓN normalization step
 We then use logistic regression to intialize the parameters  of classification layer, and adopt Rademacher vectors (i.e.,  each of its components are chosen independently using a  fair coin toss from the set {−N, N}) as good initializations [NN] of homogenous polynomial kernels for the N × N con- volutional filters
In training phase, following [NN], we  transform the input image by cropping the largest image region around its center, resizing it to NNN × NNN, and creat- ing its mirrored version to double the training set
During  fine-tuning, the learning rates of those pre-trained VGG-NN  layers and the newly added layers, including N × N con- volutional layers and classification layer, are initialized as  0.00N
We train all the networks using stochastic gradient  descent with a batch size of NN, momentum of 0.N
In testing phase, we follow the popular CNN-SVM scheme [NN],  i.e., use softmax loss in training and then perform evaluation on the extracted features by SVM
Our code is implemented on the open source MatConvNet framework with  a single NVIDIA GeForce GTX TITAN X GPU and can  be downloaded at http://wwwN.comp.polyu.edu
 hk/˜cslzhang/code/hihca.zip
 N.N
Analysis of the proposed framework  N.N.N Effect of number of N× N convolutional filters  To validate the effectiveness of introducing tensor decomposition in our polynomial predictor, we investigate  the effect of different Dr for the approximation of each  r-order tensor Wr
We first evaluate the classification accuracies on the CUB dataset on a single layer  reluN N using different homogeneous polynomial kernels  for solely modeling the degree-r variable interactions, i.e.,  xi, xixj , xixjxk, xixjxkxl
The number D r for degree-r  convolutional filters varies from NNN to NN,NNN
The results  are shown in Fig
N
As expected, increasing Dr leads  higher accuracies on all degrees
Interestingly, when Dr is  small, degree-N always leads a higher accuracy than those with higher degrees, which indicates that modeling higherorder part interactions often yields a tensor of dense parameters
It is observed that the performance gain is slight when  the number Dr increases from N,NNN to NN,NNN, which infers that a relative sparse tensor Wr can comprehensively  encode the distinguishing part interactions of fine-grained  objects from the very sparse activation features
Therefore,  we uniformly use N,NNN N× N convolutional filters in all the polynomial modules in consideration of feature dimension,  computational complexity as well as accuracy
 Figure N
Accuracies achieved by using polynomial kernels with  varied numbers of N× N convolutional filters on the CUB dataset
 N.N.N Effect of polynomial degree r  We further demonstrate the superiority of using higherorder part interactions both with and without finetuning on  the CUB dataset in Table N
We observe that the degree-N  polynomial kernel significantly outperforms the linear kernel
It implies that the co-occurrence statistics is very effective in capturing part relations, which is more informative in  distinguishing objects with homogeneous appearance than  the simple part occurrence statistics
The accuracy degrades  considerably as the degree r increases from N to N, which might be explained by the fact the low-degree interactions  with high counts are more reliable
As the reliable highdegree interactions are usually a few in number, the sum  NNN  http://wwwN.comp.polyu.edu.hk/~cslzhang/code/hihca.zip http://wwwN.comp.polyu.edu.hk/~cslzhang/code/hihca.zip   pooling will abate those scarce interactions in the pooled  polynomial representation, which weakens the discriminative ability of the final concatenated representation
Table
N  lists the frame-per-second (FPS) comparison in both training and testing phases using different polynomial kernels
 Since there is high computational overhead involved in the  polynomial modules in the network, a large degree r will  significantly slow the speed
Therefore, we suggest to adopt  N as the practical degree in all the experiments in Section  N.N even though degree-N kernel can achieve slightly better  results on Aircraft and Cars datasets
 Table N
Accuracy comparison with different non-homogeneous  polynominal kernels
 r N N N N N N  non-ft NN.N NN.N NN.N NN.N NN.N NN.N  ft NN.N NN.N NN.N NN.0 NN.N NN.N  Table N
FPS with different non-homogeneous polynomial kernels
 r N N N N N  Training N.N N.N N.N N.N N.N  Testing NN.N NN.N NN.N NN.N N0.N  N.N.N Effect of feature integration  We then provide details of the results by using higherorder integration for hierarchical convolutional activations
 We focus on reluN N, reluN N and reluN N as they exhibit  good capacity in capturing semantic part information compared with lower layers
And we analyze the impact factors of layers, kernels, and finetuning on the CUB dataset
 The accuracies are obtained under five polynomial kernels including linear kernel, degree-N homogeneous kernel, degree-N non-homogeneous (single + pairwise interactions), degree-N homogeneous kernel and degree-N nonhomogeneous kernel (single + pairwise + triple interactions)
We consider the following baselines: reluN N uses  only reluN N activations
reluN N+reluN N, reluN N+reluN N  and reluN N+reluN N are integration baselines that use N layers
reluN N+reluN N+reluN N is the full integration of three  layers
The results in Table N demonstrate that the performance gain of our framework comes from three factors: (i)  higher-order integration, (ii) finetuning, (iii) multiple layers
 Notably, we observe the remarkable performance benefits  on the baseline reluN N+reluN N and the full model of three  layers by exploiting the degree-N and degree-N polynomial  kernels, which implies that the discriminative power can  be enhanced by the complementary capacities of hierarchical convolutional layers compared with the isolated reluN N  layer
As the baseline reluN N+reluN N already presents the  best performance, thus we set the feature integration as  reluN N+reluN N in all the experiments in Section N.N
 Table N
Accuracy comparison with different baselines
 rN N rN N+  rN N  rN N+  rN N  rN N+  rN N  rN N+  rN N+  rN N  degree-N  non-ft NN.N NN.N NN.N NN.N NN.0  ft NN.N N0.N NN.N NN.N N0.N  degree-N homogeneous  non-ft NN.N NN.N NN.N NN.N NN.N  ft NN.N NN.0 NN.N NN.0 NN.N  degree-N non-homogeneous  non-ft NN.N NN.N NN.N NN.N NN.N  ft NN.N NN.N NN.N NN.N NN.N  degree-N homogeneous  non-ft NN.N NN.N NN.0 N0.N NN.N  ft NN.N NN.N NN.N NN.N NN.N  degree-N non-homogeneous  non-ft NN.N NN.N NN.N NN.N NN.N  ft NN.N NN.N NN.N NN.N NN.N  We also compare our higher-order integration with hypercolumn [NN] and HED [NN] based feature integrations
 Since the original hypercolumn and HED are introduced  for pixel-wise classification, for fair comparison, we revise hypercolumn as the feature concatenation of reluN N,  reluN N and reluN N, following by max pooling (denoted  as Hypercolumn∗); and revise HED by training classifiers  for the pooled activation features at each layer and then  fuse the predictions (denoted as HED∗)
Table N shows  that our integration framework is significantly superior to  Hypercolumn∗ and HED∗
This is not surprising since  Hypercolumn∗ and HED∗ can be treated as degree-N integration to some extent
 Table N
Accuracy comparison with different feature integrations
 Degree-N integration Hypercolumn∗ HED∗  NN.N N0.N NN.N  N.N
Comparison with state-of-the-art methods  N.N.N Results on the CUB dataset  We first compare our framework along with both the  annotation-based methods (i.e., using object bounding  boxes or part annotations) and annotation-free methods (i.e.,  only using image-level labels) on the CUB dataset
As  shown in Table N, unlike the state-of-the-art result obtained  from SPDA-CNN (NN.N%) [NN] which relys on the additional annotations of seven parts, we can still achieve a  comparable accuracy of NN.N% with only image-level labels  and significant improvements over PB R-CNN [NN] and FGWithout [N0]
Furthermore, our method is slightly inferior  to BoostCNN [N0] and outperforms all other annotationfree methods with a modest improvement (about N%) compared with STN [NN], B-CNN [NN] and PDFS [NN]
However, STN [NN] uses a better baseline CNN (Inception [NN])  than our VGG-NN network and PDFS [NN] cannot be trained  NNN    by end-to-end manner
B-CNN [NN] attempts to achieve the  feature complementary based on the outer product of convolutional activations from two networks (i.e., VGG-M and  VGG-NN)
However, our framework shows that the better  complementarity can be achieved by exploiting the natural  hierarchical structures of CNNs
BoostCNN uses BCNN as  the base CNN and adopts an ensemble learning method to  incorporate boosting weights
Thus, a fair comparison is to  use ours as the base CNN in BoostCNN
 Table N
Accuracies (%) on the CUB dataset
“bbox” and “parts”  refer to object bounding box and part annotations
 methods train anno
test anno
acc
 PB R-CNN [NN] bbox+parts n/a NN.N  FG-Without [N0] bbox bbox NN.0  SPDA-CNN [NN] bbox+parts bbox+parts NN.N  STN [NN] n/a n/a NN.N  B-CNN [NN] n/a n/a NN.N  PDFS [NN] n/a n/a NN.N  BoostCNN [N0] n/a n/a NN.N  Ours n/a n/a NN.N  N.N.N Results on the Aircraft and Cars datasets  The methods for the Aircraft and Cars datasets are all  annotation-free since there are no ground-truth part annotations on these two datasets
We first evaluate our framework on the Aircraft dataset, and the related results are  shown in the second column of Table N
Our network  achieves significantly better classification accuracy than the  state-of-the-art B-CNN which can be seemed as a specific  degree-N case in our framework
As we find that reluN N  instead of reluN N achieves the best performance in Aircraft  dataset, our improvement might be due to the reasons: (N)  B-CNN only focuses on reluN N where the the discriminative parts are highly out-numbered, thus these parts might be  overwhelmed by large non-discriminative region in pooling  stage; (N) the discriminative parts in this dataset may occur  simultaneously in both the coarse and fine scales
While  the rich representation by incorporating multiple layers in  our integration framework mitigates the local ambiguities  of single-layer representation to a large extent
 The third column of Table N provides the comparision  on the Cars dataset
B-CNN [NN] shows the similar accuray behavior with ours and both present a large margin over  Symbiotic [N] and FV-FGC [NN]
The accuracy of B-CNN  [NN] using two networks is very close to ours (NN.N% vs
 NN.N%), yet for the single network case, it still has the accuracy gap of N.N%, which infers that the hierarchical feature  integration on a single network can also contribute the feature complementary as done by two different networks
 Table N
Accuracies (%) on the Aircraft and Cars datasets
 methods acc
(Aircraft) acc
(Cars)  Symbiotic [N] NN.N NN.0  FV-FGC [NN] N0.N NN.N  B-CNN [NN] NN.N NN.N (N0.N)  Ours NN.N NN.N  N.N.N Visualization for the learned image patches  In Fig
N, we visualize some image patches with the highest  activations in the deeper layers of our fine-tuned networks  and the patches in each column come from different feature channels/maps
We obviously observe strong semanticrelated parts such as heads, legs and tails in CUB; cockpit,  tail stabilizers and engine in Aircraft; front bumpers, wheels  and lights in Cars
Such observations exactly reflect the nature of our approach which aims to improve the feature discrimination by the effective combinations of these parts
 CUB Aircraft Cars  Figure N
Visualization of the learned image patches in our finetuned networks on the CUB, Aircraft and Cars datasets
 N
Conclusion  It is preferred to perform FGVC under a more realistic  setting without part annotations and any prior knowledge  for explicit object appearance modeling
In this paper, by  considering the weak parts in CNN itself, we present a novel  higher-order integration framework of hierarchical convolutional layers to derive a rich representation for FGVC
 Based on the kernel mapping scheme, we propose a polynomial predictor to exploit the higher-order part relations and  presented the trainable polynomial modules which can be  plugged in conventional CNNs
Furthermore, the higherorder integration framework can be naturally extended to  mine the multi-scale part relations in hierarchical layers
 The results on the CUB, Aircraft and Cars datasets manifest  competitive performance, and demonstrate the effectiveness  of our integration framework
 NNN    References  [N] R
Arandjelović, P
Gronat, A
Torii, T
Pajdla, and J
Sivic
 Netvlad: Cnn architecture for weakly supervised place  recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, N0NN
 [N] A
Babenko and V
Lempitsky
Aggregating local deep features for image retrieval
In Proceedings of the IEEE International Conference on Computer Vision, pages NNNN–NNNN,  N0NN
 [N] S
Branson, G
Van Horn, S
Belongie, and P
Perona
Bird  species categorization using pose normalized deep convolutional nets
arXiv preprint arXiv:NN0N.NNNN, N0NN
 [N] S
Branson, C
Wah, F
Schroff, B
Babenko, P
Welinder,  P
Perona, and S
Belongie
Visual recognition with humans  in the loop
In European Conference on Computer Vision,  pages NNN–NNN
Springer, N0N0
 [N] J
Carreira, R
Caseiro, J
Batista, and C
Sminchisescu
 Semantic segmentation with second-order pooling
In European Conference on Computer Vision, pages NN0–NNN
 Springer, N0NN
 [N] Y
Chai, V
Lempitsky, and A
Zisserman
Symbiotic segmentation and part localization for fine-grained categorization
In Proceedings of the IEEE International Conference  on Computer Vision, pages NNN–NNN, N0NN
 [N] M
Cimpoi, S
Maji, and A
Vedaldi
Deep filter banks for  texture recognition and segmentation
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
 [N] D
Ciregan, U
Meier, and J
Schmidhuber
Multi-column  deep neural networks for image classification
In Computer  Vision and Pattern Recognition (CVPR), N0NN IEEE Conference on, pages NNNN–NNNN
IEEE, N0NN
 [N] M
Dixit, S
Chen, D
Gao, N
Rasiwasia, and N
Vasconcelos
Scene classification with semantic fisher vectors
In Proceedings of the IEEE Conference on Computer Vision and  Pattern Recognition, pages NNNN–NNNN, N0NN
 [N0] J
Feng, B
Ni, Q
Tian, and S
Yan
Geometric p-norm  feature pooling for image classification
In Computer Vision  and Pattern Recognition (CVPR), N0NN IEEE Conference on,  pages NN0N–NN0N
IEEE, N0NN
 [NN] Y
Gao, O
Beijbom, N
Zhang, and T
Darrell
Compact  bilinear pooling
In Proceedings of the IEEE Conference on  Computer Vision and Pattern Recognition, pages NNN–NNN,  N0NN
 [NN] R
Girshick, J
Donahue, T
Darrell, and J
Malik
Rich feature hierarchies for accurate object detection and semantic  segmentation
In Proceedings of the IEEE conference on  computer vision and pattern recognition, pages NN0–NNN,  N0NN
 [NN] Y
Gong, L
Wang, R
Guo, and S
Lazebnik
Multi-scale  orderless pooling of deep convolutional activation features
 In European Conference on Computer Vision, pages NNN–  N0N
Springer, N0NN
 [NN] P.-H
Gosselin, N
Murray, H
Jégou, and F
Perronnin
Revisiting the fisher vector for fine-grained classification
Pattern Recognition Letters, NN:NN–NN, N0NN
 [NN] B
Hariharan, P
Arbeláez, R
Girshick, and J
Malik
Hypercolumns for object segmentation and fine-grained localization
In Proceedings of the IEEE Conference on Computer  Vision and Pattern Recognition, pages NNN–NNN, N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
In Proceedings of the IEEE conference on computer vision and pattern recognition, pages  NN0–NNN, N0NN
 [NN] M
Jaderberg, K
Simonyan, A
Zisserman, et al
Spatial  transformer networks
In Advances in Neural Information  Processing Systems, pages N0NN–N0NN, N0NN
 [NN] P
Kar and H
Karnick
Random feature maps for dot product  kernels
In AISTATS, volume NN, pages NNN–NNN, N0NN
 [NN] T
G
Kolda and B
W
Bader
Tensor decompositions and  applications
SIAM review, NN(N):NNN–N00, N00N
 [N0] J
Krause, H
Jin, J
Yang, and L
Fei-Fei
Fine-grained  recognition without part annotations
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
 [NN] J
Krause, M
Stark, J
Deng, and L
Fei-Fei
Nd object representations for fine-grained categorization
In Proceedings  of the IEEE International Conference on Computer Vision  Workshops, pages NNN–NNN, N0NN
 [NN] Q
Le, T
Sarlós, and A
Smola
Fastfood-approximating kernel expansions in loglinear time
In Proceedings of the international conference on machine learning, N0NN
 [NN] C.-Y
Lee, S
Xie, P
Gallagher, Z
Zhang, and Z
Tu
Deeplysupervised nets
In AISTATS, volume N, page N, N0NN
 [NN] T.-Y
Lin, A
RoyChowdhury, and S
Maji
Bilinear cnn models for fine-grained visual recognition
In Proceedings of the  IEEE International Conference on Computer Vision, pages  NNNN–NNNN, N0NN
 [NN] L
Liu, C
Shen, and A
van den Hengel
The treasure beneath  convolutional layers: Cross-convolutional-layer pooling for  image classification
In Proceedings of the IEEE Conference  on Computer Vision and Pattern Recognition, pages NNNN–  NNNN, N0NN
 [NN] R
Livni, S
Shalev-Shwartz, and O
Shamir
On the computational efficiency of training neural networks
In Advances  in Neural Information Processing Systems, pages NNN–NNN,  N0NN
 [NN] J
Long, E
Shelhamer, and T
Darrell
Fully convolutional  networks for semantic segmentation
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNN0, N0NN
 [NN] J
Mairal, P
Koniusz, Z
Harchaoui, and C
Schmid
Convolutional kernel networks
In Advances in Neural Information  Processing Systems, pages NNNN–NNNN, N0NN
 [NN] S
Maji, E
Rahtu, J
Kannala, M
Blaschko, and A
Vedaldi
 Fine-grained visual classification of aircraft
arXiv preprint  arXiv:NN0N.NNNN, N0NN
 [N0] M
Moghimi, S
J
Belongie, M
J
Saberian, J
Yang, N
Vasconcelos, and L.-J
Li
Boosted convolutional neural networks
In BMVC, N0NN
 [NN] N
Murray and F
Perronnin
Generalized max pooling
In  Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNN0, N0NN
 NNN    [NN] N
Pham and R
Pagh
Fast and scalable polynomial kernels  via explicit feature maps
In Proceedings of the NNth ACM  SIGKDD international conference on Knowledge discovery  and data mining, pages NNN–NNN
ACM, N0NN
 [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  et al
Imagenet large scale visual recognition challenge
 International Journal of Computer Vision, NNN(N):NNN–NNN,  N0NN
 [NN] M
Simon and E
Rodner
Neural activation constellations:  Unsupervised part model discovery with convolutional networks
In Proceedings of the IEEE International Conference  on Computer Vision, pages NNNN–NNNN, N0NN
 [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In Advances  in Neural Information Processing Systems, pages NNN–NNN,  N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
 [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In Proceedings of the IEEE  Conference on Computer Vision and Pattern Recognition,  pages N–N, N0NN
 [NN] D
Tao, X
Li, W
Hu, S
Maybank, and X
Wu
Supervised  tensor learning
In Data Mining, Fifth IEEE International  Conference on, pages N–pp
IEEE, N00N
 [NN] C
Wah, S
Branson, P
Welinder, P
Perona, and S
Belongie
 The caltech-ucsd birds-N00-N0NN dataset
N0NN
 [N0] C
Wah, G
Van Horn, S
Branson, S
Maji, P
Perona, and  S
Belongie
Similarity comparisons for interactive finegrained categorization
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NNN–NNN, N0NN
 [NN] Y
Wang, J
Choi, V
Morariu, and L
S
Davis
Mining discriminative triplets of patches for fine-grained classification
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
 [NN] S
Xie and Z
Tu
Holistically-nested edge detection
In Proceedings of the IEEE International Conference on Computer  Vision, pages NNNN–NN0N, N0NN
 [NN] Z
Xu, Y
Yang, and A
G
Hauptmann
A discriminative  cnn video representation for event detection
In Proceedings of the IEEE Conference on Computer Vision and Pattern  Recognition, pages NNNN–NN0N, N0NN
 [NN] F
Yang, W
Choi, and Y
Lin
Exploit all the layers: Fast  and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
 [NN] Z
Yang, M
Moczulski, M
Denil, N
de Freitas, A
Smola,  L
Song, and Z
Wang
Deep fried convnets
In Proceedings  of the IEEE International Conference on Computer Vision,  pages NNNN–NNNN, N0NN
 [NN] H
Zhang, T
Xu, M
Elhoseiny, X
Huang, S
Zhang, A
Elgammal, and D
Metaxas
Spda-cnn: Unifying semantic  part detection and abstraction for fine-grained recognition
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
 [NN] N
Zhang, J
Donahue, R
Girshick, and T
Darrell
Partbased r-cnns for fine-grained category detection
In European Conference on Computer Vision, pages NNN–NNN
 Springer, N0NN
 [NN] X
Zhang, H
Xiong, W
Zhou, W
Lin, and Q
Tian
Picking deep filter responses for fine-grained image recognition
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
 NN0MIHash: Online Hashing With Mutual Information   MIHash: Online Hashing with Mutual Information  Fatih Cakir∗ Kun He∗ Sarah Adel Bargal Stan Sclaroff  Department of Computer Science  Boston University, Boston, MA  {fcakir,hekun,sbargal,sclaroff}@cs.bu.edu  Abstract  Learning-based hashing methods are widely used for  nearest neighbor retrieval, and recently, online hashing  methods have demonstrated good performance-complexity  trade-offs by learning hash functions from streaming data
 In this paper, we first address a key challenge for online  hashing: the binary codes for indexed data must be recomputed to keep pace with updates to the hash functions
 We propose an efficient quality measure for hash functions,  based on an information-theoretic quantity, mutual information, and use it successfully as a criterion to eliminate  unnecessary hash table updates
Next, we also show how to  optimize the mutual information objective using stochastic  gradient descent
We thus develop a novel hashing method,  MIHash, that can be used in both online and batch settings
 Experiments on image retrieval benchmarks (including a  N.NM image dataset) confirm the effectiveness of our formulation, both in reducing hash table recomputations and  in learning high-quality hash functions
 N
Introduction  Hashing is a widely used approach for practical nearest  neighbor search in many computer vision applications
It  has been observed that adaptive hashing methods that learn  to hash from data generally outperform data-independent  hashing methods such as Locality Sensitive Hashing [N]
In  this paper, we focus on a relatively new family of adaptive  hashing methods, namely online adaptive hashing methods  [N, N, N, NN]
These techniques employ online learning in the  presence of streaming data, and are appealing due to their  low computational complexity and their ability to adapt to  changes in the data distribution
 Despite recent progress, a key challenge has not been  addressed in online hashing, which motivates this work:  the computed binary representations, or the “hash table”,  may become outdated after a change in the hash mapping
 To reflect the updates in the hash mapping, the hash table  ∗First two authors contributed equally
 Figure N: We study online hashing for efficient nearest  neighbor retrieval
Given a hash mapping Φ, an image x̂, along with its neighbors in �x̂ and non-neighbors in �x̂,  are mapped to binary codes, yielding two distributions of  Hamming distances
In this example, ΦN has higher quality than ΦN since it induces more separable distributions
The information-theoretic quantity Mutual Information can be  used to capture the separability, which gives a good quality  indicator and learning objective for online hashing
 may need to be recomputed frequently, causing inefficiencies in the system such as successive disk I/O, especially  when dealing with large datasets
We thus identify an important question for online adaptive hashing systems: when  to update the hash table? Previous online hashing solutions  do not address this question, as they usually update both the  hash mapping and hash table concurrently
 We make the observation that achieving high quality  nearest neighbor search is an ultimate goal in hashing systems, and therefore any effort to limit computational complexity should preserve, if not improve, that quality
Therefore, another important question is: how to quantify quality? Here, we briefly describe our answer to this question,  but first introduce some necessary notation
We would like  to learn a hash mapping Φ from feature space X to the b- dimensional Hamming space Hb, whose outputs are b-bit  NNN    binary codes
The goal of hashing is to preserve a neighborhood structure in X after the mapping to Hb
Given x̂ ∈ X , the neighborhood structure is usually given in terms of a set  of its neighbors �x̂, and a set of non-neighbors �x̂
We  discuss how to derive the neighborhood structure in Sec
N
 As shown in Fig
N, the distributions of the Hamming  distances from x̂ to its neighbors and non-neighbors are histograms over {0, N, 


, b}
Ideally, if there is no overlap between these two distributions, we can recover �x̂ and �x̂ by simply thresholding the Hamming distance
A nonzero  overlap results in ambiguity, as observing the Hamming distance is no longer sufficient to determine neighbor relationships
Our discovery is that this overlap can be quantified using an information-theoretic quantity, mutual information, between two random variables induced by Φ
We then use mutual information to define a novel measure to  quantify quality for hash functions in general
 With a quality measure defined, we answer the motivating question of when to update the hash table
We propose  a simple solution by restricting updates to times when there  is an estimated improvement in hashing quality, based on  an efficient estimation method in the presence of streaming  data
Notably, since mutual information is a good generalpurpose quality measure for hashing, this results in a general plug-in module for online hashing that does not require  knowledge of the learning method
 Inspired by this strong result, we next ask, can we optimize mutual information as an objective to learn hash functions? We propose a novel hashing method, MIHash, by  deriving gradient descent rules on the mutual information  objective, which can be applied in online stochastic optimization, as well as on deep architectures
The mutual information objective is free of tuning parameters, unlike others that may require thresholds, margins, etc
 We conduct experiments on three image retrieval benchmarks, including the PlacesN0N dataset [NN] with N.NM images
For four recent online hashing methods, our mutual information based update criterion consistently leads to  over an order of magnitude reduction in hash table recomputations, while maintaining retrieval accuracy
Moreover,  our novel MIHash method achieves state-of-the-art retrieval  results, in both online and batch learning settings
 N
Related Work  In this section, we mainly review hashing methods that  adaptively update the hash mapping with incoming data,  given that our focus is on online adaptive hashing
For a  more general survey on hashing, please refer to [NN]
 Huang et al
[N] propose Online Kernel Hashing, where  a stochastic environment is considered with pairs of points  arriving sequentially
At each step, a number of hash functions are selected based on a Hamming loss measure and parameters are updated via stochastic gradient descent (SGD)
 Cakir and Sclaroff [N] argue that, in a stochastic setting,  it is difficult to determine which hash functions to update as  it is the collective effort of all the hash functions that yields  a good hash mapping
Hamming loss is considered to infer  the hash functions to be updated at each step and a squared  error loss is minimized via SGD
 In [N], binary Error Correcting Output Codes (ECOCs)  are employed in learning the hash functions
This work  follows a more general two-step hashing framework [NN],  where the set of ECOCs are generated beforehand and are  assigned to labeled data as they appear, allowing the label  space to grow with incoming data
Then, hash functions are  learned to fit the binary ECOCs using Online Boosting
 Inspired by the idea of “data sketching”, Leng et al
introduce Online Sketching Hashing [NN] where a small fixedsize sketch of the incoming data is maintained in an online  fashion
The sketch retains the Frobenius norm of the full  data matrix, which offers space savings, and allows to apply certain batch-based hashing methods
A PCA-based  batch learning method is applied on the sketch to obtain  hash functions
 None of the above online hashing methods offer a solution to decide whether or not the hash table shall be updated  given a new hash mapping
However, such a solution is  crucial in practice, as limiting the frequency of updates will  alleviate the computational burden of keeping the hash table up-to-date
Although [N] and [N] include strategies to  select individual hash functions to recompute, they still require computing on all indexed data instances
 Recently, some methods employ deep neural networks  to learn hash mappings, e.g
[NN, NN, NN, N0] and others
 These methods use minibatch-based stochastic optimization, however, they usually require multiple passes over a  given dataset to learn the hash mapping, and the hash table  is only computed when the hash mapping has been learned
 Therefore, current deep learning based hashing methods are  essentially batch learning methods, which differ from the  online hashing methods that we consider, i.e
methods that  process streaming data to learn and update the hash mappings on-the-fly while continuously updating the hash table
Nevertheless, when evaluating our mutual information  based hashing objective, we compare against state-of-theart batch hashing formulations as well, by contrasting different objective functions on the same model architecture
 Lastly, Ustinova et al
[NN] recently proposed a method  to derive differentiation rules for objective functions that require histogram binning, and apply it in learning deep embeddings
When optimizing our mutual information objective, we utilize their differentiable histogram binning technique for deriving gradient-based optimization rules
Note  that both our problem setup and objective function are quite  different from [NN]
 NNN    N
Online Hashing with Mutual Information  As mentioned in Sec
N, the goal of hashing is to learn a  hash mapping Φ : X → Hb such that a desired neighbor- hood structure is preserved
We consider an online learning  setup where Φ is continuously updated from input stream- ing data, and at time t, the current mapping Φt is learned from {xN, 


,xt}
We follow the standard setup of learn- ing Φ from pairs of instances with similar/dissimilar labels [N, N, N, NN]
These labels, along with the neighborhood  structure, can be derived from a metric, e.g
two instances  are labeled similar (i.e
neighbors of each other) if their Euclidean distance in X is below a threshold
Such a setting is often called unsupervised hashing
On the other hand, in  supervised hashing with labeled data, pair labels are derived  from individual class labels: instances are similar if they are  from the same class, and dissimilar otherwise
 Below, we first derive the mutual information quality  measure and discuss its use in determining when to update  the hash table in Sec
N.N
We then describe a gradient-based  approach for optimizing the same quality measure, as an objective for learning hash mappings, in Sec
N.N
Finally, we  discuss the benefits of using mutual information in Sec
N.N
 N.N
MI as Update Criterion  We revisit our motivating question: When to update the  hash table in online hashing? During the online learning of Φt, we assume a retrieval set S ⊆ X , which may include the streaming data after they are received
We  define the hash table as the set of hashed binary codes:  T (S,Φ) = {Φ(x)|x ∈ S}
Given the adaptive nature of online hashing, T may need to be recomputed often to keep pace with Φt; however, this is undesirable if S is large or the change in Φt’s quality does not justify the cost of an update
 We propose to view the learning of Φt and computa- tion of T as separate events, which may happen at different rates
To this end, we introduce the notion of a snapshot,  denoted Φs, which is occasionally taken of Φt and used to recompute T 
Importantly, this happens only when the nearest neighbor retrieval quality of Φt has improved, and we now define the quality measure
 Given hash mapping Φ : X → {−N,+N}b, Φ induces Hamming distance dΦ : X × X → {0, N, 


, b} as  dΦ(x, x̂) = N  N  (  b− Φ(x)⊤Φ(x̂) )  
(N)  Consider some instance x̂ ∈ X , and the sets contain- ing neighbors and non-neighbors, �x̂ and �x̂
Φ induces two conditional distributions, P (dΦ(x, x̂)|x ∈ �x̂) and P (dΦ(x, x̂)|x ∈ �x̂) as seen in Fig
N, and it is desir- able to have low overlap between them
To formulate the  idea, for Φ and x̂, define random variable Dx̂,Φ : X → {0, N, 


, b},x N→ dΦ(x, x̂), and let Cx̂ : X → {0, N} be  the membership indicator for �x̂
The two conditional distributions can now be expressed as P (Dx̂,Φ|Cx̂ = N) and P (Dx̂,Φ|Cx̂ = 0), and we can write the mutual information between Dx̂,Φ and Cx̂ as  I(Dx̂,Φ; Cx̂) = H(Cx̂)−H(Cx̂|Dx̂,Φ) (N)  = H(Dx̂,Φ)−H(Dx̂,Φ|Cx̂) (N)  where H denotes (conditional) entropy
In the following, for brevity we will drop subscripts Φ and x̂, and denote the two conditional distributions and the marginal P (Dx̂,Φ) as p+ D  , p− D  , and pD, respectively
By definition, I(D; C) measures the decrease in uncertainty in the neighborhood information C when observing the Hamming distances D
We claim that I(D; C) also cap- tures how well Φ preserves the neighborhood structure of x̂
If I(D; C) attains a high value, which means C can be de- termined with low uncertainty by observing D, then Φ must have achieved good separation (i.e
low overlap) between  p+ D  and p− D  
I is maximized when there is no overlap, and minimized when p+  D and p−  D are exactly identical
Recall,  however, that I is defined with respect to a single instance x̂; therefore, for a general quality measure, we integrate I over the feature space:  Q(Φ) =  ∫  X  I(Dx̂,Φ;Cx̂)p(x̂)dx̂
(N)  Q(Φ) captures the expected amount of separation between p+ D  and p− D  achieved by Φ, over all instances in X 
In the online setting, given the current hash mapping Φt  and previous snapshot Φs, it is then straightforward to pose the update criterion as  Q(Φt)−Q(Φ s) > θ, (N)  where θ is a threshold; a straightforward choice is θ = 0
However, Eq
N is generally difficult to evaluate due to the  intractable integral; in practice, we resort to Monte-Carlo  approximations to this integral, as we describe next
 Monte-Carlo Approximation by Reservoir Sampling  We give a Monte-Carlo approximation of Eq
N
Since we  work with streaming data, we employ the Reservoir Sampling algorithm [NN], which enables sampling from a stream  or sets of large/unknown cardinality
With reservoir sampling, we obtain a reservoir set R , {xrN, 


,x r K} from  the stream, which can be regarded as a finite sample from  p(x)
We estimate the value of Q on R as:  QR(Φ) = N  |R|  ∑  xr∈R  IR(Dxr,Φ; Cxr)
(N)  We use subscript R to indicate that when computing the mutual information I, the p+  D and p−  D for a reservoir instance xr are estimated from R
This can be done in O(|R|)  NNN    Sample  Reservoir   Hashing  Method  Trigger  Update     Streaming Data Hash  Table  Figure N: We present the general plug-in module for online hashing methods: Trigger Update (TU)
We sample a  reservoir R from the input stream, and estimate the mutual information criterion QR
Based on its value, TU decides whether a hash table update should be executed
 time for each xr, as the discrete distributions can be estimated via histogram binning
 Fig
N summarizes our approach
We use the reservoir  set to estimate the quality QR, and “trigger” an update to the hash table only when QR improves over a threshold
Notably, our approach provides a general plug-in module  for online hashing techniques, in that it only needs access  to streaming data and the hash mapping itself, independent  of the hashing method’s inner workings
 N.N
MI as Learning Objective  Having shown that mutual information is a suitable measure of neighborhood quality, we consider its use as a  learning objective for hashing
Following the notation in  Sec
N.N, we define a loss L with respect to x̂ ∈ X and Φ as  L(x̂,Φ) = −I(Dx̂,Φ; Cx̂)
(N)  We model Φ as a collection of parameterized hash func- tions, each responsible for generating a single bit: Φ(x) = [φN(x;W ), ..., φb(x;W )], where φi : X → {−N,+N}, ∀i, and W represents the model parameters
For example, lin- ear hash functions can be written as φi(x) = sgn(w  ⊤ i x),  and for deep neural networks the bits are generated by  thresholding the activations of the output layer
 Inspired by the online nature of the problem and recent  advances in stochastic optimization, we derive gradient descent rules for L
The entropy-based mutual information is differentiable with respect to the entries of pD, p  +  D and  p− D  , and, as mentioned before, these discrete distributions  can be estimated via histogram binning
However, it is not  clear how to differentiate histogram binning to generate gradients for model parameters
We describe a differentiable  histogram binning technique next
 Differentiable Histogram Binning  We borrow ideas from [NN] and estimate p+ D  , p− D  and pD us- ing a differentiable histogram binning technique
For b-bit Hamming distances, we use (K + N)-bin normalized his- tograms with bin centers v0 = 0, ..., vK = b and uniform bin width ∆ = b/K, where K = b by default
Consider,  for example, the k-th entry in p+ D  , denoted as p+ D,k
It can  be estimated as  p+ D,k =  N  | � |  ∑  x∈�  δx,k, (N)  where δx,k records the contribution of x to bin k
It is ob- tained by interpolating dΦ(x, x̂) using a triangular kernel:  δx,k =            (dΦ(x, x̂)− vk−N)/∆, dΦ(x, x̂) ∈ [vk−N, vk],  (vk+N − dΦ(x, x̂))/∆, dΦ(x, x̂) ∈ [vk, vk+N],  0, otherwise
(N)  This binning process admits subgradients:  ∂δx,k ∂dΦ(x, x̂)  =            N/∆, dΦ(x, x̂) ∈ [vk−N, vk],  −N/∆, dΦ(x, x̂) ∈ [vk, vk+N],  0, otherwise
 (N0)  Gradients of MI  We now derive the gradient of I with respect to the output of the hash mapping, Φ(x̂)
Using standard chain rule, we can first write  ∂I  ∂Φ(x̂) =  K ∑  k=0  [  ∂I  ∂p+ D,k  ∂p+ D,k  ∂Φ(x̂) +  ∂I  ∂p− D,k  ∂p− D,k  ∂Φ(x̂)  ]  
(NN)  We focus on terms involving p+ D,k, and omit derivations  for p− D,k due to symmetry
For k = 0, 


,K, we have  ∂I  ∂p+ D,k  = − ∂H(D|C)  ∂p+ D,k  + ∂H(D)  ∂p+ D,k  (NN)  = p+(log p+ D,k + N)− (log pD,k + N)  ∂pD,k  ∂p+ D,k  (NN)  = p+(log p+ D,k − log pD,k), (NN)  where we used the fact that pD,k = p +p+  D,k + p −p−  D,k, with  p+ and p− being shorthands for the priors P (C = N) and P (C = 0)
We next tackle the term ∂p+  D,k/∂Φ(x̂) in Eq
NN
 From the definition of p+ D,k in Eq.N, we have  ∂p+ D,k  ∂Φ(x̂) =  N  | � |  ∑  x∈�  ∂δx,k ∂Φ(x̂)  (NN)  = N  | � |  ∑  x∈�  ∂δx,k ∂dΦ(x, x̂)  ∂dΦ(x, x̂)  ∂Φ(x̂) (NN)  = N  | � |  ∑  x∈�  ∂δx,k ∂dΦ(x, x̂)  −Φ(x)  N 
(NN)  Note that ∂δx,k/∂dΦ(x, x̂) is already given in Eq
N0
For the last step, we used the definition of dΦ in Eq
N
 Lastly, to back-propagate gradients to Φ’s inputs and ul- timately model parameters, we approximate the discontinuous sign function with sigmoid, which is a standard technique in hashing, e.g
[N, NN, NN]
 NN0    0 0.0N 0.N 0.NN  Mutual Information  0.0N  0.N  0.NN  0.N  0.NN  0.N  0.NN  A v e  ra g  e  P  re c is  io n  MI vs AP  CIFAR (0.NN)  PLACES (0.N0)  LABELME (0.NN)  0 0.0N 0.N 0.NN  Mutual Information  0  N00  N00  N00  N00  D is  c o u n te  d  C  u m  u la  ti v e  G  a in  MI vs DCG  CIFAR (0.NN)  PLACES (0.NN)  LABELME (0.NN)  0 0.0N 0.N 0.NN  Mutual Information  0.N  0.NN  0.N  0.NN  0.N  0.NN  N o  rm a  liz e  d  D  C G  MI vs NDCG  CIFAR (0.NN)  PLACES (0.NN)  LABELME (0.NN)  Figure N: We show Pearson correlation coefficients between mutual information (MI) and AP, DCG, and NDCG, evaluated  on the CIFAR-N0, LabelMe, and PlacesN0N datasets
We sample N00 instances to form the query set, and use the rest to populate the hash table
The hash mapping parameters are randomly sampled from a Gaussian, similar to LSH [N]
Each  experiment is conducted N0 times
There exist strong correlations between MI and the standard metrics
 N.N
Benefits of MI  For monitoring the performance of hashing algorithms,  it appears that one could directly use standard ranking metrics, such as Average Precision (AP), Discounted Cumulative Gain (DCG), and Normalized DCG (NDCG) [NN]
 Here, we discuss the benefits of instead using mutual information
First, we note that there exist strong correlations  between mutual information and standard ranking metrics
 Fig
N demonstrates the Pearson correlation coefficients between MI and AP, DCG, and NDCG, on three benchmarks
 Although a theoretical analysis is beyond the scope of this  work, empirically we find that MI serves as an efficient and  general-purpose ranking surrogate
 We also point out the lower computational complexity of  mutual information
Let n be the reservoir set size
Com- puting Eq
N involves estimating discrete distributions via  histogram binning, and takes O(n) time for each reservoir item, since D only takes discrete values from {0, N, 


, b}, In contrast, ranking measures such as AP and NDCG have  O(n log n) complexity due to sorting, which render them disadvantageous
 Finally, Sec
N.N showed that the mutual information objective is suitable for direct, gradient-based optimization
In  contrast, optimizing metrics like AP and NDCG is much  more challenging as they are non-differentiable, and existing works usually resort to optimizing their surrogates  [NN, NN, NN] rather than gradient-based optimization
Furthermore, mutual information itself is essentially parameterfree, whereas many other hashing formulations require (and  can be sensitive to) tuning parameters, such as thresholds or  margins [NN, NN], quantization strength [NN, NN, N0], etc
 N
Experiments  We evaluate our approach on three widely used image  benchmarks
We first describe the datasets and experimental setup in Sec
N.N
We evaluate the mutual information update criterion in Sec
N.N and the mutual information based objective function for learning hash mappings  in Sec
N.N
Our implementation is publicly available at  https://github.com/fcakir/mihash
 N.N
Datasets and Experimental Setup  CIFAR-N0 is a widely-used dataset for image classification and retrieval, containing N0K images from N0 different  categories [N]
For feature representation, we use CNN features extracted from the fcN layer of a VGG-NN network [NN] pre-trained on ImageNet
 PlacesN0N is a subset of the large-scale Places dataset  [NN] for scene recognition
PlacesN0N contains N.NM images from N0N scene categories
This is a very challenging dataset due to its large size and number of categories,  and it has not been studied in the hashing literature to our  knowledge
We extract CNN features from the fcN layer of an AlexNet [N] pre-trained on ImageNet, and reduce the  dimensionality to NNN using PCA
 LabelMe
The NNK LabelMe dataset [NN, NN] has NN,0NN  images represented as NNN-dimensional GIST descriptors
 This is an unsupervised dataset without labels, and standard  practice uses the Euclidean distance to determine neighbor  relationships
Specifically, xi and xj are considered neighbor pairs if their Euclidean distance is within the smallest  N% in the training set
For a query, the closest N% examples  are considered true neighbors
 All datasets are randomly split into a retrieval set and a  test set, and a subset from the retrieval set is used for learning hash functions
Specifically, for CIFAR-N0, the test set  has NK images and the retrieval set has NNK
A random subset of N0K images from the retrieval set is used for learning,  and the size of the reservoir is set to NK
For PlacesN0N, we  sample N0 images from each class to construct a test set of  N.NK images, and use the rest as the retrieval set
A random  subset of N00K images is used to for learning, and the reservoir size is NK
For LabelMe, the dataset is split into retrieval and test sets with N0K and NK samples, respectively
 Similar to CIFAR-N0, we use a reservoir of size NK
 NNN  https://github.com/fcakir/mihash   0 0.N N N.N N  Examples N0N  0.N0  0.NN  0.NN  0.NN  0.NN  0.N0 Reduction: NNX   OKH   OKH + TU   AdaptHash   AdaptHash + TU   SketchHash   SketchHash + TU   OSH   OSH + TU  0 0.N N N.N N  Examples N0N  0.NN  0.N0  0.NN  0.NN  0.NN  0.NN Reduction: N0X  0 0.N N N.N N  Examples N0N  0.NN  0.NN  0.NN  0.NN  0.NN  0.NN Reduction: NNX  0 0.N N N.N N  Examples N0N  0.N0  0.NN  0.NN  0.NN  0.NN  0.N0 Reduction: NNX  0 N N0  Examples N0N  0.NN  0.NN  0.NN  0.NN  0.NN  0.NN Reduction: NNX  0 N N0  Examples N0N  0.N0  0.NN  0.NN  0.NN  0.NN  0.NN Reduction: NNX  0 N N0  Examples N0N  0.N0  0.NN  0.NN  0.NN  0.NN  0.NN Reduction: NNX  0 N N0  Examples N0N  0.N0  0.NN  0.NN  0.NN  0.NN  0.NN Reduction: NNX  0 0.N N N.N N  Examples N0N  0.N0  0.N0  0.N0  0.N0 Reduction: NNX  0 0.N N N.N N  Examples N0N  0.N0  0.N0  0.N0  0.N0 Reduction: NNX  0 0.N N N.N N  Examples N0N  0.N0  0.N0  0.N0  0.N0 Reduction: NNX  C if a rN 0  L a b e lM  e P  la c e s N 0 N  Figure N: Retrieval mAP vs
number of processed training examples for four hashing methods on the three datasets, with  and without Trigger Update (TU)
We use default threshold θ = 0 for TU
Circles indicate hash table updates, and the ratio of reduction in the number of updates is shown in the titles
TU substantially reduces the number of updates while having  a stabilizing effect on the retrieval performance
Note: since OSH [N] assumes supervision in terms of class labels, it is not  applicable to the unsupervised LabelMe dataset
 For online hashing experiments, we run three randomized trials for each experiment and report averaged results
 To evaluate retrieval performances, we adopt the widelyused mean Average Precision (mAP)
Due to the large size  of PlacesN0N, mAP is very time-consuming to compute,  and we compute mAP on the top N000 retrieved examples  (mAP@N000), as done in [NN]
 N.N
Evaluation: Update Criterion  We evaluate our mutual information based update criterion, the Trigger Update module (TU)
We apply TU to all  existing online hashing methods known to us: Online Kernel Hashing (OKH) [N], Online Supervised Hashing (OSH)  [N], Adaptive Hashing (AdaptHash) [N] and Online Sketching Hashing (SketchHash) [NN]
We use publicly available  implementations of all methods
The hash code length is  fixed at NN bits
As our work is the first in addressing the hash table update criterion for online hashing, we compare to a dataagnostic baseline, which updates the hash table at a fixed  rate
The rate is controlled by a parameter U , referred to  as the “update interval”: after processing every U exam- ples, the baseline unconditionally triggers an update, while  TUmakes a decision using the mutual information criterion
 For each dataset, U is set such that the baseline updates N0N times in total
This ensures that the baseline is never too  outdated, but updates are still fairly infrequent: in all cases,  the smallest U is N00
 Results for the Trigger Update module
Fig
N depicts the retrieval mAP over time for all four online hashing methods considered, on three datasets, with and without  incorporating TU
We can clearly observe a significant reduction in the number of hash table updates, between one  and two orders of magnitude in all cases
For example, the  number of hash table updates is reduced by a factor of NN for the OKH method on LabelMe
 The quality-based update criterion is particularly important for hashing methods that may yield inferior hash  mappings due to noisy data and/or imperfect learning techniques
In other words, TU can be used to filter updates to  the hash mapping with negative or small improvement
This  has a stabilizing effect on the mAP curve, notably for OKH  NNN    0 0.N N N.N N  Examples N0 N  0.N  0.N  0.N  0.N  0.N  0.N  0.N  m A  P  Cifar-N0  MIHash + TU  OSH + TU  OKH + TU  AdaptHash + TU  SketchHash + TU  0 0.N N N.N N  Examples N0 N  0.NN  0.N  0.NN  0.N  0.NN  0.N  0.NN  m A  P  LabelMe  0 N.N N N.N N0  Examples N0 N  0.N  0.NN  0.N  0.NN  0.N  m A  P  @   N 0 0 0  PlacesN0N  Figure N: Online hashing performance comparison on three datasets, where all methods use the Trigger Update module (TU)  with default threshold θ = 0
MIHash clearly outperforms other competing methods
OSH, AdaptHash, and SketchHash perform very similarly on PlacesN0N, thus their curves overlap
 and AdaptHash
For OSH, which appears to stably improve  over time, TU nevertheless significantly reduces revisits to  the hash table while maintaining its performance
 All results in Fig
N are obtained using the default threshold parameter θ = 0, defined in Eq
N
We do not tune θ in order to show general applicability
We also discuss the impact of the reservoir set R
There is a trade-off regarding the size of R: a larger R leads to better approximation but increases the overhead
Nevertheless, we observed robust  and consistent results with |R| not exceeding N% of the size of the training stream
 N.N
Evaluation: Learning Objective  We evaluate the mutual information based hashing objective
We name our method MIHash, and train it using  stochastic gradient descent (SGD)
This allows it to be applied to both the online setting and batch setting in learning  hash functions
 During minibatch-based SGD, to compute the mutual information objective in Eq
N and its gradients, we need access to the sets �x̂, �x̂ for each considered x̂, in order to  estimate p+ D  and p− D  
For the online setting in Sec
N.N.N,  a standalone reservoir set R is assumed as in the previous experiment, and we partition R into {�x̂,�x̂} with respect to each incoming x̂
In this case, even a batch size of N can  be used
For the batch setting in Sec
N.N.N, {�x̂,�x̂} are defined within the same minibatch as x̂
 N.N.N Online Setting  We first consider an online setting that is the same as in  Sec
N.N
We compare against other online hashing methods: OKH, OSH, AdaptHash and SketchHash
All methods  are equipped with the TU module with the default threshold  θ = 0, which has been demonstrated to work well
Results for Online Setting
We first show the mAP  curve comparisons in Fig
N
For competing online hashing methods, the curves are the same as the ones with  TU in Fig
N, and we remove markers to avoid clutter
 MIHash clearly outperforms other online hashing methods  on all three datasets, and shows potential for further improvement with more training data
The combination of TU  and MIHash gives a complete online hashing system that  enjoys a superior learning objective with a plug-in update  criterion that improves efficiency
 We next give insights into the distribution-separating effect from optimizing mutual information
In Fig
N, we plot  the conditional distributions p+ D  and p− D  averaged on the  CIFAR-N0 test set, before and after learning MIHash with  the N0K training examples
Before learning, with a randomly initialized hash mapping, p+ D  and p− D  exhibit high  overlap
After learning, MIHash achieves good separation  between p+ D  and p− D  : the overlap reduces significantly, and  the mass of p+ D  is pushed towards 0
This separation is reflected in the large improvement in mAP (0.NN vs
0.NN)
 In contrast with the other methods, the mutual information formulation is parameter-free
For instance, there is no  threshold parameter that requires separating p+ D  and p− D  at  a certain distance value
Likewise, there is no margin parameter that dictates the amount of separation in absolute  terms
Such parameters usually need to be tuned to fit to  data, whereas the optimization of mutual information is automatically guided by the data itself
 N.N.N Batch Setting  To further demonstrate the potential of MIHash, we consider the batch learning setting, where the entire training  set is available at once
We compare against state-of-theart batch formulations, including: Supervised Hashing with  Kernels (SHK) [NN], Fast Supervised Hashing with Decision Trees (FastHash) [NN], Supervised Discrete Hashing  (SDH) [N0], Efficient Training of Very Deep Neural Networks (VDSH) [N0], Deep Supervised Hashing with Pairwise Labels (DPSH) [NN] and Deep Supervised Hashing  with Triplet Labels (DTSH) [NN]
These competing methods have shown to outperform earlier and other work such  as [N, N, NN, NN, N0, NN]
We focus on comparisons on the  CIFAR-N0 dataset, which is the canonical benchmark for  supervised hashing
Similar to [NN], we consider two experNNN    0 N NN NN NN  Initialization, mAP = 0.NN        p +  D  p −  D  0 N NN NN NN  Hamming distance  Learned MIHash, mAP = 0.NN        p +  D  p −  D  Figure N: We plot the distributions p+ D  and p− D  , averaged on the CIFAR-N0 test set, before and after learning  MIHash with N0K training examples
Optimizing the mutual information objective substantially reduces the overlap  between them, resulting in state-of-the-art mAP for the online setting, as shown in Fig
N
 imental settings, which we detail below
 Setting N: NK training examples are sampled for learning hash mappings, and NK examples are used as the test  set
All methods learn shallow models on top of fcN features from a VGG-NN network [NN] pretrained on ImageNet
For three gradient-based methods (DPSH, DTSH,  and MIHash), this means learning linear hash functions
 Note that VDSH uses customized architectures consisting  of only fully-connected layers, and it is unclear how to adapt  it to use standard architectures; we used its full model with  NN layers and N0NN nodes per layer
 Setting N: We use the full training set of size N0K and  test set of size N0K
We focus on comparing the end-to-end  performance of MIHash against two recent leading methods: DPSH and DTSH, using the same VGG-F network architecture [N] that they are trained on
 We use publicly available implementations for the compared methods, and exhaustively search parameter settings  for them
For MIHash, the minibatch size is set to N00 and  NN0 in Settings N and N, respectively
We use SGD with momentum, and decrease the learning rate when the training  loss saturates
See supplementary material for more details
 Results for Batch Setting
In Table N, we list batch  learning results for all methods
In Setting N, MIHash outperforms all competing methods in terms of mAP, in some  cases with only a single training epoch (e.g
against VDSH,  DPSH)
This suggests that mutual information is a more effective learning objective for hashing
MIHash learns a linear layer on the input features, while some other methods  Method  Code Length  NN NN NN NN  S et  ti n  g N  SHK 0.NNN 0.NNN 0.NNN 0.NNN  SDH 0.NNN 0.NNN 0.NNN 0.NNN  VDSH 0.NNN 0.NNN 0.NNN 0.NNN  DPSH 0.NN0 0.NNN 0.NNN 0.NNN  DTSH 0.NNN 0.NNN 0.NNN 0.N0N  FastHash 0.NNN 0.N00 0.NNN 0.NNN  MIHash N 0.NNN 0.NNN 0.NNN 0.N0N  MIHash 0.NNN 0.NN0 0.NNN 0.NNN  Method NN NN NN NN  S et  ti n  g N  DPSHN 0.NNN 0.NNN 0.NNN 0.N0N  DTSHN 0.NNN 0.NNN 0.NNN 0.NNN  DPSH 0.N0N 0.N0N 0.NNN 0.NNN  DTSH 0.NNN 0.NNN 0.NNN 0.NNN  MIHash 0.NNN 0.NNN 0.NNN 0.NNN  N Results after a single training epoch
N Results as reported in DPSH [NN] and DTSH [NN]
 Table N: Comparison against state-of-the-art hashing methods on CIFAR-N0
We report mean Average Precision  (mAP) on the test set, with best results in bold
See text  for the details of the two experimental settings
 can learn non-linear hash functions: for instance, the closest  competitor, FastHash, is a two-step hashing method based  on sophisticated binary code inference and boosted trees
 In Setting N, with end-to-end finetuning, MIHash significantly outperforms DPSH and DTSH, the two most competitive deep hashing methods, and sets the current stateof-the-art for CIFAR-N0
Again, note that MIHash has no  tuning parameters in its objective function
In contrast, both  DPSH and DTSH have parameters to control the quantization strength that need to be tuned
 N
Conclusion  We advance the state-of-the-art for online hashing in two  aspects
In order to resolve the issue of hash table updates  in online hashing, we define a quality measure using the  mutual information between variables induced by the hash  mapping
This measure is efficiently computable, correlates well with standard evaluation metrics, and leads to  consistent computational savings for existing online hashing methods while maintaining their retrieval accuracy
Inspired by these strong results, we further propose a hashing  method MIHash, by optimizing mutual information as an  objective with stochastic gradient descent
In both online  and batch settings, MIHash achieves superior performance  compared to state-of-the-art hashing techniques
 Acknowledgements  This research was supported in part by a BU IGNITION  award, US NSF grant N0NNNN0, and gifts from NVIDIA
 NNN    References  [N] F
Cakir and S
Sclaroff
Adaptive hashing for fast similarity search
In Proc
IEEE International Conf
on Computer  Vision (ICCV), N0NN
 [N] F
Cakir and S
Sclaroff
Online supervised hashing
In Proc
 IEEE International Conf
on Image Processing (ICIP), N0NN
 [N] K
Chatfield, K
Simonyan, A
Vedaldi, and A
Zisserman
 Return of the devil in the details: Delving deep into convolutional nets
In British Machine Vision Conference (BMVC),  N0NN
 [N] A
Gionis, P
Indyk, and R
Motwani
Similarity search in  high dimensions via hashing
In Proc
International Conf
 on Very Large Data Bases (VLDB), NNNN
 [N] Y
Gong and S
Lazebnik
Iterative quantization: A procrustean approach to learning binary codes
In Proc
IEEE  Conf
on Computer Vision and Pattern Recognition (CVPR),  N0NN
 [N] L.-K
Huang, Q
Y
Yang, and W.-S
Zheng
Online hashing
 In Proc
International Joint Conf
on Artificial Intelligence  (IJCAI), N0NN
 [N] A
Krizhevsky and G
Hinton
Learning multiple layers of  features from tiny images, N00N
 [N] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Proc
Advances in Neural Information Processing Systems  (NIPS), N0NN
 [N] B
Kulis and T
Darrell
Learning to hash with binary reconstructive embeddings
In Proc
Advances in Neural Information Processing Systems (NIPS), N00N
 [N0] H
Lai, Y
Pan, Y
Liu, and S
Yan
Simultaneous feature  learning and hash coding with deep neural networks
In  Proc
IEEE Conf
on Computer Vision and Pattern Recognition (CVPR), N0NN
 [NN] C
Leng, J
Wu, J
Cheng, X
Bai, and H
Lu
Online sketching hashing
In Proc
IEEE Conf
on Computer Vision and  Pattern Recognition (CVPR), N0NN
 [NN] W.-J
Li, S
Wang, and W.-C
Kang
Feature learning based  deep supervised hashing with pairwise labels
In Proc
International Joint Conf
on Artificial Intelligence (IJCAI), N0NN
 [NN] G
Lin, F
Liu, C
Shen, J
Wu, and H
T
Shen
Structured  learning of binary codes with column generation for optimizing ranking measures
International Journal of Computer  Vision (IJCV), pages N–NN, N0NN
 [NN] G
Lin, C
Shen, Q
Shi, A
van den Hengel, and D
Suter
 Fast supervised hashing with decision trees for highdimensional data
In Proc
IEEE Conf
on Computer Vision  and Pattern Recognition (CVPR), N0NN
 [NN] K
Lin, J
Lu, C.-S
Chen, and J
Zhou
Learning compact  binary descriptors with unsupervised deep neural networks
 In Proc
IEEE Conf
on Computer Vision and Pattern Recognition (CVPR), N0NN
 [NN] J
W
Liu, Wei and, R
Ji, Y.-G
Jiang, and S.-F
Chang
Supervised hashing with kernels
In Proc
IEEE Conf
on Computer Vision and Pattern Recognition (CVPR), N0NN
 [NN] C
D
Manning, P
Raghavan, and H
Schütze
Introduction  to information retrieval
N00N
 [NN] M
Norouzi and D
J
Fleet
Minimal loss hashing for compact binary codes
In Proc
International Conf
on Machine  Learning (ICML), N0NN
 [NN] B
C
Russell, A
Torralba, K
P
Murphy, and W
T
Freeman
 Labelme: a database and web-based tool for image annotation
International journal of computer vision, N00N
 [N0] F
Shen, C
S
Wei, L
Heng, and T
Shen
Supervised discrete hashing
In Proc
IEEE Conf
on Computer Vision and  Pattern Recognition (CVPR), N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
ICLR, N0NN
 [NN] A
Torralba, R
Fergus, and Y
Weiss
Small codes and large  image databases for recognition
In Proc
IEEE Conf
on  Computer Vision and Pattern Recognition (CVPR)
IEEE,  N00N
 [NN] E
Ustinova and V
Lempitsky
Learning deep embeddings  with histogram loss
In Proc
Advances in Neural Information Processing Systems (NIPS), pages NNN0–NNNN, N0NN
 [NN] J
S
Vitter
Random sampling with a reservoir
ACM  Transactions on Mathematical Software (TOMS), NN(N):NN–  NN, NNNN
 [NN] J
Wang, H
T
Shen, J
Song, and J
Ji
Hashing for similarity  search: A survey
CoRR
 [NN] Q
Wang, Z
Zhang, and L
Si
Ranking preserving hashing  for fast similarity search
In Proc
International Joint Conf
 on Artificial Intelligence (IJCAI), N0NN
 [NN] Y
Wang, Xiaofang Shi and K
M
Kitani
Deep supervised  hashing with triplet labels
In Proc
Asian Conf
on Computer  Vision (ACCV), N0NN
 [NN] R
Xia, Y
Pan, H
Lai, C
Liu, and S
Yan
Supervised hashing for image retrieval via image representation learning
In  Proc
AAAI Conf
on Artificial Intelligence (AAAI), volume N,  page N, N0NN
 [NN] Y
Yue, T
Finley, F
Radlinski, and T
Joachims
A support  vector method for optimizing average precision
In Proc
 ACM Conf
on Research & Development in Information Retrieval (SIGIR), pages NNN–NNN
ACM, N00N
 [N0] Z
Zhang, Y
Chen, and V
Saligrama
Efficient training of  very deep neural networks for supervised hashing
In Proc
 IEEE Conf
on Computer Vision and Pattern Recognition  (CVPR), N0NN
 [NN] F
Zhao, Y
Huang, L
Wang, and T
Tan
Deep semantic  ranking based hashing for multi-label image retrieval
In  Proc
IEEE Conf
on Computer Vision and Pattern Recognition (CVPR), N0NN
 [NN] B
Zhou, A
Lapedriza, J
Xiao, A
Torralba, and A
Oliva
 Learning deep features for scene recognition using places  database
In Proc
Advances in Neural Information Processing Systems (NIPS), N0NN
 NNNBenchmarking and Error Diagnosis in Multi-Instance Pose Estimation   Benchmarking and Error Diagnosis in Multi-Instance Pose Estimation  Matteo Ruggero Ronchi Pietro Perona  www.vision.caltech.edu/˜mronchi perona@caltech.edu  California Institute of Technology, Pasadena, CA, USA  Abstract  We propose a new method to analyze the impact of errors in algorithms for multi-instance pose estimation and a  principled benchmark that can be used to compare them
 We define and characterize three classes of errors - localization, scoring, and background - study how they are influenced by instance attributes and their impact on an algorithm’s performance
Our technique is applied to compare the two leading methods for human pose estimation on  the COCO Dataset, measure the sensitivity of pose estimation with respect to instance size, type and number of visible  keypoints, clutter due to multiple instances, and the relative  score of instances
The performance of algorithms, and the  types of error they make, are highly dependent on all these  variables, but mostly on the number of keypoints and the  clutter
The analysis and software tools we propose offer a  novel and insightful approach for understanding the behavior of pose estimation algorithms and an effective method  for measuring their strengths and weaknesses
 N
Introduction  Estimating the pose of a person from a single monocular frame is a challenging task due to many confounding factors such as perspective projection, the variability of lighting and clothing, self-occlusion, occlusion by  objects, and the simultaneous presence of multiple interacting people
Nevertheless, the performance of human  pose estimation algorithms has recently improved dramatically, thanks to the development of suitable deep architectures [N, NN, NN, NN, NN, NN, NN, NN, NN, NN] and the availability of well-annotated image datasets, such as MPII Human  Pose Dataset and COCO [N, NN]
There is broad consensus that performance is saturated on simpler single-person  datasets [NN, NN], and researchers’ focus is shifting towards  less constrained and more challenging datasets [N, NN, NN],  where images may contain multiple instances of people, and  a variable number of body parts (or keypoints) are visible
 However, evaluation is challenging: more complex datasets  make it harder to benchmark algorithms due to the many  sources of error that may affect performance, and existing  0 N 0  Recall  P re  c is  io n  N  Inversion: .NNN  .NNN  Localization Errors (Sec
N.N):  Jitter:  Swap:  Miss:  .NNN  .NNN  Original Detections:  @OKS .N: .NNN (AP)  Background Errors (Sec
N.N):  False Pos.:  False Neg.:  .NNN  N.00  Scoring Errors (Sec
N.N):  Opt
Score: .NNN  Figure N
Coarse to Fine Error Analysis
We study the errors  occurring in multi-instance pose estimation, and how they are affected by physical characteristics of the portrayed people
We  build upon currently adopted evaluation metrics and provide the  tools for a fine-grained description of performance, which allows  to quantify the impact of different types of error at a single glance
 The fine-grained Precision-Recall curves are obtained by fixing an  OKS threshold and evaluating the performance of an algorithm after progressively correcting its mistakes
 metrics, such as Average Precision (AP) or mean Percentage of Correct Parts (mPCP), hide the underlying causes of  error and are not sufficient for truly understanding the behaviour of algorithms
 Our goal is to propose a principled method for analyzing  pose algorithms’ performance
We make four contributions:  N
Taxonomization of the types of error that are typical of  the multi-instance pose estimation framework;  N
Sensitivity analysis of these errors with respect to measures of image complexity;  N
Side-by-side comparison of two leading human pose estimation algorithms highlighting key differences in behaviour  that are hidden in the average performance numbers;  N
Assessment of which types of datasets and benchmarks  may be most productive in guiding future research
 Our analysis extends beyond humans, to any object category where the location of parts is estimated along with  detections, and to situations where cluttered scenes may  contain multiple object instances
This is common in  fine-grained categorization [N], or animal behavior analysis [N0, NN], where part alignment is often crucial
 NNN  www.vision.caltech.edu/~mronchi   N
Related Work  N.N
Error Diagnosis  Object Detection: Hoiem et al
[NN] studied how a detailed error analysis is essential for the progress of recognition research, since standard benchmark metrics do not tell  us why certain methods outperform others and how could  they be improved
They determined that several modes of  failure are due to different types of error and highlighted the  main confounding factors for object detection algorithms
 While [NN] pointed out the value of discriminating between  different errors, it did not show how to do so in the context  of pose estimation, which is one of our contributions
 Pose Estimation: In their early work on pose regression,  Dollár et al
[NN] observed that unlike human annotators,  algorithms have a distribution of the normalized distances  between a part detection and its ground-truth that is typically bimodal, highlighting the presence of multiple error  modes
The MPII Human Pose Dataset [N] Single-Person  benchmark enables the evaluation of the performance of algorithms along a multitude of dimensions, such as NN pose  priors, NN viewpoints and N0 human activities
However,  none of the currently adopted benchmarks for Multi-Person  pose estimation [NN, NN, NN] carry out an extensive error  and performance analysis specific to this framework, and  mostly rely on the metrics from the Single-Person case
 No standards for performing or compactly summarizing detailed evaluations has yet been defined, and as a result only  a coarse comparison of algorithms can be carried out
 N.N
Evaluation Framework  We conduct our study on COCO [NN] for several reasons: (i) it is the largest collection of multi-instance person keypoint annotations; (ii) performance on it is far from  saturated and conclusions on such a large and non-iconic  dataset can generalize to easier datasets; (iii) adopting their  framework, with open source evaluation code, a multitude  of datasets built on top of it, and annual competitions, will  have the widest impact on the community
The framework  involves simultaneous person detection and keypoint estimation, and the evaluation mimics the one used for object detection, based on Average Precision and Recall (AP,  AR)
Given an image, a distance measure is used to match  algorithm detections, sorted by their confidence score, to  ground-truth annotations
For bounding-boxes and segmentations, the distance of a detection and annotation pair is  measured by their Intersection over Union
In the keypoint  estimation task, a new metric called Object Keypoint Similarity (OKS) is defined
The OKS between a detection θ̂(p)  and the annotation θ(p) of a person p, Eq
N, is the average  over the labeled parts in the ground-truth (vi = N, N), of the Keypoint Similarity between corresponding keypoint pairs,  Fig
N; unlabeled parts (vi = 0) do not affect the OKS [N]
 .N  K e y p o in t   S im il a ri ty .NN  .NN  N
 x  Figure N
Keypoint Similarity (ks)
The ks between two detections, eye (red) and wrist (green), and their corresponding groundtruth (blue)
The red concentric circles represent ks values of .N  and .NN in the image plane and their size varies by keypoint type,  see Sec.N.N
As a result, detections at the same distance from the  corresponding ground-truth can have different ks values
          ks(θ̂ (p) i , θ  (p) i ) = e  −  ||θ̂ (p) i  −θ (p) i  ||NN  NsNkN i  OKS(θ̂(p), θ(p)) = ∑  i ks(θ̂  (p) i  ,θ (p) i  )δ(vi>0)∑ i δ(vi>0)  (N)  The ks is computed by evaluating an un-normalized Gaussian function, centered on the ground-truth position of a  keypoint, at the location of the detection to evaluate
The  Gaussian’s standard deviation ki is specific to the keypoint  type and is scaled by the area of the instance s, measured  in pixels, so that the OKS is a perceptually meaningful and  easy to interpret similarity measure
For each keypoint type,  ki reflects the consistency of human observers clicking on  keypoints of type i and is computed from a set of N000 redundantly annotated images [N]
 To evaluate an algorithm’s performance, its detections  within each image are ordered by confidence score and assigned to the ground-truths that they have the highest OKS  with
As matches are determined, the pool of available annotations for lower scored detections is reduced
Once all  matches have been found, they are evaluated at a certain  OKS threshold (ranging from .N to .NN in [N]) and classified as True or False Positives (above or below threshold), and unmatched annotations are counted as False Negatives
Overall AP is computed as in the PASCAL VOC Challenge [NN], by sorting the detections across all the images by  confidence score and averaging precision over a predefined  set of N0N recall values
AR is defined as the maximum  recall given a fixed number of detections per image [N0]
 Finally, we will refer to cocoAP and cocoAR when AP and  AR are additionally averaged over all OKS threshold values  (.N:.0N:.NN), as done in the COCO framework [N]
 NN0    (Jitter) (Inversion) (Swap) (Miss)  (a) (b) (c) (d) (e) (f)  Figure N
Taxonomy of Keypoint Localization Errors
Keypoint localization errors, Sec
N.N, are classified based on the position of a  detection as, Jitter: in the proximity of the correct ground-truth location, but not within the human error margin - left hip in (a); Inversion:  in the proximity of the ground-truth location of the wrong body part - inverted skeleton in (b), right wrist in (c); Swap: in the proximity  of the ground-truth location of the body part of a wrong person - right wrist in (d), right elbow in (e); Miss: not in the proximity of any  ground-truth location - both ankles in (f)
While errors in (b,d) appear to be more excusable than those in (c,e) they have the same weight
 Color-coding: (ground-truth) - concentric red circles centered on each keypoint’s location connected by a green skeleton; (prediction) red/green dots for left/right body part predictions connected with colored skeleton, refer to the Appendix for an extended description
 Score = NN.NN  OKS = .NN  Score = N.NN  OKS = .NN  Figure N
Instance Scoring Error
The detection with highest  confidence score (Left) is associated to the closest ground-truth by  the evaluation algorithm described in Sec
N.N
However, its OKS  is lower than the OKS of another detection (Right)
This results in  a loss in performance at high OKS thresholds, details in Sec
N.N
 N.N
Algorithms  We conduct our analysis on the the top-two ranked algorithms [NN, NN] of the N0NN COCO Keypoints Challenge [N],  and observe the impact on performance of the design differences between a top-down and a bottom-up approach
 Top-down (instance to parts) methods first detect humans contained in an image, then try to estimate their pose  separately within each bounding box [NN, NN, NN, NN]
The  Grmi [NN] algorithm is a two step cascade
In the first stage,  a Faster-RCNN system [NN] using ResNet-Inception architecture [NN] combining inception layers [NN] with residual  connections [NN] is used to produce a bounding box around  each person instance
The second stage serves as a refinement where a ResNet with N0N layers [NN] is applied to the  image crop extracted around each detected person instance  in order to localize its keypoints
The authors adopt a combined classification and regression approach [NN, NN]: for  each spatial position, first a classification problem is solved  to determine whether it is in the vicinity of each of the keypoints of the human body, followed by a regression problem  Table N
N0NN COCO Keypoints Challenge Leaderboard [N]  Cmu Grmi DLNN RND Umichvl  cocoAP 0.N0N 0.NNN 0.NNN 0.NNN 0.NNN  to predict a local offset vector for a more precise estimate  of the exact location
The results of both stages are aggregated to produce highly localized activation maps for each  keypoint in the form of a voting process: each point in a  detected bounding box casts a vote with its estimate for the  position of every keypoint, and the vote is weighted by the  probability that it lays near the corresponding keypoint
 Bottom-up (parts to instance) methods first separately  detect all the parts of the human body from an image, then  try to group them into individual instances [N, NN, NN, NN]
 The Cmu [NN] algorithm estimates the pose for all the people in an image by solving body part detection and part  association jointly in one end-to-end trainable network, as  opposed to previous approaches that train these two tasks  separately [NN, NN] (typically part detection is followed by  graphical models for the association)
Confidence maps  with gaussian peaks in the predicted locations, are used to  represent the position of individual body parts in an image
 Part Affinity Fields (PAFs) are defined from the confidence  maps, as a set of ND vector fields that jointly encode the location and orientation of a particular limb at each position  in the image
The authors designed a two-branch VGG [NN]  based architecture, inspired from CPMs [NN], to iteratively  refine confidence maps and PAFs with global spatial contexts
The final step consists of a maximum weight bipartite  graph matching problem [NN, NN] to associate body parts  candidates and assemble them into full body poses for all  the people in the image
A greedy association algorithm  over a minimum spanning tree is used to group the predicted  parts into consistent instance detections
 NNN    N.N%  N.N%  N.N%  NN.N% N0.N%  NN.N%  NN.N%  N0.N% NN.N%  Legend:  C m  u G  rm i  MissGood  Jitter  Inversion  Swap Nose  Eyes  Ears  Shoulders  Elbows  Wrists  Hips  Knees Ankles  overall miss swap inversion jitter  NN.N%  NN.N%  N.N%  N.N%  N.N% N0.N%  N%  N.N%  N.N% N.N%  NN.N%  NN.N%  NN% NN.N%  NN.N%  NN%  N.N%  N.N% N.N%  NN.N%  0%  N.N%  0.N% NN.N%  N.N% NN.N%  NN.N%  NN.N%  NN.N%  N.N%  N.N%  N.N%  NN.N% NN%  NN.N%  NN.N% NN%  N.N%  N.N%  N.N%  NN.N% N0.N%  NN.N%  NN.N%  N0.N% NN%  NN.N%  0%  N.N%  0.N% NN.N%  N.N% NN.N%  NN.N%  N0%  N.N%  N.N%  N.N%  N%  NN%  NN.N%  NN.N%  NN.N% N0.N%  NN.N%  N.N%  N.N%  N.N% N.N%  NN.N%  NN.N%  NN.N% NN.N%  .N .N .N .N .N  Instance OKS improvement  @ O  K S   
N  N @  O K  S  
 N N  @ O  K S   
N  Cmu median  Grmi median  Overall AP improvement  @ O  K S   
N  N  Cmu  Grmi  N% N% N% N% N0% NN% NN%  Quartiles  Nst Nrd  (a) (b) (c) (d)  Figure N
Distribution and Impact of Localization Errors
(a) Outcome for the predicted keypoints: Good indicates correct localization
 (b) The breakdown of errors over body parts
(c) The algorithm’s detections OKS improvement obtained after separately correcting errors  of each type; evaluated over all the instances at OKS thresholds of .N, .NN and .NN; the dots show the median, and the bar limits show the first  and third quartile of the distribution
(d) The AP improvement obtained after correcting localization errors; evaluated at OKS thresholds of  .NN (bars) and .N (dots)
A larger improvement in (c) and (d) shows what errors are more impactful
See Sec
N.N for details
 N
Multi-Instance Pose Estimation Errors  We propose a taxonomy of errors specific to the multiinstance pose estimation framework: (i) Localization,  Fig
N, due to the poor localization of the keypoint predictions belonging to a detected instance; (ii) Scoring, Fig
N,  due to a sub-optimal confidence score assignment; (iii)  Background False Positives, detections without a groundtruth annotation match; (iv) False Negatives, missed detections
We assess the causes and impact on the behaviour  and performance of [NN, NN] for each error type
 N.N
Localization Errors  A localization FP occurs when the location of the  keypoints in a detection results in an OKS score with the  corresponding ground-truth match that is lower than the  evaluation threshold
They are typically due to the fact that  body parts are difficult to detect because of self occlusion  or occlusion by other objects
We define four types of  localization errors, visible in Fig
N, as a function of the  keypoint similarity ks(., .), Eq
N, between the keypoint i  of a detection θ̂ (p) i and j of the annotation θ  (p) j of a person p
 Jitter: small error around the correct keypoint location
 .N ≤ ks(θ̂ (p) i , θ  (p) i ) < .NN  The limits can be chosen based on the application of interest; in the COCO framework, .N is the smallest evaluation  threshold, and .NN is the threshold above which also human  annotators have a significant disagreement (around N0%) in estimating the correct position [N]
 Miss: large localization error, the detected keypoint is  not within the proximity of any body part
 ks(θ̂ (p) i , θ  (q) j ) < .N ∀q ∈ P and ∀j ∈ J  Inversion: confusion between semantically similar parts  belonging to the same instance
The detection is in the proximity of the true keypoint location of the wrong body part
 ks(θ̂ (p) i , θ  (p) i ) < .N  ∃j ∈ J | ks(θ̂ (p) i , θ  (p) j ) ≥ .N  In our study we only consider inversions between the left  and right parts of the body, however, the set of keypoints J can be arbitrarily defined to study any kind of inversion
 Swap: confusion between semantically similar parts of  different instances
The detection is within the proximity of  a body part belonging to a different person
 ks(θ̂ (p) i , θ  (p) i ) < .N  ∃j ∈ J and ∃q ∈ P | ks(θ̂ (p) i , θ  (q) j ) ≥ .N  Every keypoint detection having a keypoint similarity  with its ground-truth that exceeds .NN is considered good,  as it is within the error margin of human annotators
We  can see, Fig
N.(a), that about NN% of both algorithm’s de- tections are good, and while the percentage of jitter and  inversion errors is approximately equal, [NN] has twice as  many swaps, and [NN] has about N% more misses
Fig
N.(b) contains a breakdown of errors over keypoint type: faces  are easily detectable (smallest percentage of miss errors);  swap errors are focused on the upper-body, as interactions  typically involve some amount of upper-body occlusion;  the lower-body is prone to inversions, as people often selfocclude their legs, and there are less visual cues to distinguish left from right; finally jitter errors are predominant  on the hips
There are no major differences between the  two algorithms in the above trends, indicating that none of  the methods contains biases over keypoint type
After defining and identifying localization errors, we measure the improvement in performance resulting from their correction
 NNN    Overall AP Improvement  N%  N%  N%  N%  N0%  @OKS .NN @OKS .NN@OKS .N  Cmu  Grmi  NN0  N00  NN0  N000  NNN0  NN00  NN0  N00  NN0  N000  NNN0  NN00  .N .N .N .N.0N .0N .0N .0N  .N .N .N .NN N0 NN N0 NN  Cmu  Original Confidence Scores Optimal Confidence Scores  Cmu  Grmi Grmi N  u m  b e  r  o  f  D  e te  c ti  o n  s N  u m  b e  r  o  f  D  e te  c ti  o n  s  average at all   OKS values  Low OKS Detections Maximum OKS Detections  Histogram of Detection Scores  (a) (b) (c)  Figure N
Scoring Errors Analysis
(a) The AP improvement  obtained when using the optimal detection scores, as defined in  Sec
N.N
The histogram of detections’ (b) original and (c) optimal  confidence scores
We histogram separately the scores of detections achieving the maximum OKS with a given ground-truth instance (green) and the other detections achieving OKS of at least  .N (red)
High overlap of the histograms, as in (b), is caused by the  presence of many detections with high OKS and low score or vice  versa; a large separation, as in (c), is indication of a better score
 Localization errors are corrected by repositioning a keypoint prediction at a distance from the true keypoint location equivalent to a ks of .NN for jitter, .N for miss, and  at a distance from the true keypoint location equivalent to  the prediction’s distance from the wrong body part detected  in the case of inversion and swapN
Correcting localization errors results in an improvement of the OKS of every instance and the overall AP, as some detections become  True Positives (TP) because the increased OKS value exceeds the evaluation threshold
Fig
N.(c) shows the OKS  improvement obtainable by correcting errors of each type:  it is most important to correct miss errors, followed by inversions and swaps, while jitter errors, although occurring  most frequently, have a small impact on the OKS
We learn,  Fig
N.(d), that misses are the most costly error in terms of  AP (∼ NN%), followed by inversions (∼ N%), relative to their low frequency
We focus on the improvement at the .NN  OKS threshold, as it has almost perfect correlation with the  value of cocoAP (average of AP over all thresholds) [NN]
 Changing the evaluation threshold changes the impact of  errors (for instance by lowering it to .N more detections are  TP so there is less AP improvement from their correction),  but the same relative trends are verified, indicating that the  above observations reflect the behavior of the methods and  are not determined by the strictness of evaluation
 N.N
Scoring Errors  Assigning scores to instances is a typical task in object  detection, but a novel challenge for keypoint estimation
A  scoring error occurs when two detections θ̂ (p) N and θ̂  (p) N are  in the proximity of a ground-truth annotation θ(p) and the  one with the highest confidence has the lowest OKS: {  Score(θ̂ (p) N ) > Score(θ̂  (p) N )  OKS(θ̂ (p) N , θ  (p)) < OKS(θ̂ (p) N , θ  (p))  NThe Appendix contains examples showing how errors are corrected
 Table N
Improvements due to the optimal rescoring of detections
 Cmu [NN] Grmi [NN]  Imgs
w
detections NNNN0 NNNNN  Imgs
w
optimal detection order NNNN (NN.N%) NNNN (NN.N%)  Number of Scoring Errors N0N NN  Increase of Matches NN NNN  Matches with OKS Improvement NN0 NN0  This can happen in cluttered scenes when many people  and their detections are overlapping, or in the case of an isolated person for which multiple detections are fired, Fig
N
 Confidence scores affect evaluation, Sec
N.N, locally by determining the order in which detections get matched to the  annotations in an image, and globally, when detections are  sorted across the whole dataset to compute AP and AR
As a  result, it is important for the detection scores to be: (i) ‘OKS  monotonic increasing’, so that a higher score always results  in a higher OKS; (ii) calibrated, so that scores reflect as  much as possible the probability of being a TP
A score possessing such properties is optimal, as it achieves the highest  performance possible for the provided detections
It follows  that the optimal score for a given detection corresponds to  the maximum OKS value obtainable with any ground-truth  annotation: monotonicity and perfect calibration are both  guaranteed, as higher OKS detections would have higher  score, and the OKS is an exact predictor of the quality of  a detection
The optimal scores can be computed at evaluation time, by an oracle assigning to each detection a confidence corresponding to the maximum OKS score achievable with any ground-truth instance
To aid performance  in the case of strong occlusion, we apply Soft-Non-MaxSuppression [N], which decays the confidence scores of detections as a function of the amount of reciprocal overlap
 Using optimal scores yields about N% AP improvement, averaged at all the OKS evaluation thresholds, and up to  N0% at OKS .NN, Fig
N.(a), pointing to the importance of assigning low scores to unmatched detections
A careful  examination shows that the reason of the improvement is  two-fold, Tab
N: (i) there is an increase in the number of  matches between detections and ground-truth instances (reduction of FP and FN) and (ii) the existing matches obtain a higher OKS value
Both methods have a significant amount of overlap, Fig
N.(b), between the histogram  of original scores for the detections with the highest OKS  with a given ground-truth (green line) and all other detections with a lower OKS (red line)
This indicates the presence of many detections with high OKS and low score or  vice versa
Fig
N.(c) shows the effect of rescoring: optimal score distributions are bi-modal and present a large  separation, so confidence score is a better OKS predictor
 Although the AP improvement after rescoring is equivalent,  [NN] provides scores that are in the same order as the optimal ones for a higher percentage of images and makes less  errors, indicating that it is using a better scoring function
 NNN    N
of Keypoints  N 
 o  f  O  v e  rl a  p s  ∈ [N, N] ∈ [N, N0] ∈ [NN, NN] ∈ [NN, NN]  0  ∈ [N, N]  ≥ N  Figure N
Images from the COCO Dataset Benchmarks
We separate the ground-truth instances in the COCO dataset into twelve  benchmarks, based on number of visible keypoints and overlap between annotations; Fig
N0.(b) shows the size of each benchmark
 Grmi  Cmu  (a) Overall AP Improvement (c) False Negatives Heatmap(b) False Positives Histogram of Size  NN%  N0%  N00%  NN%  GrmiCmu  N%  N%  N%  N%  @OKS .N @OKS .NN @OKS .NN  N0%  False Negatives  False Positives Grmi  Cmu  N0  N0  N0  N0  N0  N0  N0  N0  N0  N0  N0  N0  N0  S M L XL XX  average at all   OKS values  average at all   OKS values  Figure N
Background Errors Analysis
(a) The AP improvement obtained after FN (top) and FP (bottom) errors are removed  from evaluation; horizontal lines show the average value for each  method
(b) The histogram of the area size of FP having a high  confidence score
(c) The heatmaps obtained by adding the resized  ground-truth COCO segmentation masks of all the FN
 N.N
Background False Positives and False Negatives  FP and FN respectively consist of an algorithm’s detections and the ground-truth annotations that remain unmatched after evaluation is performed
FP typically occur when objects resemble human features or when body  parts of nearby people are merged into a wrong detection
 Most of the FP errors could be resolved by performing better Non-Max-Suppression and scoring, since their impact  is greatly reduced when using optimal scores, i.e
Fig
N
 Small size and low number of visible keypoints are instead  the main cause of FN
In Fig
N.(a) we show the impact  of background errors on the AP at three OKS evaluation  thresholds: FN affect performance significantly more than  FP, on average about N0% versus only N%
For both meth- ods, the average number of people in images containing FP  and FN is about N and N, compared to the dataset’s aver- age of N, suggesting that cluttered scenes are more prone to having background errors
Interestingly, the location of  FN errors for the two methods differs greatly, Fig.N.(c): [NN]  predominantly misses annotations around the image border,  while [NN] misses those at the center of an image
Another  significant difference is in the quantity of FP detections  having a high confidence score (in the top-N0th percentile  of overall scores), Fig.N.(b): [NN] has more than twice the  number, mostly all with small pixel area size (< NNN)
 N
Sensitivity to Occlusion, Crowding and Size  One of the goals of this study is to understand how the  layout of people portrayed in images, such as the number  of visible keypoints (occlusion), the amount of overlap between instances (crowding) and size, affects the errors and  performance of algorithms
This section is focused on the  properties of the data, so we analyze only on method, [NN]
 The COCO Dataset contains mostly visible instances having little overlap: Fig
N0 shows that only N.N% of the anno- tations have more than two overlaps with an IoU ≥ .N, and NN.N% have N or more visible keypoints
Consequently, we divide the dataset into twelve benchmarks, Fig
N, and study  the performance and occurrence of errors in each sepatate  one
The PR curves obtained at the evaluation threshold of  .NN OKS, after sequentially correcting errors of each type  are shown in Fig
N.(a)
It appears that the performance of  methods listed in Tab
N is a result of the unbalanced data  NNN    0  ∈ [N ,N ]  ≥ N  ∈ [NN, NN]∈ [NN, NN] N
of Keypoints  ∈ [N, N]  0 NRecall 0 NRecall 0 NRecall 0 NRecall  N 
 o  f  O  v e  rl a  p s  0  N P  re c  is io  n  0  N  P re  c is  io n  0  N  P re  c is  io n  Swap: .NNN  .NNN  Localization Errors  Miss:  Inversion:  Jitter:  .NN0  .NNN  Original Detections  @OKS .NN: .NNN (AP)  Background Errors  False Neg.:  False Pos.:  N.00  .NN0  Scoring Errors  Opt
Score: .NN0  .0NN  .NNN  .0N0  .NNN  .0NN  N.00  .NNN  .NNN  .NNN  .NNN  .NNN  .NNN  .NNN  N.00  .NN0  .NN0  .N0N  .NNN  .0N0  .NNN  .0NN  N.00  .NNN  .NNN  .00N  .00N  .00N  .00N  .00N  N.00  .NNN  .NNN  .NNN  .NNN  .N0N  .NNN  .NNN  N.00  .NNN  .NN0  .0NN  .NNN  .0N0  .NNN  .0NN  N.00  .NNN  .NN0  .NNN  .NNN  .NNN  .NNN  .NNN  N.00  .NN0  .NN0  .NNN  .NNN  .NNN  .NNN  .NNN  N.00  .NN0  .NN0  .NNN  .NNN  .NN0  .NNN  .N0N  N.00  .NN0  .NN0  .NNN  .NN0  .NNN  .NNN  .NNN  N.00  .NN0  .NNN  .NNN  .NNN  .NNN  .NNN  .NNN  N.00  .NN0  .NN0  ∈ [N, N0] (a)  ∈ [N ,N ]  N
of Keypoints N  
o f O  v e  rla p  s  M is  s S  w a  p In  v e  rs io  n J it  te r  NN% NN% N% N%  NN% NN% N% N%  NN% NN% N0% N%  N% N% 0% 0%  NN% NN% N% N%  NN% NN% NN% N%  N% N% N% N%  N% N% N% N%  N% N% N% N%  NN% N0% NN% N%  NN% NN% NN% NN%  NN% NN% NN% NN%  0  ≥ N  ∈ [NN, NN]∈ [NN, NN]∈ [N, N] ∈ [N, N0](b)  Figure N
Performance and Error Sensitivity to Occlusion and Crowding
(a) The PR curves showing the performance of [NN] obtained  by progressively correcting errors of each type at the OKS evaluation threshold of .NN on the twelve Occlusion and Crowding Benchmarks  described in Sec
N; every legend contains the overall AP values
(b) The frequency of localization errors occurring on each benchmark set
 N
of Keypoints  N0NN NNNNN NNNNN NNNNN  NNNN NNNNN NNNNN NNNN  NNN NNN NN0 NNN  N 
 o  f  O  v e  rl a  p s  M XL XXL  Instance Size Distribution  N e  N N  e N  N e  N  NNNNN  NN00N  NNNNN  NNN0N  M:area ∈ [NN N, NNN] area ∈ [NNN, NNN]L:  area ∈ [NNN, NNNN]XL: area > NNN N  XX:  ∈ [N ,N ]  0  ≥ N  ∈ [NN, NN]∈ [NN, NN]∈ [N, N] ∈ [N, N0] (a) (b)  Figure N0
Benchmarks of the COCO Dataset
The number of  instances in each benchmark of the COCO training set based on  (a) the size of instances, or (b) the number of overlapping groundtruth annotations with IoU ≥ .N and visible keypoints, Fig
N
 distribution, and that current algorithms still vastly underperform humans in detecting people and computing their  pose, specifically when less than N0 keypoints are visible  and overlap is present
Localization errors degrade the performance across all benchmarks, but their impact alone does  not explain the shortcomings of current methods
Over N0% of the annotations are missed when the number of visible  keypoints is less than N (regardless of overlap), and background FP and scoring errors account for more than N0% of the loss in precision in the benchmarks with high overlap
In Fig
N.(b), we illustrate the frequency of each localization error
Miss and jitter errors are predominant when  there are few keypoints visible, respectively with high and  low overlap
Inversions are mostly uncorrelated with the  amount of overlap, and occur almost always in mostly visible instances
Conversly, swap errors depend strongly on  the amount of overlap, regardless of the number of visible  keypoints
Compared to the overall rates in Fig
N.(a-cmu)  we can see that inversion and jitter errors are less sensitive to instance overlap and number of keypoints
A similar  analysis can be done by separating COCO into four size  groups: medium, large, extra-large and extra-extra large,  Fig
N0.(a)
The performance at all OKS evaluation thresholds improves with size, but degrades when instances occupy such a large part of the image that spatial context is  lost, Fig
NN.(a)
AP is affected by size significantly less  than by the amount of overlap and number of visible keypoints
In Fig
NN.(b) we show the AP improvement obtainable by separately correcting each error type in all benchmarks
Errors impact performance less (they occur less often) on larger instances, except for scoring and FP
Finally,  while FN, miss and jitter errors are concentrated on medium  instances, all other errors are mostly insensitive to size
 N
Discussion and Recommendations  Multi-instance pose estimation is a challenging visual  task where diverse errors have complex causes
Our analysis defines three types of error - localization, scoring, background - and aims to discover and measure their causes,  rather than averaging them into a single performance metric
Furthermore, we explore how well a given dataset may  be used to probe methods’ performance through its statistics  of instances’ visibility, crowding and size
 NNN    M L XL XX M L XL XX M L XL XX  O v e  ra ll  A  P  @OKS .N @OKS .NN @OKS .NN  N0%  N0%  N0%  N0%  M L XL XX M L XL XX M L XL XX M L XL XX M L XL XX M L XL XX M L XL XX  N%  N0%  NN%  N0%  NN%  N0%  NN%  Jitter Inversion Swap Miss Score Bckgd
FP
 Bkgd
FN
 O v e  ra ll  A  P  I  m p  ro v e  m e  n t   @ O  K S   
N  N  S = .0NN   I = .0NN   S = .NNN   I = .0NN   S = .NNN   I = .0NN   S = .0NN   I = .0NN   S = .0NN   I = .00N   S = .0NN   I = .00N   S = .0NN   I = .0N0   S = .0NN   I = .0NN   S = .0NN   I = .0NN   S = .N0N   I = .N0N   overall M medium, area ∈ [NNN, NNN] area ∈ [NNN, NNN]L large, area ∈ [NNN, NNNN]XL extra-large, area > NNNNXX extra-extra large,  (a) (b)  Figure NN
Performance and Error Sensitivity to Size
(a) The overall AP obtained by evaluating [NN] at three OKS evaluation thresholds  on the four Size Benchmarks described in Sec
N
(b) The AP improvement at the OKS threshold of .NN obtained after separately correcting  each error type on the benchmarks
In both figures, the dashed red line indicates evaluation over all the instance sizes, Sensitivity (S) and  Impact (I) are respectively computed as the difference between the maximum and minimum, and the maximum and average, values
 The biggest problem for pose estimation is localization  errors, present in about NN% of the predicted keypoints in  state of the art methods, Fig
N.(a)
We identify four distinct  causes of localization errors, Miss, Swap, Inversion, and Jitter, and study their occurrence in different parts of the body,  Fig
N.(b)
The correction of such errors, in particular Miss,  can bring large improvements in the instance OKS and AP,  especially at higher evaluation thresholds, Fig
N.(c-d)
 Another important source of error is noise in the detection’s confidence scores
To minimize errors, the scores  should be (i) ‘OKS monotonic increasing’ and (ii) calibrated over the whole dataset, Sec
N.N
The optimal score  of a given detection corresponds to the maximum OKS  value obtainable with any annotation
Replacing a method’s  scores with the optimal scores yields an average AP improvement of N%, Fig
N.(a), due to the fact that groundtruth instances match detections that obtain higher OKS,  and the overall number of matches is increased, Tab
N
A  key property of good scoring functions is to separate as  much as possible the distribution of confidence scores for  detections obtaining high OKS versus low OKS, Fig
N.(c)
 Characteristics of the portrayed people, such as the  amount of overlap with other instances and the number  of visible keypoints, substantially affects performance
A  comparison between Fig
N.(a) and Tab
N, shows that average performance strongly depends on the properties of the  images, and that state of the art methods still vastly underperform humans when multiple people overlap and significant occlusion is present
Since COCO is not rich in such  challenging pictures, it remains to be seen whether poor performance is due to the low number of training instances,  Fig
N0.(b), and a new collection and annotation effort will  be needed to investigate this question
The size of instances  also affects the quality of the detections, Fig
NN.(a), but is  less relevant than occlusion or crowding
This conclusion  may be biased by the fact that small instances are not annotated in COCO and excluded from our analysis
 In this study we also observe that despite their design differences, [NN, NN] display similar error patterns
Nonetheless, [NN] is more sensitive to swap errors, as keypoint predictions from the entire image can be erroneously grouped  into the same instance, while [NN] is more prone to misses,  as it only predicts keypoint locations within the detected  bounding box
[NN] has more than twice the number of high  confidence FP errors, compared to [NN]
Finally, we observe  that FN are predominant around the image border for [NN],  where grouping keypoints into consistent instances can be  harder, and concentrated in the center for [NN], where there  is typically clutter and bounding boxes accuracy is reduced
 Improving Localization: ND reasoning along with the estimation of ND body parts [N0] can improve localization by  both incorporating constraints on the anatomical validity of  the body part predictions, and learning priors on where to  expect visually occluded parts
Two promising directions  for improvement are possible: (i) collecting ND annotations [N] for the humans in COCO and learning to directly  regress ND pose end-to-end [N0]; (ii) modeling the manifold  of human poses [N, N, NN] and learning how to jointly predict the ND pose of a person along with its ND skeleton [NN]
 Improving Scoring: Graphical models [NN] can be used to  learn a scoring function based on the relative position of  body part locations, improving upon [NN, NN] which only  use the confidence of the predicted keypoints
Another  promising approach is to use the validation set to learn a  regressor for estimating optimal scores (Sec
N.N) from the  confidence maps of the predicted keypoints and from the  sub-optimal detection scores generated by the algorithm
 Comparing scores of detections in the same image relatively  to each other will allow optimizing their order
 We release our codeN for future researchers to analyze the  strengths and weaknesses of their methods
 Nhttps://goo.gl/NEyDyN  NNN  https://goo.gl/NEyDyN   References  [N] COCO Keypoints Challenge, ECCV N0NN
http:  //image-net.org/challenges/ilsvrc+  cocoN0NN
October, N0NN
N, N  [N] COCO Keypoints Evaluation
http://mscoco.org/  dataset/#keypoints-eval
October, N0NN
N, N  [N] I
Akhter and M
J
Black
Pose-conditioned joint angle limits for Nd human pose reconstruction
In Proceedings of the  IEEE Conference on Computer Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N  [N] M
Andriluka, L
Pishchulin, P
Gehler, and B
Schiele
Nd  human pose estimation: New benchmark and state of the art  analysis
In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June N0NN
N, N  [N] N
Bodla, B
Singh, R
Chellappa, and L
S
Davis
Improving object detection with one line of code
arXiv preprint  arXiv:NN0N.0NN0N, N0NN
N  [N] F
Bogo, A
Kanazawa, C
Lassner, P
Gehler, J
Romero,  and M
J
Black
Keep it smpl: Automatic estimation of Nd  human pose and shape from a single image
In European  Conference on Computer Vision, pages NNN–NNN
Springer,  N0NN
N  [N] L
Bourdev and J
Malik
Poselets: Body part detectors  trained using Nd human pose annotations
In Computer Vision, N00N IEEE NNth International Conference on, pages  NNNN–NNNN
IEEE, N00N
N  [N] S
Branson, G
Van Horn, S
Belongie, and P
Perona
Bird  species categorization using pose normalized deep convolutional nets
arXiv preprint arXiv:NN0N.NNNN, N0NN
N  [N] A
Bulat and G
Tzimiropoulos
Human pose estimation via  convolutional part heatmap regression
In European Conference on Computer Vision, pages NNN–NNN
Springer, N0NN
N,  N  [N0] X
Burgos-Artizzu, P
Dollár, D
Lin, D
Anderson, and  P
Perona
Social behavior recognition in continuous videos
 In CVPR, N0NN
N  [NN] Z
Cao, T
Simon, S.-E
Wei, and Y
Sheikh
Realtime multiperson Nd pose estimation using part affinity fields
arXiv  preprint arXiv:NNNN.0N0N0, N0NN
N, N, N, N, N, N, N  [NN] X
Chen and A
L
Yuille
Articulated pose estimation by a  graphical model with image dependent pairwise relations
In  Advances in Neural Information Processing Systems, pages  NNNN–NNNN, N0NN
N  [NN] P
Dollár, P
Welinder, and P
Perona
Cascaded pose regression
In Computer Vision and Pattern Recognition (CVPR),  N0N0 IEEE Conference on, pages N0NN–N0NN
IEEE, N0N0
N  [NN] M
Eichner and V
Ferrari
We are family: Joint pose estimation of multiple persons
In European Conference on  Computer Vision, pages NNN–NNN
Springer, N0N0
N, N, N  [NN] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
International journal of computer vision, NN(N):N0N–  NNN, N0N0
N  [NN] E
Eyjolfsdottir, S
Branson, X
P
Burgos-Artizzu, E
D
 Hoopfer, J
Schor, D
J
Anderson, and P
Perona
Detecting social actions of fruit flies
In European Conference on  Computer Vision, pages NNN–NNN
Springer, N0NN
N  [NN] G
Gkioxari, B
Hariharan, R
Girshick, and J
Malik
Using k-poselets for detecting people and localizing their keypoints
In Proceedings of the IEEE Conference on Computer  Vision and Pattern Recognition, pages NNNN–NNNN, N0NN
N,  N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages  NN0–NNN, N0NN
N  [NN] D
Hoiem, Y
Chodpathumwan, and Q
Dai
Diagnosing error  in object detectors
In European conference on computer  vision, pages NN0–NNN
Springer, N0NN
N  [N0] J
Hosang, R
Benenson, P
Dollár, and B
Schiele
What  makes for effective detection proposals? IEEE transactions  on pattern analysis and machine intelligence, NN(N):NNN–  NN0, N0NN
N  [NN] J
Huang, V
Rathod, C
Sun, M
Zhu, A
Korattikara,  A
Fathi, I
Fischer, Z
Wojna, Y
Song, S
Guadarrama, et al
 Speed/accuracy trade-offs for modern convolutional object  detectors
arXiv preprint arXiv:NNNN.N00NN, N0NN
N  [NN] E
Insafutdinov, L
Pishchulin, B
Andres, M
Andriluka, and  B
Schiele
Deepercut: A deeper, stronger, and faster multiperson pose estimation model
In European Conference on  Computer Vision, pages NN–N0
Springer, N0NN
N  [NN] S
Johnson and M
Everingham
Clustered pose and nonlinear appearance models for human pose estimation
In Proceedings of the British Machine Vision Conference, N0N0
 doi:N0.NNNN/C.NN.NN
N  [NN] S
Johnson and M
Everingham
Learning effective human  pose estimation from inaccurate annotation
In Proceedings  of IEEE Conference on Computer Vision and Pattern Recognition, N0NN
N  [NN] D
Koller and N
Friedman
Probabilistic graphical models:  principles and techniques
MIT press, N00N
N  [NN] H
W
Kuhn
The hungarian method for the assignment problem
Naval research logistics quarterly, N(N-N):NN–NN, NNNN
 N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European Conference on Computer Vision, pages NN0–NNN
Springer, N0NN
N, N  [NN] A
Newell, K
Yang, and J
Deng
Stacked hourglass  networks for human pose estimation
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N  [NN] G
Papandreou, T
Zhu, N
Kanazawa, A
Toshev, J
Tompson, C
Bregler, and K
Murphy
Towards accurate  multi-person pose estimation in the wild
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
N, N, N, N, N, N  [N0] G
Pavlakos, X
Zhou, K
G
Derpanis, and K
Daniilidis
 Coarse-to-fine volumetric prediction for single-image Nd human pose
arXiv preprint arXiv:NNNN.0NNNN, N0NN
N  [NN] L
Pishchulin, E
Insafutdinov, S
Tang, B
Andres, M
Andriluka, P
V
Gehler, and B
Schiele
Deepcut: Joint subset partition and labeling for multi person pose estimation
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
N, N  NNN  http://image-net.org/challenges/ilsvrc+cocoN0NN http://image-net.org/challenges/ilsvrc+cocoN0NN http://image-net.org/challenges/ilsvrc+cocoN0NN http://mscoco.org/dataset/#keypoints-eval http://mscoco.org/dataset/#keypoints-eval   [NN] L
Pishchulin, A
Jain, M
Andriluka, T
Thormählen, and  B
Schiele
Articulated people detection and pose estimation: Reshaping the future
In Computer Vision and Pattern Recognition (CVPR), N0NN IEEE Conference on, pages  NNNN–NNNN
IEEE, N0NN
N, N  [NN] V
Ramakrishna, D
Munoz, M
Hebert, A
J
Bagnell, and  Y
Sheikh
Pose machines: Articulated pose estimation via  inference machines
In ECCV, N0NN
N, N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  Advances in neural information processing systems, pages  NN–NN, N0NN
N  [NN] M
R
Ronchi, J
S
Kim, and Y
Yue
A rotation invariant  latent factor model for moveme discovery from static poses
 arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N  [NN] C
Szegedy, S
Ioffe, V
Vanhoucke, and A
Alemi
InceptionvN, inception-resnet and the impact of residual connections  on learning
arXiv preprint arXiv:NN0N.0NNNN, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In Proceedings of the IEEE  Conference on Computer Vision and Pattern Recognition,  pages N–N, N0NN
N  [NN] C
Szegedy, S
Reed, D
Erhan, and D
Anguelov
 Scalable, high-quality object detection
arXiv preprint  arXiv:NNNN.NNNN, N0NN
N  [N0] C
J
Taylor
Reconstruction of articulated objects from point  correspondences in a single uncalibrated image
In Computer Vision and Pattern Recognition, N000
Proceedings
 IEEE Conference on, volume N, pages NNN–NNN
IEEE, N000
 N  [NN] D
Tome, C
Russell, and L
Agapito
Lifting from the deep:  Convolutional Nd pose estimation from a single image
arXiv  preprint arXiv:NN0N.00NNN, N0NN
N  [NN] S.-E
Wei, V
Ramakrishna, T
Kanade, and Y
Sheikh
Convolutional pose machines
arXiv preprint arXiv:NN0N.00NNN,  N0NN
N, N  [NN] D
B
West et al
Introduction to graph theory, volume N
 Prentice hall Upper Saddle River, N00N
N  [NN] Y
Yang and D
Ramanan
Articulated pose estimation with  flexible mixtures-of-parts
In Computer Vision and Pattern Recognition (CVPR), N0NN IEEE Conference on, pages  NNNN–NNNN
IEEE, N0NN
N, N  NNNMulti-Label Image Recognition by Recurrently Discovering Attentional Regions   Multi-label Image Recognition by Recurrently Discovering Attentional Regions  Zhouxia WangN,N Tianshui ChenN Guanbin LiN,N Ruijia XuN Liang LinN,N,N ∗  N Sun Yat-sen University, China N SenseTime Group Limited N Engineering Research Center for Advanced Computing Engineering Software of Ministry of Education, China  {wzhoux, chtiansh, xurjN}@mailN.sysu.edu.cn, liguanbin@mail.sysu.edu.cn, linliang@ieee.org  Abstract  This paper proposes a novel deep architecture to address  multi-label image recognition, a fundamental and practical  task towards general visual understanding
Current solutions for this task usually rely on an extra step of extracting hypothesis regions (i.e., region proposals), resulting  in redundant computation and sub-optimal performance
 In this work, we achieve the interpretable and contextualized multi-label image classification by developing a recurrent memorized-attention module
This module consists of  two alternately performed components: i) a spatial transformer layer to locate attentional regions from the convolutional feature maps in a region-proposal-free way and ii)  an LSTM (Long-Short Term Memory) sub-network to sequentially predict semantic labeling scores on the located  regions while capturing the global dependencies of these  regions
The LSTM also output the parameters for computing the spatial transformer
On large-scale benchmarks of  multi-label image classification (e.g., MS-COCO and PASCAL VOC 0N), our approach demonstrates superior performances over other existing state-of-the-arts in both accuracy and efficiency
 N
Introduction  Recognizing multiple labels of images is a fundamental yet practical problem in computer vision, as real-world  images always contain rich and diverse semantic information
Besides the challenges shared with single-label image classification (e.g., large intra-class variation caused by  viewpoint, scale, occlusion, illumination), multi-label image classification is much more difficult since accurately  predicting the presence of multiple object categories usu∗Zhouxia Wang and Tianshui Chen contribute equally to this work and  share first-authorship
Corresponding author is Liang Lin (Email: linliang@ieee.org)
This work was supported by the State Key Development  Program under Grant N0NNYFBN00N00N, Special Program for Applied Research on Super Computation of the NSFC-Guangdong Joint Fund (second  phase), and CCF-Tencent Open Research Fund (NO.AGRN0NN0NNN)
 multi-label prediction..
..
 Figure N
Multi-label image recognition with discovered attentional regions by our approach
These regions (highlighted by  different colors) corresponding to the semantic labels (visualized  below the images) are contextualized and discriminative in terms  of classification, although they may not preserve object boundaries  well
 ally needs understanding the image in depth (e.g., associating semantic labels with regions and capturing their dependencies)
 Recently, convolutional neural networks (CNNs) [NN,  NN] achieve great success in visual recognition/classification  tasks by learning powerful feature representations from  raw images, and they have been also applied to the problem of multi-label image classification by combining with  some object localization techniques [N0, NN]
The resulting common pipeline usually involves two steps
A batch  of hypothesis regions are first produced by either exploiting bottom-up image cues [NN] or casting extra detectors  [N], and these regions are assumed to contain all possible  foreground objects in the image
A classifier or neural network is then trained to predict the label score on these hypothesis regions, and these predictions are aggregated to  achieve the multi-label classification results
Despite acknowledged successes, these methods take the redundant  computational cost of extracting region proposals and usually over-simplify the contextual dependencies among foreground objects, leading to a sub-optimal performance in  complex scenarios
Recently, Wang et al
[NN] proposed to  jointly characterize the semantic label dependency and the  image-label relevance by combining recurrent neural networks (RNNs) with CNNs
However, their model disregards the explicit associations between semantic labels and  NNNN    image contents, and lacks fully exploiting the spatial context in images
In contrast to all these mentioned methods, we introduce an end-to-end trainable framework that  explicitly discovers attentional regions over image scales  corresponding to multiple semantic labels and captures the  contextual dependencies of these regions from a global perspective
No extra step of extracting hypothesis regions is  needed in our approach
Two examples generated by our  approach are illustrated in Figure N
 To search for meaningful and discriminative regions in  terms of multi-label classification, we propose a novel recurrent memorized-attention module, which is combined  with convolutional neural networks in our framework
 Specifically, this module consists of two components: i)  a spatial transformer layer to locate attentional regions on  the convolutional maps and ii) an LSTM (Long-Short Term  Memory) sub-network to sequentially predict the labeling  scores over the attentional regions and output the parameters of the spatial transformer layer
Notably, the global contextual dependencies among the attentional regions are naturally captured (i.e., memorized) together with the LSTM  sequential encoding
And the two components are alternately performed during the recurrent learning
In this way,  our approach enables to learn a contextualized and interpretable region-label relevance while improving the discriminability for multi-label classification
 The main contributions of this work are three-fold
 • We develop a proposal-free pipeline for multi-label image recognition, which is capable of automatically discovering  semantic-aware regions over image scales and simultaneously capturing their long-range contextual dependencies
 • We further propose three novel constraints on the spatial transformer, which help to learn more meaningful and interpretable regions, and in turn, facilitate multi-label classification
 • We conduct extensive experiments and evaluations on large-scale benchmarks such as PASCAL VOC [N] and Microsoft COCO [N0], and demonstrate the superiority of our  proposed model in both recognition accuracy and efficiency  over other leading multi-label image classification methods
 N
Related Works  The performance of image classification has recently  witnessed a rapid progress due to the establishment of largescale labeled datasets (i.e., PASCAL VOC [N], COCO [N0])  and the fast development of deep CNNs [NN, NN]
In recent  years, many researchers have attempted to adapt the deep  CNNs to multi-label image recognition problem and have  achieved great success
 N.N
Multi-label image recognition  Traditional multi-label image recognition methods apply the bag-of-words (BOW) model to solve this problem [N, N]
Although performing well on the simple benchmarks, these methods may fail in classifying images with  complex scenes since BOW based models depend largely  on the hand-crafted low-level features
In contrast, features learned by deep models have been confirmed to  be highly versatile and far more effective than the handcrafted features
Since this paper focuses on deep learning  based multi-label image recognition, we discuss the relevant  works in the following context
 Recently, there have been attempts to apply deep learning to multi-label image recognition task [NN, NN, N0, NN,  NN]
Razavian et al
[NN] applies off-the-shelf features extracted from deep network pretrained on ImageNet [NN] for  multi-label image classification
Gong et al
[N0] propose  to combine convolutional architectures with an approximate  top-k ranking objective function for annotating multi-label  images
Instead of extracting off-the-shelf deep features,  Chatfield et al
[N] fine tune the network with the target  multi-label datasets, which can learn task-specific features  and thus boost the classification performance
To better  consider the correlations between labels instead of treating  each label independently, traditional graphical models are  widely incorporated, such as Conditional Random Field [N],  Dependency Network [NN], and co-occurrence matrix [NN]
 Recently, Wang et al
[NN] utilize the RNNs to learn a joint  image-label embedding to characterize the semantic label  dependency as well as the image-label relevance
 All of the aforementioned methods consider extracting  the features of the whole image with no spatial information, which on one hand were unable to explicitly perceive  the corresponding image regions to the detected classification labels, and on the other hand, were extremely vulnerable to the complex background
To overcome this issue, some researchers propose to exploit object proposals  to only focus on the informative regions, which effectively  eliminate the influences of the non-object areas and thus  demonstrate significant improvement in multi-label image  recognition task [N0, NN]
More specifically, Wei et al
[NN]  propose a Hypotheses-CNN-Pooling framework to aggregate the label scores of each specific object hypotheses to  achieve the final multi-label predictions
Yang et al
[N0]  formulate the multi-label image recognition problem as a  multi-class multi-instance learning problem to incorporate  local information and enhance the discriminative ability of  the features by encoding the label view information
However, these object proposals based methods are generally not  efficient with the preprocessing step of object proposal generation being the bottleneck
Moreover, the training stage  is not perfect and can hardly be modeled as an end-to-end  scheme in both training and testing
In this paper, we proNNN    fI  f0  fN  fK  CNN  …  fI ST LSTM  c0 , h0  MN  M0 cinit , hinit  ST LSTM  MN c0 , h0  MN  sNcN , hN  fI  MN , c0 , h0  ST LSTM  MK  sK  cK-N , hK-N  fI  …  category-wise        max-pooling  label   distribution  Figure N
Overview of our proposed framework for multi-label image recognition
Our model iteratively locates the attentional regions  corresponding to semantic labels and predicts the score for the current region
 pose to incorporate a recurrent memorized-attention module in the neural network to simultaneously locate the attentional regions and predict the labels on various located  regions
Our proposed method does not resort to the extraction of object proposals and is thus very efficient and can be  trained in an end-to-end mode
 N.N
Visual attention model  Attention model has been recently applied to various  computer vision tasks, including image classification [NN,  N], saliency detection [NN], and image captioning [NN]
 Most of these works use the recurrent neural network for  sequential attentions, and optimized their models with reinforcement learning technique
Works [NN, N] formulate a  recurrent attention model and apply it to the digital classification tasks for which the images are low-resolution with  a clean background, using the small attention network
The  model is non-differential and addressed with reinforcement  learning to learn task-specific policies
Jaderberg et al
[NN]  propose a differential spatial transformer module which  could be used to extract attentional regions with any spatial transformation, including scaling, rotation, transition,  and cropping
Moreover, it could be easily integrated into  the neural network and optimized using the standard backpropagation algorithm without reinforcement learning
 N
Model  Figure N illustrates the architecture of the proposed  model
The input image I is first fed into a VGG-NN Con- vNet without additional object proposals
The network  first processes the whole image with several convolutional  (conv) and max pooling layers to produce the conv feature maps, denoted as fI 
Here, we use the conv feature  maps from the last conv layer (i.e., convN N)
The recurrent  memorized-attention module, comprising a spatial transformer (ST) [NN] and an LSTM network [NN] that work collaboratively in an iterative manner, predicts the label distributions directly from the input image features
Specifically,  in one iterative procedure, the ST locates an attentional region for the LSTM, and the LSTM predicts the scores regarding this region for multi-label classification and simultaneously updates the parameters of ST
Finally, the scores  from several attentional regions are fused to achieve the final label distribution
 N.N
ST for Attentional Region Localization  We briefly introduce the spatial transformer (ST) [NN]  for completeness before diving deep into the recurrent  memorized-attention module
ST is a sample-based differential module that spatially transforms its input maps to the  output maps with a given size which correspond to a subregion of the input maps
It is convenient to embed an ST  layer in the neural network and train it with the standard  back-propagation algorithm
In our model, the ST is incorporated in the recurrent memorized-attention module for the  localization of attentional regions
 Formally, the ST layer extracts features of an attentional  region, denoted as fk, from the feature maps fI of the whole  input image
The computational procedure is as follows
A  transformation matrix M is first estimated by a localization  network (explained later)
After that, the corresponding coordinate grid in fI is obtained, based on the coordinates of  fk
Then the sampled feature maps fk that correspond to  the attentional region are generated by bilinear interpolation
Fig
N shows an example of coordinate mapping
As  we aim to locate the attentional regions, we constrain the  transformation matrix M to involve only cropping, translation and scaling, expressed as  NNN    （b）（a）  U  V  M  U  V  M  Figure N
Illustration of coordinate grid mapping on (a) the feature  maps and (b) the corresponding input image
 M =  [  sx 0 tx 0 sy ty  ]  , (N)  where sx, sy , tx, ty are the scaling and translation param- eters
In our model, we apply a standard neural network to  estimate these parameters to facilitate an end-to-end learning scheme
 N.N
Recurrent Memorized-Attention Module  The core of our proposed model is the recurrent  memorized-attention module, which combines the recurrent computation process of an LSTM network and a spatial transformer
It iteratively searches the most discriminative regions, and predicts the scores of label distribution for  them
In this subsection, we introduce this module in detail
 In the k-th iteration, our model searches an attentional region, and extracts the corresponding features by applying  the following spatial transformer, expressed as  fk = st(fI ,Mk),Mk =  [  skx 0 t k x  0 sky t k y  ]  , (N)  where st(·) is the spatial transformation function, and Mk is the transformation matrix estimated in the previous round  by the localization network
We initialize the attentional  region with the whole image at the first iteration, i.e., the  initial transformation matrix is set to be  M0 =  [  N 0 0 0 N 0  ]  
(N)  Note that we apply the spatial transformer operation on the  feature maps fI instead of the input image to avoid repeating  the computational intensive convolutional processes
The  LSTM takes the sampled feature map fk as input to compute  the memory cell and hidden state
The computation process  can be expressed as  xk = relu(Wfxfk + bx)  ik = σ(Wxixk +Whihk−N + bi)  gk = σ(Wxgxk +Whghk−N + bg)  ok = σ(Wxoxk +Whohk−N + bo)  mk = tanh(Wxmxk +Whmhk−N + bm)  ck = gk ⊙ ck−N + ik ⊙mk  hk = ok ⊙ ck  (N)  where relu(·) is the rectified linear function, σ(·) is the sig- moid function, tanh(·) is the hyperbolic tangent function; hk−N and ck−N are the hidden state and memory cell of previous iteration; ik, gk, ok and mk are the outputs of the  input gate, forget gate, output gate, and input modulation  gate, respectively
These multiplicative gates can ensure the  robust training of LSTMs as they work well in exploding  and vanishing gradients [NN]
 The memory cell ck encodes the useful information of  previous (k − N) regions, and it is possible to benefit our task in the following two aspects
First, previous works  [NN, NN] have shown that different categories of objects  exhibit strong co-occurrence dependencies
Therefore, it  helps to recognize objects within the current attentional region aided by “remembering” information of previous ones
 Second, it is expected that our model can find out all relevant and useful regions for classification
Simultaneously  considering the information of previous regions is a feasible  approach that implicitly enhances the diversity and complementarity among the attentional regions
 Update rule of M
Given the hidden state hk, the classifier  and localization network can be expressed as  zk = relu(Whzhk + bz)  sk = Wzszk + bs, k N= 0  Mk+N = Wzmzk + bm  (N)  where sk is the predicted score distribution of the k-th re- gion, and Mk+N is the transformation matrix for the next  iteration
Note that at the first iteration (k = 0), we make no prediction of s and just estimate the matrix M because  no attentional region is obtained initially
 Category-wise max-pooling
The iterations are repeated for K + N times, resulting in K score vectors {sN, sN, 


, sK}, where sk = {s  N k, s  N k, 


, s  C k } denotes the  scores over C class labels
Following [NN], we employ the category-wise max-pooling to fuse the scores into the final  result s = {sN, sN, 


, sC}
It simply maximizes out the scores over regions for each category  sc = max(scN, s c N, 


, s  c K), c = N, N, 


, C
(N)  NNN    N
Learning  N.N
Loss for Classification  We employ the Euclidean loss as the objective function following [NN, N0]
Suppose there are N training samples, and each sample xi has its label vector yi = {yNi , y  N i , 


, y  C i }
y  c i (c = N, N, 


, C) is assigned as N if  the sample is annotated with the class label c, and 0 other- wise
The ground-truth probability vector of the i-th sample is defined as p̂i = yi/||yi||N
Given the predicted probabil- ity vector pi  pci = exp(sci )  ∑C  c′=N exp(s c′  i ) c = N, N, 


, C, (N)  and the classification loss function is expressed as  Lcls = N  N  N ∑  i=N  C ∑  c=N  (pci − p̂ c i )  N
(N)  N.N
Loss for Attentional Region Constraints  As discussed above, we obtain the final result by aggregating the scores of the attentional regions
Thus, we hope  that the attentional regions selected by our model contain  all of the objects in the input image
If one object is left out  unexpectedly, an inevitable error occurs because the LSTM  network has never seen this object during the prediction procedure
We experimentally found that the proposed model  can be trained with the defined classification loss, however,  has notable drawbacks:  • Redundancy
The ST layer usually picks up the same region that corresponds to the most salient objects
As  a result, it would be difficult to retrieve all of the objects appearing in the input image, since the set of attentional regions are redundant
 • Neglect of tiny objects
The ST layer tends to locate regions in a relatively large size and ignores the tiny  objects, which hampers the classification performance
 • Spatial flipping
The selected attentional region may be mirrored vertically or horizontally
 To address these issues, we further define a loss function  that consists of three constraints on the parameters of the  transformation matrix M
 Anchor constraint
It would be better if the attentional regions scatter over different semantic regions in the image
 For the first iteration, adding no constraint helps to find the  most discriminative region
After that, we push the other  (K − N) attentional regions away from the image center by  an anchor constraint
We draw a circle of radius √  N  N centered on the image center, and pick up the anchor points on  Figure N
Anchor selection for left: K=N and right: K=N
 the circle uniformly, as depicted in Figure N
We use K = N in the experiments, so four anchor points are generated at  (0.N, 0.N), (0.N,−0.N), (−0.N, 0.N), and (−0.N,−0.N), re- spectivelyN
The anchor constraint is formulated as  ℓA = N  N {(tkx − c  k x)  N + (tky − c k y)  N}, (N)  where (ckx, c k y) is the location of the k-th anchor point
 Scale constraint
This constraint attempts to push the scale  parameters in a certain range, so that the located attentional  region will not be too large in size
It can be formulated as  ℓS = ℓsx + ℓsy , (N0)  in which  ℓsx = (max(|sx| − α, 0)) N  ℓsy = (max(|sy| − α, 0)) N  (NN)  where α is a threshold value, and it is set as 0.N in our ex- periments
 Positive constraint
The last one also constrains the scale  parameters
Positive constraint prefers a transformation matrix with positive scale parameters, leading to attentional regions that are not be mirrored:  ℓP = max(0, β − sx) + max(0, β − sy), (NN)  where β is a threshold value, set as 0.N in our experiments
Finally, we combine the aforementioned three types of  constraints on the parameters of the transformation matrix  to define a loss of localization of attentional regions
It is  formulated as the weighted sum of the three components:  Lloc = ℓS + λNℓA + λNℓP , (NN)  where λN and λN are the weighted parameters, and they are set as 0.0N and 0.N, respectively
 Our model is jointly trained with the classification loss  and the localization loss, so the overall loss function can be  expressed as  L = Lcls + γLloc
(NN)  We set the balance parameter γ as 0.N since the classifica- tion task is dominated in our model
Optimization is performed using the recently proposed Adam algorithm [NN]  and standard back-propagation
 NThe range of coordinate is rescaled to [-N, N]  NNN    N
Experiments  N.N
Settings  Implementation details
We implemented our method on  the basis of Caffe [NN] for deep network training and testing
In the training stage, we employed a two-step training mechanism to initialize the convolutional neural network following [NN]
The CNN is first pre-trained on the  ImageNet, a large scale single label classification dataset,  and further fine-tuned on the target multi-label classification dataset
The learned parameters are used to initialize  the parameters of the corresponding layers in our proposed  model, while the parameters of other newly added layers  in our network are initialized with Xavier algorithm [N, NN]  rather than manual tuning
All the training images are first  resized to N × N , and randomly cropped with a size of (N − NN) × (N − NN)
The training samples are also augmented by horizontal flipping
In our experiments, we  trained two models with N = NNN and N = NN0, respec- tively
Both of the models are optimized using Adam with  a batch size of NN, momentum of 0.N and 0.NNN
The learning rate is set to 0.0000N initially and divided by N0 after N0  epochs
We trained the models for about NN epochs for each  scale, and selected the model with the lowest validation loss  as the best model for testing
 In the testing phase, we follow [NN] to perform ten-view  evaluation across different scales
Specifically, we first resized the input image to N × N (N = NNN, NN0), and ex- tracted five patches (i.e., the four corner patches and the  center patch) with a size of (N −NN)× (N −NN), as well as their horizontally flipped versions
Instead of repeatedly extracting features for each patch, the model feeds the N×N image to the VGG-NN ConvNet, and crops the features on  the convN N features maps accordingly to achieve the features of all patches
Then, for each patch, the model extracts  the features for each located attentional region, and eventually aggregates the features of this patch by max-pooling
 The image representation is obtained via averaging the features of all the patches
At last, we trained a one-vs-rest  SVM classifier for each category using the LIBLINEAR library [N]
We test our model on a single NVIDIA GeForce  GTX TITAN-X, and it takes about NN0ms for ten-view evaluation for scale NNN, and about N00 ms for scale NN0
It  reduces the execution time by more than an order of magnitude, compared with previous proposal-based methods, e.g.,  HCP [NN], which costs about N0s per image
 Evaluation metrics
We use the same evaluation metrics as  [NN]
For each image, we assign top k highest-ranked labels to the image, and compare with the ground-truth labels
We  compute the overall precision, recall, FN (OP, OR, OFN) and  per-class precision, recall, FN (CP, CR, CFN) in Eq
NN
 Following [NN, N], we also apply average precision (AP) for  each category, and the mean average precision (mAP) over  all categories as well
Generally, overall FN, per-class FN,  and mAP are relatively important
 OP =  ∑  i N c i  ∑  i N p i  OR =  ∑  i N c i  ∑  i N g i  OFN = N×OP ×OR  OP +OR  CP = N  C  ∑  i  N ci Npi  CR = N  C  ∑  i  N ci Ngi  CFN = N× CP × CR  CP + CR ,  (NN)  where C is the number of labels, N ci is the number of im- ages that are correctly predicted for the i-th label, Npi is the number of predicted images for the i-th label, Ngi is the number of ground truth images for the i-th label
 N.N
Comparison with State-of-the-art Methods  To validate the effectiveness of our model, we conduct  the experiments on two benchmarks, PASCAL VOC N00N  [N] and Microsoft COCO [N0]
VOC N00N is the most  widely used benchmark, and most works have reported the  results on this dataset
We compare the performance of  our proposed method against the following state-of-the-art  approaches: FeV+LV-N0-VD [N0], HCP [NN], RLSD [NN],  CNN-RNN [NN], VeryDeep [NN] and CNN-SVM [NN] on  the VOC N00N dataset
MS-COCO is released later than  VOC and more challenging
Recent works have also used  this benchmark for evaluation
We compare with CNNRNN [NN], RLSD [NN] and WARP [N0] on the COCO  dataset as well
 N.N.N Performance on the VOC N00N dataset  The PASCAL VOC N00N dataset contains N,NNN images  from N0 object categories, which is divided into train, val  and test sets
We train our model on the trainval set, and  evaluate the performance on the test set, following other  competitors
Table N presents the experimental results
The  previous best-performing methods are HCP and FeV+LV,  which achieve a mAP of N0.N% and N0.N%, respectively
 Both of them share a similar two-step pipeline: they first  extract the object proposals of the image, and then aggregate the features of them for multi-label classification
Different from them, our method is proposal-free since the attentional regions are selected by the ST layer that works  collaboratively with the LSTM network
In this way, the  interaction between attentional region localization and classification is well explored, leading to improvement in performance
Our proposed method achieves a mAP of NN.N%,  that outperforms previous state-of-the-art algorithms
Note  that our model learned with a single scale of NNN or NN0  also surpasses previous works
This better demonstrates the  effectiveness of the proposed method
 NNN    Methods aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP  CNN-SVM [NN] NN.N NN.0 NN.N NN.0 NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N  CNN-RNN [NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0  VeryDeep [NN] NN.N NN.0 NN.N NN.N NN.N N0.N NN.N NN.0 NN.N NN.N NN.N NN.0 NN.N NN.N NN.N N0.0 NN.N N0.N NN.N NN.0 NN.N  RLSD [NN] NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  HCP [NN] NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N N0.N N0.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N  FeV+LV [N0] NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.0 N0.N  Ours (NNN) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours (NN0) NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Ours NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
Comparison of AP and mAP in % of our model and state-of-the-art methods on the PASCAL VOC N00N dataset
The best results  and second best results are highlighted in red and blue, respectively
Best viewed in color
 Methods C-P C-R C-FN O-P O-R O-FN  WARP [N0] NN.N NN.N NN.N NN.N NN.N N0.N  CNN-RNN [NN] NN.0 NN.N N0.N NN.N NN.N NN.N  RLSD [NN] NN.N NN.N NN.0 N0.N NN.N NN.N  Ours (NNN) NN.N NN.N NN.N NN.N NN.N NN.N  Ours (NN0) NN.0 NN.N NN.N NN.N NN.N NN.N  Ours NN.N NN.N NN.N NN.0 NN.0 NN.0  Table N
Comparison of our model and state-of-the-art methods on  the MS-COCO dataset
The best results and second best results  are highlighted in red and blue, respectively
Best viewed in color
 N.N.N Performance on the MS-COCO dataset  The MS-COCO dataset is primarily built for object detection, and it is also widely used for multi-label recognition  recently
It comprises a training set of NN,0NN images, and  a validation set of N0,NNN images
The dataset covers N0  common object categories, with about N.N object labels per  image
The label number for each image also varies considerably, rendering MS-COCO even more challenging
As  the ground truth labels of the test set are not available, we  evaluate the performance of all the methods on the validation set instead
We follow [NN] to select the top k = N labels for each image, and filter out the labels with probabilities lower than a threshold 0.N, so the label number of  some images would be less than N
 We compare the overall precision, recall, FN, and perclass precision, recall, FN in Table N
Our model outperforms the existing methods by a sizable margin
Specifically, it achieves a per-class FN score of NN.N% and an overall FN score of NN.0, improving those of the previously best  method by N.N% and N.N%, respectively
Similar to the results on VOC, the model learned with a single scale also  beats the state-of-the-art approaches
 N.N
Ablation Study  In this subsection, we perform ablative studies to carefully analyze the contribution of the critical components of  our proposed model
 N.N.N Attentional regions v.s
object proposals  One of the main contributions of this work is that our model  is capable of discovering the discriminative regions, which  Figure N
Comparison of visualization of the attentional regions  (indicated by green boxes) located by our method, and the object  proposals (indicated by blue boxes) generated by EdgeBox
 facilitates the task of multi-label image classification compared with proposal-based methods
In this subsection, we  present a comparison to reveal the fact that attentional regions have significant advantages against object proposals
 Proposal-based methods are proved to be powerful for  objectness detection
However, satisfactory recall rates are  difficult to achieve until thousands of proposals are provided
In addition, it is extremely time-consuming to examine all of the provided proposals with a deep network
 As an example, although HCP selects some representative  proposals, it still needs N00 proposals to obtain desired performance
Besides, computing the object proposals also  introduces additional computational overhead
In contrast,  our model utilizes an efficient spatial transformation layer  to find out a small number of discriminative regions, making the model runs much faster
Here we also present the visualization results of attentional regions discovered by our  model, and those generated by EdgeBox [NN], a representative proposal method
For our method, K is set as N, so five attentional regions are found
For EdgeBox, we directly  use the codes provided by [NN] to extract the proposals, and  adopt non-maximum suppression (NMS) with a threshold  of 0.N on them based on their objectness scores to exclude  the seriously overlapped proposals
We also visualize the  top five ones for a fair comparison
Figure N shows that the  regions generated by our model better capture the discriminative regions (e.g., the head part of dogs), and most of them  concentrate on the area of semantic objects
For EdgeBox,  although its top-N proposals cover most objects in the given  image, most of them contain non-object areas that carry less  discriminative information for classification
 NN0    In order to clearly show the advantages of attentional region localization, we conduct experiments to compare the  classification performance when using attentional regions  or object proposals in the same framework
To this end, we  first remove the spatial transformer and replace the attentional region with the selected five object proposals, with  the other components left unchanged
Table N gives the results on the VOC N00N dataset
It is shown that attentional  regions lead to better performance
In fact, proposal-based  methods need hundreds of regions or even more proposals  to cover most objects
Our model also achieves better performance than those using hundreds of proposals, such as  HCP and FeV+LV, which use N00 and N00 proposals, respectively (see Table N)
 type mAP  object proposals NN.N  attentional regions N0.N  Table N
Comparison of the mAPs of our model using attentional  regions and object proposals, respectively, on the PASCAL VOC  N00N dataset
The results are all evaluated using single-crop at  scale of NNN×NNN  N.N.N Analysis of the attentional region constraints  We propose three types of novel constraints for attentional  region localization, facilitating the task of multi-label image  classification
To validate their contributions, we remove all  three constraints, and retrain the model on the VOC N00N  dataset
The results, depicted in Table N, show a significant  drop in mAP, well demonstrating the effectiveness of the  constraints as a whole
We further remove one of three constraints, and retrain the model to evaluate the effectiveness  of each constraint individually
The performance also declines when any constraint is excluded (see Table N)
Therefore, it suggests that all of the three constraints are of importance for our model, they work cooperatively to facilitate  the improvement of classification
We also conduct similar  experiments on the MS-COCO dataset
As Table N shown,  although MS-COCO is far different from VOC, similar results have been observed, again demonstrating their contributions on various scenes
 N.N.N Multi-scale multi-view evaluation  We assess the impact of fusion of multi-scale and multi-crop  at the test stage
Two scales (NNN× NNN and NN0× NN0) are used in our experiments
For each scale, we extract ten crop  features
Hence, we reported the performance of singlescale + single-crop, single-scale + multi-crop and multiscale + multi-crop, in Table N
The results show that aggregating information from multi-crop on a single-scale can  constraints mAP  null NN.N  S+P N0.N  A+S N0.N  A+P N0.N  A+S+P NN.N  Table N
Comparison of mAP of our model learned using different constraints of attentional region localization on the PASCAL  VOC N00N dataset
The results are all evaluated using multi-crop  at the scale of NNN×NNN
We abbreviate anchor, scale and positive constraints as A, S, P for simple illustration
 constraints C-FN O-FN mAP  null NN.N N0.N NN.N  A+S+P NN.N NN.N NN.N  Table N
Comparison of C-FN, O-FN and mAP of our model learned  with and without constraints of attentional region localization on  the MS-COCO dataset
The results are all evaluated using multicrop at the scale of NNN×NNN
We abbreviate anchor, scale and positive constraints as A, S, P for simple illustration
 VOC N00N MS-COCO  s=NNN + single-crop N0.N N0.N  s=NN0 + single-crop N0.N N0.N  s=NNN + ten-crop NN.N NN.N  s=NN0 + ten-crop NN.N NN.N  two scales + ten-crop NN.N NN.N  Table N
Comparison of mAP with multi-scale and multi-crop on  the PASCAL VOC N00N and MS-COCO datasets
 boost the performance, and fusing the results of both of two  scale shows a further improvement
 N
Conclusion  In this paper, we have introduced a recurrent memorizedattention module into the deep neural network architecture to solve the problem of multi-label image recognition
 Specifically, our proposed recurrent memorized-attention  module is composed of a spatial transformer layer for localizing attentional regions from the image and an LSTM  unit to predict the labeling score based on the feature of a  localized region and preserve the past information for the  located regions
Experimental results on large-scale benchmarks (e.g., PASCAL VOC, COCO) demonstrate that our  proposed deep model can significantly improve the state of  the art in both accuracy and efficiency
 References  [N] J
Ba, V
Mnih, and K
Kavukcuoglu
Multiple object recognition with visual attention
arXiv preprint arXiv:NNNN.NNNN,  N0NN
 NNN    [N] K
Chatfield, K
Simonyan, A
Vedaldi, and A
Zisserman
 Return of the devil in the details: Delving deep into convolutional nets
arXiv preprint arXiv:NN0N.NNNN, N0NN
 [N] Q
Chen, Z
Song, Y
Hua, Z
Huang, and S
Yan
Hierarchical matching with side information for image classification
 In Proceedings of the IEEE Conference on Computer Vision  and Pattern Recognition, pages NNNN–NNNN, N0NN
 [N] M.-M
Cheng, Z
Zhang, W.-Y
Lin, and P
Torr
Bing: Binarized normed gradients for objectness estimation at N00fps
 In Proceedings of the IEEE conference on computer vision  and pattern recognition, pages NNNN–NNNN, N0NN
 [N] J
Dong, W
Xia, Q
Chen, J
Feng, Z
Huang, and S
Yan
 Subcategory-aware object classification
In Proceedings  of the IEEE Conference on Computer Vision and Pattern  Recognition, pages NNN–NNN, N0NN
 [N] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
International journal of computer vision, NN(N):N0N–  NNN, N0N0
 [N] R.-E
Fan, K.-W
Chang, C.-J
Hsieh, X.-R
Wang, and C.J
Lin
Liblinear: A library for large linear classification
 Journal of machine learning research, N(Aug):NNNN–NNNN,  N00N
 [N] N
Ghamrawi and A
McCallum
Collective multi-label classification
In Proceedings of the NNth ACM international conference on Information and knowledge management, pages  NNN–N00
ACM, N00N
 [N] X
Glorot and Y
Bengio
Understanding the difficulty of  training deep feedforward neural networks
In Aistats, volume N, pages NNN–NNN, N0N0
 [N0] Y
Gong, Y
Jia, T
Leung, A
Toshev, and S
Ioffe
Deep  convolutional ranking for multilabel image annotation
arXiv  preprint arXiv:NNNN.NNNN, N0NN
 [NN] Y
Guo and S
Gu
Multi-label classification using conditional dependency networks
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning for image recognition
arXiv preprint arXiv:NNNN.0NNNN,  N0NN
 [NN] K
He, X
Zhang, S
Ren, and J
Sun
Delving deep into  rectifiers: Surpassing human-level performance on imagenet  classification
In Proceedings of the IEEE International Conference on Computer Vision, pages N0NN–N0NN, N0NN
 [NN] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural computation, N(N):NNNN–NNN0, NNNN
 [NN] M
Jaderberg, K
Simonyan, A
Zisserman, et al
Spatial  transformer networks
In Advances in Neural Information  Processing Systems, pages N0NN–N0NN, N0NN
 [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional  architecture for fast feature embedding
In Proceedings of  ACM international conference on Multimedia, pages NNN–  NNN, N0NN
 [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
arXiv preprint arXiv:NNNN.NNN0, N0NN
 [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  Advances in neural information processing systems, pages  N0NN–NN0N, N0NN
 [NN] J
Kuen, Z
Wang, and G
Wang
Recurrent attentional networks for saliency detection
arXiv preprint  arXiv:NN0N.0NNNN, N0NN
 [N0] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In European Conference on Computer Vision, pages NN0–NNN
Springer, N0NN
 [NN] V
Mnih, N
Heess, A
Graves, et al
Recurrent models of visual attention
In Advances in Neural Information Processing  Systems, pages NN0N–NNNN, N0NN
 [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  et al
Imagenet large scale visual recognition challenge
 International Journal of Computer Vision, NNN(N):NNN–NNN,  N0NN
 [NN] A
Sharif Razavian, H
Azizpour, J
Sullivan, and S
Carlsson
Cnn features off-the-shelf: an astounding baseline for  recognition
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages N0N–  NNN, N0NN
 [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
 [NN] J
R
Uijlings, K
E
Van De Sande, T
Gevers, and A
W
 Smeulders
Selective search for object recognition
International journal of computer vision, N0N(N):NNN–NNN, N0NN
 [NN] J
Wang, Y
Yang, J
Mao, Z
Huang, C
Huang, and W
Xu
 Cnn-rnn: A unified framework for multi-label image classification
arXiv preprint arXiv:NN0N.0NNNN, N0NN
 [NN] Y
Wei, W
Xia, M
Lin, J
Huang, B
Ni, J
Dong, Y
Zhao,  and S
Yan
Hcp: A flexible cnn framework for multi-label  image classification
IEEE transactions on pattern analysis  and machine intelligence, N0NN
 [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
Courville, R
Salakhutdinov, R
S
Zemel, and Y
Bengio
Show, attend and tell:  Neural image caption generation with visual attention
arXiv  preprint arXiv:NN0N.0N0NN, N0NN
 [NN] X
Xue, W
Zhang, J
Zhang, B
Wu, J
Fan, and Y
Lu
Correlative multi-label multi-instance image annotation
In N0NN  International Conference on Computer Vision, pages NNN–  NNN
IEEE, N0NN
 [N0] H
Yang, J
Tianyi Zhou, Y
Zhang, B.-B
Gao, J
Wu, and  J
Cai
Exploit bounding box annotations for multi-label object recognition
In Proceedings of the IEEE Conference on  Computer Vision and Pattern Recognition, pages NN0–NNN,  N0NN
 [NN] J
Zhang, Q
Wu, C
Shen, J
Zhang, and J
Lu
Multi-label  image classification with regional latent semantic dependencies
arXiv preprint arXiv:NNNN.0N0NN, N0NN
 [NN] C
L
Zitnick and P
Dollár
Edge boxes: Locating object  proposals from edges
In European Conference on Computer  Vision, pages NNN–N0N
Springer, N0NN
 NNNDualNet: Learn Complementary Features for Image Recognition   DualNet: Learn Complementary Features for Image Recognition  Saihui Hou, Xu Liu and Zilei Wang  Department of Automation, University of Science and Technology of China  {saihui, liuxuNN}@mail.ustc.edu.cn, zlwang@ustc.edu.cn  Abstract  In this work we propose a novel framework named DualNet aiming at learning more accurate representation for image recognition
Here two parallel neural networks are coordinated to learn complementary features and thus a wider  network is constructed
Specifically, we logically divide  an end-to-end deep convolutional neural network into two  functional parts, i.e., feature extractor and image classifier
 The extractors of two subnetworks are placed side by side,  which exactly form the feature extractor of DualNet
Then  the two-stream features are aggregated to the final classifier for overall classification, while two auxiliary classifiers  are appended behind the feature extractor of each subnetwork to make the separately learned features discriminative  alone
The complementary constraint is imposed by weighting the three classifiers, which is indeed the key of DualNet
 The corresponding training strategy is also proposed, consisting of iterative training and joint finetuning, to make the  two subnetworks cooperate well with each other
Finally,  DualNet based on the well-known CaffeNet, VGGNet, NIN  and ResNet are thoroughly investigated and experimentally evaluated on multiple datasets including CIFAR-N00, Stanford Dogs and UEC FOOD-N00
The results demonstrate that DualNet can really help learn more accurate image representation, and thus result in higher accuracy for  recognition
In particular, the performance on CIFAR-N00  is state-of-the-art compared to the recent works
 N
Introduction  Recent years have witnessed the bloom of deep convolutional neural network (DCNN), which has remarkably boosted the performance of various visual assignments [NN, NN, N]
The success of DCNN is largely attributed to its deep architecture and end-to-end learning approach, which can learn hierarchical representation of the  input
And the fundamental research on DCNN is to develop advanced networks and corresponding training algorithms, with the aims of extracting more discriminative features for recognition
 Fused  Features  Subnetwork N  Subnetwork N  Fused  Classifier  Auxiliary  Classifier N  Auxiliary  Classifier N  Figure N
Illustration of the proposed DualNet
The input images  are fed into two subnetworks, which are coordinated to learn complementary features
The two-stream features are then fused to  form a unified representation, and passed into the Fused Classifier  for overall classification
The auxiliary classifiers are appended to  keep the separately learned features discriminative
 There have been considerable interests in enhancing DCNN with greater capacity, in which the networks are generally designed to be deeper or wider
For example, He et  al
[NN] propose a NNN-layer ResNet, which is N× deep- er than VGGNet [NN] and achieves state-of-the-art performance in several ILSVRCN0NN tasks
On the other hand,  going wider has also been proven applicable [N0, NN, NN]
 For instance, the recent WRN [NN] decreases depth and increases width of ResNet and achieves comparable performance
In fact, according to neurobiology, human visual  system only activates several neurons of cortex for a certain input pattern, but the details of visual stimulus can be  perfectly perceived in the VN zone, especially for the area  of central fovea [N]
Such powerful capability implies that  there exist plenty of neurons in visual cortex to represent the details of input stimulus, although each of them just  has a simple response pattern
Inspired by such a mechanism, we particularly propose to construct a wider network  to learn richer features for image recognition, i.e., assigning more sibling nodes in each layer
However, developing  innovative networks is nontrivial, which needs expertise in  neuroscience and labor-intensive parameter tuning
 In this work, we present a framework named DualNet  to effectively learn more accurate representation for image  recognition, as illustrated in Figure N
The core idea of DualNet is to coordinate two parallel DCNNs to learn features  complementary to each other, and thus richer features can  be extracted from the raw images
Specifically, we consider  N0N    an end-to-end DCNN to be composed of two logical parts,  i.e., feature extractor and image classifier, although they are  integrated without explicit division
Then a network of double width is constructed by placing the feature extractors of  two subnetworks side by side, which exactly form the feature extractor of DualNet
Consequently, two streams of  features can be extracted for an input image, which are then  aggregated to form a unified representation to the final classifier for overall classification
Meanwhile, two auxiliary  classifiers are appended behind the feature extractor of each  subnetwork to make the separately learned features discriminative alone
And the complementary constraint is imposed  by weighting the three involved classifiers, which is indeed  the key of DualNet
Besides, the corresponding training  strategy is proposed to make two subnetworks cooperate  well, which consists of iterative training and joint finetuning
Compared to straightly doubling the layer widthN, our  method is practically feasible without introducing too much  memory cost, and is able to bring significant improvement  for image recognition
 Finally, we thoroughly investigate the proposed DualNet framework based on the well-known CaffeNet [NN],  VGGNet [NN], NIN [NN] and ResNet [NN], and experimentally evaluate its effectiveness on multiple datasets including CIFAR-N00 [NN], Stanford Dogs [NN] and UEC FOODN00 [NN]
The results show that DualNet performs well with  different DCNN architectures and datasets of diverse domains, and reports promising improvement compared to the  baselines
In particular, the performance achieved by DualNet on CIFAR-N00 is state-of-the-art, with less computation  cost introduced compared to the recent works [NN, NN]
To  the best of our knowledge, this work is the first to focus on  the cooperation of multiple DCNNs, with the same input  and only simple fusion method considered, such as SUM,  Max and Concat
 The following paper is organized as follows
In Section N, we review the related works on image recognition  and DCNN
Section N presents the details of DualNet, and  Section N provides the experimental evaluation, which is  further discussed in Section N
Finally, the whole work is  concluded in Section N
 N
Related Work  Image recognition is a basic issue of computer vision,  in which adopted features are critical in determining the  classification performance
While traditional methods are  usually based on hand-crafted features, such as SIFT [NN]  and HOG [N], more recent works [NN, NN, NN, NN] have resorted to the Deep Convolutional Neural Network (DCNN),  which is able to automatically learn discriminative features  NTo train a VGGNet with double neurons at each layer without decreasing the mini-batch size (NN) will exceed the memory limit of a Tesla KN0  GPU
 from millions of labelled images
A typical DCNN consists  of a number of convolution and pooling layers optionally  followed by fully connected layers [NN]
 The convolutional neural network had its earlier root  in [N, N0]
But the real milestone was not set until recent years by AlexNet [NN], whose massive improvement  shown on ILSVRCN0NN rekindled people’s interests in DCNN
Due to the availability to large training data and GPU accelerated computation, multiple efforts have been taken to enhance DCNN for greater capacity, e.g., increased  depth [NN, NN, NN], enlarged width [N0, NN, NN], smaller  stride in convolution or pooling [NN, NN], new nonlinear activations [NN, NN], novel layers [NN, NN], effective regulations [NN] and so on
These improvements have turned out  to be helpful first on generic classification and then applied  to other visual assignments
However, the design of innovative networks is of high complexity which needs expertise in neuroscience [NN], and the parameter tuning is laborintensive [NN, NN, NN]
Different from these previous works,  in DualNet, we do not redesign a certain component in DCNN and instead exploit the potentials of existing networks
 We assemble multiple DCNNs to learn complementary features and thus form a wider network to extract more accurate image representation for recognition
 More related works lie in [NN, NN]
Compared to the  multi-stream learning [NN] in which parallel streams have  different parameter numbers and receptive field sizes, the  subnetworks in DualNet have the same architectures
Besides, the motivation and optimization are totally different
 As for [NN], it specially deals with fine-grained categorization, while DualNet aims at generic classification
And our  work further differs from [NN] in design philosophy, network architecture and computation cost
Firstly, in [NN], the  output of two DCNNs are multiplied to assemble the information of each location, while DualNet is designed to make  each unit of high layers to describe its corresponding image  patch more accurately
Secondly, the bilinear pooling is taken as the fusion method in [NN], but in DualNet only simple  SUM is adopted
In fact, the bilinear model that performs  best in [NN] is eventually implemented with a single DCNN  because of weight sharing, however, DualNet holds two DCNNs which do not share weights and are complementary  to each other
At the same time, the auxiliary classifiers are  introduced in DualNet and the training process is quite different
Thirdly, the features after the bilinear pooling are of  high dimension, which can model the subtle difference of  fine-grained categories better but requires much more training complexity and memory cost
In contrast, DualNet is  rather computationally efficient
Furthermore, DualNet is  not a specific network but a fairly generic framework which  can generalize multiple DCNN architectures
Particularly,  the performance achieved by DualNet is state-of-the-art on  CIFAR-N00 with SUM as the fusion method
 N0N    N
Our Approach  The extraction of visual features is usually treated as the  most important design choice in computer vision tasks, including image recognition
Despite of the great improvement brought by DCNN, the top-N accuracy for image  recognition is still not satisfying enough for practical applications, e.g., NN.NN% top-N error on ImageNet validation  set with ResNet-NNN [NN]
Hence it is still vital and necessary to develop advanced models to learn more accurate  image representation
So far DCNN is considered to be the  most competitive approach for feature extraction, which is  able to abstract hierarchical features ranging from edges to  entire objects [NN]
In practice, DCNN is trained by optimizing the objective loss function, i.e., the training is driven by the errors generated at the highest layer according to  back propagation (BP)
Consequently, in the optimization  of single network, some distinctive details of the objects,  which are low-level but essential to discriminate the classes  of strong similarity, are likely to be dropped in the middle layers or overwhelmed by massive useless information,  since the loss signals received by shallow layers for parameter update have been filtered by multiple upper layers
In  other words, it is difficult for single network to learn the  whole details of input images
 To deal with this issue, in this work, we propose a novel DualNet framework consisting of two parallel networks
 The highlight of DualNet is to coordinate two networks to  learn complementary features from input images, i.e., one  network is able to learn details about the objects of interest  which are missing in the other, such that after fusion richer and more accurate image representation can be extracted  for recognition
Particularly, in the design of DualNet, we  follow the principles listed below:  PN The features after fusion are expected to be the most  discriminative compared to the features extracted by  each subnetwork, which exactly indicates the complementary learning embedded in DualNet
 PN The framework should be fairly generic to perform  well with most of typical DCNNs, e.g., VGGNet and  ResNet, and popular datasets, e.g., CIFAR-N00
 PN In terms of computation cost, the networks should be  efficient as much as possible for training and test, and  compatible for a Tesla KN0 GPU (NN GB memory limit) without decreasing the mini-batch size
 PN Only simple fusion methods, such as SUM, MAX and  Concat, are considered to ensure the generalization ability and computation efficiency, and the focus is the  cooperation and complementarity of two subnetworks
 From another perspective, DualNet can be also viewed  to provide an approach to construct a wider network
By  convN_SN+relu  poolN_SN  normN_SN  convN_SN+relu  poolN_SN  normN_SN  convN_SN+relu  convN_SN+relu  convN_SN+relu  poolN_SN  normN_SN  convN_SN+relu  poolN_SN  normN_SN  convN_SN+relu  convN_SN+relu  fcN_Fused+relu  fcN_Fused+relu  fcN_Fused  fcN_SN+relu  fcN_SN+relu  fcN_SN  fcN_SN+relu  fcN_SN+relu  fcN_SN  SN Extrator  SN Classifier SN  Classifier Fused Classifier  SUM  SN Extrator  convN_SN+relu  poolN_SN  convN_SN+relu  poolN_SN  Softmax_SN Softmax_Fused  Softmax_SN  Figure N
The architecture of DNC
The dropout layers following  fcN and fcN in the {Fused, SN, SN} Classifier are omitted
On the whole, the feature maps of the SN Extractor and SN Extractor are  summed into the Fused Classifier, which is expected to achieve  higher accuracy for recognition by coordinating two extractors to  learn complementary features
 effectively assembling the feature extractors of two subnetworks, we finally acquire a network with double neurons at  each layer such that the input patterns can be more fully represented
In the following, we will respectively elaborate on  the architecture of DualNet and the corresponding training  strategy
 N.N
Network Architecture  In DualNet, two identical DCNNs are adopted for the  complementary learning, as illustrated in Figure N
The  Subnetwork N and Subnetwork N can be any existing modN0N    Input  SN Extractor  Fused Classifier  SN Extractor  SN Classifier  SN Classifier  r  (c) Joint Finetuning  (a) Finetune CaffeNet  Input  SN Extractor  Fused Classifier  SN Extractor  SN Classifier  SN Classifier  Input  SN Extractor  Fused Classifier  SN Extractor  SN Classifier  SN Classifier  (b) Iterative Training  Input  Extractor  Classifier  Zero  iter_0 iter_i (i=N,N, )   iter_j (j=N,N, )   Figure N
Illustration of the training strategy of DNC
Caffenet is first finetuned on specific dataset for the initialization of DNC
Then  the SN Extractor and SN Extractor are trained in an iterative way through the iterative training
Finally, the last fully connected layers of  the Fused Classifier, SN Classifier, SN Classifier are jointly finetuned
At each stage, only the components surrounded by the red rectangle  are finetuned with the rest fixed
Best viewed electronically
 els, and here we evaluate DualNet based on the well-known  CaffeNet, VGGNet, NIN and ResNetN (referring to PN)
 They are coordinated to learn complementary features from  input images, which are then aggregated to build richer and  more accurate representation for recognition compared to  single network
 An example architecture of the DualNet From CaffeNet  (DNC) is shown in Figure N, where the two involved Caffenet are denoted as SN and SN for simplicity
Particularly,  we logically divide an end-to-end CaffeNet into two functional parts, i.e., feature extractor and image classifier
The  division is not explicitly specified and theoretically can occur at any layer, e.g., poolN here
On the whole, DNC has  a symmetrical architecture, in which the SN Extractor and  SN Extractor are placed side by side and the feature maps  produced by them are integrated into the Fused Classifier
 The auxiliary SN Classifier and SN Classifier are appended to make the features produced by each feature extractor  discriminative alone
And the complementary constraint is  imposed by weighting the three classifiers
In fact, the key  component of DualNet is the Fused Classifier, which can  assemble two extractors to describe the objects of interest  from different aspects, and thus result in higher accuracy  for recognition (referring to PN)
 The fusion layer poolN is empirically selected for the following reasons
Each activation unit in poolN corresponds  to a NN × NN patch in the input image, while the one in fulNAll using the public version in the Caffe Model Zoo [N]
For NIN,  the settings of NIN-CIFARN0 is adopted
And for ResNet, since there is  no complete model in the Caffe Model Zoo (only testing code), we implement it on CIFAR-N0 with Caffe, according to [NN, NN] and the third-party  implementation available at [N]
 ly connected layers sees the entire scene
We expect that  one of the image extractors can learn more specific characteristics about the objects complementary to the other, and  these details are usually presented in small regional areas
 So it is implied that the fusion is better performed on the  local patch, not the full image scope
In addition, fusing at  poolN also has computational benefit when performing test  (referring to PN)
SUM is chosen as the fusion method for  simplicity, and also for transferring the parameters from the  last fully connected layer, i.e., classifier, of the original CaffeNet
And the coefficients are fixed as {0.N, 0.N}
Further discussion about the selection of fusion method is provided  in Section N according to PN, PN and PN
 The same strategy is applied to the NN-layer VGGNet to  construct the DualNet From VGGNet (DNV)
Please refer  to [NN] for the details of VGGNet
The SN Extractor and SN  Extractor are comprised of the layers before poolN in VGGNet
The feature maps from two extractors are averaged  and sent into the following Fused Classifier for overall classification
The auxiliary classifier is appended after each  extractor to keep the features discriminative alone
 For NIN [NN] and ResNet [NN] in which there does not  exist fully connected layers and the input size is much smaller (NN×NN), following the same philosophy, we choose to construct the DualNet From NIN (denoted as DNI instead  of DNN to avoid confusion) and DualNet From ResNet (DNR) by averaging the features maps of two subnetworks  at the penultimate convolution layer (e.g., cccpN in NIN),  while the last convolution layer is for predictionN
 NThe network architectures of DNV, DNI and DNR are illustrated in  the supplementary material
 N0N    N.N
Training Strategy  The training strategy plays vital roles in coordinating the  two extractors to learn complementary features, in which  the whole network is not just globally finetuned (referrring  to PN, PN)
Taking the training of DNC as example, as illustrated in Figure N, it mainly consists of two aspects: the  iterative training between the SN Extractor and SN Extractor to make them cooperate well, and the joint finetuning  of the Fused Classifier, SN Classifier and SN Classifier for  further performance improvement
 N.N.N Iterative Training  The iterative training means, between the SN Extractor and  SN Extractor, fixing one of them and finetuning the other  in an iterative way
On the one hand, it is out of the consideration of conserving GPU memory (referring to PN) and  reducing overfitting
On the other hand, we hope that this  way would explicitly make one extractor learn complementary features to the other during each iteration, thus yielding  more discriminative fused features (referring to PN)
Particularly, the domain-specific finetuning of CaffeNet can be  treated as a special case of the iterative training for DNC,  i.e., iter 0, in which another extractor is assigned with zero and the auxiliary classifiers are omitted for the moment
 Then the finetuned CaffeNet is utilized to initialize the SN  Extractor and Fused Classifier for the next iteration, i.e.,  iter N, and meanwhile the SN Classifier is also initialized  but not involved in iter N
 While training the SN Extractor (in iter i, i=N,N,...), the  parameters of the SN Extractor are fixed
Appending the SN  Classifier at the top of poolN SN can prevent that, the SN Extractor moves towards the same weights as the SN Extractor during trainingN, and thus will have little effect on the  fused features
Specifically, the modules including the SN  Extractor, SN Classifier and Fused Classifier are optimized  according the loss function defined as:  LN = LFused + λSNLSN (N)  where LFused and LSN are both cross entropy loss computed by the Softmax Fused and Softmax SN
The loss weight  λSN is empirically set to 0.N
To some extent, the second  term in the loss plays as the regularization for training, and  λSN < N is to inform the SN Extractor that the Fused Classifier is more important in the optimization
 And while the SN Extractor is fixed (in iter j, j=N,N,...),  the SN Classifier is appended for the finetuning of the SN Extractor (as well as the Fused Classifier) and the loss function is defined as:  LN = LFused + λSNLSN (N)  NFor example, in iter N, without the SN Classifier, the modules to train  will be the SN Extractor and Fused Classifier, which are basically the same  as CaffeNet in in iter 0
The ablation study is taken in Section N.N  where LFused and LSN are cross entropy loss computed by  the Softmax Fused and Softmax SN
The loss weight λSN is  also set to 0.N
 The maximum iteration (denoted as max iter) is set according to the cross validation
Generally speaking, it is  no more than N to gain the satisfying improvement
When  testing, the output of the Fused Classifier is taken for a fair  comparison with the base model, e.g., CaffeNet
Certainly, we can also assemble the predictions of three classifiers,  i.e., the probability of each class is computed as:  score = scoreFused + λSNscoreSN + λSNscoreSN (N)  where scoreFused, scoreSN, scoreSN are the output of the  Fused Classifier, SN Classifier and SN Classifier at testing  time
Then score is taken to evaluate for the recognition
 N.N.N Joint Finetuning  There are three classifier modules, i.e., the SN Classifier, SN  Classifier and Fused Classifier, involved in DualNet, but  in the above methods their abilities have not been fully exploited
Here an alternative integration method is proposed  to further boost the performance
 Since global finetuning of the whole network, e.g., DNV,  is time-consuming and requires large GPU memory (referring to PN), we instead choose to jointly finetune the last  fully connected layer of three classifier modules (e.g., fcN  in DNC, cccpN in DNI) with the following loss function:  LN = LFused + λSNLSN + λSNLSN (N)  where as above LFused, LSN, LSN are all cross entropy loss  output by the Fused Classifier, SN Classifier and SN Classifier respectively
The loss weights λSN and λSN keep as  0.N
Correspondingly, in the testing phase, the prediction  for each image is obtained according to Equation (N)
 N
Experiment  In this section, we evaluate the DualNet From CaffeNet (DNC), DualNet From VGGNet (DNV), DualNet  From NIN (DNI), DualNet From ResNet (DNR) on multiple widely-used datasets, including CIFAR-N00 [NN], Stanford Dogs [NN] and UEC FOOD-N00 [NN] (referring to  PN)
The hyper-parameters for the iterative training are identical to those of finetuning the standard deep models on  specific datasets
For the joint finetuning, the base learning  rate is reduced by a factor of N0 for a few additional iterations
All the networks are implemented with Caffe [NN]  and trained/tested on a Tesla KN0 GPU (referring to PN) N
 NThe implementation with the parameters for training each model, as  well as the pretrained models, is available at https://github.com/  ustc-vim/dualnet
 N0N  https://github.com/ustc-vim/dualnet https://github.com/ustc-vim/dualnet   Table N
The top-N accuracy on CIFAR-N00 achieved by  the standard deep models (NIN&ResNet) and DualNet (DNI&DNR)
The first row shows the results of NIN&ResNet and  the rest are all achieved by DNI&DNR
After the iterative training,  we respectively evaluate the performance of the Fused Classifier  and the weighted average of three classifiers, while the latter one  can validate the necessity of the joint finetuning
w/o aug-without  data augmentation, w/ aug-with data augmentation
 Model  Training DNI  (w/o aug)  DNR  (w/ aug)  standard deep model (NIN&ResNet) NN.NN% NN.0N%  iterative training (Fused Classifier) NN.0N% NN.NN%  iterative training (classifier average) NN.NN% NN.NN%  joint finetuning (classifier average) NN.NN% NN.NN%  N.N
CIFAR-N00  CIFAR-N00 [NN] consists of N0000 NN × NN natural im- ages in N00 classes, which are split into N0000 for training and N0000 for test
The dataset is pre-processed using  global contrast normalization and ZCA whitening following [N0, NN, NN, NN], and then is taken to evaluate DNI and  DNR here
 NIN [NN] is chosen as the base model because it yields  one of the best performances on CIFAR-N00, and the recent  works [NN, NN] are also built upon it
We follow the network  setting of NIN-CIFARN0 available in the Caffe Model Zoo  and change the output number of the last convolution layer  to N00
The results achieved by the standard NIN and our  DNI are shown in Table N, and reported in the top-N accuracy
After the iterative training, the Fused Classifier of DNI  achieves NN.0N% testing accuracy, which improves the performance of NIN by more than N%
At this stage, i.e., before the joint finetuing, we also evaluate the weighted average of three classifiers (computed according to Equation (N)  and denoted as classifier average) and get NN.NN%
Finally, the performance is further improved to NN.NN% when the  joint finetuning is done
It is worth noticing that, max iter is  set to N for the iterative training, and thus the computation  cost of training DNI is not heavy
 Since data augmentation for CIFAR-N00 is not standardized and it is hard to isolate the impact of data augmentation from the methods, following [NN], DNI is trained on  CIFAR-N00 without augmentation to enable a fair comparison with the existing literatures
Table N shows the performance comparison of different methods
There are some  works, e.g., [NN, NN, NN, N], which are not listed here because  they report the results only with data augmentation
Directly comparable to our DNI are [NN, NN, NN], which are also built upon NIN
Note that we reproduce higher accuracy  with NIN than the original version [NN] that was implemented with cuda-convnet [N], and some parameters have been  updated
HD-CNN [NN] actually adopts the cropping strategy and N0 view testing
It is listed here because it is one of  the most representative works and also taken for compariTable N
Performance comparison of DNI with the existing  methods on CIFAR-N00 without data augmentation
The results are all reported in the top-N accuracy
∗-with cropping strategy and N0 view testing [NN]
 Method Test accuracy  Maxout Network [N0] NN.NN%  Tree based priors [NN] NN.NN%  Network In Network [NN] NN.NN%  DSN [NN] NN.NN%  NIN+LA units [N] NN.N0%  HD-CNN∗ [NN] NN.NN%  DDN [NN] NN.NN%  DNI (ours) NN.NN%  son in [NN]
To the best of our knowledge, DDN [NN] reports  the best result on CIFAR-N00 without augmentation before  this work
And our DNI performs better than DDN [NN] by  N.NN%
 Next, in order to evaluate DNR, we first follow the description in [NN, NN] to implement a N0-layer ResNet (denoted as ResNet-N0) with Caffe
And it achieves the top-N  accuracy of N0.NN% on CIFAR-N0 without any data augmentation, which basically agrees with the result in [NN]
 But when the same setting is used for CIFAR-N00, the performance (only N0.NN%) is much worse than NIN, which  is probably caused by data scarcity
After all, the number  of images in each class of CIFAR-N00 is only one tenth  of CIFAR-N0
In that case, we augment the training data  with padding and randomly changing contrast and brightness, and then get NN.0N% on CIFAR-N00 with ResNet-N0N
 Then DNR is constructed and also trained on the augmented  data
As shown in Table N, the iterative training (max iter  is also set to N) improves the performance to NN.NN%
After the joint finetuning, DNR finally achieves NN.NN% on  CIFAR-N00, which is N.NN% higher than the base model
 N.N
Stanford Dogs  Since the image size of CIFAR-N00 (NN × NN) is much smaller than the input size of CaffeNet (NNN×NNN) and VG- GNet (NNN × NNN), it is not proper to resize the images of CIFAR-N00 to train them
So another dataset, i.e., Stanford  Dogs [NN], is chosen to evaluate DNC and DNV
The dataset  is made up of NN0 classes and N0NN0 images
Throughout  the experiments, no bounding box annotation or data augmentation is involved, except that the flip is randomly executed on the images before being input into the network
 We follow the standard split way attached in the dataset for  training and test, i.e., for each class there are N00 images  used for training and the rest for test
At testing time, the  evaluation is done with one single center crop of input images
For simplicity, we only report the results of DNC and  DNV when all the training is done
And max iter is set to  NThe network architecture of ResNet-N0 and the parameters for data  augmentation are provided in the supplementary material
 N0N    Table N
The top-N accuracy on Stanford Dogs and UEC  FOOD-N00 achieved by the standard deep models (CaffeNet&VGGNet) and DualNet (DNC&DNV)
The results are  reported using the weighted average of three classifiers when all  training is done
The performance comparison demonstrates that  the proposed DualNet performs well with CaffeNet and VGGNet
 Method  Dataset Stanford Dogs UEC FOOD-N00  CaffeNet NN.NN% NN.NN%  DNC (From CaffeNet) NN.NN% NN.NN%  VGGNet NN.NN% NN.N0%  DNV (From VGGNet) NN.NN% NN.NN%  N for the iterative training
According to Table N, DNC and  DNV perform well and both achieve higher accuracy than  the corresponding baselines, i.e., CaffeNet and VGGNet
 N.N
UEC FOOD-N00  UEC FOOD-N00 [NN] is selected to further evaluate DNC and DNV since it is a totally fresh domain which differs from CIFAR-N00 and Stanford Dogs (referring to PN),  and max iter is set to N on it according to the cross validation
There are N00 food categories in the dataset with more  than N00 images for each category
There is no split way  provided in it, so for each class we randomly pick N00 images for training with the rest for test
The identical experimental settings on Stanford Dogs are followed, i.e., neither  bounding box annotation nor data augmentation for training, and one single center crop for test
The performance  comparison of DNC and DNV with their base models on  UEC FOOD-N00 is shown in Table N
Besides, we further  evaluate DNV on the dataset after each training procedure,  i.e., the domain-specific finetuning of VGGNet, each iteration of the iterative training (iter i, i=N,N), and the joint  finetuning
As shown in Figure N, the top-N accuracy is improved step by step in the training process, indicating that  each training procedure helps
 Note that the focus of experiments on Stanford Dogs and  UEC FOOD-N00 is on the behaviors of DualNet from CaffeNet and VGGNet on datasets of diverse domains, not reporting state-of-the-art results
So we only compare the performance of DNC and DNV with the base models, i.e., CaffeNet and VGGNet, and do not apply any data augmentation [NN] or utilize parts [NN]
The philosophy of DualNet is  to coordinate two DCNNs to learn complementary features,  so it is natural to adopt the single DCNN as baselines
And  this work makes sense by providing a generic framework to  effectively integrate two DCNNs to extract more discriminative representation
 N.N
Experimental Analyses  In this section we take further experiments to analyze the  performance achieved by DualNet
Here DNI is taken to  illustrate the proof, and the results are reported on CIFARVGGNet iterN-* iterN-* iterN-# jt-#  T o  p -N   A cc   ( %  )  NN  NN.N  NN  NN.N  NN  NN.N  Figure N
The top-N accuracy on UEC FOOD-N00 by DNV after each training procedure
*-Fused Classifier, #-classifier average, iter i (i=N,N)-iterative training, jt-joint finetuning
 N00 without data augmentation
 In Section N.N.N, we have explained the importance of  the auxiliary classifiers and here assess it experimentally  through ablation study
Specifically, in the first iteration of  iterative training for DNI, i.e., iter N, the identical settings  are followed except that the SN Classifier is removed
That  is, the finetuned NIN is utilized to initialize the SN Extractor  and Fused Classifier, and then the SN Extractor and Fused  Classifier are trained without the SN Classifier
When iter N  is done, the output of the Fused Classifier is taken for evaluation
In this way we get NN.N0% on CIFAR-N00, which  is much lower than the NN.0N% obtained in the same conditions but with the SN Classifier appended
So introducing  the auxiliary classifiers is indeed necessary
 We also have tried to remove both the auxiliary classifiers of DNI and then train the remaining network globally,  including the SN Extractor, SN Extractor and Fused Classifier
In that case, there exist two initialization strategies
 The first one is to train from scratch
Then the Fused Classifier only achieves NN.NN% on CIFAR-N00
The second is to  initialize the SN Extractor, SN Extractor and Fused Classifier with NIN after being finetuned on CIFAR-N00
Then we  obtain NN.NN% with the Fused Classifier
Both the results  are lower than the NN.NN% achieved by DNI
 Another attempt is to directly double the number of features maps in each layer, which is feasible with NIN on a  Tesla KN0 GPU (not feasible with VGGNet) and then train  the network from scratch on CIFAR-N00
Compared to the  standard NIN (NN.NN%, also trained from scratch), the testing accuracy is improved to NN.NN%
However, it still performs worse than DNI (NN.NN%), although more computation cost is introduced, such as time and GPU memory required for the optimization
 Besides, since the final accuray of DNI is reported by assembling the three classifiers after the joint finetuning, we  further evaluate the performance of each individual classiN0N    SN SN Fused Average  T o p -N   A cc   ( %  )  NN  NN  NN  NN  NN  NN  N0  Figure N
The evaluation of each classifier after the joint finetuning of DNI on CIFAR-N00 without data augmentation
SNSN Classifier, SN-SN Classifier, Fused-Fused Classifier, Averagethe weighted average of three classifiers
 fier (SN Classifier, SN Classifier and Fused Classifier)
According to Figure N, among the three classifiers, the Fused  Classifier performs best as expected (referring to PN), and  the weighted average can further improve the performance.N  N
Discussion  Fusion method
In DualNet, we focus on the cooperation of two subnetworks to learn complementary features  guided by PN-PN
Referring to PN and PN, we deal with  the issue considering only simple fusion methods, such as  SUM, MAX and Concat, to ensure the generalization ability
Besides SUM, we have tried Concat but it does not  perform well
The probable reason is that much more parameters are introduced
Max is not a better choice either because it does not make semantic sense for the fusion, in which two extractors are supposed to be complementary, not competitive
Besides, adopting SUM as the  fusion method makes all our models, especially DNV, compatible to train on a Tesla KN0 GPU without decreasing the  mini-batch size (referring to PN)
Certainly, other complex  methods, e.g., the bilinear pooling, can also be adopted for  DualNet, which, however, will incur high computation cost  and impair the generalization ability
 Computational effort
In the iterative training of DualNet, one of the feature extractor is fixed, and thus the  memory consumption is less compared to global finetuning  or directly doubling the width of each layer
And for the  joint finetuning, only the last layer of three classifiers are  jointly finetuned
Besides, max iter is always no more than  N, and in most cases, it is in the first iteration that most of the  improvement is gained (e.g., DNI on CIFAR-N00)
More iterations for the iterative training and the joint training are  NMore experimental analyses are provided in the supplementary material
 Table N
Comparison of GPU memory consumption and time  cost between NIN and DNI for the test on CIFAR-N00 (N0000  images)
The first row shows the complexity statistics of NIN,  while the rest are about DNI
 Model  Training Memory  (MB)  Time  (sec.)  standard deep model (NIN) NNN N.N  iterative training (Fused Classifier) N0NN NN.N  iterative training (classifier average) N0NN NN.N  joint finetuning (classifier average) N0NN NN.N  both optional
Hence the cost of training DualNet is not  heavy
Besides, we provide some complexity statistics for  the test shown in Table N
NIN and DNI are further evaluated on CIFAR-N00 in terms of GPU memory consumption  and time cost for the entire test set
The mini-batch size for  test is set to N00
Through the comparison, it can be seen  that the improvement brought by DNI is achieved without  introducing too much computation cost (two times as much  as NIN)
By contrast, in HD-CNN [NN] which is also built  on NIN, the GPU memory consumption and testing time are  three times and four times as much as the base model
 N
Conclusion  In this work, we deal with the issue of image recognition  through providing a fairly generic framework named DualNet, in which two parallel DCNNs are coordinated to learn  complementary features, thus constructing a wider network  and yielding more discriminative representation
Following the principles PN-PN, two-stream features are extracted  for an input image, which are then fused through SUM to  form a unified representation
Apart from the overall classifier based on the fused features, two auxiliary classifiers  are proposed to be appended behind each extractor to keep  the separately learned features discriminative alone
And  the complementary constraint is imposed through weighting the three classifiers
Finally, DualNet based on CaffeNet, VGGNet, NIN and ResNet are thoroughly investigated and experimentally evaluated on CIFAR-N00, Stanford Dogs and UEC FOOD-N00, which all achieve higher  top-N accuracy than the baselines
In particular, the performance on CIFAR-N00 is state-of-the-art compared to the recent works
In the future, we plan to explore the utilization  of more advanced fusion methods and apply the network  compression techniques to further optimize DualNet
 Acknowledgment  This work is supported partially by the National Natural Science Foundation of China under Grant NNNNNNNN and  NNNNN00N, Youth Innovation Promotion Association CAS,  and the Fundamental Research Funds for the Central Universities.We are grateful for the generous donation of Tesla  GPU KN0 from the NVIDIA corporation
 N0N    References  [N] https://github.com/BVLC/caffe/wiki/  Model-Zoo
N  [N] https://github.com/twtygqyy/  resnet-cifarN0/tree/master/resnetN0
 N  [N] https://code.google.com/p/cuda-convnet/
 N  [N] F
Agostinelli, M
Hoffman, P
Sadowski, and P
Baldi
 Learning activation functions to improve deep neural networks
arXiv preprint arXiv:NNNN.NNN0, N0NN
N  [N] L.-C
Chen, Y
Yang, J
Wang, W
Xu, and A
L
Yuille
Attention to scale: Scale-aware semantic image segmentation
 In CVPR, N0NN
N  [N] D.-A
Clevert, T
Unterthiner, and S
Hochreiter
Fast and  accurate deep network learning by exponential linear units  (elus)
In ICLR, N0NN
N  [N] N
Dalal and B
Triggs
Histograms of oriented gradients for  human detection
In CVPR, N00N
N  [N] K
Fukushima
Neocognitron: A self-organizing neural network model for a mechanism of pattern recognition unaffected by shift in position
Biological cybernetics, NN(N):NNN–  N0N, NNN0
N  [N] M
Gazzaniga, R
Ivry, and G
Mangun
Cognitive Neuroscience: The Biology of the Mind (Fourth Edition)
W
W
 Norton, N0NN
N  [N0] I
J
Goodfellow, D
Warde-Farley, M
Mirza, A
C
 Courville, and Y
Bengio
Maxout networks
In ICML, N0NN
 N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Spatial pyramid pooling  in deep convolutional networks for visual recognition
In  ECCV, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Delving deep into  rectifiers: Surpassing human-level performance on imagenet  classification
In ICCV, N0NN
N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Deep residual learning  for image recognition
In CVPR, N0NN
N, N, N, N, N  [NN] K
He, X
Zhang, S
Ren, and J
Sun
Identity mappings in  deep residual networks
In ECCV, N0NN
N, N, N  [NN] S
Ioffe and C
Szegedy
Batch normalization: Accelerating  deep network training by reducing internal covariate shift
In  ICML, N0NN
N  [NN] Y
Jia, E
Shelhamer, J
Donahue, S
Karayev, J
Long, R
Girshick, S
Guadarrama, and T
Darrell
Caffe: Convolutional  architecture for fast feature embedding
arXiv preprint arXiv:NN0N.N0NN, N0NN
N  [NN] A
Khosla, N
Jayadevaprakash, B
Yao, and F.-F
Li
Novel  dataset for fine-grained image categorization: Stanford dogs
 In CVPR Workshop, N0NN
N, N, N  [NN] A
Krizhevsky and G
Hinton
Learning multiple layers of  features from tiny images
N00N
N, N, N  [NN] A
Krizhevsky, I
Sutskever, and G
E
Hinton
Imagenet  classification with deep convolutional neural networks
In  NIPS, N0NN
N, N  [N0] Y
LeCun, L
Bottou, Y
Bengio, and P
Haffner
Gradientbased learning applied to document recognition
Proceedings of the IEEE, NN(NN):NNNN–NNNN, NNNN
N  [NN] C.-Y
Lee, S
Xie, P
Gallagher, Z
Zhang, and Z
Tu
Deeplysupervised nets (N0NN)
arXiv preprint arXiv:NN0N.NNNN
N  [NN] M
Lin, Q
Chen, and S
Yan
Network in network
In ICLR,  N0NN
N, N, N  [NN] T.-Y
Lin, A
RoyChowdhury, and S
Maji
Bilinear cnn models for fine-grained visual recognition
In ICCV, N0NN
N  [NN] D
G
Lowe
Distinctive image features from scaleinvariant keypoints
International journal of computer vision, N0(N):NN–NN0, N00N
N  [NN] A
L
Maas, A
Y
Hannun, and A
Y
Ng
Rectifier nonlinearities improve neural network acoustic models
In ICML,  N0NN
N  [NN] Y
Matsuda, H
Hoashi, and K
Yanai
Recognition of  multiple-food images by detecting candidate regions
In  ICME, N0NN
N, N, N  [NN] V
N
Murthy, V
Singh, T
Chen, R
Manmatha, and D
Comaniciu
Deep decision network for multi-class image classification
In CVPR, N0NN
N, N  [NN] N
Neverova, C
Wolf, G
W
Taylor, and F
Nebout
Multiscale deep learning for gesture detection and localization
In  ECCV Workshops, N0NN
N  [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
N  [N0] P
Sermanet, D
Eigen, X
Zhang, M
Mathieu, R
Fergus,  and Y
LeCun
Overfeat: Integrated recognition, localization  and detection using convolutional networks
In ICLR, N0NN
 N, N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional  networks for large-scale image recognition
arXiv preprint  arXiv:NN0N.NNNN, N0NN
N, N, N  [NN] J
Snoek, O
Rippel, K
Swersky, R
Kiros, N
Satish, N
Sundaram, M
Patwary, M
Ali, R
P
Adams, et al
Scalable  bayesian optimization using deep neural networks
In ICML,  N0NN
N  [NN] N
Srivastava and R
R
Salakhutdinov
Discriminative transfer learning with tree-based priors
In NIPS, N0NN
N  [NN] C
Szegedy, W
Liu, Y
Jia, P
Sermanet, S
Reed,  D
Anguelov, D
Erhan, V
Vanhoucke, and A
Rabinovich
 Going deeper with convolutions
In CVPR, N0NN
N  [NN] Z
Yan, H
Zhang, R
Piramuthu, V
Jagadeesh, D
DeCoste,  W
Di, and Y
Yu
Hd-cnn: Hierarchical deep convolutional  neural networks for large scale visual recognition
In ICCV,  N0NN
N, N, N  [NN] S
Zagoruyko and N
Komodakis
Wide residual networks
 In BMVC, N0NN
N, N, N  [NN] M
D
Zeiler and R
Fergus
Visualizing and understanding  convolutional networks
In ECCV, N0NN
N, N, N  [NN] N
Zhang, J
Donahue, R
B
Girshick, and T
Darrell
Partbased r-cnns for fine-grained category detection
In ECCV,  N0NN
N  NN0  https://github.com/BVLC/caffe/wiki/Model-Zoo https://github.com/BVLC/caffe/wiki/Model-Zoo https://github.com/twtygqyy/resnet-cifarN0/tree/master/resnetN0 https://github.com/twtygqyy/resnet-cifarN0/tree/master/resnetN0 https://code.google.com/p/cuda-convnet/Recurrent Models for Situation Recognition   Recurrent Models for Situation Recognition  Arun Mallya and Svetlana Lazebnik  University of Illinois at Urbana-Champaign  {amallyaN,slazebni}@illinois.edu  Abstract  This work proposes Recurrent Neural Network (RNN)  models to predict structured ‘image situations’ – actions  and noun entities fulfilling semantic roles related to the action
In contrast to prior work relying on Conditional Random Fields (CRFs), we use a specialized action prediction  network followed by an RNN for noun prediction
Our system obtains state-of-the-art accuracy on the challenging recent imSitu dataset, beating CRF-based models, including  ones trained with additional data
Further, we show that  specialized features learned from situation prediction can  be transferred to the task of image captioning to more accurately describe human-object interactions
 N
Introduction  Recognition of actions and human-object interactions in  still images has been widely studied in computer vision
 Early datasets and approaches focused on identifying a relatively small number of actions, such as N0 in PASCAL  VOC [N] and N0 in the Stanford Dataset [N0]
Newer and  larger datasets such as MPII Human Pose [NN] have enlarged the number of action classes to around N00
The  COCO-A [NN] and HICO [N] datasets aim to recognize interactions between multiple humans, and humans and objects, expanding the scope of recognition to outputs such  as human-riding-bicycle, human-repairing-bicycle, humanriding-horse, etc
 Of late, the focus has shifted to predicting even more  structured outputs, tackling higher-level questions such as  who is doing what and with which object
The recently introduced imSitu Dataset [NN] generalizes the task of action  recognition to ‘situation recognition’ — the recognition of  all entities fulfilling semantic roles in an instance of an action performed by a human or non-human actor
Given a  particular action, situations are represented by a set of relevant (semantic role: noun entity) pairs
An example image  and associated situation from imSitu are shown in Fig
N,  where “a woman arranging flowers in a vase on the countertop” is represented by Action: arranging, {(Agent: woman), (Item: flowers), (Tool: vase), (Place: countertop)}
As anVGG-NN Network  arranging  RNN  Agent woman  woman woman   Tool   vase   hand  Item flowers flowers flowers  N)  N) N)  Place countertop  kitchen inside  Verb: arranging  Fusion Network  Image Image  Detected Person   Box  Image  ∅  Figure N: Each image in imSitu is labeled with an action verb (orange),  and each verb is associated with a unique set of semantic roles (bold black)  which are fulfilled by noun entities present in the image (green)
Each  image has multiple annotations to account for the intrinsic ambiguity of the  task
Our approach first uses the fusion network of [NN] to predict the action  verb
Then it feeds the verb and a visual feature from a separate network  into an RNN to predict the noun roles in a fixed sequence conditioned on  the action
 other example, “A horse rearing outside” can be mapped to  Action: rearing, {(Agent: horse), (Place: outside)}
imSitu consists of N0N actions, N,N00 semantic roles, and NN,000  noun entities resulting in around N00,000 unique situations
 Along with the dataset, Yatskar et al
[NN, NN] also introduced Conditional Random Field (CRF) models to predict  situations given an image
In our work, we propose and  train Recurrent Neural Networks (RNNs) to predict such  situations and outperform the previously state of the art  CRFs
 Our use of RNNs for situation prediction is motivated  by their popularity for tasks like image caption generation, where they have proven to be successful at capturing grammar and forming coherent sentences linking multiple concepts
The standard framework for caption generation involves feeding high-level features from a CNN, often  trained for image classification on ImageNet [NN], into an  RNN that proceeds to generate one word of the caption at a  time [NN, NN, NN, N, NN]
Situation recognition involves the  prediction of a sequence of noun entities for a particular action, so it can be viewed as a more structured version of the  NNN    captioning task with a grammar that is fixed given an action
 Figure N gives an overview of our best proposed system
 First, we predict the action verb using the specialized action  recognition architecture of [NN], which fuses features from  a detected person box with a global representation of the  image
Conditioned on the action, we treat the prediction  of noun entities as a sequence generation problem and use  an RNN
Details of our model, along with several baselines,  will be given in Section N
Through extensive experiments  (Section N) we found that using separate networks for predicting the action verb and the noun entities produces higher  accuracy than jointly training a visual representation for the  two tasks
Finally, in Section N we explore how knowledge gained from situation prediction can obtain meaningful improvements for image captioning on the MSCOCO  dataset [NN] through feature transfer
 N
The Situation Prediction Task and Methods  Situations are based on a discrete set of action verbs  V , noun entities N , and semantic roles R
Each verb  v ∈ V is paired with a unique frame f ∈ F derived from FrameNet [N], a lexicon for semantic role labeling
A frame  is a collection of semantic roles Rv ⊂ R which are asso- ciated with the verb v
For example, the semantic roles  {Agent, Item, Tool, Place} ⊂ R are associated with the verb arranging
In an instantiation of an action in an image, each  semantic role is fulfilled by some noun n ∈ N∪{∅}, where ∅ indicates that the value is either not known or does not apply
The set of nouns N is derived from WordNet [NN]
An  instance of an action v in an image I forms a realized frame  F(I,v) in which each semantic role is associated with some  noun n, i.e.F(I,v) = {(ri, ni) : ri ∈ Rv, ni ∈ N∪{∅}, i = N, · · · , |Rv|}
Finally, a situation S is the pair of action and realized frame for that action, S = {v, F(I,v)}
The task of situation prediction is to predict an action verb and its associated realized frame given an image
Though each image  is annotated with a single verb, multiple situations might  be applicable for an image due to the choice of nouns used  to form a realized frame
For example, one might use the  term countertop instead of kitchen as the noun associated  with the semantic role of Place in Fig
N
To account for  this multiplicity, the imSitu dataset provides three independently labeled situations per image
 The authors who introduced situtation prediction also  proposed a CRF-based approach for the task [NN]
They  decompose the structured prediction of a situation, S = {v, F(I,v)}, over the verb v and semantic role value pairs (r, n) in the realized frame F(I,v)
They learn a potential functionψv(v; θ) for every verb, and a potential function for every verb, semantic role, noun entity tuple ψr(v, r, n; θ) (v ∈ V , r ∈ Rv , n ∈ N ∪ {∅}), where θ denotes the pa- rameters of the deep neural network used to predict these  potentials
The probability of a particular situation S given  input image I can thus be represented by:  p(S|I; θ) = N  Z · ψv(v|I; θ) ·  ∏  (ri,ni) ri∈Rv,ni∈N∪{∅}  ψr(v, ri, ni|I; θ)
(N)  The CRF normalization constant Z required for computing  the loss during training is obtained by predicting the potentials for all valid tuples found in the training set and then  summing them
The potentials are predicted using a fully  connected layer on top of the fcN layer of the VGG-NN network [NN]
During inference time, all valid tuples are scored  and ranked
A difficulty with this approach is the large number of potentials that need to be predicted: N0N for all possible verbs and NNN,NNN for all valid verb, semantic role,  noun entity tuples
Further, this model does not explicitly  account for the fact that nouns are shared across semantic  roles, though it is possible that the deep neural network implicitly learns such representations
In order to explicitly  enforce the sharing of information and reduce the number  of parameters, the follow-up work by Yatskar et al
[NN]  further decomposes the potentials as a tensor product over  verbs, semantic roles, and noun entities
This makes for a  complex model, details of which can be found in [NN]
 We take an alternate view of situation prediction by observing that given a verb v, the set of semantic roles Rv associated with it is fixed
For example, given the verb  arranging, we know that we have to predict relevant noun  entities for the semantic roles of Rarranging={Agent, Item, Tool, Place} (see Fig
N)
Conditioned on a given verb, if we assume some arbitrary but fixed ordering over these semantic roles, we can reduce the problem to that of sequential  prediction of noun entities corresponding to the semantic  roles
We decompose p(S|I; θ) as:  p(S|I; θ) = p (  v, (rN, nN), · · · , (r|Rv|, n|Rv|)|I; θ )  (N)  = p (  v, nN, · · · , n|Rv||I; θ )  (N)  = p(v|I; θ)  |Rv| ∏  t=N  p (nt|v, nN, · · · , nt−N, I; θ) 
(N)  Note that if an arbitrary but fixed ordering is chosen for  semantic roles belonging to every verb, then Eq
(N) follows from Eq
(N) as the correspondence of nouns to roles  is implicit
In our implementation, we use the semantic role  ordering provided in the dataset, which was derived from  FrameNet [N]
We explore the sensitivity of methods to the  specific ordering in the experiments of Section N, and find  that the accuracy is affected only to a very small degree
 We represent each p (nt|v, nN, · · · , nt−N, I; θ) in Eq
(N) with a softmax over all the noun entities in the training  dataset, referred to as the noun vocabulary
This is a  standard formulation first introduced for natural language  translation [NN] and widely adopted for image captionNNN    Figure N: The four approaches used for action and noun entity prediction: a) The baseline no-vision model, which only tries to predict noun entities  nN, · · · , nN in the chosen arbitrary but fixed semantic role ordering, given the ground truth verb v
b) Training an RNN which takes image features as input  and predicts action, followed by noun entities, c) Training a VGG-NN network for action prediction, and feeding its features to the RNN that predicts nouns  associated with the semantic roles, and d) Using separate networks for action and noun entity prediction
Bold colored text (orange and green) indicates  training targets
 ing [NN, NN, NN, NN]
Similar to these works, we use a softmax classification loss with the corresponding ground truth  noun entity as the target at every prediction step
 It is worth pointing out that both formulations, those of  CRF-based structured prediction (Eq
(N)) and sequential  prediction (Eq
(N)), are equally powerful in their representational abilities as both model the joint probability of the  verb and noun entities in a proposed situation
At inference  time, in the CRF approach of [NN, NN], all valid tuples of  verb and noun entities are evaluated and the most likely one  is reported, while in our sequential approach, we perform  approximate inference by selecting the most likely noun entity at each step
Despite this limitation, we obtain satisfactory empirical results (we also experimented with beam  search but did not see an improvement)
 Next, we present the progression of models we developed, starting with a language-only baseline and ending in  our highest-performing method illustrated in Figure N
 A) No vision, RNN for Nouns
In order to verify that sequential situation prediction can actually work and that an  RNN can memorize the specific ordering of semantic roles  for each verb, we propose a basic language-only model that  only tries to predict noun entities given the ground truth  verb
This model also acts as a strong baseline by exploiting bias in the dataset labeling as it does not use any visual feature input
This model is depicted in Fig
Na
The  ground truth verb is fed in at the first time step
Note that  it is essential to feed in the verb at the first time step as the  ordering and number of semantic roles for which noun entities are produced is decided by the choice of verb
At the  following time step, the RNN tries to predict the noun entity associated with the first semantic role in the arbitrarily  selected but fixed ordering, and so on, until a noun entity is  predicted for each semantic role for that verb
In line with  prior work [NN, NN], we feed in the initial verb and the output  of the previous time step as a one-hot vector through a word  embedding layer
As will be discussed in the next section,  this RNN can indeed memorize the arbitrary semantic role  ordering to make noun entity predictions in the appropriate  order
 B) Shared network, RNN for Actions & Nouns
The next  natural step is to extend the above no-vision model to use  image features and predict the action as well
This model is  shown in Fig
Nb
After consuming the fcN image features  from a VGG-NN network at the first time step, the model  predicts the action at the second time step and then continues on to predict noun entities
The noun vocabulary (space  of all noun entities) is extended with that of possible actions  to allow the prediction of both
Note that we use the ground  truth action as input during training and the predicted action  during testing
At inference time, we enforce that only an  action can be predicted at the second time step, followed by  noun entities only thereafter
 C) Shared network, Actions classifier, RNN for Nouns
 Since situation recognition has such a strong up-front dependence on the action verb, the next question we want to  explore is whether we can improve performance by breaking off the action prediction into a specialized task, instead  of treating it the same as the other roles
It also helps  that imSitu has many fewer verbs (N0N) than noun entities (NNK), giving us enough data to train a dedicated action  classifier
Accordingly, our second model predicts actions  using a separate fully-connected classification layer on top  of the fcN layer of the VGG-NN network as shown in Fig
Nc
 At the first step of the RNN, we feed in the one-hot representation of the action (at training time, we use the ground  truth action and at test time, the predicted action)
At the  second time step, we feed in the fcN image features to the  RNN to predict noun entities
Our experiments will investigate how to train the VGG network to get the highest acNNN    curacy for the overall task
One option is to train it solely  for action prediction and another is to jointly train it for  both action and noun prediction
Interestingly, our results  in Section N will show that the former strategy works better
 D) Separate networks, Actions classifier, RNN for  Nouns
The lack of success of joint training leads to the  question of whether we can do even better by not sharing  parameters between action and noun entity prediction
Accordingly, our final model decouples the two tasks and uses  two separate networks that are independently fine-tuned, as  depicted in Fig
Nd
For predicting actions, we use the feature fusion network of [NN] which obtained state-of-the-art  performance on the HICO dataset [N]
This network (called  Fusion in the following) combines local features from detected human boxes and global features from the whole image to make predictions that are then pooled
It defaults  to the full image in case no human is detected in the image
As a large number of images in the imSitu dataset  feature humans, this is a reasonable choice of architecture
 Along with a vanilla RNN for predicting noun entities, we  will also report experiments with an attention model based  on [NN] which consumes image features through a soft attention module at each time step
Note that instead of the  fcN features, the attention-based RNN uses the convN feature map
 N
Situation Prediction Experiments  Implementation Details
We use the simplified LongShort Term Memory (LSTM) cell [N, NN] as our RNN  model
We use a single-layer LSTM and with input and hidden layer sizes of NNN
We did not observe any significant  improvement by using larger layer sizes or more layers
The  imSitu dataset has a total of N0N actions and NN,NN0 noun  entities, leading to an LSTM output layer size of NN,NN0  in the case of models A, C, and D and NN,NN0+N0N in the  case of model B
We train all our RNNs with Adam [NN]  using an initial learning rate of Ne-N, decayed by a factor  of N0 every NN,N00 iterations using a batch size of NN
For  noun entity prediction, we first train the RNN for N0k iterations
We then turn on fine-tuning for the CNN with an  initial learning rate of Ne-N and use Adam with the same  learning rate decay scheme for an additional N00k iterations
The Fusion network [NN] is trained using stochastic gradient descent with momentum using a learning rate  of Ne-N for N0k iterations
Person boxes are detected using  the Faster-RCNN [N0] with a confidence threshold of 0.N
 Similar to [NN], we use a weighted loss during action prediction, unless otherwise specified
The weight for a class  is inversely proportional to its frequency in the training set
 Using weighted loss or beam search for noun entity prediction did not help
We only train on the imSitu train set of  NNk images
During training, we evaluate the model on the  dev set of NNk images and retain the best-performing model
 Finally, we evaluate the best model on the imSitu test set of  NNk images
All hyperparameters are tuned on the dev set
 Metrics
We evaluate performance on action verb predictions (verb), and (semantic role: noun entity) pair predictions (value, value-all) as well as the average across all measures (mean), as proposed in [NN]
Value-all measures the  percentage of predictions for which all of the (semantic role:  noun entity) pairs of an action verb matched with at least N  of the N ground truth (GT) annotations, while Value measures the percentage of pairs which matched at least one of  the three GT annotations
We report accuracy at top-N, topN action verb predictions and given the GT verb
Similar  to [NN], we also report performance on examples with ten or  fewer samples in the imSitu training set (rare setting)
 Results
We report results on the full dev set in Table N
 Section I of the table presents results from prior work of  Yatskar et al
[NN, NN]
Their baseline, a method they call  the Discrete Classifier, restricts its output space to the N0  most frequent realized frames for each verb
The Image Regression CRF uses the formulation of Eq
(N) with an output space of NNN,NNN for (verb, semantic role, noun entity)  tuples + N0N for actions, while Tensor Composition CRF  uses a tensor-based potential decomposition in an attempt  to reduce the number of parameters
The authors had to  combine the potentials produced by both models in order to  improve performance, leading to the Tensor Comp
+ Reg
 CRF method
Finally, by using five million web-sourced  images based on semantic querying [NN] in addition to the  NNk train set images, they were able to slightly improve performance
 Our baseline presented in Section II of Table N, corresponding to the architecture of Fig
Na, shows that RNNs  can indeed memorize an arbitrary ordering of semantic roles  for each verb and produce relevant noun entities in the correct and corresponding order
Further, by simply exploiting  the labeling bias, it beats the Discrete Classifier baseline by  a large margin, given the ground truth action verb
 Section III shows results from our next model (Fig
Nb),  which tries to predict both the action and noun entities using the same RNN
It improves the value metric by over  NN% given the ground truth verb over our no-vision baseline model, by using information from visual features
 Section IV reports the results of separating the action  prediction parameters from those of the noun entity predicting RNN (see Fig
Nc)
We see a large improvement in action verb prediction accuracy (NN.NN% to NN.NN%) as long as  we first fine-tune the network for the action task
By simply  using features from the network trained for action prediction, we only observe a very small drop in the value metric  given ground truth verbs, as compared to jointly fine-tuning  for verb and noun entity prediction (NN.NN% to NN.NN%)
 Here, we also try predicting the noun entities in a reversed  order so as to determine whether the order affects perforNNN    top-N predicted verb top-N predicted verbs ground truth verbs mean  verb value value-all verb value value-all value value-all  I)  Discrete Classifier [NN] NN.N N.0 0.N NN.N N.N 0.N NN.N 0.N NN.N  Image Regression CRF [NN] NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.N0 NN.N0 NN.NN  Tensor Composition CRF [NN] NN.NN NN.0N NN.NN NN.0N NN.NN NN.N0 NN.NN NN.NN NN.NN  Tensor Comp
+ Image Reg
CRF [NN] NN.NN NN.NN NN.NN NN.NN NN.N0 NN.0N NN.NN NN.NN NN.0N  Above + Extra NM Images [NN] NN.N0 NN.NN NN.NN NN.NN NN.NN NN.NN N0.N0 NN.NN NN.NN  II) Baseline RNN Method  Fig
Na No Vision, RNN for Nouns - - - - - - NN.NN NN.NN III) Joint Prediction – VGG jointly fine-tuned for Action and Noun Prediction  Fig
Nb VGG, RNN for Actions & Nouns NN.NN N0.0N NN.N0 NN.NN NN.NN N0.N0 NN.NN NN.NN NN.NN  Fig
Nc  IV)  VGG, Actions class., RNN for Nouns NN.0N NN.NN N0.N0 NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  Joint Prediction – VGG fine-tuned for Action Prediction Only  VGG, Actions class., RNN for Nouns NN.NN NN.N0 NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  VGG, Actions class., RNN for Nouns (reversed) NN.NN NN.NN NN.N0 NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  Joint Prediction – VGG fine-tuned for Action Prediction first, then jointly with Noun Prediction  VGG, Actions class., RNN for Nouns NN.NN NN.NN NN.NN N0.NN NN.NN NN.N0 NN.NN NN.NN NN.NN  V)  Action Prediction Only  VGG, Actions class
(no weighted loss) NN.NN - - NN.0N - - - - VGG, Actions class
NN.NN - - NN.NN - - - - Fusion (no weighted loss) NN.NN - - NN.0N - - - - Fusion NN.NN - - NN.NN - - - - Noun Prediction Only  VGG+RNN for Nouns - - - - - - NN.NN NN.NN VGG+RNN for Nouns, VGG fine-tuned (ft) - - - - - - N0.NN NN.NN VGG+RNN with Attention for Nouns - - - - - - NN.NN NN.NN VGG+RNN with Attention for Nouns (ft) - - - - - - NN.NN NN.NN Fig
Nd  VI) Separate Action and Noun Prediction  Fusion for Actions, VGG+RNN for Nouns (ft) NN.NN NN.NN NN.N0 NN.NN NN.0N NN.NN N0.NN NN.NN N0.N0  henceforth ref
to as Fusion, VGG+RNN Table N: Situation prediction results on the full imSitu dev set (see text for detail)
 top-N predicted verb top-N predicted verbs ground truth verbs mean  verb value value-all verb value value-all value value-all  Image Regression CRF [NN] NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN  Tensor Comp
+ Image Reg
CRF [NN] NN.NN NN.NN NN.NN N0.NN NN.NN NN.00 NN.N0 NN.NN NN.NN  Above + Extra NM Images [NN] NN.NN NN.NN NN.NN NN.NN NN.NN NN.NN N0.NN NN.NN NN.NN  Fusion, VGG+RNN NN.N0 NN.NN NN.NN NN.0N NN.NN NN.0N N0.NN NN.NN N0.NN  Table N: Situation prediction results on the full imSitu test set
 top-N predicted verb top-N predicted verbs ground truth verbs mean  verb value value-all verb value value-all value value-all  Image Regression CRF [NN] N0.NN NN.NN N.0N NN.NN NN.NN N.NN N0.NN N.NN NN.NN  Tensor Comp
+ Image Reg
CRF [NN] NN.NN NN.NN N.N0 NN.NN NN.NN N.NN NN.NN N0.NN NN.NN  Above + Extra NM Images [NN] N0.NN NN.NN N.NN NN.0N NN.N0 N.NN NN.NN NN.NN NN.NN  Fusion, VGG+RNN NN.0N NN.NN N.NN NN.NN NN.NN N.NN NN.NN NN.NN NN.NN  Table N: Situation prediction results on the rare portion of the imSitu test set
Along with better verb prediction accuracy, our method also produces more  accurate role values given GT verbs, indicating better generalization probably due to the use of shared parameters and word embeddings
 mance
We clearly see that this has very little effect on accuracy (0.N-0.N%)
However, we cannot rule out that some optimal ordering of semantic roles might exist for every verb
 We find that joint fine-tuning, either from the start or later, is  detrimental for action verb prediction, leading us to the final  models of Sections V and VI, which use separate networks  for action and noun entity prediction
 In Section V of Table N, we compare various methods of  separately predicting actions and noun entities
The Fusion  network of [NN] outperforms the VGG-NN network at action  prediction and using a weighted softmax loss helps in both  cases
By using a stand-alone action prediction network, we  obtain a top-N and top-N accuracy of NN.NN% and NN.NN%  in contrast to the previous best of NN.NN% and NN.NN%  from [NN], respectively
Even the method from [NN] that  uses an additional N Million images only obtains NN.N0%  and NN.NN% accuracies, respectively
 Apart from using LSTMs for predicting noun entities, we  NNN    Erasing - (Agent : man), (Erased : word), (Source : blackboard), (Place : ∅)  Talking - (Agent : woman), (Listener : woman), (Place : office)  Figure N: Predicted situations and attention maps associated with produced noun entities
In the top row, attention focuses on the correct regions
In the bottom example, attention cannot distinguish between the  Agent and Listener women instances
 also try using the soft attention-based architecture of Xu et  al
[NN]
The attention-based RNN works better, as long  as we do not fine-tune the underlying VGG-NN network
 Turning on fine-tuning makes the simple LSTM architecture work better, in line with results obtained on image captioning [NN]
Figure N shows some predicted situations and  associated attention maps
Qualitatively, attention produces  plausible results in simple cases, but is unable to make fine  distinctions, e.g., between multiple instances of a noun entity in different roles (bottom row of the figure)
 Finally, we combine our best action prediction and our  best noun entity prediction networks to propose our final  method referred to as Fusion, VGG+RNN (Fig
Nd) in Section VI of Table N
We beat the previous state-of-the-art  method trained on the imSitu train set on every metric
Additionally, we also beat the method trained on the extra NM  images, except on the value given ground truth verb metric,  on which we lag by just 0.NN%
 Table N compares our best-performing method against  the previous work on the full imSitu test set
We observe  a trend similar to that on the imSitu dev test
We improve  upon both the top-N and top-N verb prediction accuracies  by around N% and by N% (value) and N.N% (value-all) on  noun entity prediction given ground truth verbs, for methods trained on the imSitu train set
 Most interestingly, Table N shows that we also do well  on the rare portion of the imSitu test set
We improve upon  the top-N and top-N verb prediction accuracies by around  N% and by N% respectively
We improve by N% (value)  and N.N% (value-all) on noun entity prediction given ground  truth verbs, for methods trained on the imSitu train set
 We believe that embedding nouns in a common continuous space during input to RNNs helps to overcome the lack  of data and aids in generalization more effectively than the  ‘semantic augmentation’ with additional data in the previous method [NN]
 Finally, Figure N shows some correctly and incorrectly  predicted situations on the imSitu test set by our bestperforming method
While most of the mistakes are due  to incorrect action predictions, we observe that mistakes are  often reasonable, e.g., ‘arresting’ instead of ‘misbehaving’  in the bottom row, middle image
By analyzing the verb  prediction results, we find that we obtain the worst performance on bothering, intermingling, and imitating, which  are very contextual and semantic in nature, while those with  a clear visual nature such as erupting, shearing, and taxiing  obtain high accuracies
The worst noun prediction performance is obtained in cases where multiple nouns can fulfill semantic roles, such as distributing, prying, repairing;  while ballooning, taxiing, scoring obtain high accuracies
 N
Application to Image Captioning  One of the key motivations of proposing the task of image situation prediction was to better understand and learn  the semantic content of images, beyond mere action recognition [NN]
A more structured and nuanced understanding  of image semantics is expected to help high-level reasoning  tasks such as image captioning and Visual Question Answering (VQA) [N]
In this work, we try to leverage our  new state-of-the-art models for action verb and noun entity  recognition to improve image captioning performance on  the MSCOCO dataset [NN]
 We modify an off-the-shelf image captioning model,  NeuralTalkN [N], by providing it features from our networks  as an additional input, as shown in Figure N
The vanilla  NeuralTalkN network takes in fcN features from a VGG-NN  network as input to an RNN through an image embedding  layer Wi
It then proceeds to output words of the caption  one by one till the <END> token is predicted or a maximum length (typically NN) is reached
We feed in features  from networks trained on imSitu at the second time step,  similar to the method proposed in [NN]
We try two types  of features: fcN features from the VGG-NN network used for  noun entity prediction (green network in Fig
N) and fcN features from the VGG-NN network trained for action verb prediction (VGG, fc for Actions of Section IV of Table N)
We  use features from the VGG-NN network for action prediction  instead of the better performing Fusion network because the  former produces features from the whole image, while the  latter produces features for each detected person box
 Implementation Details and Results
We use a singlelayer LSTM with NNN hidden units and input size of NNN
 We train our captioning networks on the MSCOCO split of  Karpathy et al
[NN] which has NNN,NNN training, Nk validation, and Nk test images
We train the RNN and VGGNN CNN using Adam, with an initial learning rate of Ne-N  and Ne-N respectively
We train the baseline network in the  following recommended stages [N, NN]: N) Fine-tune RNN  only for N00k iterations, N) Fine-tune RNN and VGG-NN  network for NN0k iterations
As shown in Table N, this baseline (NeuralTalkN) obtains a CIDEr score of NN.0 on the test  NN0    N)      Verb: glowing   Agent Place   candle   N)      Verb: igniting   Agent Item Tool Place   person candle match   GT)     Verb: glowing   Agent Place   candle   N)      Verb: deflecting   Agent Deflec-  tedItem   Desti-  nation   Place   soccer  player   soccer  ball   field   Predictions Predictions   ∅   ∅   ∅   GT)      Verb: browsing   Agent GoalItem Place   woman book bookshop   Predictions   N)      Verb: shelving   Agent Item Destination Place   woman book shelf library   GT)      Verb: misbehaving   Agent Place   boy walkway   N)      Verb: arresting   Agent Suspect Place   policeman boy sidewalk   Predictions   N)      Verb: grieving   Agent Place   child cemetery   Predictions   GT)      Verb: leaning   Agent Item Against Place   woman head hand office   N)      Verb: studying   Agent Place   woman desk   N)      Verb: phoning   Agent Tool Place   woman telephone office   Predictions   GT)      Verb: celebrating   Agent Occasion Place   people parade river   N)      Verb: celebrating   Agent Occasion Place   people outside   N)      Verb: parading   Agent Place   people street   ∅   GT)      Verb: scoring   Agent Place   soccer player field   N)      Verb: scoring   Agent Place   soccer player field   N)      Verb: browsing   Agent GoalItem Place   woman book bookshop   ∅   Figure N: Correct (top row) and wrong (bottom row) predictions on the imSitu test set
One of the three groundtruth labels (GT) is shown to the top right  of each image
The top N predictions (as numbered) are shown below the ground truth
Mistakes can be due to incorrect action verb prediction (bottom row  first two images) or incorrect noun entity prediction (bottom right image)
  wN  wN  wN  <START>   We  We   We   wN   wN    wN-N wN  <END>   We    wNVGG fcN  imSitu fcN   We  Wr Wi  Word Embedding  Figure N: The modified NeuralTalkN [N] recurrent neural network that accepts the fcN feature vector from the networks trained on the imSitu situation prediction task at time step N
All units with the same color share  weights
Bold words wN, · · · ,wN are targets at training time
 Methods B@N B@N B@N B@N M C S  LRCN [N] NN.N NN.N N0.N NN.0 - - img-gLSTM [N0] NN.N NN.N NN.N NN.N N0.N NN.N NIC [NN]†,Σ NN.N NN.N NN.N NN.N - - img-gLSTM [N0] NN.0 NN.N NN.N NN.N NN.N NN.N Hard-Attention [NN] NN.N N0.N NN.N NN.0 NN.0 - Soft-Attention [NN] N0.N NN.N NN.N NN.N NN.N - ATT-FCN [NN]Σ N0.N NN.N N0.N N0.N NN.N - NeuralTalkN [N] (Ours) N0.N NN.N N0.N N0.N NN.N NN.0 NN.N  Image + Actions (Ours) NN.N NN.N N0.N N0.N NN.N NN.N NN.N  Image + Nouns (Ours) NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N: Caption generation model performance on the COCO test set  (N000 images) of Karpathy et al
[NN]
B@N, M, C, and S indicate  BLEU@N [NN], METEOR [NN], CIDEr [NN], and SPICE [N] respectively
 † indicates a different split of N000 images and Σ indicates an ensemble of  models
Bold values indicate the highest value for metrics obtained using  a single model
 set
We then modify the baseline model to accept an additional imSitu-based feature as input, as shown in Fig
N and  fine-tune the whole RNN+CNN for another N00k iterations
 Beam search of N and N was found to help the baseline and  improved model respectively (recall that it did not help in  situation prediction)
We see that feeding in imSitu-based  features improves the CIDEr score by N.N points
Feeding  features from the network that produces noun entity predictions (Image+Nouns) works better than features from the  action prediction network (Image+Actions)
Similar improvements are also observed on the held-out MSCOCO  test set as shown in Table N
Note that competing methods  listed in that table use ensembles and improved architectures to obtain better captioning performance
 While the quantitative improvements afforded by our additional semantic features are small (and automatic captioning metrics have well-known limitations [N]), we have qualitatively observed that our captions can describe interactions with objects more accurately, as can be seen from images and captions in the top row of Figure N
For example,  we can correctly identify that a person is holding a baseball  bat instead of a frisbee, or a hairbrush instead of a phone
 When our model goes wrong (Figure N, bottom row), it is  prone to hallucinating interactions with people
 N
Conclusion  This paper framed the recently introduced task of situation recognition as sequential prediction and conducted an  extensive evaluation of RNN-based models on the imSitu  dataset [NN]
Our most important findings are below
 • RNNs-based methods are a straightforward fit for the task and work quite well
 • Accurate action prediction is one of the main keys to beating the CRF methods of [NN, NN], which do not train  an explicit action classifier but predict actions jointly  NNN    Methods BLEU-N BLEU-N BLEU-N BLEU-N METEOR ROUGE CIDEr  cN cN0 cN cN0 cN cN0 cN cN0 cN cN0 cN cN0 cN cN0  ATT-FCN [NN]Σ NN.N N0.0 NN.N NN.N NN.N N0.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.N  OriolVinyals [NN]Σ NN.N NN.N NN.N N0.N N0.N NN.N N0.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N  MSR Captivator [N]? NN.N NN.0 NN.N NN.N NN.0 NN.0 NN.0 NN.0 NN.N NN.N NN.N NN.0 NN.N NN.N  Q.Wu [NN]? NN.N NN.N NN.N N0.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  NeuralTalkN [N] (Ours) N0.N NN.N NN.N NN.N NN.N NN.N NN.0 NN.N NN.N NN.N NN.N NN.0 NN.N NN.N  Image + Actions (Ours) NN.N NN.N NN.N NN.0 N0.N NN.N N0.N NN.N NN.N NN.0 NN.N NN.N N0.N N0.N  Image + Roles (Ours) NN.N NN.N NN.0 NN.N N0.N NN.N N0.N NN.N NN.N NN.N NN.N NN.0 N0.N NN.N  Table N: Caption generation model performance on the COCO testN0NN online leaderboard
We list results that have been published and highlight our  implemented baseline and methods
Note that the top methods use ensembles, better model architectures, and other engineering tricks such as scheduled  sampling, beyond the scope of this work
The cN test setting uses N reference captions and cN0 uses N0 reference captions
Σ indicates an ensemble of  models, ? indicates unspecified if ensemble
 VGG: A man sitting on a couch with a cat VGG+imSitu: A man sitting on a chair  with a cell phone GT: An old man is trying to use his cell  phone  VGG: A woman is holding a  frisbee in a park VGG+imSitu: A young girl is  holding a baseball bat on a field GT: A girl with a bat standing in  a field  VGG: A man with a beard and a tie VGG+imSitu: A man is holding a pair of  scissors GT: A person holding a pair of scissors  open intently  VGG: A herd of elephants walking across a lush green field VGG+imSitu: A group of people standing around a large elephant GT: A herd of elephants walking across a grass covered field  VGG: A truck is parked on the side of the road VGG+imSitu: A man standing next to a blue truck GT: A truck is parked on the side of a street  VGG: A woman holding a cell phone in her hand VGG+imSitu: A woman is brushing her hair in a  bathroom GT: A little girl is brushing her hair in a bathroom  VGG: A man and a woman are playing a video game VGG+imSitu: Two men standing in front of a kitchen counter GT: A man and a woman are playing video games  Figure N: Sample images from COCO test set of Karpathy et al
[NN] for which adding imSitu features provided the largest gain (top row) and largest drop  (bottom row) in CIDEr scores
We also show one of the five ground truth captions that is most similar to the produced captions
We notice that adding  imSitu features helps identify and better describe interactions with objects
At the same time, in some of the failure cases, it hallucinates interactions with  humans or misidentifies actions
 with all the other roles
Further, we found that training a separate action classifier that does not share parameters with noun entity prediction works best
This  suggests that the representations needed to predict actions and nouns may be different in non-trivial ways, as  it was difficult to fine-tune them jointly
 • Weakly-supervised attention gives minor improvements but is hard to fine-tune, limiting its absolute accuracy
 This is consistent with findings from captioning [NN]
 Qualitatively, we found this form of attention to have  limited ability to distinguish between entities, indicating  the need for advanced attention mechanisms [NN]
 • We have preliminary evidence that situations can help improve captioning quality, though the improvement is  currently small
In the future, we will explore better  methods to integrate the external knowledge provided  by the imSitu dataset into captioning
 A limitation of the RNN-based models over CRF-based  models is that they cannot produce outputs for verbs unseen  at train time as they are unaware of the semantic role ordering associated with the verb
We believe that this can be  fixed by making the RNN also output semantic roles, which  will be explored in future work
 Acknowledgments
We would like to thank Mark Yatskar  for his help with the imSitu dataset
This work was partially supported by the National Science Foundation under Grants CIF-NN0NNNN and IIS-NNNNNNN, Xerox UAC, the  Sloan Foundation, and a Google Research Award
 NNN    References  [N] NeuraltalkN
https://github.com/karpathy/  neuraltalkN
Accessed: N0NN-0N-0N
N, N, N  [N] P
Anderson, B
Fernando, M
Johnson, and S
Gould
Spice:  Semantic propositional image caption evaluation
In ECCV,  N0NN
N  [N] S
Antol, A
Agrawal, J
Lu, M
Mitchell, D
Batra, C
L
 Zitnick, and D
Parikh
Vqa: Visual question answering
In  ICCV, N0NN
N  [N] Y.-W
Chao, Z
Wang, Y
He, J
Wang, and J
Deng
Hico:  A benchmark for recognizing human-object interactions in  images
In ICCV, N0NN
N, N  [N] J
Devlin, H
Cheng, H
Fang, S
Gupta, L
Deng, X
He,  G
Zweig, and M
Mitchell
Language models for image captioning: The quirks and what works
ACL, N0NN
N  [N] J
Donahue, L
A
Hendricks, S
Guadarrama, M
Rohrbach,  S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual recognition and description
In CVPR, N0NN
N, N  [N] M
Everingham, L
Van Gool, C
K
Williams, J
Winn, and  A
Zisserman
The pascal visual object classes (voc) challenge
IJCV, N0N0
N  [N] C
J
Fillmore, C
R
Johnson, and M
R
Petruck
Background to framenet
International journal of lexicography,  N00N
N  [N] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural computation, N(N), NNNN
N  [N0] X
Jia, E
Gavves, B
Fernando, and T
Tuytelaars
Guiding  the long-short term memory model for image caption generation
In ICCV, N0NN
N  [NN] A
Karpathy and L
Fei-Fei
Deep visual-semantic alignments for generating image descriptions
In CVPR, N0NN
 N, N, N, N  [NN] Y
Kim, C
Denton, L
Hoang, and A
M
Rush
Structured  attention networks
In ICLR, N0NN
N  [NN] D
Kingma and J
Ba
Adam: A method for stochastic optimization
In ICLR, N0NN
N  [NN] M
D
A
Lavie
Meteor universal: Language specific translation evaluation for any target language
ACL, N0NN
N  [NN] T.-Y
Lin, M
Maire, S
Belongie, J
Hays, P
Perona, D
Ramanan, P
Dollár, and C
L
Zitnick
Microsoft coco: Common objects in context
In ECCV
N0NN
N, N, N  [NN] A
Mallya and S
Lazebnik
Learning models for actions and  person-object interactions with transfer to question answering
In ECCV, N0NN
N, N, N, N  [NN] G
A
Miller
Wordnet: a lexical database for english
Communications of the ACM, NNNN
N  [NN] K
Papineni, S
Roukos, T
Ward, and W.-J
Zhu
Bleu: a  method for automatic evaluation of machine translation
In  ACL, N00N
N  [NN] L
Pishchulin, M
Andriluka, and B
Schiele
Fine-grained  activity recognition with holistic and pose based features
In  GCPR
N0NN
N  [N0] S
Ren, K
He, R
Girshick, and J
Sun
Faster r-cnn: Towards  real-time object detection with region proposal networks
In  NIPS, N0NN
N  [NN] M
R
Ronchi and P
Perona
Describing common human  visual actions in images
In BMVC, N0NN
N  [NN] O
Russakovsky, J
Deng, H
Su, J
Krause, S
Satheesh,  S
Ma, Z
Huang, A
Karpathy, A
Khosla, M
Bernstein,  A
C
Berg, and L
Fei-Fei
ImageNet Large Scale Visual  Recognition Challenge
IJCV, N0NN
N  [NN] K
Simonyan and A
Zisserman
Very deep convolutional networks for large-scale image recognition
CoRR,  abs/NN0N.NNNN, N0NN
N  [NN] I
Sutskever, O
Vinyals, and Q
V
Le
Sequence to sequence  learning with neural networks
In NIPS, N0NN
N, N  [NN] R
Vedantam, C
Lawrence Zitnick, and D
Parikh
Cider:  Consensus-based image description evaluation
In CVPR,  N0NN
N  [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: A neural image caption generator
In CVPR, N0NN
N, N,  N  [NN] O
Vinyals, A
Toshev, S
Bengio, and D
Erhan
Show and  tell: Lessons learned from the N0NN mscoco image captioning challenge
TPAMI, N0NN
N, N, N, N  [NN] Q
Wu, C
Shen, L
Liu, A
Dick, and A
van den Hengel
 What value do explicit high level concepts have in vision to  language problems? In CVPR, N0NN
N  [NN] K
Xu, J
Ba, R
Kiros, K
Cho, A
C
Courville, R
Salakhutdinov, R
S
Zemel, and Y
Bengio
Show, attend and tell:  Neural image caption generation with visual attention
In  ICML, N0NN
N, N, N, N  [N0] B
Yao, X
Jiang, A
Khosla, A
L
Lin, L
Guibas, and L
FeiFei
Human action recognition by learning bases of action  attributes and parts
In ICCV, N0NN
N  [NN] T
Yao, Y
Pan, Y
Li, Z
Qiu, and T
Mei
Boosting image  captioning with attributes
arXiv preprint arXiv:NNNN.0NNNN,  N0NN
N  [NN] M
Yatskar, V
Ordonez, L
Zettlemoyer, and A
Farhadi
 Commonly uncommon: Semantic sparsity in situation recognition
arXiv:NNNN.00N0N, N0NN
N, N, N, N, N, N, N  [NN] M
Yatskar, L
Zettlemoyer, and A
Farhadi
Situation recognition: Visual semantic role labeling for image understanding
In CVPR, N0NN
N, N, N, N, N, N, N  [NN] Q
You, H
Jin, Z
Wang, C
Fang, and J
Luo
Image captioning with semantic attention
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, N0NN
 N, N, N  [NN] W
Zaremba, I
Sutskever, and O
Vinyals
Recurrent neural network regularization
arXiv preprint arXiv:NN0N.NNNN,  N0NN
N  NNN  https://github.com/karpathy/neuraltalkN https://github.com/karpathy/neuraltalkNLearning Video Object Segmentation With Visual Memory   Learning Video Object Segmentation with Visual Memory  Pavel Tokmakov Karteek Alahari  Inria∗  Cordelia Schmid  Abstract  This paper addresses the task of segmenting moving  objects in unconstrained videos
We introduce a novel  two-stream neural network with an explicit memory module to achieve this
The two streams of the network encode spatial and temporal features in a video sequence respectively, while the memory module captures the evolution of objects over time
The module to build a “visual  memory” in video, i.e., a joint representation of all the  video frames, is realized with a convolutional recurrent unit  learned from a small number of training video sequences
 Given a video frame as input, our approach assigns each  pixel an object or background label based on the learned  spatio-temporal features as well as the “visual memory”  specific to the video, acquired automatically without any  manually-annotated frames
The visual memory is implemented with convolutional gated recurrent units, which allows to propagate spatial information over time
We evaluate our method extensively on two benchmarks, DAVIS and  Freiburg-Berkeley motion segmentation datasets, and show  state-of-the-art results
For example, our approach outperforms the top method on the DAVIS dataset by nearly N%
 We also provide an extensive ablative analysis to investigate  the influence of each component in the proposed framework
 N
Introduction Video object segmentation is the task of extracting  spatio-temporal regions that correspond to object(s) moving in at least one frame in the video sequence
The  top-performing methods for this problem [NN, NN] continue  to rely on hand-crafted features and do not leverage a  learned video representation, despite the impressive results  achieved by convolutional neural networks (CNN) for other  vision tasks, e.g., image segmentation [NN], object detection [NN]
Very recently, there have been attempts to build  CNNs for video object segmentation [N, NN, NN]
They are  indeed the first to use deep learning methods for video segmentation, but suffer from various drawbacks
For example, [N, NN] rely on a manually-segmented subset of frames  (typically the first frame of the video sequence) to guide the  ∗Univ
Grenoble Alpes, Inria, CNRS, Grenoble INP, LJK, NN000  Grenoble, France
 Figure N
Sample results on the DAVIS dataset
Segmentations  produced by MP-Net [NN] (left) and our approach (right), overlaid  on the video frame
 segmentation pipeline
Our previous work [NN] relies solely  on optical flow between pairs of frames to segment independently moving objects in a video, making it susceptible  to errors in flow estimation
It also can not extract objects  if they stop moving
Furthermore, none of these methods  has a mechanism to memorize relevant features of objects  in a scene
In this paper, we propose a novel framework to  address these issues; see sample results in Figure N
 We present a two-stream network with an explicit memory module for video object segmentation (see Figure N)
 The memory module is a convolutional gated recurrent unit  (GRU) that encodes the spatio-temporal evolution of object(s) in the input video sequence
This spatio-temporal  representation used in the memory module is extracted from  two streams—the appearance stream which describes static  features of objects in the video, and the temporal stream  which captures motion cues
 The appearance stream is the DeepLab network [N] pretrained on the PASCAL VOC segmentation dataset and operates on individual video frames
The temporal one is a  motion prediction network [NN] pretrained on the synthetic  FlyingThingsND dataset and takes optical flow computed  from pairs of frames as input, as shown in Figure N
The two  streams provide complementary cues for object segmentation
With these spatio-temporal CNN features in hand,  we train the convolutional GRU component of the framework to learn a visual memory representation of object(s)  in the scene
Given a frame t from the video sequence as  NNNN    input, the network extracts its spatio-temporal features and  then: (i) computes the segmentation using the memory representation aggregated from all frames previously seen in  the video, and (ii) updates the memory unit with features  from t
The segmentation is improved further by process- ing the video bidirectionally in the memory unit, with our  bidirectional convolutional GRU
 The contributions of the paper are two-fold
First, we  present an approach for moving object segmentation in  unconstrained videos that does not require any manuallyannotated frames in the input video (see §N)
Our network  architecture incorporates a memory unit to capture the evolution of object(s) in the scene (see §N)
To our knowledge,  this is the first recurrent network based approach to accomplish the video segmentation task
It helps address challenging scenarios where the motion patterns of the object  change over time; for example, when an object in motion  stops to move, abruptly, and then moves again, with potentially a different motion pattern
Second, we present stateof-the-art results on two video object segmentation benchmarks, namely DAVIS [NN] and Freiburg-Berkeley motion  segmentation (FBMS) dataset [N0] (see §N.N)
Additionally,  we provide an extensive experimental analysis, with ablation studies to investigate the influence of all the components of our framework (see §N.N)
We will make the source  code and the models available online
 N
Related work Video object segmentation
Several approaches have been  proposed over the years to accomplish the task of segmenting objects in video
One of the more successful  ones presented in [N] clusters pixels spatio-temporally based  on motion features computed along individual point trajectories
Improvements to this framework include dense  trajectory-level segmentation [NN], an alternative clustering  method [NN], and detection of discontinuities in the trajectory spectral embedding [NN]
These trajectory based approaches lack robustness in cases where feature matching  fails
 An alternative to using trajectories is formulating the  segmentation problem as a foreground-background classification task [NN, NN, NN]
These methods first estimate  a region [NN, NN] or regions [NN], which correspond(s) to  the foreground object, and then use them to compute foreground and background appearance models
The final object segmentation is obtained by integrating these appearance models with other cues, e.g., saliency maps [NN], shape  estimates [NN], pairwise constraints [NN]
Variants to this  framework have introduced occlusion relations to compute  a layered video segmentation [NN], and long-range interactions to group re-occurring regions in video [NN]
Two  methods from this class of segmentation approaches [NN,NN]  show a good performance on the DAVIS benchmark
While  our proposed method is similar in spirit to this class of approaches, in terms of formulating segmentation as a classification problem, we differ from previous work significantly
 We propose an integrated approach to learn appearance and  motion features and update them with a memory module, in  contrast to estimating an initial region heuristically and then  propagating it over time
Our robust model outperforms all  these methods [NN, NN, NN, NN, NN], as shown in Section N.N
 Video object segmentation is also related to the task of  segmenting objects in motion, irrespective of camera motion
Two recent methods to address this task use optical  flow computed between pairs of frames [N, NN]
Classical  methods in perspective geometry and RANSAC-based feature matching are used in [N] to estimate moving objects  from optical flow
It achieved state-of-the-art performance  on a subset of the Berkeley motion segmentation (BMS)  dataset [N], but lacks robustness due to a heuristic initialization, as shown in the evaluation on DAVIS in Table N
 Our previous approach MP-Net [NN] learns to recognize motion patterns in a flow field
This frame-based approach  is the state of the art on DAVIS and is on par with [N] on  the BMS subset
Despite its excellent performance, MPNet is limited by its frame-based nature and also overlooks  appearance features of objects
These issues are partially  addressed in a heuristic post-processing step with objectness cues in [NN]
Nevertheless, the approach fails to extract objects if they stop moving, i.e., if no motion cues are  present
We use MP-Net as the temporal stream of our approach (see Figure N)
We show a principled way to integrate this stream with appearance information and a new  visual memory module based on convolutional gated recurrent units (ConvGRU)
As shown in Table N, our approach  outperforms MP-Net
 Very recently, two CNN-based methods for video object  segmentation were proposed [N, NN]
Starting with CNNs  pretrained for image segmentation, they find objects in  video by fine-tuning on the first frame in the sequence
Note  that this setup, referred to as semi-supervised segmentation,  is very different from the more challenging unsupervised  case we address in this paper, where no manually-annotated  frames are available for the test video
Furthermore, these  two CNN architectures are primarily developed for images,  and do not model temporal information in video
We, on  the other hand, propose a recurrent network specifically for  the video segmentation task
 Recurrent neural networks (RNNs)
RNN [NN, NN] is a  popular model for tasks defined on sequential data
Its main  component is an internal state that allows to accumulate information over time
The internal state in classical RNNs is  updated with a weighted combination of the input and the  previous state, where the weights are learned from training data for the task at hand
Long short-term memory  (LSTM) [N0] and gated recurrent unit (GRU) [N] architectures are improved variants of RNN, which partially mitNNNN    Figure N
Overview of our segmentation approach
Each video frame is processed by the appearance (green) and the motion (yellow)  networks to produce an intermediate two-stream representation
The ConvGRU module combines this with the learned visual memory to  compute the final segmentation result
The width (w’) and height (h’) of the feature map and the output are w/N and h/N respectively
 igate the issue of vanishing gradients [NN, NN]
They introduce gates with learnable parameters, to update the internal state selectively, and can propagate gradients further  through time
 Recurrent models, originally used for text and speech  recognition, e.g., [NN, NN], are becoming increasingly popular for visual data
Initial work on vision tasks, such as image captioning [N], future frame prediction [N0] and action  recognition [NN], has represented the internal state of the recurrent models as a ND vector—without encoding any spatial information
LSTM and GRU architectures have been  extended to address this issue with the introduction of ConvLSTM [NN,NN,NN] and ConvGRU [N] respectively
In these  convolutional recurrent models the state and the gates are  ND tensors and the weight vectors are replaced by ND convolutions
These models have only recently been applied  to vision tasks, such as video frame prediction [NN, NN, NN],  action recognition and video captioning [N]
 In this paper, we employ a visual memory module based  on a convolutional GRU (ConvGRU) and show that it is  an effective way to encode the spatio-temporal evolution of  objects in video for segmentation
Further, to fully benefit  from all the frames in a video sequence, we apply the recurrent model bidirectionally [NN, NN], i.e., apply two identical  model instances on the sequence in forward and backward  directions, and combine the predictions for each frame
 N
Approach Our model takes video frames together with their estimated optical flow as input, and outputs binary segmentations of moving objects, as shown in Figure N
We target  the most general form of this task, wherein objects are to  be segmented in the entire video if they move in at least  one frame
The proposed model is comprised of three key  components: appearance and motion networks, and a visual  memory module
 Appearance network
The purpose of the appearance  stream is to produce a high-level encoding of a frame that  will later aid the visual memory module in forming a representation of the moving object
It takes an RGB frame  as input and produces a NNN × w/N × h/N feature repre- sentation (shown in green in Figure N)
This encodes the  semantic content of the scene
We use a state-of-the-art  CNN for this stream, namely the largeFOV version of the  DeepLab network [N]
This network relies on dilated convolutions [N], which preserve a relatively high spatial resolution of features, and also incorporate context information  in each pixel’s representation
It is pretrained on a semantic segmentation dataset, PASCAL VOC N0NN [N0], resulting in features that can distinguish objects from background  as well as from each other—a crucial aspect for the video  object segmentation task
We extract features from the fcN  layer of the network, which has a feature dimension of N0NN  for each pixel
This feature map is further passed through  two N × N convolutional layers, interleaved with tanh non- linearities, to reduce the dimension to NNN
These layers are  trained together with ConvGRU (see §N.N for details)
 Motion network
For the temporal stream we employ MPNet [NN], a CNN pretrained for the motion segmentation  task
It is trained to estimate independently moving objects  (i.e., irrespective of camera motion) based on optical flow  computed from a pair of frames as input (shown in yellow  in Figure N)
This stream produces a w/N×h/N motion pre- diction output, where each value represents the likelihood  of the corresponding pixel being in motion
Its output is  further downsampled by a factor N (in w and h) to match the  dimensions of the appearance stream output
 The intuition behind using two streams is to benefit from  their complementarity for building a strong representation  of objects that evolves over time
For example, both appearance and motion networks are equally effective when  an object is moving in the scene, but as soon as it becomes  stationary, the motion network can not estimate the object,  unlike the appearance network
We leverage this complementary nature, as done by two-stream networks for other  vision tasks [NN]
Note that our approach is not specific to  the particular networks described above, but is in fact a general framework for video object segmentation
As shown is  the Section N.N, its components can easily be replaced with  other networks, providing scope for future improvement
 Memory module
The third component, i.e., a visual memory module based on convolutional gated units (ConvGRU),  takes the concatenation of appearance and motion stream  outputs as its input
It refines the initial estimates from these  two networks, and also memorizes the appearance and location of objects in motion to segment them in frames where:  (i) they are static, or (ii) motion prediction fails; see the  NNNN    Figure N
Illustration of ConvGRU with details for the candidate  hidden state module, where h̃t is computed with two convolutional operations and a tanh nonlinearity
 example in Figure N
The output of this ConvGRU memory  module is a NN×w/N×h/N feature map obtained by combin- ing the two-stream input with the internal state of the memory module, as described in detail in Section N
We further  improve the model by processing the video bidirectionally;  see Section N.N
The output from the ConvGRU module  is processed by a N × N convolutional layer and softmax nonlinearity to produce the final pixelwise segmentation result
These layers are trained together with ConvGRU, as  detailed in Section N.N
 N
Visual memory module The key component of the ConvGRU module is the state  matrix h, which encodes the visual memory
For frame t in the video sequence, ConvGRU uses the two-stream representation xt and the previous state ht−N to compute the new state ht
The dynamics of this computation are guided by an update gate zt, a forget gate rt
The states and the gates are ND tensors, and can characterize spatio-temporal patterns in  the video, effectively memorizing which objects move, and  where they move to
These components are computed with  convolutional operators and nonlinearities as follows
 zt = σ(xt ∗ wxz + ht−N ∗ whz + bz), (N)  rt = σ(xt ∗ wxr + ht−N ∗ whr + br), (N)  h̃t = tanh(xt ∗ wxh̃ + rt ⊙ ht−N ∗ whh̃ + bh̃), (N)  ht = (N− zt)⊙ ht−N + zt ⊙ h̃t, (N)  where ⊙ denotes element-wise multiplication, ∗ represents  a convolutional operation, σ is the sigmoid function, w’s are learned transformations, and b’s are bias terms
 The new state ht in (N) is a weighted combination of the previous state ht−N and the candidate memory h̃t
The up- date gate zt determines how much of this memory is incor- porated into the new state
If zt is close to zero, the mem- ory represented by h̃t is ignored
The reset gate rt controls  Figure N
Illustration of the bidirectional processing with our ConvGRU module
 the influence of the previous state ht−N on the candidate memory h̃t in (N), i.e., how much of the previous state is let through into the candidate memory
If rt is close to zero, the unit forgets its previously computed state ht−N
 The gates and the candidate memory are computed with  convolutional operations over xt and ht−N shown in equa- tions (N-N)
We illustrate the computation of the candidate  memory state h̃t in Figure N
The state at t−N, ht−N, is first multiplied (element-wise) with the reset gate rt
This mod- ulated state representation and the input xt are then con- volved with learned transformations, w  hh̃ and w  xh̃ respectively, summed together with a bias term b h̃  , and passed  through a tanh nonlinearity
In other words, the visual memory representation of a pixel is determined not only by  the input and the previous state at that pixel, but also its local neighborhood
Increasing the size of the convolutional  kernels allows the model to handle spatio-temporal patterns  with larger motion
 The update and reset gates, zt and rt, are computed in an analogous fashion using a sigmoid function instead of  tanh
Our ConvGRU applies a total of six convolutional operations at each time step
All the operations detailed  here are fully differentiable, and thus the parameters of the  convolutions (w’s and b’s) can be trained in an end-to-end fashion with back propagation through time [NN]
In summary, the model learns to combine appearance features of  the current frame with the memorized video representation  to refine motion predictions, or even fully restore them from  the previous observations in case a moving object becomes  stationary
 N.N
Bidirectional processing Consider an example where an object is stationary at the  beginning of a video sequence, and starts to move in the latter frames
Our approach described so far, which processes  video frames sequentially (in the forward direction), can not  segment the object in the initial frames
This is due to the  lack of prior memory representation of the object in the first  frame
We improve our framework with a bidirectional processing step, inspired by the application of recurrent models  bidirectionally in the speech domain [NN, NN]
 The bidirectional variant of our ConvGRU is illustrated  NNNN    in Figure N
It is composed of two ConvGRU instances with  identical learned weights, which are run in parallel
The  first one processes frames in the forward direction, starting  with the first frame (shown at the bottom in the figure)
The  second instance process frames in the backward direction,  starting with the last video frame (shown at the top in the  figure)
The activations from these two directions are concatenated at each time step, as shown in the figure, to produce a NNN×w/N× h/N output
It is then passed through a N×N convolutional layer to finally produce a NN×w/N×h/N for each frame
Pixel-wise segmentation is then obtained  with a final N× N convolutional layer and a softmax nonlin- earity, as in the unidirectional case
 Bidirectional ConvGRU is used both in training and in  testing, allowing the model to learn to aggregate information over the entire video
In addition to handling cases  where objects move in the latter frames, it improves the  ability of the model to correct motion prediction errors
As  discussed in the experimental evaluation, bidirectional ConvGRU improves segmentation performance by nearly N%  on the DAVIS dataset (see Table N)
The influence of bidirectional processing is more prominent on FBMS, where  objects can be static in the beginning of a video, with N%  improvement over the unidirectional variant
 N.N
Training  We train our visual memory module with the back propagation through time algorithm [NN], which unrolls the recurrent network for n time steps and keeps all the intermediate activations to compute the gradients
Thus, our ConvGRU  model, which has N internal convolutional layers, trained on  a video sequence of length n, is equivalent to a Nn layer CNN for the unidirectional variant, or NNn for the bidirec- tional model at training time
This memory requirement  makes it infeasible to train the whole model, including appearance and motion streams, end-to-end
We resort to using pretrained versions of the appearance and motion networks and train the ConvGRU
 We use the training split of the DAVIS dataset [NN] for  learning the ConvGRU weights
Objects move in all the  frames in this dataset, which biases the memory module towards the presence of an uninterrupted motion stream
This  results in the ConvGRU learned from this data failing, when  an object stops to move in a test sequence
We augment  the training data to simulate such stop-and-go scenarios and  thus learn a more robust model for realistic videos
 We create additional training sequences, where ground  truth moving object segmentation (instead of responses  from the motion network) is provided for all the frames,  except for the last five frames, which are duplicated, simulating a case where objects stop moving
No motion input is  used for these last five frames
These artificial examples are  used in place of the regular ones for a fixed fraction of iterations
Replacing motion stream predictions with ground  Aspect Variant Mean IoU  Ours (fcN, ConvGRU, Bidir, DAVIS) N0.N  App stream  no NN.N  RGB NN.N  N-layer CNN N0.N  DeepLab fcN NN.N  DeepLab convN NN.N  App pretrain ImageNet only NN.N  Motion stream no NN.N  Memory module  ConvRNN NN.N  ConvLSTM NN.N  no NN.N  Bidir processing no NN.N  Train data FTND GT Flow NN.N  FTND LDOF Flow NN.N  Table N
Ablation study on the DAVIS validation set showing  variants of appearance and motion streams and memory module
 “Ours” refers to the model using fcN features together with a motion stream, and a bidirectional ConvGRU trained on DAVIS
 truth segmentations for these sequences allows to decouple  the task of motion mistake correction from the task of object  tracking, which simplifies the learning
Given that ground  truth segmentation determines the loss for training, i.e., it  is used for all the frames, ConvGRU explicitly memorizes  the moving object in the initial part of the sequence, and  then segments it in frames where motion is missing
We do  a similar training set augmentation by duplicating the first  five frames in a batch, to simulates the cases where an object  is static in the beginning of a video
 N
Experiments N.N
Datasets and evaluation  We use four datasets in the experimental analysis:  DAVIS for training and test, FBMS and SegTrack-vN only  for test, and FTND for training a variant of our approach
 DAVIS
It contains N0 full HD videos with accurate pixellevel annotation in all the frames [NN]
The annotations correspond to the task of video object segmentation
Following  the N0/N0 training/validation split provided with the dataset,  we train on the N0 sequences, and test on the N0 validation  videos
We also follow the standard protocol for evaluation  from [NN], and report intersection over union, F-measure for  contour accuracy and temporal stability
 FBMS
The Freiburg-Berkeley motion segmentation  dataset [N0] is composed of NN videos with ground truth  annotations in a subset of the frames
In contrast to DAVIS,  it has multiple moving objects in several videos with  instance-level annotations
Also, objects may move only in  a fraction of the frames, but they are annotated in frames  where they do not exhibit independent motion
The dataset  is split into training and test sets
Following the standard  protocol on this dataset [NN], we do not train on any of these  sequences, and evaluate separately for both with precision,  NNNN    recall and F-measure scores
We also convert instance-level  annotation to binary ones by merging all the foreground  labels into a single category, as in [NN]
 SegTrack-vN
It contains NN videos with instance-level  moving object annotations in all the frames
We convert  these annotations into a binary form for evaluation and use  intersection over union as a performance measure
 FTND
The FlyingThingsND dataset [NN] consists of NNN0  synthetic videos for training, composed of N0 frames, where  objects are in motion along random trajectories in rendered  scenes
Ground truth optical flow, depth, camera parameters, and instance segmentations are provided by [NN], and  the ground truth motion segmentation is available from [N]
 N.N
Implementation details  We train our model by minimizing binary crossentropy loss using back-propagation through time and RMSProp [NN] with a learning rate of N0−N
The learning rate is gradually decreased after every epoch
The weight decay is set to 0.00N
Initialization of all the convolutional layers, except for those inside the ConvGRU, is done with  the standard xavier method [NN]
We clip the gradients to  the [−N0, N0] range before each parameter update, to avoid numerical issues [NN]
We form batches of size NN by randomly selecting a video, and a subset of NN consecutive  frames in it
Random cropping and flipping of sequences  is also performed for data augmentation
Our full model  uses N × N convolutions in all the ConvGRU operations
The weights of the two N × N convolutional (dimension- ality reduction) layers in the appearance network and the  final N× N convolutional layer following the memory mod- ule are learned jointly with the memory module
The model  is trained for N0000 iterations and the proportion of batches  with additional sequences (see Section N.N) is set to N0%
 Our final model uses a fully-connected CRF [NN] to refine boundaries in a post-processing step
The parameters of  this CRF are taken from [NN]
In the experiments where objectness is used, it is also computed according to [NN]
We  use LDOF [N] for optical flow estimation and convert the  raw flow to flow angle field, as in [NN]
We used the code  and the trained models for MP-Net available at [N]
Our  method is implemented in the Torch framework and will be  made available online
Many sequences in FBMS are several hundred frames long and do not fit into GPU memory  during evaluation
We apply our method in a sliding window fashion in such cases, with a window of NN0 frames  and a step size of N0
 N.N
Ablation study  Table N demonstrates the influence of different components of our approach on the DAVIS validation set
First,  we study the role of the appearance stream
As a baseline, we remove it completely (“no” in the “App stream”  in the table), i.e., the output of the motion stream is the  Method Mean IoU  Ours N0.N  Ours + CRF NN.N  MP-Net NN.N  MP-Net + Obj NN.N  MP-Net + Obj + FST (MP-Net-V) NN.0  MP-Net + Obj + CRF (MP-Net-F) N0.0  Table N
Comparison to MP-Net [NN] variants on the DAVIS validation set
“Obj” refers to the objectness cues used in [NN]
MPNet-V(ideo) and MP-Net-F(rame) are variants of MP-Net which  use FST [NN] and CRF respectively, in addition to objectness
 only input to our visual memory module
In this setting,  the memory module lacks sufficient information to produce  accurate segmentations, which results in an NN.N% drop in  performance compared to the method where the appearance stream with fcN features is used (“Ours” in the table)
We then provide raw RGB frames, concatenated with  the motion prediction, as input to the ConvGRU
This simplest form of image representation leads to a NN.N% improvement, compared to the motion only model, showing  the importance of the appearance features
The variant  where RGB input is passed through two convolutional layers, interleaved with tanh nonlinearities, that are trained jointly with the memory module (“N-layer CNN”), further  improves this
This shows the potential of learning appearance representation as a part of the video segmentation  pipeline
Finally, we compare features extracted from the  fcN and convN layers of the DeepLab model to those from  fcN used by default in our method
Features from fcN and  fcN show comparable performance, but fcN ones are more  expensive to compute
ConvN features perform significantly  worse, perhaps due to a smaller field of view
 The importance of appearance network pretrained on the  semantic segmentation task is highlighted by the “ImageNet  only” variant in Table N, where the PASCAL VOC pretrained segmentation network is replaced with a network  trained on ImageNet classification
Although ImageNet  pretraining provides a rich feature representation, it is less  suitable for the video object segmentation task, which is  confirmed by an N% drop in performance
Discarding the  motion information (“no” in “Motion stream”), although  being N0.N% below our complete method, still outperforms  most of the motion-based approaches on DAVIS (see Table N)
This variant learns foreground/background segmentation, which is sufficient for videos with a single dominant  object, but fails in more challenging cases
 Next, we evaluate the design choices in the visual memory module
Using a simple recurrent model (ConvRNN)  results in a slight decrease in performance
Such simpler  architectures can be used in case of a memory vs segmentation quality trade off
The other variant using ConvLSTM  is comparable to ConvRNN, possibly due to the lack of sufficient training data
We also evaluated the influence of the  NNNN    Measure PCM [N] CVOS [NN] KEY [NN] MSG [N] NLC [NN] CUT [NN] FST [NN] MP-Net-F [NN] Ours  J  Mean N0.N NN.N NN.N NN.N NN.N NN.N NN.N N0.0 NN.N  Recall NN.N NN.0 NN.N NN.N NN.N NN.N NN.N NN.0 NN.N  Decay NN.N N0.N NN.N N.N NN.N N.N 0.0 N.N 0.0  F  Mean NN.N NN.N NN.N N0.N NN.N NN.N NN.N NN.N NN.N  Recall NN.N NN.N NN.N N0.0 NN.N NN.0 NN.N NN.N NN.N  Decay NN.N NN.N N0.N N.N NN.N N.N N.N N.N N.N  T Mean NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N NN.N  Table N
Comparison to state-of-the-art methods on DAVIS with intersection over union (J ), F-measure (F ), and temporal stability (T )
 Ground truth CUT [NN] FST [NN] MP-Net-Frame [NN] Ours Figure N
Qualitative comparison with top-performing methods on DAVIS
Left to right: ground truth, results of CUT [NN], FST [NN],  MP-Net-Frame [NN], and our method
 memory module (ConvGRU) by replacing it with a stack of  N convolutional layers to obtain a memoryless variant of our  model (“no” in “Memory module” in Table N), but with the  same number of parameters
This variant results in a N%  drop in performance compared to our full model
The performance of the memoryless variant is comparable to [NN]  (NN.N), the approach without any memory
Using only a unidirectional processing decreases the performance by nearly  N% (“no” in “Bidir processing”)
 Lastly, we train two variants (“FTND GT Flow” and  “FTND LDOF Flow”) on the synthetic FTND dataset [NN]  instead of DAVIS
Both of them show a significantly lower  performance than our method trained on DAVIS
This is  due to the appearance of synthetic FTND videos being very  different from the real-world ones
The variant trained on  ground truth flow (GT Flow) is inferior to that trained on  LDOF flow because the motion network (MP-Net) achieves  a high performance on FTND with ground truth flow, and  thus our visual memory module learns to simply follow the  motion stream output
We provide a visualization of the  gate activations in the arXiv version of the paper [NN]
 N.N
Comparison to MP-Net variants  In Table N we compare our method to MP-Net and its  variants presented in [NN] on the DAVIS validation set
Our  visual memory-based approach (“Ours” in the table) outperforms the MP-Net baseline (“MP-Net”), which serves as  the motion stream in our model, by NN.N%
This clearly  demonstrates the value of the appearance stream and our  memory unit for video segmentation
The post-processing  variants in [NN], using objectness cues, CRF, and video segmentation method [NN], improve this baseline, but remain  inferior to our result
Our full method (“Ours + CRF”) is  nearly N% better than “MP-Net-Frame”, which is the best  performing MP-Net variant on DAVIS
Note that “MP-NetVideo” which combines MP-Net with objectness cues and  the video segmentation method of [NN] is also inferior to  our method, as it relies strongly on the tracking capabilities  of [NN], which is prone to segmentation leaking in case of  errors in the flow estimation
The example in the first row  in Figure N shows a typical error of [NN]
 MP-Net-Video performs better than MP-Net-Frame on  the FBMS dataset (see Table N) since the frame-only variant  does not segment objects when they stop moving
The propagation of segment(s) over time with tracking in MP-NetVideo addresses this, but is less precise due to segmentation  leaks, as shown by the comparison with precision measure  in the table and the qualitative results in Figure N
 N.N
Comparison to the state-of-the-art  DAVIS
Table N compares our approach to the state-ofthe-art methods on DAVIS
In addition to comparing our  results to the top-performing unsupervised approaches reported in [NN], we evaluated two more recent methods:  CUT [NN] and PCM [N], with the authors’ implementation
Our method outperforms MP-Net-Frame, the previous state of the art, by N.N% on the IoU measure, and is  N0.N% better than the next best method [NN]
We also obNNNN    Measure Set KEY [NN] MP-Net-F [NN] FST [NN] CVOS [NN] CUT [NN] MP-Net-V [NN] Ours  P Training NN.N NN.0 NN.N NN.N NN.N NN.N N0.N  Test NN.N NN.0 NN.N NN.N NN.N NN.N NN.N  R Training NN.N NN.N N0.N NN.0 N0.N N0.N NN.N  Test NN.0 NN.N NN.N NN.N NN.N NN.N NN.N  F Training NN.N NN.N NN.0 NN.N NN.N NN.N NN.N  Test NN.0 NN.N NN.N NN.N NN.N NN.N NN.N  Table N
Comparison to state-of-the-art methods on FBMS with precision (P), recall (R), and F-measure (F )
 CUT [NN] MP-Net-Video [NN] Ours  Figure N
Qualitative comparison with top-performing methods on  FBMS
Left to right: results of CUT [NN], MP-Net-Video [NN], and  our method
 serve a N0.N% improvement in temporal stability over MPNet-Frame
PCM [N], which performs well on a subset of  FBMS (as shown in [NN]), is in fact significantly worse on  DAVIS
 Figure N shows qualitative results of our approach, and  the next three top-performing methods on DAVIS: MP-NetFrame [NN], FST [NN] and CUT [NN]
In the first row, both  CUT and our method segment the dancer, but our result is  more accurate
FST leaks to the people in the background  and MP-Net misses parts of the person due to the incorrectly  estimated objectness
Our approach does not include any  heuristics, which makes it robust to this type of errors
In  the next row, our approach shows very high precision, being  able to correctly separate the dog and the pole occluding it
 In the last row, we illustrate a failure case of our method
 The people in the background move in some of the frames  in this example
CUT, MP-Net and our method segment  them to varying extents
FST focuses on the foreground  object, but leaks to the background partially nevertheless
 FBMS
As shown in Table N, MP-Net-Frame [NN] is outperformed by most of the methods on this dataset
Our  approach based on visual memory outperforms MP-NetFrame by NN.N% on the test set and by NN.N% on the  training set according to the F-measure
FST [NN] based  post-processing (“MP-Net-V” in the table) significantly improves the results of MP-Net on FBMS, but it remains below our approach on both precision and F-measure
OverCUT [NN] FST [NN] NLC [NN] Ours  NN.N NN.N NN.N NN.N  Table N
Comparison to state-of-the-art methods on SegTrack-vN  with mean IoU
 all, our method shows top results in terms of precision and  F-measure but is outperformed by some methods on recall
This is due to very long static sequences in FBMS,  which our recurrent memory-based method can not handle  as well as methods with explicit tracking components, such  as CUT [NN]
 Figure N shows qualitative results of our method and  the two next-best methods on FBMS: MP-Net-Video [NN]  and CUT [NN]
MP-Net-Video relies highly on FST’s [NN]  tracking capabilities, and thus demonstrates the same background leaking failure mode, as seen in all the three examples
CUT misses parts of objects and incorrectly assigns background regions to the foreground in some cases,  whereas our method demonstrates very high precision
 SegTrack-vN
Our approach achieves IoU of NN.N on this  dataset
The relatively lower IoU compared to DAVIS is  mainly due to the low resolution of some of the SegTrackvN videos, which differ from the high resolution ones used  for training
We have also evaluated the state-of-the-art approaches for comparison on SegTrack
As shown in Table N, our method performs better than [NN,NN] on SegTrack,  but worse than NLC [NN]
Note that NLC was designed  and evaluated on SegTrack; we outperform it on DAVIS by  N0.N% (see Table N)
 N
Conclusion This paper introduces a novel approach for video object  segmentation
Our method combines two complementary  sources of information: appearance and motion, with a visual memory module, realized as a bidirectional convolutional gated recurrent unit
The memory module encodes  spatio-temporal evolution of objects in a video and uses this  encoding to improve motion segmentation
The effectiveness of our approach is validated on the DAVIS and FBMS  datasets, where it shows top performance
Instance-level  video object segmentation is a promising direction for future work
 Acknowledgments
This work was supported in part by the ERC  advanced grant ALLEGRO, a Google research award, a Facebook  and an Intel gift
We gratefully acknowledge NVIDIAs support  with the donation of GPUs used for this work
 NNNN    References  [N] Learning motion patterns in videos
http://thoth
 inrialpes.fr/research/mpnet
 [N] N
Ballas, L
Yao, C
Pal, and A
Courville
Delving deeper  into convolutional networks for learning video representations
ICLR, N0NN
 [N] P
Bideau and E
G
Learned-Miller
It’s moving! A probabilistic model for causal motion segmentation in moving  camera videos
In ECCV, N0NN
 [N] T
Brox and J
Malik
Object segmentation by long term  analysis of point trajectories
In ECCV, N0N0
 [N] T
Brox and J
Malik
Large displacement optical flow: descriptor matching in variational motion estimation
PAMI,  N0NN
 [N] S
Caelles, K.-K
Maninis, J
Pont-Tuset, L
Leal-Taixé,  D
Cremers, and L
Van Gool
One-shot video segmentation
 In CVPR, N0NN
 [N] L.-C
Chen, G
Papandreou, I
Kokkinos, K
Murphy, and  A
L
Yuille
Semantic image segmentation with deep convolutional nets and fully connected CRFs
In ICLR, N0NN
 [N] K
Cho, B
van Merrienboer, Ç
Gülçehre, F
Bougares,  H
Schwenk, and Y
Bengio
Learning phrase representations  using RNN encoder-decoder for statistical machine translation
In EMNLP, N0NN
 [N] J
Donahue, L
A
Hendricks, S
Guadarrama, M
Rohrbach,  S
Venugopalan, K
Saenko, and T
Darrell
Long-term recurrent convolutional networks for visual recognition and description
In CVPR, N0NN
 [N0] M
Everingham, L
Van Gool, C
K
I
Williams,  J
Winn, and A
Zisserman
The PASCAL Visual Object Classes Challenge N0NN (VOCN0NN) Results
http:  //www.pascal-network.org/challenges/VOC/  vocN0NN/workshop/index.html
 [NN] A
Faktor and M
Irani
Video segmentation by non-local  consensus voting
In BMVC, N0NN
 [NN] C
Finn, I
Goodfellow, and S
Levine
Unsupervised learning for physical interaction through video prediction
In  NIPS, N0NN
 [NN] K
Fragkiadaki, G
Zhang, and J
Shi
Video segmentation by  tracing discontinuities in a trajectory embedding
In CVPR,  N0NN
 [NN] X
Glorot and Y
Bengio
Understanding the difficulty of  training deep feedforward neural networks
In AISTATS,  N0N0
 [NN] A
Graves
Generating sequences with recurrent neural networks
arXiv preprint arXiv:NN0N.0NN0, N0NN
 [NN] A
Graves, N
Jaitly, and A
Mohamed
Hybrid speech recognition with deep bidirectional LSTM
In Workshop on Automatic Speech Recognition and Understanding, N0NN
 [NN] A
Graves, A
Mohamed, and G
Hinton
Speech recognition  with deep recurrent neural networks
In ICASSP, N0NN
 [NN] A
Graves and J
Schmidhuber
Framewise phoneme classification with bidirectional LSTM and other neural network  architectures
Neural Networks, NN(N):N0N–NN0, N00N
 [NN] S
Hochreiter
The vanishing gradient problem during learning recurrent neural nets and problem solutions
Int
J
Uncertain
Fuzziness Knowl.-Based Syst., NNNN
 [N0] S
Hochreiter and J
Schmidhuber
Long short-term memory
 Neural computation, N(N):NNNN–NNN0, NNNN
 [NN] J
J
Hopfield
Neural networks and physical systems with  emergent collective computational abilities
Proc
National  Academy of Sciences, NN(N):NNNN–NNNN, NNNN
 [NN] M
Keuper, B
Andres, and T
Brox
Motion trajectory segmentation via minimum cost multicuts
In ICCV, N0NN
 [NN] A
Khoreva, F
Perazzi, R
Benenson, B
Schiele, and  A
Sorkine-Hornung
Learning video object segmentation  from static images
In CVPR, N0NN
 [NN] P
Krähenbühl and V
Koltun
Efficient inference in fully  connected CRFs with Gaussian edge potentials
In NIPS,  N0NN
 [NN] Y
J
Lee, J
Kim, and K
Grauman
Key-segments for video  object segmentation
In ICCV, N0NN
 [NN] N
Mayer, E
Ilg, P
Häusser, P
Fischer, D
Cremers,  A
Dosovitskiy, and T
Brox
A large dataset to train convolutional networks for disparity, optical flow, and scene flow  estimation
In CVPR, N0NN
 [NN] T
Mikolov, M
Karafiát, L
Burget, J
Cernockỳ, and S
Khudanpur
Recurrent neural network based language model
In  Interspeech, N0N0
 [NN] J
Y
Ng, M
J
Hausknecht, S
Vijayanarasimhan, O
Vinyals,  R
Monga, and G
Toderici
Beyond short snippets: Deep  networks for video classification
In CVPR, N0NN
 [NN] P
Ochs and T
Brox
Object segmentation in video: a hierarchical variational approach for turning point trajectories into  dense regions
In ICCV, N0NN
 [N0] P
Ochs, J
Malik, and T
Brox
Segmentation of moving objects by long term video analysis
PAMI, NN(N):NNNN–NN00,  N0NN
 [NN] A
Papazoglou and V
Ferrari
Fast object segmentation in  unconstrained video
In ICCV, N0NN
 [NN] R
Pascanu, T
Mikolov, and Y
Bengio
On the difficulty of  training recurrent neural networks
ICML, N0NN
 [NN] V
Patraucean, A
Handa, and R
Cipolla
Spatio-temporal  video autoencoder with differentiable memory
In ICLR  Workshop track, N0NN
 [NN] F
Perazzi, J
Pont-Tuset, B
McWilliams, L
V
Gool,  M
Gross, and A
Sorkine-Hornung
A benchmark dataset  and evaluation methodology for video object segmentation
 In CVPR, N0NN
 [NN] P
O
Pinheiro, T.-Y
Lin, R
Collobert, and P
Dollár
Learning to refine object segments
ECCV, N0NN
 [NN] S
Ren, K
He, R
Girshick, and J
Sun
Faster R-CNN: Towards real-time object detection with region proposal networks
In NIPS, N0NN
 [NN] D
E
Rumelhart, G
E
Hinton, and R
J
Williams
Learning  representations by back-propagating errors
Nature, NNNN
 [NN] X
Shi, Z
Chen, H
Wang, D.-Y
Yeung, W
Wong, and  W
Woo
Convolutional LSTM network: A machine learning  approach for precipitation nowcasting
In NIPS, N0NN
 [NN] K
Simonyan and A
Zisserman
Two-stream convolutional  networks for action recognition in videos
In NIPS, N0NN
 [N0] N
Srivastava, E
Mansimov, and R
Salakhutdinov
Unsupervised learning of video representations using LSTMs
In  ICML, N0NN
 NNNN  http://thoth.inrialpes.fr/research/mpnet http://thoth.inrialpes.fr/research/mpnet http://www.pascal-network.org/challenges/VOC/vocN0NN/workshop/index.html http://www.pascal-network.org/challenges/VOC/vocN0NN/workshop/index.html http://www.pascal-network.org/challenges/VOC/vocN0NN/workshop/index.html   [NN] B
Taylor, V
Karasev, and S
Soatto
Causal video object segmentation from persistence of occlusions
In CVPR, N0NN
 [NN] T
Tieleman and G
Hinton
RMSProp
COURSERA: Lecture N.N - Neural Networks for Machine Learning, N0NN
 [NN] P
Tokmakov, K
Alahari, and C
Schmid
Learning motion  patterns in videos
In CVPR, N0NN
 [NN] P
Tokmakov, K
Alahari, and C
Schmid
Learning video  object segmentation with visual memory
arXiv:NN0N.0NNNN,  N0NN
 [NN] W
Wang, J
Shen, and F
Porikli
Saliency-aware geodesic  video object segmentation
In CVPR, N0NN
 [NN] P
J
Werbos
Backpropagation through time: what it does  and how to do it
Proc
IEEE, NN(N0):NNN0–NNN0, NNN0
 NNN0